2910
Polynomials with 0-1 coefficients that are hard to evaluate
16th Annual Symposium on Foundations of Computer Science
None
1975
We show the existence of polynomials with 0-1 coefficients that are hard to evaluate even when arbitrary preconditioning is allowed. Further we show that there are power series with 0-1 coefficients such that their initial segments are hard to evaluate.
[Computer science, Costs, Computational modeling, Polynomials]
Characterization of the synchronization languages for PV systems
17th Annual Symposium on Foundations of Computer Science
None
1976
A basic question in the area of asynchronous computation is: Given a synchronization problem, what synchronization primitives are needed for a solution? This paper is directed toward answering this question by characterizing the "behavior" of synchronization systems incorporating PV, PV multiple, PV chunk and PV general synchronization primitives.
[Computer science, Computational modeling, Mathematics]
Conflunt reductions: Abstract properties and applications to term rewriting systems
18th Annual Symposium on Foundations of Computer Science
None
1977
This paper gives new results, and presents old ones in a unified formalism, concerning Church-Rosser theorems for rewriting systems. Part 1 gives abstract confluence properties, depending solely on axioms for a binary relation called reduction. Results of Newman and others are presented in a unified formalism. Systematic use of a powerful induction principle permits to generalize results of Sethi on reduction modulo equivalence. Part 2 concerns simplification systems operating on terms of a first-order logic. Results by Rosen and Knuth and Bendix are extended to give several new criteria for confluence of these systems, using the results of part 1. It is then shown how these results yield efficient methods for the mechanization of equational theories.
[Induction generators, Logic, Equations]
Should tables be sorted?
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Computer science, Costs, Information retrieval, Data structures, Probes, Artificial intelligence]
Efficient computation of continuous skeletons
20th Annual Symposium on Foundations of Computer Science
None
1979
An O(n lgn) algorithm is presented for the construction of skeletons of arbitrary n-line polygonal figures. This algorithm is based on an O(n lgn) algorithm for the construction of generalized Voronoi diagrams (our generalization replaces point sets by sets of line segments constrained to intersect only at end points). The generalized Voronoi diagram algorithm employs a linear time algorithm for the merging of two arbitrary (standard) Voronoi diagrams.
[Computer science, Geography, Merging, Interference, Biology computing, Approximation algorithms, Skeleton, Pattern recognition, Application software, Robots]
An O(v|v| c |E|) algoithm for finding maximum matching in general graphs
21st Annual Symposium on Foundations of Computer Science
None
1980
In this paper we present an 0(&#x0221A;|V|&#x000B7;|E|) algorithm for finding a maximum matching in general graphs. This algorithm works in 'phases'. In each phase a maximal set of disjoint minimum length augmenting paths is found, and the existing matching is increased along these paths. Our contribution consists in devising a special way of handling blossoms, which enables an O(|E|) implementation of a phase. In each phase, the algorithm grows Breadth First Search trees at all unmatched vertices. When it detects the presence of a blossom, it does not 'shrink' the blossom immediately. Instead, it delays the shrinking in such a way that the first augmenting path found is of minimum length. Furthermore, it achieves the effect of shrinking a blossom by a special labeling procedure which enables it to find an augmenting path through a blossom quickly.
[Scholarships, Labeling, History, Delay]
On the asymptotic complexity of matrix multiplication
22nd Annual Symposium on Foundations of Computer Science
None
1981
The main results of this paper have the following flavor: given one algorithm for multiplying matrices, there exists another, better, algorithm. A consequence of these results is that &#x03C9;, the exponent for matrix multiplication, is a limit point, that is, cannot be realized by any single algorithm. We also use these results to construct a new algorithm which shows that &#x03C9; &#x226A; 2.495364.
[Tensile stress, Equations, Arithmetic]
Generalised symmetries of polynomials in algebraic complexity
23rd Annual Symposium on Foundations of Computer Science
None
1982
false
[Computer science, Discrete transforms, Convolution, Polynomials, Robustness]
Monte-Carlo algorithms for enumeration and reliability problems
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Boolean functions, Fasteners, Approximation algorithms, Polynomials, Random variables, Approximation methods, Read only memory]
How To Share Memory In A Distributed System
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We study the power of shared-memory in models of parallel computation. We describe a novel distributed data structure that eliminates the need for shared mernory without significantly increasing the run time of the parallel computation. We also show how a complete network of processors can deterministicly simulate one PRAM step in O(log n(loglog n)2) time, when both models use n processors, and ttie size of the PRAM'S shared memory is polynomial in n. (The best previously known upper bound was the trivial O(n)). We also establish that this upper bound is nearly optimal. We prove that an online simulation of T PRAM steps by a complete network of processors requires &#x003A9;(Tlog n/loglog n) time.
[Concurrent computing, Upper bound, Computational modeling, Random access memory, Parallel machines, Phase change random access memory, Data structures, Time sharing computer systems, Polynomials, Distributed computing]
Three theorems on polynomial degrees of NP-sets
26th Annual Symposium on Foundations of Computer Science
None
1985
We show that recursive ascending sequences of polynomial time (p-) degrees do not possess minimal upper bounds; that, for every nonzero p-degree a, there is a lesser nonzero p-degree b which does not help a; and that every nonzero p-degree is half of a minimal pair.
[Upper bound, Lattices, Polynomials, NP-complete problem, Helium]
What search algorithm gives optimal average-case performance when search resources are highly limited?
27th Annual Symposium on Foundations of Computer Science
None
1986
This paper presents a probabilistic model for studying the question: given n search resources, where in the search tree should they be expended? Specifically, a least-cost root-to-leaf path is sought in a random tree. The tree is known to be binary and complete to depth N. Arc costs are independently set either to 1 (with probability p)or to 0 (with probability l-p). The cost of a leaf is the sum of the arc costs on the path from the root to that leaf. The searcher (scout) can learn n arc values. How should these scarce resources be dynamically allocated to minimize the average cost of the leaf selected? A natural decision rule for the scout is to allocate resources to arcs that lie above leaves whose current expected cost is minimal. The bad-news theorem says that situations exist for which this rule is nonoptimal, no matter what the value of n. The good-news theorem counters this: for a large class of situations, the aforementioned rule is an optimal decision rule if p &#x02264; 1/2 and within a constant of optimal if p &#x226B; 1/2.
[Costs, Laboratories, Binary trees, Random variables, Resource management, Counting circuits]
Diversity-based inference of finite automata
28th Annual Symposium on Foundations of Computer Science
None
1987
We present a new procedure for inferring the structure of a finitestate automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments. Our procedure uses a new representation for FSA's, based on the notion of equivalence between testa. We call the number of such equivalence classes the diversity of the automaton; the diversity may be as small as the logarithm of the number of states of the automaton. The size of our representation of the FSA, and the running time of our procedure (in some case provably, in others conjecturally) is polynomial in the diversity and ln(1/&#x03B5;), where &#x03B5; is a given upper bound on the probability that our procedure returns an incorrect result. (Since our procedure uses randomization to perform experiments, there is a certain controllable chance that it will return an erroneous result.) We also present some evidence for the practical efficiency of our approach. For example, our procedure is able to infer the structure of an automaton based on Rubik's Cube (which has approximately 1019 states) in about 2 minutes on a DEC Micro Vax. This automaton is many orders of magnitude larger than possible with previous techniques, which would require time proportional at least to the number of global states. (Note that in this example, only a small fraction (10-14) of the global states were even visited.)
[Computer science, Gold, Upper bound, Automatic testing, Laboratories, Automata, Automatic control, Inference algorithms, Polynomials, History]
A fast planar partition algorithm. I
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A fast randomized algorithm is given for finding a partition of the plane induced by a given set of linear segments. The algorithm is ideally suited for a practical use because it is extremely simple and robust, as well as optimal; its expected running time is O(m+n log n) where n is the number of input segments and m is the number of points of intersection. The storage requirement is O(m+n). Though the algorithm itself is simple, the global evolution of the partition is complex, which makes the analysis of the algorithm theoretically interesting in its own right.<<ETX>>
[Algorithm design and analysis, plane, global evolution, points of intersection, computational geometry, Data structures, fast planar partition algorithm, Partitioning algorithms, Application software, input segments, Clustering algorithms, Computer graphics, Ear, fast randomized algorithm, Robustness, computational complexity, linear segments]
The weighted majority algorithm
30th Annual Symposium on Foundations of Computer Science
None
1989
The construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes is studied. It is assumed that the learner has reason to believe that one of some pool of known algorithms will perform well but does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. It is called the weighted majority algorithm and is shown to be robust with respect to errors in the data. Various versions of the weighted majority algorithm are discussed, and error bounds for them that are closely related to the error bounds of the best algorithms of the pool are proved.<<ETX>>
[Algorithm design and analysis, weighted voting, Protocols, Voting, error bounds, Laboratories, Prediction algorithms, weighted majority algorithm, prediction algorithms, learning systems]
Competitive k-server algorithms
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Deterministic competitive k-server algorithms are given for all k and all metric spaces. This settles the k-server conjecture of M.S. Manasse et al. (1988) up to the competitive ratio. The best previous result for general metric spaces was a three-server randomized competitive algorithm and a nonconstructive proof that a deterministic three-server competitive algorithm exists. The competitive ratio the present authors can prove is exponential in the number of servers. Thus, the question of the minimal competitive ratio for arbitrary metric spaces is still open. The methods set forth here also give competitive algorithms for a natural generalization of the k-server problem, called the k-taxicab problem.<<ETX>>
[natural generalization, Costs, queueing theory, Extraterrestrial measurements, Mathematics, metric spaces, nonconstructive proof, Computer science, three-server randomized competitive algorithm, k-taxicab, Upper bound, Current measurement, file servers, Space exploration, competitive k-server algorithms]
Languages that are easier than their proofs
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Languages in NP are presented for which it is harder to prove membership interactively than it is to decide this membership. Similarly, languages where checking is harder than computing membership are presented. Under assumptions about triple-exponential time, incoherent sets in NP are constructed. Without any assumptions, incoherent sets are constructed in DSPACE (n to the log n), yielding the first uncheckable and non-random-self-reducible sets in that space.<<ETX>>
[Computer science, triple-exponential time, formal languages, NP, incoherent sets, DSPACE, nonrandom self-reducible sets, Search problems, Polynomials, membership, NP-complete problem, computational complexity]
Efficient inference of partial types
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Partial types for the lambda -calculus were introduced by Thatte (1988) as a means of typing objects that are not typable with simple types, such as heterogeneous lists and persistent data. He showed that type inference for partial types was semidecidable. Decidability remained open until O'Keefe and Wand gave an exponential time algorithm for type inference. The authors give an O(n/sup 3/) algorithm. The algorithm constructs a certain finite automaton that represents a canonical solution to a given set of type constraints. Moreover, the construction works equally well for recursive types.<<ETX>>
[lambda calculus, recursive types, partial types, type constraints, persistent data, type theory, finite automaton, Computer science, lambda -calculus, decidability, heterogeneous lists, exponential time algorithm, Automata, Inference algorithms, type inference]
Sensitive functions and approximate problems
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We investigate properties of functions that are good measures of the CRCW PRAM complexity of computing them. While the block sensitivity is known to be a good measure of the CREW PRAM complexity, no such measure is known for CRCW PRAMs. We show that the complexity of computing a function is related to its everywhere sensitivity, introduced by Vishkin and Wigderson (1985). Specifically we show that the time required to compute a function f:D/sup n//spl rarr/R of everywhere sensitivity es(f) with P/spl ges/n processors and unbounded memory is /spl Omega/(log[log es(f)/(log 4P|D|- log es(f))]). This improves previous results of Azar (1992), and Vishkin and Wigderson. We use this lower bound to derive new lower bounds for some approximate problems. These problems can often be solved faster than their exact counterparts and for many applications, it is sufficient to solve the approximate problem. We show that approximate selection requires time /spl Omega/(log[log n/log k]) with kn, processors and approximate counting with accuracy /spl lambda//spl ges/2 requires time /spl Omega/(log[log n/(log k+log /spl lambda/)]) with kn processors. In particular, for constant accuracy, no lower bounds were known for these problems.<<ETX>>
[everywhere sensitivity, Circuits, Phase change random access memory, CREW PRAM complexity, CRCW PRAM complexity, Postal services, sensitive functions, CRCW PRAMs, Boolean functions, Writing, block sensitivity, sensitivity, computational complexity]
A theory of competitive analysis for distributed algorithms
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We introduce a theory of competitive analysis for distributed algorithms. The first steps in this direction were made in the seminal papers of Y. Bartal et al. (1992), and of B. Awerbuch et al. (1992), in the context of data management and job scheduling. In these papers, as well as in other subsequent sequent work, the cost of a distributed algorithm is compared to the cost of an optimal global-control algorithm. In this paper we introduce a more refined notion of competitiveness for distributed algorithms, one that reflects the performance of distributed algorithms more accurately. In particular, our theory allows one to compare the cost of a distributed on-line algorithm to the cost of an optimal distributed algorithm. We demonstrate our method by studying the cooperative collect primitive, first abstracted by M. Saks, N. Shavit, and H. Woll (1991). We provide the first algorithms that allow processes to cooperate to finish their work in fewer steps. Specifically, we present two algorithms (with different strengths), and provide a competitive analysis for each one.<<ETX>>
[Algorithm design and analysis, Performance evaluation, Technological innovation, data management, competitive algorithms, competitiveness, Leg, Computer science, distributed algorithms, Optimal control, optimal global-control algorithm, scheduling, Cost function, Particle measurements, distributed on-line algorithm, Distributed algorithms, job scheduling, competitive analysis]
Markov chain algorithms for planar lattice structures
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Consider the following Markov chain, whose states are all domino tilings of a 2n/spl times/2n chessboard: starting from some arbitrary tiling, pick a 2/spl times/2 window uniformly at random. If the four squares appearing in this window are covered by two parallel dominoes, rotate the dominoes in place. Repeat many times. This process is used in practice to generate a random tiling and is a key tool in the study of the combinatorics of tilings and the behavior of dimer systems in statistical physics. Analogous Markov chains are used to randomly generate other structures on various two-dimensional lattices. The paper presents techniques which prove for the first time that, in many interesting cases, a small number of random moves suffice to obtain a uniform distribution.
[Solid modeling, Lattices, two-dimensional lattices, Ice, chessboard, Combinatorial mathematics, uniform distribution, Physics, Computer science, Geometry, statistical physics, planar lattice structures, domino tilings, algorithm theory, Markov processes, Markov chain algorithms, parallel dominoes, Arctic, arbitrary tiling, Random number generation, Testing, combinatorics]
Computationally hard algebraic problems
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
In this paper we present a simple geometric-like series of elements in a finite field F/sub q/, and show that computing its sum is NP-hard. This problem is then reduced to the problem of counting mod p the number of zeroes in a linear recurrence sequence with elements in a finite F/sub p/, where p is a small prime. Hence the latter problem is also NP-hard. In the lecture we shall also survey other computationally hard algebraic problems.
[Monte Carlo methods, NP-hard, zeroes, Algebra, geometric-like series, finite field, linear recurrence sequence, Polynomials, Galois fields, computational complexity, computationally hard algebraic problems]
A 7/8-approximation algorithm for MAX 3SAT?
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We describe a randomized approximation algorithm which takes an instance of MAX 3SAT as input. If the instance-a collection of clauses each of length at most three-is satisfiable, then the expected weight of the assignment found is at least 7/8 of optimal. We provide strong evidence (but not a proof) that the algorithm performs equally well on arbitrary MAX 3SAT instances. Our algorithm uses semidefinite programming and may be seen as a sequel to the MAX CUT algorithm of Goemans and Williamson (1995) and the MAX 2SAT algorithm of Feige and Goemans (1995). Though the algorithm itself is fairly simple, its analysis is quite complicated as it involves the computation of volumes of spherical tetrahedra. Hastad has recently shown that, assuming P/spl ne/NP, no polynomial-time algorithm for MAX 3SAT can achieve a performance ratio exceeding 7/8, even when restricted to satisfiable instances of the problem. Our algorithm is therefore optimal in this sense. We also describe a method of obtaining direct semidefinite relaxations of any constraint satisfaction problem of the form MAX CSP(F), where F is a finite family of Boolean functions. Our relaxations are the strongest possible within a natural class of semidefinite relaxations.
[randomized approximation algorithm, Linear programming, semidefinite relaxations, randomised algorithms, Computer science, semidefinite programming, Boolean functions, satisfiable, constraint satisfaction, algorithm theory, Approximation algorithms, Polynomials, MAX 3SAT, constraint handling, 7/8-approximation algorithm]
Protocols for asymmetric communication channels
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
In this paper we examine the problem of sending an n-bit data item from a client to a server across an asymmetric communication channel. We demonstrate that there are scenarios in which a high-speed link from the server to the client can be used to greatly reduce the number of bits sent from the client to the server across a slower link. In particular, we assume that the data item is drawn from a probability distribution D that is known to the server but not to the client. We present several protocols in which the expected number of bits transmitted by the server and client are O(n) and O(H(D)+1), respectively, where H(D) is the binary entropy of D (and can range from 0 to n). These protocols are within a small constant factor of optimal in terms of the number of bits sent by the client. The expected number of rounds of communication between the server and client in the simplest of our protocols is O(H(D)). We also give a protocol for which the expected number of rounds is only 0(1), but which requires more computational effort on the part of the server. A third technique provides a tradeoff between the computational effort and the number of rounds.
[Protocols, telecommunication channels, n-bit data item, binary entropy, Educational institutions, Entropy, Read only memory, Computer science, Network servers, asymmetric communication channels, Communication channels, Bandwidth, Cities and towns, Telephony, probability distribution, protocols]
Primal-dual approximation algorithms for metric facility location and k-median problems
40th Annual Symposium on Foundations of Computer Science
None
1999
We present approximation algorithms for the metric uncapacitated facility location problem and the metric k-median problem achieving guarantees of 3 and 6 respectively. The distinguishing feature of our algorithms is their low running time: O(m log m) and O(m log m(L+log(n))) respectively, where n and m are the total number of vertices and edges in the underlying graph. The main algorithmic idea is a new extension of the primal-dual schema.
[Costs, Operations research, k-median problems, Educational institutions, approximation algorithms, Postal services, facility location, Microwave integrated circuits, Upper bound, metric facility location, algorithm theory, primal-dual schema, Cities and towns, Approximation algorithms, Joining processes]
A combinatorial approach to planar non-colliding robot arm motion planning
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We propose a combinatorial approach to plan noncolliding motions for a polygonal bar-and-joint framework. Our approach yields very efficient deterministic algorithms for a category of robot arm motion planning problems with many degrees of freedom, where the known general roadmap techniques would give exponential complexity. It is based on a novel class of one-degree-of-freedom mechanisms induced by pseudo triangulations of planar point sets, for which we provide several equivalent characterization and exhibit rich combinatorial and rigidity theoretic properties. The main application is an efficient algorithm for the Carpenter's rule problem: convexify a simple bar-and-joint planar polygonal linkage using only non self-intersecting planar motions. A step in the convexification motion consists in moving a pseudo-triangulation-based mechanism along its unique trajectory in configuration space until two adjacent edges align. At that point, a local alteration restores the pseudo triangulation. The motion continues for O(n/sup 2/) steps until all the points are in convex position.
[planar point sets, graph theory, pseudo triangulations, Manipulators, Educational institutions, Mechanical factors, path planning, deterministic algorithms, polygonal bar-and-joint framework, exponential complexity, Motion planning, Couplings, Computer science, non-self-intersecting planar motions, rule problem, combinatorial approach, planar noncolliding robot arm motion planning, 1DOF mechanisms, one-degree-of-freedom mechanisms, manipulator kinematics, Robots, Bars, computational complexity]
Improved inapproximability results for MaxClique, chromatic number and approximate graph coloring
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The author presents improved inapproximability results for three problems: the problem of finding the maximum clique size in a graph, the problem of finding the chromatic number of a graph, and the problem of coloring a graph with a small chromatic number with a small number of colors. J. Hastad's (1996) result shows that the maximum clique size in a graph with n vertices is inapproximable in polynomial time within a factor n/sup 1-/spl epsi// or arbitrarily small constant /spl epsi/>0 unless NP=ZPP. We aim at getting the best subconstant value of /spl epsi/ in Hastad's result. We prove that clique size is inapproximable within a factor n/2((log n))/sup 1-y/ corresponding to /spl epsi/=1/(log n)/sup /spl gamma// for some constant /spl gamma/>0 unless NP/spl sube/ZPTIME(2((log n))/sup O(1)/). This improves the previous best inapproximability factor of n/2/sup O(log n//spl radic/log log n)/ (corresponding to /spl epsi/=O(1//spl radic/log log n)) due to L. Engebretsen and J. Holmerin (2000). A similar result is obtained for the problem of approximating chromatic number of a graph. We also present a new hardness result for approximate graph coloring. We show that for all sufficiently large constants k, it is NP-hard to color a k-colorable graph with k/sup 1/25 (log k)/ colors. This improves a result of M. Furer (1995) that for arbitrarily small constant /spl epsi/>0, for sufficiently large constants k, it is hard to color a k-colorable graph with k/sup 3/2-/spl epsi// colors.
[NP-hard, hardness factor, chromatic number, k-colorable graph, best inapproximability factor, graph colouring, MaxClique, Computer science, optimisation, approximate graph coloring, arbitrarily small constant, Approximation algorithms, hardness result, Polynomials, polynomial time, subconstant value, Error correction, Error correction codes, theorem proving, inapproximability results, proof, maximum clique size, computational complexity]
Zero-knowledge: abstract of a tutorial
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Zero-knowledge proofs are fascinating and extremely useful constructs. Their fascinating nature is due to their seemingly contradictory definition; zero-knowledge proofs are both convincing and yet yield nothing beyond the validity of the assertion being proven. Their applicability in the domain of cryptography is vast; they are typically used to force malicious parties to behave according to a predetermined protocol. In addition to their direct applicability in cryptography, zero-knowledge proofs serve as a good benchmark for the study of various problems regarding cryptographic protocols (e.g., "secure composition of protocols" and the "use of of the adversary's program within the proof of security"). We present the basic definitions and results regarding zero-knowledge as well as some recent developments regarding this notion.
[Tutorial, secure protocol composition, cryptographic protocols, Computational modeling, proof of security, cryptography, Security, contradictory definition, Cryptographic protocols, Computer science, Uniform resource locators, predetermined protocol, malicious parties, assertion, theorem proving, Cryptography, protocols, zero-knowledge proofs]
Algorithms and complexity results for #SAT and Bayesian inference
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Bayesian inference is an important problem with numerous applications in probabilistic reasoning. Counting satisfying assignments is a closely related problem of fundamental theoretical importance. In this paper, we show that plain old DPLL equipped with memorization (an algorithm we call #DPLLCache) can solve both of these problems with time complexity that is at least as good as state-of-the-art exact algorithms, and that it can also achieve the best known time-space tradeoff. We then proceed to show that there are instances where #DPLLCache can achieve an exponential speedup over existing algorithms.
[computability, uncertainty handling, probabilistic reasoning, #SAT, memorization, Polynomials, time-space tradeoff, Bayesian inference, Computational modeling, Computer simulation, Government, complexity results, time complexity, exponential speedup, Application software, inference mechanisms, DPLL, Computer science, exact algorithms, Bayesian methods, #DPLLCache algorithm, Inference algorithms, Bayes methods, satisfying assignment counting, computational complexity]
Quantum and classical strong direct product theorems and optimal time-space tradeoffs
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
A strong direct product theorem says that if we want to compute k independent instances of a function, using less than k times the resources needed for one instance, then our overall success probability is exponentially small in k. We establish such theorems for the classical as well as quantum query complexity of the OR function. This implies slightly weaker direct product results for all total functions. We prove a similar result for quantum communication protocols computing k instances of the disjointness function. These results imply a time-space tradeoff T/sup 2/S = /spl Omega/(N/sup 3/) for sorting N items on a quantum computer, which is optimal up to polylog factors. They also give several tight time-space and communication-space tradeoffs for the problems of Boolean matrix-vector multiplication and matrix multiplication.
[Error probability, OR function, quantum communication protocols, Ink, Probability distribution, disjointness function, Complexity theory, optimal time-space tradeoff, Distributed computing, quantum theorem, polylog factors, slightly weaker direct product, Quantum computing, quantum strong direct product theorems, Boolean matrix-vector multiplication, protocols, communication-space tradeoff, quantum communication, quantum computer, Computational modeling, quantum query complexity, probability, Boolean algebra, classical strong direct product theorems, Sorting, Computer science, matrix multiplication, Quantum mechanics, quantum computing, total function, computational complexity]
On the unique games conjecture
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Summary form only given. The discovery of the PCP theorem in 1992 led to an avalanche of hardness of approximation results, i.e. results showing that for certain NP hard optimization problems, computing even approximate solutions is hard. However, for many fundamental problems, obtaining satisfactory hardness results seems out of reach of current techniques. The unique games conjecture (UGC) was proposed in 2002 as an approach towards settling some of these open problems. A 2-Prover-1-Round game is called unique if for every answer of either prover, there is exactly one answer of the other prover if the verifier is to accept. The UGC states that for every constant /spl epsiv/ > 0, it is NP hard to distinguish whether the optimal strategy of provers in a unique 2P1R game has acceptance probability at least 1 - /spl epsiv/ or at most /spl epsiv/. The answer size k = k(/spl epsiv/) could be an arbitrary function of /spl epsiv/. The UGC has been shown to imply optimal hardness results for vertex cover and MAX-CUT problems, and superconstant hardness results for sparsest cut and Min-2SAT-Deletion problems. A variation of the conjecture has been shown to imply hardness of coloring 3-colorable graphs with constantly many colors. Apart from these applications to hardness results, the UGC has led to important (unconditional) results in Fourier analysis, the theory of metric embeddings, and integrality gap results for semidefinite programming relaxations. The tutorial aims to give an overview of the UGC, its applications, and attempts to prove or disprove it.
[2-Prover-1-Round game, User-generated content, programming relaxation, game theory, integrality gap, Min-2SAT-Deletion problem, 3-colorable graph, Application software, sparsest cut problem, MAX-CUT problem, superconstant hardness result, Computer science, optimal hardness result, unique games conjecture, vertex cover problem, computational complexity]
Heat Flow and a Faster Algorithm to Compute the Surface Area of a Convex Body
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We draw on the observation that the amount of heat diffusing outside of a heated body in a short period of time is proportional to its surface area, to design a simple algorithm for approximating the surface area of a convex body given by a membership oracle. Our method has a complexity of O*(n4), where n is the dimension, compared to O*( n8.5) for the previous best algorithm. We show that our complexity cannot be improved given the current state-of-the-art in volume estimation
[Algorithm design and analysis, heat diffusion, computational geometry, Quadratic programming, Computer science, surface area, heated body, Design engineering, Interpolation, Heat engines, Polynomials, heat flow, convex body, State estimation, heat transfer, computational complexity, algorithm]
A Brief Look at Pairings Based Cryptography
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
This note provides a brief summary of how a new algebraic tool, bilinear groups, is transforming public-key cryptography. For the examples mentioned, the best solutions without bilinear groups either do not exist or are far less efficient. Many of the systems discussed in this note were implemented by Lynn [45] in a software library freely available under the GPL.
[public-key cryptography, algebra, Galois fields, bilinear group, Computer science, Computational geometry, algebraic tool, Elliptic curves, Bibliographies, public key cryptography, Elliptic curve cryptography, Public key cryptography, Polynomials, pairing based cryptography]
The Sign-Rank of AC^O
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
The sign-rank of a matrix A = [A<sub>ij</sub>] with plusmn1 entries is the least rank of a real matrix B = [B<sub>ij</sub>] with A<sub>ij</sub>B<sub>ij</sub> &gt; 0 for all i, j. We obtain the first exponential lower bound on the sign-rank of a function in AC0. Namely, let f(x, y) = Lambda<sub>i=1</sub> m Lambda<sub>j=1</sub> m 2(x<sub>ij</sub> Lambda y<sub>ij</sub>). We show that the matrix [f(x, y)]<sub>x,</sub> <sub>y</sub> has sign-rank 2Omega(m). This in particular implies that Sigma<sub>2</sub> ccnsubeUPPcc, which solves a long-standing open problem posed by Babai, Frankl, and Simon (1986). Our result additionally implies a lower bound in learning theory. Specifically, let Phi<sub>1</sub>,..., Phi<sub>r</sub> : {0, 1}n rarrRopf be functions such that every DNF formula f : {0, 1}n rarr {-1, +1} of polynomial size has the representation f equiv sign(a<sub>1</sub>Phi<sub>1</sub> + hellip + a<sub>r</sub>Phi<sub>r</sub>) for some reals a<sub>1</sub>,..., a<sub>r</sub>. We prove that then r ges 2Omega(n 1/3 ), which essentially matches an upper bound of 2Otilde(n 1/3 ) due to Klivans and Servedio (2001). Finally, our work yields the first exponential lower bound on the size of threshold-of-majority circuits computing a function in AC0. This substantially generalizes and strengthens the results of Krause and Pudlak (1997).
[Complexity classes Sigma_2^cc and UPP^cc, sign-rank, Circuits, Constant-depth AND/OR/NOT circuits, Complexity theory, Sign-rank, matrix algebra, polynomial size, Computer science, real matrix, Upper bound, Boolean functions, USA Councils, Bismuth, Polynomials, Communication complexity, Error correction, Error correction codes, threshold-of-majority circuits, first exponential lower bound, Multivariate polynomials, computational complexity]
Local Graph Partitions for Approximation and Testing
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We introduce a new tool for approximation and testing algorithms called partitioning oracles. We develop methods for constructing them for any class of bounded-degree graphs with an excluded minor, and in general, for any hyperfinite class of bounded-degree graphs. These oracles utilize only local computation to consistently answer queries about a global partition that breaks the graph into small connected components by removing only a small fraction of the edges. We illustrate the power of this technique by using it to extend and simplify a number of previous approximation and testing results for sparse graphs, as well as to provide new results that were unachievable with existing techniques. For instance:1. We give constant-time approximation algorithms for the size of the minimum vertex cover, the minimum dominating set, and the maximum independent set for any class of graphs with an excluded minor.2. We show a simple proof that any minor-closed graph property is testable in constant time in the bounded degree model.3. We prove that it is possible to approximate the distance to almost any hereditary property in any bounded degree hereditary families of graphs. Hereditary properties of interest include bipartiteness, k-colorability, and perfectness.
[approximation algorithms, graph colouring, hereditary property, constant-time approximation algorithm, Tree graphs, constant time algorithms, testing algorithm, minor-closed graph property, partitioning oracles, Polynomials, perfectness property, Testing, approximation theory, Particle separators, local graph partitions, k-colorability property, testing, Partitioning algorithms, bounded-degree graph, maximum independent set, bipartiteness property, Computer science, separator theorem, minimum dominating set, Binary trees, Approximation algorithms, minimum vertex cover, computational complexity]
The Limits of Two-Party Differential Privacy
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We study differential privacy in a distributed setting where two parties would like to perform analysis of their joint data while preserving privacy for both datasets. Our results imply almost tight lower bounds on the accuracy of such data analyses, both for specific natural functions (such as Hamming distance) and in general. Our bounds expose a sharp contrast between the two-party setting and the simpler client-server setting (where privacy guarantees are one-sided). In addition, those bounds demonstrate a dramatic gap between the accuracy that can be obtained by differentially private data analysis versus the accuracy obtainable when privacy is relaxed to a computational variant of differential privacy. The first proof technique we develop demonstrates a connection between differential privacy and deterministic extraction from Santha-Vazirani sources. A second connection we expose indicates that the ability to approximate a function by a low-error differentially private protocol is strongly related to the ability to approximate it by a low communication protocol. (The connection goes in both directions).
[Data privacy, client-server systems, Protocols, Additives, Hamming distance, data analysis, Complexity theory, communication complexity, randomness extractors, Privacy, Sensitivity, two party differential privacy, distributed setting, client server setting, data privacy, computational variant, private data analysis, Santha-Vazirani sources, differential privacy]
The Graph Minor Algorithm with Parity Conditions
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We generalize the seminal Graph Minor algorithm of Robertson and Seymour to the parity version. We give polynomial time algorithms for the following problems: 1) the parity H-minor (Odd K<sub>k</sub> -minor) containment problem, and 2) the disjoint paths problem with k terminals and the parity condition for each path, as well as several other related problems. We present an O(ma(m, n)n) time algorithm for these problems for any fixed k, where n, m are the number of vertices and the number of edges, respectively, and the function a(m,n) is the inverse of the Ackermann function (see Tarjan [69]). Note that the first problem includes the problem of testing whether or not a given graph contains k disjoint odd cycles (which was recently solved in [24], [34]), if we fix H to be equal to the graph of k disjoint triangles. The algorithm for the second problem generalizes the Robertson Seymour algorithm for the k-disjoint paths problem. As with the Robertson-Seymour algorithm for the k-disjoint paths problem for any fixed k, in each iteration, we would like to either use the presence of a huge clique minor, or alternatively exploit the structure of graphs in which we cannot find such a minor. Here, however, we must maintain the parity of the paths and can only use an "odd clique minor". This requires new techniques to describe the structure of the graph when we cannot find such a minor. We emphasize that our proof for the correctness of the above algorithms does not depend on the full power of the Graph Minor structure theorem [56]. Although the original Graph Minor algorithm of Robertson and Seymour does depend on it and our proof does have similarities to their arguments, we can avoid the structure theorem by building on the shorter proof for the correctness of the graph minor algorithm in [35]. This work was done as a part of an INRIA-NII collaboration under MOU grant, and partially supported by MEXT Grant-in-Aid for Scientific Research on Priority Areas "New Horizons in Computing" Research partly supported by Japan Society for the Promotion of Science, Grant-in-Aid for Scientific Research, by C &amp; C Foundation, by Kayamori Foundation and by Inoue Research Award for Young Scientists. Consequently, we are able to avoid the much of the heavy machinery of the Graph Minor structure theory. Utilizing some results of [35] and [62], [63], our proof is less than 50 pages.
[Algorithm design and analysis, TV, Image edge detection, Buildings, graph theory, odd cycles and the parity disjoint paths problem, parity, Odd minor, parity minor, Robertson Seymour algorithm, Computer science, graph minor structure theorem, Runtime, polynomial time algorithms, graph minor algorithm, parity conditions, Polynomials, Ackermann function]
Split and Join: Strong Partitions and Universal Steiner Trees for Graphs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We study the problem of constructing universal Steiner trees for undirected graphs. Given a graph G and a root node r, we seek a single spanning tree T of minimum stretch, where the stretch of T is defined to be the maximum ratio, over all terminal sets X, of the cost of the minimal sub-tree T<sub>X</sub> of T that connects X to r to the cost of an optimal Steiner tree connecting X to r in G. Universal Steiner trees (USTs) are important for data aggregation problems where computing the Steiner tree from scratch for every input instance of terminals is costly, as for example in low energy sensor network applications. graphs with 2O(&#x221A;log n)-stretch. We also give a polynomial time We provide a polynomial time UST construction for general polylog(n)-stretch construction for minor-free graphs. One basic building block of our algorithms is a hierarchy of graph partitions, each of which guarantees small strong diameter for each cluster and bounded neighbourhood intersections for each node. We show close connections between the problems of constructing USTs and building such graph partitions. Our construction of partition hierarchies for general graphs is based on an iterative cluster merging procedure, while the one for minor-free graphs is based on a separator theorem for such graphs and the solution to a cluster aggregation problem that may be of independent interest even for general graphs. To our knowledge, this is the first subpolynomial-stretch (o(n&#x03B5;) for any &#x03B5; &gt;; 0) UST construction for general graphs, and the first polylogarithmic-stretch UST construction for minor-free graphs.
[Steiner trees, Measurement, Approximation methods, iterative cluster merging procedure, graph partition hierarchy, minor-free graphs, polynomial time UST construction, Clustering algorithms, cluster aggregation problem, bounded neighbourhood intersections, undirected graphs, hierarchical graph partition, general polylog(n)-stretch construction, merging, trees (mathematics), Educational institutions, minimal subtree, Partitioning algorithms, graph clustering, universal Steiner tree, data aggregation problems, separator theorem, pattern clustering, spanning tree, UST, universal Steiner trees, Joining processes, strong partitions, computational complexity]
Constant-Round Concurrent Zero Knowledge from P-Certificates
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We present a constant-round concurrent zero-knowledge protocol for NP. Our protocol relies on the existence of families of collision-resistant hash functions, and a new, but in our eyes, natural complexity-theoretic assumption: the existence of P-certificates-that is, "succinct" non-interactive proofs/arguments for P. As far as we know, our results yield the first constant-round concurrent zero-knowledge protocol for NP with an explicit zero-knowledge simulator based on any assumption.
[Protocols, Computational modeling, zero-knowledge interactive proofs, explicit zero-knowledge simulator, collision-resistant hash functions, concurrency theory, Complexity theory, Security, Concurrent Zero-Knowledge, natural complexity-theoretic assumption, Concurrent computing, Awards activities, constant-round concurrent zero-knowledge protocol, P-certificates, Polynomials, theorem proving, Cryptography, Non-Black-Box Technique, computational complexity]
Circuit Complexity, Proof Complexity, and Polynomial Identity Testing
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We introduce a new and natural algebraic proof system, which has tight connections to (algebraic) circuit complexity. In particular, we show that any super-polynomial lower bound on any Boolean tautology in our proof system implies that the permanent does not have polynomial-size algebraic circuits (VNP&#x2260;VP). As a corollary, super-polynomial lower bounds on the number of lines in Polynomial Calculus proofs (as opposed to the usual measure of number of monomials) imply the Permanent versus Determinant Conjecture. Note that, prior to our work, there was no proof system for which lower bounds on an arbitrary tautology implied any computational lower bound. Our proof system helps clarify the relationships between previous algebraic proof systems, and begins to shed light on why proof complexity lower bounds for various proof systems have been so much harder than lower bounds on the corresponding circuit classes. In doing so, we highlight the importance of polynomial identity testing (PIT) for understanding proof complexity.
[circuit complexity, Boolean tautology, Frequency modulation, proof complexity, determinant conjecture, permanent conjecture, polynomial identity testing, Calculus, natural algebraic proof system, Complexity theory, Boolean algebra, Standards, super-polynomial lower bound, AC0[p, p, polynomial calculus proofs, Polynomials, PIT, theorem proving, Testing]
Quadratic Conditional Lower Bounds for String Problems and Dynamic Time Warping
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Classic similarity measures of strings are longest common subsequence and Levenshtein distance (i.e., the classic edit distance). A classic similarity measure of curves is dynamic time warping. These measures can be computed by simple O(n2) dynamic programming algorithms, and despite much effort no algorithms with significantly better running time are known. We prove that, even restricted to binary strings or one-dimensional curves, respectively, these measures do not have strongly subquadratic time algorithms, i.e., no algorithms with running time O(n2-&#x03B5;) for any &#x03B5; &gt; 0, unless the Strong Exponential Time Hypothesis fails. We generalize the result to edit distance for arbitrary fixed costs of the four operations (deletion in one of the two strings, matching, substitution), by identifying trivial cases that can be solved in constant time, and proving quadratic-time hardness on binary strings for all other cost choices. This improves and generalizes the known hardness result for Levenshtein distance [Backurs, Indyk STOC'15] by the restriction to binary strings and the generalization to arbitrary costs, and adds important problems to a recent line of research showing conditional lower bounds for a growing number of quadratic time problems. As our main technical contribution, we introduce a framework for proving quadratic-time hardness of similarity measures. To apply the framework it suffices to construct a single gadget, which encapsulates all the expressive power necessary to emulate a reduction from satisfiability. Finally, we prove quadratic-time hardness for longest palindromic subsequence and longest tandem subsequence via reductions from longest common subsequence, showing that conditional lower bounds based on the Strong Exponential Time Hypothesis also apply to string problems that are not necessarily similarity measures.
[dynamic time warping, Heuristic algorithms, computability, dynamic programming, binary strings, Time measurement, classic similarity measures, Levenshtein distance, Computer science, quadratic-time hardness, string problems, Current measurement, subquadratic time algorithms, satisfiability, palindromic subsequence, 1D curves, strong exponential time hypothesis, Approximation algorithms, Dynamic programming, string matching, quadratic time problems, quadratic conditional lower bounds, dynamic programming algorithms]
Heavy Hitters via Cluster-Preserving Clustering
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
In the turnstile &#x2113;<sub>p</sub> heavy hitters problem with parameter &#x03B5;, one must maintain a high-dimensional vector x &#x2208; &#x211D;n subject to updates of the form update (i,&#x0394;) causing the change x<sub>i</sub> &#x2190; x<sub>i</sub> + &#x0394;, where i &#x03B5;[n], &#x0394; &#x2208; &#x211D;. Upon receiving a query, the goal is to report every "heavy hitter" i &#x2208; [n] with |x<sub>i</sub>| &#x2265; &#x03B5; &#x2225;x&#x2225;<sub>p</sub> as part of a list L &#x2286; [n] of size O(1/&#x03B5;p), i.e. proportional to the maximum possible number of heavy hitters. For any p&#x03B5;(0,2] the COUNTSKETCH of [CCFC04] solves &#x2113;<sub>p</sub> heavy hitters using O(&#x03B5;-p lg n) words of space with O(lg n) update time, O(n lg n) query time to output L, and whose output after any query is correct with high probability (whp) 1 - 1/poly(n) [JST11, Section 4.4]. This space bound is optimal even in the strict turnstile model [JST11] in which it is promised that x<sub>i</sub> &#x2265; 0 for all i &#x2208; [n] at all points in the stream, but unfortunately the query time is very slow. To remedy this, the work [CM05] proposed the "dyadic trick" for the COUNTMIN sketch for p = 1 in the strict turnstile model, which to maintain whp correctness achieves suboptimal space O(&#x03B5;-1lg2 n), worse update time O(lg2 n), but much better query time O(&#x03B5;-1poly(lg n)). An extension to all p &#x2208; (0,2] appears in [KNPW11, Theorem 1], and can be obtained from [Pag13]. We show that this tradeoff between space and update time versus query time is unnecessary. We provide a new algorithm, EXPANDERSKETCH, which in the most general turnstile model achieves optimal O(&#x03B5;-plog n) space, O(log n) update time, and fast O(&#x03B5;-ppoly(log n)) query time, providing correctness whp. In fact, a simpler version of our algorithm for p = 1 in the strict turnstile model answers queries even faster than the "dyadic trick" by roughly a log n factor, dominating it in all regards. Our main innovation is an efficient reduction from the heavy hitters to a clustering problem in which each heavy hitter is encoded as some form of noisy spectral cluster in a much bigger graph, and the goal is to identify every cluster. Since every heavy hitter must be found, correctness requires that every cluster be found. We thus need a "cluster-preserving clustering" algorithm, that partitions the graph into clusters with the promise of not destroying any original cluster. To do this we first apply standard spectral graph partitioning, and then we use some novel combinatorial techniques to modify the cuts obtained so as to make sure that the original clusters are sufficiently preserved. Our cluster-preserving clustering may be of broader interest much beyond heavy hitters.
[Algorithm design and analysis, Complexity theory, optimal O(&#x03B5;-p lg n) space, query processing, COUNTMIN sketch, strict turnstile model, streaming, Clustering algorithms, cluster-preserving clustering, O(lg2 n), cluster-preserving clustering algorithm, O(&#x03B5;-p lg n), Computational modeling, Estimation, probability, turnstile l<sub>p</sub> heavy hitters, dyadic trick, query time, O(&#x03B5;-1 poly(lg n)), Partitioning algorithms, heavy hitters, pattern clustering, combinatorial techniques, O(lg n) update time, O(&#x03B5;-p poly(lgn)) query time, O(nlgn) query time, standard spectral graph, clustering, O(1/&#x03B5;p), computational complexity, suboptimal space O(&#x03B5;-1 lg2 n)]
On Learning Mixtures of Well-Separated Gaussians
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We consider the problem of efficiently learning mixtures of a large number of spherical Gaussians, when the components of the mixture are well separated. In the most basic form of this problem, we are given samples from a uniform mixture of k standard spherical Gaussians with means μ<sub>1</sub>, . . . , μ<sub>k</sub> &#x2208; &#x211D;d, and the goal is to estimate the means up to accuracy &#x03B4; using poly(k, d, 1/&#x03B4;) samples. In this work, we study the following question: what is the minimum separation needed between the means for solving this task? The best known algorithm due to Vempala and Wang [JCSS 2004] requires a separation of roughly min{k, d}1/4. On the other hand, Moitra and Valiant [FOCS 2010] showed that with separation o(1), exponentially many samples are required. We address the significant gap between these two bounds, by showing the following results.; We show that with separation o(&#x221A;(log k)), superpolynomially many samples are required. In fact, this holds even when the k means of the Gaussians are picked at random in d = O(log k) dimensions.; We show that with separation &#x03A9;(&#x221A;(log k)), picked at random in d = O(log k) dimensions. poly(k, d, 1/&#x03B4;) samples suffice. Notice that the bound on the separation is independent of &#x03B4;. This result is based on a new and efficient &#x201C;accuracy boosting&#x201D; algorithm that takes as input coarse estimates of the true means and in time (and samples) poly(k, d, 1/&#x03B4;) outputs estimates of the means up to arbitrarily good accuracy &#x03B4; assuming the separation between the means is &#x03A9;(min{&#x221A;(log k), &#x221A;d}) (independently of &#x03B4;). The idea of the algorithm is to iteratively solve a &#x201C;diagonally dominant&#x201D; system of non-linear equations. We also (1) present a computationally efficient algorithm in d = O(1) dimensions with only &#x03A9;(&#x221A;d) separation, and (2) extend our results to the case that components might have different weights and variances. These results together essentially characterize the optimal order of separation between components that is needed to learn a mixture of k spherical Gaussians with polynomial samples.
[Parameter estimation, graph theory, learning, mixtures of Gaussians, Gaussian mixture model, separation &#x03A9;(&#x221A;(log k)), sample complexity, O(log k) dimensions, Clustering algorithms, parameter estimation, Iterative methods, Mathematical model, learning (artificial intelligence), computationally efficient algorithm, iterative algorithms, approximation theory, uniform mixture, standard spherical Gaussians, polynomials, accuracy boosting algorithm, Standards, polynomial samples, clustering, diagonally dominant system, input coarse estimates, computational complexity]
Foreword
16th Annual Symposium on Foundations of Computer Science
None
1975
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
The effect of the field of constants on the number of multiplications
16th Annual Symposium on Foundations of Computer Science
None
1975
false
[Image recognition, Algebra]
The exact time required to perform generalized addition
16th Annual Symposium on Foundations of Computer Science
None
1975
false
[Computer science, Encoding, Added delay, Digital circuits, Adders]
Fast parallel matrix inversion algorithms
16th Annual Symposium on Foundations of Computer Science
None
1975
In this paper, an investigation of the parallel arithmetic complexity of matrix inversion, solving systems of linear equations, computing determinants and computing the characteristic polynomial of a matrix is reported. The parallel arithmetic complexity of solving equations has been an open question for several years. The gap between the complexity of the best algorithms (2n + 0(1), where n is the number of unknowns/ equations) and the only proved lower bound (2 log n (All logarithms in this paper are of base two.)) was huge. The first breakthrough came when Csanky reported that the parallel arithmetic complexity of all these four problems has the same growth rate and exhibited an algorithm that computes these problems in 2n - O(log2n) steps. It will be shown in the sequel that the parallel arithmetic complexity of all these four problems is upper bounded by O(log2n) and the algorithms that establish this bound use a number of processors polynomial in n. This disproves I. Munro's conjecture.
[Concurrent computing, Computer science, Computational modeling, Laboratories, Digital arithmetic, Polynomials, Computational complexity, Equations]
Parallel computations in graph theory
16th Annual Symposium on Foundations of Computer Science
None
1975
In parallel computation two approaches are common; namely unbounded parallelism and bounded parallelism. In this paper both approaches will be considered. The problem of unbounded parallelism is studied in section II and some lower and upper bounds on different connectivity problems for directed and undirected graphs are presented. In section III we mention bounded parallelism and three different k-parallel graph search techniques, namely k-depth search, breadth depth search, and breadth-first search. Each algorithm is analyzed with respect to the optimal serial algorithm. It is shown that for sufficiently dense graphs the parallel breadth first search technique is very close to the optimal bound. Techniques for searching sparse graphs are also discussed.
[Concurrent computing, Algorithm design and analysis, Computer science, Performance evaluation, Upper bound, Tree graphs, Parallel processing, Graph theory, Arithmetic, Testing]
Synchronization and computing capabilities of linear asynchronous structures
16th Annual Symposium on Foundations of Computer Science
None
1975
A model is defined in which questions concerning delay bounded asynchronous parallel systems may be investigated. Persistence and determinacy are introduced for this model. These two conditions are shown to be sufficient to guarantee that a synchronous execution policy can be relaxed to an asynchronous execution policy with no change to the result of the computation. In addition, the asynchronous execution time is only (D+1) times the synchronous execution time, where D is the delay bound. A wide class of recognition problems is identified which can be solved by linear asynchronous structures. Also, it is shown that synchronization problems, similar to the "firing squad synchronization problem," cannot be solved by delay bounded asynchronous systems.
[Concurrent computing, Context, Physics computing, Delay effects, Delay systems, Computational modeling, Integrated circuit interconnections, Production, Synchronization, Clocks]
Flow of control in the proof theory of structured programming
16th Annual Symposium on Foundations of Computer Science
None
1975
The proof theory of structured programming insofar as concerned with flow of control is investigated. Various proof rules for the while, repeat-until and simple iteration statements - all essentially variants of Hoare's original while rule - are analyzed with respect to their soundness and adequacy. Next, a recently proposed proof rule for recursive procedures due to Dijkstra is - after correction - shown to be a simple instance of Scott's induction rule. Finally, Manna &#x00026; Pnueli's rule for total correctness of the while statement is formally justified using the Hitchcock &#x00026; Park theory of program termination based on well-founded relations.
[Diversity reception, Termination of employment, Mathematical programming]
Bases for chain-complete posets
16th Annual Symposium on Foundations of Computer Science
None
1975
Given partially ordered sets (posets) P and Q, it is often useful to construct maps g:P&#x02192;Q which are chain-continuous: least upper bounds (supremums) of nonempty linearly ordered subsets are preserved. Chaincontinuity is analogous to topological continuity and is generally much more difficult to verify than isotonicity: the preservation of the order relation. This paper introduces the concept of an extension basis: a subset B of P such that any isotone f:B&#x02192;Q has a unique chain-continuous extension g:P&#x02192;Q. Two characterizations of the chain-complete posets which have extension bases are obtained. These results are then applied to the problem of constructing an extension basis for the poset [P&#x02192;Q] of chain-continuous maps from P to Q, given extension bases for P and Q. This is not always possible, but it becomes possible when a mild (and independently motivated) restriction is imposed on either P or Q. A lattice structure is not needed. Finally, we consider extension bases which can be recursively listed and derive a recently established theorem as a corollary.
[Computer science, Upper bound, Lattices, Computer applications, Application software]
Correct computation rules for recursive languages
16th Annual Symposium on Foundations of Computer Science
None
1975
This paper considers simple LISP-like languages for the recursive definition of functions, focusing upon the connections between formal computation rules for calculation and the mathematical semantics of recursive definitions. A computation rule is correct when it is capable of computing the minimal fixpoint of a recursive definition. We give necessary and sufficient conditions for the correctness of rules under (a) all possible interpretations and (b) particular interpretations.
[Computer science, Concurrent computing, Computer languages, Sufficient conditions, Formal languages, Lead, Equations, Testing]
On time versus space and related problems
16th Annual Symposium on Foundations of Computer Science
None
1975
false
[Turing machines, Computational modeling, Magnetic heads, Partitioning algorithms, Clocks, Context modeling]
A note on tape bounds for sla language processing
16th Annual Symposium on Foundations of Computer Science
None
1975
In this note we show that the tape bounded complexity classes of languages over single letter alphabets are closed under complementation. We then use this result to show that there exists an infinite hierarchy of tape bounded complexity classes of sla languages between log n and log log n tape bounds. We also show that every infinite sla language recognizable on less than log n tape has infinitely many different regular subsets, and, therefore, the set of primes in unary notation, P, requires exactly log n tape for its recognition and every infinite subset of P requires at least log n tape.
[Computer science, Turing machines, Magnetic heads, Computational complexity]
Minimean optimality in sorting algorithms
16th Annual Symposium on Foundations of Computer Science
None
1975
We prove that an algorithm for selecting both the minimum and maximum element from an unordered set is minimean optimal. This algorithm had already been shown to be minimax optimal. The circumstances in which the same algorithm is optimum for both norms leads to a conjecture relating both norms. The method of proof utilizes a new idea in the theory of sorting optimality, namely degree information of the Hasse digraphs.
[Algorithm design and analysis, Optimized production technology, Minimax techniques, Chromium, Sorting]
Preserving order in a forest in less than logarithmic time
16th Annual Symposium on Foundations of Computer Science
None
1975
We present a data structure, based upon a stratified binary tree, which enables us to manipulate on-line a priority queue whose priorities are selected from the interval 1...n, with an average and worst case processing time of O(log log n) per instruction. The structure is used to obtain a mergeable heap whose time requirements are about as good.
[Tree data structures, Algorithm design and analysis, Computer aided instruction, Runtime, Binary trees, Data structures, Testing, Sorting]
On the complexity of comparison problems using linear functions
16th Annual Symposium on Foundations of Computer Science
None
1975
false
[Veins, Binary trees, Minimax techniques, Mathematics, Decision trees, Testing]
Evaluating relational expressions with dense and sparse arguments
16th Annual Symposium on Foundations of Computer Science
None
1975
We consider expressions whose arguments are relations and whose operators are chosen from among &#x0222A;, &#x03BF;, *, and -1. We further assume that operands may be designated "sparse" or "dense\
[Algorithm design and analysis, Tree graphs]
On the decision tree complexity of the shortest path problems
16th Annual Symposium on Foundations of Computer Science
None
1975
false
[Shortest path problem, Computer aided instruction, Costs, Mathematics, Decision trees, Contracts]
An O (N2.5) algorithm for maximum matching in general graphs
16th Annual Symposium on Foundations of Computer Science
None
1975
This work presents a new efficient algorithm for finding a maximum matching in an arbitrary graph. Two implementations are suggested, the complexity of the first is O(n2.5) and the complexity of the second is O(m&#x0221A;n&#x000B7;log n) where n, m are the numbers of the vertices and the edges in the graph.
[Terminology, Law, Circuits, Lead, Data structures, Bipartite graph, Legal factors]
Information theory and the complexity of switching networks
16th Annual Symposium on Foundations of Computer Science
None
1975
false
[Boolean functions, Upper bound, Cost function, Inverters, Appropriate technology, Relays, Information theory]
The effect of basis on size of Boolean expressions
16th Annual Symposium on Foundations of Computer Science
None
1975
To within a constant factor, only two complexity classes of complete binary bases exist. We show that they are separated by at most O(nlog310), or about O(n2.095), complementing a result of Khrapchenko that establishes an order n2 lower bound.
[Boolean functions, Upper bound]
Economy of description by parsers, DPDA's, and PDA's
16th Annual Symposium on Foundations of Computer Science
None
1975
It is shown that there is a sequence of languages E1, E2,... such that every correct prefix parser (one which detects errors at the earliest possible moment, e.g., LR or LL parsers) for En has size 2cn, yet a deterministic PDA recognizing En exists and has size O(n2). There is another easily described sequence of languages N1,N2,... for which Nn has a nondeterministic PDA of size O(n2) but no deterministic PDA of size less than 2cn. It is shown moreover, that this latter exponential gap can be made arbitrarily large for different sequences of languages.
[Economics, Automata, Production, Polynomials, Error correction, Contracts]
An improvement on valiant's decision procedure for equivalence of deterministic finite-turn pushdown automata
16th Annual Symposium on Foundations of Computer Science
None
1975
In this paper Valiant's decision procedure for equivalence of deterministic finite-turn pushdown machines is improved upon. The improved equivalence test is: Given two mahcines, one constructs a pushdown machine that simulates them simultaneously and accepts a string iff it is accepted by exactly one of them. The given machines are equivalent iff the simulating pda accepts the empty language. The simulating machine uses its pushdown store to hold the contents of the stores of the two simulated machines. In order ot prevent the tops of the two stores to get too far apart the simulating machine sometimes replaces the contents of one of the stores. Valiant 1,2 has proved the existence of a suitable function which determines the replacements. The crux of this paper is an algorithm for constructing such a function. The existence of such an algorithm enables us to calculate an upper bound on the time complexity of the improved algorithm. We obtain the results that equivalence of deterministic finite-turn pushdown automata can be tested in super-exponential time and equivalence of deterministic two-tape automata can be tested in exponential time.
[Upper bound, Runtime, Automatic testing, Automata]
A grammatical characterization of exponential-time languages
16th Annual Symposium on Foundations of Computer Science
None
1975
We show that the languages generated by a constrained form of Chomsky's transformational grammars characterize the languages recognized by Turing machines in deterministic exponential (2cn) time. The constraints on the transformational grammars are satisfied by many, though not all, known grammars in linguistic practice. We also give a simple algebraic characterization of the same class of languages and use it for the linguistic characterization.
[System testing, Turing machines, Natural languages, Character generation, Automata, Writing, Mathematical model, Character recognition, Context modeling]
Decidability of equivalence, containment, intersection, and separability of context-free languages
16th Annual Symposium on Foundations of Computer Science
None
1975
false
[Military computing, Scholarships, Production, Brazil Council, Optical character recognition software, Contracts]
Closest-point problems
16th Annual Symposium on Foundations of Computer Science
None
1975
A number of seemingly unrelated problems involving the proximity of N points in the plane are studied, such as finding a Euclidean minimum spanning tree, the smallest circle enclosing the set, k nearest and farthest neighbors, the two closest points, and a proper straight-line triangulation. For most of the problems considered a lower bound of O(N log N) is shown. For all of them the best currently-known upper bound is O(N2) or worse. The purpose of this paper is to introduce a single geometric structure, called the Voronoi diagram, which can be constructed rapidly and contains all of the relevant proximity information in only linear space. The Voronoi diagram is used to obtain O(N log N) algorithms for all of the problems.
[Computer science, Algorithm design and analysis, Computational geometry, Upper bound, Tree graphs, Clustering algorithms, Linear programming, Manufacturing, Wire, Design optimization]
An optimal bound for two dimensional bin packing
16th Annual Symposium on Foundations of Computer Science
None
1975
Bin packing and dynamic allocation problems hold an important place both in theoretical computer science and in practical issues arising in computer applications and operations research. While major analytic advances have been made for the 1-dimension case, optimal results for 2-dimensional problems have been elusive. As a model for analysis of various multidimensional bin packing and cutting stock problems, we consider the following question. Let be an arbitrary finite family of squares of total area at most unity; what is the smallest rectangle into which any such family can be packed without overlap? We answer this with the following optimal result: 1) any unit family can be packed into a rectangle with sides 2/&#x0221A;3 and &#x0221A;2; 2) any other rectangle with this packing property has larger area. The proof of this is rather lengthy and technical. We therefore restrict this presentation to a general discussion of the methods used and an outline of the major cases together with some specific examples.
[Computer science, Algorithm design and analysis, Operations research, Multidimensional systems, Numerical analysis, Heuristic algorithms, Computer applications, Mathematics, High level synthesis, Application software]
Computational complexity of decision procedures for polynomials
16th Annual Symposium on Foundations of Computer Science
None
1975
false
[Computer science, Turing machines, Laboratories, Tin, Polynomials, Encoding, Logic, Computational complexity, Equations]
An application of graph coloring to printed circuit testing
16th Annual Symposium on Foundations of Computer Science
None
1975
A proposed method for testing printed circuit boards for the existence of possible (undesired) short circuits transforms the test minimization problem into one of finding minimum vertex colorings of certain special graphs, called line-of-sight graphs. Under certain assumptions on the possible types of short circuits, we analyze the structure of such graphs and show that a well-known and efficient algorithm can be used to color them with a small number of colors. In particular, we show that no more than 5, 8, or 12 colors (depending on the particular assumptions) will ever be required for such a graph, independent of the number of vertices. Thus, in such cases, the potential advantage of the proposed method over exhaustive testing could be considerable.
[Algorithm design and analysis, Minimization methods, Printed circuits, Color, Conductors, Polynomials, Manufacturing, Circuit testing, Joining processes, Circuit analysis]
On the complexity of time table and multi-commodity flow problems
16th Annual Symposium on Foundations of Computer Science
None
1975
A very primitive version of Gotlieb's timetable problem is shown to be NP-complete, and therefore all the common timetable problems are NP-complete. A polynomial time algorithm, in case all teachers are binary, is shown. The theorem that a meeting function always exists if all teachers and classes have no time constraints is proved. The multi-commodity integral flow problem is shown to be NP-complete even if the number of commodities is two. This is true both in the directed and undirected cases. Finally, the two commodity real flow problem in undirected graphs is shown to be solvable in polynomial time. The time bound is O(|v|2|E|).
[Computer science, Education, Constraint theory, Educational institutions, Polynomials, Time factors, Mathematical model, NP-complete problem]
Foreword
17th Annual Symposium on Foundations of Computer Science
None
1976
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
The mutual exclusion problem for unreliable processes
17th Annual Symposium on Foundations of Computer Science
None
1976
Consider n processes operating asynchronously in parallel, each of wich maintains a single "special" variable which can be read (but not written) by the other processes. All coordination between processes is to be accomplished by means of the execution of the primitive operations of a process (1) reading another process's special variable, and (2) setting its own special variable to some value. A process may "die" at any time, when its special variable is (automatically) set a special "dead" value. A dead process may revive. Reading a special variable which is being simultaneously written returns either the old or the new value. Each process may be in a certain "critical" state (which it leaves if it dies). We present a coordination scheme with the following properties. (1) At most one process is ever in its critical state at a time. (2) If a process wants to enter its critical state, it may do so before any other process enters its critical state more than once. (3) The special variables are bounded in value. (4) Some process wanting to enter its critical state can always make progress to that goal. By the definition of the problem, no process can prevent another from entering its critical state by repeatedly failing and restarting. In the case of two processes, what makes our solution of particular interest is its remarkable simplicity when compared with the extant solutions to this problem. Our n-process solution uses the two-process solution as a subroutine, and is not quite as elegant as the two-process solution.
[Delay, Counting circuits]
Concurrency control for database systems
17th Annual Symposium on Foundations of Computer Science
None
1976
false
[Distributed databases, System recovery, Writing, Concurrency control, Database systems, Application software, Distributed computing]
A Linear time algorithm for deciding security
17th Annual Symposium on Foundations of Computer Science
None
1976
The Folklore is replete with stories of "secure" protection systems being compromised in a matter of hours. This is quite astounding since one is not likely to claim that a system is secure without some sort of proof to support the claim. In practice, proof is not provided and one reason for this is clear: although the protection primitives are apparently quite simple, they may potentially interact in extremely complex ways. Vague and informal arguments, therefore, often overlook subtleties that an adversary can exploit. Precision is not merely desirable for protection systems, it is mandatory.
[Data security, Concrete, Protection]
Graph grammars and global program data flow analysis
17th Annual Symposium on Foundations of Computer Science
None
1976
Program structure is defined in terms of a simple graph grammar, the "semi-structured flow graph grammar," which admits many of the control structure extensions suggested for "structured programming." The grammar defines a set of graph reductions which are shown to have the "Finite Church-Rosser (FCR)" property; i.e., when applied in any order to a graph, the limit (when no further reductions are possible) is unique. In particular, if a given graph is generated by the grammar, repeated application of the reductions will result in a single node regardless of the order in which they are applied. This property gives rise to an algorithm that parses a given program flow graph in time linear in the size of the graph. The resulting parse is used in a global data flow analysis algorithm which requires a number of bit-vector steps which is also linear in the size of the given graph.
[Algorithm design and analysis, Computer aided instruction, Data analysis, Program processors, Flow graphs, Code standards, Artificial intelligence, Information analysis]
Assignment commands and array structures
17th Annual Symposium on Foundations of Computer Science
None
1976
Straight line programs in which array elements can be referenced and set are considered. Two programs are equivalent if they compute the same expression as a function of the inputs. Testing the equivalence of programs with arrays is shown to be NP-complete, while programs without arrays can be tested for equivalence in linear time. Equivalence testing takes polynomial time when programs have either no references or no assignments to array elements.
[Software testing, Computer science, System testing, Computational modeling, Interactive systems, Input variables, Software algorithms, Programmable logic arrays, Debugging, Polynomials]
K + 1 heads are better than K
17th Annual Symposium on Foundations of Computer Science
None
1976
There are languages which can be recognized by a deterministic (k + 1)-headed oneway finite automaton but which cannot be recognized by a k-headed one-way (deterministic or non-deterministic) finite automaton. Furthermore, there is a language accepted by a 2-headed nondeterministic finite automaton which is accepted by no k-headed deterministic finite automaton.
[Computer science, Laboratories, Automata, Doped fiber amplifiers, Automatic control, Mathematics, Contracts]
A second step toward the polynomial hierarchy
17th Annual Symposium on Foundations of Computer Science
None
1976
Some of the questions posed by Baker, Gill, and Solovay [1] are here answered. The principal result is that there exists a recursive oracle for which the relativized polynomial hierarchy exists through the second level; that is, there is a recursive set B such that &#x03A3;2P,B &#x02260; &#x03C0;2P,B. It follows that &#x03A3;2P,B &#x02282;&#x02260; &#x03A3;3P,B.
[Visualization, Humans, Polynomials, Mathematics, Concrete, Arithmetic]
On the structure of complete sets: Almost everywhere complexity and infinitely often speedup
17th Annual Symposium on Foundations of Computer Science
None
1976
In this paper we investigate the structure of sets which are complete for various classes. We show, for example, that sets complete for deterministic time classes contain infinite polynomial time recognizable subsets, thus showing that they are not complex almost everywhere. We show by a related technique that any set complete for NEXP_TIME contains an infinite subset in DEXP_TIME, thereby showing that these sets do not require the use of non-determinism almost everywhere. Furthermore, we show that complete sets for deterministic time classes have effective I.O. speed-up to a polynomial; this strengthens a result of Stockmeyer.
[Computer science, Polynomials, NP-complete problem]
Diophantine complexity
17th Annual Symposium on Foundations of Computer Science
None
1976
false
[Computer interfaces, Computer science, Costs, Turing machines, Computational modeling, Laboratories, Polynomials, Time measurement, Logic, Computational complexity]
On parallelism in turing machines
17th Annual Symposium on Foundations of Computer Science
None
1976
A model of parallel computation based on a generalization of nondeterminism in Turing machines is introduced. Complexity classes //T(n)-TIME, //L(n)-SPACE, //LOGSPACE, //PTIME, etc. are defined for these machines in a way analogous to T(n)-TIME, L(n)-SPACE, LOGSPACE, PTIME, etc. for deterministic machines. It is shown that, given appropriate honesty conditions, L(n)-SPACE &#x02286; //L(n)2-TIME T(n)-TIME &#x02286; //log T(n)-SPACE //L(n)-SPACE &#x02286; exp L(n)-TIME //T(n)-TIME &#x02286; T(n)2-SPACE thus &#x000B7; &#x000B7; //EXPTIME = EXPSPACE //PSPACE = EXPTIME //PTIME = PSPACE //LOGSPACE = PTIME ? = LOGSPACE That is, the deterministic hierarchy LOGSPACE &#x02286; PTIME &#x02286; PSPACE &#x02286; EXPTIME &#x02286; ... shifts by exactly one level when parallelism is introduced. We give a natural characterization of the polynomial time hierarchy of Stockmeyer and Meyer in terms of parallel machines. Analogous space hierarchies are defined and explored, and a generalization of Saviten's result NONDET-L(n)-SPACE &#x02286; L(n)2-SPACE is given. Parallel finite automata are defined, and it is shown that, although they accept only regular sets, in general 22k states are necessary and sufficient to simulate a k-state parallel finite automaton deterministically.
[Concurrent computing, Computer science, Turing machines, Computational modeling, Automata, Parallel processing, Tin, Parallel machines, Magnetic heads, Polynomials]
Alternation
17th Annual Symposium on Foundations of Computer Science
None
1976
We define alternating Turing Machines which are like nondeterministic Turing Machines, except that existential and universal quantifiers alternate. Alternation links up time and space complexities rather well, in that alternating polynomial time equals deterministic polynomial space, and alternating linear space equals deterministic exponential time. Such considerations lead to a two-person game complete in polynomial time, and other games complete in exponential time. We also find that computability on a parallel processing machine is a rather rugged notion, and present two parallel processing models that are polynomially equivalent in their running times. We also show that while n-state alternating finite automata accept only regular sets that can be accepted by 22n-O(logn) state deterministic automata, alternating pushdown automata accept all languages accepted by Turing machines in deterministic exponential time.
[Concurrent computing, Bridges, Turing machines, Automata, Parallel processing, Polynomials, Mice, Character recognition, Game theory, Counting circuits]
Semantical consideration on floyo-hoare logic
17th Annual Symposium on Foundations of Computer Science
None
1976
This paper deals with logics of programs. The objective is to formalize a notion of program description, and to give both plausible (semantic) and effective (syntactic) criteria for the notion of truth of a description. A novel feature of this treatment is the development of the mathematics underlying Floyd-Hoare axiom systems independently of such systems. Other directions that such research might take are also considered. This paper grew out of, and is intended to be usable as, class notes [27] for an introductory semantics course. The three sections of the paper are: 1. A framework for the logic of programs. Programs and their partial correctness theories are treated as binary relations on states and formulae respectively. Truth-values are assigned to partial correctness assertions in a plausible (Tarskian) but not directly usable way. 2. Particular Programs. Effective criteria for truth are established for some programs using the Tarskian criteria as a benchmark. This leads directly to a sound, complete, effective axiom system for the theories of these programs. The difficulties involved in finding such effective criteria for other programs are explored. The reader's attention is drawn to Theorems 4, 16, 18 and 22-24, as worthy of mention even out of the context in which they now appear. 3. Variations and extensions of the framework. Alternatives to binary relations for both programs and theories are speculated on, and their possible roles in semantics are considered. We discuss a hierarchy of varieties of programs and the importance of this hierarchy to the issues of definability and describability. Modal logic is considered as a first-order alternative to Floyd-Hoare logic. We give an appropriate axiom system which is complete for loop-free programs and also puts conventional predicate calculus in a different light by lumping quantifiers with non-logical assignments rather than treating them as logical concepts. Proofs of all theorems are relegated to an appendix.
[Boolean functions, Calculus, Mathematics, Logic, Testing]
Categories for fixpoint-semantics
17th Annual Symposium on Foundations of Computer Science
None
1976
false
[Computer science, Computer languages, Upper bound, Councils, Roentgenium, Equations]
An algebraic formulation of knuthian semantics
17th Annual Symposium on Foundations of Computer Science
None
1976
This paper presents a formulation, within the framework of initial algebra semantics, of Knuthian semantic systems (K-systems) which contain both synthesized and inherited attributes. This formulation permits a precise definition of K-systems, and combines their intuitive appeal with the theoretical power of algebraic methods. The basic approach consists of algebraically specifying the semantic portion of a given K-system, converting this K-system into another equivalent one which contains only synthesized attributes, and then defining the new equivalent K-system by means of an algebraic formulation. The practical implications of the algebraic definition of K-systems are discussed, and the combined use of Knuth's original formulation and the algebraic approach for the development of semantic definitions is advocated.
[Computer science, Computer languages, Algebra, Writing, Research and development, Contracts, Design optimization, Sorting]
Algebraic families of interpretations
17th Annual Symposium on Foundations of Computer Science
None
1976
To each family C of interpretations corresponds an equivalence relation among program schemes, namely the equivalence of the program schemes for all interpretation of C. A family C is algebraic if any two programs are C-equivalent iff every partial finite computation of one of them is C-equivalent to some partial finite computation of the other. Our main theorem states that a family C is algebraic iff it is represented with respect to the equivalence of programs by a single interpretation (a C-Herbrand interpretation) which is algebraic (in Scott's sense, roughly speaking). We give examples of algebraic and non algebraic families.
[Lattices, Equations]
Rational algebraic theories and fixed-point solutions
17th Annual Symposium on Foundations of Computer Science
None
1976
In a wide variety of situations, computer science has found it convenient to define complex object as (fixed-point) solutions of certain equations. This has been done in both algebraic and order-theoretic settings, and has often been contrasted with other approaches. This paper shows how to formulate such solutions in a setting which encompasses both algebraic and order-theoretic aspects, so that the advantages of both worlds are available. Moreover, we try to show how this is consistent with other approaches to defining complex objects, through a number of applications, including: languages defined by context-free grammars; flow charts and their interpretations; and monadic recursive program schemes. The main mathematical results concern free rational theories and quotients of rational theories. However, the main goal has been to open up what we believe to be a beautiful and powerful new approach to the syntax and semantics of complex recursive specifications.
[Computer science, Flowcharts, Upper bound, Algebra, Formal languages, Automata, Concrete, Equations]
Simple languages and free schemes
17th Annual Symposium on Foundations of Computer Science
None
1976
A context-free language is said to be simple if it is accepted by a single-state deterministic push-down store acceptor that operates in real-time and accepts by empty store. While the problem remains open of deciding whether or not the language accepted by a deterministic pushdown store acceptor is simple, it is shown that this problem is equivalent to another problem in schemata theory. This question is that of determining whether or not a monadic recursion scheme has a strongly equivalent free scheme.
[Computer science, Automata, Automatic control]
Self-organizing binary search trees
17th Annual Symposium on Foundations of Computer Science
None
1976
We consider heuristics which attempt to maintain a binary search tree in a near optimal form, assuming that elements are requested with fixed, but unknown, independent probabilities. A "move to root" heuristic is shown to yield an expected search time within a constant factor of that of an optimal static binary search tree. On the other hand, a closely related "simple exchange" technique is shown not to have this property. The rate of convergence of the "move to root" heuristic is discussed. We also consider the more general case in which elements not in the tree may have non-zero probability of being requested.
[Computer science, Performance evaluation, Binary search trees, Cost function, Time measurement, Convergence]
The complexity of searching an ordered random table
17th Annual Symposium on Foundations of Computer Science
None
1976
false
[Interpolation, Upper bound, Minimax techniques, Information retrieval, Data structures, Mathematics, Probes]
Using computer trees to derive lower bounds for selection problems
17th Annual Symposium on Foundations of Computer Science
None
1976
false
[Computer science, Estimation theory, Binary trees, Minimax techniques, Data systems]
The analysis of hashing algorithms that exhibit k-ary clustering
17th Annual Symposium on Foundations of Computer Science
None
1976
In this paper we investigate the performance of hashing algorithms that begin the search into the table with k independent random probes.
[Algorithm design and analysis, Measurement, Clustering algorithms, Cost function, Performance analysis, Probes]
Complexity of trie index construction
17th Annual Symposium on Foundations of Computer Science
None
1976
Trie structures are a convenient way of indexing files in which keys are specified by values of attributes. Records correspond to leaves in the trie. Retrieval proceeds by following a path from the root to a leaf, the choice of edges being determined by attribute values. The size of a trie for a file depends on the order in which attributes are tested. We show that determining minimal size tries is an NP-complete problem for several variants of tries. For tries in which leaf chains are deleted we show that determining the trie for which average access time is minimal is also an NP-complete problem. Our results hold even for files in which attribute values are chosen from a binary or ternary alphabet.
[Computer science, Tree data structures, Measurement standards, Binary trees, Binary search trees, Information retrieval, NP-complete problem, Indexing, Testing]
Geometric intersection problems
17th Annual Symposium on Foundations of Computer Science
None
1976
We develop optimal algorithms for forming the intersection of geometric objects in the plane and apply them to such diverse problems as linear programming, hidden-line elimination, and wire layout. Given N line segments in the plane, finding all intersecting pairs requires O(N2) time. We give an O(N log N) algorithm to determine whether any two intersect and use it to detect whether two simple plane polygons intersect. We employ an O(N log N) algorithm for finding the common intersection of N half-planes to show that the Simplex method is not optimal. The emphasis throughout is on obtaining upper and lower bounds and relating these results to other problems in computational geometry.
[Computer science, Computational geometry, Operations research, Layout, Computer graphics, Linear programming, Displays, Mathematics, Application software, Wire]
Approximation algorithms for some routing problems
17th Annual Symposium on Foundations of Computer Science
None
1976
Several polynomial time approximation algorithms for some NP-complete routing problems are presented, and the worst-case ratios of the cost of the obtained route to that of an optimal are determined. A mixed-strategy heuristic with a bound of 9/5 is presented for the Stacker-Crane problem (a modified Traveling Salesman problem). A tour-splitting heuristic is given for k-person variants of the Traveling Salesman problem, the Chinese Postman problem, and the Stacker-Crane problem, for which a minimax solution is sought. This heuristic has a bound of e + 1 - 1/k, where e is the bound for the corresponding 1-person algorithm.
[Computer science, Cranes, Traveling salesman problems, Cities and towns, Minimax techniques, Approximation algorithms, Routing, Cost function, Educational institutions, Polynomials]
Variations of a new machine model
17th Annual Symposium on Foundations of Computer Science
None
1976
Motivated by an elementary programming system for formal language generation, we propose a generalized a pushdown acceptor which uses both a checking stack and a pushdown store in original manner. In the analysis of the machine-model parallel rewriting appears to be a fundamental tool for breaking the implicit recursion-structure. Variations of the machine-model (in the sense of Cook) lead to a natural hierarchy of machines defining DLOG, NLOG, P, NP, and PSPACE, showing different ways in which one class may be considered a restriction of another (as anticipated also in Galil's work on hierarchies of complete problems). Various applications in language theory and in the study of complexity classes are discussed.
[Computer science, Automata, Formal languages, Computational complexity, Counting circuits]
Recognizing certain repetitions and reversals within strings
17th Annual Symposium on Foundations of Computer Science
None
1976
Let P1 = {w &#x03B5; &#x03A3;*:w = wR, |w| &#x226B; 1} be the set of all nontrivial palindromes over &#x03A3;. In Part I, we present a linear-time on-line recognition algorithm for P1* ("palstar") on a random-access machine with addition and uniform cost criterion. We also present a lineartime on-line recognition algorithm for P12 on a multitape Turing machine and a recognition algorithm for P12 on a two-way deterministic pushdown automaton. The correctness of these algorithms is based on new "cancellation lemmas" for the languages P1* and P12. In Part II, we present real-time recognition algorithms for the languages {wxyxz &#x03B5; &#x03A3;*: |w|=r|x|, |y|=s|x|, |z|=t|x|} and {wxyxRz &#x03B5; &#x03A3;*: |w|=r|x|, |y|=s|x|, |z|=t|x|} on multitape Turing machines, for arbitrary fixed r, s, and t.
[Computer science, Turing machines, Automata, Read-write memory, Cost function]
Parenthesis generators
17th Annual Symposium on Foundations of Computer Science
None
1976
false
[Gold, RNA, Ice, Counting circuits]
On the evaluation of powers and related problems
17th Annual Symposium on Foundations of Computer Science
None
1976
false
[Computer aided instruction, Costs, Upper bound, Computational modeling, Polynomials, Sparse matrices, Helium]
Some polynomial and integer divisibility problems are NP-HARD
17th Annual Symposium on Foundations of Computer Science
None
1976
In an earlier paper [1], the author showed that certain problems involving sparse polynomials and integers are NP-hard. In this paper we show that many related problems are also NP-hard. In addition, we exhibit some new NP-complete problems. Most of the new results concern problems in which the nondeterminism is "hidden". That is, the problems are not explicitly stated in terms of one of a number of possibilities being true. Furthermore, most of these problems are in the areas of number theory or the theory of functions of a complex variable. Thus there is a rich mathematical theory that can be brought to bear. These results therefore introduce a class of NP-hard and NP-complete problems different from those known previously.
[Computer science, NP-hard problem, Polynomials, NP-complete problem]
Lower bounds from complex function theory
17th Annual Symposium on Foundations of Computer Science
None
1976
We employ elementary results from the theory of several complex variables to obtain a quadratic lower bound on the complexity of computing the mean distance between points in the plane. This problem has 2N inputs and a single output and we show that exactly N(N-1)/2 square roots must be computed by any program over +, -, &#x0D7;, &#x0F7;,) &#x0221A;, log and comparisons, even allowing an arbitrary field of constants. The argument is based on counting the total number of sheets of the Riemann surface of the analytic continuation to the complex domain of the (real) function computed by any algorithm which solves the problem. While finding an exact answer requires O(N2) operations, we show that an &#x03B5;-approximate solution can be obtained in O(N) time for any &#x03B5; &#x226B; 0, even if no square roots are permitted.
[Computer science, Algorithm design and analysis, Gaussian processes, Mathematics, Roundoff errors, Hardware, Machinery, Arithmetic]
Foreword
18th Annual Symposium on Foundations of Computer Science
None
1977
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
A necessary and sufficient condition for the existence of hoare logics
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Sufficient conditions, Computer languages, Flowcharts, Logic programming, Tellurium]
Data types
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer science, Computer languages, Algebra, Lattices, Mathematics, Equations]
The category-theoretic solution of recursive domain equations
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Lattices, Concrete, Artificial intelligence, Equations, Organizing]
Program invariants as fixed points
18th Annual Symposium on Foundations of Computer Science
None
1977
We argue that relative soundness and completeness theorems for Floyd-Hoare Axiom Systems ([6], [5], [18]) are really fixed point theorems. We give a characterization of program invariants as fixed points of functionals which may be obtained in a natural manner from the text of a program. We show that within the framework of this fixed point theory, relative soundness and completeness results have a particularly simple interpretation. Completeness of a Floyd-Hoare Axiom system is equivalent to the existence of a fixed point for an appropriate functional, and soundness follows from the maximality of this fixed point, The functionals associated with regular procedure declarations are similar to predicate transformers of Dijkstra; for non-regular recursions it is necessary to use a generalization of the predicate transformer concept which we call a relational transformer.
[Computer science, Computer languages, Transformers]
The temporal logic of programs
18th Annual Symposium on Foundations of Computer Science
None
1977
A unified approach to program verification is suggested, which applies to both sequential and parallel programs. The main proof method suggested is that of temporal reasoning in which the time dependence of events is the basic concept. Two formal systems are presented for providing a basis for temporal reasoning. One forms a formalization of the method of intermittent assertions, while the other is an adaptation of the tense logic system Kb, and is particularly suitable for reasoning about concurrent programs.
[Real time systems, Operating systems, System recovery, Reasoning about programs, Safety, Logic, Power system modeling, Stress, Programming profession, Clocks]
Language representation theorems: How to generate the R. E. sets from the regular sets
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer science, Turing machines, Automata, Formal languages, Writing, Mathematics, Polynomials, Books, Computational complexity, Power generation]
A new decidable problem, with applications
18th Annual Symposium on Foundations of Computer Science
None
1977
A number of decision problems that are unsolvable in general are solvable when restricted to systems with sufficiently simple "loop structure". Examples of such problems are the equivalence problems for flowchart schemata with nonintersecting loops and for the LOOP(l) programs of Meyer and Ritchie. We here present a theorem that gives a unifying view of the solvability of both of these problems, and also of a variety of other old and new solvable decision problems in automata theory, schematology, and logic.
[Flowcharts, Laboratories, Automata, Logic, Tellurium, Equations, Arithmetic]
The unsolvability of the equivalence problem for e-free NGSM's with unary input (output) alphabet and applications
18th Annual Symposium on Foundations of Computer Science
None
1977
It is shown that the equivalence problem is unsolvable for &#x03B5;-free nondeterministic generalized sequential machines whose input/output are restricted to unary/binary (binary/unary) alphabets. This strengthens a known result of Griffiths. Applications to some decision problems concerning right-linear grammars and directed graphs are also given.
[Computer science, Transducers, Application software]
Several results in program size complexity
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer languages, Laboratories, Size measurement, Encoding, Time measurement, Binary sequences]
The typed &#x003BB;-calculus is not elementary recursive
18th Annual Symposium on Foundations of Computer Science
None
1977
Historically, the principal interest in the typed &#x003BB;-calculus is in connection with Godel's functional ("Dialectica") interpretation'of intuitionistic arithmetic. However, since the early sixties interest has shifted to a wide variety of applications in diverse branches of logic, algebra, and computer science. For example, in proof-theory, in constructive logic, in the theory of functionals, in cartesian closed categories, in automatic theorem proving, and in the semantics of natural languages. In almost all such applications there is a point at which one must ask, for closed terms t<sub>1</sub> and t<sub>2</sub>, whether t<sub>1</sub> &#x003B2;-converts to t<sub>2</sub>. We shall show that in general this question cannot be answered by a Turing machine in elementary time. We shall also investigate the computational complexity of related questions concerning the typed. &#x003BB;-calculus (for example, the question of whether a given type contains a closed term).
[Computer science, Algebra, Turing machines, Automatic logic units, Logic functions, Polynomials, Arithmetic]
Precise bounds for presburger arithmetic and the reals with addition: Preliminary report
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Q measurement, Upper bound, Measurement standards, Time measurement, Polynomials, Samarium, Arithmetic]
Recursion theoretic characterizations of complexity theoretic properties
18th Annual Symposium on Foundations of Computer Science
None
1977
In this paper we exhibit recursion theoretic characterizations for two types of r.e. complexity sequences and use these characterizations to prove interesting facts about the classes of sets (or functions) possessing such complexity sequences.
[Cost function, Mathematics, Computational complexity]
The theory of joins in relational data bases
18th Annual Symposium on Foundations of Computer Science
None
1977
Answering queries in a relational database often requires that the natural join of two or more relations be computed. However, not all joins are semantically meaningful. This paper gives an efficient algorithm to determine whether the join of several relations is semantically meaningful (lossless) and an efficient algorithm to determine whether a set of relations has a subset with a lossy join. These algorithms assume that all data dependencies are functional. Similar techniques also apply to the case where data dependencies are multivalued.
[Relational databases, Proposals]
Fast decision algorithms based on union and find
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer science, Laboratories, Automata, Data structures, Decision feedback equalizers, Partitioning algorithms, Artificial intelligence]
An efficient parallel garbage collection system and ITS correctness proof
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer science, Upper bound, Writing, Data structures, Topology]
A space efficient method for the lowest common ancestor problem and an application to finding negative cycles
18th Annual Symposium on Foundations of Computer Science
None
1977
We present a method for computing ancestor information in trees. We show the method is tunable to specific applications, and compare it to other methods. Finally, we apply our procedures to the problem of finding negative cycles in sparse graphs.
[Performance evaluation, Tree graphs, Testing]
On uniquely represented data strauctures
18th Annual Symposium on Foundations of Computer Science
None
1977
A model for searching algorithms is developed which includes most tree-like searching structures such as lists, binary trees, AVL trees and 2, 3-trees. It is shown that no searching algorithm employing a data structure that is uniquely represented (up to isomorphism) can provide search, insert and delete functions all operating faster than c&#x0221A;n time for every n key tree. The c&#x0221A;n bound is shown to be achievable for uniquely represented data structures.
[Tree data structures, Computer science, Sufficient conditions, Stability, Binary trees, Data structures, Extraterrestrial measurements, Time measurement, Programming profession, Sorting]
On the capability of finite automata in 2 and 3 dimensional space
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer aided instruction, Power engineering computing, Laboratories, Automata, Search problems, Space exploration, Books, Game theory, Tellurium, Power generation]
Applications of a planar separator theorem
18th Annual Symposium on Foundations of Computer Science
None
1977
Any n-vertex planar graph has the property that it can be divided into components of roughly equal size by removing only O(&#x0221A;n) vertices. This separator theorem, in combination with a divide-and-conquer strategy, leads to many new complexity results for planar graph problems. This paper describes some of these results.
[Computer science, Costs, Particle separators, Circuits, Approximation algorithms, Complexity theory, Dynamic programming, Application software, NP-complete problem]
The power of commutativity
18th Annual Symposium on Foundations of Computer Science
None
1977
In this paper we show that the computation of the determinant requires an exponential number of multiplications if the commutativity of indeterminates is not allowed. The determinant can be computed in polynomial time with the commutation of indeterminates. Hence the use of commutativity can reduce a computation of exponential complexity to a computation of polynomial complexity.
[Tensile stress, Polynomials]
On taking roots in finite fields
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Jacobian matrices, Councils, Gaussian processes, Polynomials, History, Galois fields, Lagrangian functions, Contracts, Random number generation]
Saving space in fast string-matching
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer science, Turing machines, Heuristic algorithms, Automata, Storage automation, Magnetic heads]
A new proof of the linearity of the Boyer-Moore string searching algorithm
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Algorithm design and analysis, Performance evaluation, Costs, Tiles, Laboratories, Linearity, Telephony, Performance analysis, Machinery, Pattern matching]
On the average number of registers required for evaluating arithmetic expressions
18th Annual Symposium on Foundations of Computer Science
None
1977
Let An be the average number of registers required for evaluating arithmetic expressions of size n, or, equivalently, the minimal stack needed for exploring binary trees with n nodes. We give explicit expressions for An and related quantities and show that: An = log4(n) + C + E(log4n) + o(1) where C = 1/2 - &#x03B3; + 2/2 log2 + log2&#x03A0; 0.292 and E is continuous, periodic with period 1, with average value 0 and amplitude less than .05.
[Computer science, Algorithm design and analysis, Shape, Binary trees, Registers, Arithmetic]
Fast approximation algorithms for knapsack problems
18th Annual Symposium on Foundations of Computer Science
None
1977
Fully polynomial approximation algorithms for knapsack problems are presented. These algorithms are based on ideas of Ibarra and Kim, with modifications which yield better time and space bounds, and also tend to improve the practicality of the procedures. Among the principal improvements are the introduction of a more efficient method of scaling and the use of a median-finding routine to eliminate sorting. The 0-1 knapsack problem, for n items and accuracy &#x03B5; &#x226B; 0, is solved in (n log(1/&#x03B5;) + 1/&#x03B5;4) time and 0(n + 1/&#x03B5;3) space. The time bound is reduced to 0(n + 1/&#x03B5;3) for the "unbounded" knapsack problem. For the "subset-sum" problem, 0(n + 1/&#x03B5;3) time and 0(n + 1/&#x03B5;2) space, or 0(n + 1/&#x03B5;2 log (1/&#x03B5;)) time and space, are achieved. The "multiple choice" problem, with m equivalence classes, is solved in 0(nm2/&#x03B5;) time and space.
[Computer science, Laboratories, Approximation algorithms, Data structures, Polynomials, Sorting, Indexing, Arithmetic]
Combinatorial analysis of an efficient algorithm for processor and storage allocation
18th Annual Symposium on Foundations of Computer Science
None
1977
A combinatorial problem related to storage allocation is analyzed. The problem falls into a class of NP-complete, one-dimensional bin-packing problems. We propose an iterative approximation algorithm and show that it is superior to an earlier heuristic presented for this problem. The bulk of the paper is devoted to the proof of a worst-case performance bound.
[Algorithm design and analysis, Computer science, Processor scheduling, Terminology, Bismuth, Approximation algorithms, Iterative algorithms, Polynomials, Performance analysis, Scheduling algorithm]
Probabilistic computations: Toward a unified measure of complexity
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer science, Solid modeling, Tree graphs, Stochastic processes, Minimax techniques, Probes, Computational complexity, Testing, Sorting]
On triangulations of a set of points in the plane
18th Annual Symposium on Foundations of Computer Science
None
1977
A set, V, of points in the plane is triangulated by a subset T, of the straight-line segments whose endpoints are in V, if T is a maximal subset such that the line segments in T intersect only at their endpoints. The weight of any triangulation is the sum of the Euclidean lengths of the line segments in the triangulation. We examine two problems involving triangulations. We discuss the problem of finding a minimum weight triangulation among all triangulations of a set of points and give counterexamples to two published solutions to this problem. Secondly, we show that the problem of determining the existence of a triangulation, in a given subset of the line segments whose endpoints are in V, is NP-Complete.
[Computational geometry, Tree graphs, Laboratories, Euclidean distance, Graph theory, Complexity theory, Finite element methods]
New NP-hard and NP-complete polynomial and integer divisibility problems
18th Annual Symposium on Foundations of Computer Science
None
1977
false
[Computer science, NP-hard problem, Differential equations, Polynomials, Eigenvalues and eigenfunctions, Sparse matrices, NP-complete problem, Galois fields, Arithmetic]
Lower bounds for natural proof systems
18th Annual Symposium on Foundations of Computer Science
None
1977
Two decidable logical theories are presented, one complete for deterministic polynomial time, one complete for polynomial space. Both have natural proof systems. A lower space bound of n/log(n) is shown for the proof system for the PTIME complete theory and a lower length bound of 2cn/log(n) is shown for the proof system for the PSPACE complete theory.
[Turing machines, Computational modeling, Chromium, Length measurement, Extraterrestrial measurements, Particle measurements, Polynomials, Logic, Computational complexity, Sorting]
Foreword
19th Annual Symposium on Foundations of Computer Science
None
1978
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Description and analysis of an efficient priority queue representation
19th Annual Symposium on Foundations of Computer Science
None
1978
We present a new data-structure for representing priority queues, the pagoda. A detailed analysis shows that the pagoda provides a very efficient implementation of priority queues, where our measure of efficiency is the average run time of the various algorithms. It handles an arbitrary sequence of n primitive operations chosen from MIN, INSERT, UNION, EXTRACT and EXTRACTMIN in time o(n log n). The constant factors affecting these asymptotic run time are small enough to make the pagoda competitive with any other priority queue, including structures which cannot handle UNION or EXTRACT. The given algorithms process an arbitrary sequence of n operations MIN, INSERT and EXTRACT in linear average time O(n), and a sequence of n INSERT in linear worst case time O(n).
[Algorithm design and analysis, Computer science, Chromium, Data structures, Size measurement, Time measurement, Performance analysis, Application software, Data mining, Queueing analysis]
A dichromatic framework for balanced trees
19th Annual Symposium on Foundations of Computer Science
None
1978
In this paper we present a uniform framework for the implementation and study of balanced tree algorithms. We show how to imbed in this framework the best known balanced tree techniques and then use the framework to develop new algorithms which perform the update and rebalancing in one pass, on the way down towards a leaf. We conclude with a study of performance issues and concurrent updating.
[Computer science, Algorithm design and analysis, Particle measurements, Performance analysis, Petroleum]
A data structure for orthogonal range queries
19th Annual Symposium on Foundations of Computer Science
None
1978
Given a set of points in a d-dimensional space, an orthogonal range query is a request for the number of points in a specified d-dimensional box. We present a data structure and algorithm which enable one to insert and delete points and to perform orthogonal range queries. The worstcase time complexity for n operations is O(n logd n); the space usea is O(n logd-1 n). (O-notation here is with respect to n; the constant is allowed to depend on d.) Next we briefly discuss decision tree bounds on the complexity of orthogonal range queries. We show that a decision tree of height O(dn log n) (Where the implied constant does not depend on d or n) can be constructed to process n operations in d dimensions. This suggests that the standard decision tree model will not provide a useful method for investigating the complexity of such problems.
[Tree data structures, Computer science, Multidimensional systems, Binary search trees, Data structures, Decision trees]
Complexity of solvable cases of the decision problem for the predicate calculus
19th Annual Symposium on Foundations of Computer Science
None
1978
We analyze the computational complexity of determining whether F is satisfiable when F is a formula of the classical predicate calculus which obeys certain syntactic restrictions. For example, for the monadic predicate calculus and the G&#x0F6;del or &#x02203; ... &#x02203;&#x02200;&#x02200;&#x02203; ... &#x02203; prefix class we obtain lower and upper nondeterministic time bounds of the form cn/log n. The main tool in in these proofs is a finite version of Wang's domino problem, about which we present an interesting open question.
[Upper bound, Computer aided software engineering, Tiles, Laboratories, Calculus, Encoding, Polynomials, Computational complexity, Arithmetic]
GO is pspace hard
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Geography, Computer science, Polynomials, History, Game theory]
The complexity of checkers on an N &#x0D7; N board
19th Annual Symposium on Foundations of Computer Science
None
1978
We consider the game of Checkers generalized to an N &#x0D7; N board. Although certain properties of positions are efficiently computable (e.g., can Black jump all of White's pieces in a single move?), the general question, given a position, of whether a specified player can force a win against best play by his opponent, is shown to be PSPACE-hard. Under certain reasonable assumptions about the "drawing rule" in force, the problem is itself in PSPACE and hence is PSPACE-complete.
[Geography, Polynomials, Complexity theory, Computational complexity, Game theory, Standards Board]
One-way log-tape reductions
19th Annual Symposium on Foundations of Computer Science
None
1978
One-way log-tape (1-L) reductions are mappings defined by log-tape Turing machines whose read head on the input can only move to the right. The 1-L reductions provide a more refined tool for studying the feasible complexity classes than the P-time [2,7] or log-tape [4] reductions. Although the 1-L computations are provably weaker than the feasible classes L, NL, P and NP, the known complete sets for those classes are complete under 1-L reductions. However, using known techniques of counting arguments and recursion theory we show that certain log-tape reductions cannot be 1-L and we construct sets that are complete under log-tape reductions but not under 1-L reductions.
[Turing machines, Automata, Magnetic heads, Polynomials]
Halting space-bounded computations
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Computer science, Turing machines, Tree graphs, Computational modeling, Automata, Magnetic heads]
Two theorems on random polynomial time
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Polynomials, Mathematics, Complexity theory, Circuit analysis, Testing]
Improved bounds on the problem of time-space trade-off in the pebble game
19th Annual Symposium on Foundations of Computer Science
None
1978
Every family of graphs Gn with n nodes and bounded in-degree can be pebbled with o(n) pebbles in time o(n 1+c) for all c&#x226B;0. There is a family of graphs Gn such that pebbling Gn with O(n/log n) pebbles requires &#x03C9;(n(log n)k) moves for all k. The n-node jellyfish-graph as defined in (1) can be pebbled with O((log n)2) pebbles and O(n) moves.
[Upper bound]
Alternating pushdown automata
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Turing machines, Automata, Storage automation, Polynomials]
On tape Bounded probabilistic turing machine transducers
19th Annual Symposium on Foundations of Computer Science
None
1978
The tape requirements of probabilistic and deterministic Turing machine transducers are polynomially related.
[Transducers, Turing machines, Parallel programming, Computational modeling, Polynomials]
On alternation
19th Annual Symposium on Foundations of Computer Science
None
1978
Every alternating t(n) -time bounded multitape Turing machine can be simulated by an alternating t(n) -time bounded 1-tape Turing machine. Every nondeterministic t(n) -time bounded 1-tape Turing machine can be simulated by an alternating O(n+(t(n))1/2) -time bounded 1-tape Turing machine. For well-behaved functions t(n) every nondeterministic t(n) -time bounded 1-tape Turing machine can be simulated by a deterministic ((n log n)1/2 + (t(n))1/2) -tape bounded off-line Turing machine. These results improve or extend results by Chandra-Stockmeyer, Lipton-Tarjan and Paterson.
[Turing machines, Particle separators, Helium, Testing]
Equality languages, fixed point languages and representations of recursively enumerable languages
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Terminology, Formal languages, Mathematics, Decision feedback equalizers, Mirrors]
Computable nondeterministic functions
19th Annual Symposium on Foundations of Computer Science
None
1978
Functions computed on nondeterministic machines consist of two parts. The halting part which consists of outputs of halting computations, is, as expected, recursively enumerable. The divergence part, which consists of inputs for which diverging computations are possible, can however be any set in &#x03A3;11. Such highly noncomputable sets arise if one admits the "finite delay property". This implies that either we make a significant modification to our notion of "computable" as applied to nondeterministic machine models, or else that we ban the finite delay property for nondeterministic models.
[Probability distribution, Delay]
On the power of the compass (or, why mazes are easier to search than graphs)
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Geometry, Computer science, Power engineering computing, Computational modeling, Automata, Counting circuits]
Limited subsets of a free monoid
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Terminology, Automata]
Lower bounds on information transfer in distributed computations
19th Annual Symposium on Foundations of Computer Science
None
1978
We derive a lower bound on the interprocessor information transfer required for computing a function in a distributed network configuration. The bound is expressed in terms of the function's derivatives, and we use it to exhibit functions whose computation requires a great deal of interprocess communication. As a sample application, we give lower bounds on information transfer in the distributed computation of some typical matrix operations. Traditional measures of computational complexity, such as the number of primitive operations or memory cells required to compute functions, do not form an adequate framework for assessing the complexity of computations carried out in distributed networks. Even in the relatively straightforward situation of memoryless processors arranged in highly structured configurations, Gentleman [4] has demonstrated that data movement, rather than arithmetic operations, can often be the significant factor in the performance of parallel computations. And for the more general kinds of distributed processing, involving arbitrary network configurations and distributed data bases, the situation is correspondingly more complex. This paper addresses the problem of measuring computational complexity in terms of the interprocess communication required when a computation is distributed among a number of processors. More precisely, we model the distributed computation of functions which depend upon large amounts of data by assuming that the data is partitioned into disjoint subsets, and that a processor is assigned to each subset. Each processor (which we can think of as a node in a computational network) computes some values based on its own "local" data, and transmits these values to other processors, which are able to use them in subsequent local comutations. This "compute locally and share information" procedure is repeated over and over until finally some (predetermined) processor outputs the value of the desired function. In measuring the complexity of such computations we will be concerned, not with the individual local computations, but rather with the total information transfer, i.e., the total number of values which must be transmitted between processors. We derive a lower bound on the total information transfer required for computing a function in a distributed network. The bound is expressed in terms of the function's derivatives, and we use it to exhibit functions whose computation requires a great deal of interprocess communicaion. As a sample application, we give lower bounds on information transfer in the distributed computation of some typical matrix operations.
[Concurrent computing, Computer science, Distributed processing, High performance computing, Computational modeling, Laboratories, Computer networks, Distributed computing, Computational complexity, Arithmetic]
An optimal lower bound on the number of total operations to compute 0-1 polynomials over the field of complex numbers
19th Annual Symposium on Foundations of Computer Science
None
1978
We show an &#x03A9;(n/log n) lower bound on the total number of operations necessary to compute 0-1 polynomials of degree n in the model with complex preconditioning. The best previous result was &#x03A9;(n1/2/log n). This yields the first asymptotically optimal lower bound on the complexity of 0-1 polynomials in this model. We show also that there are 0-1 polynomials of degree n that require &#x03A9;(n1/2/log n) additive operations over C. The best previously shown lower bound on additions was &#x03A9;(n1/3/log n).
[Upper bound, Computational modeling, Polynomials, Concrete, Arithmetic]
Strassen's algorithm is not optimal trilinear technique of aggregating, uniting and canceling for constructing fast algorithms for matrix operations
19th Annual Symposium on Foundations of Computer Science
None
1978
A new technique of trilinear operations of aggregating, uniting and canceling is introduced and applied to constructing fast linear non-commutative algorithms for matrix multiplication. The result is an asymptotic improvement of Strassen's famous algorithms for matrix operations.
[Linear systems, Laser sintering, Matrices, Graph theory, Complexity theory, Equations, Arithmetic]
A decidability result for a second order process logic
19th Annual Symposium on Foundations of Computer Science
None
1978
We prove the decidability of the validity problem for a rather general language for talking about computations. As corollaries of our result, we obtain some decidability results of Pratt, Constable, Fischer-Ladner, and Pnueli and also a new decidability result for deterministic propositional dynamic logic.
[Frequency locked loops, Laboratories, Logic, Personal communication networks, Clocks, Testing]
Consistent and complete proof rules for the total correctness of parallel programs
19th Annual Symposium on Foundations of Computer Science
None
1978
We describe a formal theory of the total correctness of parallel programs, including such heretofore theoretically incomplete properties as safety from deadlock and starvation. We present a consistent and complete set of proof rules for the total correctness of parallel programs expressed in nondeterministic form. The proof of consistency and completeness is novel in that we show that the weakest preconditions for each correctness criterion are actually fixed-points (least or greatest) of continuous functions over the complete lattice of total predicates. We have obtained proof rule schemata which can universally be applied to least or greatest fixed points of continuous functions. Therefore, our proof rules are a priori consistent and complete once it is shown that certain weakest preconditions are extremum fixed-points. The relationship between true parallelism and nondeterminism is also discussed.
[Computer science, Concurrent computing, Automation, Region 3, Microprocessors, Lattices, System recovery, Parallel processing, Safety]
Model theoretic aspects of computational complexity
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Computer science, Turing machines, Set theory, Polynomials, Logic, Computational complexity, Contracts, Equations, Arithmetic]
on recursive equations having a unique solution
19th Annual Symposium on Foundations of Computer Science
None
1978
We give conditions on a left-linear Church-Rosser term rewriting system S allowing to define S-normal forms for infinite terms. We obtain a characterization of the S-equivalence of recursive program schemes (i.e. equivalence in all interpretations which validate S considered as a set of axioms). We give sufficient conditions for a recursive program scheme &#x03A3; to be S-univocal i.e. to have only one solution up to S-equivalence (considering &#x03A3; as a system of equations). For such schemes, we obtain proofs of S-equivalence which do not use any "induction principle". We also consider (SUE)-equivalence where S satisfies the above conditions and E is a set of bilinear equations such that no E-normal form does exist.
[Out of order, Sufficient conditions, Differential equations]
On the algebra of order extended abstract
19th Annual Symposium on Foundations of Computer Science
None
1978
Algebras whose carriers are partially ordered sets and operations are monotone and algebras whose carriers are complete partial orders and operations are continuous are studied. A quotient construction is provided for both types of algebras. The notion of a variety of algebras is defined and it is shown that the analogue of Birkhoff variety theorem holds for ordered algebras but not for continuous algebras. The results presented are a good first step towards a theory of ordered data types and a study of families of interpretations of schemas.
[Computer science, Computer languages, Flowcharts, Casting, Algebra, Terminology, Differential equations, Application software]
Data types as initial algebras: A unification of Scottery and ADJery
19th Annual Symposium on Foundations of Computer Science
None
1978
This paper presents a formulation within the framework of data types as initial algebras, of parameterizations of data types. We will observe that Scott's data type constructors like +, &#x0D7;, and circular definitions are parameterizations in our sense. Thence we will provide a uniform ground to compare ADJ-like data types with Lehmann-Smyth's data types, and will justify recent ADJ's claim that polynomial data types of Lehmann-Smyth could be simulated within ADJery.
[Computer science, Algebra, Lattices, Polynomials, Mathematical model]
A new algorithm for the maximal flow problem
19th Annual Symposium on Foundations of Computer Science
None
1978
A new algorithm for finding the maximal flow in a given network is presented. The algorithm runs in time O(V5/3E2/3) or O(n2.33) where n = V + E is the length of the input.
[Computer science, Law, Integral equations, Mathematics, Complexity theory, History, Legal factors]
A fast algorithm for single processor scheduling
19th Annual Symposium on Foundations of Computer Science
None
1978
false
[Computer science, Algorithm design and analysis, Processor scheduling, Job design, Polynomials, Scheduling algorithm]
Selection and sorting with limited storage
19th Annual Symposium on Foundations of Computer Science
None
1978
When selecting from, or sorting, a file stored on a read-only tape and the internal storage is rather limited, several passes of the input tape may be required. We study the relation between the amount of internal storage available and the number of passes required to select the Kth highest of N inputs. We show, for example, that to find the median in two passes requires at least &#x03A9;(N1/2) and at most O(N1/2 log N) internal storage. For probabilistic methods, &#x0398;(N1/2) internal storage is necessary and sufficient for a single pass method which finds the median with arbitrarily high probability.
[Computer science, Upper bound, Terminology, Computational modeling, Councils, Sampling methods, Large-scale systems, Sorting]
Improving the bounds on optimal merging
19th Annual Symposium on Foundations of Computer Science
None
1978
This paper presents a new merging algorithm and uses a counterstrategy argument to derive new upper and lower bounds on the complexity of the optimal merging problem. These improved bounds differ by at most &#x02308; m/4 &#x02309; comparisons, where m is the number of elements of the shorter sequence.
[Upper bound, Councils, Merging, H infinity control, Sorting, Testing, Information theory]
On lifted problems
19th Annual Symposium on Foundations of Computer Science
None
1978
This study may be viewed from the more general context of a theory of computational problems. An environment E= &#x02329;L,D&#x0232A; consists of a class of structures D and a language L for D. A problem in E is a pair of sets of formulas P = &#x02329;&#x03A0;|&#x0393;&#x0232A;, with problem predicate &#x03A0;. Let Ereal = &#x02329;Lreal,{R}&#x0232A; and Elin = &#x02329;Llin,Dlin&#x0232A; where R are the reals, Dlin is the class of totally ordered structures, Lreal and Llin are the languages of real ordered fields and linear orders, respectively. A problem P = &#x02329;&#x03A0;|&#x0393;&#x0232A; in Ereal is a lifted problem (from Elin) if &#x03A0; &#x03B5; Llin. The following interpretes an informal conjecture of Yao: CONJECTURE: Binary comparisons can solve nonredundant, full, lifted problems in Ereal as efficiently as general linear comparisons. The conjecture remains open. We may attack the conjecture by eliminating those comparisons that do not help or by studying those subclass of problems that are not helped by general linear comparisons. Various partial results are obtained, corresponding to these two approaches.
[Computer science, Input variables, Concrete, IEEE Foundation, Computational complexity, Arithmetic]
On the average-case complexity of selecting k-th best
19th Annual Symposium on Foundations of Computer Science
None
1978
Let Vk (n) be the minimum average number of pairwise comparisons needed to find the k-th largest of n numbers (k&#x02265;2), assuming that all n! orderings are equally likely. D. W. Matula proved that, for some absolute constant c, Vk(n)- n &#x02264; ck log log n as n &#x02192; &#x0221E;. In the present paper, we show that there exists an absolute constant c&#x02032; &#x226B; 0 such that Vk(n) - n &#x02265; c&#x02032;k log log n as n &#x02192; &#x0221E;, proving a conjecture by Matula.
[Computer science, Costs, Binary trees, Constraint theory, Decision trees]
Foreword
20th Annual Symposium on Foundations of Computer Science
None
1979
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Some theoretical aspects of position-location problems
20th Annual Symposium on Foundations of Computer Science
None
1979
The position-location problem is that of computing the coordinates of a set of objects in space (usually a plane) from a sparse set of distance measurements. Because the problem is analogous to that of constructing a pin-Jointed structure from rigid bars (of given respective lengths), it is intimately linked to problems of structural rigidity. In addition to its practical significance, the problem leads to a number of surprising results and intriguing theoretical problems in geometry, combinatorics, and algorithm design. This paper presents some of the theoretical algorithmic aspects of the position-location problem; its major objective is to attract researchers to complexity problems of structural rigidity. Among the major results presented is the discovery of a large class of geometrical decision problems, all of which are randomly decidable (i.e., decidable by a probabilistic polynomial-time algorithm), but many of which seem to be Intractable.
[Geometry, Algorithm design and analysis, Coordinate measuring machines, Position measurement, Extraterrestrial measurements, Reflection, Polynomials, Distance measurement, Rotation measurement, Bars]
On a general method for maximizing and minimizing among certain geometric problems
20th Annual Symposium on Foundations of Computer Science
None
1979
Problems concerned with finding inscribing or circumscribing polygons that maximize some measurement are considered such as: Find an area maximizing triangle inscribed in a given convex polygon. Algorithms solving a number of these problems in linear time are presented. They use the common approach of finding an initial solution with respect to a fixed bounding point and then iteratively transforming this solution into a new solution with respect to a new point. The generality of this approach is discussed and several open problems are noted.
[Computer science, Algorithm design and analysis, Area measurement, Optimization methods, Data structures, Iterative algorithms, Application software, Clocks]
Field extension and trilinear aggregating, uniting and canceling for the acceleration of matrix multiplications
20th Annual Symposium on Foundations of Computer Science
None
1979
The acceleration of matrix multiplication MM, is based on the combination of the method of algebraic field extension due to D. Bini, M. Capovani, G. Lotti, F. Romani and S. Winograd and of trilinear aggregating, uniting and canceling due to the author. A fast algorithm of O(N2.7378) complexity for N &#x0D7; N matrix multiplication is derived. With A. Sch&#x0F6;nhage's Theorem about partial and total MM, our approach gives the exponent 2.6054 by the price of a serious increase of the constant.
[Acceleration, Matrix decomposition, Books, Equations, Arithmetic]
Canonical labelling of graphs in linear average time
20th Annual Symposium on Foundations of Computer Science
None
1979
Canonical labelling of graphs (CL, for short) can be used, e.g., to test isomorphism. We prove that a simple vertex classification procedure results after only two refinement steps in a CL of random graphs with probability 1 - exp(-cn). With a slight modification we obtain a linear time CL algorithm with only exp(-cn log n/log log n) probability of failure. An additional depth-first search yields a CL of all graphs in linear average time.
[Algorithm design and analysis, Classification algorithms, Labeling, Testing]
Succinct certificates for the solvability of binary quadratic Diophantine equations
20th Annual Symposium on Foundations of Computer Science
None
1979
false
[Turing machines, Length measurement, Polynomials, Complexity theory, Cryptography, Computational complexity, Equations]
A subexponential algorithm for the discrete logarithm problem with applications to cryptography
20th Annual Symposium on Foundations of Computer Science
None
1979
false
[Computer science, Algorithm design and analysis, Laboratories, Public key, Public key cryptography, Mathematics, Application software, History, Security, Testing]
Computational complexity in algebraic function fields
20th Annual Symposium on Foundations of Computer Science
None
1979
false
[Manifolds, Euclidean distance, Cost function, Acceleration, Computational complexity, Motion analysis, Machinery, Qualifications]
Formal languages: Origins and directions
20th Annual Symposium on Foundations of Computer Science
None
1979
false
[Computer science, Conferences, Operating systems, Natural languages, Formal languages, Automata, Abstracts, Computational linguistics, Adders, Switching circuits]
The decidability of the equivalence of context-free grammar forms
20th Annual Symposium on Foundations of Computer Science
None
1979
false
[Automatic programming, Character generation, Automata, Formal languages, Production, Bismuth, Probes, Power generation, Tellurium]
Bijective A-transducers
20th Annual Symposium on Foundations of Computer Science
None
1979
In this paper we study bijective a-transducers. We derive necessary and sufficient conditions on pairs of regular sets (R,S) such that a bijective a-transducer, mapping R cnto S exists. The results obtained allow the systematic construction of an a-transducer, mapping a set R onto a set S bijectively for surprisingly "different" regular sets R and S.
[Sufficient conditions, Influenza, Cryptography, Radio access networks]
Semantics of probabilistic programs
20th Annual Symposium on Foundations of Computer Science
None
1979
Two complementary but equivalent semantic interpretations of a high level probabilistic programming language are given. One of these interprets programs as partial measurable functions on a measurable space. The other interprets programs as continuous linear operators on a Banach space of measures. It is shown how the ordered domains of Scott and others are embedded naturally into these spaces. Two general results about probabilistic programs are proved.
[Algorithm design and analysis, Computer languages, Turing machines, Stochastic processes, Power generation economics, Extraterrestrial measurements, Decision trees, Power system modeling, Surges, Combinatorial mathematics]
Models of program logics
20th Annual Symposium on Foundations of Computer Science
None
1979
We briefly survey the major proposals for models of programs and show that they all lead to the same propositional theory of programs. Methods of algebraic logic dominate in the proofs. One of the connections made between the models, that involving language models, is quite counterintuitive. The common theory has already been shown to be complete in deterministic exponential time; we give here a simpler proof of the upper bound.
[Upper bound, Algebra, Laboratories, Logic functions, Educational institutions, Page description languages, Concrete, Proposals, Equations]
Orderings for term-rewriting systems
20th Annual Symposium on Foundations of Computer Science
None
1979
Methods of proving that a term-rewriting system terminates are presented. They are based on the notion of "simplification orderings\
[Computer science]
Complexity of partial satisfaction
20th Annual Symposium on Foundations of Computer Science
None
1979
A conjunctive normal form (cnf) is 2-satisfiable, iff any 2 of its clauses are satisfiable. It is shown that every 2-satisfiable cnf s has an interpretation which satisfies at least h&#x000B7;length(s) clauses (h=(&#x0221A;5-1)/2&#x0223C;0.618). This result is optimal, insofar as the given constant h is maximal. The proof is polynomially constructive, i.e., it yields a polynomial algorithm, which computes an interpretation satisfying h&#x000B7;length(s) clauses for the 2-satisfiable cnf's s. Moreover, if h&#x0003C;h' and h' is e.g. algebraic, the following set is NP-complete: The 2-satisfiable cnf's s having an interpretation which satisfies at least h'&#x000B7;length(s) clauses.
[Education, Polynomials, Mathematics, Calculus]
The cube-connected-cycles: A versatile network for parallel computation
20th Annual Symposium on Foundations of Computer Science
None
1979
We introduce a network of processing elements, the cube-connected-cycles (CCC), complying with the present technological constraints of VLSI design. By combining the principles of parallelism and pipelining, the CCC can emulate the cube-connected machine with no significant degradation of performance but with a much more compact structure. We describe in detail how to program the CCC for efficiently solving a large class of problems, which includes Fast-Fourier-Transform, sorting, permutations, and derived algorithms. The CCC can also be used as a general purpose parallel processor.
[Concurrent computing, Algorithm design and analysis, Parallel processing, Very large scale integration, Read-write memory, Computer networks, Timing, Sorting, Pipeline processing, Communication standards]
Transforming static data structures to dynamic structures
20th Annual Symposium on Foundations of Computer Science
None
1979
In this paper we will investigate transformations that serve as tools in the design of new data structures. Specifically, we study general methods for converting static structures (in which all elements are known before any searches are performed) to dynamic structures (in which the insertion of a new element can be mixed with searches). We will see three classes of such transformations (each based on a different counting scheme for representing the integers) and then use a combinatorial model to show the optimality of many of the transformations. Issues such as online data structures and deletion of elements are also examined. To demonstrate the applicability of these tools, we will study six new data structures that have been developed by applying the transformations.
[Computer science, Tin, Writing, Data structures, Contracts, Nearest neighbor searches]
Toward self-organizing linear search
20th Annual Symposium on Foundations of Computer Science
None
1979
We consider techniques for adapting linear lists so that the more frequently accessed elements are found near the front, even though we are not told the probabilities of various elements being accessed. The main results are discussed in two sections. Perhaps the most interesting deals with techniques which move an element toward the front only after it has been requested k times in a row. The other, technically more difficult, section deals with the analysis of the heuristic which moves an element to the head of the list each time it is accessed. The behaviour of this scheme under a number of interesting probability distributions is discussed. Two basic approaches to the technique of moving an element forward after it has been accessed k times in a row are discussed. The first performs the transformation after any k identical requests. The second essentially groups requests into batches of at least k, and performs the action only if the last k requests of a batch are the same. Adopting as the transformation, the moving of the requested element to the front of the list, the second approach is shown to lead to faster average search time under all nontrivial probability distributions for k &#x02265;2. It is also shown that the "periodic" approach, with k = 2, never leads to an average search time greater than 1.21.. times that of the optimal ordering. For the more direct approach, a ratio of 1.36.. is shown under the same constraints. In studying the simple move to front heuristic (i.e. k = 1), it is shown that for a particular distribution this scheme can lead to an average number of probes &#x03C0;/2 times that of the optimal order. Within an interesting class of distributions, this is shown to be the worst average behaviour.
[Computer science, Lead, Cost function, Probability distribution, Probes, Counting circuits]
New classes and applications of hash functions
20th Annual Symposium on Foundations of Computer Science
None
1979
In this paper we exhibit several new classes of hash functions with certain desirable properties, and introduce two novel applications for hashing which make use of these functions. One class of functions is small, yet is almost universal2. If the functions hash n-bit long names into m-bit indices, then specifying a member of the class requires only O((m + log2log2(n)) log2(n)) bits as compared to O(n) bits for earlier techniques. For long names, this is about a factor of m larger than the lower bound of m+log2n-log2m bits. An application of this class is a provably secure authentication techniques for sending messages over insecure lines. A second class of functions satisfies a much stronger property than universal2. We present the application of testing sets for equality. The authentication technique allows the receiver to be certain that a message is genuine. An 'enemy' - even one with infinite computer resources - cannot forge or modify a message without detection. The set equality technique allows the the operations 'add member to set', 'delete member from set' and 'test two sets for equality' to be performed in expected constant time and with less than a specified probability of error.
[Performance evaluation, Associative memory, Algorithms, Authentication, Computer errors, Packaging, Public key cryptography, Application software, Digital signatures, Testing]
Towards analysing sequences of operations for dynamic data structures
20th Annual Symposium on Foundations of Computer Science
None
1979
This paper presents the average case performance analysis of dynamic data structures subjected to arbitrary sequences of insert, delete and query operations. To such sequences of operations are associated, for each data type, a specific continued fraction and a familly of orthogonal polynomials : Tchebycheff for stacks, Laguerre for dictionaries, Hermite for priority queues, Meixner for linear lists and Charlier for symbol tables. We define a notion of integrated cost of a data structure as the average cost over all possible sequences of operations. Our main result is an explicit expression, for each of these data structures, of the generating function for integrated costs as a linear integral transform of the generating functions for individual operation costs. We use the result to explicitly compute integrated costs of various efficient data structure implementations.
[Computer science, Dictionaries, Data analysis, Data structures, Cost function, Polynomials, Performance analysis, History, Helium]
Efficient algorithms for simple matroid intersection problems
20th Annual Symposium on Foundations of Computer Science
None
1979
Given a matroid, where each element has a realvalued cost and is colored red or green; we seek a minimum cost base with exactly q red elements. This is a simple case of the matroid intersection problem. A general algorithm is presented. Its efficiency is illustrated in the special case of finding a minimum spanning tree with q red edges; the time is O(m log log n + n &#x03B1; (n,n) log n). Efficient algorithms are also given for job scheduling matroids and partition matroids. An algorithm is given for finding a minimum spanning tree where a vertex r has prespecified degree; it shows this problem is equivalent to finding a minimum spanning tree, without the degree constraint. An algorithm is given for finding a minimum spanning tree on a directed graph, where the given root r has prespecified degree; the time is O(m log n), the same as for the problem without the degree constraint.
[Computer science, Tree graphs, Cost function, Polynomials, Computer networks, Partitioning algorithms, Bipartite graph, Communication networks, Scheduling algorithm, Contracts]
A polynomial time algorithm for solving systems of linear inequalities with two variables per inequality
20th Annual Symposium on Foundations of Computer Science
None
1979
We present a constructive algorithm for solving systems of linear inequalities (LI) with at most two variables per inequality. The algorithm is polynomial in the size of the input. The LI problem is of importance in complexity theory since it is polynomial time (Turing) equivalent to linear programming. The subclass of LI treated in this paper is also of practical interest in mechanical verification systems, and we believe that the ideas presented can be extended to the general LI problem.
[Computer science, Sections, Cities and towns, Linear programming, Polynomials, Vectors, Encoding, Complexity theory, Linear matrix inequalities, Time factors]
Random walks, universal traversal sequences, and the complexity of maze problems
20th Annual Symposium on Foundations of Computer Science
None
1979
false
[Veins, Automata, Polynomials, Mice, Labeling, Testing, Random sequences]
Observations about the development of theoretical computer science
20th Annual Symposium on Foundations of Computer Science
None
1979
This paper gives a personal account of some developments in automata theory and computational complexity theory. Though the account is subjective and deals primarily with the research areas of direct interest to the author, it discusses the underlying beliefs and philosophy which guided this research as well as the intellectual environment and the ideas and contacts which influenced it. An attempt is also made to draw some general conclusions about computer science research and to discuss the nature of theoretical computer science.
[Computer science, Automata, Computer applications, Mathematics, Explosives, Application software, Acceleration, Computational complexity, Physics, Tellurium]
Resource allocation with immunity to limited process failure
20th Annual Symposium on Foundations of Computer Science
None
1979
Upper and lower bounds are proved for the shared space requirements for solution of several problems involving resource allocation among asynchronous processes. Controlling the degradation of performance when a limited number of processes fail is of particular interest.
[Computer science, Algorithm design and analysis, Degradation, Space technology, Process control, Access protocols, System recovery, Resource management, Petroleum, Testing]
Approximate algorithms for optimization of busy waiting in parallel programs
20th Annual Symposium on Foundations of Computer Science
None
1979
Traditional implementations of conditional critical regions and monitors can lead to unproductive "busy waiting" if processes are allowed to wait on arbitrary boolean expressions. Techniques from global flow analysis may be employed at compile time to obtain information about which critical regions (monitor calls) are enabled by the execution of a given critical region (monitor call). We investigate the complexity of computing this information and show how it can be used to obtain efficient scheduling algorithms with less busy waiting.
[Parallel programming, Scheduling algorithm, Delay, Information analysis]
Modeling communications protocols by automata
20th Annual Symposium on Foundations of Computer Science
None
1979
Using a pair of finite-state automata to model the transmitter-receiver protocol in a data communications system, we derive lower bounds on the size of automata needed to achieve reliable communication across an error-phone channel. We also show that, at the cost of increasing the size of the automata, a transmission rate close to the theoretical maximum can be achieved.
[Protocols, Redundancy, Automata, Error correction codes, Decoding, Error correction, Delay, Radio access networks, Clocks]
Controlling concurrency using locking protocols
20th Annual Symposium on Foundations of Computer Science
None
1979
This paper is concerned with the problem of developing locking protocols for ensuring the consistency of database systems that are accessed concurrently by a number of independent transactions. It is assumed that the database is modelled by a directed acyclic graph whose vertices correspond to the database entities, and whose arcs correspond to certain locking restrictions. Several locking protocols are presented. The weak protocol is shown to ensure consistency and deadlock-freedom only for databases that are organized as trees. For the databases that are organized as directed acyclic graphs, the strong protocol is presented. Discussion of SHARED and EXCLUSIVE locks is also included.
[Concurrent computing, Tree graphs, Resumes, Access protocols, System recovery, Control systems, Database systems, Concurrency control, Transaction databases, Programming profession]
Locking policies: Safety and freedom from deadlock
20th Annual Symposium on Foundations of Computer Science
None
1979
false
[Concurrent computing, Bioreactors, System recovery, Tin, Concurrency control, Safety, Transaction databases, Protection, Tellurium, Contracts]
On time versus space II
20th Annual Symposium on Foundations of Computer Science
None
1979
Logarithmically t(n)-time bounded RAMs can be simulated by t(n)/log t(n)-tape bounded Turing machines, t(n)-time bounded multidimensional multitape Turing machines can be simulated by t(n) loglog t(n)/log t(n)-tape bounded Turing machines.
[Multidimensional systems, Costs, Turing machines, Computational modeling, Writing, Time measurement, Computational complexity]
On simultaneous resource bounds
20th Annual Symposium on Foundations of Computer Science
None
1979
It is well known that time bounds for machines correspond closely to size bounds for networks, and that space bounds correspond to depth bounds. It is not known whether simultaneous time and space bounds correspond to simultaneous size and depth bounds. It is shown here that simultaneous time and "reversal" bounds correspond to simultaneous size and depth bounds, and that simultaneous time and space bounds correspond to simultaneous size and "width" bounds.
[Computer networks, Polynomials, Logic]
On uniform circuit complexity
20th Annual Symposium on Foundations of Computer Science
None
1979
We consider uniform circuit complexity, introduced by Borodin as a model of parallel complexity. Three main results are presented. First, we show that simultaneous size/depth of uniform circuits is the same as space/time of alternating Turing machines, with depth and time within a constant factor and likewise log(size) and space. Second, we apply this to characterize the class of polynomial size and polynomial-in-log depth circuits in terms of tree-size bounded alternating TM's, in particular showing that context-free recognition is in this class of circuits. Third, we investigate various definitions of uniform circuit complexity, showing that it is fairly insensitive to the choice of definition.
[Computer science, Costs, Size measurement, Particle measurements, Hardware, Time measurement, Polynomials, Vectors, Complexity theory, Combinational circuits]
A time-space tradeoff for sorting on non-oblivious machines
20th Annual Symposium on Foundations of Computer Science
None
1979
A model of computation is introduced which permits the analysis of both the time and space requirements of non-oblivious programs. Using this model, it is demonstrated that any algorithm for sorting n inputs which is based on comparisons of individual inputs requires time-space product proportional to n2. Uniform and non-uniform sorting algorithms are presented which show that this lower bound is nearly tight.
[Computer science, Upper bound, Computational modeling, Councils, Merging, Marketing and sales, Registers, Sorting, Testing]
A TcS2 = 0 (2n) time/space tradeoff for certain NP-complete problems
20th Annual Symposium on Foundations of Computer Science
None
1979
In this paper we develop a general purpose algorithm that can solve a number of NP-complete problems in time T = O(2n/2) and space S = O(2n/4). The algorithm can be generalized to a family of algorithms whose time and space complexities are related by T&#x000B7;S2 = O(2n). The problems it can handle are characterized by a few decomposition axioms, and they include knapsack problems, exact satisfiability problems, set covering problems, etc. The new algorithm has a considerable cryptanalytic significance, since it can break the Merkle-Hellman public key cryptosystem whose recommended size is n = 100.
[Space technology, Computational modeling, Cities and towns, Public key cryptography, Polynomials, Mathematics, Partitioning algorithms, Safety, NP-complete problem, Contracts]
Length of predicate calculus formulas as a new complexity measure
20th Annual Symposium on Foundations of Computer Science
None
1979
We introduce a new complexity measure, QR[f(n)], which clocks the size of formulas from predicate calculus needed to express a given property. Techniques from logic are used to prove sharp lower bounds in the measure. These results demonstrate space requirements for computations and may provide techniques for seperating Time and Space complexity classes because we show that: NSPACE[f(n)] &#x02286; QR[(f(n))2/log(n)] &#x02286; DSPACE[f(n)2].
[Computer science, Turing machines, Length measurement, Extraterrestrial measurements, Size measurement, Calculus, Time measurement, Logic, Clocks, Testing]
Multiple-person alternation
20th Annual Symposium on Foundations of Computer Science
None
1979
We generalize the alternation machines of Chandra, Kozen and Stockmeyer [1] and the private alternation machines of Reif [14] to model multiple person (team) games of incomplete information. The resulting classes of machines are "multiple person alternation machines". The characterization of certain time and space bounded versions of these machines demonstrate interesting relationships between ordinary time and space hierarchies (Table 1). Our results are applied to relative succintness and power questions of finite state machines and to complexity questions of parallel finite state machines. Other machine variants, including private alternating pushdown store automata and Markovian alternation machines, are discussed.
[Computer science, Upper bound, Law, Computational modeling, Automata, Concrete, History, Game theory, Legal factors, Information analysis]
Explicit constructions of linear size superconcentrators
20th Annual Symposium on Foundations of Computer Science
None
1979
false
[Graph theory, Bipartite graph, Joining processes]
Origins of recursive function theory
20th Annual Symposium on Foundations of Computer Science
None
1979
For over two millennia mathematicians have used particular examples of algorithms for determining the values of functions. The notion of "&#x03BB;-definability" was the first of what are now accepted as equivalent exact mathematical descriptions of the class of all number-theoretic functions for which algorithms exist. This article explains the notion, and traces the investigation in 1931-3 by which quite unexpectedly it was so recognized. The Herbrand-G&#x0F6;del notion of "general recursiveness" 1934, and the Turing notion of "computability" 1936 were the second and third of the equivalent notions. Techniques developed in the study of &#x03BB;-definability were applied in the analysis of general recursiveness and Turing computability.
[Interpolation, Eyes, Mathematics, Logic, Arithmetic]
Relativized cryptography
20th Annual Symposium on Foundations of Computer Science
None
1979
It seems very difficult to give a formal definition of computational security for Public Key Cryptography. We define a slightly different notion, called Transient-Key Cryptography, for which a natural definition of security against chosen-plaintext-attacks can be given. The main result presented here is the existence of a relativized model of computation under which there exists a provably secure transientkey cryptosystem. Indeed, there exists a computable oracle that can be used by cryptographers to efficiently encipher and decipher messages, yet it is of no help to the cryptanalyst trying to decode messages not intended for him. As a corollary, there exists a length-preserving permutation, the inverse of which is hard to compute on most elements of its domain even if arbitrary evaluations of the function itself are allowed for free.
[Art, Computational modeling, Information security, Public key, Public key cryptography, Decoding, Communication system security, Computational complexity, Distributed computing, Delay]
Succinctness, verifiability and determinism in representations of polynomial-time languages
20th Annual Symposium on Foundations of Computer Science
None
1979
Several representations of P, the class of deterministic polynomial time acceptable languages, are compared with respect to succinctness. It is shown that requirements such as polynomial running time, verifiability of running time, and verifiability of accepting a set in P can be causes for differences in succinctness that are not recursively bounded. Relating succinctness to nondeterminism, it is shown that P &#x02260; NP if and only if the relative succinctness of representing languages in P by deterministic and nondeterministic clocked polynomial time machines is not recursively bounded. questions are posed, concerning the implications of P = NP, with respect to translatability and succinctness between other pairs of deterministic and nondeterministic representations for P.
[Computer science, Cities and towns, Polynomials, Time factors, NP-complete problem, Cryptography, Computational complexity, Clocks]
Reductions that lie
20th Annual Symposium on Foundations of Computer Science
None
1979
All of the reductions currently used in complexity theory (&#x02264;p, &#x02264;&#x03B3;, &#x02264;R) have the property that they are honest. If A &#x02264; B then whatever machine M reduces A to B is such that: if on input x, M outputs y then x &#x03B5; A &#x02194; y &#x03B5; B. It would appear that this membership preserving property is intrinsic to the notion of reduction. We will see that it is not. We introduce reductions that lie and sometimes produce outputs y &#x03B5; B when x ? A. We will use these reductions to further clarify the computational complexity of some problems raised by Gauss.
[Computer science, Turing machines, Laboratories, Gaussian processes, Polynomials, Mathematics, Complexity theory, NP-complete problem, Computational complexity, Hip]
Division is good
20th Annual Symposium on Foundations of Computer Science
None
1979
We study the power of RAM acceptors with several instruction sets. We exhibit several instances where the availability of the division operator increases the power of the acceptors. We also show that in certain situations parallelism and stochastic features ('distributed random choices') are provably more powerful than either parallelism or randomness alone. We relate the class of probabilistic Turing machine computations to random access machines with multiplication (but without boolean vector operations). Again, the availability of integer division seems to play a crucial role in these results.
[Concurrent computing, Turing machines, Instruction sets, Computational modeling, Stochastic processes, Read-write memory, Parallel processing, Polynomials, Magnetic heads, Distributed computing]
Complexity of the mover's problem and generalizations
20th Annual Symposium on Foundations of Computer Science
None
1979
This paper concerns the problem of moving a polyhedron through Euclidean space while avoiding polyhedral obstacles.
[Computer science, Computational geometry, Turing machines, Polynomials, Slabs, Arm, Elbow, Robots]
Foreword
21st Annual Symposium on Foundations of Computer Science
None
1980
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
On linear characterizations of combinatorial optimization problems
21st Annual Symposium on Foundations of Computer Science
None
1980
We show that there can be no computationally tractable description by linear inequalities of the polyhedron associated with any NP-complete combinatorial optimization problem unless NP = co-NP -- a very unlikely event. We also apply the ellipsoid method for linear programming to show that a combinatorial optimization problem is solvable in polynomial time if and only if it admits a small generator of violated inequalities.
[Computer science, Laboratories, Optimization methods, Linear programming, Polynomials, Vectors, Ellipsoids, Indexing]
On a class of totally unimodular matrices
21st Annual Symposium on Foundations of Computer Science
None
1980
We examine the class of matrices that satisfy Commoner's sufficient condition for total unimodularity [C], which we call restricted totally unimodular (RTUM). We show that a matrix is RTUM if and only if it can be decomposed in a very simple way into the incidence matrices (or their transposes) of bipartite graphs or directed graphs, and give a linear time algorithm to perform this task. Based on this decomposition, we show that the 0,1 Integer Programming Problem with an RTUM matrix of constraints has the same time complexity as the b-matching and the max flow problems.
[Sufficient conditions, Linear programming, Vectors, Polynomials, Bipartite graph, Computational Intelligence Society, Matrix decomposition, Testing]
Some theorems about matrix multiplication
21st Annual Symposium on Foundations of Computer Science
None
1980
This paper considers the computation of matrix chain products of the form M1 &#x0D7; M2 &#x0D7;...&#x0D7; Mn-1. If the matrices are of different dimensions, the order in which the matrices are computed affects the number of operations. An optimum order is an order which minimizes the total number of operations. We present some theorems about an optimum order of computing the matrices. Based on these theorems, an O(n log n) algorithm for finding the optimum order is presented.
[Costs, Heuristic algorithms, Graph theory, Partitioning algorithms, Dynamic programming, Joining processes, Clocks]
Polynomial-time algorithms for permutation groups
21st Annual Symposium on Foundations of Computer Science
None
1980
A permutation group on n letters may always be represented by a small set of generators, even though its size may be exponential in n. We show that it is practical to use such a representation since many problems such as membership testing, equality testing, and inclusion testing are decidable in polynomial time. In addition, we demonstrate that the normal closure of a subgroup can be computed in polynomial time, and that this proceaure can be used to test a group for solvability. We also describe an approach to computing the intersection of two groups. The procedures and techniques have wide applicability and have recently been used to improve many graph isomorphism algorithms.
[Computer science, Algorithm design and analysis, Tin, Polynomials, Mathematics, Lagrangian functions, Testing]
Isomorphism of graphs of bounded valence can be tested in polynomial time
21st Annual Symposium on Foundations of Computer Science
None
1980
Suppose we are given a set of generators for a group G of permutations of a colored set A. The color automorphism problem for G involves finding generators for the subgroup of G which stabilizes the color classes. Testing isomorphism of graphs of valence &#x02264; t is polynomial-time reducible to the color automorphism problem for groups with small simple sections. The algorithm for the latter problem involves several divide-and-conquer tricks. The problem is solved sequentially on the G-orbits. An orbit is broken into a minimal set of blocks permuted by G. The hypothesis on G guarantees the existence of a 'large' subgroup P which acts as a p-group on the blocks. A similar process is repeated for each coset of P on G. Some results on primitive permutation groups are used to show that the algorithm runs in polynomial time.
[Tree graphs, Terminology, Resumes, Polynomials, Mathematics, Machinery, Probes, Testing]
A fast algorithm for multiprocessor scheduling
21st Annual Symposium on Foundations of Computer Science
None
1980
false
[Processor scheduling, Parallel machines, Polynomials, Scheduling algorithm, Single machine scheduling, Finishing]
Sparse complete sets for NP: Solution of a conjecture of Berman and Hartmanis
21st Annual Symposium on Foundations of Computer Science
None
1980
A set S &#x02282; {0,1}* is sparse if there is a polynomial p such that the number of strings in S of size at most n is at most p(n). All known NP-complete sets, such as SAT, are not sparse. The main result of this paper is that if there is a sparse NP-complete set under many-one reductions, then P = NP. We also show that if there is a sparse NP-complete set under Turing reductions, then the polynomial time hierarchy collapses to &#x0394;2P.
[Computer science, Search methods, Polynomials]
Efficient algorithms for path system problems and applications to alternating and time-space complexity classes
21st Annual Symposium on Foundations of Computer Science
None
1980
Let SPS(f(n)) denote the solvable path system problem for path systems of bandwidth f(n) and SPS (f(n)) the corresponding problem for monotone systems. Let DTISP (poly, f(n)) denote the polynomial time and simultaneous f(n) space class and SC = UkDTISP (poly, logkn). Let ASPACE (f(n)) denote the sets accepted by f(n) space bounded alternating TMs and ASPACE (f(n)) the corresponding one-way TM family. Then, for "well-behaved" functions f&#x03B5;O(n)-o(log n), (1) SPS (f(n)) is &#x02264;log-complete for DTISP (poly, f(n)), (2) {SPS(f(n)k)}k&#x02265;1 is &#x02264;log-complete for ASPACE (logf(n)), (3) {SPS (f(n)k)}k&#x02265;1 is &#x02264;log-complete for ASPACE (log f(n)), (4) SPS(f(n)) &#x03B5; DSPACE(f(n) &#x0D7; log n), (5) ASPACE(log f(n)) &#x02286; UkDSPACE(f(n)k), and (6) SC = CLOSURE &#x02264;log(ASPACE(log log n)).
[Bandwidth, Polynomials, Sparse matrices]
Upper and lower bounds for first order expressibility
21st Annual Symposium on Foundations of Computer Science
None
1980
We continue the study of first order expressibility as a measure of complexity, introducing the new class Var &#x00026;Sz[v(n),z(n)] of languages expressible with v(n) variables in sentences of size z(n). We show that when the variables are restricted to boolean values: BVar &#x00026;Sz[v(n),z(n)] = ASPACE&#x00026;TIME[v(n),t(n)] That is variables and size correspond precisely to alternating space and time respectively. Returning to variables ranging over an n element universe, it follows that: Var[O(1)] = ASPACE[log n] = PTIME That is the family of properties uniformly expressible with a constant number of variables is just PTIME. These results hold for languages with an ordering on the objects in question, e.g. for graphs a successor relation on the vertices. We introduce an "alternating pebbling game" to prove lower bounds on the number of variables and size needed to express properties without successor. We show, for example, that k variables are needed to express Clique(k), suggesting that this problem requires DTIME[nk].
[Computer science, Reactive power, Turing machines, Size measurement, Polynomials, Logic]
The equivalence problem for deterministic two-way sequential transducers is decidable
21st Annual Symposium on Foundations of Computer Science
None
1980
The equivalence problem for deterministic twoway sequential transducers is a long time open problem which is known to be decidable for some restricted cases. Here, the problem is shown to be decidable also for the general case. This even when the devices are allowed to make some finite number of nondeterministic moves.
[Concurrent computing, Computer science, Transducers, Computational modeling, Automata, Writing, Magnetic heads, Counting circuits]
Succinct representation random strings, and complexity classes
21st Annual Symposium on Foundations of Computer Science
None
1980
A general paradigm for relating measures of succinctness of representation and complexity theory is presented. The measures are based on the new Private and Blindfold Alternation machines. These measures are used to indicate the inherent information (or "randomness") of a string, but with respect to time and space complexity classes. These measures are then used to show that the existence of strings which are random with respect to one measure but not to another can show the relationship between the corresponding complexity classes. The basic hierarchy theorems given allow different and possibly more powerful approaches to these problems.
[Computer science, Heart, Automata, Extraterrestrial measurements, Time measurement, Complexity theory, Information theory]
Proofs by induction in equational theories with constructors
21st Annual Symposium on Foundations of Computer Science
None
1980
We show how to prove (and disprove) theorems in the initial algebra of an equational variety by a simple extension of the Knuth-Bendix completion algorithm. This allows us to prove by purely equational reasoning theorems whose proof usually requires induction. We show applications of this method to proofs of programs computing over data structures, and to proofs of algebraic summation identities. This work extends and simplifies recent results of Musser15 and Goguen6.
[Computer science, System testing, Computer languages, Algebra, Induction generators, Optimization methods, Data structures, Logic, Equations, Contracts]
An improved algorithm for computing with equations
21st Annual Symposium on Foundations of Computer Science
None
1980
Implementation of programming language interpreters, proving theorems of the form A=B, and implementation of abstract data types are all problems that can be reduced to the problem of finding a normal form for an expression with respect to a finite set of equation (axiom) schemata The definition of a nonoverlapping s set of axiom schemata is given and the directed congruence closure algorithm is presented; an algorithm that efficiently solves this kind of problem provided the axiom schemata are nonoverlapping. The algorithm is a variation on the congruence closure algorithm? and, like the congruence closure algorithm, it has the advantage of remembering which expressions have already been proved to be equivalent. However, unlike the congruence closure algorithm, which can use only a finite set of axioms, the directed congruence closure algorithm allows a possibly infinite set of axioms generated by a finite set of axiom schemata.
[Computer science, Equations]
Programs and types
21st Annual Symposium on Foundations of Computer Science
None
1980
The first two sections of this paper motivate and outline a constructive theory of (data) types which we developed for formal program verification. The executable component of the theory provides a very high level programming language with a rich type structure. A theory of this generality appears necessary to manage complex programming and formal reasoning about it. The logical component, influencea by AUTOMATH and LCF and based on Martin-L&#x0F6;f's ITT, appears strong enough to formalize constructive mathematics; hence a theory or this generality is probably sufficient for program development and verification. The last two sections of the paper illustrate the richness of the theory and the benefits of generality by describing with it different "denotational" semantics for programs. Because the theory is constructive, these abstract semantics are also computational.
[Computers, Buildings, Programming, Mathematics, Cognition, Complexity theory, Finite element methods, Compounds, History, Indexes, Equations, Computer science, Computer languages, Automata, Production, Titanium, Writing, Set theory, Chapters]
Process logic: Expressiveness, decidability, completeness
21st Annual Symposium on Foundations of Computer Science
None
1980
We define a process logic PL that subsumes Pratt's process logic, Parikh's SOAPL, Nishimura's process logic, and Pnueli's Temporal Logic in expressiveness. The language of PL is an extension of the language of Propositional Dynamic Logic (PDL). We give a deductive system for PL which includes the Segerberg axioms for PDL and prove that it is complete. We also show that PL is decidable.
[Heuristic algorithms, Hydrogen, Laboratories, System recovery, Reasoning about programs, Page description languages, Logic, Vehicle dynamics, Joining processes, Vehicles]
A linear history semantics for distributed languages extended abstract
21st Annual Symposium on Foundations of Computer Science
None
1980
A denotational semantics is given for a distributed language based on communication (CSP). The semantics uses linear sequences of communications to record computations; for any well formed program segment the semantics is a relation between attainable states and the communication sequences needed to attain these states. In binding two or more processes we match and merge the communication sequences assumed by each process to obtain a sequence and State of the combined process. The approach taken here is distinguished by relatively simple semantic domains and ordering.
[Concurrent computing, Computer languages, System recovery, History, Abortion]
The complexity of recursion schemes and recursive programming languages
21st Annual Symposium on Foundations of Computer Science
None
1980
Deterministic exponential lower time bounds are obtained for analyzing monadic recursion schemes, multi-variable recursion schemes, and recursive programs. The lower bound for multivariable recursion schemes holds for any domain of interpretation with at least two elements. The lower bound for recursive programs holds for any recursive programming language with a nontrivial predicate test (i.e. a predicate test that is neither identically true nor identically false). Exponential lower bounds on depth of nesting of recursive function calls play an important role in the proofs of these bounds. In contrast, polynomial upper bounds on depth of nesting are obtained for total and linear monadic recursion schemes. As corollaries, several decision problems for these scheme classes are shown to have nondeterministic polynomially time-bounded algorithms.
[Computer science, Computer languages, Upper bound, Polynomials, Testing]
On the expressive power of attribute grammars
21st Annual Symposium on Foundations of Computer Science
None
1980
We examine the possibility of translating an attribute system into a recursive program scheme taking derivation trees as arguments. This is possible if and only if the attribute system is strongly non-circular. The strong non circularity is decidable in polynomial time. Our recursive program schemes allow us to attack the equivalence problem for attribute systems and solve it in a special case properly including the case of purely synthesized systems.
[Terminology, Optimizing compilers, Formal languages, Polynomials, Labeling, Contracts]
Loop elimination and loop reduction A model-theoretic analysis of programs
21st Annual Symposium on Foundations of Computer Science
None
1980
false
[Mathematics, Fats, Logic]
Complexity of flow analysis, inductive assertion synthesis and a language due to Dijkstra
21st Annual Symposium on Foundations of Computer Science
None
1980
Two different methods of flow analysis are discussed, one a significant generalization of the other. It is shown that the two methods have significantly different intrinsic computational complexities. As an outgrowth of our observations it is shown that a feature of the programming language used by Dijkstra in A Discipline of Programming makes it unsuitable for compile-time type checking, thus suggesting that flow analysis is applicable to the design of programming languages, as well as to their implementation. It is also shown that program verification by the method of inductive assertions is very likely to lead to assertions whose lengths and proofs are not polynomially bounded in the size of the program being verified, even for very simple programs. This last observation casts further doubt on the practicality and relevance of mechanized verification of arbitrary programs.
[Computer science, Computer languages, Flowcharts, Data analysis, Lattices, Optimization methods, Polynomials, NP-complete problem, Computational complexity, Equations]
The inherent complexity of dynamic data structures which accommodate range queries
21st Annual Symposium on Foundations of Computer Science
None
1980
A formal framework is presented in which to explore the complexity issues of data structures which accommodate various types of range queries. Within this framework, a systematic and reasonably tractable method for assessing inherent complexity is developed. Included among the interesting results are the following: the fact that non-linear lower bounds are readily accessible, and the existence of a complexity gap between linear time and n log n time.
[Computational modeling, Data structures]
Efficient uses of the past
21st Annual Symposium on Foundations of Computer Science
None
1980
A failing of existing data structures for maintaining balanced trees is their inability to remember the situation they held at previous times. We propose a structure from which it is possible to efficiently reconstruct the state of the data it represented at any time. Applications of this data structure to a number of important problems in geometric computation are also given.
[Computer science, Tree data structures, Graphics, Tree graphs, Councils, Object detection, Data structures, History, Statistics]
Exploring binary trees and other simple trees
21st Annual Symposium on Foundations of Computer Science
None
1980
The average height of a binary tree With n internal nodes is shown to be asymptotic to 2&#x0221A;&#x03C0;n. More generally, the average height of a tree in a simple family S with n nodes is asymptotic to c(S) &#x0221A;&#x03C0;n where c(S) is a number (usually algebraic) which can be explicitly determined from S. These results are achieved by means of a detailed singularity analysis of corresponding generating functions.
[Algorithm design and analysis, TV, Binary trees, Tin, Performance analysis, Equations, Power generation, Sorting]
A general class of resource tradeoffs
21st Annual Symposium on Foundations of Computer Science
None
1980
In this paper we study a class of resource tradeoffs that arise in such problems as parallel sorting algorithms, linear recursion schemata, VLSI layouts, and searching problems. The tradeoffs can all be traced to the common structure of a multiway tree, and the special class of binomial trees (which are isomorphic to the binomial coefficients) correspond to particularly efficient algorithms. Although all of the tradeoffs that we exhibit are upper bounds, we present evidence to show that the approach can also lead to lower bounds.
[Algorithm design and analysis, Computer science, Upper bound, Sensitivity analysis, Military computing, Very large scale integration, Mathematics, Contracts, Sorting, Electrons]
Some observations on the average behavior of heapsort
21st Annual Symposium on Foundations of Computer Science
None
1980
In this note some aspects of the average behavior of the well known sorting algorithm heapsort are investigated. There are two different methods to construct heaps, which are due to Williams, and to Floyd, and for both methods the respective distribution of the heaps is computed in a continuous model. For Floyd's method moreover the expected number of interchanges, and comparisons which are necessary to construct a heap are derived. For the selection phases of both algorithms the respective distributions are investigated.
[Algorithm design and analysis, Phased arrays, Performance evaluation, Stochastic processes, Data structures, Random variables, Labeling, Distributed computing, Sorting, Information analysis]
Tuning the coalesced hashing method to obtain optimum performance
21st Annual Symposium on Foundations of Computer Science
None
1980
This paper analyzes the coalesced hashing method, in which a portion of memory (called the address region) serves as the range of the hash function while the rest of memory (called the cellar) is devoted solely to storing records that collide when inserted. If the cellar should get full, subsequent colliders must be stored in empty slots in the address region and, thus, may cause later collisions. Varying the relative size of the cellar affects search performance. The main result of this paper expresses the average search times as a function of the number of records and the cellar size, solving the long-standing open problem described in [Knu73, &#x0A7;6.4-43]. We use these formulas to pick the cellar size that leads to optimum search performance and then show that this "tuned" method is competitive with several well-known hashing schemes.
[Computer science, Data structures, Performance analysis, Probes]
Biased 2-3 trees
21st Annual Symposium on Foundations of Computer Science
None
1980
We describe a new data structure for maintaining collections of weighted items. The access time for an item of weight w in a collection of total weight W is proportional to log(W/w) in the worst case (which is optimal in a certain sense), and several other useful operations can be made to work just, as fast. The data structure is simpler than previous proposals, but the running time must be amortized over a sequence of operations to achieve the time bounds.
[Tree data structures, Computer science, Dictionaries, Costs, Binary search trees, Data structures, Proposals, Contracts]
Implicit data structures with fast update
21st Annual Symposium on Foundations of Computer Science
None
1980
Several new data structures for dictionaries are presented that use just one location in addition to those required for key values. The structures are generalizations of a rotated sorted list, with the best realizing a search time of 0(log n) and insert and delete times of 0(n&#x0221A;2/log n(log n)3/2). Similar structures are presented for a dictionary with records containing k&#x226B;1 keys, under the operations of search, partial match, insert and delete.
[Computer science, Dictionaries, Costs, Data structures]
The compilation of regular expressions into integrated circuits
21st Annual Symposium on Foundations of Computer Science
None
1980
We consider the design of integrated circuits to implement arbitrary regular expressions. In general, we may use the McNaughton-Yamada algorithm to convert a regular expression of length n into a nondeterministic finite automaton with at most 2n states and 4n transitions. Instead of converting the nondeterministic device to a deterministic one, we propose two ways of implementing the nondeterministic device directly. First, we could produce a PLA (programmable logic array) of approximate dimensions 4n &#x0D7; 4n by representing the states directly by columns, rather than coding the states in binary. This approach, while theoretically suboptimal, makes use of carefully developed technology and, because of the care with which PLA implementation has been done, may be the preferred technique in many real situations. Another approach is to use the hierarchical structure of the automaton produced from the regular expression to guide a hierarchical layout of the circuit. This method produces a circuit 0(&#x0221A;n) on a side and is, to within a constant factor, the best that can be done in general.
[Space technology, Wires, Automata, Process control, Programmable logic arrays, Signal processing, Circuit synthesis, Integrated circuit modeling, Counting circuits, MOS devices]
Area-efficient graph layouts
21st Annual Symposium on Foundations of Computer Science
None
1980
Minimizing the area of a circuit is an important problem in the domain of Very Large Scale Integration. We use a theoretical VLSI model to reduce this problem to one of laying out a graph, where the transistors and wires of the circuit are identified with the vertices and edges of the graph. We give an algorithm that produces VLSI layouts for classes of graphs that have good separator theorems. We show in particular that any planar graph of n vertices has an O(n lg2 n) area layout and that any tree of n vertices can be laid out in linear area. The algorithm maintains a sparse representation for layouts that is based on the well-known UNION-FIND data structure, and as a result, the running time devoted to bookkeeping is nearly linear.
[Concurrent computing, Costs, Computational modeling, Particle separators, Wires, Integrated circuit interconnections, Binary trees, Very large scale integration, Hardware, Contracts]
A polynomial time algorithm for optimal routing around a rectangle
21st Annual Symposium on Foundations of Computer Science
None
1980
In this paper we present an algorithm for a special case of wire routing. Given a rectangular circuit component on a planar surface with terminals around its boundary, the algorithm finds an optimal set of paths in the plane connecting specified pairs of terminals. The paths are restricted to lie on the outside of the component and must consist of line segments orthogonal to the sides of the component. Paths may intersect at a point but may not overlap. The criterion for optimality is the area of a rectangle with sides orthogonal to those of the component which circumscribes the component and paths. The algorithm has running time O(t3), where t is the number of terminals on the component.
[Algorithm design and analysis, Computer science, Integrated circuit layout, Wires, Laboratories, Integrated circuit interconnections, Logic gates, Routing, Polynomials, Joining processes]
A combinatorial limit to the computing power of V.L.S.I. circuits
21st Annual Symposium on Foundations of Computer Science
None
1980
We introduce a property of boolean functions, called transitivity which holds of integer, polynomial, and matrix products as well as of many interesting related computational problems. We show that the area of any circuit computing a transitive function grows quadratically with the circuit's maximum data-rate, expressed in bit/second. This result provides a precise analytic expression of an area-time tradeoff for a wide class of V.L.S.I. circuits. Furthermore, (as shown elsewhere), this tradeoff is achievable. Thus we have matching (to within a constant multiplicative factor) upper and lower complexity bounds for the three above products, in the V.L.S.I. circuits computational model.
[Boolean functions, Turing machines, Convolution, Computational modeling, Circuits, Wires, Very large scale integration, Polynomials]
On the priority approach to hidden-surface algorithms
21st Annual Symposium on Foundations of Computer Science
None
1980
The task of eliminating invisible parts when generating an image is one of the central problems in computer graphics. One approach achieves the desired obscuring effect by assigning priority numbers to the faces of an object. Image generation can be speeded up significantly in these priority schemes if priority information of the faces can be partly computed in advance, before a viewing position is specified. These observations were first made by Schumacker and are utilized to advantage in simulation applications. However, the priority calculations are usually done manually, for lack of an adequate theoretical understanding. In this paper we study the underlying mathematical structure of these priority orders, and present efficient algorithms for handling some special classes of geometrical configurations.
[Performance evaluation, Computer displays, Computational modeling, Layout, Image generation, Computer graphics, Aerospace simulation, Application software, Paints, Testing]
A linear time algorithm for the lowest common ancestors problem
21st Annual Symposium on Foundations of Computer Science
None
1980
We investigate two lowest common ancestor (LCA) problems on trees. We give a linear time algorithm for the off-line problem, on a random access machine (RAM). The half-line problem is one in which LCA queries on a fixed tree are arriving on line. We extend our RAM algorithm to answer each individual query in 0(1) time, with 0(n) preprocessing time. Tarjan observed that this result helps to explicate the difference in power between RAM and pointer machines. We also show how to modify our algorithm to achieve a linear preprocessing time, optimal query time, algorithm on a reference machine.
[Computer science, Algorithm design and analysis, Tree graphs, Computational modeling, Binary trees, Read-write memory, Mathematics, Indexing]
Parsing for structural editors
21st Annual Symposium on Foundations of Computer Science
None
1980
We present techniques that enable the construction of an algorithm capable of re-parsing a string after another string has been inserted into it. Let M be the minimal, under certain restrictions, number of changes which must be made to the parse tree to reflect the insertions. Then the algorithm we present should work in no more time than M times a log factor of the height of the parse tree. The grammars we allow include all LR(1) grammars.
[Tree data structures, Data structures]
Algebraic dependencies
21st Annual Symposium on Foundations of Computer Science
None
1980
false
[Computer science, Heart, Instruction sets, Laboratories, Relational databases, Logic, Indexes, Power system modeling, Equations, Testing]
Structure and complexity of relational queries
21st Annual Symposium on Foundations of Computer Science
None
1980
This paper is an attempt at laying the foundations for the classification of queries on relational data bases according to their structure and their computational complexity. Using the operations of composition and fixpoints, a &#x03A3;-&#x03A0; hierarchy of height, &#x03C9;2, called the fixpoint query hierarchy, is defined, and its properties investigated. The hierarchy includes most of the queries considered in the literature including those of Codd and Aho and Ullman. The hierarchy to level &#x03C9; characterizes the first-order queries, and the levels up to &#x03C9; are shown to be strict. Sets of queries larger than the fixpoint query hierarchy are obtained by considering the queries computable in polynomial time, queries computable in polynomial space, etc. It is shown that classes of queries defined from such complexity classes behave (with respect to containment) in a manner very similar to the corresponding complexity classes. Also, the set of second-order queries turns out to be the same as the set of queries defined from the polynomialtime hierarchy. Finally, these classes of queries are used to characterize a set of queries defined from language considerations: those expressible in a programming language with only typed (or ranked) relation variables. At the end of the paper is a list of symbols used therein.
[Computer science, Gold, Computer languages, Algebra, Polynomials, Calculus, Database languages, Computational complexity]
On similarity and duality of computation
21st Annual Symposium on Foundations of Computer Science
None
1980
false
[Concurrent computing, Computer science, Turing machines, Computational modeling, Aggregates, Circuit simulation, Computer simulation, Size measurement, Polynomials, Hardware]
Hardware complexity and parallel computation
21st Annual Symposium on Foundations of Computer Science
None
1980
false
[Concurrent computing, Computer science, Costs, Turing machines, Computational modeling, Circuits, Hardware, Polynomials, Magnetic heads, Complexity theory]
A distributed abstract data type implemented by a probabilistic communication scheme
21st Annual Symposium on Foundations of Computer Science
None
1980
From the users' point of view, resource management schemes may be considered as an abstract data type. An abstract specification of such schemes using axioms holding in partial algebras and relatively distributed implementations (expressed as CSP programs) are given and analyzed. Then the idea of probabilistic implementation of guard scheduling is suggested, which allows completely distributed symmetric programs. It frees the designer of an algorithm from looking for specific probabilistic algorithms, by allowing the compiler to generate probabilistic target code from nonprobabilistic source code.
[Computer science, Algorithm design and analysis, Costs, Algebra, Parallel programming, Scheduling, Hardware, Resource management, Distributed algorithms, Parallel algorithms]
A time-luck tradeoff in cryptography
21st Annual Symposium on Foundations of Computer Science
None
1980
New definitions are proposed for the security of Transient-Key Cryptography (a variant on Public-Key Cryptography) that account for the possibility of super-polynomial-time, Monte Carlo cryptanalytic attacks. The basic question we address is: how can one relate the amount of time a cryptanalyst is willing to spend decoding cryptograms to his likelihood of success? This question and others are partially answered in a relativized model of computation in which there provably exists a transient-key cryptosystem such that even a cryptanalyst willing to spend as much as (almost) O(2n/log n) steps on length n cryptograms cannot hope to break but an exponentially small fraction of them, even if he is allowed to make use of a true random bit generator.
[Monte Carlo methods, Art, Computational modeling, Information security, Public key cryptography, Polynomials, Decoding, Computer security, Information theory, Guidelines]
On distinguishing prime numbers from composite numbers
21st Annual Symposium on Foundations of Computer Science
None
1980
A new algorithm for testing primality is presented. The algorithm is distinguishable from the lovely algorithms of Solvay and Strassen [36], Miller [27] and Rabin [32] in that its assertions of primality are certain (i.e., provable from Peano's axioms) rather than dependent on unproven hypothesis (Miller) or probability (Solovay-Strassen, Rabin). An argument is presented which suggests that the algorithm runs within time c1ln(n)c2ln(ln(ln(n))) where n is the input, and C1, c2 constants independent of n. Unfortunately no rigorous proof of this running time is yet available.
[Computer science, Automatic testing, Government, Gaussian processes, Mathematics, History, Lagrangian functions]
N-process synchronization by 4.log2N-valued shared variable
21st Annual Symposium on Foundations of Computer Science
None
1980
The problem of implementing mutual exclusion of N asynchronous parallel processes in a model where the primitive communication mechanism is a test-and-set operation on a shared variable, was the subject of extensive research. While a two-valued variable suffices to insure mutual exclusion, it is shown in [1] that N/2 values are necessary to avoid lockout of any process, and N + 1 values are required to insure bounded waiting time. We introduce the idea of employing randomization in the synchronization protocol and achieve a mutual exclusion, lockout-free, bounded-waiting solution using just 4(log2N+4)-valued shared variable. The protocol is extremely simple, easy to implement, and avoids certain undesirable features present in some of the other solutions.
[Processor scheduling, Access protocols, Mathematics, Mathematical model, Testing]
A recognition algorithm for deterministic CFLs optimal in time and space
21st Annual Symposium on Foundations of Computer Science
None
1980
An algorithm is presented which recognizes arbitrary deterministic CFLs in space O(log2n) and time O(n2/log2n) simultaneously on a deterministic multitape Turing machine. Furthermore, the algorithm provides a general space-time trade-off for deterministic CFLs: the lower bound of the space time product is n2 and our algorithm uses time O(n2/s(n)) for any "acceptable" space functions s(n) between log2n and n. The same methods also give very small upper bounds for DCFL recognition using a fast read-only memory for the input (e.g. space (log n)2 and time n1+&#x03B5; simultaneously for any &#x03B5; &#x226B; o).
[Upper bound, Turing machines, Automata, Polynomials, Personal digital assistants, Counting circuits]
Foreword
22nd Annual Symposium on Foundations of Computer Science
None
1981
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
New lower bound techniques for VLSI
22nd Annual Symposium on Foundations of Computer Science
None
1981
In this paper, we use crossing number and wire area arguments to find lower bounds on the layout area and maximum edge length of a variety of computationally useful networks. In particular, we describe 1) an N-node planar graph which has layout area &#x0398;(NlogN), and maximum edge length &#x0398;(N1/2/log1/2N), 2) an N-node graph with an O(N1/2)-separator which has layout area &#x0398;(Nlog2N) and maximum edge length &#x0398;(N1/2logN/loglogN), and 3) an N-node graph with an O(N1-1/r)-separator which has maximum edge length &#x0398;(N1-1/r) for any r &#x02265; 3.
[Computer science, Upper bound, Particle separators, Laboratories, Network-on-a-chip, Very large scale integration, Wire]
Census functions: An approach to VLSI upper bounds
22nd Annual Symposium on Foundations of Computer Science
None
1981
A model of VLSI computation suitable for the description of algorithms at a high level is introduced. The model is basically a language to express parallel computations which can be efficiently implemented by a VLSI circuit. This language is used to describe area-time efficient algorithms for a few well known graph problems. The exact complexity of these algorithms and their relevance to recent work on the inherent limitations of VLSI computations are also presented.
[Concurrent computing, Jacobian matrices, Upper bound, Processor scheduling, Computational modeling, Microprocessors, Circuits, Very large scale integration, Systolic arrays, Scheduling algorithm]
Optimizing synchronous systems
22nd Annual Symposium on Foundations of Computer Science
None
1981
The complexity of integrated-circuit chips produced today makes it feasible to build inexpensive, special-purpose subsystems that rapidly solve sophisticated problems on behalf of a general-purpose host computer. This paper contributes to the design methodology of efficient VLSI algorithms. We present a transformation that converts synchronous systems into more time-efficient, systolic implementations by removing combinational rippling. The problem of determining the optimized system can be reduced to the graph-theoretic single-destination-shortest-paths problem. More importantly from an engineering standpoint, however, the kinds of rippling that can be removed from a circuit at essentially no cost can be easily characterized. For example, if the only global communication in a system is broadcasting from the host computer, the broadcast can always be replaced by local communication.
[Computer science, Costs, Design methodology, Circuits, Laboratories, Signal processing algorithms, Broadcasting, Signal processing, Logic, Clocks]
On relations between input and communication/computation in VLSI
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Computer science, Algorithm design and analysis, Upper bound, Terminology, Computational modeling, Very large scale integration, Iron]
Two-way counter machines and Diophantine equations
22nd Annual Symposium on Foundations of Computer Science
None
1981
Let Q be the class of deterministic two-way one-counter machines accepting only bounded languages. Each machine in Q has the property that in every accepting computation, the counter makes at most a fixed number of reversals. We show that the emptiness problem for Q is decidable. When the counter is unrestricted or when the machine is provided with two reversal-bounded counters, the emptiness problem becomes undecidable. The decidability of the emptiness problem for Q is useful in proving the solvability of some numbertheoretic problems. It can also be used to prove that the language L = {u1iu2i2|i&#x02265;0} cannot be accepted by any machine in Q (u1 and u2 are distinct symbols). The proof technique is new in that it does not employ the usual "pumping\
[Computer science, Upper bound, Pumps, Equations, Counting circuits]
A time-space tradeoff for language recongnition
22nd Annual Symposium on Foundations of Computer Science
None
1981
We define a language L and show that its time and space complexities T and S must satisfy T2S &#x02265; cn3 even allowing machines with multiple (non random) access to the input.
[Costs, Multidimensional systems, Upper bound, Tiles, Space charge, Magnetic heads, Petroleum, Sorting, Testing]
Simulations among multidimensional turing machines
22nd Annual Symposium on Foundations of Computer Science
None
1981
For all d &#x02265; 1 and all e &#x226B; d, every deterministic multihead e-dimensional Turing machine of time complexity T(n) can be simulated on-line by a deterministic multihead d-dimensional Turing machine in time O(T(n)1+1/d-1/e(log T(n))O(1)). This simulation almost achieves the known lower bound &#x03A9;(T(n)1+1/d-1/e) on the time required. Furthermore, there is a deterministic d-dimensional machine with just two worktape heads that simulates the e-dimensional machine on-line in time O(T(n)1+1/d-1/delog T(n)). These simulations are interpreted in terms of dynamic embeddings among data structures.
[Computer languages, Computer aided instruction, Multidimensional systems, Turing machines, Computational modeling, Binary trees, Data structures, Magnetic heads, Programming profession, Contracts]
On heads versus tapes
22nd Annual Symposium on Foundations of Computer Science
None
1981
2-dimensional 2-tape Turing machines cannot simulate 2-dimensional Turing machines with 2 heads on 1 tape in real time.
[Turing machines, Shape, Instruction sets, Magnetic heads, Encoding]
On the equivalence and containment problems for unambiguous regular expressions, grammars, and automata
22nd Annual Symposium on Foundations of Computer Science
None
1981
The known proofs that the equivalence and containment problems for the regular and for the linear context-free grammars are PSPACE-complete and undecidable, respecitvely, depend upon consideration of ambiguous grammars. We prove that this dependence is inherent. Deterministic polynomial time algorithms are presented for; (1) the equivalence and containment problems for the unambiguous regular grammars; (2) for all k &#x02265; 2, the equivalence and containment problems for the regular grammars of degree of ambiguity &#x02264; k; and (3) the problems of determining if an unambiguous linear context-free grammar is equivalent to or contains an arbitrary regular set. Simple extensions of the grammar classes in (1), (2), and (3) are shown to yield problems that are NP-hard or undecidable. Several new results on the relative economy of description of ambiguous versus unambiguous regular and linear contextfree grammars are also obtained. These results depend upon several observations on the solutions of systems of homogeneous linear difference equations and their relationship with the number of strings of a given length generated by an unambiguous regular or linear context-free grammar.
[Computer science, Gold, Terminology, Difference equations, Automata, Polynomials, Samarium]
On the direct sum conjecture
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Minimization methods, Transmission line matrix methods, Laboratories, Bismuth, Complexity theory, Tellurium]
Computation of algebraic functions with root extractions
22nd Annual Symposium on Foundations of Computer Science
None
1981
We consider the problem of computing a set of algebraic functions that involve extracting roots of various degrees. We show that the complexity of computing a large class of algebraic functions is determined by the Galois group G of the extension generated by the functions. We relate the minimum cost to decomposing G into a sequence of normal subgroups such that each factor group is cyclic. We derive an exact answer for the case when the cost is logarithmic, while we provide upper and lower bounds for all the other cases. On the other hand, we develop a reasonably fast algorithm for the abelian case which has been already solved by Pippenger.
[Computer science, Computer aided instruction, Terminology, Algebra, Cost function, Polynomials, Helium]
An O(N4/3) lower bound on the mono tone network complexity of N-TH degree convolution
22nd Annual Symposium on Foundations of Computer Science
None
1981
We prove an &#x03A9;(n4/3) lower bound for the number of &#x02227;-gates in any monotone network computing n-th degree convolution.
[Boolean functions, Convolution, Computer networks, Complexity theory, MONOS devices]
Non-existence of one-dimensional expanding graphs
22nd Annual Symposium on Foundations of Computer Science
None
1981
Expanding graphs are the basic building blocks used in constructions of graphs with special connectivity properties such as superconcentrators. The only known explicit method (Margulis[7], Gabber and Galil[5]) of constructing arbitrarily large expanding graphs with a linear number of edges, uses graphs whose edges are defined by a finite set of linear mappings restricted to a two-dimensional set, Zn &#x0D7; Zn, where Zn denotes the integers mod n. In this paper we prove that for any finite set of onedimensional linear mappings with rational coefficients, the graph they define by their restriction to Zn is not an expanding graph. We also show that shuffle exchange graphs can not be expanding graphs.
[Computer science, Connectors, Costs, Computational modeling, Switches, Polynomials, Bipartite graph]
A minimum spanning ellipse algorithm
22nd Annual Symposium on Foundations of Computer Science
None
1981
An algorithm to find the minimum spanning ellipse of a convex set of points in the plane, i.e., the ellipse of minimum area containing the set, is described. The result for higher dimensions is suggested, along with a brief discussion of possible applications.
[Algorithm design and analysis, Computer science, Runtime, Object detection, Ellipsoids]
A direct dynamic solution to range search and related problems for product regions
22nd Annual Symposium on Foundations of Computer Science
None
1981
A simple property of number representations yields a unit cross section relation between points and interval representations. Applied to product regions in a vector space, one obtains simple, practical and flexible algorithms for dynamic range search and related queries.
[Computer science, Q measurement, Measurement units, Heuristic algorithms, Data visualization, Dynamic range, Parallel processing, Data structures, Search problems, Mathematics]
Deletion algorithms for hashing that preserve randomness
22nd Annual Symposium on Foundations of Computer Science
None
1981
This paper studies the problem of finding efficient deletion algorithms for the coalesced hashing method, in which a portion of memory (called the address region) serves as the range of the hash function while the rest of memory (called the cellar) is devoted solely to storing records that collide when inserted. We present a deletion algorithm, which solves the open problem described in [Knu73, &#x0A7;6.4-23]. The main result of this paper, Theorem 3, shows that the deletion algorithm preserves randomness for the special case of standard coalesced hashing, in that deleting a record is in some sense like never having inserted it. This means that the formulas for the search times (which are analyzed in [Vit80a] and [Vit80b]) are still valid after deletions. There is as yet no known deletion algorithm that preserves randomness for the general case (when there is a cellar). We give some reasons why and then discuss some heuristics that seem to make deletions practical anyway.
[Computer science]
Implicit data structures for the weighted dictionary problem
22nd Annual Symposium on Foundations of Computer Science
None
1981
Several new data structures are presented for dictionaries containing elements with different weights (access probabilities). The structures use just one location in addition to those required for the values of the elements, and support access times that are within a constant multiplicative factor of optimal, in terms of the rank of the weight of the desired element. Self-organizing heuristics are analyzed, in terms of both weights and "near-ranks" of weights. The benefits of additional space are investigated within the context of self-organization and unsuccessful search.
[Computer science, Dictionaries, Binary search trees, Data structures]
Possible futures, acceptances, refusals, and communicating processes
22nd Annual Symposium on Foundations of Computer Science
None
1981
Two distinct models for the notion of communicating processes are introduced, developed and related. The first, called the possible-futures model, is a generalization to nondeterministic systems of the familiar derivative (Nerode equivalence class) construction. The second, called the acceptance-refusals model, is a slight strengthening of a model introduced by Hoare, Brookes, and Roscoe. The PF model can be mapped onto the AR model homomorphically, and the equivalence classes of this map can be characterized by imposing a very natural equivalence relation on the PF model. The resulting quotient algebra admits a complete partial order structure in which the algebraic operations are continuous.
[Algebra, Formal languages, Automata, Parallel processing, Mathematical model, Labeling, Arm, Equations, Testing]
Symmetry breaking in distributive networks
22nd Annual Symposium on Foundations of Computer Science
None
1981
Given a ring (cycle) of n processes it is required to design the processes so that they will be able to choose a leader (a uniquely designated process) by sending messages along the ring. If the processes are indistiguishable there is no deterministic algorithm, and therefore probabilistic algorithms are proposed. These algorithms need not terminate, but their expected complexity (time or number of bits of communication) is bounded by a function of n. If the processes work asynchronously then on the average O(n log2n) bits are transmitted. In the above cases the size n of the ring was assumed to be known. If n is not known it is suggested first to determine the value of n and then use the above algorithm. However, n may only be determined probabilistically and any algorithm may yield an incorrect value. In addition, it is shown that the size of the ring cannot be calculated by any probabilistic algorithm in which the processes can sense termination.
[Process design, Computer science, Context, Algorithm design and analysis, Intelligent networks, Change detection algorithms, Network topology, Design methodology, Cities and towns, Random number generation]
Unanimity in an unknown and unreliable environment
22nd Annual Symposium on Foundations of Computer Science
None
1981
Can unanimity be achieved in an unknown and unreliable distributed system? We analyze two extreme models of networks: one in which all the routes of communication are known, and the other in which not even the topology of the network is known. We prove that independently of the model, unanimity is achievable if and only if the number of faulty processors in the system is 1. less than one half of the connectivity of the system's network, and 2. less than one third of the total number of processors. In cases where unanimity is achievable, an algorithm to obtain it is given.
[Computer science, Algorithm design and analysis, Pathology, Upper bound, Network topology, Laboratories, Relays]
Symmetry in systems of asynchronous processes
22nd Annual Symposium on Foundations of Computer Science
None
1981
A new solution to the problem of deadlock-free mutual exclusion of N processes is given which uses less shared space than earlier solutions (one variable which may take on N values and N binary variables). The solution uses only indivisible reads and writes of shared variables for communication and is symmetric among the processes. Two definitions of symmetry are developed. The strong definition of symmetry requires that all processes be identically programmed and be started in identical states. However, this definition does not allow any solution to the problem of deadlock-free mutual exclusion using only reads and writes. The weaker definition admits the solution given. It is also shown that under weak symmetry N shared variables, at least one of which must be able to take on N values, are necessary.
[Computer science, System recovery, Writing, Interleaved codes]
A model of concurrent database transactions
22nd Annual Symposium on Foundations of Computer Science
None
1981
When several transactions (processes) read and write items in a database, the question of consistency of the database arises. Consistency is maintained if transactions are serial: all actions of a transaction execute completely before the actions of the next transaction begin. A particular history of interleaved actions belonging to several transactions is correct if it is equivalent to a serial history. We provide a natural framework for studying serializability that encompasses models that have been considered in the literature. A history is represented by a dag in which there is a vertex for each instantaneous value taken by an item in the database. The main tool for determining serializability are "exclusion rules" which determine implied orderings between vertices. For example, a vertex that uses a value must be serialized before the value is overwritten. An exclusion closure -- the result of applying the rules as long as possible -- can be constructed in time cubic in the number of vertices. Since determining serializability is NP-complete, exclusion closures cannot always decide serializability, but useful sufficient conditions can be proven. Exclusion closures allow the largest known classes of polynomially serializable histories to be properly extended. When studying serializability it is not necessary to work solely with the dag containing instantaneous values of all items. Three abstractions of the dag are presented which correspond to models of transactions and to "version graphs" in the literature. Since serializability of histories is known to be NP-complete, subclasses of serializable histories have been studied. One such class consists of histories serializable in a strict sense: transactions that are already in serial in a history must remain in the same relative order. When there are no useless actions in a history, strict serializability can be determined in polynomial time. If useless actions are permitted then strict serializability becomes NP-complete. The above results apply to two step transactions in which there is a read step followed by a write step, Each step involves some subset of the items in the database. With multistep transactions strict serializability is NP-complete even if there are no useless actions.
[Sufficient conditions, Airplanes, Merging, Writing, Polynomials, Encoding, Transaction databases, History]
The complexity of distributed concurrency control
22nd Annual Symposium on Foundations of Computer Science
None
1981
We present a formal framework for distributed databases, and we study the complexity of the concurrency control problem in this framework. Our transactions are partially ordered sets, of actions, as opposed to the straight-line programs of the centralized case. The concurrency control algorithm, or scheduler, is itself a distributed program. Three notions of performance of the scheduler are studied and interrelated: (i) its parallelism, (ii) the computational complexity of the problems it needs to solve, and (iii) the cost of communication between the various parts of the scheduler. We show that the number of messages necessary and sufficient to support a given level of parallelism is equal to the minmax value of a combinatorial game. We show that this game is PSPACE-complete. It follows that, unless NP=PSPACE, a scheduler cannot simultaneously minimize communication and be computationally efficient. This result, we argue, captures the quantum jump in complexity of the transition from centralized to distributed concurrency control problems.
[Concurrent computing, Costs, Laboratories, Distributed databases, Minimax techniques, Distributed control, Concurrency control, Polynomials, Transaction databases, Scheduling algorithm]
Global decision problems for relational databases
22nd Annual Symposium on Foundations of Computer Science
None
1981
Database dependencies are first-order sentences describing the semantics of databases. Decision problems concerning dependencies divide into local problem, such as whether a set of dependencies logically implies another dependency, and global problems, such as whether a set of dependencies is redundant. In this paper we investigate global problems, that of recognizing properties of sets of dependencies. The main result is a negative result in the spirit of Adjan-Markov-Rabin result for global properties of finitely presented semigroups and groups. We show that the decision problem for any property which is wellbehaved in a certain sense (specifically, if it is nice, non-trivial and hereditary) is recursively unsolvable.
[Computer science, Chaos, Relational databases, Spatial databases, Formal specifications, Logic]
Optimizing conjunctive queries when attribute domains are not disjoint
22nd Annual Symposium on Foundations of Computer Science
None
1981
We present polynomial time algorithms for minimizing and testing equivalence of what we call "fan-out free" queries. The fan-out free queries form a more general and more powerful subclass of the conjunctive queries than those previously studied, as they can be used to express questions about transitive properties of databases, questions that are impossible to express if one operates under the "disjoint domain assumption" implicit in previous work. Our algorithms are graph-theoretic in nature, and the equivalence algorithm can be viewed as solving a special case of the graph isomorphism problem (by reducing it to a series of labelled forest isomorphism questions).
[Performance evaluation, Algorithm design and analysis, Tree graphs, Query processing, Relational databases, Cost function, Polynomials, Data models, Testing, Design optimization]
A fast probabilistic parallel sorting algorithm
22nd Annual Symposium on Foundations of Computer Science
None
1981
We describe a probabilistic parallel algorithm to sort n keys drawn from some arbitrary total ordered set. This algorithm can be implemented on a parallel computer consisting of n RAMs, each with small private memory, and a common memory of size O(n) such that the average runtime is bounded by O(log n). Hence for this algorithm the product of time and number of processors meets the information theoretic lower bound for sorting.
[Concurrent computing, Runtime, Computational modeling, Merging, Random access memory, Writing, Decision trees, Parallel algorithms, Sorting, Arithmetic]
The effect of number of Hamiltonian paths on the complexity of a vertex-coloring problem
22nd Annual Symposium on Foundations of Computer Science
None
1981
A generalization of Dobkin and Lipton's element uniqueness problem is introduced: for any fixed undirected graph G on vertex set {v1, v2, ..., vn}, the problem is to determine, given n real numbers x1, x2, ..., xn, whether xi &#x02260; xj for every edge {vi, vj} in G. This problem is shown to have upper and lower bounds of &#x0398;(nlogn) linear comparisons if G is any dense graph. The proof of the lower bound involves showing that any dense graph must contain a subgraph with many Hamiltonian paths, and demonstrating the relevance of these Hamiltonian paths to a geometric argument. In addition, we exhibit relatively sparse graphs for which the same lower bound holds, and relatively dense graphs for which a linear upper bound holds.
[Computer science, Upper bound, RNA, Tellurium, Contracts]
Time-space trade-offs for general recursion
22nd Annual Symposium on Foundations of Computer Science
None
1981
A lower bound for the time-space trade-off of pebble games on PD-Graphs (which represent computations of push-down automata or recursion schemes) is proved, that is only a bit lower than the best known upper bound (the lower and upper time bound is about n &#x000B7; 2 logn/log(s/log n)). The best lower bound known up to now is the bound for linear recursion (about n &#x000B7; log n/log(s/log n) for s &#x226B; log n.
[Upper bound, Turing machines, Tiles, Automata, Personal digital assistants]
Towards separating nondeterministic time from deterministic time
22nd Annual Symposium on Foundations of Computer Science
None
1981
It would be of interest to separate nondeterminism from determinism i.e., to show that for all "nice" functions t(n), NTIME (t(n)), (the class of languages accepted by multitape nondeterministic Turing machines in time O(t(n))) strictly contains DTIME (t(n)) (the class of languages accepted by multitape deterministic Turing machines in time O(t(n)). We establish a weaker form of the statement. We show that there is a universal constant k such that for all "nice" functions t(n), the class of languages that can be accepted simultaneously in deterministic time O(t(n)) and space o((t(n))1/k) is strictly contained in NTIME (t(n)). (We will use the notation SPACE, TIME (s(n),t(n)) to denote the class of languages accepted by a deterministic TM in time O(t(n)) and simultaneously space O(s(n)).) This result is proved using a time-alternation trade-off and several other applications of this trade-off are presented. For example, we show that for each language L in SPACE, TIME (nl-&#x03B5;, ni) (where o&#x226A;&#x03B5;&#x226A;l, &#x03B5;, i constants) there exists a j such that L is accepted by a O(n) time bounded alternating Turing machine with j alternations. The trade-off also leads to the separation &#x0222A;S&#x03B5;St SPACE, TIME (s,t)&#x02282;+&#x03A3;2 TIME(t) where t(n) is any "nice" function and St is a class of "nice" functions in o(t). Here St includes most natural functions for natural t. For example, nj/log*n is in Snj.
[Turing machines, Mathematics, Encoding]
A complexity theory based on Boolean algebra
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Computer science, Boolean functions, Circuits, Packaging, Linear programming, Complexity theory, Boolean algebra, Dynamic programming]
Relativizing time and space
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Computer science, Upper bound, Space technology, Automata, Automatic control, Polynomials, Mathematics, Complexity theory, Books, Helium]
Parity, circuits, and the polynomial-time hierarchy
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Computer science, Turing machines, Logic circuits, Programmable logic arrays, Very large scale integration, Writing, Polynomials, Mathematics, Logic arrays, Testing]
On the number of P-isomorphism classes of NP-complete sets
22nd Annual Symposium on Foundations of Computer Science
None
1981
All known NP-complete sets are P-isomorphic (i.e. there are polynomial time, one-to-one and onto, polynomial time invertible reductions between any two known NP-complete sets) [BH]. If all NP-complete sets are P-isomorphic, then. P &#x02260; NP. However it is not known if the existence of more than one P-isomorphism class of NP-complete sets has implications for the P = NP? problem. In the main result of this paper we prove: Theorem: If there is an NP-complete set that is not P-isomorphic to SAT, then there are infinitely many NP-complete sets that are mutually non-P-isomorphic. Thus, the number of P-isomorphism classes of NP-complete sets is either one or (countably) infinite. Two proof techniques are developed in this paper: we use delayed diagonalization [BCH, L] to construct new sets that are not P-isomorphic to existing sets; the diagonalization conditions are used to defeat P-isomorphisms. We also examine certain properties of 'generic' NP-complete sets and introduce techniques based on padding functions to assure that the sets constructed will be NP-complete. The results on P-isomorphisms and constructing non-P-isomorphic sets apply also to sets complete for PTAPE, EXPTIME, and EXPTAPE and other classes. We also examine the structure of NP-complete sets based on size increasing, invertible reductions, The degrees are P-isomorphism classes [BH]. We show that if there is more than one degree, then there is an &#x03C9; chain of degrees with SAT representing a maximal element.
[Computer science, Hydrogen, Polynomials, Delay]
Number theoretic functions computable by polymorphic programs
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Computer languages, Computational modeling, Polynomials, Tires, Equations, Arithmetic]
The power of parallelism for automatic program synthesis
22nd Annual Symposium on Foundations of Computer Science
None
1981
Inductive inference machines (IIMs) are algorithmic devices which accept as input the graph of a computable function, an ordered pair at a time, and which output a succession of programs each conjectured to compute the input function. IIMs synthesize programs given examples of their intended input-output behavior. Several different criterion for successful synthesis by IIMs are defined. A given criterion is said to be more general than some other criterion if the class of sets which can be inferred by some IIM with respect to the given criteria is larger than the class of sets which can be inferred by some IIM with respect to the other criterion. The tradeoffs between the number of IIMs involved in the learning process and the generality of the criteria of success are examined.
[Concurrent computing, Algorithm design and analysis, Data analysis, Turing machines, Planets, Machine learning, Parallel processing, Inference algorithms, Terrain factors, Robots]
On the relation between descriptional complexity and algorithmic probability
22nd Annual Symposium on Foundations of Computer Science
None
1981
Several results in Algorithmic Information Theory establish upper bounds on the difference between descriptional complexity and the logarithm of "apriori probability". It was conjectured that these two quantities coincide to within an additive constant. Here, we disprove this conjecture and show that the known overall upper bound on the difference is exact. The proof uses a memory-allocation game between two players called User and Server. User sends incremental requests of memory space for certain structured items, Server allocates this space in a write-once memory. For each item, some of the allocated space is required to be in one piece, in order to live a short address. We also present some related results.
[Computer science, Upper bound, Writing, Approximation algorithms, Entropy, Probability distribution, Inference algorithms, Encoding, Information theory, Binary sequences]
A circuit-size lower bound
22nd Annual Symposium on Foundations of Computer Science
None
1981
As remarked in Cook (1980), we do not know any nonlinear lower bound on the circuit size of a language in P or even in NP. The best known lower bound seems to be due to Paul (1975). Instead of trying to prove lower bounds on the circuit-size of a "natural" language, this note raises the question of whether some language in a class is of provably high circuit complexity. We show that for each nonnegative integer k, there is a language Lk in &#x03A3;2P &#x02229; &#x03C0;2P (of the Meyer and Stockmeyer (1972) hierarchy) Which does not have O(nk)-size circuits. The method is indirect and does not produce the language Lk. Other results of a similar nature are presented and several questions raised.
[Circuit simulation, Integrated circuit interconnections, Polynomials, Mathematics, Indium phosphide]
Propositional dynamic logic of context-free programs
22nd Annual Symposium on Foundations of Computer Science
None
1981
The borderline between decidable and undecidable Propositional Dynamic Logic (PDL) is sought when iterative programs represented by regular expressions are augmented with increasingly more complex recursive programs represented by context-free languages. The results in this paper and its companion [HPS] indicate that this line is extremely close to the original regular PDL. The main result of the present paper is: The validity problem for PDL with additional programs &#x03B1;&#x0394;(&#x03B2;)&#x03B3;&#x0394; for regular &#x03B1;, &#x03B2; and &#x03B3;, defined as Ui&#x03B1;i; &#x03B2;; &#x03B3;i, is &#x03A0;11-complete. One of the results of [HPS] shows that the single program A&#x0394;(B) A&#x0394; for atomic A and B is actually sufficient for obtaining &#x03A0;11- completeness. However, the proofs of this paper use different techniques which seem to be worthwhile in their own right.
[Flowcharts, Page description languages, Mathematics, Polynomials, Roentgenium, Logic testing]
The prepositional dynamic logic of deterministic, well-structured programs
22nd Annual Symposium on Foundations of Computer Science
None
1981
We consider a restricted propositional dynamic logic, Strict Deterministic Propositional Dynamic Logic (SDPDL), which is appropriate for reasoning about deterministic well-structured programs. In contrast to PDL, for which the validity problem is known to be complete in deterministic exponential time, the validity problem for SDPDL is shown to be polynomial space complete. We also show that SDPDL is less expressive than PDL. The results rely on structure theorems for models of satisfiable SDPDL formulas, and the proofs give insight into the effects of nondeterminism on intractability and expressiveness in program logics.
[Computer languages, Logic programming, Councils, Laboratories, Binary trees, Reasoning about programs, Polynomials, Page description languages]
Unbounded program memory adds to the expressive power of first-order dynamic logic
22nd Annual Symposium on Foundations of Computer Science
None
1981
The aim of this paper is to-compare various logics of programs with respect to their expressibility. The main result of the paper states that no logic of bounded memory programs is capable of defining the algebra of standard binary trees T = (T, CONS, NIL). Since the usual logics of unbounded memory programs are able to define the above algebra - we derive from the main result a couple of results which solve some questions about comparing expressive powers of programming logics.
[Computer science, Sociotechnical systems, Algebra, Laboratories, Binary trees, Programmable logic arrays, Books]
Temporal logic can be more expressive
22nd Annual Symposium on Foundations of Computer Science
None
1981
We start by proving that some properties of sequences are not expressible in Temporal Logic though they are expressible using for instance regular expressions. Then, we show how Temporal Logic can be extended to express any such property definable by a right-linear grammar and hence a regular expression, Finally, we give a decision procedure and complete axiomatization for the extended Temporal Logic.
[Influenza, Logic]
On the security of public key protocols
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Public key, Access protocols, Decoding, Security, Cryptography]
Worst-case ration for planar graphs and the method of induction on faces
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Greedy algorithms, Computer science, Algorithm design and analysis, Upper bound, Tiles, Laboratories, Graph theory, Gas insulated transmission lines]
Maximum matching in sparse random graphs
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Algorithm design and analysis, Computer science, Heuristic algorithms, Space technology, Laboratories, Stochastic processes, Data structures, Mathematics, Testing]
The complexity of searching a graph
22nd Annual Symposium on Foundations of Computer Science
None
1981
T. Parsons proposed and partially analyzed the following pursuit-evasion problem on graphs: A team of searchers traverse the edges of a graph G in pursuit of a fugitive, who moves along the edges of the graph with complete knowledge of the locations of the pursuers. What is the smallest number s(G) of searchers that will suffice for guaranteeing capture of the fugitive? We show that determining whether s(G) &#x02264; K, for a given integer K, is NP-hard for general graphs but can be solved in linear time for trees. We also provide a structural characterization of those graphs with s(G) &#x02264; K for K = 1,2,3.
[Tree graphs, Combinatorial mathematics, Network address translation]
A complexity calculus for classes of recursive search programs over tree structures
22nd Annual Symposium on Foundations of Computer Science
None
1981
We study a restricted programming language over tree structures. For this language, we give systematic translation rules which map programs into complexity descriptors. The descriptors are in the form of generating functions of average costs. Such a direct approach avoids the recourse to recurrences; it therefore simplifies the task of analyzing algorithms in the class considered and permits analysis of structurally complex programs. It also allows for a clear discussion of analytic properties of complexity descriptors whose singularities are related to the asymptotic behavior of average costs. Algorithms that are analyzed in this way include: formal differentiation, tree matching, tree embedding and simplification of expressions in a diversity of contexts. Some general results relating (average case) complexity properties to structural properties of programs in the class can also be derived in this framework.
[Tree data structures, Algorithm design and analysis, Computer languages, Laboratories, Cost function, Data structures, Calculus, Performance analysis, Equations, Sorting]
Probabilistic algorithms in finite fields
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Algorithm design and analysis, Polynomials, Mathematics, Galois fields, Equations, Arithmetic, Testing]
Applying parallel computation algorithms in the design of serial algorithms
22nd Annual Symposium on Foundations of Computer Science
None
1981
The goal of this paper is to point out that analyses of parallelism in computational problems have practical implications even when multi-processor machines are not available. This is true because, in many cases, a good parallel algorithm for one problem may turn out to be useful for designing an efficient serial algorithm for another problem. A unified framework for cases like this is presented. Particular cases, which are discussed in this paper, provide motivation for examining parallelism in problems like sorting, selection, minimum-spanning-tree, shortest route, maxflow, matrix multiplication, as well as scheduling and locational problems.
[Concurrent computing, Algorithm design and analysis, Processor scheduling, Merging, Parallel processing, Concrete, Parallel algorithms, Statistics, Equations, Sorting]
Irreducibility testing and factorization of polynomials
22nd Annual Symposium on Foundations of Computer Science
None
1981
false
[Computer science, Polynomials, Mathematics, Gamma ray bursts, Testing]
A decidable mu-calculus: Preliminary report
22nd Annual Symposium on Foundations of Computer Science
None
1981
We describe a mu-calculus which amounts to modal logic plus a minimization operator, and show that its satisfiability problem is decidable in exponential time. This result subsumes corresponding results for propositional dynamic logic with test and converse, thus supplying a better setting for those results. It also encompasses similar results for a logic of flowgraphs. This work provides an intimate link between PDL as defined by the Segerberg axioms and the mu-calculi of de Bakker and Park.
[Flowcharts, Boolean functions, Absorption, Filtration, Lattices, Minimization, Logic functions, Transformers, Page description languages, Logic testing]
Foreword
23rd Annual Symposium on Foundations of Computer Science
None
1982
Presents the introductory welcome message from the conference proceedings.
[]
A complexity theory for unbounded fan-in parallelism
23rd Annual Symposium on Foundations of Computer Science
None
1982
A complexity theory for unbounded fan-in parallelism is developed where the complexity measure is the simultaneous measure (number of processors, parallel time). Two models of unbounded fan-in parallelism are (1) parallel random access machines that allow simultaneous reading from or writing to the same common memory location, and (2) circuits containing AND's, OR's and NOT's with no bound placed on the fan-in of gates. It is shown that these models can simulate one another with the number of processors preserved to within a polynomial and parallel time preserved to within a constant factor. Reducibilities that preserve the measure in this sense are defined and several reducibilities and equivalences among problems are given. New upper bounds on the (unbounded fan-in) circuit complexity of symmetric Boolean functions are proved.
[Computer science, Computational modeling, Circuits, Random access memory, Read-write memory, Writing, Parallel processing, Time measurement, Polynomials, Complexity theory]
On the complexity of unique solutions
23rd Annual Symposium on Foundations of Computer Science
None
1982
We show that the problem of deciding whether an instance of the traveling salesman problem has a uniquely optimal solution is complete for &#x0394;2P.
[Costs, Turing machines, Tiles, Traveling salesman problems, Soil, Calculus, Polynomials, Encoding]
Deciding the inequivalence of context-free grammars with 1-letter terminal alphabet is S2p-complete
23rd Annual Symposium on Foundations of Computer Science
None
1982
This paper deals with the complexity of context-free grammars with 1-1etter terminal alphabet. We study the complexity of the membership problem and the inequivalence problem. We show that the first problem is NP-complete and the second one is &#x03A3;2p- complete with respect to log-space reduction. The second result also implies that the inequivalence problem is in PSPACE, solving an open problem stated in [3] by Hunt III, Rosenkrantz and Szymanski.
[Formal languages, Automata, Mathematics, Polynomials, Ice, Complexity theory, Computational complexity, Tellurium]
The computational complexity of simultaneous Diophantine approximation problems
23rd Annual Symposium on Foundations of Computer Science
None
1982
Simultaneous Diophantine approximation in d dimensions deals with the approximation of a vector &#x03B1; = (&#x03B1;1, ..., &#x03B1;d) of d real numbers by vectors of rational numbers all having the same denominator. This paper considers the computational complexity of algorithms to find good simultaneous approximations to a given vector &#x03B1; of d rational numbers. We measure the goodness of an approximation using the sup norm. We show that a result of H. W. Lenstra, Jr. produces polynomial-time algorithms to find sup norm best approximations to a given vector &#x03B1; when the dimension d is fixed. We show that a recent algorithm of A. K. Lenstra, H. W. Lenstra, Jr., and L. Lovasz to find short vectors in an integral lattice can be used to find a good approximation to a given vector &#x03B1; in d dimensions with a denominator Q satisfying 1 &#x02264; Q &#x02264; 2d/2 N which is within a factor &#x0221A;5d 2d+1/2 of the best approximation with denominator Q* with 1 &#x02264; Q* &#x02264; N. This algorithm runs in time polynomial in the input size, independent of the dimension d. We prove results complementing these, showing certain natural simultaneous Diophantine approximation problems are NP-hard. We show that the problem of deciding whether a given vector &#x03B1; of rational numbers has a simultaneous approximation of specified accuracy with respect to the sup norm with denominator Q in a given interval 1 &#x02264; Q &#x02264; N is NP-complete. (Here the dimension d is allowed to vary.) We prove two other complexity results, which suggest that the problem of locating best (sup norm) simultaneous approximations is harder than this NP-complete problem.
[Q measurement, Lattices, Approximation algorithms, Belts, Linear programming, Public key cryptography, Polynomials, NP-complete problem, Computational complexity]
A natural encoding scheme proved probabilistic polynomial complete
23rd Annual Symposium on Foundations of Computer Science
None
1982
We prove a natural encoding scheme intractable (by showing it UR-complete, a technique which may be used when a problem does not yield to a proof of NP-completeness). This is the first non number-theoretic problem that is UR-complete but not known to be NP-complete. We also redefine UR-completeness (henceforth refered to as PR-completeness) in probabilistic terms thus making the notion conceptually simpler. Our result suggests that PR-completeness may be a more widely applicable technique than was previously believed.
[Turing machines, Scholarships, Encoding, Polynomials, NP-complete problem]
Three applications of Kolmogorov-complexity
23rd Annual Symposium on Foundations of Computer Science
None
1982
false
[Algorithm design and analysis, Multidimensional systems, Turing machines, Routing, Sorting]
On-line simulation of k+1 tapes by k tapes requires nonlinear time
23rd Annual Symposium on Foundations of Computer Science
None
1982
On-line simulation of real-time (k+1)-tape Turing machines by k-tape Turing machines requires time n(log n)1/(k+1).
[Turing machines, Computational modeling, Laboratories, Magnetic heads]
A polynomial-time reduction from bivariate to univariate integral polynomial factorization
23rd Annual Symposium on Foundations of Computer Science
None
1982
An algorithm is presented which reduces the problem of finding the irreducible factors of a bivariate polynomial with integer coefficients in polynomial time in the total degree and the coefficient lengths to factoring a univariate integer polynomial. Together with A. Lenstra's, H. Lenstra's and L. Lovasz' polynomial-time factorization algorithm for univariate integer polynomials and the author's multivariate to bivariate reduction the new algorithm implies the following theorem. Factoring a polynomial with a fixed number of variables into irreducibles, except for the constant factors, can be accomplished in time polynomial in the total degree and the size of its coefficients. The new algorithm can be generalized to reducing multivariate factorization directly to univariate factorization and to factoring multivariate polynomials with coefficients in algebraic number fields and finite fields in polynomial time.
[Computer science, Integer linear programming, Polynomials, Galois fields, Equations, Testing]
Fast parallel matrix and GCD computations
23rd Annual Symposium on Foundations of Computer Science
None
1982
We present parallel algorithms to compute the determinant and characteristic polynomial of n&#x0D7;n-matrices and the gcd of polynomials of degree &#x02264;n. The algorithms use parallel time O(log2n) and a polynomial number of processors. We also give a fast parallel Las Vegas algorithm for the rank of matrices. All algorithms work over arbitrary fields.
[Concurrent computing, Monte Carlo methods, Parallel programming, Phase change random access memory, Polynomials, Vectors, Parallel algorithms, Galois fields, Equations, Testing]
Theory and application of trapdoor functions
23rd Annual Symposium on Foundations of Computer Science
None
1982
The purpose of this paper is to introduce a new information theory and explore its appplications. Using modern computational complexity, we study the notion of information that can be accessed through a feasible computation. In Part 1 of this paper, we lay the foundation of the theory and set up a framework for cryptography and pseudorandom number generation. In Part 2, we study the concept of trapdoor functions and examine applications of such functions in cryptography, pseudorandom number generation, and abstract complexity theory.
[Computer science, Modems, Extraterrestrial measurements, Entropy, Complexity theory, Cryptography, Data mining, Computational complexity, Information theory]
An application of number theory to the organization of raster-graphics memory
23rd Annual Symposium on Foundations of Computer Science
None
1982
A high-resolution raster-graphics display is usually combined with processing power and a memory organization that facilitates basic graphics operations. For many applications, including interactive text processing, the ability to quickly move or copy small rectangles of pixels is essential. This paper proposes a novel organization of raster-graphics memory that permits all small rectangles to be moved efficiently. The memory organization is based on a doubly periodic assignment of pixels to M memory chips according to a "Fibonacci" lattice. The memory organization guarantees that if a rectilinearly oriented rectangle contains fewer than M/&#x0221A;5 pixels, then all pixels will reside in different memory chips, and thus can be accessed simultaneously. We also define a continuous amdogue of the problem which can be posed as, "What is the maximum density of a set of points in the plane such that no two points are contained in the interior of a rectilinearly oriented rectangle of area N." We give a lower bound of 1/2N on the density of such a set, and show that 1/&#x0221A;5N can be achieved.
[Computer science, Computer displays, Lattices, Large screen displays, Computer graphics, Hardware, Application software, Artificial intelligence, Hip, Text processing]
An application of higher reciprocity to computational number theory
23rd Annual Symposium on Foundations of Computer Science
None
1982
The Higher Reciprocity Laws are considered to be among the deepest and most fundamental results in number theory. Yet, they have until recently played no part in number theoretic algorithms. In this paper we explore the power of the laws in algorithms. The problem we consider is part of a group of well-studied problems about roots in finite fields and rings. Let F denote a finite field, let m denote a direct product of finite fields. Consider the following problems: Problem 1. Is Xn = a solvable in F; Problem 2. If Xn = a is solvable in F find X; Problem 3. Is Xn = a solvable in m; Problem 4. If Xn = a solvable in m find X.
[Upper bound, Turing machines, Gaussian processes, Public key cryptography, Polynomials, Galois fields, Artificial intelligence]
Probabilistic analysis of some bin-packing problems
23rd Annual Symposium on Foundations of Computer Science
None
1982
We analyze the average-case behaviour of the Next-Fit algorithm for bin-packing, and obtain closed-form expressions for distributions of interest. Our analysis is based on a novel technique of partitioning the interval (0, 1) suitably and then formulating the problem as a matrix-differential equation. We compare our analytic results with previously known simulation results and show that there is an excellent agreement between the two. We also explain a certain empirically observed anomaly in the behaviour of the algorithm. Finally we establish that asymptotically perfect packing is possible when input items are drawn from a monotonically decreasing density function.
[Algorithm design and analysis, Computer science, Analytical models, Differential equations, Approximation algorithms, Density functional theory, Closed-form solution, Probability distribution, Partitioning algorithms, Steady-state]
How to generate cryptographically strong sequences of pseudo random bits
23rd Annual Symposium on Foundations of Computer Science
None
1982
false
[Computational modeling, Circuits, Length measurement, Frequency, Polynomials, Complexity theory, Cryptography, Spinning, Random number generation]
An O(n3 log n) deterministic and an O (n 3) probabilistic isomorphism test for trivalent graphs
23rd Annual Symposium on Foundations of Computer Science
None
1982
The main results of this paper are an O(n3) probabilistic algorithm and an O(n3 log n) deterministic algorithm that test whether two given trivalent graphs are isomorphic. In fact, the algorithms construct the set of all isomorphisms of the two graphs. Variants of these algorithms construct the set of all automorphisms of a trivalent graph. The algorithms make use of some new improved permutation group algorithms that exploit the fact that the groups involved are 2-groups. A remarkable property of the probabilistic algorithm is that it computes Isoe,ei(X,Y), i = 1,...,m, m = O(n) (the set of all isomorhisms &#x03C6;: X &#x02192; Y with &#x03C6;(e)=ei) for the cost of computing the single set Isoe,el(X,Y).
[Computer science, Algorithm design and analysis, Costs, Polynomials, Mathematics, Tellurium, Testing]
A compact representation for permutation groups
23rd Annual Symposium on Foundations of Computer Science
None
1982
An O(n2) space representation for permutation groups of degree n is presented. The representation can be constructed in time O(n5), and supports fast membership testing. Applications of the representation to the generation of systems of coset representatives, and of complete block systems, are discussed.
[Computer science, Performance evaluation, Glands, Computer applications, Polynomials, Testing]
Why and how to establish a private code on a public network
23rd Annual Symposium on Foundations of Computer Science
None
1982
The Diffie and Hellman model of a Public Key Cryptosystem has received much attention as a way to provide secure network communication. In this paper, we show that the original Diffie and Hellman model does not guarantee security against other users in the system. It is shown how users, which are more powerful adversarys than the traditionally considered passive eavesdroppers, can decrypt other users messages, in implementations of Public Key Cryptosystem using the RSA function, the Rabin function and the Goldwasser&#x00026;Micali scheme. This weakness depends on the bit security of the encryption function. For the RSA (Rabin) function we show that computing, from the cyphertext, specific bits of the cleartext, is polynomially equivalent to inverting the function (factoring). As for many message spaces, this bit can be easily found out by communicating, the system is insecure. We present a modification of the Diffie and Hellman model of a Public-Key Cryptosystem, and one concrete implementation of the modified model. For this implementation, the difficulty of extracting partial information about clear text messages from their encoding, by eavesdroppers, users or by Chosen Cyphertext Attacks is proved equivalent to the computational difficulty of factoring. Such equivalence proof holds in a very strong probabilistic sense and for any message space. No additional assumptions, such as the existence of a perfect signature scheme, or a trusted authentication center, are made.
[Shafts, Law, Public key, Public key cryptography, Polynomials, Decoding, Data mining, Power system security, Power system modeling, Legal factors]
A polynomial time algorithm for breaking the basic Merkle-Hellman cryptosystem
23rd Annual Symposium on Foundations of Computer Science
None
1982
The cryptographic security of the Merkle-Hellman cryptosystem has been a major open problem since 1976. In this paper we show that the basic variant of this cryptosystem, in which the elements of the public key are modular multiples of a superincreasing sequence, is breakable in polynomial time.
[Greedy algorithms, Public key, Communication channels, H infinity control, Public key cryptography, Polynomials, Mathematics, Performance analysis, Security, Protection]
Inferring a sequence generated by a linear congruence
23rd Annual Symposium on Foundations of Computer Science
None
1982
Suppose it is known that {X<sub>0</sub>, X<sub>1</sub>,...,X<sub>n</sub>} is produced by a pseudo-random number generator of the form X<sub>i+1</sub> = aX<sub>i</sub> + b mod m, but a, b, and m are unknown. Can one efficiently predict the remainder of the sequence with knowledge of only a few elements from that sequence? This question is answered in the affirmative and an algorithm is given.
[Computer science, Algorithm design and analysis, Data analysis, Tiles, Sections, Inference algorithms, Error correction, Cryptography, Random number generation, Power generation]
Protocols for secure computations
23rd Annual Symposium on Foundations of Computer Science
None
1982
The author investigates the following problem: Suppose m people wish to compute the value of a function f(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>m</sub>), which is an integer-valued function of m integer variables xi of bounded range. Assume initially person Pi knows the value of xi and no other x's. Is it possible for them to compute the value of f, by communicating among themselves, without unduly giving away any information about the values of their own variables? The author gives a precise formulation of this general problem and describe three ways of solving it by use of one-way functions (i.e., functions which are easy to evaluate but hard to invert). These results have applications to secret voting, private querying of database, oblivious negotiation, playing mental poker, etc.. He also discusses the complexity question "How many bits need to be exchanged for the computation," and describes methods to prevent participants from cheating. Finally, he studies the question "What cannot be accomplished with one-way functions."
[Algorithm design and analysis, Privacy, Protocols, Databases, Voting, Stochastic processes, Telephony, Security, Cryptography]
Storing a sparse table with O(1) worst case access time
23rd Annual Symposium on Foundations of Computer Science
None
1982
We describe a data structure for representing a set of n items from a universe of m items, which uses space n+o(n) and accommodates membership queries in constant time. Both the data structure and the query algorithm are easy to implement.
[Computer aided software engineering, Data structures, Size measurement, Mathematics, Polynomials, Time measurement, Probes]
On the program size of perfect and universal hash functions
23rd Annual Symposium on Foundations of Computer Science
None
1982
We address the question of program size of of perfect and universal hash functions. We prove matching upper and lower bounds (up to constant factors) on program size. Furthermore, we show that minimum or nearly minimum size programs can be found efficiently. In addition, these (near) minimum size programs have time complexity at most O(log* N) where N is the size of the universe in the case of perfect hashing, and time complexity 0(1) in the case of universal hashing. Thus for universal hashing programs of minimal size and minimal time complexity have been found.
[Computer languages, Dictionaries, Measurement units, Upper bound, Length measurement, Cost function, Time measurement]
On decomposition of relational databases
23rd Annual Symposium on Foundations of Computer Science
None
1982
A central issue in relational database theory is that of decomposition. It has been agreed that decompositions should be injective, so as not to lose information, and surjective, so they decompose a relation into independent components. Injectiveness and surjectiveness are in general second-order notions. We show here how to express these notions in a first-order manner, assuming that we are dealing only with first-order constraints. As a consequence we get that the reconstruction map, which is the inverse to the decomposition map, is also first-order, but is not necessarily the natural join. This result is derived by applying Beth's Definability Theorem from model theory. For the case that the constraints used are implicational dependencies, we derive the exact syntactic form of the reconstruction map, and show that if the decomposition map is both injective and surjective then the reconstruction map is the natural join.
[Computer science, Sufficient conditions, Law, Relational databases, Legal factors]
Generic transformation of data structures
23rd Annual Symposium on Foundations of Computer Science
None
1982
We consider the notion of a (data) format where each format defines a family of data structures. These formats arose from the theory of databases. Previous works have investigated the notion of generic transformations of data structures between formats. We give a novel grouptheoretic view of genericity which unifies the original approaches of Hull-Yap and Aho-Ullman. Among the results are: A necessary and sufficient condition for the existence of generic embeddings; the fact that digraphs cannot be generically embedded in hypergraphs; the striking fact that there is no hypergraph on more than two vertices with the alternating group as its automorphism group, and combinatorial techniques for counting structures with a prescribed automorphism group.
[Sufficient conditions, Computer languages, Relational databases, Data structures, Spatial databases, Encoding, Computational efficiency]
'Eventual' is earlier than 'immediate'
23rd Annual Symposium on Foundations of Computer Science
None
1982
Two different notions of Byzantine Agreement - immediate and eventually - are defined depending on whether the agreement involves an action to be performed synchronously or not. The lower bounds for time complexity depend on what kind of agreement has to be achieved. All previous algorithms to reach Byzantine Agreement ensure immediate agreement. We present two algorithms that in many cases reach the second type of agreement faster than previously known algorithms showing that there actually is a difference between the two notions: Eventual Byzantine Agreement can be reached earlier than Immediate.
[Laboratories, Authentication, Polynomials, Stress]
Deterministic process logic is elementary
23rd Annual Symposium on Foundations of Computer Science
None
1982
Process Logic (PL) is a language for reasoning about the behavior of a program during a computation, while Propositional Dynamic Logic (PDL) can only reason about the input-output states of a program. Nevertheless, we show that to each PL model M there corresponds in a natural way a PDL, model Mt such that every path in M is represented by a state in Mt. Moreover, to every PL formula p there corresponds a PDL formula pt, whose length is linear in that of p, such that p is true of a path in M iff pt is true of the state which represents that path in Mt. We then show that p is satisfiable iff pt is satisfiable in a finite PDL model with special properties which we call a pseudomodel. The size of the pseudomodel is in general nonelementary, and depends on the depth of nesting of the suf operator in PL. However, for PDL, a deterministic version of PL, the pseudomodel has size 2|p|2, giving us a decision procedure for PDL which runs in deterministic time O(2cn2). These results suggest that it is the interaction between nondeterministic programs and thc suf operator that makes the general decision problem for PL so difficult.
[Councils, Laboratories, Logic]
A temporal logic to deal with fairness in transition systems
23rd Annual Symposium on Foundations of Computer Science
None
1982
In this paper, we propose a notion of fairness for transition systems and a logic for proving properties under the fairness assumption corresponding to this notion. We consider that the concept of fairness which is useful is "fair reachability" of a given set of states P in a system, i.e. reachability of states of P when considering only the computations such that if, during their execution, reaching states of P is possible infinitely often, then states of P are visited infinitely often. This definition of fairness suggests the introduction of a branching time logic FCL, the temporal operators of which express, for a given set of states P, the modalities "it is possible that P" and "it is inevitable that P" by considering fair reachability of P. The main result is that, given a transition system S and a formula f of FCL expressing some property of S under the assumption of fairness, there exists a formula f&#x02032; belonging to a branching time logic CL such that : f is valid for S in FCL iff f&#x02032; is valid for S in CL. This result shows that proving a property under the assumption of fairness is equivalent to proving some other property without this assumption and that the study of FCL can be made via the "unfair" logic CL, easier to study and for which several results already exist.
[Processor scheduling, Computational modeling, Logic]
On equations including string variables
23rd Annual Symposium on Foundations of Computer Science
None
1982
S-equations are of the form E1(x1,..., xk) &#x02287; E2 (X1,...,xk) where E1 and E2 are shuffle expressions having two types of symbols; variables and constants. E1&#x02287;E2 is said to be S-satisfiable if the language expressed by E1(&#x03B1;1,...,&#x03B1;k) includes the language expressed by E2(&#x03B1;1,...,&#x03B1;k) where &#x03B1;1,...,&#x03B1;k are some strings of constants. A wide range of problems in string manipulation, data bases, etc., can be described in terms of S-equations. Major results include the solvability and complexity of several classes of S-satisfiability problems.
[Data communication, Equations, Pattern matching]
Substitution of bounded rational cone
23rd Annual Symposium on Foundations of Computer Science
None
1982
We study the family S of rational cones obtained by iterated substitutions from rational cones L1, .., Ln. This family is a semi-group and to every non empty word u defined on the alphabet {L1, ..., Ln}, corresponds a rational cone U of S. We give sufficient conditions for S to be free (U = U&#x02032; implies u = u&#x02032;) and to verify the subpattern property (U &#x02282; U&#x02032; implies u is a subpattern of u&#x02032;). We study, more particularly, the case where L1, ..., Ln are bounded rational cones.
[Sufficient conditions, Ferroelectric films, Nonvolatile memory, Random access memory]
Parallel algorithms for minimum cuts and maximum flows in planar networks
23rd Annual Symposium on Foundations of Computer Science
None
1982
Algorithms are given that compute maximum flows in planar directed networks either in O((logn)3) parallel time using O(n4) processors or O((logn)2) parallel time using O(n6) processors. The resource consumption of these algorithms is dominated by the cost of finding the value of a maximum flow. When such a value is given, or when the computation is on an undirected network, the bound is O((logn)2) time using O(n4) processors. No efficient parallel algorithm is known for the maximum flow problem in general networks.
[Concurrent computing, Computer science, Shortest path problem, Intelligent networks, Costs, Read-write memory, Parallel processing, Computer networks, Parallel algorithms, Testing]
Priority queues with variable priority and an O(EV log V) algorithm for finding a maximal weighted matching in general graphs
23rd Annual Symposium on Foundations of Computer Science
None
1982
We define two generalized types of a priority queue by allowing some forms of changing the priorities of the elements in the queue. We show that they can be implemented efficiently. Consequently, each operation takes O(log n) time. We use these generalized priority queues to construct an O(EV log V) algorithm for finding a maximal weighted matching in general graphs.
[Computer science, RNA, Data structures]
Polynomial time algorithms for the MIN CUT problem on degree restricted trees
23rd Annual Symposium on Foundations of Computer Science
None
1982
Polynomial algorithms are described that solve the MIN CUT LINEAR ARRANGEMENT problem on degree restricted trees. For example, the cutwidth or folding number of an arbitrary degree d tree can be found in O(n(logn)d-2) steps. This also yields an algorithm for determining the black/white pebble demand of degree three trees. A forbidden subgraph characterization is given for degree three trees having cutwidth k. This yields an interesting corollary: for degree three trees, cutwidth is identical to search number.
[Computer science, Tree graphs, Binary trees, Bandwidth, Approximation algorithms, Polynomials]
Using clerk in parallel processing
23rd Annual Symposium on Foundations of Computer Science
None
1982
Some models of parallel computation consist of copies of a single finite automaton, connected together in a regular fashion. In such computers clerks can be a useful data structure, enabling one to simulate a more powerful computer for which optimal algorithms are easier to design. Clerks are used here to give optimal algorithms for the 3-dimensional connected is problem on a parallel processing array, and a circle construction problem on a pyramid cellular automaton.
[Computer science, Algorithm design and analysis, Concurrent computing, Computational modeling, Automata, Random access memory, Parallel processing, Read-write memory, Data structures, Principal component analysis]
On the movement of robot arms in 2-dimensional bounded regions
23rd Annual Symposium on Foundations of Computer Science
None
1982
The classical mover's problem is the following: can a rigid object in 3-dimensional space be moved from one given position to another while avoiding obstacles? It is known that a more general version of this problem involving objects with movable joints is PSPACE-complete, even for a simple tree-like structure. In this paper, we investigate a 2-dimensional mover's problem in which the object being moved is a robot arm with an arbitrary number of joints. We reduce the mover's problem for arms constrained to move within bounded regions whose boundaries are made up of straight lines to the mover's problem for a more complex linkage that is not constrained. We prove that the latter problem is PSPACE-hard even in 2-dimensional space and then turn to special cases of the mover's problem for arms. In particular, we give a polynomial time algorithm for moving an arm confined within a circle from one given configuration to another. We also give a polynomial time algorithm for moving the arm from its initial position to a position in which the end of the arm reaches a given point within the circle.
[Computer science, Couplings, Manipulators, Educational institutions, Robot sensing systems, Polynomials, Mathematics, Arm, Contracts, Orbital robotics]
Parallel time O(log N) acceptance of deterministic CFLs
23rd Annual Symposium on Foundations of Computer Science
None
1982
We give a parallel RAM algorithm for simulating a deterministic auxiliary pushdown machine. If the pushdown machine uses space s(n) &#x02265; log n and time 2O(s(n)) then our parallel simulation algorithm takes time O(s(n)) and requires 2<sup,O(s(n)) processors. Thus any deterministic context free language is accepted in time O(log n) by our parallel RAM algorithm using a polynomial number of processors. (Our algorithm can easily be extended to also accept the LR(k) languages in time O(log n) and 2O(k) Processors. Our simulation algorithm is near optimal for parallel RAMs, since we show that the language accepted in time T(n) by a parallel RAM is accepted by a deterministic auxiliary pushdown machine with space T(n) and time 2O(T(n)2).
[Concurrent computing, Turing machines, Computational modeling, Laboratories, Random access memory, Read-write memory, Polynomials, Magnetic heads, Tellurium, Context modeling]
Wafer-scale integration of systolic arrays
23rd Annual Symposium on Foundations of Computer Science
None
1982
This paper describes and analyzes several algorithms for constructing systolic array networks from cells on a silicon wafer. Some of the cells may be defective, and thus the networks must be configured to avoid them. We adopt a probabilistic model of cell failure, and attempt to construct networks whose maximum wire length is minimal Although the algorithms presented are designed principally for application to the wafer-scale integration of one and two-dimensional systolic arrays, they can also be used to construct networks in well studied models of geometric complexity. Some of the algorithms are of considerable practical interest.
[Semiconductor device modeling, Assembly systems, Laboratories, Very large scale integration, Systolic arrays, Silicon, Wafer scale integration, Wire, Contracts, Nearest neighbor searches]
An efficient approximation scheme for the one-dimensional bin-packing problem
23rd Annual Symposium on Foundations of Computer Science
None
1982
We present several polynomial-time approximation algorithms for the one-dimensional bin-packing problem. using a subroutine to solve a certain linear programming relaxation of the problem. Our main results are as follows: There is a polynomial-time algorithm A such that A(I) &#x02264; OPT(I) + O(log2 OPT(I)). There is a polynomial-time algorithm A such that, if m(I) denotes the number of distinct sizes of pieces occurring in instance I, then A(I) &#x02264; OPT(I) + O(log2 m(I)). There is an approximation scheme which accepts as input an instance I and a positive real number &#x03B5;, and produces as output a packing using as most (1 + &#x03B5;) OPT(I) + O(&#x03B5;-2) bins. Its execution time is O(&#x03B5;-c n log n), where c is a constant. These are the best asymptotic performance bounds that have been achieved to date for polynomial-time bin-packing. Each of our algorithms makes at most O(log n) calls on the LP relaxation subroutine and takes at most O(n log n) time for other operations. The LP relaxation of bin packing was solved efficiently in practice by Gilmore and Gomory. We prove its membership in P, despite the fact that it has an astronomically large number of variables.
[Pulp and paper industry, Costs, Optimized production technology, Linear programming, Approximation algorithms, Polynomials, Ellipsoids]
The ellipsoid algorithm for linear inequalities in exact arithmetic
23rd Annual Symposium on Foundations of Computer Science
None
1982
A modification of the ellipsoid algorithm is shown to be capable of testing for satisfiability a system of linear equations and inequalities with integer coefficients of the form Ax = b, x &#x02265; 0. All the rational arithmetic is performed exactly, without losing polynomiality of the computing time. In case of satisfiability, the approach always provides a rational feasible point. The bulk of the computations consists of a sequence of linear least squares problems, each a rank one modification of the preceding one. The continued fractions jump is used to compute some of the coordinates of a feasible point.
[Algorithm design and analysis, System testing, Numerical analysis, Bibliographies, Polynomials, Least squares approximation, Ellipsoids, Equations, Least squares methods, Arithmetic]
An old linear programming algorithm runs in polynomial time
23rd Annual Symposium on Foundations of Computer Science
None
1982
The Ellipsoid Algorithm (EA) for linear programming attracted recently great attention. EA was proposed in [N76] and developed in [K79, G81] and other works. It is a modification of Method of Centralized Splitting presented in [L65], which differs from EA in two essential respects. Firstly, [L65] uses simplexes instead of ellipsoids; it is admitted, secondly, that, several (q(n))splittings of the n-dimensional simplex may be needed before the remaining polyhedron can be enclosed into a simplex of a smaller volume. Only a very rough upper bound q(n) &#x226A; nlog(n)follows from the reasoning of [L65]. This does not imply polynomiality of the computation time, since n, log(n) splittings may make the simplex very complex. We prove below that, q(n)= 1. Let the problem be to find x&#x02208;Rn such that Ax &#x226B; 0, where A is an m &#x0D7; n matrix of rank n. We normalize solutions by a restriction (e - Ax) = 1 where e &#x226B; 0. On every step the algorithm considers a simplex BAx &#x02265; 0 containing all solutions, where B is a non-negative n &#x0D7; m matrix with det(BA) &#x02260; 0. Let us denote this simplex by &#x0394;B, its volume by VB and its center by CB. Initially we take an arbitrary B and e = BT(1,..,1).
[Stability, Linear programming, Polynomials, Reflection, Ellipsoids, Equations]
Linear-time algorithms for linear programming in R3 and related problems
23rd Annual Symposium on Foundations of Computer Science
None
1982
Linear-time for Linear Programming in R2 and R3 are presented. The methods used are applicable for some other problems. For example, a linear-time algorithm is given for the classical problem of finding the smallest circle enclosing n given points in the plane. This disproves a conjecture by Shamos and Hoey that this problem requires &#x03A9;(n log n) time. An immediate consequence of the main result is that the problem of linear separability is solvable in linear-time. This corrects an error in Shamos and Hoey's paper, namely, that their O(n log n) algorithm for this problem in the plane was optimal. Also, a linear-time algorithm is given for the problem of finding the weighted center of a tree and algorithms for other common location-theoretic problems are indicated. The results apply also to the problem of convex quadratic programming in three-dimensions. The results have already been extended to higher dimensions and we know that linear programming can be solved in linear-time when the dimension is fixed. This will be reported elsewhere; a preliminary report is available from the author.
[Algorithm design and analysis, Computational geometry, Linear programming, Error correction, Quadratic programming]
A theorem on polygon cutting with applications
23rd Annual Symposium on Foundations of Computer Science
None
1982
Let P be a simple polygon with N vertices, each being assigned a weight &#x02208; {0,1}, and let C, the weight of P, be the added weight of all vertices. We prove that it is possible, in O(N) time, to find two vertices a,b in P, such that the segment ab lies entirely inside the polygon P and partitions it into two polygons, each with a weight not exceeding 2C/3. This computation assumes that all the vertices have been sorted along some axis, which can be done in O(Nlog N) time. We use this result to derive a number of efficient divide-and-conquer algorithms for: 1. Triangulating an N-gon in O(Nlog N) time. 2. Decomposing an N-gon into (few) convex pieces in O(Nlog N) time. 3. Given an O(Nlog N) preprocessing, computing the shortest distance between two arbitrary points inside an N-gon (i.e., the internal distance), in O(N) time. 4. Computing the longest internal path in an N-gon in O(N2) time. In all cases, the algorithms achieve significant improvements over previously known methods, either by displaying better performance or by gaining in simplicity. In particular, the best algorithms for Problems 2,3,4, known so far, performed respectively in O(N2), O(N2), and O(N4) time.
[Computer science, Particle separators, Partitioning algorithms, Application software, Contracts, Clocks]
Three layers are enough
23rd Annual Symposium on Foundations of Computer Science
None
1982
In this paper we show that any channel routing problem of density d involving two-terminal nets can always be solved in the knock-knee mode in a channel of width equal the density d with three conducting layers. An algorithm is described which produces a layout of n nets with the following properties: (i) it has minimal width d; (ii) it can be realized with three layers; (iii) it has at most 3n vias; (iv) any two wires share at most four grid points.
[Wiring, Computer science, Terminology, Circuits, Very large scale integration, Routing, Wire, Joining processes]
The complexity of compacting hierarchically specified layouts of integrated circuits
23rd Annual Symposium on Foundations of Computer Science
None
1982
In many CAD systems for VLSI design the specification of a layout is internally represented by a set of geometric constraints that take the form of linear inequalities between pairs of layout components. Some of the constraints may be explicitly stated by the circuit designer. Others are internally generated by the CAD system, using the design rules of the fabrication process. Layout compaction is then equivalent to finding a minimum area layout satisfying all constraints. We discuss the complexity of the constraint resolution problem arising in this context. Hereby we allow circuits to be specified hierarchically. The complexity of the constraint resolution is then measured in terms of the length of the hierarchical specification. We show the following results: 1. It is decidable in polynomial (cubic) time whether a given hierarchical layout specification yields a consistent set of geometric constraints. The size of minimum area layouts satisfying the constraints can also be determined in cubic time. 2. For every layout specification that is consistent a hierarchical description L of a minimum area layout can be computed in polynomial time in the length of L. 3. There is a consistent layout specification with the following property: No layout satisfying the constraints is concise, i.e., every hierarchical layout description consistent with the specification has a length which grows exponentially in the length of the specification. 4. We define a subclass of so-called well-formed layout specifications. Each well-formed specification has a concise layout, which can be hierarchically described in linear space. Such a layout can be found in polynomial time. However, it is in general not a minimum area layout. Indeed, there is a consistent well-formed specification all of whose minimum area layouts are inconcise,.i.e., need exponential space to be described.
[Fabrication, Integrated circuit layout, Design automation, Wires, Very large scale integration, Length measurement, Polynomials, Silicon, Compaction, Zinc]
On driving many long lines in a VLSI layout
23rd Annual Symposium on Foundations of Computer Science
None
1982
We assume that long wires represent large capacitive loads, and investigate the effect on the area of a VLSI layout when drivers are introduced along many long wires in the layout. We present a layout for which the introduction of drivers along long wires squares the area of the layout; we show, however, that the increase in area is never greater than this, if the driver can be laid out in a square region. We also show an area-time trade-off for a single long wire by which we can reduce the area of its driver to &#x0398;(lq), q &#x226A; 1, from &#x0398;(l), if we can tolerate a delay of &#x0398;(l1-q) rather than &#x0398;(log l); and we obtain tight bounds on the worst-case area increase in general lay-outs having these drivers, using the Brouwer fixed-point theorem. We also derive results for the case when drivers are embedded in rectangles that are not square. Finally, we extend the use of our upper-bound technique to other layout, problems.
[Wires, Very large scale integration, Delay lines, Predictive models, Repeaters, Capacitance, Joining processes, Equations, Driver circuits, Propagation delay]
Optimal allocation of computational resources in VLSI
23rd Annual Symposium on Foundations of Computer Science
None
1982
false
[Particle separators, Circuits, Very large scale integration, Resource management, Wire, Delay]
Foreword
24th Annual Symposium on Foundations of Computer Science
None
1983
Presents the introductory welcome message from the conference proceedings.
[]
Solving low density subset sum problems
24th Annual Symposium on Foundations of Computer Science
None
1983
The subset sum problem is to decide whether or not the 0-1 integer programming problem &#x03A3;i=1n aixi = M; all xi = 0 or 1; has a solution, where the ai and M are given positive integers. This problem is NP-complete, and the difficulty of solving it is the basis of public key cryptosystems of knapsack type. We propose an algorithm which when given an instance of the subset sum problem searches for a solution. This algorithm always halts in polynomial time, but does not always find a solution when one exists. It converts the problem to one of finding a particular short vector v in a lattice, and then uses a lattice basis reduction algorithm due to A. K. Lenstra, H. W. Lenstra, Jr., and L. Lov&#x0E1;sz to attempt to find v. We analyze the performance of the proposed algorithm. Let the density d of a subset sum problem be defined by d=n/log2(maxi ai). Then for "almost all" problems of density d &#x226A; .645 the vector v we are searching for is the shortest nonzero vector in the lattice. We prove that for "almost all" problems of density d &#x226A; 1/n the lattice basis reduction algorithm locates v. Extensive computational tests of the algorithm suggest that it works for densities d &#x226A; dc (n), where dc (n) is a cutoff value that is substantially larger than 1/n. This method gives a polynomial time attack on knapsack public key cryptosystems that can be expected to break them if they transmit information at rates below dc (n), as n &#x02192; &#x0221E;.
[Algorithm design and analysis, Density measurement, Lattices, Public key cryptography, Linear programming, Polynomials, Performance analysis, Testing, Information rates]
How to simultaneously exchange a secret bit by flipping a symmetrically-biased coin
24th Annual Symposium on Foundations of Computer Science
None
1983
We present a cryptographic protocol allowing two mutually distrusting parties, A and B, each having a secret bit, to "simultaneously" exchange the values of those bits. It is assumed that initially each party presents a correct encryption of his secret bit to the other party. We develop a new tool to implement our protocol: a slightly biased symmetric coin. The key property of this coin is that from each flip A receives a piece of probabilistic information about B's secret bit which is symmetric to the piece of information B receives about A's secret bit.
[Computer science, Turing machines, Polynomials, Cryptography, Cryptographic protocols]
Trapdoor pseudo-random number generators, with applications to protocol design
24th Annual Symposium on Foundations of Computer Science
None
1983
We define the class of trapdoor pseudo-random number generators, and introduce a new technique for using these in cryptography. As an application for this technique, we present a provably secure protocol for One-Bit Disclosures i.e. for giving a one-bit message in exchange for receipt.
[Earthquakes, Polynomials, Cryptography, Random number generation, Mesh generation, Postal services, Cryptographic protocols, Random sequences, Testing]
A topological approach to evasiveness
24th Annual Symposium on Foundations of Computer Science
None
1983
The complexity of a digraph property is the number of entries of the vertex adjacency matrix of a digraph which must be examined in worst case to determine whether the digraph has the property. Rivest and Vuillemin proved the result (conjectured by Aanderaa and Rosenberg) that every graph property that is monotone (preserved by addition of edges) and nontrivial (holds for some but not all graphs) has complexity &#x03B8;(v2) where v is the number of vertices. Karp conjectured that every such property is evasive, i.e., requires that every entry of the incidence matrix be examined. In this paper it is shown that Karp's conjecture follows from another conjecture concerning group actions on topological spaces. A special case of this conjecture is proved and applied to prove Karp's conjecture for the case of properties of graph and digraph properties on a prime power number of vertices.
[Boolean functions]
On the security of multi-party ping-pong protocols
24th Annual Symposium on Foundations of Computer Science
None
1983
We define a p-party ping-pong protocol and its security problem, along the lines of Dolev and Yao's definition for twoparty ping-pong protocol. In the case of two parties, it was assumed, with no loss of generality, that there exists a single saboteur in the net and the protocol was defined to be secure iff it was secure against the active interventions of one saboteur. We show that for more than 2 parties this assumption can no longer be made and that for p parties 3(p-2) + 1 is a lower bound on the number of saboteurs which should be considered for the security problem. On the other hand we establish a 3(p-2) + 2 upper bound on the number of saboteurs which should be considered. We conclude that for a fixed p, p-party ping-pong protocols can be tested for security in 0(n3) time and 0(n2) space, when n is the length of the protocol. We show that if p, the number of participants in the protocol, is part of the input then the security problem becomes NP-Hard. Relaxing the definition of a ping-pong protocol so that operators can operate on half words (thus introducing commutativity of the operators) causes the security problem to become undecidable.
[Computer science, Upper bound, Tiles, Public key, DH-HEMTs, Cryptography, Communication system security, Protection, Cryptographic protocols, Testing]
The program complexity of searching a table
24th Annual Symposium on Foundations of Computer Science
None
1983
Given a fixed set S of n keys, we would like to store them so that queries of the form "Is x &#x02208; S?" can be answered quickly. A commonly employed scheme to solve this problem uses a table to store the keys, and a special purpose program depending on S which probes the table. We analyze the tradeoff between the maximum number of probes allowable to answer a query, and the information-theoretic complexity of the program to do so. Perfect hashing (where the query must be answered in one probe) has a program complexity of nlog2 e(1 + o(1)) bits, and this lower bound can be achieved. Under a model combining perfect hashing and binary search methods, it is shown that for k probes to the table, nk/2k+1(1 + o(1)) bits are necessary and sufficient to describe a table searching algorithm. This model gives some information-theoretic bounds on the complexity of searching an external memory. We examine some schemes where pointers are allowed in the table, and show that for k probes to the table, about nlog2e/e(k+1)!(1+o(1)) bits are necessary and sufficient to describe the search. Finally, we prove some lower bounds on the worst case performance of hash functions described by bounded Boolean circuits, and worst case performance of universal classes of hash functions.
[Computer science, Search methods, Circuits, Time measurement, Application software, Probes, Contracts, Information analysis, Arithmetic]
Improved upper bounds on shellsort
24th Annual Symposium on Foundations of Computer Science
None
1983
The running time of Shellsort, with the number of passes restricted to O(log N), was thought for some time to be &#x0398;(N3/2), due to general results of Pratt. Sedgewick recently gave an O(N4/3) bound, but extensions of his method to provide better bounds seem to require new results on a classical problem in number theory. In this paper, we use a different approach to achieve O(N1+4/&#x0221A;2lgN).
[Computer science, Algorithm design and analysis, Upper bound, Hydrogen, Sorting, Information analysis, Testing]
Optimum algorithms for two random sampling problems
24th Annual Symposium on Foundations of Computer Science
None
1983
Several fast new algorithms are presented for sampling n records at random from a file containing N records. The first problem we solve deals with sampling when N is known, and the the second problem considers the case when N is unknown. The two main results in this paper are Algorithms D and Z. Algorithm D solves the first problem by doing the sampling with a small constant amount of space and in O(n) time, on the average; roughly n uniform random variates are generated, and approximately n exponentiation operations are performed during the sampling The sample is selected sequentially and online; it answers an open problem in [Knuth 81]. Algorithm Z solves the second problem by doing the sampling using O(n) space, roughly n ln(N/n) uniform random variates and O(n(1 + log(N/n))) time, on the average. Both algorithms are time- and space-optimum and are short and easy to implement.
[Computer science, Algorithm design and analysis, Quality control, Read-write memory, Sampling methods, Iterative algorithms, Time measurement, Application software, Statistics, Sorting]
Probabilistic counting
24th Annual Symposium on Foundations of Computer Science
None
1983
We present here a class of probabilistic algorithms with which one can estimate the number of distinct elements in a collection of data (typically a large file stored on disk) in a single pass, using only 0(1) auxiliary storage and 0(1) operations per element. We precisely quantify the accuracy-storage trade-offs: for instance a typical accuracy of about 5&#x025; can be achieved using only 256 binary words, even for very large files. The algorithms are totally insensitive to the replicative structure of the elements in the file. They are particularly adapted to data base systems in the context of query optimization and can be implemented in a decentralized manner (thus making them also useful for distributed data base applications).
[Algorithm design and analysis, Degradation, Distributed processing, Laboratories, Winches, Performance gain, Sampling methods, Counting circuits, Sorting]
Constructing arrangements of lines and hyperplanes with applications
24th Annual Symposium on Foundations of Computer Science
None
1983
An optimal algorithm is presented for constructing an arrangement of hyperplanes in arbitrary dimensions. It relies on a combinatorial result that is of interest in its own right. The algorithm is shown to improve known worst-case time complexities for five problems: computing all order-k Voronoi diagrams, computing the &#x03BB;-matrix, estimating halfspace queries, degeneracy testing, and finding the minimum volume simplex determined by a set of points.
[Computer science, Computational geometry, Parallel processing, Data structures, Application software, Testing]
Dynamic computational geometry
24th Annual Symposium on Foundations of Computer Science
None
1983
We consider problems in computational geometry when every one of the input points is moving in a prescribed manner. We present and analyze efficient algorithms for a number of problems and prove lower bounds for some of them.
[Algorithm design and analysis, Computational geometry, Terminology, Polynomials, Arithmetic]
A kinetic framework for computational geometry
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Computational geometry, Convolution, Shape, Constraint theory, Kinetic theory, State-space methods, Topology, Collision avoidance, Robots, Testing]
Geometric retrieval problems
24th Annual Symposium on Foundations of Computer Science
None
1983
A large class of geometric retrieval problems has the following form. Given a set X of geometric objects, preprocess to obtain a data structure D(X). Now use D(X) to rapidly answer queries on X. We say an algorithm for such a problem has (worst-case) space-time complexity O(f(n),g(n)) if the space requirement for D(X) is O(f) and the 'locate run-time' required for each retrieval is O(g). We show three techniques which can consistently be exploited in solving such problems. For instance, using our techniques, we obtain an O(n2+e, lognlog(l/&#x02208;)) spacetime algorithm for the polygon retrieval problem, for arbitrarily small &#x02208;, improving on the previous solution having complexity O(n7,logn).
[Strontium, Runtime, Costs, Aerospace simulation, Data structures, Robustness, Time measurement, Nearest neighbor searches]
Filtering search: A new approach to query-answering
24th Annual Symposium on Foundations of Computer Science
None
1983
We introduce a new technique for solving problems of the following form: preprocess a set of objects so that those satisfying a given property with respect to a query object can be listed very effectively. Among well-known problems to fall into this category we find range query, point enclosure, intersection, near-neighbor problems, etc. The approach which we take is very general and rests on a new concept called fitering search. We show on a number of examples how it can be used to improve the complexity of known algorithms and simplify their implementations as well. In particular, filtering search allows us to improve on the worst-case complexity of the best algorithms known so far for solving the problems mentioned above.
[Computer science, Graphics, Computational geometry, Costs, Filtering, Spatial databases, Circuit synthesis, Statistics]
Representations of rational functions
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Concurrent computing, Computer science, Performance evaluation, Interpolation, Computational modeling, Parallel processing, Taylor series, Polynomials, Parallel algorithms, Testing]
Logarithmic depth circuits for algebraic functions
24th Annual Symposium on Foundations of Computer Science
None
1983
This paper describes circuits for computation of various algebraic functions on polynomials, power series, integers, and reals for which it has been a long standing open problem to compute in depth less then (log n)2. Let R[x] be the polynomials and power series over a commutative ring which supports a fast Fourier transform and let L[x] be the polynomials and power series over the rationals L. For polynomials of degree n-1, we give circuits of depth O(log n) for computing - the m-th power of a polynomial and the product of m polynomials in R[x], where m=nO(1) - the symmetric functions on R[x] - the remainder and quotient of division of polynomials in L[x] - interpolation of a polynomial in L[x]. For power series with n given low order terms, we give circuits of depth O(log n) for computing the first n low order terms of - the m-th power of a power series in R[x] and the product of m power series in R[x] where m=nO(1) - the composition of power series in R[x] - the reciprocal of a power series and the division of two power series in L[x] -the reversion of a power series in L[x] - various elementary functions applied to power series in L[x] such as (fixed) powers, roots, exponentation, logarithm, sin, cos, arctangent, and hyperbolic cosine. For integers represented by n bit binary numbers, we give boolean circuits (whose gates compute the boolean operations &#x02227;, &#x02228;, and &#x0AC;) of depth O(log n (loglog n) 2) for computing: - the m-th power of an integer and the product of m = nO(1) integers, - the remainder and quotient of the division of two integers. There are many immediate consequences of this result. For reals on a finite interval [a,b] represented as floating point numbers within relative accuracy o(2-n), we have boolean circuits of depth O (log n(loglog n) 2) for computing within relative accuracy o(2-n): - the m-th power of a real and the product of m = nO(1) reals - the reciprocal of a real and division of reals - various elementary functions on reals. Also, as a consequence of the above, for polynomials and power series in L[x] we have uniform boolean circuits of depth O(log n(loglog n)2) for all the above listed problems for polynomials and power series, and also: - evaluation of a polynomial or power series in L[x] at n points, within relative accuracy o(2-n). All our circuits may be uniformly constructed by a deterministic Turning machine with space O(log n) and have constant indegree.
[Concurrent computing, Fast Fourier transforms, Circuits, Laboratories, Turning, Polynomials, Modules (abstract algebra), Parallel algorithms, Contracts, Arithmetic]
Trade-offs between depth and width in parallel computation
24th Annual Symposium on Foundations of Computer Science
None
1983
A new technique for proving lower bounds for parallel computation is introduced. This technique enables us to obtain, for the first time. non-trivial tight lower bounds for shared-memory models of parallel computation that allow simultaneous read/write access to the same memory location. The size m of the common memory is called communication width or width in short. For a wide variety of problems (including parity and majority) we show that the time complexity T (depth) and the communication width m are related by the trade-off curve mT2 = &#x03A9;(n) (where n is the size of the input). This bound is tight lot every m &#x02264;n/log2n We extend our technique to prove mT3 = &#x03A9;(n) trade-off for a class of "simpler" functions (includind Boolean Or) on a weaker model that forbids simultaneous write access. This result improves the lower bound of Cook and Dwork [CD-82] when communication is limited.
[Concurrent computing, Computer science, Availability, Upper bound, Computational modeling, Random access memory, Read-write memory, Phase change random access memory, Time sharing computer systems, Polynomials]
The parallel complexity of the abelian permutation group membership problem
24th Annual Symposium on Foundations of Computer Science
None
1983
We show that the permutation group membership problem can be solved in depth (logn)3 on a Monte Carlo Boolean circuit of polynomial size in the restricted case in which the group is abelian. We also show that this restricted problem is NC1-hard for NSPACE(logn).
[Concurrent computing, Computer science, Monte Carlo methods, Councils, Voting, Circuits, Linear programming, Polynomials, Parallel algorithms, Testing]
Computational complexity and the classification of finite simple groups
24th Annual Symposium on Foundations of Computer Science
None
1983
We address the graph isomorphism problem and related fundamental complexity problems of computational group theory. The main results are these: A1. A polynomial time algorithm to test simplicity and find composition factors of a given permutation group (COMP). A2. A polynomial time algorithm to find elements of given prime order p in a permutation group of order divisible by p. A3. A polynomial time reduction of the problem of finding Sylow subgroups of permutation groups (SYLFIND) to finding the intersection of two cosets of permutation groups (INT). As a consequence, one can find Sylow subgroups of solvable groups and of groups with bounded nonabelian composition factors in polynomial time. A4. A polynomial time algorithm to solve SYLFIND for finite simple groups. A5. An ncd/log d algorithm for isomorphism (ISO) of graphs of valency less than d and a consequent improved moderately exponential general graph isomorphism test in exp(c&#x0221A;n log n) steps. A6. A moderately exponential, n,c&#x0221A;n algorithm for INT. Combined with A3, we obtain an nc&#x0221A;n algorithm for SYLFIND as well. All these problems have strong links to each other. ISO easily reduces to INT. A subcase of SYLFIND was solved in polynomial time and applied to bounded valence ISO in [Lul]. Now, SYLFIND is reduced to INT. Interesting special cases of SYLFIND belong to NP &#x02229; coNP and are not known to have subexponential solutions. All the results stated depend on the classification of finite simple groups. We note that no previous ISO test had no(d) worst case behavior for graphs of valency less than d. It appears that unless there is another radical breakthrough in ISO, independent of the previous one, the simple groups classification is an indispensable tool for further developments.
[Algebra, ISO standards, Polynomials, Computational complexity, Artificial intelligence, Combinatorial mathematics, Testing]
Factoring sparse multivariate polynomials
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Computer science, Information geometry, Polynomials, Encoding, History, Galois fields, Testing]
Some relationships between logics of programs and complexity theory
24th Annual Symposium on Foundations of Computer Science
None
1983
The aim of this paper is to show that some open problems in Comparative Schematology and in Logics of Programs are equivalent to open problems in Complexity Theory. In particular we show that PSPACE = PTIME holds if and only if flow-diagrams with arrays are of the same computational power as recursive procedures. These statements are also equivalent to the following statement: Logics based on the above-mentioned classes of program schemes have equal expressive power. A similar characterization may be given for other complexity classes.
[Mathematics, Complexity theory, Logic, Books, Testing]
Reasoning about infinite computation paths
24th Annual Symposium on Foundations of Computer Science
None
1983
We investigate extensions of temporal logic by finite automata on infinite words. There are three different types of acceptance conditions (finite, looping and repeating) that one can give for these finite automata. This gives rise to three different logics. It turns out, however. that these logics have the same expressive power but differ in the complexity of their decision problem. We also investigate the addition of alternation and show that it does not increase the complexity of the decision problem.
[Automata, Robustness, Polynomials, Logic]
Propositional game logic
24th Annual Symposium on Foundations of Computer Science
None
1983
We define a propositional logic of games which lies in expressive power between the Propositional Dynamic Logic of Fischer and Ladner [FL] and the &#x0B5;-calculus of Kozen [K]. We show that the logic is decidable and give a very simple, complete set of axioms, one of the rules being Brouwer's bar induction. Even though decidable, this logic is powerful enough to define well orderings. We state some other results, open questions and indicate directions for further research.
[Information science, Law, Power generation economics, Educational institutions, Set theory, Calculus, Logic, Game theory, Legal factors, Scheduling algorithm]
Decision procedures for time and chance
24th Annual Symposium on Foundations of Computer Science
None
1983
Decision procedures are provided for checking the satisfiability of a formula in each of the three systems TCg. TCb and TCf defined in [LS]. The procedures for TCg and TCf run in non-deterministic time 22on where n is the size of the formula and c is a constant. The procedure for TCb runs in non-deterministic time 22on2. A deterministic exponential lower bound is proved for the three systems. All three systems are also shown to be PSPACE-hard using results of [SC]. Those decision procedures are not as efficient as the deterministic (one or two)- exponential time procedures proposed in [BMP] and [EH1] for different logics of branching time that are weaker than ours in expressive power. No elementary decision procedure is known for a logic of branching time that is as expressive as ours. The decision procedures of the probabilistic logics of [HS] run in deterministic exponential time but their language is essentially less expressive than ours.
[Computer science, Probabilistic logic, Page description languages, Mathematics, Power system modeling]
Algebras of feasible functions
24th Annual Symposium on Foundations of Computer Science
None
1983
What happens if we interpret the syntax of primitive recursive functions in finite domains rather than in the (Platonic) realm of all natural numbers? The answer is somewhat surprising: primitive recursiveness coincides with LOGSPACE computability. Analogously, recursiveness coincides with PTIME computability on finite domains (cf. [Sa]). Inductive definitions for some other complexity classes are discussed too.
[Computer science, Algebra, Databases, Logic]
On content-free generators
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Transducers, Character generation]
The power of geometric duality
24th Annual Symposium on Foundations of Computer Science
None
1983
This paper uses a new formulation of the notion of duality that allows the unified treatment of a number of geometric problems. In particular, we are able to apply our approach to solve two long-standing problems of computational geometry: one is to obtain a quadratic algorithm for computing the minimum-area triangle with vertices chosen among n points in the plane; the other is to produce an optimal algorithm for the half-plane range query problem. This problem is to preprocess n points in the plane, so that given a test half-plane, one can efficiently determine all points lying in the half-plane. We describe an optimal O(k + log n) time algorithm for answering such queries, where k is the number of points to be reported. The algorithm requires O(n) space and O(n log n) preprocessing time. Both of these results represent significant improvements over the best methods previously known. In addition, we give a number of new combinatorial results related to the computation of line arrangements.
[Computational geometry, Ear, H infinity control, Topology, Equations, Testing, Clocks]
Fast algorithms for the all nearest neighbors problem
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Computer science, Computational geometry, Data analysis, Probability density function, Cost function, Data structures, Partitioning algorithms, Pattern recognition, Nearest neighbor searches]
Minimum partition of polygonal regions into trapezoids
24th Annual Symposium on Foundations of Computer Science
None
1983
We consider the problem of partitioning a polygonal region into a minimum number of trapezoids with two horizontal sides. Triangles with a horizontal side are considered to be trapezoids with two horizontal sides one of which is degenerate. In this paper we show that this problem is equivalent to the problem of finding a maximum independent set of a straight-lines-in-the-plane graph. Thus it is shown to be NP-complete. Next we present an O(n log n) natural approximation algorithm which uses only horizontal chords to partition a polygonal region P into trapezoids, where n is the number of vertices of P. We show that the absolute performance ratio of the algorithm is three. We can also design another approximation algorithm with the ratio (1 + 2/c) if we have a (1 - 1/c) approximation algorithm for the maximum independent set problem on straight-lines-in-the-plane graphs, where c is some constant. Finally, we give an O(n3) exact algorithm for polygonal regions without windows.
[Algorithm design and analysis, Electron traps, Instruments, Lithography, Very large scale integration, Apertures, Approximation algorithms, Polynomials, Partitioning algorithms, Physics]
Shortest path problems in planar graphs
24th Annual Symposium on Foundations of Computer Science
None
1983
Graph decomposition and data structures techniques are presented that make possible faster algorithms for shortest paths in planar graphs. Improved algorithms are presented for the single source problem, the all pairs problem, and the problem of finding a minimum cut in an undirected graph.
[Shortest path problem, Costs, Particle separators, Character generation, Data structures, Iterative algorithms, Partitioning algorithms]
Scaling algorithms for network problems
24th Annual Symposium on Foundations of Computer Science
None
1983
A network is a graph with numeric parameters such as edge lengths, capacities, costs, etc. We present efficient algorithms for network problems that work by scaling the numeric parameters. Scaling takes advantage of efficient nonnumeric algorithms such as the Hopcroft-Karp matching algorithm. Let n, m and N denote the number of vertices, number of edges, and largest numeric parameter of the network, respectively; assume all numeric parameters are integers. A scaling algorithm for maximum weight matching on a bipartite graph runs in O(n3/4 m log N) time. This can improve the traditional Hungarian method which runs in O(n m log n) time. This result gives similar improvements for the following problems: single-source shortest paths for arbitrary edge lengths (Bellman's algorithm); maximum weight degree-constrained subgraph; minimum cost flow in a 0-1 network (Edmonds and Karp). Scaling also gives simple algorithms that match the best time bounds (when log N = O(log n)) for shortest paths on a directed graph with nonnegative lengths (Dijkstra's algorithm) and maximum value network flow (Sleator and Tarjan).
[Computer science, Costs, Computer networks, Polynomials, Combinatorial mathematics]
Partition of planar flow networks
24th Annual Symposium on Foundations of Computer Science
None
1983
We give a new characterization of the planar separator theorem in terms of mutually non-containing closed Jordan Curves. using this, we develop an O(n &#x0221A;n logn) maximum flow algorithm for directed planar networks (hence for any planar network).
[Computer science, Tree graphs, Particle separators, Partitioning algorithms]
Approximation algorithms for NP-complete problems on planar graphs
24th Annual Symposium on Foundations of Computer Science
None
1983
This paper describes a general technique that can be used to obtain approximation algorithms for various NP-complete problems on planar graphs. The strategy depends on decomposing a planar graph into subgraphs of a form we call k- outerplanar. For fixed k, the problems of interest are solvable optimally in linear time on k-outerplanar graphs by dynamic programming. For general planar graphs, if the problem is a maximization problem, such as maximum independent set, this technique gives for each k a linear time algorithm that produces a solution whose size is at least (k-1)/k optimal. If the problem is a minimization problem, such as minimum vertex cover, it gives for each k a linear time algorithm that produces a solution whose size is at most (k + 1)/k optimal. Taking k = c log log n or k = c log n, where n is the number of nodes and c is some constant, we get polynomial time approximation schemes, i.e. algorithms whose solution sizes converge toward optimal as n increases. The class of problems for which this approach provides approximation schemes includes maximum independent set, maximum tile salvage, partition into triangles, maximum H-matching, minimum vertex cover, minimum dominating set, and minimum edge dominating set. For these and certain other problems, the proof of solvability on k-outerplanar graphs also enlarges the class of planar graphs for which the problems are known to be solvable.
[Heart, Minimization methods, Tiles, Particle separators, Laboratories, Approximation algorithms, Polynomials, Dynamic programming, Partitioning algorithms, NP-complete problem]
A polynomial algorithm for the MIN CUT linear arrangement of trees
24th Annual Symposium on Foundations of Computer Science
None
1983
An algorithm is presented which finds a min-cut linear arrangement of a tree in O(nlogn) time. An extension of the algorithm determines the number of pebbles needed to play the black and white pebble game on a tree.
[Tree graphs, Wires, Circuits, Bandwidth, Very large scale integration, Cost function, Minimization, Polynomials, Joining processes]
Tree structures for partial match retrieval
24th Annual Symposium on Foundations of Computer Science
None
1983
This paper describes general evaluation methods for "partial-match retrieval" in multikey record files. An expected cost analysis is given for some of the major multidimensional tree structures which have been proposed in the data base and graphics literature.
[Tree data structures, Algorithm design and analysis, Multidimensional systems, Costs, Computer graphics, Binary search trees, Information retrieval, Data structures, Application software, Data mining]
Bin packing with items uniformly distributed over intervals [a,b]
24th Annual Symposium on Foundations of Computer Science
None
1983
We consider the problem of packing n items which are drawn uniformly from intervals of the form [a,b], where 0 &#x226A; a &#x226A; b &#x226A; 1. For a fairly large class of a and b, we determine a lower bound on the asymptotic expected number of bins used in an optimum packing. The method of proof is related to the dual of the linear programming problem corresponding to the bin packing problem. We demonstrate that these bounds are tight by providing simple packing strategies which achieve them.
[Algorithm design and analysis, Computer science, Differential equations, Linear programming, Density functional theory]
Hash functions for priority queues
24th Annual Symposium on Foundations of Computer Science
None
1983
The complexity of priority queue operations is analyzed with respect to the cell probe computational model of A. Yao. A method utilizing families of hash functions is developed which permits priority queue operations to be implemented in constant worst case time provided that a size constraint is satisfied. The minimum necessary size of a family of hash functions for computing the rank function is estimated and contrasted with the minimum size required for perfect hashing.
[Computational modeling, Data structures, Encoding, Probes, Queueing analysis]
Lower bounds on graph threading by probabilistic machines
24th Annual Symposium on Foundations of Computer Science
None
1983
It is likely that reliable and fast space-bounded probabilistic acceptors are less powerful than nondeterministic ones. We consider a restricted model of space-bounded probabilistic computation, the random analog of a model studied in [CR]. We show that maze traversal (a complete problem for nondeterministic space log n) requires space &#x03A9;(log2n/loglogn) by random machines, even if 'fast' is relaxed to mean only 'subexponential'. In particular, the lower bound on space holds for the time complexity of Savitch's algorithm (which can be simulated in the model).
[Semiconductor device modeling, Computer science, Error probability, Turing machines, Computational modeling, Automata, Stochastic processes, Chromium, Analog computers, Polynomials]
On the computational complexity of the permanent
24th Annual Symposium on Foundations of Computer Science
None
1983
We consider the problem of computing the permanent of an mxn matrix, m&#x226A;n, over an arbitrary commutative ring. The permanent is a central problem in the computational complexity of enumeration problems and arises in several applications. We introduce the class of multilinear programs, where the monomials of the factors of any multiplication come from different sets of rows. All the known algorithms to compute the permanent satisfy this property. These programs can be represented by arithmetic circuits with unbounded fan in. We show the following: 1) Given any positive integer k, no constant depth multilinear program with &#x226A;nk arithmetic operations can compute the permanent of an mxn matrix, where m = &#x03A9;(&#x03B3;(n)logn) for any increasing function &#x03B3;(n). 2) There exists a polynomial-size multilinear program of depth 4 which computes the permanent of an mxn matrix for m = 0(logn/loglogn). 3) Any arbitrary arithmetic circuit that computes the permanent of an mxn matrix with depth &#x226A;3 must be of exponential size for any m nonconstant. Our proofs use, in a nontrivial way, probabilistic techniques to establish several combinatorial facts.
[Computer science, Statistical analysis, Physics computing, Circuits, Polynomials, Modules (abstract algebra), Computational complexity, Erbium, Arithmetic]
Multiplication is the easiest nontrivial arithmetic function
24th Annual Symposium on Foundations of Computer Science
None
1983
It is shown that floating point (or integer) multiplication can be reduced to the evalution of a very large class of functions including most of the nontrivial functions used in practice. That means that whenever any such function can be evaluated by boolean circuits of size S(n), then multiplication can be done with circuits of size O(S(n)). as well.
[Computer science, Circuits, Size measurement, Digital arithmetic, Polynomials, Convergence]
On depth-reduction and grates
24th Annual Symposium on Foundations of Computer Science
None
1983
For each &#x03B5;(0 &#x02264; &#x03B5; &#x226A; 1) a family Gn =(V(Gn), E(Gn)) of a cyclic digraphs can be constructively defined having the following properties: (a) #V(Gn) &#x02264; n &#x000B7; 2n+2 (b) degree (Gn) &#x02264; constant (c) it is necessary to remove &#x03A9;(n &#x000B7; 2n) edges in order to reduce the depth of Gn to (2n)&#x03B5;. It is then shown: For suitable constants c1, C2 &#x226B; 0, there are (fn, n)- grates (see Definition 1) of size linear in n, where fn(x):= c1 &#x000B7; n2 x &#x02264; c2 &#x02264'; n/0 otherwise
[Computer science, Upper bound, Turing machines, Computational modeling, Polynomials, Vectors, Game theory]
Relativized circuit complexity
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Computer science, Turing machines, Circuit simulation, Measurement standards, Linear circuits, Size measurement, Polynomials, Time measurement, Concrete, Complexity theory]
Randomness and the density of hard problems
24th Annual Symposium on Foundations of Computer Science
None
1983
A language L is random with respect to a given complexity class C if for all &#x02032; &#x02208; C L and &#x02032; disagree on half of all strings. It is known that for any complexity class there are recursive languages that are random with respect to that class. Here it is shown that there are tight space and time hierarchies of random languages, and that EXPTIME contains P-isomorphism classes containing only languages that are random with respect to polynomial-time computations. The technique used is extended to show that for any constructible bound on time or space it is possible to deterministically generate binary sequences that appear random to all prediction algorithms subject to the given resource bound. Furthermore, the generation of such a sequence requires only slightly more resources than the given bound.
[Approximation algorithms, Prediction algorithms, Polynomials, Random sequences, Binary sequences]
Lower bounds on the time of probabilistic on-line simulations
24th Annual Symposium on Foundations of Computer Science
None
1983
We study probabilistic on-line simulators for several machine models (or memory structures). The simulators have a more constrained access to data than the virtual machines, but are allowed to use probabilistic means to improve average access time. We show that in many cases coin tosses can not make up for inadequate access.
[Computer science, Transducers, Turing machines, Computational modeling, Computer simulation, Random access memory, Magnetic heads, Virtual machining, Contracts, Centralized control]
Techniques for solving graph problems in parallel environments
24th Annual Symposium on Foundations of Computer Science
None
1983
We introduce new paradigms for the construction of efficient parallel graph algorithms. These paradigms, called filtration and funnelled pipelining, are illustrated with VLSI circuits for computing connected components, minimum spanning forests, and biconnected components. These circuits use realistic I/O schedules and require time and area of O(n1+&#x03B5;). Thus they are essentially optimal. Filtration is a technique used to rapidly discard irrelevant input data. This greatly reduces storage, time, and communications costs in a wide variety of problems. A funnelled pipeline is obtained by building a series of increasingly thorough filter stages. Transition times along such a pipeline of filters form an exponentially increasing sequence. The increasing amount of time exactly balances the increasing degree of filtration. This balance makes possible the cascaded filtration critical to the minimum spanning forest and the biconnected components algorithms.
[Concurrent computing, Costs, Filtration, Processor scheduling, Circuits, Very large scale integration, Parallel processing, Finite impulse response filter, Parallel algorithms, Pipeline processing]
An algorithm for the optimal placement and routing of a circuit within a ring of pads
24th Annual Symposium on Foundations of Computer Science
None
1983
As the final stage in laying out a chip, the logic of the integrated circuit is assembled into one (not necessarily rectangular) module which must then be connected to pads lying along a rectangular frame. A placement for the module must be determined to assure the feasibility of the (river) routing from the logic inside to the pads on the periphery. We first show how to solve the routing problem in a stationary context: given the placement, can the signals be wired in the given doughnut-shaped area? Then we use the routability analysis developed in the first part to find a placement of the circuit that yields a feasible routing (if one exists). Both algorithms run in time that is quadratic in the size of the input, and there exist cases for which this bound cannot be improved upon.
[Algorithm design and analysis, Fabrication, Logic circuits, Wires, Routing, Logic design, Rivers, Chip scale packaging, Assembly, Clocks]
Period-time tradeoffs for VLSI models with delay
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Concurrent computing, Embedded computing, Delay effects, Computational modeling, Very large scale integration, Parallel architectures, Data flow computing, Wire, Time factors, Pipeline processing]
Estimating the multiplicities of conflicts in multiple access channels
24th Annual Symposium on Foundations of Computer Science
None
1983
A conflict of multiplicity k occurs when k stations transmit simultaneously to a multiple access channel. As a result, all stations receive feedback indicating whether k is 0, 1, or is &#x02265; 2. If k = 1 the transmission succeeds, whereas if k &#x02265; 2 all the transmissions fail. In general, no a priori information about k is available. We present and analyze an algorithm that enables the conflicting stations to cooperatively compute a statistical estimate of k, at small cost, as a function of the feedback elicited during its execution. An algorithm to resolve a conflict among two or more stations controls the retransmissions of the conflicting stations so that each eventually transmits singly to the channel. Combining our estimation algorithm with a binary tree algorithm leads to a hybrid algorithm that resolves conflicts faster on average than any other reported to date.
[Computer science, Algorithm design and analysis, Coaxial cables, Upper bound, Feedback, Binary trees, Cost function, Particle measurements, Time measurement, Artificial intelligence]
On the minimal synchronism needed for distributed consensus
24th Annual Symposium on Foundations of Computer Science
None
1983
Reaching agreement is a primitive of distributed computing. While this poses no problem in an ideal, failure-free environment, it imposes certain constraints on the capabilities of an actual system: a system is viable only if it permits the existence of consensus protocols tolerant to some number of failures. Fischer, Lynch and Paterson [FLP] have shown that in a completely asynchronous model, even one failure cannot be tolerated. In this paper we extend their work, identifying several critical system parameters, including various synchronicity conditions, and examine how varying these affects the number of faults which can be tolerated. Our proofs expose general heuristic principles that explain why consensus is possible in certain models but not possible in others.
[Computer science, Fault diagnosis, Protocols, Upper bound, Communication systems, Laboratories, Synchronization, Distributed computing, Clocks]
Randomized byzantine generals
24th Annual Symposium on Foundations of Computer Science
None
1983
We present a randomized solution for the Byzantine Generals Problems. The solution works in the synchronous as well as the asynchronous case and produces Byzantine Agreement within a fixed small expected number of computational rounds, independent of the number n of processes and the bound t on the number of faulty processes. The solution uses A. Shamir's method for sharing secrets. It specializes to produce a simple solution for the Distributed Commit problem.
[Decision support systems, Protocols, Electric breakdown, Authentication, Public key, Robustness, Gas insulated transmission lines, Digital signatures]
A tight bound for black and white pebbles on the pyramid
24th Annual Symposium on Foundations of Computer Science
None
1983
Lengauer and Tarjan proved that the number of black and white pebbles needed to pebble the root of a tree is at least 1/2 the number of black pebbles needed to pebble the root. We extend this result to a larger class of acyclic directed graphs including pyramid graphs.
[Computer science, Computer languages, Program processors, Optimizing compilers, Computational modeling, Laboratories, Application software]
Lower bounds by probabilistic arguments
24th Annual Symposium on Foundations of Computer Science
None
1983
The purpose of this paper is to resolve several open problems in the current literature on Boolean circuits, communication complexity, and hashing functions. These lower bound results share the common feature that their proofs utilize probabilistic arguments in an essential way. Specifically, we prove that, to compute the majority function of n Boolean variables, the size of any depth-3 monotone circuit must be greater than 2n&#x03B5;, and the size of any width-2 branching program must have super-polynomial growth. We also show that, for the problem of deciding whether i &#x02264; j for two n-bit integers i and j, the probabilistic &#x03B5;-error one-way communication complexity is of order &#x03B8;(n), while the two-way &#x03B5;-error complexity is O((log n)2). We will also prove that, to compute i &#x000B7; j mod p for an n-bit prime p, the probabilistic &#x03B5;-error two-way communication complexity is of order &#x03B8;(n). Finally, we prove a conjecture of Ullman that uniform hashing is asymptotically optimal in its expected retrieval cost among open address hashing schemes.
[Computer science, Circuits, Computation theory, Binary decision diagrams, Size measurement, Cost function, Polynomials, Complexity theory, Computational complexity, Combinatorial mathematics]
On determinism versus non-determinism and related problems
24th Annual Symposium on Foundations of Computer Science
None
1983
We show that, for multi-tape Turing machines, non-deterministic linear time is more powerful than deterministic linear time. We also discuss the prospects for extending this result to more general Turing machines.
[Legged locomotion, Turing machines, Computational modeling, Laboratories, Time factors]
Generalized Kolmogorov complexity and the structure of feasible computations
24th Annual Symposium on Foundations of Computer Science
None
1983
In this paper we define a generalized, two-parameter, Kolmogorov complexity of finite strings which measures how much and how fast a string can be compressed and we show that this string complexity measure is an efficient tool for the study of computational complexity. The advantage of this approach is that it not only classifies strings as random or not random, but measures the amount of randomness detectable in a given time. This permits the study how computations change the amount of randomness of finite strings and thus establish a direct link between computational complexity and generalized Kolmogorov complexity of strings. This approach gives a new viewpoint for computational complexity theory, yields natural formulations of new problems and yields new results about the structure of feasible computations.
[Computer science, Length measurement, Polynomials, Time measurement, Mathematics, Encoding, Complexity theory, Application software, Computational complexity, Petroleum]
Games against nature
24th Annual Symposium on Foundations of Computer Science
None
1983
false
[Uncertainty, Turing machines, Decision making, Cost function, Polynomials, Dynamic programming, Random processes, History, Books, Game theory]
Global wire routing in two-dimensional arrays
24th Annual Symposium on Foundations of Computer Science
None
1983
We examine the problem of routing wires on a VLSI chip, where the pins to be connected are arranged in a regular rectangular array. We obtain tight bounds for the worst-case "channel-width" needed to route an n &#x0D7; n array, and develop provably good heuristics for the general case. An interesting "rounding algorithm" for obtaining integral approximations to solutions of linear equations is used to show the near-optimality of single-turn routings in the worst-case.
[Computer science, Wiring, Military computing, Integral equations, Laboratories, Very large scale integration, Routing, Mathematics, Pins, Wire]
Reasoning about functional programs and complexity classes associated with type disciplines
24th Annual Symposium on Foundations of Computer Science
None
1983
We present a method of reasoning directly about functional programs in Second-Order Logic, based on the use of explicit second-order definitions for inductively defined data-types. Termination becomes a special case of correct typing. The formula-as-type analogy known from Proof Theory, when applied to this formalism, yields &#x03BB;-expressions representing objects of inductively defined types, as well as &#x03BB;-expressions representing functions between such types. A proof that a functional closed expression e is of type T maps into a &#x03BB;-expression representing (the value of) e; and a proof that a function f is correctly typed maps into a &#x03BB;-expression representing f (modulo the representations of objects of those types). When applied to integers and to numeric functions the mapping yields Church's numerals and the traditional function representations over them. The &#x03BB;-expressions obtained under the isomorphism are typed (in the Second-Order Lambda Calculus). This implies that, for functions defined over inductively defined types, the property of being proved everywhere-defined in Second-Order Logic is equivalent to the property of being representable in the Second-Order Lambda Calculus. Extensions and refinements of this result lead to other characterizations of complexity classes by type disciplines. For example, log-space functions over finite structures are precisely the functions over finite-structures definable by &#x03BB;-pairing-expressions in a predicative version of the Second-Order Lambda Calculus.
[Computer science, Computer languages, Logic programming, Binary trees, Reasoning about programs, Calculus, Equations, Power generation, Arithmetic]
Legal coloring of graphs
24th Annual Symposium on Foundations of Computer Science
None
1983
The following computational problem was initiated by Manber and Tompa (22nd FOCS Conference, 1981) : Given a graph G = (V,E) and a real function f : V&#x02192;R which is a proposed vertex coloring. Decide whether f is a proper vertex coloring of G. The elementary steps are taken to be linear comparisons. Lower bounds on the complexity of this problem are derived using the chromatic polynomial of G. It is shown how geometric parameters of a space partition associated with G influence the complexity of this problem. In particular we show (theorem 6) a lower bound of (m/2)1/2 log m + O(m1/2), where m is the number of edges of the graph in question. Existing methods for analyzing such space partitions are suggested as a powerful tool for establishing lower bounds for a variety of computational problems. Many interesting open problems are presented.
[Computer science, Law, Tin, Polynomials, Legal factors]
Information bounds are good for search problems on ordered data structures
24th Annual Symposium on Foundations of Computer Science
None
1983
The complexity of the search problem for a very broad class of data structures is estimated. The lower (Information Theoretic) bound and the upper bound differ by a small multiplicative constant.
[Computer science, Upper bound, Search problems, Data structures, Mathematics, Partitioning algorithms]
Sublinear parallel algorithm for computing the greatest common divisor of two integers
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
The advent of practical parallel processors has caused a reexamination of many existing algorithms with the hope of discovering a parallel implementation. One of the oldest and best known algorithms is Euclid's algorithm for computing the greatest common divisor (GCD). In this paper we present a parallel algorithm to compute the GCD of two integers. The two salient features of the algorithm are: the observation based on the pigeon hole principle that we can easily find an integer combination of the two integers A and B which has fewer bits than n and the idea of working in phases so as to perform arithmetics on n-bit integers only once every phase, the more frequent operations being performed on O(log/sup 2/n)-bit integers. It appears that yet another approach is needed if the GCD is to be computed in poly-log parallel time.
[Concurrent computing, Computer science, Computational modeling, Parallel processing, Polynomials, Mathematics, Systolic arrays, Parallel algorithms]
Finding biconnected componemts and computing tree functions in logarithmic parallel time
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We propose a new algorithm for finding the blocks (biconnected components) of an undirected graph. A serial implementation runs in 0[n+m] time and space on a graph of n vertices and m edges. A parallel implmentation runs in 0[log n] time and 0[n+m] space using 0[n+m] processors on a concurrent-read, concurrent-write parallel RAM. An alternative implementation runs in 0[n/sup 2/p] time and 0[n/sup 2/] space using any number p &#x02A7D; n/sup 2/log/sup 2/-n of processors, on a concurrent-read, exclusive-write parallel RAM. The latter algorithm has optimal speedup, assuming an adjacency matrix representation of the input. A general algorithmic technique which simplifies and improve computation of various functions on tress is introduced. This technique typically requires 0(log n) time using 0(n) space on an exclusive-read exclusive-write parallel RAM.
[US Department of Energy, parallel graph algorithm, biconnected components, Computational modeling, Random access memory, blocks, Read-write memory, Phase change random access memory, Parallel algorithms, Concurrent computing, Computer science, Tree graphs, spanning tree, Writing]
River Routing Every Which Way, But Loose
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
A solution to the 'Detailed Routing given a Homotopy' (DRH) problem is given in O(n + mlogm + D(m)) operations. The solution uses n + mlogm homotopy queries that are elementary; they are answerable based solely on "local properties" of modules, terminals, and wire connections. In addition, we need O(m) more complex queries, which are represented in the D(m) term. These queries must account for the total number of crossin s occurringfor selected test segments.
[Wiring, Design methodology, Wires, Lattices, Integrated circuit interconnections, Very large scale integration, Routing, Rivers, Joining processes]
A Comparative Study Of X-Tree, Pyramid And Related Machines
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
The intent of this paper was to investigate data movement techniques for some special networks which are derived from the binary tree and the mesh machines. We presented optimal bounds for some problems and close bounds for others. A new lower bound technique which incorporates the entire network topdogy was introduced. We believe that this technique is quite powerful and can be exploited to yield good lower bounds for conservative flow algorithms on other networks. However, it seems to be diacult to generalize it for nonconservative flow algorithms. Though we have obtained close bounds, several problems that remain open are noted.
[Tree graphs, Network topology, Merging, Wires, Binary trees, Read-write memory, Computer networks, Polynomials, Sorting]
A Polynomial Time Algorithm For Fault Diagnosability
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We present the first polynomial time algorithm for testing t-diagnosability. This is a significant advance in system level fault diagnosis. We also presented part of our analysis of t/s-diagnosability, including the fact that it is co-NP-complete and that there are polynomial algorithms for t/t and t/(t+1)-diagnosability.
[Fault diagnosis, Performance evaluation, Algorithm design and analysis, Software testing, System testing, Software performance, Built-in self-test, Polynomials, Labeling]
Graph Bisection Algorithins With Good Average Case Behavior
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We describe a polynomial time algorithm that, for every input graph, either outputs the minimum bisection of the graph or halts without output. More importantly, we show that the algorithm chooses the former course with high probability for many natural classes of graphs. In particular, for every fixed d&#x02A7E;3, all suffciently large n and all b = o(n1-(1/[(d+1)/2]), the algorithm finds the minimum bisection for almost all d-regular labelled simple graphs with 2n nodes and bisection width b.
[Computer science, Computer aided software engineering, Wires, Approximation algorithms, Routing, Mathematics, Polynomials]
Linear Verification For Spanning Trees
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Given a rooted tree with values associated with the n vertices and a set A of directed paths (queries), we describe an algorithm which finds the maximum value of every one of the given paths, and which uses only 5n + n log [(|A|+n)/n] comparisons. This leads to a spanning tree verification algorithm using O(n+e) comparisons in a graph with n vertices and e edges. No implementation is offered.
[Costs, Tree graphs, Circuit testing, Sorting]
On The Complexity Of Matrix Group Problems I
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We build a theory of black box groups, and apply it to matrix groups over finite fields. Elements of a black box group are encoded by strings of uniform length and group operations are performd by an oracle. Subgroups are given by a list of generators. We prove that for such subgroups, membership and divisor of the order are in NPB. (B is the group box oracle.) Under a plausible mathematical hypothesis on short presentations of finite simple groups, nom membership and exaact order will also be in NPB and thus in NPB &#x02229; NPB.
[Upper bound, Probability, Polynomials, Complexity theory, Galois fields, Equations]
Coordinating Pebble Motion On Graphs, The Diameter Of Permutation Groups, And Applications
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We have obtaincd some results in pebble coordination problems and the diameter of permutation groups.
[Algorithm design and analysis, Upper bound, Light emitting diodes, Testing]
Evaluating Rational Functions: Infinite Precision Is Finite Cost And Tractable On Average
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We consider the following generalization of the familiar '15-puzzle' which arises from issues in memory manngrment in distributed systems: Iet G be a graph with n vertices with k < n pebbles numbered 1,...,k on distinct verticcs. A move consistes of transferring a pebble to an adjacent unocccupied vertex. Is one arrangement of the pebbles reachable from another? We present a P-time decision algorithm, and prove matching O(n3) upper and lower bounds on the number of moves required. We have the following subexponential bound for certain unbounded cycles, one of which has prime length p &#x02A7D; 2n/3, and G is primitive, then G = A<sub>n</sub> or S<sub>n</sub> and has diameter &#x02A7D; 26 [(&#x0221A;(p+4))n8.
[Computational geometry, Computational modeling, Cost function, Roundoff errors, Polynomials, Computational complexity]
Semantic Models For Second-Order Lambda Calculus
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
The second-order lambda calculus is a typed expression language with polymorphic functions and abstract data typcs. Several definitions of models for this language have been proposed, each relying on the syntax of terms to characterize closure under explicite definition. This work aims to releive the model theorist of syntactic considerations.
[Algebra, Tiles, Sections, Buildings, Calculus, Logic, Personal communication networks]
Minimal Degrees For Honest Polynomial Reducibilities
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
The existence of minimal degrees is investigated for several polynomial reducibilities. It is shown that no set has minimal degree with respect to polynomial many-one or Turing reducibility. This extends a result of Ladner [L] whew reciirsive sets are considered. An "honest" polynomial reducibility, &#x02A7D;is defined which is a strengthening of polynomial Turing reducibility. We prove that no recursive set, (or igeeand P-immune set) has minimal < ;-degree. However, proving this same fact for all &#x00394;<sub>s</sub> sets (or even all 3 sets) would imply P 2 .y/l. Finally, a partial converse of this result is obtained, proving that if a certain class of one-way functions exists then no set has minimal (h/t)-degree.
[Computer science, Turing machines, Polynomials, Concrete, Complexity theory]
Efficient Implementation Of Graph Algorithms Using Contraction
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We define a graph problem which we refer to as the component merging problem. Versions of the problem appear as bottlenecks in various graph algorithms. We show how to solve an important special case of the problem.
[Algorithm design and analysis, Tree graphs, Merging, Data structures]
Computing on a free tree via complexity-preserving mappings
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
It appears that in a number of cases computing over free trees is no more difficult than computing over linear lists. In the arithmetic model, we have established a strict equivalence between the interval query problem and its generalization on a tree structure. In the reference machine model, the concept of efficiency is traditionally captured by the class of retrieval problems which can be solved in O(nP0LY LOG(n)) space and O(POLYLOG(n)) time. We have shown that in a number of examples this class is closed under transformations from lists to trees. Characterizing the set of problems and techniques for which this holds is an interesting open problem.
[Tree graphs, Mathematical model, Communication networks, Arithmetic]
A Fast Approximation Algorithm For Minimum Spanning Trees In K-Dimensional Space
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We study the problem of finding a minimum spanning tree on the complete graph on n points in Ek, with the weight of an edge between any two points being the distance between the two points under some distance metric. A fast algorithm, which finds an approximate minimum spanning tree with weight at most (1+&#x003F5;) times optimal O(nlogn((logn)k + log (&#x003F5;-1)(logn)k-1&#x003F5;-(k-1))) time, is developed for the L<sub>q</sub>, q =2,3, ..., distance metrics. Moreover, if the n points are assumed to be independently and uniformly distributed in the box [0,l]<sub>k, then the probability that the approximate minimum spanning tree found is an exact minimum spanning tree is shown to be (1 -o(l/n)).
[Computer science, Tree graphs, Clustering algorithms, Euclidean distance, Approximation algorithms, Extraterrestrial measurements]
A "Paradoxical" Solution To The Signature Problem
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
false
[chosen message attacks, Law, claw-free pairs of functions, cryptography, factoring, randomization, Public key, Seals, Forgery, Robustness, Polynomials, digital signatures, Cryptography, Digital signatures, Legal factors, authentication]
RSA/Rabin Bits are 1/2 + 1 / Poly (Log N) Secure
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We prove that RSA least significant bit is 1/2 + (1/[logc N]) secure, for any constant c (where N is the RSA modulus). This means that an adversary, given the ciphertext, cannot guess the least sigiiilicatnt bit of the plaintext with probability better than 1/2 + (1/[logc N]), unless he can break RSA.
[Utility programs, Information security, Public key cryptography]
Multiplication Of Polynomials Over The Ring Of Integers
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Let R be a ring, and let f(/spl alpha/), g(/spl alph/) /spl epsi/ R[/spl alpha/] be univariate polynomials over R of degree n. We Present an algorithm for computing the coefficients of the product f(/spl alpha/)G (/spl alpha/) by O (nlgn) multiplications. This algorithm is based on an algorithm for multiplying polynomials over the ring of integers, and does not depend on R. Also we prove that multiplying the third degree polynomials over the ripg of integers requires at least nine multiplications. This bound is tight.
[Computer science, Integral equations, Polynomials, Vectors, Galois fields]
Independent Unbiased Coin Flips From A Correlated Biased Source: A Finite State Markov Chain
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
von Neumann's trick for generating an absolutely unbiased coin from a biased one is this: 1. Toss the biased coin twice, getting 00, 01, 10, or 11. 2. If 00 or 11 occur, go back to step 1; else 3. Call 10 a H, 01 a T. Since p[H] = p[1]*p[0] = p[T], the output is unbiased. Example: 00 10 11 01 01 /spl I.oarr/ H T T. Peter Elias gives an algorithm to generate an independent unbiased sequence of Hs and Ts that nearly achieves the Entropy of the one-coin source. His algorithm is excellent, but certain difficulties arise in trying to use it (or the original von Neumann scheme) to generate bits in expected linear time from a Markov chain. In this paper, we return to the original one-coin von Neumann scheme, and show how to extend it to generate an independent unbiased sequence of Hs and Ts from any Markov chain in expected linear time. We give a right and wrong way to do this. Two algorithms A and B use the simple von Neumann trick on every state of the Markov chain. They differ in the time they choose to announce the coin flip. This timing is crucial.
[Upper bound, Entropy, Timing]
Constructing O(n Log n) Size Monotone Formulae For The k-th Elementary Symmetric Polynomial Of n Boolean Variables
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
In this paper, we construct formulae for the k-th elementary symmetry polynomial of n Boolean variables, using only conjunction and disjunction, which for fixed k are of size O(n log n), with the construction taking time polynomial in n. We also prove theorems involving n log n/spl middot/(polynomial in k) upper bounds on such formulae. Our methods involve solving the following combinatorial problem: for fixed k and any n construct a collection of r=O(log n) functions f/sub 1/,...,f/sub r/ from {1,...,n} to {1,...,K} such that any subset of {1,...,n} of order k is mapped 1-1 to {1,...,k} by at least one f/sub i/..
[Reed-Solomon codes, Linear code, Upper bound, Costs, Polynomials, Error correction codes]
An Efficient Algrithm To Find All 'Bidirectional' Edges Of An Undirected Graph
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
An efficient algorithm for the All-Bidirectional-Edges Problem is presented. The All-Bidirectional-Edges Problem is to find an edge-labelling of an undirected network, G = (V,E), with a source and a sink, such that an edge [u,v] /spl epsi/ E is labelled (u,v) or (v,u) (or both) depending on the existence of a (simple) path from the source to sink that visits the vertices u and v, in the order u,v or v,u, respectively. The algorithm presented works by partitioning the graph into a set of bridges and analyzing them recursively. The time complexity of the algorithm is shown to be O(\\E\\ . \\V\\). The problem arises naturally in the context of the simulation of in MOS transistor network, in which a transistor may operate as a unilateral or a bilateral device, depending on the voltages at its source and drain nodes. For efficient simulation, it is required to detect the set of transistors that may operate as bilateral devices. Also, this algorithm can be used in order to detect all the sneak paths in a network of pass transistor.
[Computer science, Circuit simulation, Bridge circuits, Voltage, Partitioning algorithms, Labeling, Joining processes, MOSFETs, Context modeling]
Shortest Paths In Euclidean Graphs
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We analyze a simple method for finding shortest paths in Euclidean graphs (where vertices are points in a Euclidean space and edge weights are distances between points). For many graph models, the running time of the algorithm to find the shortest path between a specified pair of vertices in a graph with V vertices and E edges is shown to be O(V) as compared with O (V log V + E) required by the classical (Dijkstra) algorithm.
[Computer science, Tree graphs, Circuits, Euclidean distance, Search problems]
Parallel Powering
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
A fast parallel computation for large powers of an integer module another integer is presented, assuming that the modulus has only small prime factors
[Concurrent computing, Interpolation, Computational modeling, Circuits, Tin, Polynomials, Galois fields, Parallel algorithms, Lagrangian functions, Arithmetic]
Complexity Measures For Public-Key Cryptosystems
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
The first part of this paper gives results about promise problems. A "promise problem" is a formulation of a partial decision problem that is useful for describing cracking problems for public-key cryptosystems (PKCS). We prove that every NP-hard promise problem is uniformly NP-hard, and we show that a number of results and a conjecture about promise problems are equivalent to separability assertions that are the natural analogues of well-known results in classical recursion theory. The conjecture, if it is true, implies nonexistence of PKCS having NP-hard cracking problems. The second part of the paper studies more appropriate measures for PKCS. Among the results obtained are the following: One-way functions exist if an only if P /spl ne/ U and one-way functions f such that range f /spl epsiv/ P exist if and only if U /spl cap/ co-U /spl ne/ P. It will allow that there exist PKCS that cannot be cracked in polynomial time (and that satisfy other reasonable assumptions) only if P /spl ne/ U.
[Computer science, Information security, Public key cryptography, Polynomials, Concrete]
Designing Systolic Algorithms Using Sequential Machines
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We offer a methodology for simplifying the design and analysis of systolic systems. Specifically, we give characterization of systolic arrays in terms of (single processor) sequential machines which are easier to analyze and to program. We give several examples to illustrate the design methodology. In particular, we show how systolic arrays can be easily designed to implement priority queues, integer bitwise multiplication, dynamic programming, etc. Because the designs are based on the sequential machine, the constructions we obtain are much simpler then those that have appeared in the literature. We also give some results concerning the properties and computational power (e.g., speed-up, hierarchy, etc.) of systolic arrays.
[Algorithm design and analysis, Concurrent computing, Computer science, Design methodology, Signal processing algorithms, Very large scale integration, Systolic arrays, Hardware, Dynamic programming, Power system modeling]
Nonlinearity Of Davenport-Schinzel Sequences And Of A Generalized Path Compression Scheme
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Davenport-Schinzel sequences are sequences that do not contain forbidden subsequences of alternating symbols. They arise in the computation of the envelope of a set of functions. We show that the maximal length of a Davenport-Schinzel sequence composed of n symbols is (n /spl alpha/(n)), where /spl alpha/ (n) is the functional inverse of Ackermann's function, and is thus very slow growing. This is achieved by establishing an equivalence between such sequences and generalized path compression schemes on rooted trees, and then by analyzing these schemes.
[Computational geometry, Differential equations, Polynomials]
A Polynomial Solution For Potato-Peeling And Other Polygon Inclusion And Enclosure Problems
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We give a finiteness criteria for the potato-peeling problem that asks for the largest convex Polygon ('Potato') contained inside a given simple polygon, answering a question of J. Goodman. This leads to a polynomial-time, solution of O(n/sup 9/log n). The techniques used turn out to be useful for other cases of what we call the polygon inclusion and enclosure problem. For instance, the largest perimeter potato can be found in O(n/sup 6/) time and finding the smallest k-gon enclosing a given polygon can be done in O(n/sup 3/log k) steps.
[Q measurement, Computational geometry, Algebra, Ferroelectric films, Nonvolatile memory, Computational modeling, Optimization methods, Random access memory, Hafnium, Polynomials]
A Semantic Characterization Of Full Abstraction For Typed Lambda Calculi
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Full abstraction is a well known issue in denotational semantics. For a special case of typed lambda calculus, PCF, Plotkin showed that the classical model consisting of domains of continuous functions is not fully abstract. Milner constructed a fully abstract model of typed lambda calculus syntactically. However, its precise relationship with the classical model was not clear, and hence it remained open whether a fully abstract model can be constructed which is related to the classical model in a pleasant way. In this paper we show that a fully abstract, extensional model of typed lambda calculus can be constructed as a homomorphic retraction of the classical model.
[Computer science, Aerospace electronics, Calculus, US Department of Defense, Contracts, Mathematical programming]
Very Fast Parallel Matrix and Polynomial Arithmetic
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We present very efficient arithmetic circuits for the computation of the determinants and inverse of band matrices and for interpolation and polynomial arithmetic over arbitrary ground fields. For input size n the circuits for band matrix inversion (for constant band width) over infinite fields, and the circuits for polynomial arithmetic over arbitrary fields, have optimal depth 0(log n) and polynomial size. The algorithm for band matrix inversion is based on an extension of the method of modification (from numerical analysis) which includes a formula for the determinant of a modified matrix.
[Circuit simulation, Optical wavelength conversion, Size measurement, Polynomials, Computer networks, Circuit testing, Arithmetic]
Space Searching For Intersecting Objects
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Determining or counting geometric objects that intersect another geometric query object is at the core of algorithmic problems in a number of applied areas of computer science. This article presents a family of space-efficient data structures that realize sublinear query time for points, line segments, lines and polygons in the plane, and points, line segments, plaraes, and polyhedra in three dimensions.
[Polynomials, Computational Intelligence Society]
A Characterization Of Probabilistic Inference
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Inductive Inference Machines (IlMs) attempt to identify functions given only input-output pairs of the functions. Probabilistic IlMs are defined, as is the probability that a probabilistic IlM identifies a function with respect to two common identification criteria: EX and BC. Let ID denote either of these criteria. Then ID/sub prob/(p) is the family of sets of functions U for which there is a probabilistic IlM identifying every f /spl epsi/ U with probability /spl ges/ p. It is shown that for all positive integers n, ID/sub prob/(1/n) is properly contained in ID/sub prob/(1/(n+1)), and that this discrete hierarchy is the "finest" possible. This hierarchy is related to others in the literature.
[Turing machines, Computational modeling, Frequency, Inference algorithms, Pattern recognition, Artificial intelligence]
Flipping Coins In Many Pockets (Byzantine Agreement On Uniformly Random Values)
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
It was recently shown by Michael Rabin that a sequence of random 0-1 values, prepared and distributed by a trusted "dealer," can be used to achieve Byzantine agreement in constant expected time in a network of processors. A natural question is whether it is possible to generate these values uniformly at random within the network. In this paper we present a cryptography based protocol for agreernent on a 0-1 randona value, if less than half of the processors are faulty. In fact the protocol allows uniform sampling from any finite set, and thus solves the problem of choosing a network leader uniformly at random. The protocol is usable both when all the communication is via "broadcast," in which case it needs three rounds of information exchange, and when each pair of processors communicate on a private line, in which case it needs 3t + 3 rounds, where t is the number of faulty proccssors. The protocol remains valid even if passive eavesdropping is allowed. On the other hand we show that no (probabilistic) protocol can achieve agreement on a fair coin in fewer phases then necessary for Byzantine agreement, and hence the "pre-dealt" nature of the random sequence required for Rabin's algorithm is crucial.
[Fault tolerance, Ethernet networks, Voting, Broadcasting, Sampling methods, Telecommunication network reliability, Cryptography, Resource management, Distributed computing, Cryptographic protocols]
Lower Bounds On Communication Complexity In Distributed Computer Networks
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We prove that for almost all boolean functions f , the conmunication complexity of f on a linear array with p+1 processors is approximately p times its commuication complexity on a system with two processors. We use this result to develop a technique for establishing lower bounds on communication complexity on general networks by simulating them on linear arrays. Using this technique, we derive optimal lower bounds for ranking, distinctness, uniqueness and triangle-detection problems on the ring. The application of this technique to meshes and trees yields nontrivial near optimal lower bounds on the communicaton complexity of ranking and distinctness problems on these networks.
[Intelligent networks, Coordinate measuring machines, Computer simulation, Electric variables measurement, Optical computing, Repeaters, Computer networks, Complexity theory, Distributed computing, Distributed algorithms]
Slowing Down Sorting Networks To Obtain Faster Sorting Algorithm
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Megiddo introduced a technique for using a parallel algorithm for one problem to construct an efficient serial algorithm for a second problem. We give a general method that trims a factor o f 0(logn) time (or more) for many applications of this technique.
[Art, Polynomials, Sorting]
Generating Quasi-Random Sequences From Slightly-Random Sources
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Several applications require truly random bit sequences, whereas physical sources of randomness are at best imperfect. We consider a general model for these slightly-random sources (e,g. zener diodes), and show how to convert their output into 'random looking ' sequences, which we call quasi -random. We show that quasi-random sequences are indistinguishable from truly random ones in a strong sense. This enables us to prove that quasi-random sequences can be used in place of truly random ones for applications such as seeds for pseudo-random number generators, randomizing algorithms, and stochastic simulation experiments.
[Graphics, Protocols, Stochastic processes, Markov processes, Frequency, Cryptography, History, Diodes, Random number generation, Random sequences]
An Augmenting Path Algorithm For The Parity Problem On Linear Matroids
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
The matroid parity problem is a generalization of matroid intersection and general graph matching (and hence network flow, degree-constrained subgraphs, etc.). A polynomial algorithm for linear matroids was presented by Lovasz. This paper presents an algorithm that uses time 0(mn/sup 3/), where m is the number of elements and n is the rank; for the spanning tree parity problem the time 0(mn/sup 2/). The algorithm is based on the method of augmenting paths used in the algorithms for all subcases of the problem.
[Computer science, Graphics, Tree graphs, Processor scheduling, Polynomials, Scheduling algorithm]
Polymorphic Arrays: A Novel VLSI Layout For Systolic Computers
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
This paper proposes a novel architecture for massively parallel systolic computers, which is based on results from lattice theory. In the proposed architecture, each processor is connected to four other processors via constant-lenght wires in an regular borderless pattern. The mapping of processes to processors is continuous, and the architecture guarantees exceptional load uniformity for rectangular process arrays of arbitrary sizes. In addition, no timesharing is ever required when the ration of processes to processors is smaller than 1//spl radic/5.
[Algorithm design and analysis, Concurrent computing, Costs, Shape, Wires, Lattices, Computer architecture, Very large scale integration, Mathematics, Wafer scale integration]
On The Limits To Speed Up Parallel Machines By Large Hardware And Unbounded Communication
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Lower bounds for sequential and parallel random access machines (RAM's, WRAM's) and distributed systems of RAM's (DRAM's) are proved. We show that, when p processors instead of one are available, the computation of certain functions cannot be speeded up by a factor p but only by a factor 0 (log(p)). For DRAM's with communication graph of degree c a maximal speedup 0 (log(c)) can be achieved for these problems. We apply these results to testing the solvability of linear diophantine equations. This generalizes a lower bond of Yao for parallel computation trees. Improving results of Dobkin/Lipton and Klein/Meyer auf der Heide, we establish large lower bounds for the above problem on RAM's. Finnaly we prove that at least log (n) + 1 steps are necessary for computing the sum of n integers by a WRAM regardless of the number of processors and the solution of write conflicts.
[Concurrent computing, Tree graphs, Computational modeling, Random access memory, Parallel machines, Hardware, NP-complete problem, Equations, Testing, Sorting]
Log Depth Circuits For Division And Related Problems
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We present optimal depth Boolean circuits (depth O(log n)) for integer division, powering, and multiple products. We also show that these three problems are of equivalent uniform depth and space complexity. In addition, we describe an algorithm for testing divisibility that is optimal for both depth and space.
[Computer science, Interpolation, Sufficient conditions, Turing machines, Polynomials, Hardware, Circuit testing]
Eigenvalues, Expanders And Superconcentrators
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Explicit construction of families of linear expanders and superconcentrators is relevant to theoretical computer science in several ways. There is essentially only one known explicit construction. Here we show a correspondence between the eigenvalues of the adjacency matrix of a graph and its expansion properties, and combine it with results on Group Representations to obtain many new examples of families of linear expanders. We also obtain better expanders than those previously known and use them to construct explicitly n-superconcentrators with 157.4 n edges, much less than the previous most economical construction.
[Computer science, Lattices, Eigenvalues and eigenfunctions, Graph theory, Mathematics, Bipartite graph, Sorting]
Sparse Oracles And Uniform Complexity Classes
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We show that several questions about the polynomial-time hierarchy can be answered by answering their counterparts for the polynomial-time hierarchy relativized to an arbitrary sparse oracle set. For each of these questions, the answer will be the same for the hierarchy relativized to S/sub 1/ as it will be for the hierarchy relativized to S/sub 2/ for any choice of S/sub 1/ and S/sub 2/ that are sparse sets, including the choice of S/sub 1/ being empty and S/sub 2/ being nonempty but sparse.
[Computer science, Polynomials, Mathematics, Cultural differences]
The Average-Case Analysis of Some On-Line Algorithms for Bin Packing
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
In this paper we give tighter bounds than were previously known for the performance of the bin packing algorithms Bets Fit and First Fit when the inputs are uniformly distributed on [0,1]. We also give a general lower bound for the performance of any on-line bin packing algorithm. These results are proven by analyzing problems concerning matching random points in a unit square. We give a new lower bound for upward right matching and grid matching.
[Algorithm design and analysis, Large Hadron Collider, Light rail systems, Random sequences]
Dynamic Segment Intersection Search With Applications
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
In this paper, we consider two restricted types of dynamic orthogonal segment intersection search problems. One is the problem in which the underlying set is updated only by insertions. In the other, the set is updated only by deletions. We show that an intermixed sequence of O(n) queries and updates in both problems can be executed on-line in O(n log n+K) time and O(n log n) space, where K is the total number of reported intersections. Our algorithms utilize set-union and set-splitting algorithms. Especially, we present a linear-time algorithm for the incremental set-splitting problem, and use it for the problem in which only insertions are allowed. The data structures developed here give new better solutions for various geometric problems on orthogonal segments. We give a paradigm of solving those geometric problems by combining graph algorithms with these data structures, and describe a variety of applications.
[Instruments, Search problems, Data structures, Physics]
Probabilistic Communication Complexity
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We study (unbounded error) probabilistic communication complexity. Our new results include -one way and two complexities differ by at most 1 - certain functions like equality and the verification of Hamming distance have upper bounds that are considerably better than their counterparts in deterministic, nondeterministic, or bounded error probabilistic model - there exists a function which requires /spl Omega/(logn) information transfer. As an application, we prove that a certain language requires /spl Omega/(nlogn) time to be recognized by a 1-tape (unbounded error) probabilistic Turing machine. This bound is optimal. (Previous lower bound results [Yao 1] require acceptance by bounded error computation. We believe that this is the first nontrivial lower bound on the time required by unrestricted probabilistic Turing machines.
[Computer science, Protocols, Upper bound, Power measurement, Hamming distance, Computer errors, Probability distribution, Complexity theory, Concatenated codes, Distributed computing]
Efficient And Secure Pseudo-Random Number Generation
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Cryptographically secure pseudo-random number generators known so far suffer from the handicap of being inefficient; the most efficient ones can generate only one bit on each modular multiplication (n/sup 2/ steps). Blum, Blum and Shub ask the open problem of outputting even two bits securely. We state a simple condition, the XOR-Condition, and show that any generator satisfying this condition can output logn bits on each multiplication. We also show that the logn least significant bits of RSA, Rabin's Scheme, and the x/sup 2/ mod N generator satisfy boolean predicates of these bits are secure. Furthermore, we strengthen the security of the x/sup 2/ mod N generator, which being a Trapdoor Generator, has several applications, by proving it as hard as Factoring.
[Computer science, Polynomials, Cryptography, History, Probes, Computer security, Random number generation, Testing]
A Lower Bound For Probabilistic Algorithms For Finite State Machines
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Freivalds recently reported a construction of a 2-way probabilistic finite automaton M that recognizes the set {a/sup m/b /sup m/ : m /spl ges/ 1} with arbitrarily small probability of error. This result implies that probabilistic machines of this type are more powerful than their deterministic, nondeterministic, and alternating counterparts. Freivalds' construction has a negative feature: the automation M runs in /spl Omega/ (2/sup n/2/n) expected time in the worst case on inputs of length n. We show that it is impossible to do significantly better. Specifically, no 2-way probabilistic finite automaton that runs in n/sup O (1)/ expected time recognizes {a/sup m/b/sup m/ : m /spl ges/ 1} with probability of error bounded away from 1/2. In passing we derive results on the densities of regular sets, the fine structure of Freivalds' construction, and the behavior of random walks controlled by Markov chains.
[Automata, Character generation, Automatic control, Magnetic heads, DC generators]
An Implicit Data Structure For The Dictionary Problem That Runs In Polylog Time
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We introduce a data structure that requires only one pointer for every k data values and permits the operations search, insert and delete to be performed in 0 (k log n) time. This structure is used to develop another that requires no pointers and supports insert, delete and search in 0 (log2 n) time.
[Computer science, Dictionaries, Costs, Data structures, Queueing analysis, Counting circuits]
Parallel Communication With Limited Buffers
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Currently known parallel communication schemes allow n nodes interconnected by arcs (in such a way that each node meets only a fixed number of arcs) to transmit n packets according to an arbitrary permutation in such a way that (1) only one packet is sent over a given arc at any step, (2) at most 0(log n) packets reside at a given node at any time and (3) with high probability, each packet arrives at its destination within 0(log n) steps. We present and analyze a new parallel communication scheme that ensures that at most a fixed number of packets reside at a given node at any time.
[Concurrent computing, Computational modeling, Laboratories, Computer networks, Timing, Random variables, Distributed computing, Delay, Sorting]
A Model-Theoretic Analysis Of Knowledge: Preliminary Report
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Understanding knowledge is a fundamental issue in many disciplines. In computer science, knowledge arises not only in the obvious contexts (such as knowledge-based systems), but also in distributed systems (where the goal is to have each processor "know" something, as in Byzantine agreement). A general semantic model of knowledge is introduced, to allow reasoning about statements such as "He knows that I know whether or not she knows whether or not it is raining." This approach more naturally models a state of knowledge than previous proposals (including Kripke structures). Using this notion of model, a model theory for knowledge is developed. This theory enables one to interpret such notions as a "finite amount of information" and "common knowledge" in different contexts.
[Computer science, Laboratories, Knowledge based systems, Pipelines, Distributed databases, Cryptography, Artificial intelligence, Distributed computing, Cryptographic protocols]
How To Construct Randolli Functions
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
This paper develops a constructive theory of randomness for functions based on computational complexity. We present a deterministic polynomial-time algorithm that transforms pairs (g,r), where g is any one-way (in a very weak sense) function and r is a random k-bit string, to polynomial-time computable functions f/sub r/:{1,..., 2/sup k} /spl I.oarr/ {1, ..., 2/sup k/}. These f/sub r/'s cannot be distinguished from random functions by any probabilistic polynomial time algorithm that asks and receives the value of a function at arguments of its choice. The result has applications in cryptography, random constructions and complexity theory.
[Polynomials, Reflection, Cryptography, Computational complexity, Indexing]
Embedding Planar Graphs In Seven Pages
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
This paper investigates the problem of embedding planar graphs in books of few pages. An efficient algorithm for embedding a planar graph in a book establishes an upper bound of seven pages for any planar graph. This disproves a conjecture of Bernhart and Kainen that the pagenumber of a planar graph can be arbitrarily large. It is also shown that the stellations of K/sub 3/ have pagenumber three, the best possible.
[Solids, Books]
Applications Of Ramsey's Theorem To Decision Trees Complexity
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Combinatorial techniques for extending lower bounds results for decision trees to general types of queries are presented. We consider problems, which we call order invariant, that are defined by simple inequalities between inputs. A decision tree is called k-bounded if each query depends on at most k variables. We make no further assumptions on the type of queries. We prove that we can replace the queries of any k-bounded decision tree that solves an order invariant problem over a large enough input dornain with k-bounded queries whose outcome depends only on the relative order of the inputs. As a consequence, all existing lower bounds for comparison based algorithms are valid for general k-bounded decision trees, where k is a constant. We also prove an /spl Omega/(n log n) lower bound for the element uniqueness problem and several other problems for any k-bounded decision tree, such that k - )(n/sup c/) and c < 1/2. This lower bound is tight since that there exist n/sup 1/2/-bounded decision trees of complexity 0(n) that solve the element uniqueness problem. All the lower bounds mentioned above are shown to hold for nondeterministic and probabilistic decision trees as well.
[Computer science, Heart, Merging, Decision trees, Application software]
Fishspear: A Priority Queue Algorithm
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
The Fishspear priority queue algorithm is presented and analyzed. Fishspear makes fewer than 80% as many comparisons as heaps in the worst case, and its relative performance is even better in rnany common situations. The code itself embodies an unusual recursive structure which permits highly dynamic and data-dependent execution. Fishspear also differs from heaps in that it can be implemented efficiently using sequential storage such as stacks or tapes, making it possibly attractive for implementation of very large queues on paged memory systems. (Details of the implementation are deferred to the full paper.)
[Algorithm design and analysis, Computer science, Computational geometry, Costs, Random access memory, Binary trees, Application software, Queueing analysis, Contracts, Sorting]
The Multi-Tree Approach To Reliability In Distributed Networks
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Consider a network of asynchronous processors communicating by sending messages over unreliable lines. There are many advantages to restrict all communications to a spanning tree. To overcome the possible failure of k<k edges, we describe a communication protocol which uses k rooted spanning trees having the property that for every vertex v the paths from v to the root are edge-disjoint. A linear algorithm to find two such trees in a 2 edge-connected graph is described. This algorithm has a distributed version which finds the two trees even when a single edge fails during their construction. Finally, a method is given to transform centralized algorithms to distributed, reliable and efficient ones.
[Intelligent networks, Protocols, Tree graphs, Message passing, Resists, Broadcasting, Cities and towns, Routing, Telecommunication network reliability]
Linear Congruential Generators Do Not Produce Random Sequences
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
One of the most popular and fast methods of generating "random" sequence are linear congruential generators. This paper discusses the predictability of the sequence given only a constant proportion /spl alpha/ of the leading bits of the first few numbers generated. We show that the rest of the sequence is predictable in polynomial time, almost always, provided /spl alpha/ > 2/5.
[Computer science, Educational institutions, Polynomials, Cryptography, Random number generation, Random sequences, Testing]
A Communication-Time Tradeoff
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
We show a nontrivial tradeoff between the communication c and time t required to compute a collection of values whose dependencies form a grid, i.e., value (i,j) depends on the values (i-1,j) and (i,j-1). No matter how we share the responsibility for computing the nodes of the n x n grid among processors, the law ct = /spl Omega/(n/sup 3/) must hold. Further, there must be a single path through the grid along which there are d communication steps, where dt = /spl Omega/(n/sup 2/). Depending on the machine organization, either law may be the more significant.
[Processor scheduling, Binary trees, Length measurement, Time factors]
Fibonacci Heaps And Their Uses In Improved Network Optimization Algorithms
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
In this paper we develop a new data structure for implementing heaps (priority queues). Our structure, Fibonacci heaps (abbreviated F-heaps), extends the binomial queues proposed by Vuillemin and studied further by Brown. F-heaps support arbitrary deletion from an n-item heap in 0(log n) amortized time and all other standard heap operations in 0(1) amortized time. Using F-heaps we are able to obtain improved running times for several network optimization algorithms.
[Shortest path problem, Intelligent networks, Tree graphs, Algorithms, Hydrogen, Data structures]
Interactive Data Compression
25th Annual Symposium onFoundations of Computer Science, 1984.
None
1984
Let X and Y be two random variables with probability distribution p(x,y), joint entropy H(X,Y) and conditional entropies H(X \\ Y) and H(Y \\ X) . Person P/sub x/ knows X and person P/sub Y/ knows Y. They communicate over a noiseless two-way channel so that both know X and Y. It is proved that, on the average, at least H(X \\ Y) + H(Y \\ X) bits must be exchanged and that H(X,Y) + 2 bits are sufficient. If p(x.y) > 0 for all (x.y), then at least H(X,Y) bits must be communicated on the average. However, if p (x,y) is uniform over its support set, the average number of bits needed is close to H(X \\ Y) + H (Y \\ X). Randomized protocols can reduce the amount of communication considerably but only when some probability of error is acceptable.
[Upper bound, Protocols, Data compression, Entropy, Random variables, Information systems]
Foreword
26th Annual Symposium on Foundations of Computer Science
None
1985
Presents the introductory welcome message from the conference proceedings.
[]
Separating the polynomial-time hierarchy by oracles
26th Annual Symposium on Foundations of Computer Science
None
1985
We present exponential lower bounds on the size of depth-k Boolean circuits for computing certain functions. These results imply that there exists an oracle set A such that, relative to A, all the levels in the polynomial-time hierarchy are distinct, i.e., &#x03A3;kP,A is properly contained in &#x03A3;k+1P,A for all k.
[Frequency selective surfaces, Computer science, Circuits, Polynomials, Complexity theory, Computational complexity, Erbium]
Deterministic simulation of probabilistic constant depth circuits
26th Annual Symposium on Foundations of Computer Science
None
1985
We explicitly construct, for every integer n and &#x03B5; &#x226B; 0, a family of functions (psuedo-random bit generators) fn,&#x03B5;:{0,1}n&#x03B5; &#x02192; {0,1}n with the following property: for a random seed, the pseudorandom output "looks random" to any polynomial size, constant depth, unbounded fan-in circuit. Moreover, the functions fn,&#x03B5; themselves can be computed by uniform polynomial size, constant depth circuits. Some (interrelated) consequences of this result are given below. 1) Deterministic simulation of probabilistic algorithms. The constant depth analogues of the probabilistic complexity classes RP and BPP are contained in the deterministic complexity classes DSPACE(n&#x03B5;) and DTIME(2n&#x03B5;) for any &#x03B5; &#x226B; 0. 2) Making probabilistic constructions deterministic. Some probablistic constructions of structures that elude explicit constructions can be simulated in the above complexity classes. 3) Approximate counting. The number of satisfying assignments to a (CNF or DNF) formula, if not too small, can be arbitrarily approximated in DSPACE(n&#x03B5;) and DTIME(2n&#x03B5;), for any &#x03B5; &#x226B; 0. We also present two results for the special case of depth 2 circuits. They deal, respectively, with finding a satisfying assignment and approximately counting the number of assignments. For example, for 3-CNF formulas with a fixed fraction of satisfying assignmemts, both tasks can be performed in polynomial time!
[Frequency selective surfaces, Algorithm design and analysis, Analytical models, Circuit simulation, Computational modeling, Buildings, Polynomials, Cryptography, Parallel algorithms, Random number generation]
Amplification of probabilistic boolean formulas
26th Annual Symposium on Foundations of Computer Science
None
1985
The amplification of probabilistic Boolean formulas refers to combining independent copies of such formulas to reduce the error probability. Les Valiant used the amplification method to produce monotone Boolean formulas of size O(n5.3) for the majority function of n variables. In this paper we show that the amount of amplification that Valiant obtained is optimal. In addition, using the amplification method we give an O(k4.3 n log n) upper bound for the size of monotone formulas computing the kth threshold function of n variables.
[Computer science, Upper bound, Boolean functions, Error probability, Voting, Circuits, Laboratories, Polynomials, Rail to rail outputs, Petroleum]
On networks of noisy gates
26th Annual Symposium on Foundations of Computer Science
None
1985
We show that many Boolean functions (including, in a certain sense, "almost all" Boolean functions) have the property that the number of noisy gates needed to compute them differs from the number of noiseless gates by at most a constant factor. This may be contrasted with results of von Neumann, Dobrushin and Ortyukov to the effect that (1) for every Boolean function, the number of noisy gates needed is larger by at most a logarithmic factor, and (2) for some Boolean functions, it is larger by at least a logarithmic factor.
[Boolean functions, Error probability, Computational modeling, Stochastic processes, Reliability theory, Computer networks]
How easy is local search?
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Pathology, Computational modeling, Simulated annealing, Traveling salesman problems, Cost function, Linear programming, Polynomials, Explosions, Partitioning algorithms, Ellipsoids]
Identification is easier than decoding
26th Annual Symposium on Foundations of Computer Science
None
1985
Several questions related to the complexity of communication over channels with noise are addressed. We compare some of our results to wellknown results in information theory. In particular we compare the following two problems. Assuming that the communication channel between two processors P1 and P2 makes an error with probability &#x03B5;&#x226B;0, the identification problem is to determine whether P1 and P2 have the same n-bit integer. The decoding problem is for P2 to determine the n-bit integer of P1. For the latter problem we show that given any arbitrarily large constant &#x03BB;&#x226B;0, there exists an &#x03B5;, 0&#x226A;&#x03B5;&#x226A;1/2, for which no scheme requiring less than &#x03BB;n bits of communication can guarantee (for large n) any bound q&#x226A;1 on the error probability. On the other hand, given any arbitrarily small constant &#x03B3;&#x226B;0 and any &#x03B5;, 0&#x226A;&#x03B5;&#x226A;1/2, the identification problem can be solved with (1+&#x03B3;)n bits of (one-way) communication with an error probability bounded by c2-&#x03B1;n, where c and &#x03B1; are positive constants. These techniques are extended to other problems, and a one-bit output Boolean function is shown to exhibit a similar behavior to that of the decoding problem regardless of how the input bits are partitioned among the two processors.
[Concurrent computing, Boolean functions, Error probability, Channel capacity, Communication channels, Educational institutions, Decoding, Complexity theory, Distributed computing, Information theory]
Simulating two pushdown stores by one tape in O(n1.5v) time
26th Annual Symposium on Foundations of Computer Science
None
1985
Based on two graph separator theorems, we present two unexpected upper bounds and resolve several open problems for on-line computations. (1) 1 tape nondeterministic machines can simulate 2 pushdown stores in time O(n1.5&#x0221A;logn) (true for both on-line and off-line machines). Together with the &#x03A9;(n1.5/&#x0221A;logn) lower bound, this solves the open problem 1 in [DGPR] for the 1 tape vs. 2 pushdown case. It also disproves the commonly conjectured &#x03A9;(n2) lower bound. (2) The languages defined by Maass and Freivalds, aimed to obtain optimal lower bound for 1 tape nondeterministic machines, can be accepted in O(n2loglogn&#x0221A;logn) and O(n1.5&#x0221A;logn) time by a 1 tape TM, respectively. (3) 3 pushdown stores are better than 2 pushdown stores. This answers a rather old open problem by Book and Greibach, and Duris and Galil. An &#x03A9;(n4/3/loge n) lower bound is also obtained. (4) 1 tape can nondeterministically simulate 1 queue in O(n1.5/&#x0221A;logn) time. This disproves the conjectured &#x03A9;(n2) lower bound. Also 1 queue can simulate 2 pushdowns in time O(n1.5&#x0221A;logn).
[Computer science, Upper bound, Turing machines, Computational modeling, Computer simulation, Particle separators, Magnetic heads, Books, Computational complexity]
Nondeterministic versus probabilistic linear search algorithms
26th Annual Symposium on Foundations of Computer Science
None
1985
The "component counting lower bound" known for deterministic linear search algorithms (LSA's) also holds for their probabilistic versions (PLSA's) for many problems, even if two-sided error is allowed, and if one does not charge for probabilistic choice. This implies lower bounds on PLSA's for e.g. the element distinctness problem (n log n) or the knapsack problem (n2). These results yield the first separations between probabilistic and non-deterministic LSA's, because the above problems are non-deterministically much easier. Previous lower bounds for PLSA's either only worked for one-sided error "on the nice side\
[Runtime, Computational modeling, Input variables, Laboratories, Polynomials, Error correction, Counting circuits]
The complexity of facets resolved
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Face recognition, Circuits, Optimization methods, Traveling salesman problems, Cost function, Polynomials, Impedance, Combinatorial mathematics]
Using dual approximation algorithms for scheduling problems: Theoretical and practical results
26th Annual Symposium on Foundations of Computer Science
None
1985
The problem of scheduling a set of n jobs on m identical machines so as to minimize the makespan time is perhaps the most well-studied problem in the theory of approximation algorithms for NP-hard optimization problems. In this paper we present the strongest possible type of result for this problem, a polynomial approximation scheme. More precisely, for each &#x03B5;, we give an algorithm that runs in time O((n/&#x03B5;)1/&#x03B5;2) and has relative error at most &#x03B5;. For algorithms that are polynomial in n and m, the strongest previously-known result was that the MULTIFIT algorithm delivers a solution with no worse than 20&#x025; relative error. In addition, we present a refinement of our scheme in the case where the performance guarantee is equal to that of MUL-TIFIT, that yields an algorithm that is both more efficient and easier to analyze than MULTIFIT. In this case, in order to guarantee a maximum relative error of 1/5+2-k, the algorithm runs in O(n(k+logn)) time. The scheme is based on a new approach to constructing approximation algorithms, which we call dual approximation algorithms, where the aim is find superoptimal, but infeasible solutions, and the performance is measured by the degree of infeasibility allowed. This notion should find wide applicability in its own right, and should be considered for any optimization problem where traditional approximation algorithms have been particularly elusive.
[Algorithm design and analysis, Process design, Additives, Optimized production technology, Approximation algorithms, Polynomials, Job design, Performance analysis, Scheduling algorithm, Design optimization]
A scaling algorithm for weighted matching on general graphs
26th Annual Symposium on Foundations of Computer Science
None
1985
This paper presents an algorithm for maximum matching on general graphs with integral edge weights, running in time O(n3/4m lg N), where n, m and N are the number of vertices, number of edges, and largest edge weight magnitude, respectively. The best previous bound is O(n(mlg lg lgd n + n lg n)) where d is the density of the graph. The algorithm finds augmenting paths in batches by scaling the weights. The algorithm extends to degree-constrained subgraphs and hence to shortest paths on undirected graphs, the Chinese postman problem and finding a maximum cut of a planar graph. It speeds up Christofides' travelling salesman approximation algorithm from O(n3) to O(n2.75 lg n). A list splitting problem that arises in Edmonds' matching algorithm is solved in O(m&#x03B1;(m,n)) time, where m is the number of operations on a universe of n elements; the list splitting algorithm does not use set merging. Applications are given to update problems for red-green matching, the cardinality Chinese postman problem and the maximum cardinality plane cut problem; also to the all-pairs shortest paths problem on undirected graphs with lengths plus or minus one.
[Computer science, Shortest path problem, Computational geometry, Merging, Very large scale integration, Approximation algorithms, Data structures, Routing, Polynomials, Structural shells]
An all pairs shortest path algorithm with expected running time O(n 2logn)
26th Annual Symposium on Foundations of Computer Science
None
1985
An algorithm is described that solves the all pairs shortest path problem for a nonnegatively weighted graph. The algorithm has an average requirement on quite general classes of random graphs of O(n2logn) time, where n is the number of vertices in the graph.
[Computer science, Shortest path problem, Algorithm design and analysis, Information science, Upper bound, Cost function, Data structures]
Recognizing circle graphs in polynomial time
26th Annual Symposium on Foundations of Computer Science
None
1985
Our main result is a polynomialtime algorithm for deciding whether a given graph is a circle graph, that is, the intersection graph of a set of chords on a circle. Our algorithm utilizes two new graph-theoretic results, regarding necessary induced subgraphs of graphs having neither articulation points nor similar pairs of vertices.
[Computer science, Industrial engineering, Polynomials, Iterative algorithms, Reflection]
Why certain subgraph computations requite only linear time
26th Annual Symposium on Foundations of Computer Science
None
1985
A general problem in computational graph theory is that of finding an optimal subgraph H of a given weighted graph G. The matching problem (which is easy) and the traveling salesman problem (which is not) are well known examples of this general problem. In the literature one can also find a variety of ad hoc algorithms for solving certain special cases in linear time. We present a general methodology for constructing linear time algorithms in the case that the graph G is defined by certain rules of composition (as are trees, series parallel graphs, and outerplanar graphs) and the desired subgraph H satisfies a "regular" property (such as independence or matching). This methodology is applied to obtain a linear time algorithm for computing the irredundance number of a tree, a problem for which no polynomial time algorithm was previously known.
[Steiner trees, Computer science, Tree graphs, Traveling salesman problems, Polynomials, Graph theory]
Efficient string matching in the presence of errors
26th Annual Symposium on Foundations of Computer Science
None
1985
Consider the string matching problem where differences between characters of the pattern and characters of the text are allowed. Each difference is due to either a mismatch between a character of the text and a character of the pattern or a superfluous character in the text or a superfluous character in the pattern. Given a text of length n, a pattern of length m and an integer k, we present an algorithm for finding all occurrences of the pattern in the text, each with at most k differences. The algorithm runs in O(m2 + k2n) time. Given the same input we also present an algorithm for finding all occurrences of the pattern in the text, each with at most k mismatches (superfluous characters in either the text or the pattern are not allowed). This algorithm runs in O(k(m logm + n)) time.
[Computer science, Tiles, Computer errors, Pattern matching, Contracts]
The least weight subsequence problem
26th Annual Symposium on Foundations of Computer Science
None
1985
The least weight subsequence (LWS) problem is introduced, and is shown to be equivalent to the classic minimum path problem for directed graphs. A special case of the LWS problem is shown to be solvable in O(n log n) time generally and, for certain weight functions, in linear time. A number of applications are given, including an optimum paragraph formation problem and the problem of finding a minimum height B-tree, whose solutions realize improvement in asymptotic time complexity.
[Instruction sets, Heuristic algorithms, Dynamic programming]
Motion planning in the presence of moving obstacles
26th Annual Symposium on Foundations of Computer Science
None
1985
This paper investigates the computational complexity of planning the motion of a body B in 2-D or 3-D space, so as to avoid collision with moving obstacles of known, easily computed, trajectories. Dynamic movement problems are of fundamental importance to robotics, but their computational complexity has not previously been investigated. We provide evidence that the 3-D dynamic movement problem is intractable even if B has only a constant number of degrees of freedom of movement. In particular, we prove the problem is PSPACE-hard if B is given a velocity modulus bound on its movements and is NP hard even if B has no velocity modulus bound, where in both cases B has 6 degrees of freedom. To prove these results we use a unique method of simulation of a Turing machine which uses time to encode configurations (whereas previous lower bound proofs in robotics used the system position to encode configurations and so required unbounded number of degrees of freedom). We also investigate a natural class of dynamic problems which we call asteroid avoidance problems: B, the object we wish to move, is a convex polyhedron which is free to move by translation with bounded velocity modulus, and the polyhedral obstacles have known translational trajectories but cannot rotate. This problem has many applications to robot, automobile, and aircraft collision avoidance. Our main positive results are polynomial time algorithms for the 2-D asteroid avoidance problem with bounded number of obstacles as well as single exponential time and nO(log n) space algorithms for the 3-D asteroid avoidance problem with an unbounded number of obstacles. Our techniques for solving these asteroid avoidance problems are novel in the sense that they are completely unrelated to previous algorithms for planning movement in the case of static obstacles. We also give some additional positive results for various other dynamic movers problems, and in particular give polynomial time algorithms for the case in which B has no velocity bounds and the movements of obstacles are algebraic in space-time.
[Motion planning, Turing machines, Polynomials, Trajectory, Automobiles, Vehicle dynamics, Computational complexity, Aircraft, Robots, Orbital robotics]
Visibility-polygon search and euclidean shortest paths
26th Annual Symposium on Foundations of Computer Science
None
1985
Consider a collection of disjoint polygons in the plane containing a total of n edges. We show how to build, in O(n2) time and space, a data structure from which in O(n) time we can compute the visibility polygon of a given point with respect to the polygon collection. As an application of this structure, the visibility graph of the given polygons can be constructed in O(n2) time and space. This implies that the shortest path that connects two points in the plane and avoids the polygons in our collection can be computed in O(n2) time, improving earlier O(n2 log n) results.
[Computational geometry, Instruments, Computer graphics, Data structures, Time sharing computer systems, Collision avoidance, Physics, Robots]
Slimming down search structures: A functional approach to algorithm design
26th Annual Symposium on Foundations of Computer Science
None
1985
We establish new upper bounds on the complexity of several "rectangle" problems. Our results include, for instance, optimal algorithms for range counting and rectangle searching in two dimensions. These involve linear space implementations of range trees and segment trees. The algorithms we give are simple and practical; they can be dynamized and taken into higher dimensions. Also of interest is the nonstandard approach which we follow to obtain these results: it involves transforming data structures on the basis of functional specifications.
[Algorithm design and analysis, Computer science, Process design, Upper bound, Shape, Buildings, Data structures]
The complexity of recognizing polyhedral scenes
26th Annual Symposium on Foundations of Computer Science
None
1985
Given a drawing of straight lines on the plane, we wish to decide whether it is the projection of the visible part of a set of opaque polyhedra. Although there is an extensive literature and reports on empirically succesful algorithm: for this problem, there has been no definite result concerning its complexity. In this paper we show that, rather surprisingly, this problem is NP-complete. This is true even in the relatively simple case of trihedral scenes (no four planes share a point) without shadows or cracks. Despite this negative result, we present a fast algorithm for the important special case of orthohedral scenes (all planes are perpendicular to one of the three axes) with a fixed number of "possible" objects.
[Computer vision, Law, Layout, Solids, Linear programming, Polynomials, Labeling, Legal factors]
Multi-layer grid embeddings
26th Annual Symposium on Foundations of Computer Science
None
1985
In this paper we propose two new multi-layer grid models for VLSI layout, both of which take into account the number of contact cuts used. For the first model in which nodes "exist" only on one layer, we prove a tight area x (number of contact cuts) = &#x0398;(n2) trade-off for embedding any degree 4 n-node planar graph in two layers. For the second model in which nodes "exist" simultaneously on all layers, we prove a number of bounds on the area needed to embed graphs using no contact cuts. For example we prove that any n-node graph which is the union of two planar subgraphs can be embedded on two layers in O(n2) area without contact cuts. This bound is tight even if more layers and an unbounded number of contact cuts are allowed. We also show that planar graphs of bounded degree can be embedded on two layers in O(n1.6) area without contact cuts. These results use some interesting new results on embedding graphs in a single layer. In particular we give an O(n2) area embedding of planar graphs such that each edge makes a constant number of turns, and each exterior vertex has a path to the perimeter of the grid making a constant number of turns. We also prove a tight &#x03A9;(n3) lower bound on the area of grid n-permutation networks.
[Fabrication, Semiconductor device measurement, Costs, Printed circuits, Laboratories, Area measurement, Very large scale integration, Length measurement, Wire, Circuit faults]
Area penalty for sublinear signal propagation delay on chip
26th Annual Symposium on Foundations of Computer Science
None
1985
Sublinear signal propagation delay in VLSI circuits carries a far greater penalty in wire area than is commonly realized. Therefore, the global complexity of VLSI circuits is more layout dependent than previously thought. This effect will be truly pronounced in the emerging wafer scale integration technology. We establish lower bounds on the trade-off between sublinear signalling speed and layout area for the implementation of a complete binary tree in VLSI. In particular, sublinear delay can only be realized at the cost of superlinear area. Designs with equal length wires can either not be laid out at all, viz. for logarithmic delay, or require such long wires in the case of radical delay (i.e., rth root of the wire length) that the aimed for gain in speed is cancelled. Also for wire length distributions commonly occurring on chip it appears that the requirements for sublinear signal propagation delay tend to cancel the gain.
[Computer science, Costs, Upper bound, Binary trees, Very large scale integration, Mathematics, Manufacturing, Wire, Propagation delay, Driver circuits]
On information flow and sorting : New upper and lower bounds for VLSI circuits
26th Annual Symposium on Foundations of Computer Science
None
1985
This work comprises two parts: lower bounds and upper bounds in VLSI circuits. The upper bounds are for the sorting problem: we describe a large number of constructions for sorting N numbers in the range [0,M] for the standard VLSI bit model. Among other results, we attain: &#x02022; VLSI sorter constructions that are within a constant factor of optimal size for almost all number ranges M (including M = N), and running times T. &#x02022; A fundamentally new merging network for sorting numbers in a bit model. &#x02022; New organizational approaches for optimal tuning of merging networks and the proper management of data flow. The lower bounds apply to a variety of problems. We present two new techniques for establishing lower bounds on the information flow in VLSI circuits. They are: &#x02022; An averaging technique, which is easy to apply to a variety of problems, including a long standing question regarding the AT2 complexity for sorting. &#x02022; A technique for constructing fooling sets in instances where our averaging method is unlikely to provide an adequate bound.
[Upper bound, Circuit optimization, Delay effects, Merging, Very large scale integration, Wire, Contracts, Zinc, Read only memory, Sorting]
Solving tree problems on a mesh-connected processor array
26th Annual Symposium on Foundations of Computer Science
None
1985
In this paper we present techniques that result in O(&#x0221A;n) time algorithms for computing many properties and functions of an n-node forest stored in an &#x0221A;n &#x0D7; &#x0221A;n mesh of processors. Our algorithms include computing simple properties like the depth, the height, the number of descendents, the preorder (resp. postorder, inorder) number of every node, and a solution to the more complex problem of computing the Minimax value of a game tree. Our algorithms are asymptotically optimal since any nontrivial computation will require &#x03A9; (&#x0221A;n) time on the mesh. All of our algorithms generalize to higher dimensional meshes.
[Concurrent computing, Algorithm design and analysis, TV, Tree graphs, Computational modeling, Multiprocessor interconnection networks, Minimax techniques, Turning, Registers, Parallel algorithms]
Solving some graph problems with optimal or near-optimal speedup on mesh-of-trees networks
26th Annual Symposium on Foundations of Computer Science
None
1985
We present a systematic approach for solving graph problems under the network models. We illustrate this approach on the mesh-of-trees networks. It is known that under the CREW PRAM model, when a undirected graph of n nodes is given by an n by n adjacency matrix, the problems of finding minimum spanning forest, connected components, and biconnected components can all be solved with optimal speedup when the number of processors p &#x02264; n2/log2n. We show that for these problems, the same optimal speedup can be achieved even under the much more restrictive mesh-of-trees network. We also show that for the problem of finding directed spanning forest of arbitrary digraphs and the problem of testing strong connectivity of 1-reachable digraphs, near-optimal speedup can be achieved.
[Computer science, Concurrent computing, TV, Tree graphs, Computational modeling, Binary trees, Read-write memory, Phase change random access memory, Computer networks, Testing]
Randomized routing on fat-tress
26th Annual Symposium on Foundations of Computer Science
None
1985
Fat-trees are a class of routing networks for hardwareefficient parallel computation. This paper presents a randomized algorithm for routing messages on a fat-tree. The quality of the algorithm is measured in terms of the load factor of a set of messages to be routed, which is a lower bound on the time required to deliver the messages. We show that if a set of messages has load factor &#x03BB; = &#x03A9;(lg n lg lg n) on a fat-tree with n processors, the number of delivery cycles (routing attempts) that the algorithm requires is O(&#x03BB;) with probability 1-O(1/n). The best previous bound was O(&#x03BB; lg n) for the off-line problem where switch settings can be determined in advance. In a VLSI-like model where hardware cost is equated with physical volume, we use the routing algorithm to demonstrate that fat-trees are universal routing networks in the sense that any routing network can be efficiently simulated by a fat-tree of comparable hardware cost.
[Costs, Channel capacity, Tree graphs, Wires, Circuits, Laboratories, Switches, Binary trees, Routing, Hardware]
Distributed BFS algorithms
26th Annual Symposium on Foundations of Computer Science
None
1985
This paper develops a new distributed BFS algorithm for an asynchronous communication network. This paper presents two new BFS algorithms with improved communication complexity. The first algorithm has complexity O((E+V1.5)&#x000B7;logV) in communication and O(V1.5&#x000B7;logV) in time. The second algorithm uses the technique of the first recursively and achieves O(E&#x000B7;2 &#x0221A;logVloglogV) in communication and O(V&#x000B7;2&#x0221A;logVloglogV) in time.
[Asynchronous communication, Particle separators, Interactive systems, Bandwidth, Complexity theory, Internet, Web sites, Distributed algorithms, Video recording, Propagation delay]
An almost linear time and O(nlogn+e) Messages distributed algorithm for minimum-weight spanning trees
26th Annual Symposium on Foundations of Computer Science
None
1985
A distributed algorithm is presented that constructs the minimum-weight spanning tree of an undirected connected graph with distinct edge weights and distinct node identities. Initially each node knows only the weight of each of its adjacent edges. When the algorithm terminates, each node knows which of its adjacent edges are edges of the tree. For a graph with n nodes and e edges, the total number of messages required by our algorithm is at most 5nlogn+2e, and each message contains at most one edge weight or one node identity plus 3+logn bits. Although our algorithm has the same message complexity as the previously known algorithm by Gallager et al., the time complexity of our algorithm takes at most O(nG(n))+ time units, an improvement from Gallager's O(nlogn)+. A worst case O(nG(n)) is also possible.
[Algorithm design and analysis, Tree graphs, Merging, Iterative algorithms, Distributed algorithms, Distributed computing]
Byzantine agreement in constant expected time
26th Annual Symposium on Foundations of Computer Science
None
1985
We present a novel cryptographic algorithm for Byzantine agreement in a network with l=O(n) faulty processors and in the most adversarial setting. Our algorithm requires, once and for all, O(t) rounds of preprocessing. Afterwards it allows us to reach each individual Byzantine agreement in constant expected time. Our solution does not make use of any trusted party.
[Protocols, Computer networks, Polynomials, Cryptography, Communication networks, Distributed computing, Clocks]
Geometrical realization of set systems and probabilistic communication complexity
26th Annual Symposium on Foundations of Computer Science
None
1985
Let d = d(n) be the minimum d such that for every sequence of n subsets F1, F2, . . . , Fn of {1, 2, . . . , n} there exist n points P1, P2, . . . , Pn and n hyperplanes H1, H2 .... , Hn in Rd such that Pj lies in the positive side of Hi iff j &#x02208; Fi. Then n/32 &#x02264; d(n) &#x02264; (1/2 + 0(1)) &#x000B7; n. This implies that the probabilistic unbounded-error 2-way complexity of almost all the Boolean functions of 2p variables is between p-5 and p, thus solving a problem of Yao and another problem of Paturi and Simon. The proof of (1) combines some known geometric facts with certain probabilistic arguments and a theorem of Milnor from real algebraic geometry.
[Geometry, Boolean functions, Upper bound, Character generation, Mathematics, Complexity theory]
Robin hood hashing
26th Annual Symposium on Foundations of Computer Science
None
1985
This paper deals with hash tables in which conflicts are resolved by open addressing. The initial contribution is a very simple insertion procedure which (in comparison to the standard approach) has the effect of dramatically reducing the variance of the number of probes required for a search. This leads to a new search procedure which requires only a constant number of probes, on average, even for full tables. Finally, an extension to these methods yields a new, simple way of performing deletions and subsequent insertions. Experimental results strongly indicate little degeneration in search time. In particular deletions and successful searches appear to require constant time (&#x226A; 2.57 probes) and insertions and unsuccessful searches, O(logn).
[Computer science, Degradation, Costs, Mathematical analysis, Proposals, Probes, Organizing]
Dynamic monotone priorities on planar sets
26th Annual Symposium on Foundations of Computer Science
None
1985
A monotonic priority set is a new data structure which supports maximum-finding and deletions over a set of weighted points in the plane. Global updates to the weights can also be made, incrementing the weights of all points above a given threshold in one of the coordinates. The weights are assumed to be always monotonic in both coordinates. An efficient implementation of this structure is presented and two main applications are described. The first is to the problem of optimal assembly of code for computers with two kinds of jump instruction: long and short. The task in the second application is the implementation of a queuing discipline based on the ranks with respect to two different criteria.
[Computer aided instruction, Costs, Councils, Binary trees, Data structures, Application software, Assembly, Contracts]
Design and analysis of dynamic Huffman coding
26th Annual Symposium on Foundations of Computer Science
None
1985
We introduce an efficient new algorithm for dynamic Huffman coding, called Algorithm V. It performs one-pass coding and transmission in real-time, and uses at most one more bit per letter than does the standard two-pass Huffman algorithm; this is optimum in the worst case among all one-pass schemes. We also analyze the dynamic Huffman algorithm due to Faller, Gallager, and Knuth. In each algorithm, both the sender and the receiver maintain equivalent dynamically varying Huffman trees. The processing time required to encode and decode a letter whose node in the dynamic Huffman tree is currently on the lth level is O(l); hence, the processing can be done in real time. Empirical tests show that Algorithm V performs quite well in practice, often better than the two-pass method. The proposed algorithm is well-suited for file compression and online encoding/decoding in data networks.
[Tree data structures, Computer science, Algorithm design and analysis, Performance evaluation, Heuristic algorithms, Frequency, Encoding, Decoding, Huffman coding, Testing]
Average case lower bounds on the construction and searching of partial orders
26th Annual Symposium on Foundations of Computer Science
None
1985
It is very well known in computer science that partially ordered files are easier to search. In the worst case, for example, a totally unordered file requires no preprocessing, but &#x003A9;(n) time to search, while a totally ordered file requires &#x003A9;(n log n) preprocessing time to sort, but can be searched in O(log n) time. Behind the casual observation, then, lurks the notion of a computational tradeoff between sorting and searching. We analyze this tradeoff in the average case, using the decision tree model. Let P be a preprocessing algorithm that produces partial orders given a set U of n elements, and let S be a searching algorithm for these partial orders. Assuming any of the n! permutations of the elements of U are equally likely, and that we search for any y isin; U with equal probability (in unsuccessful search, all "gaps" are considered equally likely), the average costs P(n) of preprocessing and S(n) of searching may be computed. We demonstrate a tradeoff of the form P(n) + n log S(n) = &#x003A9;(n log n), for both successful and unsuccessful search. The bound is tight up to a constant factor. In proving this tradeoff, we show a lower bound on the average case of searching a partial order. Let A be a partial order on n elements consistent with &#x03A0; permutations. We show S(n) = &#x003A9;(&#x03A0;3/n/n2) for successful search of A, and S(n) = &#x003A9;(&#x03A0;2/n/n) for unsuccessful search. These lower bounds show, for example, that heaps require linear time to search on the average.
[Computer science, Costs, Merging, Binary trees, Data structures, Particle measurements, Decision trees, Application software, Sorting, Information analysis]
On minima of function, intersection patterns of curves, and davenport-schinzel sequences
26th Annual Symposium on Foundations of Computer Science
None
1985
We present several results related to the problem of estimating the complexity M(f1, ..., fn) of the pointwise minimum of n continuous univariate or bivariate functions f1, ..., fn under the assumption that no pair (resp. triple) of these functions intersect in more than some fixed number s of points. Our main result is that in the one-dimensional case M(f1, ..., fn) - O(n&#x03B1;(n)O(&#x03B1;(n)s-3)) (&#x03B1;(n) is the functional inverse of Ackermann's function). In the twodimensional case the problem is substantially harder, and we have only some initial estimates on M, including a tight bound &#x0398;(n2) if s = 2, and a worst-case lower bound &#x03A9;(n2&#x03B1;(n)) for s &#x02265; 6. The treatment of the twodimensional problem is based on certain properties of the intersection patterns of a collection of planar Jordan curves, which we also develop and prove here.
[Computer science, Computational geometry, Upper bound, Differential equations]
Inferring the structure of a Markov Chain from its output
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Computer science, Upper bound, Automata, Switches, H infinity control, Probability, Entropy, Inference algorithms, Statistics]
Automatic verification of probabilistic concurrent finite state programs
26th Annual Symposium on Foundations of Computer Science
None
1985
The verification problem for probabilistic concurrent finite-state program is to decide whether such a program satisfies its linear temporal logic specification. We describe an automata-theoretic approach, whereby probabilistic quantification over sets of computations is reduced to standard quantification over individual computations. Using new determinization construction for &#x03C9;-automata, we manage to improve the time complexity of the algorithm by two exponentials. The time complexity of the final algorithm is polynomial in the size of the program and doubly exponential in the size of the specification.
[Algorithm design and analysis, Concurrent computing, Protocols, USA Councils, Automata, Automatic logic units, Probabilistic logic, Polynomials, Context modeling]
Partial polymorphic type inference is undecidable
26th Annual Symposium on Foundations of Computer Science
None
1985
Polymorphic type systems combine the reliability and efficiency of static type-checking with the flexibility of dynamic type checking. Unfortunately, such languages tend to be unwieldy unless they accommodate omission of much of the information necessary to perform type checking. The automatic inference of omitted type information has emerged as one of the fundamental new implementation problems of these languages. We show here that a natural formalization of the problem is undecidable. The proof is directly applicable to some practical situations, and provides a partial explanation of the difficulties encountered in other cases.
[Computer science, Program processors, Runtime, Debugging, Binary search trees, Trademarks, Data structures, Programming profession]
Fixed-point extensions of first-order logic
26th Annual Symposium on Foundations of Computer Science
None
1985
We prove that the three extensions of first-order logic by means of positive inductions, monotone inductions, and so-called non-monotone (in our terminology, inflationary) inductions respectively, all have the same expressive power in the case of finite structures. As a by-product, the collapse of the corresponding fixed-point hierarchies can be deduced.
[Computer science, Gold, Computer languages, Power engineering computing, Logic programming, Terminology, Calculus, Mathematics, Books, Database languages]
Equivalences and transformations of recursive definitions
26th Annual Symposium on Foundations of Computer Science
None
1985
This work presents a unified theory of recursive program schemes, context-free grammars, grammars on arbitrary algebraic structures (and actually of recursive definitions of all kind) in terms of regular systems of equations. Several equivalence relations on regular systems (depending on sets of equational axioms) are defined. They are systematically investigated and characterized (in some cases) in terms of system transformations by folding, unfolding and rewriting according to the equational algebraic laws.
[Computer science, Tree graphs, Mathematics, Application software, Equations]
A private interactive test of a boolean predicate a minimum-knowledge public-key cryptosystems
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Computer science, Turing machines, Public key, Bismuth, Public key cryptography, Polynomials, Power system modeling, Testing, Cryptographic protocols, Context modeling]
A robust and verifiable cryptographically secure election scheme
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Privacy, Protocols, Voting, Government, Nominations and elections, Robustness, Polynomials, Cryptography, Distributed computing, Clocks]
Verifiable secret sharing and achieving simultaneity in the presence of faults
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Shafts, Fault tolerance, Voting, Tiles, Broadcasting, Robustness, Polynomials, Cryptography, Communication networks, Cryptographic protocols]
The bit extraction problem or t-resilient functions
26th Annual Symposium on Foundations of Computer Science
None
1985
We consider the following adversarial situation. Let n, m and t be arbitrary integers, and let f : {0, 1}n &#x02192; {0, 1}m be a function. An adversary, knowing the function f, sets t of the n input bits, while the rest (n-t input, bits) are chosen at random (independently and with uniform probability distribution) The adversary tries to prevent the outcome of f from being uniformly distributed in {0, 1}m. The question addressed is for what values of n, m and t does the adversary necessarily fail in biasing the outcome of f : {0,1}n &#x02192; {0, 1}m, when being restricted to set t of the input bits of f. We present various lower and upper bounds on m's allowing an affirmative answer. These bounds are relatively close for t &#x02264; n/3 and for t &#x02265; 2n/3. Our results have applications in the fields of faulttolerance and cryptography.
[Fault tolerance, Upper bound, Cryptography]
Collective coin flipping, robust voting schemes and minima of Banzhaf values
26th Annual Symposium on Foundations of Computer Science
None
1985
The power of players in a collective decision process is a central issue in Mathematical Economics and Game Theory. Similar issues arise in Computer Science in the study of distributed, fault tolerant computations when several processes, some perhaps faulty, have to reach agreement. In the present article we study voting schemes which are relatively immune to the presence of unfair players. In particular, we discuss how to perform collective coin flipping which is only slightly biased despite the presence of unfair players. Mathematically this corresponds to problems concerning the minima of Banzhaf values in certain n -person games. These are measures of power studied in Game Theory. It is quite remarkable that while dictatorial voting games are, of course, the most sensitive to the presence of unfair players, some voting schemes that we propose here are significantly more robust than majority voting. Coin flipping was selected as a study case because of its simplicity and because collective coin flipping is widely used in randomized algorithms for distributed computations. It is our feeling that Game Theory has much to contribute to Computer Science and we are sure that further applications will be found.
[Computer science, Fault tolerance, Protocols, Voting, Decision making, Power generation economics, Robustness, Mathematics, Game theory, Distributed computing]
Random polynomial time is equal to slightly-random polynomial time
26th Annual Symposium on Foundations of Computer Science
None
1985
Random Polynomial Time (Rp) is currently considered to be the class of tractable computational problems. Here one assumes a source of truly random bits. However, the known sources of randomness are imperfect. They can be modeled as an adversary source, called slightly-random source. Slightlyrandom Polynomial Time (SRp) is the class of problems solvable in polynomial time using such a source. SRp is thus a more realistic definition of a tractable computational problem. In this paper we give an affirmative answer to the question "is Rp = SRp?" Our proof method is constructive: given an Rp algorithm for a problem, we show how to obtain an SRp algorithm for it. Studying the relationship between randomized and deterministic computation is currently an important issue. A central question here is "is Rp = P?" Our result may be a step towards answering this question.
[Strontium, Computer science, Computational modeling, Circuit simulation, Polynomials, Diodes, Testing, Counting circuits]
Unbiased bits from sources of weak randomness and probabilistic communication complexity
26th Annual Symposium on Foundations of Computer Science
None
1985
We introduce a general model for physical sources or weak randomness. Loosely speaking, we view physical sources as devices which output strings according to probability distributions in which no single string is too probable. The main question addressed is whether it is possible to extract alrnost unbiased random bits from such "probability bounded" sources. We show that most or the functions can be used to extract almost unbiased and independent bits from the output of any two independent "probability-bounded" sources. The number of extractable bits is within a constant factor of the information theoretic bound. We conclude this paper by establishing further connections between communication complexity and the problem discussed above. This allows us to show that most Boolean functions have linear communication complexity in a very strong sense.
[Boolean functions, Physics computing, Laboratories, Sampling methods, Entropy, Complexity theory, Data mining, Diodes, Marine vehicles, Counting circuits]
Factoring with cyclotomic polynomials
26th Annual Symposium on Foundations of Computer Science
None
1985
This paper discusses some new integer factoring methods involving cyclotomic polynomials. There are several polynomials f(X) known to have the following property: given a multiple of f(p), we can quickly split any composite number that has p as a prime divisor. For example -- taking f(X) to be X- 1 -- a multiple of p - 1 will suffice to easily factor any multiple of p, using an algorithm of Pollard. Other methods (due to Guy, Williams, and Judd) make use of X + 1, X2 + 1, and X2 &#x0B1; X + 1. We show that one may take f to be &#x03A6;k, the k-th cyclotomic polynomial. In constrast to the ad hoc methods used previously, we give a universal construction based on algebraic number theory that subsumes all the above results. Assuming generalized Riemann hypotheses, the expected time to factor N (given a multiple E of &#x03A6;k(p)) is bounded by a polynomial in k, logE, and logN.
[Computer science, Algorithm design and analysis, Design methodology, Polynomials, Galois fields, Testing]
Computing with polynomials given by straight-line programs II sparse factorization
26th Annual Symposium on Foundations of Computer Science
None
1985
We develop an algorithm for the factorization of a multivariate polynomial represented by a straight-line program into its irreducible factors represented as sparse polynomials. Our algorithm is in random polynomial-time for the usual coefficient fields and outputs with controllably high probability the correct factorization. It only requires an a priori bound for the total degree of the input and over rational numbers a bound on the size of the polynomial coefficients.
[Computer science, Computational modeling, Read-write memory, Polynomials]
An application of simultaneous approximation in combinatorial optimization
26th Annual Symposium on Foundations of Computer Science
None
1985
We present a preprocessing algorithm to make certain polynomial algorithms strongly polynomial. The running time of some of the known combinatorial optimization algorithms depends on the size of the objective function w. Our preprocessing algorithm replaces w by an integral valued w whose size is polynomially bounded in the size of the combinatorial structure and which yields the same set of optimal solutions as w. As applications we show how existing polynomial algorithms for finding the maximum weight clique in a perfect graph and for the minimum cost submodular flow problem can be made strongly polynomial. The method relies on Lov&#x0E1;sz's simultaneous approximation algorithm.
[Computer science, Costs, Integral equations, Approximation algorithms, Polynomials, Application software, Ellipsoids, Arithmetic, Testing]
Computing ears and branchings in parallel
26th Annual Symposium on Foundations of Computer Science
None
1985
An ear-decomposition of a digraph is a representation of it as the union of (open or closed) directed paths, each having its endpoints in common with the union of the previous paths but nothing else. We prove that finding an ear-decomposition of a strongly directed graph is in NC, i.e. an eardecomposition can be constructed in parallel in polylog time, using a polynomial number of processors. Using a similar technique, we show that the problem of finding a minimum weight spanning arborescence in an arcweighted rooted digraph is in NC.
[Concurrent computing, Computer science, Circuits, Ear, Polynomials, Inference algorithms]
Parallel computational geometry
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Concurrent computing, Computational geometry, Optimization methods, Computer graphics, Very large scale integration, Data structures, Application software, Parallel algorithms, Robots, Sorting]
Parallel tree contraction and its application
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Concurrent computing, Computer science, Particle separators, Forward contracts, Circuits, Random access memory, Parallel processing, Application software, Parallel algorithms, Arithmetic]
Improved processor bounds for algebraic and combinatorial problems in RNC
26th Annual Symposium on Foundations of Computer Science
None
1985
false
[Computer science, Algorithm design and analysis, Concurrent computing, Computational modeling, Circuits, Polynomials, Parallel algorithms, Equations, Arithmetic, Pipeline processing]
An optimal parallel algorithm for integer sorting
26th Annual Symposium on Foundations of Computer Science
None
1985
We assume a parallel RAM model which allows both concurrent writes and concurrent reads of global memory. Our algorithms are randomized: each processor is allowed an independent random number generator. However our stated resource bounds hold for worst case input with overwhelming likelihood as the input size grows. We give a new parallel algorithm for integer sorting where the integer keys are restricted to at most polynomial magnitude. Our algorithm costs only logarithmic time and is the first known where the product of the time and processor bounds are bounded by a linear function of the input size. These simultaneous resource bounds are asymptotically optimal. All previous known parallel sorting algorithms required at least a linear number of processors to achieve logarithmic time bounds, and hence were nonoptimal by at least a logarithmic factor.
[Concurrent computing, Random access memory, Read-write memory, Cost function, Polynomials, Registers, Parallel algorithms, Random number generation, Sorting, Arithmetic]
Fast parallel computation with permutation groups
26th Annual Symposium on Foundations of Computer Science
None
1985
We develop fast parallel solutions to a number of basic problems involving solvable and nilpotent permutation groups. Testing solvability is in NC, and RNC includes, for solvable groups, finding order, testing membership, finding the derived series and finding a composition series. Additionally, for nilpotent groups, one can, in RNC, find the center, a central composition series, and point-wise stabilizers of sets. There are applications to graph isomorphism. In fact, we exhibit a class of vertex-colored graphs for which determining isomorphism is NC-equivalent to computing ranks of matrices Over small fields. A useful tool is the observation that the problem of finding the smallest subspace containing a given set of vectors and closed under a given set of linear transformations (all over a small field) belongs to RNC.
[Concurrent computing, Information science, Councils, Instruments, Linear algebra, Vectors, Polynomials, Encoding, Parallel algorithms, Testing]
Algebraic cell decomposition in NC
26th Annual Symposium on Foundations of Computer Science
None
1985
We give an algorithm to construct a cell decomposition of Rd, including adjacency information, defined by any given set of rational polynomials in d variables. The algorithm runs in single exponential parallel time, and in NC for fixed d. The algorithm extends a recent algorithm of Ben-Or, Kozen, and Reif for deciding the theory of real closed fields.
[Computer science, Motion planning, Computational geometry, Computer displays, Motion-planning, Computer graphics, Polynomials, Topology, Application software, Robots]
Fast and efficient algorithms for sequential and parallel evaluation of polynomial zeros and of matrix polynomials
26th Annual Symposium on Foundations of Computer Science
None
1985
We evaluate all the real and complex zeros &#x03BB;1,...,&#x03BB;n of an n-th degree univariate polynomial with the relative precision 1/2nc for a given positive constant c. If for all g,h, log |&#x03BB;g/&#x03BB;h-1| &#x02265; 1/2O(n) unless &#x03BB;g = &#x03BB;h, then we need O(n3log2n) arithmetic operations or O(n2log n) steps, n log n processors. O(n2log n) operations or O(n log n) parallel steps, n processors suffice if either all the zeros are real or for all g,h either |&#x03BB;g| = |&#x03BB;h| or 2O(n) &#x02265; (|&#x03BB;g/&#x03BB;h| - 1)| &#x02265; 1/2O(n). If all the zeros are either multiple or form complex conjugate pairs or if their moduli pairwise differ by the factors at least 1+1/nO(1), then O(n log2n) operations or O(log2n) steps, n processors suffice. Replacing 1+1/nO(1) above by 1+1/nO(loghn) for a positive h only requires to increase the time-complexity bounds by the factor loghn. Some of the presented algorithms extend Graeffe's method, other algorithms use the power sum techniques and the companion matrix computation; the latter ones are related to Bernoulli's and Leverrier's methods and to the power method and are extended in this paper to the evaluation of a matrix polynomial u(X) of degree N, (X is an n&#x0D7;n matrix), using O(N log N+n2.496) arithmetic operations. Such evaluation can be performed using O(log N+log2n) parallel steps, Nn+n3.496 processors or alternatively O(log2(nN)) steps, N/log N+n3.496 processors over arbitrary field of constants. Over rational constants, for almost all matrices X the number of processors can be reduced to Nn+n2.933 or to N/log N+n2.933, respectively; the bounds can be further reduced to O(log N+log2n)steps, N+n2.933 processors if u(X) is to be computed with a fixed arbitrarily high precision rather than exactly. For integer and well-conditioned matrices, the exponent 2.933 above can be decreased to 2.496. The results substantially improve the previously known upper estimates for the complexity of sequential and parallel evaluation of polynomial zeros and of matrix polynomials.
[Concurrent computing, Computer science, Performance evaluation, Costs, DH-HEMTs, Polynomials, Newton method, Artificial intelligence, Computational complexity, Arithmetic]
The complexity of parallel sorting
26th Annual Symposium on Foundations of Computer Science
None
1985
We consider PRAM's with arbitrary computational power for individual processors, infinitely large shared memory and "priority" writeconflict resolution. The main result is that sorting n integers with n processors requires &#x03A9;(&#x0221A;log n) steps in this strong model. We also show that computing any symmetric polynomial (e.g. the sum or product) of n integers requires exactly log2n steps, for any finite number of processors.
[Concurrent computing, Upper bound, Input variables, Laboratories, Circuits, Magnetic resonance, Phase change random access memory, Polynomials, Sorting]
The complexity of parallel computation on matroids
26th Annual Symposium on Foundations of Computer Science
None
1985
In [KUW1] we have proposed the setting of independence systems to study the relation between the computational complexity of search and decision problems. The universal problem that captures this relation, which we termed the S-search problem, is: "Given an oracle for the input system, find a maximal independent subset in it". Many interesting and important search problems can be described by a special class of independence systems, called matroids. This paper is devoted to die complexity of the S- search problem for matroids. Our main result is a lower bound on any probabilistic algorithm for the S-search problem that acquires information about the input system by interrogating an independence oracle. We prove that the expected time of any such probabilistic algorithm that uses a sub-exponential number of processors is &#x03A9;(n1/3-&#x03B5;). This is one of the first nontrivial, super-logarithmic lower bounds on a randomized parallel computation. It implies that in our model of computation Random-NC is strictly contained in P. Another consequence of the lower bound is that the O(&#x0221A;n) time probabilistic upper bound for arbitrary independence systems, presented in [KUW1], is close to optimal and cannot be significantly improved, even for matroids. However, fills O(&#x0221A;n) upper bound can be improved in a different sense for matroids -it can be made deterministic, still with polynomially many processors. Finally, we show that the lower bound can be beaten for the special case of graphic matroids. Here, the S-search problem is simply to find a spanning forest of a graph, when the algorithm cannot see the graph, but can only ask whether subsets of edges are forests or not. We give an O(logn) time deterministic parallel algoritlun that uses nO(logn) processors. From the upper bounds on parallel time above we deduce similar bounds (up to a poly-log factor) on thc sequential space required by a deterministic Turing machine with an independence oracle to solve the S-search problem.
[Concurrent computing, Computer science, Graphics, Upper bound, Laboratories, Search problems, Size measurement, Time measurement, Parallel algorithms, Computational complexity]
Foreword
27th Annual Symposium on Foundations of Computer Science
None
1986
Presents the introductory welcome message from the conference proceedings.
[]
An O(n2(m + n log n) log n) min-cost flow algorithm
27th Annual Symposium on Foundations of Computer Science
None
1986
The minimum-cost flow problem is the following: given a network with n vertices and m edges, find a maximum flow of minimum cost. Many network problems are easily reducible to this problem. A polynomial-time algorithm for the problem has been known for some time [EK], but only recently a strongly polynomial algorithm was discovered [Ts]. In this paper we design an O(n2(m + n log n)log n) algorithm. The previous best algorithm had an O(m2 (m + n log n) log n) time bound ([F], [O]). Thus, we obtain an improvement of two orders of magnitude for dense graphs. Our algorithm is based on Fujishige's algorithm [F] (which is based on Tardos' algorithm [Ts]). Fujishige's algorithm consists of up to O(m log n) steps. Each step solves a single source shortest path problem with nonnegative edge lengths. We modify this algorithm in order to make an improved analysis possible. The new algorithm may still consist of up to m iterations, and an iteration may still consist of up to O(m log n) steps, but we can still show that the total number of steps is bounded by O(n2 log n). The improvement is due to a new technique that relates the time spent to the progress achieved.
[Shortest path problem, Computer science, Algorithm design and analysis, Road transportation, Cost function, Linear programming, Polynomials, Bipartite graph, Arithmetic]
Probabilistic construction of deterministic algorithms: Approximating packing integer programs
27th Annual Symposium on Foundations of Computer Science
None
1986
We consider the problem of approximating an integer program by first solving its relaxation linear program and "rounding" the resulting solution. For several packing problems, we prove probabilistically that there exists an integer solution close to the optimum of the relaxation solution. We then develop a methodology for converting such a probabilistic existence proof to a deterministic approximation algorithm. The methodology mimics the existence proof in a very strong sense.
[Computer science, Strontium, Multidimensional systems, Lattices, Approximation algorithms, Linear programming, Routing, Vectors, Random variables, Design optimization]
On a search problem related to branch-and-bound procedures
27th Annual Symposium on Foundations of Computer Science
None
1986
false
[Computer science, Costs, Upper bound, Binary trees, Read-write memory, Search problems, Mathematics, Registers, Arithmetic]
Probabilistic Boolean decision trees and the complexity of evaluating game trees
27th Annual Symposium on Foundations of Computer Science
None
1986
The Boolean Decision tree model is perhaps the simplest model that computes Boolean functions; it charges only for reading an input variable. We study the power of randomness (vs. both determinism and non-determinism) in this model, and prove separation results between the three complexity measures. These results are obtained via general and efficient methods for computing upper and lower bounds on the probabilistic complexity of evaluating Boolean formulae in which every variable appears exactly once (AND/OR tree with distinct leaves). These bounds are shown to be exactly tight for interesting families of such tree functions. We then apply our results to the complexity of evaluating game trees, which is a central problem in AI. These trees are similar to Boolean tree functions, except that input variables (leaves) may take values from a large set (of valuations to game positions) and the AND/OR nodes are replaced by MIN/MAX nodes. Here the cost is the number of positions (leaves) probed by the algorithm. The best known algorithm for this problem is the alpha-beta pruning method. As a deterministic algorithm, it will in the worst case have to examine all positions. Many papers studied the expected behavior of alpha-beta pruning (on uniform trees) under the unreasonable assumption that position values are drawn independently from some distribution. We analyze a randomized variant of alphabeta pruning, show that it is considerably faster than the deterministic one in worst case, and prove it optimal for uniform trees.
[Computer science, Boolean functions, Costs, Power measurement, Input variables, Mathematics, Decision trees, Mathematical model, Artificial intelligence, Context modeling]
A physical interpretation of graph connectivity, and its algorithmic applications
27th Annual Symposium on Foundations of Computer Science
None
1986
false
[Monte Carlo methods, Algebra, Graph theory, Springs, Physics]
The asymptotic spectrum of tensors and the exponent of matrix multiplication
27th Annual Symposium on Foundations of Computer Science
None
1986
We introduce an asymptotic data structure for the relative bilinear complexity of bilinear maps (tensors). It consists of a compact Hausdorff space &#x0394; together with an interpretation of the tensors under consideration as continuous functions on &#x0394;. The asymptotic rank of a tensor is simply the maximum of the associated function. On the way we present a new method for estimating the exponent &#x03C9; of matrix multiplication, leading at present to the bound &#x03C9; &#x226A; 2.48. The paper gives only brief indications of proofs, if any. Detailed arguments may be found in 26,27.
[Tensile stress, Upper bound, Linear algebra, Data structures]
Storing a dynamic sparse table
27th Annual Symposium on Foundations of Computer Science
None
1986
We present a family of data structures that can process a sequence of insert, delete, and lookup instructions such that each lookup and deletion is done in constant worst-case time and each insertion is done in constant expected time. The amount of space used by each data structure is proportional to the maximal number of elements that need to be stored at any moment in time.
[Algorithm design and analysis, Computer science, Dictionaries, Data structures, Compaction]
Lower bounds for accessing binary search trees with rotations
27th Annual Symposium on Foundations of Computer Science
None
1986
We describe two methods for obtaining lower bounds on the cost of accessing a sequence of nodes of a symmetrically ordered binary search tree, where rotations can be done on the tree. The bounds apply to offline as well as online algorithms.
[Costs, Heuristic algorithms, Binary trees, Binary search trees, Mirrors]
Geometric applications of Davenport-Schinzel sequences
27th Annual Symposium on Foundations of Computer Science
None
1986
We present efficient algorithms for the following geometric problems: (i) Preprocessing of a 2-D polyhedral terrain so as to support fast ray shooting queries from a fixed point. (ii) Determining whether two disjoint interlocking simple polygons can be separated from one another by a sequence of translations. (iii) Determining whether a given convex polygon can be translated and rotated so as to fit into another given polygonal region. (iv) Motion planning for a convex polygon in the plane amidst polygonal barriers. All our algorithms make use of Davenport Schinzel sequences and on some generalizations of them; these sequences are a powerful combinatorial tool applicable in contexts which involve the calculation of the pointwise maximum or minimum of a collection of functions.
[Computational geometry, Upper bound, Poles and towers]
Lower bounds on the complexity of multidimensional searching
27th Annual Symposium on Foundations of Computer Science
None
1986
We establish new lower bounds on the complexity of several searching problems. We show that the time for solving the partial sum problem on n points in d dimensions is at least proportional to (log n/log 2m/n)d-1 in both the worst and average cases; m denotes the amount of storage used. This bound is provably tight for m = &#x03A9;(nlogcn) and any c &#x226B; d- 1. We also prove a lower bound of &#x03A9;(n(log n/log log n)d) on the time required for executing n inserts and queries. Other results include a lower bound on the complexity of orthogonal range searching in d dimensions (in report-mode). We show that on a pointer machine a query time of O(s+polylog(n)) time can only be achieved at the expense of &#x03A9;(n(log n/log log n)d-1) space, which is optimal; n and s denote respectively the input and output sizes.
[Computer science, Graphics, Multidimensional systems, Databases, Computational modeling, Data structures]
Planar realizations of nonlinear Davenport-Schinzel sequences by segments
27th Annual Symposium on Foundations of Computer Science
None
1986
Let G={l1,...,ln} be a collection of n segments in the plane, none of which is vertical. Viewing them as the graphs of partially defined linear functions of x, let YG be their lower envelope (i.e. pointwise minimum). YG is a piecewise linear function, whose graph consists of subsegments of the segments lt. Hart and Sharir [HS] have shown that YG consists of at most O(n&#x03B1;(n)) segments (where &#x03B1;(n) is the extremely slowly growing inverse Ackermann's function). We present here a construction of a set G of n segments for which YG consists of &#x03A9;(n&#x03B1;(n)) subsegments, proving that the Hart-Sharir bound is tight in the worst case. Another interpretation of our result is in terms of Davenport-Sehinzel sequences: The sequence EG of indices of segments in G in the order in which they appear along YG is a Davenport-Schinzel sequence of order 3 - i.e. no two adjacent elements of EG are equal and EG contains no subsequence of the form a...b...a...b...a. Hart and Sharir have shown that the maximal length of such a sequence composed of n symbols is &#x0398;(n&#x03B1;(n)). Our result actually shows that the lower bound construction of Hart and Sharir can be realized by the lower envelope of n straight segments, thus settling one the main open problems in this area.
[Computational geometry, Piecewise linear techniques]
Proving by example and gap theorems
27th Annual Symposium on Foundations of Computer Science
None
1986
This paper proposes a proving_by_example method, which works as follows: Given a geometry proposition we can easily present a concrete numerical example such that in order to determine whether the proposition is generally true, one need only to try this example up to a certain number of significant digits. This is an application of a recently discovered gap theorem.
[Concurrent computing, Computational geometry, Upper bound, Humans, Educational institutions, Polynomials, Concrete, Computational Intelligence Society, Equations, Convergence]
An optimal algorithm for the all-nearest-neighbors problem
27th Annual Symposium on Foundations of Computer Science
None
1986
Given a set V of n points in k-dimensional space, and an Lq-metric (Minkowski metric), the All-Nearest-Neighbors problem is defined as follows: For each point p in V, find all those points in V-{p} that are closest to p under the distance metric Lq. We give an O(nlogn) algorithm for the All-Nearest-Neighbors problem, for fixed dimension k and fixed metric Lq. Since there is an &#x03A9;(n logn) lower bound, in the algebraic decision tree model of computation, on the time complexity of any algorithm that solves the All-Nearest-Neighbors problem (for k = 1), the running time of our algorithm is optimal upto a constant.
[Computer science, Computational geometry, Multidimensional systems, Computational modeling, Extraterrestrial measurements, Decision trees, Arithmetic, Testing]
An algorithm for constructing the aspect graph
27th Annual Symposium on Foundations of Computer Science
None
1986
In this paper we present tight bounds on the maximum size of aspect graphs and give worstcase optimal algorithms for their construction, first in the convex case and then in the general case. In particular, we give upper and lower bounds on the maximum size (including vertex labels) of &#x0398;(n3) and &#x0398;(n5) and algorithms for constructing the aspect graph which run in time O(n3) and O(n5) for the convex and general cases respectively. The algorithm for the general case makes use of a new 3D object representation called the aspect representation or asp. We also show a different way to label the aspect graph in order to save a factor of n in the asymptotic size (at the expense of label retrieval time) in both the convex and general cases, and we suggest alternatives to the aspect graph which require less space and store more information.
[Computer vision, Image recognition, Shape, Bifurcation, Application specific processors, Information retrieval]
An algorithmic approach to the automated design of parts orienters
27th Annual Symposium on Foundations of Computer Science
None
1986
This paper concerns the design of parts orienters - the dual to the motion planning problem. Two particular paradigms are considered and their abstractions to the computational domain lead to interesting problems in graph pebbling and function composition on finite sets. Polynomial time algorithms are developed for the abstracted problems.
[Algorithm design and analysis, Robotic assembly, Motion planning, CADCAM, Design automation, Manufacturing processes, Computer aided manufacturing, Polynomials, Feeds, Robots]
Finite-resolution computational geometry
27th Annual Symposium on Foundations of Computer Science
None
1986
Geometric algorithms are usually designed with continuous parameters in mind. When the underlying geometric space is intrinsically discrete, as is the case for computer graphics problems, such algorithms are apt to give invalid solutions if properties of a finite-resolution space are not taken into account. In this paper we discuss an approach for transforming geometric concepts and algorithms from the continuous domain to the discrete domain. As an example we consider the discrete version of the problem of finding all intersections of a collection of line segments. We formulate criteria for a satisfactory solution to this problem, and design an interface between the continuous domain and the discrete domain which supports certain invariants. This interface enables us to obtain a satisfactory solution by using plane-sweep and a variant of the continued fraction algorithm.
[Algorithm design and analysis, Computational geometry, Solid modeling, Computer graphics, Concrete, Topology, Application software]
On Newton's method for polynomials
27th Annual Symposium on Foundations of Computer Science
None
1986
Let Pd be the set of polynomials over the complex numbers of degree d with all its roots in the unit ball. For f &#x02208; Pd, let &#x0393;f be the set of points for which Newton's method converges to a root, and let Af &#x02261; |&#x0393;f &#x02229; B2(O)|/|B2(O)|, i.e. the density of &#x0393;f in the ball of radius 2. For each d we consider Ad, the worst-case density Af for f &#x02208; Pd. In |S|, S. Smale conjectured that Ad &#x226B; 0 for all d &#x02265; 3 (it was wellknown that A1 = A2 = 1). In this paper we prove that (1/d)cd2 log d &#x02264; Ad for some constant c. In particular, Ad &#x226B; 0 for all d.
[Geometry, Polynomials, Newton method, Equations]
How to generate and exchange secrets
27th Annual Symposium on Foundations of Computer Science
None
1986
In this paper we introduce a new tool for controlling the knowledge transfer process in cryptographic protocol design. It is applied to solve a general class of problems which include most of the two-party cryptographic problems in the literature. Specifically, we show how two parties A and B can interactively generate a random integer N = p&#x000B7;q such that its secret, i.e., the prime factors (p, q), is hidden from either party individually but is recoverable jointly if desired. This can be utilized to give a protocol for two parties with private values i and j to compute any polynomially computable functions f(i,j) and g(i,j) with minimal knowledge transfer and a strong fairness property. As a special case, A and B can exchange a pair of secrets sA, sB, e.g. the factorization of an integer and a Hamiltonian circuit in a graph, in such a way that sA becomes computable by B when and only when sB becomes computable by A. All these results are proved assuming only that the problem of factoring large intergers is computationally intractable.
[Computer science, Privacy, Turing machines, Circuits, Polynomials, Probability distribution, History, Cryptography, Knowledge transfer, Cryptographic protocols]
Information theoretic reductions among disclosure problems
27th Annual Symposium on Foundations of Computer Science
None
1986
Alice disposes of some number of secrets. She is willing to disclose one of them to Bob. Although she agrees to let him choose which secret he wants, she is not willing to allow him to gain any information on more than one secret. On the other hand, Bob does not want Alice to know which secret he wishes. An all-or-nothing disclosure is one by which, as soon as Bob has gained any information whatsoever on one of Alice's secrets, he has wasted his chances to learn anything about the other secrets. We assume that Alice is honest when she claims to be willing to disclose one secret to Bob (i.e. she is not about to send junk). The only cheating Alice is susceptible of trying is to figure out which secret is of interest to Bob. We address the following question from an information theoretic point of view: what is the most elementary disclosure problem? The main result is that the general all-or-nothing disclosure of secrets is equivalent to a much simpler problem, which we call the two-bit problem.
[Computer science, Terrorism, Cryptography, Stress, Cryptographic protocols]
Proofs that yield nothing but their validity and a methodology of cryptographic protocol design
27th Annual Symposium on Foundations of Computer Science
None
1986
In this paper we demonstrate the generality and wide applicability of zero-knowledge proofs, a notion introduced by Goldwasser, Micali and Rackoff. These are probabilistic and interactive proofs that, for the members x of a language L, efficiently demonstrate membership in the language without conveying any additional knowledge. So far, zero-knowledge proofs were known only for some number theoretic languages in NP &#x02229; Co-NP.
[Computer science, Polynomials, Cryptography, Cryptographic protocols]
Non-transitive transfer of confidence: A perfect zero-knowledge interactive protocol for SAT and beyond
27th Annual Symposium on Foundations of Computer Science
None
1986
A perfect zero-knowledge interactive proof is a protocol by which Alice can convince Bob of the truth of some theorem in a way that yields no information as to how the proof might proceed (in the sense of Shannon's information theory). We give a general technique for achieving this goal for any problem in NP (and beyond). The fact that our protocol is perfect zero-knowledge does not depend on unproved cryptographic assumptions. Furthermore, our protocol is powerful enough to allow Alice to convince Bob of theorems for which she does not even have a proof. Whenever Alice can convince herself probabilistically of a theorem, perhaps thanks to her knowledge of some trap-door information, she can convince Bob as well without compromising the trap-door in any way. This results in a non-transitive transfer of confidence from Alice to Bob, because Bob will not be able to subsequently convince someone else that the theorem is true. Our protocol is dual to those of [GMW1, BC].
[Computer science, Polynomials, Probability distribution, Cryptography, NP-complete problem, Cryptographic protocols, Information theory, Counting circuits]
Dynamic deadlock resolution protocols
27th Annual Symposium on Foundations of Computer Science
None
1986
The deadlock resolution problem can be informally stated as follows. There exists a set of actions, generated at different times, with some complex and contradictory precedence constraints between their executions. To resolve a deadlock, some of the actions need to be aborted; this enables to execute the remaining ones. This problem naturally arises in the context of distributed systems, e.g. communication networks, distributed operating systems and distributed databases, where actions are generated by many processors, and are not coordinated by a central controller. In this paper, we are concerned with efficient distributed algorithms (protocols) for resolution of dynamic deadlocks. Such resolution protocols must operate on-line without any knowledge of the future and using only local information. The main contribution of the paper is a reduction of the most general dynamic deadlock resolution problem to a conceptually simpler static problem, in which all actions are known a priori. The complexity of our reduction is O (m + n logn) in communication and O (n) in time, where n is the number of actions and m is total number of constraints. Since the static deadlock resolution requires at least &#x03A9;(m + n logn) in communication and &#x03A9;(n) in time, our reduction essentially shows that the resolution of dynamic deadlocks is not any harder than resolution of the static ones. We also show here a simple and optimal algorithm for the static problem, which, together with the above reduction, yields an optimal dynamic deadlock resolution protocol.
[Context, Protocols, Operating systems, Distributed databases, Communication system control, System recovery, Control systems, Communication networks, Distributed algorithms, Centralized control]
Programming simultaneous actions using common knowledge: Preliminary version
27th Annual Symposium on Foundations of Computer Science
None
1986
This work applies the theory of knowledge in distributed systems to the design of faulttolerant protocols for problems involving coordinated simultaneous actions in synchronous systems. We give a simple method for transforming specifications of such problems into high-level protocols programmed using explicit tests of whether certain facts are common knowledge. The resulting protocols are optimal in all runs: for every possible input to system and pattern of processor failures, they are guaranteed to perform the simultaneous actions as soon as any other protocol can possibly perform them. A careful analysis of when facts become common knowledge shows how to efficiently implement these protocols in many variants of the omissions failure model. In the generalized omissions model, however, it is shown that any protocol that is optimal in this sense must require co-NP hard computations. The analysis in this paper exposes subtle differences between the failure models, including the precise point at which this gap in complexity occurs.
[Computer science, Protocols, Laboratories, Fault tolerant systems, Failure analysis, Distributed databases, Mathematics, Timing, Contracts, Testing]
Flipping persuasively in constant expected time
27th Annual Symposium on Foundations of Computer Science
None
1986
We present a distributed protocol for achieving a distributed coin in the presence of an extremely powerful adversary in constant time. The protocol can tolerate up to n/log n malicious processor failures where n is the number of processors in the system. The protocol needs only a fixed constant number of rounds of message exchange; no preprocessing is required. As a corollary we obtain an (n/log n)-resilient probabilistic protocol for Byzantine agreement running in constant expected time. Combining this with a generalization of a technique of Bracha, we obtain a probabilistic Byzantine agreement protocol tolerant of almost n/3 failures with O(log log n) expected running time.
[Upper bound, Turing machines, Broadcasting, Polynomials, Cryptography, Cryptographic protocols]
Atomic shared register access by asynchronous hardware
27th Annual Symposium on Foundations of Computer Science
None
1986
The contribution of this paper is two-fold. First, we describe two ways to construct multivalued atomic n-writer n-reader registers. The first solution uses atomic 1-writer 1-reader registers and unbounded tags. the other solution uses atomic 1-writer n-reader registers and bounded tags. The second part of the paper develops a general methodology to prove atomicity, by identifying a set of criteria which guaranty an effective construction for the required atomic mapping. We apply the method to prove atomicity of the two implementations for atomic multiwriter multireader registers.
[Computer science, Message passing, Laboratories, Writing, Read-write memory, Hardware, Mathematics, Registers, Contracts, Testing]
Competitive snoopy caching
27th Annual Symposium on Foundations of Computer Science
None
1986
In a snoopy cache multiprocessor system, each processor has a cache in which it stores blocks of data. Each cache is connected to a bus used to communicate with the other caches and with main memory. For several of the proposed models of snoopy caching, we present new on-line algorithms which decide, for each cache, which blocks to retain and which to drop in order to minimize communication over the bus. We prove that, for any sequence of operations, our algorithms' communication costs are within a constant factor of the minimum required for that sequence; for some of our algorithms we prove that no on-line algorithm has this property with a smaller constant.
[Computer science, Costs, Memory architecture, Cache memory, Broadcasting, Writing, Time measurement, Multiprocessing systems]
The distance bound for sorting on mesh-connected processor arrays is tight
27th Annual Symposium on Foundations of Computer Science
None
1986
In this paper, We consider the problem of sorting n2 numbers, initially distributed randomly in an n &#x0D7; n mesh-connected processor array with one element per processor. We show a lower bound, based on distance arguments, of 4n routing steps on mesh-connected processors operating in an SIMD mode with no wraparounds in rows or columns, We present an algorithm using a novel approach, which is optimal upto the conslant of the leading term, and hence, succeed in proving the tightness of the lower bound based on distance. Keeping in mind the practical difficulties in implementation of this algorithm, we also present an extremely practical O(n) algorithm amenable for VLSI implementation and for existing mesh- connected computers. All the results in this paper were derived by using a new method of analysis inspired by the discovery of shear-sort or row-column sort.
[Algorithm design and analysis, Gold, Computational modeling, Spine, Very large scale integration, Routing, Sorting, Indexing]
Meshes with multiple buses
27th Annual Symposium on Foundations of Computer Science
None
1986
This paper considers mesh computers with buses, where each bus provides a broadcasting capability to the processors connected to it. We first disprove a published claim by showing that on a 2-dimensional mesh with a bus for each row, where each row must solve its own problem with data that is independent of all other rows, there are problems where the rows can cooperatively solve all subproblems faster than any single row can solve its own problem. As a corollary we obtain efficient solutions to some graph problems. We also consider the optimal layout of buses for a given dimension and number of buses per processor, where optimality is defined in terms of the time needed to simulate any other machine with the same constraints. Using new families of layouts, optimal or nearly optimal families of layouts are determined for each possible choice of dimension and number of buses per processor.
[Computer science, Gold, Broadcasting, Very large scale integration, Phase change random access memory, Grid computing, Finite element methods, Joining processes, Nearest neighbor searches]
Optimal simulations of tree machines
27th Annual Symposium on Foundations of Computer Science
None
1986
Universal networks offer the advantage that they can execute programs written for simpler architectures without significant run-time overhead. In this paper we investigate simulations of tree machines; the fact that divide-and-conquer algorithms are programmed naturally on trees motivates our investigation. Among various proposals for parallel computing the boolean hypercube has emerged as a particularly versatile network. It is well known that programs for multidimensional grid machines, for example, can be executed on a hypercube with no communications overhead by embedding the grid as a subgraph of the hypercube. Our first result is that a program for any tree machine can be executed on the hypercube with constant overhead. More precisely, every cycle of a synchronous binary tree can be simulated in O(1) cycles on a hypercube, independent of the shape of the tree. The algorithm to embed the tree within the hypercube runs in polynomial time. We also give efficient simulations of arbitrary binary trees on the complete binary tree, the FFT and shuffle-exchange networks. It is natural to ask if any sparse network can simulate every binary tree efficiently. Somewhat surprisingly, we construct a universal bounded-degree network on N nodes for which every N node binary tree is a spanning tree. In other words, every binary tree can be simulated on our universal network with no overhead. This improves previous bounds on the sizes of universal graphs for trees.
[Runtime, Multidimensional systems, Tree graphs, Shape, Computational modeling, Binary trees, Computer architecture, Parallel processing, Hypercubes, Proposals]
How robust is the n-cube?
27th Annual Symposium on Foundations of Computer Science
None
1986
The n-cube network is called faulty if it contains any faulty processor or any faulty link. For any number k we are interested in the minimum number f(n, k) of faults, necessary for an adversary to make any (n-k)-dimensional subcube faulty. Reversely formulated: The existence of a (n-k)- dimensional nonfaulty subcube can be guaranteed, unless there are at least f(n,k) faults in the n-cube. In this paper several lower and upper bounds for f(n, k) are derived such that the resulting gaps are "small". For instance if k &#x02265; 2 is constant, then f(n, k) = &#x03B8;(log n). Especially for k = 2 and large n: f(n, 2) &#x02208; [&#x02308;&#x03B1;n&#x02309; : &#x02308;&#x03B1;n&#x02309; + 2] where &#x03B1;n = log n + 1/2 log log n + 1/2. Or if k = &#x03C9;(log log n) then 2k &#x226A; f(n, k) &#x226A; 2(1+&#x03B5;)k, with &#x03B5; chosen arbitrarily small. The above upper bounds are obtained by analysing the behaviour of an adversary, who makes "worst-case" distributions of a given number of faulty processors. For k = 2 the distribution is obtained constructively, whereas in the general case only the existence is shown using probabilistic arguments. The above bounds change if the notions are relativized with respect to some given parallel faultchecking procedure P. In this case only those subcubes must be made faulty by the adversary, which are possible outputs of P. In the case k = 2 the notion of directed chromatic index is defined to analyse this situation. Relations between the directed chromatic index and the chromatic number are derived, which are of interest in their own right.
[Upper bound, Network topology, Parallel processing, Routing, Robustness, Parallel algorithms, Sorting]
Parallel algorithms for permutation groups and graph isomorphism
27th Annual Symposium on Foundations of Computer Science
None
1986
We develop parallel techniques for dealing with permutation group problems. These are most effective on the class of groups with bounded non-abelian composition factors. For this class, we place in NC problems such as membership testing, finding the center and composition factors, and, of particular significance, finding pointwise-set-stabilisers. The last has applications to instances of graph-isomorphism and we show that NC contains isomorphism-testing for vertex-colored graphs with bounded color multiplicity, a problem not long known to be in polynomial time.
[Concurrent computing, Information science, Linear algebra, System recovery, Polynomials, Vectors, Parallel algorithms, Machinery, Lagrangian functions, Testing]
A Las Vegas - NC algorithm for isomorphism of graphs with bounded multiplicity of eigenvalues
27th Annual Symposium on Foundations of Computer Science
None
1986
false
[Geometry, Operations research, Layout, Lattices, Eigenvalues and eigenfunctions, Orbits, Parallel algorithms, Galois fields]
The complexity of isomorphism testing
27th Annual Symposium on Foundations of Computer Science
None
1986
A polynomial time isomorphism test for a class of groups, properly containing the class of abelian groups, is presented. Isomorphism testing of group presentations for (a subclass of) the same class of groups is shown to be (graph) isomorphism complete. These seem to be the first known isomorphism complete problems in group theory. Subexponential tests are presented as well for rings and algebras.
[Algebra, Terminology, Polynomials, Eigenvalues and eigenfunctions, Testing]
FFD bin packing for item sizes with distributions on [0,1/2]
27th Annual Symposium on Foundations of Computer Science
None
1986
We study the expected behavior of the FFD binpacking algorithm applied to items whose sizes are distributed in accordance with a Poisson process with rate N on the interval [0,1/2] of item sizes. By viewing the algorithm as a succession of queueing processes we show that the expected wasted space for FFD bin-packing is bounded above by 9.4 bins, independent of N. We extend this upper bound to a FFD bin-packing of items in accordance with a non-homogeneous Poisson process with a nonincreasing intensity function &#x03BB;(t) on [0,1/2].
[Computer science, Upper bound, H infinity control, Mathematics, History]
Fast solution of some random NP-hard problems
27th Annual Symposium on Foundations of Computer Science
None
1986
false
[NP-hard problem, Educational institutions, Polynomials, Concrete, Random variables, Partitioning algorithms, Computational complexity]
Complexity classes in communication complexity theory
27th Annual Symposium on Foundations of Computer Science
None
1986
We take a complexity theoretic view of A. C. Yao's theory of communication complexity. A rich structure of natural complexity classes is introduced. Besides providing a more structured approach to the complexity of a variety of concrete problems of interest to VLSI, the main objective is to exploit the analogy between Turing machine (TM) and communication complexity (CC) classes. The latter provide a more amicable environment for the study of questions analogous to the most notorious problems in TM complexity. Implicitly, CC classes corresponding to P, NP, coNP, BPP and PP have previously been considered. Surprisingly, pcc = Npcc &#x02229; coNPcc is known [AUY]. We develop the definitions of PSPACEcc and of the polynomial time hierarchy in CC. Notions of reducibility are introduced and a natural complete member in each class is found. BPPcc &#x02286; &#x03A3;2cc &#x02229; &#x03A0;2cc [Si2] remains valid. We solve the question that BPPcc &#x02289; NPcc by proving an &#x03A9;(&#x0221A;n) lower bound for the bounded-error complexity of the coNPcc- complete problem "disjointness". Similar lower bounds follow for essentially any nontrivial monotone graph property. Another consequence is that the deterministically exponentially hard "equality" relation is not NPcc-hard with respect to oracle-protocol reductions. We prove that the distributional complexity of the disjointness problem is O(&#x0221A;n log n) under any product measure on {0, 1}n &#x0D7; {0, 1}n. This points to the difficulty of improving the &#x03A9;(&#x0221A;n) lower bound for the B2PP complexity of "disjointness". The variety of counting and probabilistic classes appears to be greater than in the Turing machine versions. Many of the simplest graph problems (undirected reachability, planarity, bipartiteness, 2-CNF-satisfiability) turn out to be PSPACEcc-hard. The main open problem remains the separation of the hierarchy, more specifically, the conjecture that &#x03A3;2cc &#x02260; &#x03A0;2cc. Another major problem is to show that PSPACEcc and the probabilistic class UPPcc are not comparable.
[Context, Protocols, Boolean functions, Costs, Turing machines, Very large scale integration, Concrete, Polynomials, Complexity theory, Testing]
A new pebble game that characterizes parallel complexity classes
27th Annual Symposium on Foundations of Computer Science
None
1986
A new two-person pebble game that models parallel computations is defined. This game extends the two-person pebble game defined in [DT85] and is used to characterize two natural parallel complexity classes, namely LOGCFL and AG1. The characterizations show a fundamental way in which the computations in these two classes differ. This game model also unifies the proofs of some well known results of complexity theory.
[Concurrent computing, Computer science, Turing machines, Computational modeling, Circuits, Switches, Phase change random access memory, Polynomials, Complexity theory, Game theory]
k+1 heads are better than k for PDA's
27th Annual Symposium on Foundations of Computer Science
None
1986
We resolve the following long-standing conjecture of Harrison and Ibarra in 1968 [HI, p.462]: There are languages accepted by (k+1)-head 1-way deterministic pushdown automata ((k+1)-DPDA) but not by k-head 1-way pushdown automata (k-PDA), for every k. (Partial solutions for this conjecture can be found in [M1,M2,C].) On the assumption that their conjecture holds, [HI] also derived many important consequences. Now all those consequences become theorems. For example, the class of languages accepted by k-PDA's is not closed under &#x02229; and complementation. Several other interesting consequences also follow: CFL &#x02286;&#x0222A;kDPDA(k) and FA(2)&#x02286;&#x0222A;kDPDA(k), where DPDA (k)={L|L is accepted by a k-DPDA} and FA(2)={L|L is accepted by a 2-head FA). Our new proof itself is also interesting in the sense that the k+l versus k heads problems was solved by diagonalization methods [I2,M2,M3,M4,S] for stronger machines (2-way, etc). and by traditional counting arguments [S2,IK,YR,M1] for weaker machines (k-FA, k-head counter machine, etc).
[Information science, Laboratories, Automata, Formal languages, Writing, Magnetic heads, Informatics, Personal digital assistants, Counting circuits]
On the power of interaction
27th Annual Symposium on Foundations of Computer Science
None
1986
A hierarchy of probabilistic complexity classes generalizing NP has recently emerged in the work of [B], [GMR], and [GS]. The IP hierarchy is defined through the notion of an interactive proof system, in which an all powerful prover tries to convince a probabilistic polynomial time verifier that a string x is in a language L. The verifier tosses coins and exchanges messages back and forth with the prover before he decides whether to accept x. This proof-system yields "probabilistic" proofs: the verifier may erroneously accept or reject x with small probability. The class IP[f(|x|)] is said to contain L if, there exists an interactive proof system with f(|x|)- message exchanges (interactions) such that with high probability the verifier accepts x if and only if x &#x03B5; L. Babai [B] showed that all languages recognized by interactive proof systems with bounded number of interactions, can be recognized by interactive proof systems with only two interactions. Namely, for every constant k, IP[k] collapses to Ip[2]. In this paper, we give evidence that interactive proof systems with unbounded number of interactions may be more powerful than interactive proof systems with bounded number of interactions. We show that for any unbounded function f(n) there exists an oracle B such that IPB [f(|x|)] &#x02284; PHB. This implies that IPB[f(n)] &#x02260; IPB[2], since IPB[2] &#x02286; &#x03A0;2B for all oracles B. The techniques employed are extensions of the techniques for proving lower bounds on small depth circuits used in [FSS], [Y] and [H1].
[Computer science, Frequency selective surfaces, Power engineering computing, Laboratories, Circuits, Polynomials, Mathematics, Optical character recognition software]
Collapsing degrees
27th Annual Symposium on Foundations of Computer Science
None
1986
An m-degree is a collection of sets equivalent under polynomial-time many-one (Karp) reductions; for example, the complete sets for NP or PSPACE are m-degrees. An m-degree is collapsing iff its members are p-isomorphic, i.e., equivalent under polynomial time, 1-1, onto, polynomial time invertible reductions. L. Berman and J. Hartmanis showed that all the then known natural NP-complete sets are isomorphic, and conjectured that the m-degree of the NP-complete sets collapses, in essence claiming that there is only one NP-complete set. However, until now no nontrivial collapsing m-degree was known to exist. In this paper we provide the first examples of such degrees, In particular, we show that there is a collapsing degree which is btt-complete for EXP (the exponential time decidable sets) and that, for every set A, there is a collapsing degree which is hard for A. We also obtain analogous results for noncollapsing degrees.
[Computer science, Polynomials, Encoding, Complexity theory, NP-complete problem]
Three results on the polynomial isomorphism of complete sets
27th Annual Symposium on Foundations of Computer Science
None
1986
This paper proves three results relating to the isomorphism question for NP-complete sets. Result 1: We construct an oracle A such that SATA is &#x02264;mP- complete for NPA and all &#x02264;mP-complete sets for NPA are pA- isomorphic to SATA. Result 2: We construct a time function T(n) such that DTIME(T(n)) contains btt-complete sets, which are many-one equivalent, but are not p-isomorphic. The proof of this result has two corollaries: 1) There is an oracle, D, such that NPD contains non-p-isomorophic &#x02264;m(D),P-complete sets. 2) There is a &#x02264;mP-degree that contains non-p-isomorphic sets. Result 3: We show that no simple modification of the diagonalization argument used by Ko, Long and Du can be used to produce sets that are both EXPtime-complete w.r.t, polynomial many-one reducibility and not p-isomorphic.
[Terminology, Polynomials, Mathematics]
Permanent and determinant
27th Annual Symposium on Foundations of Computer Science
None
1986
The n &#x0D7; n-permanent is not a projection of the m &#x0D7; m-determinant if m &#x02264; &#x0221A;8/7 n - 1.
[Computer science, Geometry, Algebra, Analog computers, Polynomials, Arithmetic]
Time-space tradeoffs for branching programs contrasted with those for straight-line programs
27th Annual Symposium on Foundations of Computer Science
None
1986
This paper establishes time-space tradeoffs for some algebraic problems in the branching program model. For a finite field F, convolution of n-vectors over F requires ST = &#x0398;(n2 log |F|), where S is space and T is time, in good agreement with corresponding results for straightline programs. Our result for n &#x0D7; n matrix multiplication over F, ST2 = &#x0398;(n6 log |F|), is stronger than the previously known bound ST = &#x03A9;(n3) for straight-line and branching programs. The problem of computing PAQ, where P and Q are n &#x0D7; n permutation matrices and A is a particular matrix, requires &#x03A9;(n3) &#x02264; ST &#x02264; O(n3logn) for branching programs, in contrast to ST = &#x03A9;(n4) for straight-line programs.
[Computer science, Costs, Convolution, Computational modeling, Discrete Fourier transforms, Merging, Binary decision diagrams, Extraterrestrial measurements, Galois fields, Sorting]
Meanders, Ramsey theory and lower bounds for branching programs
27th Annual Symposium on Foundations of Computer Science
None
1986
A novel technique for obtaining lower bounds for the time versus space complexity of certain functions in a general input oblivious sequential model of computation is developed. This is demonstrated by studying the intrinsic complexity of the following set equality problem SE(n,m): Given a sequence x1,x2,....,xn, y1,....,yn of 2n numbers of m bits each, decide whether the sets [x1,....,xn] and [y1,...,yn] coincide. We show that for any log log n &#x02264; m &#x02264;1/2log n and any 1 &#x02264; s &#x02264; log n, any input oblivious sequential computation that solves SE(n,m) using 2m/s space, takes &#x03A9;(n ? s) time. This result is sharp for all admissible values of n,m,s and is the first known nontrivial time space tradeoff lower bound (for space = &#x03C9; (log n) of a set recognition problem on such a general model of computation. Our method also supplies lower bounds on the length of arbitrary (not necessarily input oblivious) branching programs for several natural symmetric functions, improving results of Chandra, Furst and Lipton, of Pudl&#x0E1;k and of Ajtai et. al. For example we show that for the majority - function any branching program of width w(n) has length &#x03C9;(n &#x000B7; log w/n (n) &#x000B7; log w (n)), in particular for bounded width we get length &#x03C9; (n log n) (independently of our work Babai et. al. [BPRS] have simultaneously proved this last result). Our lower bounds for branching programs imply lower bounds on the number of steps that are needed to pebble arbitrary computation graphs for the same computational problems. To establish our lower bounds we introduce the new concept of a meander that captures superconcentrator-type properties of sequences. We prove lower bounds on the length of meanders via a new Ramsey theoretic lemma that is of interest in its own right. This lemma has other applications, including a tight lower bound on the size of weak superconcentrators of depth 2 that strengthens the known lower bound of Pippenger [Pi]. A surprising new feature of these applications of Ramsey theory in lower bound arguments is the fact that no numbers are required to be unusually large and that several of the resulting superlinear lower bounds are in fact optimal.
[Computational modeling, Input variables, Binary decision diagrams, Time sharing computer systems, Business process re-engineering, Decision trees]
The token distribution problem
27th Annual Symposium on Foundations of Computer Science
None
1986
false
[Concurrent computing, Algorithm design and analysis, Multiprocessor interconnection networks, Routing, Hypercubes, Iterative algorithms, Computer networks, Communication networks, Distributed computing, Sorting]
Separator-based strategies for efficient message routing
27th Annual Symposium on Foundations of Computer Science
None
1986
Message routing strategies are given for networks with certain separator properties. These strategies use considerably less space than complete routing tables, keep node names to O(log n) bits, and still route along near-shortest paths. For any network with separators of size at most a small constant c, a total of O(n log n) items of routing information is stored, and any message is routed along a path of length at most (2/&#x03B1;) + 1 times the length of an optimal path, where &#x03B1; &#x226B; 1 is the positive root of the equation &#x03B1;&#x02308;(c+1)/2&#x02309; - &#x03B1; - 2 = 0. For planar networks, O(n1+&#x03B5;) items are stored, for any constant &#x03B5;, 0 &#x226A; &#x03B5; &#x226A; 1/3, and the length of any message path is at most 7 times that of an optimal path.
[Computer science, Costs, Network topology, Particle separators, Routing, Encoding, Labeling, Equations]
Parallel complexity of logical query programs
27th Annual Symposium on Foundations of Computer Science
None
1986
We consider the parallel time complexity of logic programs without function symbols, called logical query programs, or Datalog programs. We give a PRAM algorithm for computing the minimum model of a logical query program, and show that for programs with the "polynomial fringe property," this algorithm runs in logarithmic time. As a result, the "linear" and "piecewise linear" classes of logic programs are in NC. Then we examine several nonlinear classes in which the program has a single recursive rule that is an "elementary chain" We show that certain nonlinear programs are related to GSM mappings of a balanced parentheses language, and that this relationship implies the "polynomial fringe property;" hence such programs are in NC. Finally, we describe an approach for demonstrating that certain logical query programs are log space complete for P, and apply it to both elementary single rule programs and nonelementary programs.
[GSM, Concurrent computing, Algebra, Piecewise linear techniques, Relational databases, Very large scale integration, Phase change random access memory, Polynomials, Logic, Contracts]
On the power one-way communication
27th Annual Symposium on Foundations of Computer Science
None
1986
We look at a very simple model of parallel computation and study the question of how restricting the flow of data to be one-way compares with two-way flow. A one-way linear iterative array (1LIA) is a finite one-dimensional array of identical finite-state machines (cells) in which information is allowed to move only in one direction- from left to right. For inputs of length n, the array uses n cells which are initially set to the quiescent state. The serial input, which is applied to the leftmost cell, is accepted if the rightmost cell ever enters an accepting state. We give results which show that 1LIA's are surprisingly very powerful in that they can accept languages which seemingly require two-way communication.
[Computer science, Concurrent computing, Computational modeling, Automata, Cellular manufacturing, Polynomials, Data flow computing, Complexity theory, Clocks]
An efficient parallel algorithm for planarity
27th Annual Symposium on Foundations of Computer Science
None
1986
We describe a parallel algorithm for testing a graph for planarity, and for finding an embedding of a planar graph. For a graph on n vertices, the algorithm runs in O(log2 n) steps on n processors of a parallel RAM. The previous best algorithm for planarity testing in parallel polylog time ([Ja'Ja' and Simon, 82]) used a reduction to solving linear systems, and hence required &#x03A9;(n2..49...) processors by known methods, whereas our processor bounds are within a polylog factor of optimal. The most significant aspect of our parallel algorithms is the use of a sophisticated data structure for representing sets of embeddings, the PQ-tree of [Booth and Lueker, 76]. Previously no parallel algorithms for PQ-trees were known. We have efficient parallel algorithms for manipulating PQ-trees, which we use in our planarity algorithm.
[Computer science, Linear systems, Semiconductor device modeling, System testing, Transmission line matrix methods, Laboratories, Read-write memory, Data structures, Parallel algorithms, Contracts]
Approximate and exact parallel scheduling with applications to list, tree and graph problems
27th Annual Symposium on Foundations of Computer Science
None
1986
We study two parallel scheduling problems and their use in designing parallel algorithms. First, we define a novel scheduling problem; it is solved by repeated, rapid, approximate reschedulings. This leads to a first optimal PRAM algorithm for list ranking, which runs in logarithmic time. Our second scheduling result is for computing prefix sums of logn bit numbers. We give an optimal parallel algorithm for the problem which runs in sublogarithmic time. These two scheduling results together lead to logarithmic time PRAM algorithms for the connectivity, biconnectivity and minimum spanning tree problems. The connectivity and biconnectivity algorithms are optimal unless m = o(nlog*n), in graphs of n vertices and m edges.
[Algorithm design and analysis, Concurrent computing, Protocols, Tree graphs, Processor scheduling, Computational modeling, Phase change random access memory, Partitioning algorithms, Parallel algorithms, Scheduling algorithm]
An optimal randomized parallel algorithm for finding connected components in a graph
27th Annual Symposium on Foundations of Computer Science
None
1986
We present a parallel randomized algorithm for finding the connected components of an undirected graph. Our algorithm takes T = O(log (n)) time and p = O(m+n/(log(n) processors, where m = number of edges and n = number of vertices. This algorithm improves the results of Cole and Vishkin1, which use O(log (n)&#x000B7;log (log (n))&#x000B7; log (log (log (n)))) time. Our algorithm is Optimal in the sense that the product P&#x000B7;T is a linear function of the input size. The algorithm requires O(m + n) space which is the input size, so it is Optimal in space as well.
[Computer science, Concurrent computing, Computational modeling, Read-write memory, Phase change random access memory, Polynomials, Parallel algorithms, Random number generation, Testing]
Tight complexity bounds for parallel comparison sorting
27th Annual Symposium on Foundations of Computer Science
None
1986
The time complexity of sorting n elements using p &#x02265; n processors on Valiant's parallel comparison tree model is considered. The following results are obtained. 1. We show that this time complexity is &#x0398;(logn/log(1+p/n)). This complements the AKS sorting network in settling the wider problem of comparison sort of n elements by p processors, where the problem for p &#x02264; n was resolved. To prove the lower bound, we show that to achieve time k &#x02264; logn, we need &#x03A9;(kn1+1/k) comparisons. H&#x0E4;ggkvist and Hell proved a similar result only for fixed k. 2. For every fixed time k, we show that: (a) &#x03A9;(n1+1/k lognl/k) comparisons are required, (O(n1+1/k logn) are known to be sufficient in this case), and (b) there exists a randomized algorithm for comparison sort in time k with an expected number of O(n1+1/k) comparisons. This implies that for every fixed k, any deterministic comparison sort algorithm must be asymptotically worse than this randomized algorithm. The lower bound improves on H&#x0E4;ggkvist-Hell's lower bound. 3. We show that "approximate sorting" in time 1 requires asymptotically more than nlogn processors. This settles a problem raised by M. Rabin.
[Computer science, Concurrent computing, Performance evaluation, Read-write memory, Phase change random access memory, Time measurement, Mathematics, Decision trees, Mathematical model, Sorting]
Parallel merge sort
27th Annual Symposium on Foundations of Computer Science
None
1986
We give a parallel implementation of merge sort on a CREW PRAM that uses n processors and O(logn) time; the constant in the running time is small. We also give a more complex version of the algorithm for the EREW PRAM; it also uses n processors and O(logn) time. The constant in the running time is still moderate, though not as small.
[Algorithm design and analysis, Computational modeling, Circuits, Merging, Read-write memory, Phase change random access memory, Graph theory, Parallel algorithms, Sorting, Radio access networks]
Foreword
28th Annual Symposium on Foundations of Computer Science
None
1987
Presents the introductory welcome message from the conference proceedings.
[]
Polytope range searching and integral geometry
28th Annual Symposium on Foundations of Computer Science
None
1987
false
[Computer science, Computational geometry, Strips, Upper bound, Multidimensional systems, Virtual manufacturing, Slabs, Organizing, Arithmetic]
An output sensitive algorithm for computing visibility graphs
28th Annual Symposium on Foundations of Computer Science
None
1987
The visibility graph of a set of nonintersecting polygonal obstacles in the plane is an undirected graph whose vertices are the vertices of the obstacles and whose edges are pairs of vertices (u, v) such that the open line segment between u and v does not intersect any of the obstacles. The visibility graph is an important combinatorial structure in computational geometry and is used in applications such as solving visibility problems and computing shortest paths. An algorithm is presented that computes the visibility graph of s set of obstacles in time O(E + n log n), where E is the number of edges in the visibility graph and n is the total number of vertices in all the obstacles.
[Computer science, Computational geometry, Automation, Tree graphs, Military computing, Fingers, Computer applications, Educational institutions, Sorting, Clocks]
Delaunay graphs are almost as good as complete graphs
28th Annual Symposium on Foundations of Computer Science
None
1987
Let S be any set of N points in the plane and let DT(S) be the graph of the Delaunay triangulation of S. For all points a and b of S, let d(a, b) be the Euclidean distance from a to b and let DT(a, b) be the length of the shortest path in DT(S) from a to b. We show that there is a constant c(&#x02264; 1+&#x0221A;5/2 &#x03C0; &#x02248; 5.08) independent of S and N such that DT(a, b)/d(a, b) &#x226A; c.
[Computer science, Tree data structures, Upper bound, Tree graphs, Scholarships, Spine, Transportation, Euclidean distance, Data structures, Joining processes]
On the lower envelope of bivariate functions and its applications
28th Annual Symposium on Foundations of Computer Science
None
1987
We consider the problem of obtaining sharp (nearly quadratic) bounds for the combinatorial complexity of the lower envelope (i.e. pointwise minimum) of a collection of n bivariate (or generally multi-variate) continuous and "simple" functions, and of designing efficient algorithms for the calculation of this envelope. This problem generalizes the well-studied univariate case (whose analysis is based on the theory of Davenport-Schinzel sequences), but appears to be much more difficult and still largely unsolved. It is a central problem that arises in many areas in computational and combinatorial geometry, and has numerous applications including generalized planar Voronoi diagrams, hidden surface elimination for intersecting surfaces, purely translational motion planning, finding common transversals of polyhedra, and more. In this abstract we provide several partial solutions and generalizations of this problem, and apply them to the problems mentioned above. The most significant of our results is that the lower envelope of n triangles in three dimensions has combinatorial complexity at most O(n2&#x03B1;(n)) (where &#x03B1;(n) is the extremely slowly growing inverse of Ackermann's function), that this bound is tight in the worst case, and that this envelope can be calculated in time O(n2&#x03B1;(n)).
[Jacobian matrices, Computer science, Algorithm design and analysis, Computational geometry, Gold, Upper bound, Tin, Application software]
A new algebraic method for robot motion planning and real geometry
28th Annual Symposium on Foundations of Computer Science
None
1987
We present an algorithm which solves the findpath or generalized movers' problem in single exponential sequential time. This is the first algorithm for the problem whose sequential time bound is less than double exponential. In fact, the combinatorial exponent of the algorithm is equal to the number of degrees of freedom, making it worst-case optimal, and equaling or improving the time bounds of many special purpose algorithms. The algorithm accepts a formula for a semi-algebraic set S describing the set of free configurations and produces a one-dimensional skeleton or "roadmap" of the set, which is connected within each connected component of S. Additional points may be linked to the roadmap in linear time. Our method draws from results of singularity theory, and in particular makes use of the notion of stratified sets as an efficient alternative to cell decomposition. We introduce an algebraic tool called the multivariate resultant which gives a necessary and sufficient condition for a system of homogeneous polynomials to have a solution, and show that it can be computed in polynomial parallel time. Among the consequences of this result are new methods for quantifier elimination and an improved gap theorem for the absolute value of roots of a system of polynomials.
[Robot motion, Motion planning, Computational geometry, Space technology, Laboratories, Polynomials, Path planning, Artificial intelligence, Contracts, Orbital robotics]
New lower bound techniques for robot motion planning problems
28th Annual Symposium on Foundations of Computer Science
None
1987
We present new techniques for establishing lower bounds in robot motion planning problems. Our scheme is based on path encoding and uses homotopy equivalence classes of paths to encode state. We first apply the method to the shortest path problem in 3 dimensions. The problem is to find the shortest path under an Lp metric (e.g. a euclidean metric) between two points amid polyhedral obstacles. Although this problem has been extensively studied, there were no previously known lower bounds. We show that there may be exponentially many shortest path classes in single-source multiple-destination problems, and that the single-source single-destination problem is NP-hard. We use a similar proof technique to show that two dimensional dynamic motion planning with bounded velocity is NP-hard. Finally we extend the technique to compliant motion planning with uncertainty in control. Specifically, we consider a point in 3 dimensions which is commanded to move in a straight line, but whose actual motion may differ from the commanded motion, possibly involving sliding against obstacles. Given that the point initially lies in some start region, the problem of finding a sequence of commanded velocities which is guaranteed to move the point to the goal is shown to be non-deterministic exponential time hard, making it the first provably intractable problem in robotics.
[Robot motion, Motion planning, Uncertainty, Laboratories, Path planning, Encoding, Polynomials, Technology planning, Contracts, Orbital robotics]
Learning one-counter languages in polynomial time
28th Annual Symposium on Foundations of Computer Science
None
1987
We demonstrate that the class of languages accepted by deterministic one-counter machines, or DOCAs (a natural subset of the context-free languages), is learnable in polynomial time. Our learning protocol is based upon Angluin's concept of a "minimally adequate teacher" who can answer membership queries about a concept and provide counterexamples to incorrect hypothesized concepts. We also demonstrate that the problem of testing DOCAs for equivalence may be solved in polynomial time, answering a question posed by Valiant and Paterson.
[Computer science, Extrapolation, Protocols, Learning automata, Doped fiber amplifiers, Machine learning, Polynomials, Testing, Counting circuits]
Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm
28th Annual Symposium on Foundations of Computer Science
None
1987
Valiant and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss on-line learning of these functions. In on-line learning, the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in the on-line setting is the number of mistakes the learner makes. For suitable classes of functions, on-line learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm, which learns disjunctive Boolean functions, and variants of the algorithm for learning other classes of Boolean functions. The algorithm can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes that it makes is relatively little affected by the presence of large numbers of irrelevant attributes in the examples; we show that the number of mistakes grows only logarithmically with the number of irrelevant attributes. At the same time, the algorithm is computationaUy time and space efficient.
[Algorithm design and analysis, Computer vision, Boolean functions, Neural networks, Data preprocessing, Detectors, Feature extraction, Libraries, Computer networks, Pattern recognition]
Incomparability in parallel computation
28th Annual Symposium on Foundations of Computer Science
None
1987
We consider the relative power of concurrentwrite PRAMs when the number of processors (and input variables) is fixed at n, and infinite shared memory is allowed. Several different models (COMMON, ARBITRARY, PRIORITY) have been used for algorithm design in the literature; these models differ in their method of write-conflict resolution. Recent work in separating these models ([FRW1,2,3], [LY]) has relied on further restrictions (limiting the size of memory or the power of processors); the only unrestricted results known concern the element distinctness problem ([FMW], [RSSW]). In this paper we contribute further unrestricted results. We consider the COLLISION model, a natural generalization of the Ethernet ([G]). Our main result is a lower bound of &#x03A9;(logloglogn) steps on COLLISION for a problem that can be done in O(1) steps on ARBITRARY. We use this result, together with a reduction performed by means of Ramsey's Theorem, to show that the powers of COMMON and COLLISION are incomparable. We also introduce a new and natural model, TOLERANT, and show that it is strictly less powerful than COLLISION and incomparable with COMMON. The proofs involved use combinatorial arguments, including Tur&#x0E1;n's Theorem for graphs and the Erd&#x0F6;s-Rado intersecting set theorem.
[Concurrent computing, Algorithm design and analysis, Computer aided instruction, Ethernet networks, TV, Input variables, Writing, Phase change random access memory, Polynomials, Parallel algorithms]
Threshold circuits of bounded depth
28th Annual Symposium on Foundations of Computer Science
None
1987
We examine a powerful model of parallel computation: polynomial size threshold circuits of bounded depth (the gates compute threshold functions with polynomial weights). Lower bounds are given to separate polynomial size threshold circuits of depth 2 from polynomial size threshold circuits of depth 3, and from probabilistic polynomial size threshold circuits of depth 2. We also consider circuits of unreliable threshold gates, circuits of imprecise threshold gates and threshold quantifiers.
[Computer science, Concurrent computing, Boolean functions, Computational modeling, Circuit simulation, Brain modeling, Polynomials, Mathematics, Pattern recognition, Statistics]
Complete and incomplete randomized NP problems
28th Annual Symposium on Foundations of Computer Science
None
1987
false
[Computer science, IEEE news, Probability distribution, Optical character recognition software, Stress]
Generic oracles and oracle classes
28th Annual Symposium on Foundations of Computer Science
None
1987
In this paper, we examine various complexity issues relative to an oracle for a generic set in order to determine which are the more "natural" conjectures for these issues. Generic oracle results should be viewed as parallels to random oracle results, as in [BG]; the two are in many ways related, but, as we shall exhibit, not equivalent. Looking at computation relative to a generic oracle is in some ways a better reflection of computation without an oracle; for example, whereas adding a random oracle allows a deterministic polynomial-time machine to solve any problem in BPP, adding a generic oracle will not help solve any recursive problem faster than it could be solved without an oracle. Generic sets were first introduced by Cohen as a tool for proving independence results in set theory [Co]. Their recursion theoretic properties have also been explored in depth; for example, see [J] and [Ku2]. Some related work using forcing and/or generic sets as tools in oracle constructions can be found in [Ku3], [Do], [P], and [A-SFH]. However, this is to our knowledge the first knowledge the first thorough examination of complexity relative to a generic Oracle.
[Computer science, Lead, Set theory, Particle measurements, Polynomials, Mathematics, Reflection, Distributed computing, Testing]
Functional decomposition of polynomials
28th Annual Symposium on Foundations of Computer Science
None
1987
false
[Computer science, Concurrent computing, Fast Fourier transforms, Circuits, Polynomials, Mathematics, Modules (abstract algebra), Parallel algorithms, Galois fields, Cryptographic protocols]
Factoring polynomials over finite fields
28th Annual Symposium on Foundations of Computer Science
None
1987
We propose a new deterministic method of factoring polynomials over finite fields. Assuming the Generalized Riemann Hypothesis (GRH), we obtain, in polynomial time, the factorization of any polynomial with a bounded number of irreducible factors. Other consequences include a polynomial time algorithm to find a nontrivial factor of any completely splitting even degree polynomial when a quadratic nonresidue in the field is given.
[Automation, Hydrogen, Polynomials, Galois fields, Equations, Network address translation]
Multiplicative complexity of polynomial multiplication over finite fields
28th Annual Symposium on Foundations of Computer Science
None
1987
Let Mq(n) denote the number of multiplications required to compute the coefficients of the product of two polynomials of degree n over a q-element field by means of bilinear algorithms. It is shown that Mq(n) &#x02265; 3n - o(n). In particular, if q/2 &#x226A; n &#x02264; q + 1, we establish the tight bound Mq(n) = 3n + 1 - &#x0230A;q/2&#x0230B;. The technique we use can be applied to analysis of algorithms for multiplication of polynomials modulo a polynomial as well.
[Computer science, Algorithm design and analysis, Linear code, Interpolation, H infinity control, Polynomials, Galois fields]
The multiplicative complexity of quadratic Boolean functions
28th Annual Symposium on Foundations of Computer Science
None
1987
Let the multiplicative complexity L(f) of a boolean function f be the minimal number of &#x02227;-gates that are sufficient to evaluate f by circuits over the basis &#x02227;,&#x02295;,1. We give a polynomial time algorithm which for quadratic boolean forms f=&#x02295;i&#x02260;jaijxixj determines L(f) from the coefficients aij. Two quadratic forms f,g have the same complexity L(f) = L(g) iff they are isomorphic by a linear isomorphism. We also determine the multiplicative complexity of pairs of quadratic boolean forms. We give a geometric interpretation to the complexity L(f1,f2) of pairs of quadratic forms.
[Boolean functions, Symmetric matrices, Circuits, Linear algebra, Polynomials, Vectors, Complexity theory, Galois fields, Kernel]
Cascading divide-and-conquer: A technique for designing parallel algorithms
28th Annual Symposium on Foundations of Computer Science
None
1987
We present techniques for parallel divide-and-conquer, resulting in improved parallel algorithms for a number of problems. The problems for which we give improved algorithms include intersection detection, trapezoidal decomposition (hence, polygon triangulation), and planar point location (hence, Voronoi diagram construction). We also give efficient parallel algorithms for fractional cascading, 3-dimensional maxima, 2-set dominance counting, and visibility from a point. All of our algorithms run in O(log n) time with either a linear or sub-linear number of processors in the CREW PRAM model.
[Algorithm design and analysis, Concurrent computing, Computational modeling, Read-write memory, Phase change random access memory, Data structures, Face detection, Parallel algorithms]
A new parallel algorithm for the maximal independent set problem
28th Annual Symposium on Foundations of Computer Science
None
1987
A new parallel algorithm for the maximal independent set problem (MIS) is constructed. It runs in O(log4 n) time when implemented on a linear number of EREW-processors. This is the first deterministic algorithm for MIS whose running time is polylogarithmic and whose processor-time product is optimal up to a polylogarithmic factor.
[Computer science, Concurrent computing, Algorithm design and analysis, Computational modeling, Phase change random access memory, Polynomials, Time measurement, Parallel algorithms, Computational complexity]
The matching problem for bipartite graphs with polynomially bounded permanents is in NC
28th Annual Symposium on Foundations of Computer Science
None
1987
It is shown that the problem of deciding and constructing a perfect matching in bipartite graphs G with the polynomial permanents of their n &#x0D7; n adjacency matrices A (perm(A) = nO(1)) are in the deterministic classes NC2 and NC3, respectively. We further design an NC3 algorithm for the problem of constructing all perfect matchings (enumeration problem) in a graph G with a permanent bounded by O(nk). The basic step was the development of a new symmetric functions method for the decision algorithm and the new parallel technique for the matching enumerator problem. The enumerator algorithm works in O(log3 n) parallel time and O(n3k+5.5 &#x000B7; log n) processors. In the case of arbitrary bipartite graphs it yields an 'optimal' (up to the log n- factor) parallel time algorithm for enumerating all the perfect matchings in a graph. It entails also among other things an efficient NC3-algorithm for computing small (polynomially bounded) arithmetic permanents, and a sublinear parallel time algorithm for enumerating all the perfect matchings in graphs with permanents up to 2n&#x03B5;.
[Algorithm design and analysis, Concurrent computing, Computer science, Circuits, Polynomials, Mathematics, Bipartite graph, Scheduling algorithm, Arithmetic, Testing]
Some polynomial and Toeplitz matrix computations
28th Annual Symposium on Foundations of Computer Science
None
1987
false
[Concurrent computing, Upper bound, Computer applications, Polynomials, Complexity theory, Newton method, Parallel algorithms, Circuit testing, Arithmetic]
How to emulate shared memory
28th Annual Symposium on Foundations of Computer Science
None
1987
We present a simple algorithm for emulating an N processor CRCW PRAM on an N node butterfly. Each step of the PRAM is emulated in time O(log N) with high probability, using FIFO queues of size O(1) at each node. The only use of randomization is in selecting a hash function to distribute the shared address space of the PRAM onto the nodes of the butterfly. The routing itself is both deterministic and oblivious, and messages are combined without the use of associative memories or explicit sorting. As a corollary we improve the result of Pippenger [8] by routing permutations with bounded queues in logarithmic time, without the possibility of deadlock. Besides being optimal, our algorithm has the advantage of extreme simplicity and is readily suited for use in practice.
[Computer science, Associative memory, Multicast algorithms, Emulation, Read-write memory, System recovery, Phase change random access memory, Routing, Computer networks, Sorting]
The complexity of parallel comparison merging
28th Annual Symposium on Foundations of Computer Science
None
1987
We prove a worst case lower bound of &#x03A9;(log log n) for randomized algorithms merging two sorted lists of length n in parallel using n processors on Valiant's parallel computation tree model. We show how to strengthen this result to a lower bound for the expected time taken by any algorithm on the uniform distribution. Finally, bounds are given for the average time required for the problem when the number of processors is less than and greater than n.
[Concurrent computing, Upper bound, Computational modeling, Scholarships, Merging, Laboratories, Parallel processing, Probability distribution, Positron emission tomography, Sorting]
Hierarchical memory with block transfer
28th Annual Symposium on Foundations of Computer Science
None
1987
In this paper we introduce a model of Hierarchical Memory with Block Transfer (BT for short). It is like a random access machine, except that access to location x takes time f(x), and a block of consecutive locations can be copied from memory to memory, taking one unit of time per element after the initial access time. We first study the model with f(x) = x&#x03B1; for 0 &#x226A; &#x03B1; &#x226A; 1. A tight bound of &#x03B8;(n log log n) is shown for many simple problems: reading each input, dot product, shuffle exchange, and merging two sorted lists. The same bound holds for transposing a &#x0221A;n &#x0D7; &#x0221A;n matrix; we use this to compute an FFT graph in optimal &#x03B8;(n log n) time. An optimal &#x03B8;(n log n) sorting algorithm is also shown. Some additional issues considered are: maintaining data structures such as dictionaries, DAG simulation, and connections with PRAMs. Next we study the model f(x) = x. Using techniques similar to those developed for the previous model, we show tight bounds of &#x03B8;(n log n) for the simple problems mentioned above, and provide a new technique that yields optimal lower bounds of &#x03A9;(n log2n) for sorting, computing an FFT graph, and for matrix transposition. We also obtain optimal bounds for the model f(x)= x&#x03B1; with &#x03B1; &#x226B; 1. Finally, we study the model f(x) = log x and obtain optimal bounds of &#x03B8;(n log*n) for simple problems mentioned above and of &#x03B8;(n log n) for sorting, computing an FFT graph, and for some permutations.
[Dictionaries, Computational modeling, Merging, Hidden Markov models, Random access memory, Read-write memory, Data structures, Phase change random access memory, Registers, Sorting]
Approximation algorithms for scheduling unrelated parallel machines
28th Annual Symposium on Foundations of Computer Science
None
1987
We consider the following scheduling problem. There are m parallel machines and n independent jobs. Each job is to be assigned to one of the machines. The processing of job j on machine i requires time pij. The objective is to find a schedule that minimizes the makespan. Our main result is a polynomial algorithm which constructs a schedule that is guaranteed to be no longer than twice the optimum. We also present a polynomial approximation scheme for the case that the number of machines is fixed. Both approximation results are corollaries of a theorem about the relationship of a class of integer programming problems and their linear programming relaxations. In particular, we give a polynomial method to round the fractional extreme points of the linear program to integral points that nearly satisfy the constraints. In contrast to our main result, we prove that no polynomial algorithm can achieve a worst-case ratio less than 3/2 unless P = NP. We finally obtain a complexity classification for all special cases with a fixed number of processing times.
[Algorithm design and analysis, Computer science, Processor scheduling, Parallel machines, Approximation algorithms, Linear programming, Polynomials, Mathematics, Performance analysis, Scheduling algorithm]
Finding near optimal separators in planar graphs
28th Annual Symposium on Foundations of Computer Science
None
1987
A k-ratio edge separator is a set of edges which separates a weighted graph into two disconnected sets of components neither of which contains more than k-1/k of the original graph's weight. An optimal quotient separator is an edge separator where the size of the separator (i.e., the number of edges) divided by the weight of the smaller set of components is minimized. An optimal quotient k-ratio separator is an edge separator where the size of the separator (i.e., the number of edges) divided by the smaller of either 1/k of the total weight or the weight of the smaller set of components is minimized. In this paper we present an algorithm that finds the optimal quotient k-ratio separator for any k &#x02265; 3. We use the optimal quotient algorithm to obtain approximation algorithms for finding optimal k-ratio edge separators for any k &#x02265; 3. Given a planar graph with a size OPT k-ratio separator, we describe an algorithm which a finds k-ratio separator which costs less than O(OPT log n). More importantly the algorithm finds ck-ratio separators (for any c &#x226B; 1) which cost less than C(c)OPT, where C(c) depends only on c.
[Computer science, Costs, Particle separators, Wires, Laboratories, Optimized production technology, Very large scale integration, Routing, Approximation algorithms, Optical character recognition software]
A parallel algorithm for finding a separator in planar graphs
28th Annual Symposium on Foundations of Computer Science
None
1987
We present a randomized parallel algorithm for finding a simple cycle separator in a planar graph. The size of the separator is O(&#x0221A;n) and it separates the graph so that the largest part contains at most 2/8 &#x000B7; n vertices. Our algorithm takes T = O(log2(n)) time and P = O(n + f1+&#x03B5;) processors, where n is the number of vertices, f is the number of faces and &#x03B5; is any positive constant. The algorithm is based on the solution of Lipton and Tarjan [8] for the sequential case which takes O(n) time. Combining our algorithm with the Pan and Reif [12] algorithm, enables us to find a BFS of planar graph in time O(log3(n)) using n1.5/log(n) processors. Using a variation of our algorithm we can construct a simple cycle separator of size O(d &#x000B7; &#x0221A;f) were d is maximum face size.
[Computer science, Algorithm design and analysis, Transmission line matrix methods, Numerical analysis, Tree graphs, Particle separators, Very large scale integration, Routing, Parallel algorithms]
Determining edge connectivity in 0(nm)
28th Annual Symposium on Foundations of Computer Science
None
1987
We describe an algorithm that determines the edge connectivity of an n-vertex m-edge graph G in O(nm) time. A refinement shows that the question as to whether a graph is k-edge connected can be determined in O(kn2). For dense graphs characterized by m = &#x03A9;(n2), the latter result implies that determination of whether a graph is k-edge connected for any fixed k can be accomplished in time linear in input size.
[Computer science, Algorithm design and analysis, Costs, Circuits, Reliability theory, Graph theory, Partitioning algorithms, Telecommunication network reliability, Communication networks, Chip scale packaging]
Improved algorithms for graph four-connectivity
28th Annual Symposium on Foundations of Computer Science
None
1987
We present a new algorithm based on ear decomposition for testing vertex four-connectivity and for finding all separating triplets in a triconnected graph. The sequential implementation of our algorithm runs in O(n2) time and the parallel implementation runs in O(logn) time using O(n2) processors on a CRCW PRAM, where n is the number of vertices in the graph. This improves previous bounds for the problem for both the sequential and parallel cases. The sequential algorithm is optimal if the input is specified in adjacency matrix form, or if the input graph is dense.
[Sequential analysis, Silicon carbide, Ear, Phase change random access memory, Argon, Joining processes, Testing]
Parallel graph algorithms that are efficient on average
28th Annual Symposium on Foundations of Computer Science
None
1987
The following three problems concerning random graphs can be solved in (log n)O(1) expected time using linearly many processors: (1) finding the lexicographically first maximal independent set, (2) coloring the vertices using a number of colors that is almost surely within twice the chromatic number, and (3) finding a Hamiltonian circuit.
[Algorithm design and analysis, Concurrent computing, Circuit analysis computing, Computational modeling, H infinity control, Color, Phase change random access memory, Polynomials, Parallel algorithms]
Canonical labeling of regular graphs in linear average time
28th Annual Symposium on Foundations of Computer Science
None
1987
An algorithm is presented to compute a canonical form of regular graphs. There is a constant c such that for each constant d the average running time of the algorithm over all d-regular graphs with N vertices is not greater than cNd, provided N is sufficiently large.
[Computer science, Heuristic algorithms, Polynomials, Mathematics, Partitioning algorithms, Labeling, Computational complexity, Testing]
Eigenvalues and graph bisection: An average-case analysis
28th Annual Symposium on Foundations of Computer Science
None
1987
Graph Bisection is the problem of partitioning the vertices of a graph into two equal-size pieces so as to minimize the number of edges between the two pieces. This paper presents an algorithm that will, for almost all graphs in a certain class, output the minimum-size bisection. Furthermore the algorithm will yield, for almost all such graphs, a proof that the bisection is optimal. The algorithm is based on computing eigenvalues and eigenvectors of matrices associated with the graph.
[Computer science, Algorithm design and analysis, Heuristic algorithms, H infinity control, Very large scale integration, Eigenvalues and eigenfunctions, Probability distribution, Polynomials, Mathematics, Partitioning algorithms]
On the second eigenvalue of random regular graphs
28th Annual Symposium on Foundations of Computer Science
None
1987
Expanders have many applications in Computer Science. It is known that random d-regular graphs are very efficient expanders, almost surely. However, checking whether a particular graph is a good expander is co-NP-complete. We show that the second eigenvalue of d-regular graphs, &#x03BB;2, is concentrated in an interval of width O(&#x0221A;d) around its mean, and that its mean is O(d3/4). The result holds under various models for random d-regular graphs. As a consequence a random d-regular graph on n vertices, is, with high probability a certifiable efficient expander for n sufficiently large. The bound on the width of the interval is derived from martingale theory and the bound on E(&#x03BB;2) is obtained by exploring the properties of random walks in random graphs.
[Computer science, Symmetric matrices, Stochastic processes, Routing, Eigenvalues and eigenfunctions, Graph theory, Polynomials, Application software, Convergence, Sorting]
Recursive construction for 3-regular expanders
28th Annual Symposium on Foundations of Computer Science
None
1987
We present an algorithm which in n3(log n)3 time constructs a 3- regular expander graph on n vertices. In each step we substitute a pair of edges of the graph by a new pair of edges so that the total number of cycles of length s = [c log n] decreases (for some fixed absolute constant c). When we reach a local minimum in the number of cycles of length s the graph is an expander. The proof is completely elementary, we use only the basic results about the eigenvalues and eigenvectors of symmetric matrices.
[Computer science, Symmetric matrices, Upper bound, Computational modeling, Computer simulation, Graph theory, Eigenvalues and eigenfunctions, Bipartite graph, Computational complexity, Sorting]
The organization of permutation architectures with bussed interconnections
28th Annual Symposium on Foundations of Computer Science
None
1987
This paper explores the problem of efficiently permuting data stored in VLSI chips in accordance with a predetermined set of permutations. By connecting chips with shared bus interconnections, as opposed to point-to-point interconnections, we show that the number of pins per chip can often be reduced. For example, for infinitely many n, we exhibit permutation architectures with &#x02308;&#x0221A;n&#x02309; pins per chip that can realize any of the n cyclic shifts on n chips in one clock tick. When the set of permutations forms a group with p elements, any permutation in the group can be realized in one clock tick by an architecture with O(&#x0221A;p lg p) pins per chip. When the permutation group is abelian, O(&#x0221A;p) pins suffice. These results are all derived from a mathematical characterization of uniform permutation architectures based on the combinatorial notion of a difference cover.
[Computer science, Costs, Wires, Laboratories, Communication system control, Computer architecture, Very large scale integration, Pins, Joining processes, Clocks]
Channel routing of multiterminal nets
28th Annual Symposium on Foundations of Computer Science
None
1987
This paper presents a new algorithm for channel routing of multiterminal nets. We first transform any multiterminal problem of density d to a socalled extended simple channel routing problem (ESCRP ) of density 3d/2+O(&#x0221A;dlog d), which will then be solved with channel width w &#x02264;3d/2+O(&#x0221A;dlog d) in the knock-knee model. The same strategy can be used for routing in the other two models: The channel width is w &#x02264; 3d/2+O(&#x0221A;dlog d)+O(f) in the Manhattan model, where f is the flux of the problem, and w &#x02264; 3d/2+O(&#x0221A;dlogd) in the unit-vertical-overlap model. In all three cases we improve the best known upper bounds.
[Wiring, Upper bound, Wires, Circuits, Routing]
Two lower bounds in asynchronous distributed computation
28th Annual Symposium on Foundations of Computer Science
None
1987
We introduce new techniques for deriving lower bounds on the message complexity in asynchronous distributed computation. These techniques combine the choice of specific patterns of communication delays and crossing sequence arguments with consideration of the speed of propagation of messages, together with careful counting of messages in different parts of the network. They enable us to prove the following results, settling two open problems: An &#x03A9;(n log* n) lower bound for the number of messages sent by an asynchronous algorithm for computing any nonconstant function on a bidirectional ring of n anonymous processors. An &#x03A9;(n log n) lower bound for the average number of messages sent by any maximum finding algorithm on a ring of n processors, in case n is known.
[Upper bound, Processor scheduling, Distributed computing, Propagation delay]
Distributive graph algorithms Global solutions from local data
28th Annual Symposium on Foundations of Computer Science
None
1987
This paper deals with distributed graph algorithms. Processors reside in the vertices of a graph G and communicate only with their neighbors. The system is synchronous and reliable, there is no limit on message lengths and local computation is instantaneous. The results: A maximal independent set in an n-cycle cannot be found faster than &#x03A9;(log* n) and this is optimal by [CV]. The d-regular tree of radius r cannot be colored with fewer than &#x0221A;d colors in time 2r / 3. If &#x0394; is the largest degree in G which has order n, then in time O(log*n) it can be colored with O(&#x0394;2) colors.
[Concurrent computing, Computer science, Distributed processing, Tree graphs, Computational modeling, Color, Mathematics, Labeling, Distributed computing, Power system modeling]
Achievable cases in an asynchronous environment
28th Annual Symposium on Foundations of Computer Science
None
1987
The paper deals with achievability of fault tolerant goals in a completely asynchronous distributed system. Fischer, Lynch, and Paterson [FLP] proved that in such a system "nontrivial agreement" cannot be achieved even in the (possible) presence of a single "benign" fault. In contrast, we exhibit two pairs of goals that are achievable even in the presence of up to t &#x226A; n/2 faulty processors, contradicting the widely held assumption that no nontrivial goals are attainable in such a system. The first pair deals with renaming processors so as to reduce the size of the initial name space. When only uniqueness is required of the new names, we present a lower bound of n + 1 on the size of the new name space, and a renaming algorithm which establishes an upper bound of n + t. In case the new names are required also to preserve the original order, a tight bound of 2t(n- t + 1) - 1 is obtained. The second pair of goals deals with the multi-slot critical section problem. We present algorithms for controlled access to a critical section. As for the number of slots required, a tight bound of t + 1 is proved in case the slots are identical. In the case of distinct slots the upper bound is 2t + 1.
[Upper bound, Computer aided software engineering, Fault tolerant systems, Distributed algorithms, Contracts]
Local management of a global resource in a communication network
28th Annual Symposium on Foundations of Computer Science
None
1987
We introduce a new primitive, the Resource Controller, which abstracts the problem of controlling the total amount of resources consumed by a distributed algorithm. We present an efficient distributed algorithm to implement this abstraction. The message complexity of our algorithm per participating node is polylogarithmic in the size of the network, compared to the linear cost per node of the naive algorithm. The implementation of our algorithm is simple and practical and the techniques used are interesting because a global quantity is managed in a distributed way. The Resource Controller can be used to construct efficient algorithms for a number of important problems, such as the problem of bounding the worst-case message complexity of a protocol and the problem of dynamically assigning unique names to nodes participating in a protocol.
[Computer science, Costs, Protocols, Communication system control, Mathematics, Resource management, Communication networks, Distributed algorithms, Contracts, Counting circuits]
Applying static network protocols to dynamic networks
28th Annual Symposium on Foundations of Computer Science
None
1987
This paper addresses the problem of how to adapt an algorithm designed for fixed topology networks to produce the intended results, when run in a network whose topology changes dynamically, in spite of encountering topological changes during its execution. We present a simple and unified procedure, called a reset procedure, which, when combined with the static algorithm, achieves this adaptation. The communication and time complexities of the reset procedure, per topological change, are independent of the number of topological changes and are linearly bounded by the size of the subset of the network which participates in the algorithm.
[Algorithm design and analysis, Protocols, Network topology, Change detection algorithms, ARPANET, Termination of employment, Data communication, Distributed algorithms, Contracts, Delay]
Bounded time-stamps
28th Annual Symposium on Foundations of Computer Science
None
1987
Time-stamps are numerical labels which enable a system to keep track of temporal precedence relation among its data elements. Traditionally time-stamps are used as unbounded numbers and inevitable overflows cause a loss of this precedence relation. In this paper we develop a complete theory of bounded time-stamps. Time-stamp systems are defined and the complexity of their implementation is fully analyzed. This theory gives a very general tool for converting timestamp based protocols to bounded protocols. The generality of this theory is demonstrated by novel, conceptually simple, protocols for a multiuser atomic registers, as well as by proving for the first time a non-trivial lower bound for such a register.
[Computer science, Protocols, Operating systems, Registers, Application software, Clocks]
Concurrent reading while writing II: The multi-writer case
28th Annual Symposium on Foundations of Computer Science
None
1987
An algorithm is given for the multi-writer version of the Concurrent Reading While Writing (CRWW) problem. The algorithm solves the problem of allowing simultaneous access to arbitrarily sized shared data without requiring waiting, and hence avoids mutual exclusion. This. demonstrates that a quite complicated concurrent control problem can be solved-without eliminating the efficiency of parallelism. One very important aspect of the algorithm are the tools developed to prove its correctness. Without these tools, proving the correctness of a solution to a problem of this complexity would be very difficult.
[Computer science, Protocols, Cogeneration, Writing, Parallel processing, History, Artificial intelligence]
Lower bounds to randomized algorithms for graph properties
28th Annual Symposium on Foundations of Computer Science
None
1987
For any property P on n-vertex graphs, let C(P) be the minimum number of edges that need to be examined by any decision tree algorithm for determining P. In 1975 Rivest and Vuillemin settled the Aanderra-Rosenberg Conjecture, proving that C(P) = &#x03A9;(n2) for every nontrivial monotone graph property P. An intriguing open question is whether the theorem remains true when randomized algorithms are allowed. In this paper we report progress on this problem, showing that &#x03A9;(n(log n)1/12) edges must be examined by a randomized algorithm for determining any nontrivial monotone graph property.
[Computer science, Boolean functions, Costs, Decision trees]
Exponential lower bounds for finding Brouwer fixed points
28th Annual Symposium on Foundations of Computer Science
None
1987
The Brouwer fixed point theorem has become a major tool for modeling economic systems during the 20th century. It was intractable to use the theorem in a computational manner until 1965 when Scarf provided the first practical algorithm for finding a fixed point of a Brouwer map. Scarf's work left open the question of worstcase complexity, although he hypothesized that his algorithm had "typical" behavior of polynomial time in the number of variables of the problem. Here we show that any algorithm for fixed points based on function evaluation (which includes all general purpose fixed-point algorithrna) must in the worst case take a number of steps which is exponential both in the number of digits of accuracy and in the number of variables.
[Upper bound, Runtime, Computational modeling, Power generation economics, Polynomials, Mathematics, Books, Mathematical model, History]
Database theory and cylindric lattices
28th Annual Symposium on Foundations of Computer Science
None
1987
false
[Algebra, Query processing, Lattices, Project management, Relational databases, Logic functions, EMP radiation effects, Argon, Remuneration, Equations]
Secret linear congruential generators are not cryptographically secure
28th Annual Symposium on Foundations of Computer Science
None
1987
This paper discusses the predictability of the sequence given by outputing a constant proportion &#x03B1; of the leading bits of the numbers produced by a linear congruential generator. First, we make the assumption that the modulus of the generator is the only known parameter and we prove that, almost surely, a significant proportion of the bits can be predicted from the previous ones, once the generator has been used K times successively where K is O(&#x0221A;log m). Next, we assume that all parameters of the generator are secret and we show how repeated observations of sequences of outputs of length K will probably allow an opponent to cryptanalyze the full sequence.
[Prediction algorithms, Polynomials, Cryptography, Leg]
A practical scheme for non-interactive verifiable secret sharing
28th Annual Symposium on Foundations of Computer Science
None
1987
This paper presents an extremely efficient, non-interactive protocol for verifiable secret sharing. Verifiable secret sharing (VSS) is a way of bequeathing information to a set of processors such that a quorum of processors is needed to access the information. VSS is a fundamental tool of cryptography and distributed computing. Seemingly difficult problems such as secret bidding, fair voting, leader election, and flipping a fair coin have simple one-round reductions to VSS. There is a constant-round reduction from Byzantine Agreement to non-interactive VSS. Non-interactive VSS provides asynchronous networks with a constant-round simulation of simultaneous broadcast networks whenever even a bare majority of processors are good. VSS is constantly repeated in the simulation of fault-free protocols by faulty systems. As verifiable secret sharing is a bottleneck for so many results, it is essential to find efficient solutions.
[Variable structure systems, Identity-based encryption, Tiles, Voting, Computational modeling, Nominations and elections, Access protocols, Broadcasting, Cryptography, Distributed computing]
Perfect zero-knowledge languages can be recognized in two rounds
28th Annual Symposium on Foundations of Computer Science
None
1987
A hierarchy of probabilistic complexity classes generalizing NP has recently emerged in the work of [Ba], [GMR], and [GS]. The IP hierarchy is defined through the notion of an interactive proof system, in which an all powerful prover tries to convince a probabilistic polynomial time verifier that a string w is in a language L. The verifier tosses coins and exchanges messages back and forth with the prover before he decides whether to accept w. This proof-system yields "probabilistic" proofs: the verifier may erroneously accept or reject w with small probability. In [GMR] such a protocol was defined to be a zero-knowledge protocol if at the end of the interaction the verifier has learned nothing except that w &#x02208; L. We study complexity theoretic implications of a language having this property. In particular we prove that if L admits a zeroknowledge proof then L can also be recognized by a two round interactive proof. This complements a result by Fortnow [F] where it is proved that the complement of L has a two round interactive proof protocol. The methods of proof are quite similar to those of Fortnow [F]. As in his case the proof works under the assumption that the original protocol is only zero-knowledge with respect to a specific verifier.
[Computer science, Protocols, Laboratories, Polynomials, Mathematics, Cryptography, Contracts]
Interactive proof systems: Provers that never fail and random selection
28th Annual Symposium on Foundations of Computer Science
None
1987
An interactive proof system with Perfect Completeness (resp. Perfect Soundness) for a language L is an interactive proof (for L) in which for every x &#x02208; L (resp. x &#x02209; L) the verifier always accepts (resp. always rejects). Zachos and Fuerer showed that any language having a bounded interactive proof has one with perfect completeness. We extend their result and show that any language having a (possibly unbounded) interactive proof system has one with perfect completeness. On the other hand, only languages in NP have interactive proofs with perfect soundness. We present two proofs of the main result. One proof extends Lautemann's proof that BPP is in the polynomial-time hierarchy. The other proof, uses a new protocol for proving approximately lower bounds and "random selection". The problem of random selection consists of a verifier selecting at random, with uniform probability distribution, an element from an arbitrary set held by the prover. Previous protocols known for approximate lower bound do not solve the random selection problem. Interestingly, random selection can be implemented by an unbounded Arthur-Merlin game but can not be implemented by a two-iteration game.
[Computer science, Protocols, Natural languages, Cities and towns, Polynomials, Mathematics, Probability distribution, Power generation]
On the cunning power of cheating verifiers: Some observations about zero knowledge proofs
28th Annual Symposium on Foundations of Computer Science
None
1987
In this paper we investigate some properties of zero-knowledge proofs, a notion introduced by Goldwasser, Micali and Rackoff. We introduce and classify various definitions of zero-knowledge. Two definitions which are of special interest are auxiliary-input zero-knowledge and blackbox-simulation zero-knowledge. We explain why auxiliary-input zero-knowledge is a definition more suitable for cryptographic applications than the original [GMR1] definition. In particular, we show that any protocol composed of subprotocols which are auxiliary-input zero-knowledge is itself auxiliary-input zero-knowledge. We show that blackbox simulation zero-knowledge implies auxiliary-input zeroknowledge (which in turn implies the [GMR1] definition). We argue that all known zero-knowledge proofs are in fact blackbox-simulation zero-knowledge (i.e. were proved zero-knowledge using blackbox-simulation of the verifier). As a result, all known zero-knowledge proof systems are shown to be auxiliary-input zero-knowledge and can be used for cryptographic applications such as those in [GMW2]. We demonstrate the triviality of certain classes of zero-knowledge proof systems, in the sense that only languages in BPP have zero-knowledge proofs of these classes. In particular, we show that any language having a Las vegas zeroknowledge proof system necessarily belongs to R. We show that randomness of both the verifier and the prover, and nontriviality of the interaction are essential properties of non-trivial auxiliary-input zero-knowledge proofs. In order to derive most of the results in the paper we make use of the full power of the definition of zero-knowledge: specifically, the requirement that there exist a simulator for any verifier, including "cheating verifiers".
[Computer science, Terminology, Access protocols, Polynomials, Cryptography, Cryptographic protocols]
Random self-reducibility and zero knowledge interactive proofs of possession of information
28th Annual Symposium on Foundations of Computer Science
None
1987
The notion of a zero knowledge interactive proof that one party "knows" some secret information is explored. It is shown that any "random self-reducible" problem has a zero knowledge interactive proof of this sort. The zero knowledge interactive proofs for graph isomorphism, quadratic residuosity, and "knowledge" of discrete logarithms all follow as special cases. Based on these results, new zero knowledge interactive proofs are exhibited for "knowledge" of the factorization of an integer, nonmembership in cyclic subgroups of Zp*, and determining whether an element generates Zp*. None of these proofs relies on any unproven assumptions.
[Computer science, Modular construction, Polynomials, Cryptographic protocols]
Correction to "A linear-time algorithm for triangulating a simple polygon"
28th Annual Symposium on Foundations of Computer Science
None
1987
In "A linear-time algorithm for triangulating a simple polygon" [Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing (1986), 380-388. 486], the analysis showing that the authors' triangulation algorithm runs in linear time is incorrect, and indeed the algorithm does not run in linear time in the worst case. So far they have been unable to obtain a linear-time algorithm for the triangulation problem. They have been able to obtain an O(n loglogn)-time algorithm, however. The details are described in "An O(n loglogn)-Time Algorithm for Triangulating a Simple Polygon," SIAM Journal on Computing 17, 1 (February, 1988), to appear.
[Computer science, Algorithm design and analysis]
Errata to "Atomic shared register access by asynchronous hardware," FOGS '86
28th Annual Symposium on Foundations of Computer Science
None
1987
The authors report that the paper by P.M.B. Vitanyi and B. Awerbuch, "Atomic shared register access by asynchronous hardware," Proc. 27th IEEE Symp. on Foundations of Computer Science (1986), 233-243, has an error in the 'Struldbrugg' algorithm (the main bounded tag algorithm) and a counterexample is presented. The "selection rule" of the reader is not sufficient to guarantee atomicity, but only the weaker regularity condition. Informally, regularity for multiwriter registers means that a reader does return a value written by a write, such that there is no complete other write strictly in between the durations of the read and the write it returns.
[Atomic measurements, Hardware]
The average complexity of deterministic and randomized parallel comparison sorting algorithms
28th Annual Symposium on Foundations of Computer Science
None
1987
In practice, the average time of (deterministic or randomized) sorting algorithms seems to be more relevant than the worst case time of deterministic algorithms. Still, the many known complexity bounds for parallel comparison sorting include no nontrivial lower bounds for the average time required to sort by comparisons n elements with p processors (via deterministic or randomized algorithms). We show that for p &#x02265; n this time is &#x0398; (log n/log(1 + p/n)), (it is easy to show that for p &#x02264; n the time is &#x0398; (n log n/p) = &#x0398; (log n/(p/n)). Therefore even the average case behaviour of randomized algorithms is not more efficient than the worst case behaviour of deterministic ones.
[Computer science, Read-write memory, Phase change random access memory, Decision trees, Parallel algorithms, Sorting]
Results on learnability and the Vapnik-Chervonenkis dimension
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The problem of learning a concept from examples in a distribution-free model is considered. The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced. This method is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension. An important variation on the problem of learning from examples, called approximating from examples, is also discussed. The problem of computing the VC dimension of a finite concept set defined on a finite domain is considered.<<ETX>>
[dynamic sampling, distribution-free model, Error analysis, Probability distribution, Time measurement, learnability, Electronic mail, Distributed computing, learning systems, Computer science, finite concept set, Vapnik-Chervonenkis dimension, Sampling methods, finite domain]
On the complexity of kinodynamic planning
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The following problem, is considered: given a robot system find a minimal-time trajectory from a start position and velocity to a goal position and velocity, while avoiding obstacles and respecting dynamic constraints on velocity and acceleration. The simplified case of a point mass under Newtonian mechanics together with velocity and acceleration bounds is considered. The point must be flown from a start to a goal, amid 2-D or 3-D polyhedral obstacles. While exact solutions to this problem are not known, the first provably good approximation algorithm is given and shown to run in polynomial time.
[kinodynamic planning, Equations, Motion planning, Computer science, Robot motion, dynamic constraints, Upper bound, Kinematics, approximation algorithm, minimal-time trajectory, Approximation algorithms, Polynomials, position control, robots, Acceleration, robot system]
Universal packet routing algorithms
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The packet-routing problem is examined in a network-independent context. The goal is to devise a strategy for routing that works well for a wide variety of networks. To achieve this goal, the routing problem is partitioned into two stages: a path-selection stage and a scheduling stage. In the first stage, paths for the packets are found with small maximum distance and small maximum congestion. Once the paths are fixed, both are lower bounds on the time required to deliver the packets. In the second stage, a schedule is found for the movement of each packet along its path so that no two packets traverse the same edge at the same time and the total time and maximum queue size required to route all of the packets to their destinations are minimized. The second stage is more challenging and is the focus of this study.<<ETX>>
[Atomic measurements, queue size, Laboratories, packet-routing problem, computer networks, packet switching, Routing, path-selection stage, Time measurement, Mathematics, Scheduling algorithm, Computer science, Concurrent computing, scheduling, network-independent context, Large-scale systems, Contracts]
New upper bounds in Klee's measure problem
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
New upper bounds are given for the measure problem of V. Klee (1977) that significantly improve the previous bounds for dimensions greater than 2. An O(n/sup d/2/ log n, n) time-space upper bound to compute the measure of a set of n boxes in Euclidean d-space is obtained. The solution requires several novel ideas including application of the inclusion/exclusion principle, the concept of trellises, streaming, and a partition of d-space.<<ETX>>
[Computational modeling, graph theory, Euclidean d-space, computational geometry, upper bounds, Time measurement, inclusion/exclusion principle, Helium, Computer science, Upper bound, streaming, trellises, partition, time-space upper bound, Klee measure problem, Books, Testing, computational complexity, dimensions]
The complexity of tree automata and logics of programs
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The computational complexity of testing nonemptiness of finite-state automata on infinite trees is investigated. It is shown that for tree automata with m states and n pairs nonemptiness can be tested in time O((mn)/sup 3n/), even though the problem is in general NP-complete. The nonemptiness algorithm is used to obtain exponentially improved, essentially tight upper bounds for numerous important modal logics of programs, interpreted with the usual semantics over structures generated by binary relations. For example, it is shown that satisfiability for the full branching time logic CTL* can be tested in deterministic double exponential time. It also follows that satisfiability for propositional dynamic logic with a repetition construct (PDL-delta) and for the propositional mu-calculus (L mu ) can be tested in deterministic single exponential time.<<ETX>>
[Algorithm design and analysis, nonemptiness algorithm, programming theory, finite automata, infinite trees, propositional dynamic logic, PDL-delta, finite-state automata, Mathematics, Calculus, logics of programs, branching time logic, Logic testing, CTL*, Upper bound, Automatic testing, satisfiability, Automata, Transformers, Polynomials, tree automata, Contracts, computational complexity]
Coordinated traversal: (t+1)-round Byzantine agreement in polynomial time
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The problem of efficiently performing Byzantine agreement in t+1 rounds in the face of arbitrarily malicious failures is treated. A communication-efficient polynomial-time protocol is presented for n>8t. The protocol is an early stopping protocol, halting in min(t+1, f+2) rounds in the worst case, where f is the number of processors that fail during the run. This is provably optimal. The protocol is based on a careful combination of early stopping, fault masking, and a technique called coordinated traversal. The combination of the three provides a powerful method for restricting the damage that a faulty processor, however malicious, can do. One of the byproducts of this protocol is a polynomial-time (t+1)-round protocol for the Byzantine firing squad problem.<<ETX>>
[Protocols, coordinated traversal, Byzantine agreement, arbitrarily malicious failures, Distributed computing, Computer science, Fault tolerance, protocol, Fault tolerant systems, Broadcasting, fault masking, Polynomials, Delta modulation, polynomial time, Telecommunication network reliability, protocols, Clocks]
An optimal algorithm for intersecting line segments in the plane
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The authors present the first optimal algorithm for the following problem: given n line segments in the plane, compute all k pairwise intersections in O(n log n+k) time. Within the same asymptotic cost the algorithm will also compute the adjacencies of the planar subdivision induced by the segments, which is a useful data structure for contour-filling on raster devices.<<ETX>>
[Algorithm design and analysis, plane, Solid modeling, Costs, pairwise intersections, Independent component analysis, data structure, computational geometry, Data structures, contour-filling, asymptotic cost, Computer science, intersecting line segments, Image segmentation, raster devices, Processor scheduling, Computer graphics, adjacencies, planar subdivision, Robots, optimal algorithm, computational complexity]
A deterministic view of random sampling and its use in geometry
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A number of efficient probabilistic algorithms based on the combination of divide-and-conquer and random sampling have been recently discovered. It is shown that all those algorithms can be derandomized with only polynomial overhead. In the process. results of independent interest concerning the covering of hypergraphs are established, and various probabilistic bounds in geometry complexity are improved. For example, given n hyperplanes in d-space and any large enough integer r, it is shown how to compute, in polynomial time, a simplicial packing of size O(r/sup d/) that covers d-space, each of whose simplices intersects O(n/r) hyperplanes. It is also shown how to locate a point among n hyperplanes in d-space in O(log n) query time, using O(n/sup d/) storage and polynomial preprocessing.<<ETX>>
[polynomial overhead, deterministic view, graph theory, divide-and-conquer, computational geometry, hyperplanes, random sampling, simplicial packing, storage, efficient probabilistic algorithms, Nearest neighbor searches, Computer science, Computational geometry, hypergraphs, polynomial preprocessing, geometry complexity, Sampling methods, derandomized, probabilistic bounds, Polynomials, polynomial time, computational complexity]
On the complexity of omega -automata
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Automata on infinite words were introduced by J.R. Buchi (1962) in order to give a decision procedure for S1S, the monadic second-order theory of one successor. D.E. Muller (1963) suggested deterministic omega -automata as a means of describing the behavior of nonstabilising circuits. R. McNaughton (1966) proved that classes of languages accepted by nondeterministic Buchi automata and by deterministic Muller automata are the same. His construction and its proof are quite complicated, and the blow-up of the construction is double exponential. The author presents a determinisation construction that is simpler and yields a single exponent upper bound for the general case. This construction is essentially optimal. It can also be used to obtain an improved complementation construction for Buchi automata that is also optimal. Both constructions can be used to improve the complexity of decision procedures that use automata-theoretic techniques.<<ETX>>
[complexity, Costs, Circuits, Laser sintering, H infinity control, Mathematics, Calculus, Upper bound, deterministic automata, Automata, omega -automata, decision procedures, Buchi automata, Logic]
On the effects of feedback in dynamic network protocols
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A framework is introduced that provides a unified way for proving correctness as well as analyzing performance of a class of communication protocols called (asynchronous) reset protocols. They are logarithmic transformers, converting protocols working in a static asynchronous network into protocols working in a dynamic asynchronous network. The design of reset protocols is a classical problem in communication networking, renowned for its complexity. A paradigm is developed that gives fresh insight into this complicated problem. This additional insight leads to the development of reset protocols with complexities bounded by the communication complexity of the original protocol.<<ETX>>
[Algorithm design and analysis, Protocols, Laboratories, communication protocols, performance evaluation, Mathematics, communication complexity, logarithmic transformers, Computer science, feedback, Intelligent networks, Feedback, dynamic network protocols, asynchronous network, Transformers, proving correctness, Performance analysis, protocols, Contracts, performance analysis]
On pointers versus addresses
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The problem of determining the cost of random-access memory (RAM) is addressed by studying the simulation of random addressing by a machine which lacks it, called a pointer machine. The model allows the use of a data type of choice. A RAM program of time t and space s can be simulated in O(t log s) time using a tree. However, this is not an obvious lower bound since a high-level data type can allow the data to be encoded in a more economical way. The major contribution is the formalization of incompressibility for general data types. The definition extends a similar property of strings that underlies the theory of Kolmogorov complexity. The main theorem states that for all incompressible data types an Omega (t log s) lower bound holds. Incompressibility is proved for the real numbers with a set of primitives which includes all functions which are continuously differentiable except on a countable closed set.<<ETX>>
[Algorithm design and analysis, Costs, addresses, pointer machine, functions, graph theory, Random access memory, space, RAM program, strings, Kolmogorov complexity, data structures, pointers, Computational modeling, Power generation economics, random addressing, Read-write memory, tree, Time measurement, lower bound, continuously differentiable, Programming profession, data type, countable closed set, Upper bound, incompressibility, random-access memory, file organisation, real numbers, set of primitives, time, Arithmetic, computational complexity]
Nonexpressibility of fairness and signaling
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Expressiveness results for indeterminate data flow primitives are established. Choice primitives with three differing fairness assumptions are considered, and it is shown that they are strictly inequivalent in expressive power. It is also shown that the ability to announce choices enhances the expressive power of two of the primitives. These results are proved using a very crude semantics and will thus apply in any reasonable theory of process equivalence.<<ETX>>
[fairness, programming theory, inequivalent, Law, signaling, Taxonomy, indeterminate, semantics, expressive power, Computer science, data flow primitives, Computer networks, Carbon capture and storage, Legal factors, theory of process equivalence]
Notes on searching in multidimensional monotone arrays
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A two-dimensional array A=(a/sub i,j/) is called monotone if the maximum entry in its ith row lies below or to the right of the maximum entry in its (i- 1)-st row. An array A is called totally monotone if every 2*2 subarray (i.e., every 2*2 minor) is monotone. The notion of two-dimensional totally monotone arrays is generalized to multidimensional arrays, and a wide variety of problems are exhibited involving computational geometry, dynamic programming, VLSI river routing, and finding certain kinds of shortest paths that can be solved efficiently by finding maxima in totally monotone arrays.<<ETX>>
[graph theory, searching, two-dimensional totally monotone arrays, Very large scale integration, computational geometry, totally monotone, VLSI river routing, (i- 1)-st row, 2*2 minor, Springs, Contracts, search problems, Multidimensional systems, two-dimensional array, dynamic programming, Routing, Rivers, maximum entry, Computer science, Computational geometry, shortest paths, Euclidean distance, multidimensional monotone arrays, 2*2 subarray, ith row, Clocks, computational complexity]
Increasing the size of a network by a constant factor can increase performance by more than a constant factor
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
In one routing scheme which has been implemented on a parallel architecture based on the butterfly graph, messages are sometimes destroyed. It is shown that if messages are sent to random destinations, the expected number of messages that reach their destinations is Theta (n(log n)-1/q), where n is the size of the butterfly graph and q is the number of messages that can move through one edge (or, equivalently, vertex) in one time step. In the analysis of this problem, interesting techniques for solving nonlinear systems of difference equations are developed that could have applications to other problems.<<ETX>>
[parallel architectures, graph theory, Parallel machines, Routing, Mathematics, parallel architecture, Parallel architectures, Concurrent computing, random destinations, routing scheme, Difference equations, butterfly graph, Nonlinear systems, nonlinear systems of difference equations, Contracts]
Lower bounds for integer greatest common divisor computations
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
An Omega (log log n) lower bound is proved on the depth of any computation tree with operations (+, -, /, mod, <or=) that computes the greatest common divisor (GCD) of all pairs of n-bit integers. A novel technique for handling the truncation operation is implicit in the proof. Also proved is a Theta (n) bound on the depth of any algebraic computation trees with operations (+, -, *, /, <or=) (where "/" stands for exact division) that solve many simple problems, e.g. testing if an n-bit integer is odd or computing the GCD of two n-bit integers.<<ETX>>
[Algorithm design and analysis, Computational modeling, Large Hadron Collider, computation tree, algebraic computation trees, trees (mathematics), lower bound, truncation operation, Radio access networks, integer greatest common divisor computations, computation theory, Contracts, Testing, Arithmetic]
Sublinear-time parallel algorithms for matching and related problems
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The authors present the first sub-linear-time deterministic parallel algorithms for bipartite matching and several related problems, including maximal node-disjoint paths, depth-first search, and flows in zero-one networks. The results are based on a better understanding of the combinatorial structure of the above problems, which lead to new algorithmic techniques. In particular, it is shown how to use maximal matching to extend, in parallel, a current set of node-disjoint paths and how to take advantage of the parallelism that arises when a large number of nodes are active during an execution of a push/relabel network flow algorithm. It is also shown how to apply the techniques to design parallel algorithms for the weighted versions of the above problems.<<ETX>>
[Algorithm design and analysis, sublinear-time parallel algorithms, parallel algorithms, zero-one networks, Costs, Laboratories, combinatorial structure, Parallel algorithms, matching, Computer science, Concurrent computing, depth-first search, node-disjoint paths, Parallel processing, maximal node-disjoint paths, Polynomials, Computer networks, Contracts]
New algorithms for finding irreducible polynomials over finite fields
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
An algorithm is presented for finding an irreducible polynomial of specified degree over a finite field. It is deterministic and runs in polynomial time for fields of small characteristics. A proof is given of the stronger result, that the problem of finding irreducible polynomials of specified degree over a finite field K is deterministic-polynomial-time reducible to the problem of factoring polynomials over the prime field of K.<<ETX>>
[Codes, Terminology, polynomials, deterministic, Encoding, irreducible polynomials, Galois fields, Radio access networks, finite fields, specified degree, Polynomials, polynomial time, Cryptography, Arithmetic]
Combinatorial complexity bounds for arrangements of curves and surfaces
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The authors study both the incidence counting and the many-faces problem for various kinds of curves, including lines, pseudolines, unit circles, general circles, and pseudocircles. They also extend the analysis to three dimensions, where they concentrate on the case of spheres, which is relevant for the three-dimensional unit-distance problem. They obtain upper bounds for certain quantities. The authors believe that the techniques they use are of independent interest.<<ETX>>
[unit circles, three dimensions, Shape, combinatorial mathematics, upper bounds, combinatorial complexity bounds, three-dimensional unit-distance problem, general circles, Research and development, Computer science, Geometry, pseudolines, pseudocircles, Upper bound, curves, spheres, Councils, surfaces, lines, computational complexity, incidence counting, many-faces problem]
The complexity of the pigeonhole principle
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The pigeonhole principle for n is the statement that there is no one-to-one function between a set of size n and a set of size n-1. This statement can be formulated as an unlimited-fan-in constant depth polynomial-size Boolean formula PHP/sub n/ in n(n-1) variables, PHP/sub n/ can be proved in the propositional calculus; that is, a sequence of Boolean formulas can be given so that each one is either an axiom of the propositional calculus or a consequence of some of the previous ones according to an inference rule of the propositional calculus, the last one being PHP/sub n/. The main result is that the pigeonhole principle cannot be proved in this way if the size of the proof (the total number or symbols of the formulas in the sequence) is polynomial in n and each formula is constant-depth (unlimited-fan-in), polynomial size and contains only the variables of PHP/sub n/.<<ETX>>
[complexity, PHP/sub n/, Calculus, Boolean algebra, formal logic, propositional calculus, inference rule, Boolean formula, Chromium, Polynomials, pigeonhole principle, Arithmetic, computational complexity]
Dynamic perfect hashing: upper and lower bounds
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A randomized algorithm is given for the dictionary problem with O(1) worst-case time for lookup and O(1) amortized expected time for insertion and deletion. An Omega (log n) lower bound is proved for the amortized worst-case time complexity of any deterministic algorithm in a class of algorithms encompassing realistic hashing-based schemes. If the worst-case lookup time is restricted to k, then the lower bound for insertion becomes Omega (kn/sup 1/k/).<<ETX>>
[Algorithm design and analysis, lookup, Dictionaries, Costs, amortized expected time, Computational modeling, amortized worst-case time complexity, upper bounds, Data structures, worst-case time, dynamic perfect hashing, randomized algorithm, deterministic algorithm, lower bounds, dictionary problem, deletion, file organisation, insertion, Contracts, computational complexity]
The influence of variables on Boolean functions
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Methods from harmonic analysis are used to prove some general theorems on Boolean functions. These connections with harmonic analysis viewed by the authors are very promising; besides the results on Boolean functions they enable them to prove theorems on the rapid mixing of the random walk on the cube and in the extremal theory of finite sets.<<ETX>>
[Computer science, extremal theory, random walk, Boolean functions, finite sets, rapid mixing, Mathematics, influence of variables, Application software, Gas insulated transmission lines, harmonic analysis]
Computing with polynomials given by black boxes for their evaluations: greatest common divisors, factorization, separation of numerators and denominators
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Algorithms are developed that adopt a novel implicit representation for multivariate polynomials and rational functions with rational coefficients, that of black boxes for their evaluation. It is shown that within this evaluation-box representation, the polynomial greatest common divisor and factorization problems as well as the problem of extracting the numerator and denominator of a rational function can be solved in random polynomial time in the usual parameters. Since the resulting evaluation programs for the goal polynomials can be converted efficiently to sparse format, solutions to sparse problems such as the sparse ration interpolation problem follow as a consequence.<<ETX>>
[implicit representation, Interpolation, black boxes, random polynomial time, rational functions, polynomials, function approximation, Polynomials, multivariate polynomials, factorization, greatest common divisor]
Fully abstract models of the lazy lambda calculus
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Much of what is known about the model theory and proof theory of the lambda -calculus is sensible in nature, i.e. only head normal forms are semantically meaningful. However, most functional languages are lazy, i.e. programs are evaluated in normal order to weak head normal forms. The author develops a theory of lazy or strongly sensible lambda -calculus that corresponds to practice. A general method for constructing fully abstract models for a class of lazy languages is illustrated. A formal system called lambda beta C ( lambda beta -calculus with convergence testing C) is introduced, and its properties are investigated.<<ETX>>
[System testing, formal languages, model theory, functional programming, convergence testing, Educational institutions, Calculus, abstract models, lazy languages, Power system modeling, Equations, Convergence, proof theory, lazy lambda calculus, Prototypes, Virtual reality, Functional programming, Virtual manufacturing, functional languages]
On the existence of pseudorandom generators
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Pseudorandom generators are known to exist, assuming the existence of functions that cannot be efficiently inverted on the distributions induced by applying the function iteratively polynomially many times. This sufficient condition is also necessary, but it is difficult to check whether particular functions, assumed to be one-way, are also one-way on their iterates. This raises the fundamental question of whether the mere existence of one-way functions suffices for the construction of pseudorandom generators. Progress toward resolving this question is presented. Regular functions in which every image of a k-bit string has the same number of preimages of length k are considered. It is shown that if a regular function is one-way, then pseudorandom generators do exist. In particular, assuming the intractability of general factoring, it can be proved that the pseudorandom generators do exist. Another application is the construction of a pseudorandom generator based on the assumed intractability of decoding random linear codes.<<ETX>>
[Algorithm design and analysis, k-bit string, linear codes, regular functions, pseudorandom generators existence, Probability distribution, Decoding, random number generation, decoding, Computer science, Sufficient conditions, Linear code, Councils, Polynomials, Cryptography, one-way functions, Distributed algorithms]
Learning via queries
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The power of various query languages is compared along two dimensions, namely the inherent power of the language and the number of alternations of quantizers. Learning by asking questions is compared to learning by passively reading data. It is found that the extent of what can be learned by queries is largely dependent on the language used by the inference mechanism to formulate questions to ask of its trainer. It is proved that inference machines that are allowed to ask first-order questions with plus and times can be used to solve the halting problem and therefore can learn all the recursive functions. Learning languages are also considered.<<ETX>>
[Humans, Educational institutions, query languages, recursive functions, inference mechanisms, inherent power, Database languages, learning systems, inference machines, Computer science, quantizers, Inference mechanisms, Machine learning, Logic, Artificial intelligence, inference mechanism, Formal verification, Arithmetic]
Learning probabilistic prediction functions
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The question of how to learn rules, when those rules make probabilistic statements about the future, is considered. Issues are discussed that arise when attempting to determine what a good prediction function is, when those prediction functions make probabilistic assumptions. Learning has at least two purposes: to enable the learner to make predictions in the future and to satisfy intellectual curiosity as to the underlying cause of a process. Two results related to these distinct goals are given. In both cases, the inputs are a countable collection of functions which make probabilistic statements about a sequence of events. One of the results shows how to find one of the functions, which generated the sequence, the other result allows to do as well in terms of predicting events as the best of the collection. In both cases the results are obtained by evaluating a function based on a tradeoff between its simplicity and the accuracy of its predictions.<<ETX>>
[Computer science, Gold, Humans, Gaussian distribution, Time measurement, Concrete, learning, probabilistic statements, probabilistic prediction functions, Physics, learning systems]
Speeding up dynamic programming
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A number of important computational problems in molecular biology, geology, speech recognition, and other areas can be expressed as recurrences which have typically been solved with dynamic programming. By using more sophisticated data structures, and by taking advantage of further structure from the applications, the authors speed up the computation of several of these recurrences by one or two orders of magnitude. The algorithms used are simple and practical.<<ETX>>
[molecular biology, Educational programs, algorithms, Geology, Binary search trees, dynamic programming, Data structures, geology, computational problems, Computer science, speech recognition, Speech recognition, Biology computing, data structures, Dynamic programming, Computer science education, Computational biology, computational complexity]
Dynamic networks are as fast as static networks
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
An efficient simulation is given to show that dynamic networks are as fast as static ones up to a constant multiplicative factor. That is, any task can be performed in a dynamic asynchronous network essentially as fast as in a static synchronous network. The simulation protocol is based on an approach in which locality is perceived as the key to fast adaptation to changes in network topology. The heart of the simulation is a technique called a dynamic synchronizer, which achieves 'local' simulation of a global 'clock' in a dynamic asynchronous network. Using this result, improved solutions to a number of well-known problems on dynamic networks are obtained. It can also be used to improve the solution to certain static network problems.<<ETX>>
[Heart, Protocols, dynamic synchronizer, Computational modeling, Computer simulation, Laboratories, computer networks, simulation, constant multiplicative factor, Mathematics, network topology, Computer science, protocol, Network topology, dynamic networks, static networks, asynchronous network, Communication networks, protocols, Contracts]
Achieving oblivious transfer using weakened security assumptions
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The authors present some general techniques for establishing the cryptographic strength of a wide variety of games. As case studies, they analyze some weakened versions of the standard forms of oblivious transfer. They also consider variants of oblivious transfer that are motivated by coding theory and physics. Among their results, they show that a noisy telephone line is in fact a very sophisticated cryptographic device. They also present an application to quantum cryptography.<<ETX>>
[Codes, Circuits, cryptography, cryptographic strength, Mathematics, weakened security, Physics, Cryptographic protocols, Computer science, coding theory, quantum cryptography, physics, Telephony, games, oblivious transfer, noisy telephone line, Cryptography, Computer security, Marine vehicles]
Removing randomness in parallel computation without a processor penalty
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Some general techniques are developed for removing randomness from randomized NC algorithms without a blowup in the number of processors. One of the requirements for the application of these techniques is that the analysis of the randomized algorithm uses only pairwise independence. The main new result is a parallel algorithm for the Delta +1 vertex coloring problem with running time O(log/sup 3/ nlog log n) using a linear number of processors on a concurrent-read-concurrent-write parallel random-access machine. The techniques also apply to several other problems, including the maximal-independent-set problem and the maximal-matching problem. The application of the general technique to these last two problems is mostly of academic interest, because NC algorithms using a linear number of processors that have better running times have been previously found.<<ETX>>
[Algorithm design and analysis, concurrent-read-concurrent-write parallel random-access machine, parallel algorithms, parallel computation, maximal-independent-set problem, random-access storage, Scholarships, parallel algorithm, Phase change random access memory, Application software, Parallel algorithms, randomness removing, graph colouring, vertex coloring, Concurrent computing, Computer science, randomized NC algorithms, Random variables, pairwise independence, maximal-matching problem]
Take a walk, grow a tree
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A simple randomized algorithm is presented for maintaining dynamically evolving binary trees on hypercube networks. The algorithm guarantees that: (1) nodes adjacent in the tree are within distance O(log log N) in an N-processor hypercube, and (2) with overwhelming probability, no hypercube processor is assigned more than O(1+M/N) tree nodes, where M is the number of nodes in the tree. The algorithm is distributed and does not require any global information. This is the first load-balancing algorithm with provably good performance. The algorithm can be used to parallelize efficiently any tree-based computation. It can also be used to maintain efficiently dynamic data structures such as quadtrees. A technique called tree surgery is introduced to deal with dependencies inherent in trees. Together with tree surgery, the study of random walks is used to analyze the algorithm.<<ETX>>
[Algorithm design and analysis, Image processing, multiprocessor interconnection networks, hypercube networks, dynamic data structures, Concurrent computing, dynamically evolving binary trees, Runtime, Computed tomography, Surgery, Hypercubes, Tree data structures, random walks, trees (mathematics), tree nodes, N-processor hypercube, randomized algorithm, dependencies, Computer science, tree surgery, Binary trees, load-balancing algorithm, quadtrees, tree-based computation, computational complexity]
Fast management of permutation groups
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Novel algorithms for computation in permutation groups are presented. They provide an order-of-magnitude improvement in the worst-case analysis of the basic permutation-group problems, including membership testing and computing the order of the group. For deeper questions about the group, including finding composition factors, an improvement of up to four orders of magnitude is realized. These and other essential investigations are all accomplished in O(n/sup 4/log/sup c/n) time. The approach is distinguished by its recognition and use of the intrinsic structure of the group at hand.<<ETX>>
[Poles and towers, Genetic mutations, probability, composition factors, Estimation theory, Orbits, Machinery, computation, group theory, permutation groups, Libraries, Polynomials, Timing, membership testing, Testing]
Fully dynamic techniques for point location and transitive closure in planar structures
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
It is shown that a planar st-graph G admits two total orders on the set V union E union F, where V, E, and F are, respectively, the sets of vertices, edges and faces of G, with mod V mod =n. An O(n) space data structure for the maintenance of the two orders is exhibited that supports an update of G (insertion of an edge and expansion of a vertex, and their inverses) in time O(log n). This data structure also supports transitive-closure queries in O(log n). Moreover, planar st-graphs provide the topological underpinning of a fully dynamic planar point location technique in monotone subdivisions, which is an interesting (unique) specialization of the chain method of Lee-Preparata (1977). While maintaining storage O(n) and query time O(log/sup 2/ n), insertion/deletion of a chain with k edges can be done in time O(log/sup 2/ n+k), and insertion/deletion of a vertex on an edge can be done in time O(log n).<<ETX>>
[insertion/deletion, total orders, set, graph theory, vertices, data structure, Lee-Preparata, edges, Data structures, update, planar structures, monotone subdivisions, chain method, Computer science, planar st-graph, Computational geometry, fully dynamic techniques, data structures, point location, faces, transitive closure, transitive-closure queries, Testing, computational complexity]
Verifying temporal properties of finite-state probabilistic programs
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The complexity of testing whether a finite-state (sequential or concurrent) probabilistic program satisfies its specification expressed in linear temporal logic. For sequential programs an exponential-time algorithm is given and it is shown that the problem is in PSPACE; this improves the previous upper bound by two exponentials and matches the known lower bound. For concurrent programs is is shown that the problem is complete in double exponential time, improving the previous upper and lower bounds by one exponential each. These questions are also addressed for specifications described by omega -automata or formulas in extended temporal logic.<<ETX>>
[Protocols, programming theory, automata theory, finite-state probabilistic programs, linear temporal logic, Probabilistic logic, Formal specifications, sequential programs, PSPACE, concurrent programs, Logic testing, Distributed computing, Sequential analysis, Upper bound, Automata, temporal properties, omega -automata, Polynomials]
Covering polygons is hard
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
It is shown that the following minimum cover problems are NP-hard, even for polygons without holes: (1) covering an arbitrary polygon with convex polygons; (2) covering the boundary of an arbitrary polygon with convex polygons; (3) covering an orthogonal polygon with rectangles; and (4) covering the boundary of an orthogonal polygon with rectangles. It is noted that these results hold even if the polygons are required to be in general position.<<ETX>>
[boundary, Spirals, NP-hard, rectangles, orthogonal polygon, arbitrary polygon, computational geometry, Partitioning algorithms, minimum cover problems, convex polygons, Polynomials, Structural beams, covering problems, computational complexity]
On a theory of computation over the real numbers; NP completeness, recursive functions and universal machines
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A model for computation over an arbitrary (ordered) ring R is presented. In this general setting, universal machines, partial recursive functions, and NP-complete problems are obtained. While the theory reflects of classical over Z (e.g. the computable functions are the recursive functions), it also reflects the special mathematical character of the underlying ring R (e.g. complements of Julia sets provide natural examples of recursively enumerable undecidable sets over the reals) and provides a natural setting for studying foundational issues concerning algorithms in numerical analysis.<<ETX>>
[Costs, Computational modeling, Educational institutions, Mathematics, recursive functions, NP completeness, foundational issues, Milling machines, Computer science, Geometry, theory of computation, universal machines, Turing machines, Numerical analysis, numerical analysis, undecidable sets, computation theory, real numbers, partial recursive functions, Mathematical model]
Three stacks
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The storage allocation for three stacks has been traditionally accomplished by using pointers to store the stacks as linked lists or by relocating the stacks within memory when collisions take place. The former approach requires additional space to store the pointers, and the latter approach requires additional time. The authors explore the extent to which some additional space or time is required to maintain three stacks. They provide a formal setting for this topic and establish upper and lower complexity bounds on various aspects.<<ETX>>
[Availability, storage allocation, relocating, Dictionaries, programming theory, linked lists, memory, Computational modeling, collisions, additional time, Data structures, three stacks, lower complexity bounds, upper complexity bounds, additional space, pointers, Software engineering, computational complexity]
A lower bound for matrix multiplication
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
It is proved that computing the product of two n*n matrices over the binary field requires at least 2.5n/sup 2/-O(n/sup 2/) multiplications.<<ETX>>
[Computer science, Linear code, matrix multiplication, Shape, binary field, Vectors, lower bound, Galois fields, computational complexity]
Optimal parallel algorithm for the Hamiltonian cycle problem on dense graphs
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
G.A. Dirac's classical theorem (1952) asserts that if every vertex of a graph G on n vertices has degree at least n/2, the G has a Hamiltonian cycle. A fast parallel algorithm on a concurrent-read-exclusive-write parallel random-access machine (CREW PRAM) is given to find a Hamiltonian cycle in such graphs. The algorithm uses a linear number of processors and is optimal up to a polylogarithmic factor. It works in O(log/sup 4/n) parallel time and uses linear number of processors on a CREW PRAM. It is also proved that a perfect matching in dense graphs can be found in NC/sup 2/. The cost of improved time is a quadratic number of processors. It is also proved that finding an NC algorithm for perfect matching in slightly less dense graphs is as hard as the same problem for all graphs, and the problem of finding a Hamiltonian cycle becomes NP-complete.<<ETX>>
[parallel algorithms, Costs, random-access storage, vertex, polylogarithmic factor, graph theory, optimal parallel algorithm, dense graphs, Phase change random access memory, NP-complete, Parallel algorithms, graph, concurrent-read-exclusive-write parallel random-access machine, perfect matching, Hamiltonian cycle problem]
A faster PSPACE algorithm for deciding the existential theory of the reals
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The decision problem for the existential theory of the reals is the problem of deciding if the set (x in R/sup n/; P(x) is nonempty, where P(x) is a predicate which is a Boolean function of atomic predicates either of which is a Boolean function of atomic predicates either of the form f/sub i/(x)>or=0 or f/sub j/(x)>, the f's being real polynomials. An algorithm is presented for deciding the existential theory of the reals that simultaneously achieves the best known time and space bounds. The time bound for the algorithm is slightly better than any previous bound.<<ETX>>
[decision problem, Operations research, set, Computational modeling, polynomials, H infinity control, Industrial engineering, PSPACE algorithm, Boolean function, set theory, existential theory, atomic predicates, Boolean functions, Turing machines, decidability, Polynomials, real polynomials]
Reachability is harder for directed than for undirected finite graphs
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
It is shown that for directed graphs, reachability can not be expressed by an existential monadic second-order sentence. The proof makes use of Ehrenfeucht-Fraisse games, along with probabilistic. However, it is shown that for directed graphs with degree at most k, reachability is expressible by an existential monadic second-order sentence. One reason for the interest in the main result is that while there is considerable empirical evidence (in terms of the efficiency of algorithms that have been discovered) that reachability in directed graphs is 'harder' than reachability in undirected graphs, this is the first proof in a precise technical sense that this is so.<<ETX>>
[undirected finite graphs, efficiency of algorithms, directed graphs, reachability, NP-complete problem, Ehrenfeucht-Fraisse games, Artificial intelligence, computational complexity]
Lattices, mobius functions and communications complexity
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A general framework for the study of a broad class of communication problems is developed. It is based on a recent analysis of the communication complexity of graph connectivity. The approach makes use of combinatorial lattice theory.<<ETX>>
[Geometry, graph connectivity, Protocols, graph theory, Lattices, communications complexity, Polynomials, Complexity theory, lattices, combinatorial lattice theory, computational complexity, mobius functions]
Hardness vs. randomness
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A simple construction for a pseudorandom bit generator is presented. It stretches a short string of truly random bits into a long string that looks random to any algorithm from a complexity class C (e.g. P, NC, PSPACE, etc.), using an arbitrary function that is hard for C. This generator reveals an equivalence between the problems of proving lower bounds and the problem of generating good pseudorandom sequences. Combining this construction with other arguments, a number of consequences are obtained.<<ETX>>
[hardness, arbitrary function, randomness, random number generation, pseudorandom bit generator, complexity class C, lower bounds, Random sequences, computational complexity]
Zero-knowledge with log-space verifiers
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Interactive proof systems are considered in which the best set of possible verifiers is restricted to the class of probabilistic log-space automata. A. Condon (1988) introduced this model and showed that if the protocols are allowed to run for arbitrarily many rounds, exponential-time languages can be proved to a log-space verifier. To better approximate the usual notion of interactive proof systems, a number of researchers have considered a more realistic, further restricted model in which protocols are polynomially bounded, both in the number of rounds of communication and in the number of computational steps allowed to the verifier. A notion of language-recognition zero-knowledge is defined for this model, and it is shown that anything provable in this model can be proved in language-recognition zero-knowledge.<<ETX>>
[Protocols, automata theory, log-space verifiers, exponential-time languages, interactive proof systems, Mathematics, probabilistic log-space automata, language-recognition, Automata, Sampling methods, Polynomials, theorem proving, Cryptography, protocols, zero knowledge]
Effect of connectivity in associative memory models
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The authors investigate how good connectivity properties translate into good error-correcting behavior in sparse networks of threshold elements. They determine how the eigenvalues of the interconnection graph (which in turn reflect connectivity properties) relate to the quantities, number of items stored, amount of error-correction, radius of attraction, and rate of convergence in an associative memory model consisting of a sparse network of threshold elements or neurons.<<ETX>>
[associative memory models, sparse networks, eigenvalues, Biological system modeling, Neurons, Humans, rate of convergence, Vectors, Power system modeling, neurons, eigenvalues and eigenfunctions, Learning systems, Associative memory, threshold elements, connectivity, Biological systems, interconnection graph, Biology computing, Brain modeling, error-correcting, neural nets, content-addressable storage]
Predicting (0, 1)-functions on randomly drawn points
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The authors consider the problem of predicting (0, 1)-valued functions on R/sup n/ and smaller domains, based on their values on randomly drawn points. Their model is related to L.G. Valiant's learnability model (1984), but does not require the hypotheses used for prediction to be represented in any specified form. The authors first disregard computational complexity and show how to construct prediction strategies that are optimal to within a constant factor for any reasonable class F of target functions. These prediction strategies use the 1-inclusion graph structure from N. Alon et al.'s work on geometric range queries (1987) to minimize the probability of incorrect prediction. They then turn to computationally efficient algorithms. For indicator functions of axis-parallel rectangles and halfspaces in R/sup n/, they demonstrate how their techniques can be applied to construct computational efficient prediction strategies that are optimal to within a constant factor. They compare the general performance of prediction strategies derived by their method to those derived from existing methods in Valiant's learnability theory.<<ETX>>
[1-inclusion graph structure, axis-parallel rectangles, probability, indicator functions, Predictive models, Probability distribution, Computational complexity, learnability model, learning systems, Feedback, randomly drawn points, target functions, prediction strategies, geometric range queries, Approximation algorithms, many-valued logics]
Bounds on the cover time
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A particle that moves on a connected unidirected graph G with n vertices is considered. At each step the particle goes from the current vertex to one of its neighbors, chosen uniformly at random. The cover time is the first time when the particle has visited all the vertices in the graph, starting from a given vertex. Upper and lower bounds are presented that relate the expected cover time for a graph to the eigenvalues of the Markov chain that describes the above random walk. An interesting consequence is that regular expander graphs have expected cover time theta (n log n).<<ETX>>
[Algorithm design and analysis, Protocols, eigenvalues, graph theory, vertices, Stochastic processes, upper bounds, eigenvalues and eigenfunctions, Markov chain, Fault tolerance, random walk, neighbors, particle, Token networks, regular expander graphs, cover time, connected unidirected graph, Computational modeling, Graph theory, State-space methods, lower bounds, Computer science, Markov processes, Joining processes, computational complexity]
Constructive results from graph minors: linkless embeddings
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A formal study of three-dimensional topological graph theory is initiated. The problem of deciding whether a graph can be embedded in 3-space so that no collection of vertex-disjoint cycles is topologically linked is considered first. The Robertson-Seymour Theory of Graph Minors is applicable to this problem and guarantees the existence of an O(n/sup 3/) algorithm for the decision problem. However, not even a finite-time decision procedure was known for this problem. A small set of forbidden minors for linkless embeddable graphs is exhibited, and it is shown that any graph with these minors cannot be embedded without linked cycles. It is further established that any graph that does not contain these minors is embeddable without linked cycles. Thus, an O(n/sup 3/) algorithm for the decision problem is demonstrated. It is believed that the proof technique will lead to an algorithm for actually embedding a graph, provided it does not contain the forbidden minors.<<ETX>>
[Robertson-Seymour Theory, graph minors, decision theory, graph theory, topology, Peak to average power ratio, Graph theory, Complexity theory, Computer science, Computational geometry, decision procedure, Polynomials, Robustness, embedding, topological, linkless embeddable graphs]
Parallel comparison algorithms for approximation problems
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The authors consider that they have n elements from a totally ordered domain and are allowed to perform p parallel comparisons in each time unit (round). They determine, up to a constant factor, the time complexity of several approximation problems in the common parallel comparison tree model of L.G. Valiant, for all admissible values of n, p, and epsilon , where epsilon is an accuracy parameter determining the quality of the required approximation. The problems considered include the approximate maximum problem, approximate sorting, and approximate merging. The results imply, as special cases, all the known results about the time complexity of parallel sorting, parallel merging, and parallel selection of the maximum (in the comparison model). They highlight one very special but representative result concerning the approximate maximum problem. They wish to find, among the given n elements, one which belongs to the biggest n/2, where in each round they are allowed to ask n binary comparisons. They show that log/sup */n+ Theta (1) rounds are both necessary and sufficient in the best algorithm for this problem.<<ETX>>
[approximation theory, parallel algorithms, approximation problems, Costs, Computational modeling, Merging, trees (mathematics), approximate merging, time complexity, accuracy parameter, totally ordered domain, Sorting, parallel comparison algorithms, common parallel comparison tree model, approximate sorting, sorting, Approximation algorithms]
Near-optimal time-space tradeoff for element distinctness
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
It was conjectured by A. Borodin et al. that to solve the element distinctness problem requires TS= Omega (n/sup 2/) on a comparison-based branching program using space S and time T, which, if true, would be close to optimal since TS=O(n/sup 2/ log n) is achievable. They showed recently (1987) that TS= Omega (n/sup 3/2/(log n)/sup 1/2/). The author shows a near-optimal tradeoff TS= Omega (n/sup 2- epsilon (n)/), where epsilon (n)=O(1/(log n)/sup 1/2/).<<ETX>>
[programming theory, Computational modeling, Circuits, element distinctness, Binary decision diagrams, Very large scale integration, comparison-based branching program, Sorting, Computer science, near optimal time-space tradeoff, Turing machines, sorting, Arithmetic]
Homogeneous measures and polynomial time invariants
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The usual probability distributions are concentrated on strings that do not differ noticeably in any fundamental characteristics, except their informational size (Kolmogorov complexity). The formalization of this statement is given and shown to distinguish a class of homogeneous probability measures suggesting various applications. In particular, it could explain why the average case NP-completeness results are so measure-independent and could lead to their generalization to this wider and more invariant class of measures. It also demonstrates a sharp difference between recently discovered pseudorandom strings and the objects known before.<<ETX>>
[Additives, homogeneous measures, Time measurement, Electronic mail, random number generation, pseudorandom strings, probability distributions, Character generation, Kolmogorov complexity, average case NP-completeness, Particle measurements, Set theory, Polynomials, polynomial time invariants, computational complexity]
Efficient parallel algorithms for chordal graphs
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The author gives efficient parallel algorithms for recognizing chordal graphs, finding a maximum clique and a maximum independent set in a chordal graph, finding an optimal coloring of a chordal graph, finding a breadth-first search tree and a depth-first search tree of a chordal graph, recognizing interval graphs, and testing interval graphs for isomorphism. The key to the results is an efficient parallel algorithm for finding a perfect elimination ordering.<<ETX>>
[parallel algorithms, chordal graphs, optimal coloring, breadth-first search tree, maximum independent set, maximum clique, depth-first search tree, Parallel algorithms, Sun, graph colouring, isomorphism, elimination ordering, Tree graphs, Databases, Polynomials, interval graphs, Contracts, Testing, Intelligent control]
Polytopes, permanents and graphs with large factors
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
Randomized algorithms for approximating the number of perfect matchings in a graph are considered. An algorithm that is a natural simplification of one suggested and analyzed previously is introduced and analyzed. One of the key ideas is to view the analysis from a geometric perspective: it is proved that for any graph G the k-slice of the well-known Edmonds matching polytope has magnification 1. For a bipartite graph G=(U, V, E), mod U mod = mod V mod =n, with d edge-disjoint perfect matchings, it is proved that the ratio of the number of almost perfect matchings to the number of perfect matchings is at most n/sup 3n/d/. For any constant alpha >0 this yields a a fully polynomial randomized algorithm for approximating the number of perfect matchings in bipartite graphs with d>or= alpha n. Moreover, for some constant c>0 it is the fastest known approximation algorithm for bipartite graphs with d>or= clog n.<<ETX>>
[Algorithm design and analysis, Scholarships, perfect matchings, graph theory, History, bipartite graph, graph, Quantum computing, Autobiographies, large factors, Quantum mechanics, Approximation algorithms, Polynomials, Computer networks, Bipartite graph, k-slice, polytope]
An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A multicommodity flow problem is considered where for each pair of vertices (u, v) it is required to send f half-units of commodity (u, v) from u to v and f half-units of commodity (v, u) from v to u without violating capacity constraints. The main result is an algorithm for performing the task provided that the capacity of each cut exceeds the demand across the cut by a Theta (log n) factor. The condition on cuts is required in the worst case, and is trivially within a Theta (log n) factor of optimal for any flow problem. The result can be used to construct the first polylog-times optimal approximation algorithms for a wide variety of problems, including minimum quotient separators, 1/3-2/3 separators, bifurcators, crossing number, and VLSI layout area. It can also be used to route packets efficiently in arbitrary distributed networks.<<ETX>>
[max-flow min-cut theorem, Particle separators, Laboratories, graph theory, Bifurcation, minimum quotient separators, Linear programming, Mathematics, multicommodity flow problems, approximation algorithms, bifurcators, VLSI layout area, Application software, Computer science, crossing number, Approximation algorithms, Constraint theory, 1/3-2/3 separators, distributed networks, optimal approximation, Contracts]
Combinatorial algorithms for the generalized circulation problem
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A generalization of the maximum-flow problem is considered in which the amounts of flow entering and leaving an arc are linearly related. More precisely, if x(e) units of flow enter an arc e, x(e) lambda (e) units arrive at the other end. For instance, nodes of the graph can correspond to different currencies, with the multipliers being the exchange rates. Conservation of flow is required at every node except a given source node. The goal is to maximize the amount of flow excess at the source. This problem is a special case of linear programming, and therefore can be solved in polynomial time. The authors present polynomial-time combinatorial algorithms for this problem. The algorithms are simple and intuitive.<<ETX>>
[Algorithm design and analysis, Costs, combinatorial algorithms, Laboratories, graph theory, maximum-flow problem, Linear programming, Mathematics, linear programming, Security, graph, Computer science, generalized circulation problem, Exchange rates, nodes, Polynomials, Contracts]
Polynomial algorithm for the k-cut problem
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
The k-cut problem is to find a partition of an edge weighted graph into k nonempty components, such that the total edge weight between components is minimum. This problem is NP-complete for arbitrary k and its version involving fixing a vertex in each component is NP hard even for k=3. A polynomial algorithm for the case of a fixed k is presented.<<ETX>>
[edge weighted graph, vertex, graph theory, Very large scale integration, polynomial algorithm, NP-complete, Partitioning algorithms, k-cut problem, partition, total edge weight, NP hard, Clustering algorithms, Tail, Polynomials, k nonempty components, computational complexity]
A Las Vegas algorithm for linear programming when the dimension is small
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
An algorithm for solving linear programming problems is given. The expected number of arithmetic operations required by the algorithm is given. The expectation is with respect to the random choices made by the algorithm, and the bound holds for any given input. The technique can be extended to other convex programming problems.<<ETX>>
[Costs, bound, Linear programming, Las Vegas algorithm, convex programming, linear programming, Linear matrix inequalities, arithmetic operations, Linear approximation, random choices, Chebyshev approximation, Sampling methods, Arithmetic, computational complexity]
Genus g graphs have pagenumber O( square root g)
[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science
None
1988
A book embedding of a graph consists of a linear ordering of the vertices along the spine of a book and an assignment of edges to pages so that edges on the same page do not intersect. The minimum number of pages in which a graph can be embedded is its pagenumber. The following results are presented: (1) any graph of genus g has pagenumber O( square root g); and (2) most n-vertex d-regular graphs have pagenumber Omega ( square root dn/sup 1/2-1/d/).<<ETX>>
[graph theory, vertices, Very large scale integration, linear ordering, edges, Mathematics, spine, Complexity theory, Wire, Application software, pagenumber, book embedding, Computer science, Fault tolerance, n-vertex d-regular graphs, pages, genus g, Hardware, Books, Contracts, computational complexity]
Dynamically computing the maxima of decomposable functions, with applications
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors present a general technique for updating the maximum (minimum) value of a decomposable function as elements are inserted into and deleted from the set S. Applications of this technique include efficient algorithms for dynamically computing the diameter or closest pair of a set of points, minimum separation among a set of rectangles, smallest distance between a set of points and a set of hyperplanes, and largest or smallest area (perimeter) rectangles determined by a set of points. The main appeal of the approach lies in its generality. Several research directions suggested by the work are noted.<<ETX>>
[Costs, Heuristic algorithms, rectangles, Very large scale integration, computational geometry, smallest distance, hyperplanes, Extraterrestrial measurements, Search problems, Data structures, smallest area, minimum separation, closest pair, Computational geometry, optimisation, dynamic algorithms, set of points, maximum value updating algorithm, decomposable functions, Robots]
On the power of 2-way probabilistic finite state automata
30th Annual Symposium on Foundations of Computer Science
None
1989
The recognition power of two-way probabilistic finite-state automata (2PFAs) is studied. It is shown that any 2PFA recognizing a nonregular language must use exponential expected time infinitely often. The power of interactive proof systems (IPSs) where the verifier is a 2PFA is also investigated. It is shown that (1) IPSs in which the verifier uses private randomization are strictly more powerful than IPSs in which the random choices of the verifier are made public to the prover. (2) IPSs in which the verifier uses public randomization are strictly more powerful than 2PFAs alone, that is, without a prover; (3) every language accepted by some deterministic Turing machine in exponential time can be accepted by some IPS. Other results concern IPSs with 2PFA verifiers that run in polynomial expected time.<<ETX>>
[public randomization, formal languages, finite automata, Computational modeling, Circuits, probability, interactive proof systems, Magnetic heads, 2-way probabilistic finite state automata, Complexity theory, recognition power, Radio access networks, nonregular language, 2PFA verifiers, exponential expected time, Turing machines, private randomization, Automata, Automatic control, Polynomials, Decision trees, computational complexity, pattern recognition]
Computing irreducible representations of finite groups
30th Annual Symposium on Foundations of Computer Science
None
1989
The bit complexity of computing irreducible representations of finite groups is considered. Exact computations in algebraic number fields are performed symbolically. A polynomial-time algorithm for finding a complete set of inequivalent irreducible representations over the field of complex numbers of a finite group given by its multiplication table is presented. It follows that some representative of each equivalence class of irreducible representations admits a polynomial-size description. The problem of decomposing a given representation V of the finite group G over an algebraic number field F into absolutely irreducible constituents is considered. It is shown that this can be done in deterministic polynomial time if V is given by the list of matrices (V(g); g in G) and in randomized (Las Vegas) polynomial time under the more concise input (V(g); g in S), where S is a set of generators of G.<<ETX>>
[deterministic polynomial time, absolutely irreducible constituents, equivalence class, bit complexity, Las Vegas polynomial time, generators set, Matrix decomposition, complex numbers, algebraic number fields, multiplication table, group theory, polynomial-size description, Algebra, finite groups, Chromium, matrices list, Polynomials, exact computations, randomized polynomial time, polynomial-time algorithm, inequivalent irreducible representations, computational complexity]
Simplification of nested radicals
30th Annual Symposium on Foundations of Computer Science
None
1989
Radical simplification is a fundamental mathematical question, as well as an important part of symbolic computation systems. The general denesting problem had not been known to be decidable. Necessary and sufficient conditions for a radical alpha over a field k to be denested, as well as the first algorithm to decide whether the expression can be denested, are given. The algorithm computes an equivalent expression of minimum nesting depth. It has running time polynomial in the size of the splitting field of the minimal polynomial of alpha over k.<<ETX>>
[Algorithm design and analysis, decidable, polynomials, minimal polynomial, Size measurement, Mathematics, radical simplification, minimum nesting depth, Equations, Sufficient conditions, decidability, equivalent expression, Tail, symbol manipulation, Polynomials, nested radicals]
Graph products and chromatic numbers
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of computing the chromatic number of a graph is considered. No known approximation algorithm can guarantee a better than O(n/sup 0.4/) coloring on a three-chromatic graph with n vertices. Evidence is provided that it is inherently impossible to achieve a better than n/sup epsilon / ratio in polynomial time by showing that 'breaking the n/sup epsilon / barrier' will automatically lead to vastly better polynomial-time approximation algorithms that achieve ratios closer to log n.<<ETX>>
[vertices, polynomial-time approximation algorithms, Approximation algorithms, chromatic numbers, three-chromatic graph, Polynomials, Bonding, graph colouring, graph, computational complexity]
An efficient parallel algorithm for the minimal elimination ordering (MEO) of an arbitrary graph
30th Annual Symposium on Foundations of Computer Science
None
1989
The first efficient parallel algorithm for computing minimal elimination ordering (MEO) of an arbitrary graph is designed. The algorithm works in O(log/sup 3/n) parallel time and O(nm) processors on a concurrent-read-concurrent-write parallel random-access machine (CRCW PRAM) for an n-vertex, m-edge graph and is optimal up to polylogarithmic factor with respect to the best sequential algorithm of D. Rose et. al. (SIAM J. Comput., vol.5, p.266-83, 1976). As an application, the first efficient parallel solution to the problem of minimal fill-in for arbitrary graphs is given. The method of solution involves the development of new techniques for solving the connected minimal set system problem and combining them with some new divide-and-conquer methods.<<ETX>>
[Algorithm design and analysis, concurrent-read-concurrent-write parallel random-access machine, parallel algorithms, Symmetric matrices, minimal elimination ordering, CRCW PRAM, arbitrary graph, graph theory, parallel algorithm, divide-and-conquer, Phase change random access memory, Parallel algorithms, Computer science, Concurrent computing, Processor scheduling, MEO]
Minimum resource zero knowledge proofs
30th Annual Symposium on Foundations of Computer Science
None
1989
Several resources relating to zero-knowledge protocols are considered. They are the number of envelopes used in the protocol, the number of oblivious transfer protocols executed during the protocol, and the total amount of communication required by the protocol. It is shown that after a preprocessing stage consisting of O(k) executions of oblivious transfer, any polynomial number of NP-theorems of any polysize can be proved noninteractively and in zero knowledge, on the basis of the existence of any one-way function, so that the probability of accepting a false theorem is less than 1/2/sup k/.<<ETX>>
[zero knowledge proofs, Protocols, Costs, NP-theorems, Security, zero-knowledge protocols, protocol, Aggregates, Polynomials, Concrete, theorem proving, Cryptography, protocols, oblivious transfer protocols]
Testing permutation polynomials
30th Annual Symposium on Foundations of Computer Science
None
1989
The simple test for determining whether an arbitrary polynomial is a permutation polynomial, by producing its list of values, is considered, and it is found that off-the-shelf techniques from computer algebra improve the running time slightly, without requiring any new insights into the problem. A probabilistic variant of the Hermite test that reduces its running time is given. A criterion for permutation polynomials is then examined, and a probabilistic test whose number of operations is essentially linear in the input size is then given. Exceptional polynomials, which are closely related to permutation polynomials, are also considered, and a random polynomial-time test for these is described.<<ETX>>
[value list, random polynomial-time test, exceptional polynomials, polynomials, Cloning, algebra, Galois fields, probabilistic variant, Computer science, Proportional control, Councils, permutation polynomials, computer algebra, Hermite test, Polynomials, probabilistic test, Australia, Cryptography, Testing, Arithmetic, computational complexity]
How to recycle random bits
30th Annual Symposium on Foundations of Computer Science
None
1989
It is shown that modified versions of the linear congruential generator and the shift register generator are provably good for amplifying the correctness of a probabilistic algorithm. More precisely, if r random bits are needed for a BPP algorithm to be correct with probability at least 2/3, then O(r+k/sup 2/) bits are needed to improve this probability to 1-2/sup -k/. A different pseudorandom generator that is optimal, up to a constant factor, in this regard is also presented. It uses only O(r+k) bits to improve the probability to 1-2/sup -k/. This generator is based on random walks on expanders. The results do not depend on any unproven assumptions. It is shown that the modified versions of the shift register and linear congruential generators can be used to sample from distributions using, in the limit, the information-theoretic lower bound on random bits.<<ETX>>
[Chaos, information-theoretic lower bound, pseudorandom generator, Computational modeling, Shift registers, linear congruential generator, linear congruential generators, random number generation, Distributed computing, Programming profession, random bits, Computer science, shift register generator, BPP algorithm, probabilistic algorithm, Polynomials, Recycling, Cryptography, computational complexity]
Lower bounds for algebraic computation trees with integer inputs
30th Annual Symposium on Foundations of Computer Science
None
1989
A proof is given of a general theorem showing that for certain sets W a certain topological lower bound is valid in the algebraic computation tree model, even if the inputs are restricted to be integers. The theorem can be used to prove tight lower bounds for the integral-constrained form of many basic problems, such as element distinctness, set disjointness, and finding the convex hull. Through further transformations it leads to lower bounds for problems such as the integer max gap and closest pair of a simple polygon. The proof involves a nontrivial extension of the Milnor-Thom techniques for finding upper bounds on the Betti numbers of algebraic varieties.<<ETX>>
[topological lower bound, TV, Computational modeling, algebraic computation trees, trees (mathematics), element distinctness, integral-constrained form, set disjointness, upper bounds, integer inputs, convex hull, integer max gap, lower bounds, algebraic computation tree model, Betti numbers, Computer science, Computational geometry, algebraic varieties, Coordinate measuring machines, Upper bound, Milnor-Thom techniques, Decision trees, computational complexity]
On the computational power of PP and (+)P
30th Annual Symposium on Foundations of Computer Science
None
1989
Two complexity classes, PP and (+)P, are compared with PH (the polynomial-time hierarchy). The main results are as follows: (1) every set in PH is reducible in a certain sense to a set in PP, an (2) every set in PH is reducible to a set in (+)P under randomized polynomial-time reducibility with two-sided bounded error probability. It follows from these results that neither PP nor (+)P is a subset of or equivalent to PH unless PH collapses to a finite level. This is strong evidence that both classes are strictly harder than PH.<<ETX>>
[Out of order, PP, randomized polynomial-time reducibility, Error probability, complexity classes, automata theory, Mathematics, Computational complexity, probabilistic Turing machine, Computer science, polynomial-time hierarchy, Turing machines, two-sided bounded error probability, (+)P, PH, Polynomials, equivalence classes, computational complexity]
A randomized maximum-flow algorithm
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors present a randomized maximum-flow algorithm, called the PLED (prudent linking excess diminishing) algorithm, whose expected running time is O(nm+n/sup 2/(log n)/sup 3/); this is O(nm) for all except relatively sparse networks. The algorithm is always correct, and in the worst case, which occurs with negligible probability, it take O(nm log n) time. The approach taken is to maintain a parameter Delta , which is a measure of the maximum flow excess of a vertex and of the maximum amount of flow sent by a single operation. Initially, Delta is less than or equal to the maximum edge capacity, and Delta =0 at termination. The execution of the PLED algorithm is partitioned into phases so that Delta stays fixed during each phase and decreases between consecutive phases. In order to achieve a bound on the number of phases that is independent of the maximum edge capacity, the algorithm decreases Delta by as large a factor (>or=2) as possible, rather than by a constant factor. The algorithm uses the dynamic trees data structure.<<ETX>>
[Tree data structures, Heuristic algorithms, vertex, graph theory, randomized maximum-flow algorithm, Partitioning algorithms, prudent linking excess diminishing, maximum flow excess, worst case, Fluid flow measurement, parameter, negligible probability, Polynomials, Computer networks, dynamic trees data structure, Bipartite graph, Labeling, Joining processes, Contracts, PLED algorithm, computational complexity]
Using cellular graph embeddings in solving all pairs shortest paths problems
30th Annual Symposium on Foundations of Computer Science
None
1989
An algorithm for generating a succinct encoding of all-pairs shortest path information in an n-vertex directed planar G with O(n) edges is presented. The edges have real-valued costs, but the graph contains no negative cycles. The time complexity is given in terms of a topological embedding measure defined in the paper. The algorithm uses a decomposition of the graph into outerplanar subgraphs satisfying certain separator properties, and a linear-time algorithm is presented to find this decomposition.<<ETX>>
[Costs, Transmission line matrix methods, shortest path information, Particle separators, graph theory, Magnetic resonance, topological embedding, time complexity, Routing, Encoding, Time measurement, Computer science, Shortest path problem, cellular graph embeddings, Contracts, computational complexity, shortest paths problems]
Asymptotically fast algorithms for spherical and related transforms
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of computing the convolution of two functions on the sphere by means of a spherical transform is considered. Such convolutions are applicable to surface recognition and the location of both rotated and translated patterns in an image. The authors give convolution theorems that relate the spherical transform to convolution, sampling theorems that allow the exact computation of the transform for band-limited functions, and algorithms with asymptotically improved running time for the exact computation of the harmonic expansion. The net result is an O(n/sup 1.5/(log n)/sup 2/) algorithm for the exact computation of the convolution of two bandlimited functions sampled at n points in accordance with the sampling theorem. The techniques developed are applicable to computing other transforms, such as the Laguerre, Hermite, and Hankel transforms.<<ETX>>
[Fourier transforms, Image recognition, convolution theorems, Discrete Fourier transforms, transforms, Harmonic analysis, Educational institutions, Mathematics, Pattern recognition, Computer science, spherical transform, Convolution, sampling theorems, Sampling methods, convolution, computational complexity]
Characterizations of the basic feasible functionals of finite type
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors define a simple typed while-programming language that generalizes the sort of simple language used in computability texts to define the familiar numerical computable functions and corresponds roughly to the mu -recursion of R.O. Gandy (1967). This language does not fully capture the notion of higher type computability. The authors define run times for their programs and prove that the feasible functionals of S. Cook and A. Urquhart (1988) are precisely those functionals computable by typed while-programs with run times feasibly length-bounded. The authors introduce the notion of a bounded typed loop program and prove that a finite type functional is feasible if it is computable by a bounded typed loop program.<<ETX>>
[programming theory, simple typed while-programming language, length-bounded, bounded typed loop program, Calculus, Complexity theory, run times, Convergence, Computer science, Turing machines, functional equations, finite type functional, mu -recursion, Polynomials, Functional programming, Cryptography, Decision trees, feasible functionals, Arithmetic]
On the complexity of a game related to the dictionary problem
30th Annual Symposium on Foundations of Computer Science
None
1989
A game on trees, which is related to the dictionary problem is considered. There are two players A and B who take turns. Player A models the user of the dictionary, and player B models its implementation. Player A modifies the tree by adding new leaves, and player B modifies the tree by replacing subtrees. The cost of an insertion is the depth of the new leaf, and the cost of an update is the size of the subtree replaced. The goal of player A is to maximize cost, and the goal of B is to minimize it. It is shown that there is a strategy for player A that forces a cost of Omega (n log log n) for an n-game, that is, a game consisting of n turns of both players, and a strategy for player B that keeps the cost in O(n log log n).<<ETX>>
[Dictionaries, Costs, dictionary problem, trees (mathematics), game theory, update, data structures, insertion, search problems, computational complexity]
Ensemble motion planning in trees
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of transporting a set of objects between the vertices of a tree by a unit capacity vehicle that travels along the edges of the tree is considered. For the case in which any object can be dropped at intermediate vertices along its route and picked up later, the authors present algorithms for finding a minimum cost transportation that run in O(k+qn) time and in O(k+n log n) time, where n is the number of vertices in the tree, k is the number of objects to be moved, and q<or=min(k,n) is the number of nontrivial connected components in a related directed graph. In the case in which each object must be carried directly from its initial vertex to its destination, the problem is NP-hard. The authors present approximation algorithms that run in O(k+n) time with a performance ratio of 3/2 and in O(k+n log beta (n,q)) time with performance ratio of 5/4.<<ETX>>
[Out of order, NP-hard, vertices, Transportation, objects, destination, approximation algorithms, trees, Vehicles, Tree graphs, directed graph, Cost function, Robots, Contracts, nontrivial connected components, trees (mathematics), edges, Path planning, performance ratio, minimum cost transportation, unit capacity vehicle, Computer science, route, initial vertex, Approximation algorithms, ensemble motion planning, computational complexity]
One-way functions are essential for complexity based cryptography
30th Annual Symposium on Foundations of Computer Science
None
1989
It is shown that many of the standard cryptographic tasks are equivalent to the usual definition of a one-way function. In particular, it is shown that for some of the standard cryptographic tasks any secure protocol for the task can be converted into a one-way function in the usual sense, and thus the security of any proposed protocol for these tasks is implicitly based on a function being 'one-way.' Thus, the usual definition of a one-way function is robust; any one-way function with respect to another definition on which a secure cryptographic protocol can be based can be used to construct a one-way function in the usual sense. The authors focus on private-key encryption, identification/authentication, bit commitment, and coin flipping by telephone. However, the proof techniques presented here can be easily adopted to prove analogous results for other cryptographic tasks.<<ETX>>
[cryptography, Mathematics, coin flipping, Cryptographic protocols, Computer science, protocol, identification, Authentication, Computer graphics, Telephony, Polynomials, Robustness, Cryptography, one-way functions, Computer security, complexity based cryptography, bit commitment, computational complexity, authentication]
Multiparty computation with faulty majority
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of performing a multiparty computation when more than half of the processors are cooperating Byzantine faults is addressed. It is shown how to compute any Boolean function of n inputs distributively, preserving the privacy of inputs held by nonfaulty processors and ensuring that faulty processors obtain the function value if and only if the nonfaulty processors do. If the nonfaulty processors do not obtain the correct function value, they detect cheating with high probability. The solution is based on a new type of verifiable secret sharing in which the secret is revealed not all at once but in small increments. This process ensures that all processors discover the secret at roughly the same time. The solution assumes the existence of an oblivious transfer protocol and uses broadcast channels. The processors are not required to have equal computing power.<<ETX>>
[IEEE news, faulty majority, nonfaulty processors, Protocols, Laboratories, faulty processors, Boolean function, Byzantine faults, multiparty computation, Circuit faults, cheating, Distributed computing, Computer science, Privacy, Boolean functions, Broadcasting, computation theory, Polynomials, oblivious transfer protocol]
Polynomial end-to-end communication
30th Annual Symposium on Foundations of Computer Science
None
1989
A dynamic communication network is one in which links may repeatedly fail and recover. In such a network, although it is impossible to establish a path of unfailed links, reliable communication is possible if there is no cut of permanently failed links between a sender and receiver. The authors consider for such a network the basic task of end-to-end communication, that is, delivery in finite time of data items generated online at the sender, to the receiver, in order and without duplication or omission. The best known previous solutions to this problem had exponential complexity. Moreover, it has been conjectured that a polynomial solution is impossible. The authors disprove this conjecture, presenting the first polynomial end-to-end protocol. The protocol uses methods adopted from shared-memory algorithms and introduces novel techniques for fast load balancing in communication networks.<<ETX>>
[Process design, Protocols, end-to-end communication, Mathematics, shared-memory algorithms, exponential complexity, Computer science, ARPANET, dynamic communication network, telecommunication networks, end-to-end protocol, Load management, fast load balancing, Polynomials, reliable communication, Communication networks, Telecommunication network reliability, protocols, Contracts, computational complexity]
The inverse of an automorphism in polynomial time
30th Annual Symposium on Foundations of Computer Science
None
1989
The first known polynomial-time algorithm for computing the inverse of a K-algebra automorphism is presented. The algorithm works over a commutative ring K and is based on a polynomial decomposition algorithm. A polynomial-time algorithm for computing the left composition factor of a multivariate decomposition is also presented. Two related open problems are stated.<<ETX>>
[inverse, Algebra, multivariate decomposition, left composition factor, K-algebra automorphism, polynomial decomposition algorithm, Polynomials, algebra, Modules (abstract algebra), polynomial-time algorithm, commutative ring, computational complexity]
Lower bounds for pseudorandom number generators
30th Annual Symposium on Foundations of Computer Science
None
1989
Computational resources necessary to generate pseudorandom strings are studied. In particular, lower bounds are proved for pseudorandom number generators, whereas previous research concentrated on upper bounds. The idea of separation of machine-based complexity classes on the basis of their ability (or inability) to generate pseudorandom strings is introduced and investigated.<<ETX>>
[Algorithm design and analysis, Predictive models, machine-based complexity, random number generation, lower bounds, Computer science, Sufficient conditions, Upper bound, Polynomials, Cryptography, Random number generation, Testing, computational complexity, pseudorandom number generators]
Processor efficient parallel algorithms for the two disjoint paths problem, and for finding a Kuratowski homeomorph
30th Annual Symposium on Foundations of Computer Science
None
1989
Given a graph G and two pairs of vertices s/sub 1/, t/sub 1/ and s/sub 2/, t/sub 2/, the two disjoint paths problem asks for vertex-disjoint paths connecting s/sub i/ with t/sub i/, i=1, 2. A fast parallel (NC) algorithm is given for this problem, which has applications in certain routing situations. If G is nonplanar, an algorithm that finds a Kuratowski homeomorph in G (i.e. a subgraph homeomorphic to K/sub 3.3/ or K/sub 5/) is given. This complements the known NC planarity algorithms, which give a planar embedding in the positive case; the algorithm provides a certificate of nonplanarity in the negative case. Both algorithms are processor efficient; in each case, the processor-time product is within a polylogarithmic factor of the best known sequential algorithm.<<ETX>>
[sequential algorithm, parallel algorithms, polylogarithmic factor, graph theory, vertex-disjoint paths, planar embedding, Routing, Search problems, Phase change random access memory, NC planarity algorithms, disjoint paths problem, Application software, Parallel algorithms, Sun, Computer science, Geometry, routing, processor-time product, Kuratowski homeomorph, Joining processes, Testing, computational complexity]
Efficient parallel algorithms for testing connectivity and finding disjoint s-t paths in graphs
30th Annual Symposium on Foundations of Computer Science
None
1989
An efficient parallel algorithm for testing whether a graph G is K-vertex connected, for any fixed k, is presented. The algorithm runs in O(log n) time and uses nC(n,m) processors on a concurrent-read, concurrent-write parallel random-access machine (CRCW PRAM), where n and m are the number of vertices and edges of G and C(n,m) is the number of processors required to compute the connected components of G in logarithmic time. An optimal speedup algorithm for computing connected components would induce an optimal speedup algorithm for testing k-vertex connectivity, for any k>4. To develop the algorithm, an efficient parallel algorithm is designed for the following disjoint s-t paths problem: Given a graph G and two specified vertices s and t, find k-vertex disjoint paths between s and t, if they exist. If no such paths exist, find a set of at most k-1 vertices whose removal disconnects s and t. The parallel algorithm for this problem runs in O(log n) time using C(n,m) processors. It is shown how to modify the algorithm to find k-edge disjoint paths, if they exist. This yields an efficient parallel algorithm for testing whether a graph G is k-edge connected, for any fixed k. The algorithm runs in O(log n) time and uses nC (n,n) processors on a CRCW PRAM. Again, an optimal speedup algorithm for computing connected components would induce an optimal speedup algorithm for testing k-edge connectivity.<<ETX>>
[Algorithm design and analysis, parallel algorithms, CRCW PRAM, random-access storage, graph theory, Reliability theory, Phase change random access memory, Graph theory, disjoint s-t paths, Parallel algorithms, testing connectivity, k-vertex connectivity, graphs, optimal speedup algorithm, k-edge disjoint paths, Clustering algorithms, Circuit synthesis, Telecommunication network reliability, Communication networks, Testing, computational complexity]
Efficient simulations of small shared memories on bounded degree networks
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of simulating a parallel random-access machine (PRAM) with n processors and memory size m>or=n on an n-node bounded degree network (BDN) is considered. Since many of the more efficient PRAM algorithms use an amount of shared memory not much larger than the number of processors, the case in which m=o(n/sup 1+ epsilon /) is considered, and a deterministic solution to the simulation problem is presented. For m=n(log n)/sup /O/sup (1)/ the running time of O(log n log log n) is within a factor of O(log log n) of the lower bound imposed by the diameter of the network.<<ETX>>
[Algorithm design and analysis, Out of order, Computational modeling, Computer simulation, bounded degree networks, parallel random-access machine, virtual storage, simulation, Phase change random access memory, small shared memories, PRAM, Parallel algorithms, parallel processing, Computer science, memory architecture, Upper bound, virtual machines, Computer networks, PRAM algorithms, Contracts]
On reversal complexity for alternating Turing machines
30th Annual Symposium on Foundations of Computer Science
None
1989
The reversal complexity of alternating Turing machines (ATM) is investigated. The strict lower bounds on reversals for recognizing nonregular languages by Sigma /sub k/ machines are settled. Some results relating reversal and space complexities are obtained.<<ETX>>
[Out of order, nonregular languages, Computational modeling, Complexity theory, Computational complexity, Computer science, Concurrent computing, space complexities, Turing machines, alternating Turing machines, Polynomials, reversal complexity, ATM, Mathematical model, Logic, computational complexity]
Area-optimal three-layer channel routing
30th Annual Symposium on Foundations of Computer Science
None
1989
The channel routing problem in the knock-knee mode is considered. The algorithm presented always constructs a correct layout in a channel of bounded size, if there is one, and guarantees that it is wirable with only three conducting layers; that is, the layout is optimal with respect to the area and to the number of layers. The algorithm thus improves all previously known layout algorithms, which either use additional columns to produce a three-layer wirable layout or construct a layout for which the three-layer wirability is not proved. For the layer assignment only O(N) (N is the number of nets) vias are used. The algorithm can be implemented to run in O(N log N) time.<<ETX>>
[knock-knee mode, time complexity, Routing, three conducting layers, Wire, layer assignment, three-layer wirable layout, Wiring, layout algorithms, vias, area optimal routing algorithm, three-layer channel routing, circuit layout CAD, computational complexity]
Flow in planar graphs with multiple sources and sinks
30th Annual Symposium on Foundations of Computer Science
None
1989
Given a planar network with many sources and sinks, the problem of computing the maximum flow from the sources to the sinks is investigated. An algorithm that runs in O(log/sup 2/n) time using O(n/sup 1.5/) processors on an exclusive-read-exclusive-write parallel random-access machine (EREW PRAM) is obtained, when the amount of flow (demand) at each source and sink is assumed as input. When the demands are unknown, the problem remains open. However, in the special case in which the sources and sinks are all on one face (and the demands unknown), an algorithm that computes the maximum flow with time complexity O(log/sup 3/n log log n) using O(n/sup 1.5/) processors is given. The results also hold for more general networks, namely, when the edge capacities have both lower and upper bounds.<<ETX>>
[Algorithm design and analysis, edge capacities, graph theory, sinks, planar graphs, upper bounds, Distributed computing, Parallel algorithms, Computer networks, Contracts, planar network, parallel algorithms, exclusive-read-exclusive-write parallel random-access machine, time complexity, Phase change random access memory, demand, lower bounds, Computer science, EREW PRAM, Upper bound, face, multiple sources, maximum flow, Joining processes, Fellows, computational complexity]
Planning and learning in permutation groups
30th Annual Symposium on Foundations of Computer Science
None
1989
Planning is defined as the problem of synthesizing a desired behavior from given basic operations, and learning is defined as the dual problem of analyzing a given behavior to determine the unknown basic operations. Algorithms for solving these problems in the context of invertible operations on finite-state environments are developed. In addition to their obvious artificial intelligence applications, the algorithms can efficiently find the shortest way to solve Rubik's cube, test ping-pong protocols, and solve systems of equations over permutation groups.<<ETX>>
[System testing, Pediatrics, Protocols, finite automata, test ping-pong protocols, Mathematics, Vectors, learning, finite-state environments, Intelligent robots, Equations, artificial intelligence, learning systems, invertible operations, planning, permutation groups, Polynomials, Artificial intelligence]
The synchronization of nonuniform networks of finite automata
30th Annual Symposium on Foundations of Computer Science
None
1989
The generalized firing squad synchronization problem (GFSSP) is the well-known firing squad synchronization problem extended to arbitrarily connected networks of finite automata. When the transmission delays associated with the links of a network are allowed to be arbitrary nonnegative integers, the problem is called GFSSP-NUD (GFSSP with nonuniform delays). A solution of GFSSP-NUD is given for the first time. The solution is independent of the structure of the network and the actual delays of the links. The firing time of the solution is bounded by O( Delta /sup 3/+ tau /sub max/), where tau /sub max/ is the maximum transmission delay of any single link and Delta is the maximum transmission delay between the general and any other node of a given network. Extensions of GFSSP and GFSSP-NUD to networks with more than one general are presented.<<ETX>>
[Computer science, nonuniform networks, finite automata, Delay effects, Automata, Fires, generalized firing squad synchronization problem, Tin, GFSSP, synchronization, transmission delays]
Generating random spanning trees
30th Annual Symposium on Foundations of Computer Science
None
1989
The author describes a probabilistic algorithm that, given a connected, undirected graph G with n vertices, produces a spanning tree of G chosen uniformly at random among the spanning trees of G. The expected running time is O(n log n) per generated tree for almost all graphs, and O(n/sup 3/) for the worst graphs. Previously known deterministic algorithms are much more complicated and require O(n/sup 3/) time per generated tree. A Markov chain is called rapidly mixing if it gets close to the limit distribution in time polynomial in the log of the number of states. Starting from the analysis of the above algorithm, it is shown that the Markov chain on the space of all spanning trees of a given graph where the basic step is an edge swap is rapidly mixing.<<ETX>>
[Algorithm design and analysis, random spanning trees, graph theory, Stochastic processes, Ice, Graph theory, Markov chain, limit distribution, Tree graphs, probabilistic algorithm, Markov processes, Polynomials, Eigenvalues and eigenfunctions, time polynomial, undirected graph, computational complexity, edge swap]
Approximation algorithms for geometric embeddings in the plane with applications to parallel processing problems
30th Annual Symposium on Foundations of Computer Science
None
1989
Given an undirected graph G with N vertices and a set P of N points in the plane, the geometric embedding problem consists of finding a bijection from the vertices of G to the points in the plane which minimize the sum total of edge lengths of the embedded graph. Fast approximation algorithms are given for embedding d-dimensional grids in the plane within a factor of O(log N) times optimal cost for d>2 and O(log/sup 2/N) for d=2. It is shown that any embedding of a hypercube, butterfly, or shuffle-exchange graph must be within an O(log N) factor of optimal cost. When the points of P are randomly distributed or arranged in a grid, a polynomial-time algorithm that can embed arbitrary weighted graphs in these points with cost within an O(log/sup 2/N) factor of optimal is given. It is shown how the algorithms developed for geometric embeddings can be used to give O(log/sup 2/N) times optimal solutions to problems of performance optimization for parallel processors.<<ETX>>
[Laboratories, graph theory, computational geometry, parallel processing problems, Routing, Mathematics, approximation algorithms, Application software, parallel processing, geometric embeddings, Parallel processing, Approximation algorithms, Cost function, Hypercubes, Polynomials, performance optimization, undirected graph, polynomial-time algorithm, Contracts]
A new algorithm for minimizing convex functions over convex sets
30th Annual Symposium on Foundations of Computer Science
None
1989
An algorithm for minimizing a convex function over a convex set is given. The notion of a volumetric center of a polytope and a related ellipsoid of maximum volume inscribable in the polytope is central to the algorithm. The algorithm has a much better rate of global convergence than the ellipsoid algorithm. A by-product of the algorithm is an algorithm for solving linear programming problems that performs a total of O(mn/sup 2/L+M(n)nL) arithmetic operations in the worst case, where m is the number of constraints, n the number of variables, and L a certain parameter. This gives an improvement in the time complexity of linear programming for m>n/sup 2/.<<ETX>>
[global convergence, time complexity, linear programming, volumetric center, Ellipsoids, Convergence, convex functions, Iterative algorithms, Computational efficiency, polytope, Testing, computational complexity, related ellipsoid]
Decidability and expressiveness for first-order logics of probability
30th Annual Symposium on Foundations of Computer Science
None
1989
Decidability and expressiveness issues for two first-order logics of probability are considered. In one the probability is on possible worlds, whereas in the other it is on the domain. It turns out that in both cases it takes very little to make reasoning about probability highly undecidable. It is shown that, when the probability is on the domain, if the language contains only unary predicates, then the validity problem is decidable. However, if the language contains even one binary predicate, the validity problem is Pi /sub 1//sup 2/ as hard as elementary analysis with free predicate and function symbols. With equality in the language, even with no other symbol, the validity problem is at least as hard as that for elementary analysis, Pi /sub infinity //sup 1/. Thus, the logic cannot be axiomatized in either case. When the probability is on the set of possible worlds, the validity problem is Pi /sub 1//sup 2/ complete with as little as one unary predicate in the language, even without equality. With equality, Pi /sub infinity //sup 1/ hardness with only a constant symbol is obtained. In many applications it suffices to restrict attention to domains of a bounded size; it is shown that the logics are decidable in this case.<<ETX>>
[decidable, Uncertainty, validity problem, probabilistic logic, probability, reasoning, Probability, first-order logics, binary predicate, Birds, Probabilistic logic, language, possible worlds, elementary analysis, Machinery, decidability, domain, function symbols, expressiveness, Expert systems, unary predicates]
Randomized search trees
30th Annual Symposium on Foundations of Computer Science
None
1989
A randomized strategy for maintaining balance in dynamically changing search trees that has optimal expected behavior is presented. In particular, in the expected case an update takes logarithmic time and requires fewer than two rotations. Moreover, the update time remains logarithmic, even if the cost of a rotation is taken to be proportional to the size of the rotated subtree. The approach generalizes naturally to weighted trees, where the expected time bounds for accesses and updates again match the worst case time bounds of the best deterministic methods. The balancing strategy and algorithms are exceedingly simple and should be fast in practice.<<ETX>>
[Costs, dynamically changing search trees, optimal expected behavior, trees (mathematics), Binary search trees, tree balancing, logarithmic time, Computer science, Concurrent computing, weighted trees, Binary trees, Computer applications, expected time bounds, Frequency, data structures, Dynamic programming, balancing strategy, search problems, update time, rotated subtree, computational complexity]
Efficient tree pattern matching
30th Annual Symposium on Foundations of Computer Science
None
1989
A classic open problem on tree pattern matching is whether the naive O(mn)-step algorithm for finding all the occurrences of a pattern tree of size m in a text tree of size n can be improved. An O(nM/sup 0.75/ polylog(m))-step algorithm for this tree pattern matching problem is designed. The problems of linear string matching with don't care symbols and linear string max-min convolution are treated.<<ETX>>
[Algorithm design and analysis, text tree, Costs, combinatorial mathematics, O(mn)-step algorithm, trees (mathematics), Partitioning algorithms, Computer science, Convolution, linear string matching, Polynomials, data structures, linear string max-min convolution, Labeling, tree pattern matching, Pattern matching]
Fast matching algorithms for points on a polygon
30th Annual Symposium on Foundations of Computer Science
None
1989
The complete graph induced by a set of 2n points on the boundary of a polygon is considered. The edges are assigned weights equal to the Euclidean distance between their endpoints if the endpoints see each other in the polygon, and + infinity otherwise. An O(n log n)-time algorithm is obtained for finding a minimum-weight perfect matching in this graph if the polygon is convex, and an O(n log/sup 2/n)-time algorithm if the polygon is simple but nonconvex. The assignment problem for a convex polygon is solved in time O(n log n), and O(n alpha (n)) and O(n alpha (n) log n) time bounds are obtained for the verification problem on convex and nonconvex polygons, respectively, where alpha (n) is the functional inverse of the Ackermann function.<<ETX>>
[boundary, Operations research, nonconvex, graph theory, Very large scale integration, computational geometry, assignment problem, minimum-weight perfect matching, simple, nonconvex polygons, Tree graphs, convex polygon, Polynomials, Ackermann function, endpoints, verification problem, Traveling salesman problems, edges, weights, Statistics, functional inverse, Euclidean distance, minimisation, complete graph, Pattern matching, computational complexity]
Solvability in asynchronous environments
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors present necessary and sufficient combinatorial conditions that determine membership in SM/sub t/ (respectively, MP/sub t/), the class of distributed decision tasks that are solvable in the shared memory (resp. message passing) model by a t-resilient randomized protocol, which never errs and works in the presence of a strong adversary. The sufficiency of the conditions is proved by designing protocols that are applicable to all tasks in the appropriate class. The computational complexity of the membership characterization is studied.<<ETX>>
[Heart, Protocols, asynchronous environments, Surface-mount technology, Law, distributed decision tasks, asynchronous sequential logic, Read-write memory, Computer crashes, combinatorial conditions, Computer science, Algorithms, membership characterization, Message passing, Writing, protocols, computational complexity]
An optimal algorithm for intersecting three-dimensional convex polyhedra
30th Annual Symposium on Foundations of Computer Science
None
1989
A linear algorithm for intersecting two convex polyhedra in 3-space is described. The algorithm is quite simple; it does not require any complicated data structure and should be practical. A number of optimal algorithms for other problems are obtained directly from this result. These include intersecting several polytopes at once or computing the convex hull of their union, merging Voronoi diagrams in the plane in linear time, and computing three-dimensional convex hulls in linear expected time.<<ETX>>
[Algorithm design and analysis, Terminology, Voronoi diagrams, Merging, computational geometry, Data structures, Topology, History, convex hull, Machinery, Computer science, Geometry, three-dimensional, Computer graphics, convex polyhedra, intersecting, optimal algorithm]
Decision versus search problems in super-polynomial time
30th Annual Symposium on Foundations of Computer Science
None
1989
The following propositions are considered: (1) E=NE (i.e. it is decidable in exponential time whether there is a solution for an exponential-type search problem). (2) Every exponential-type search problem is solvable in exponential time. (3) The first solution to every exponential-type search problem can be found in exponential time. (4) E=E/sup NP/. It is easy to see that (4) implies (3) implies (2) implies (1). It has been conjectured that the first and last of these assumptions are equivalent in every relativized world. It is proved here that there exist relativized words in which the last two implications are not reversible. This is evidence that the search problem is not reducible to decision problems in exponential time. It is also proved that the third and fourth assumptions are equivalent. The combinatorial core of the separation results is a lower bound on the parallel complexity of a generalized version of the X-search problem.<<ETX>>
[decidable, parallel complexity, Computational modeling, Search problems, X-search problem, NP-complete problem, Computational complexity, Concurrent computing, decidability, super-polynomial time, Polynomials, decision problems, search problems, computational complexity]
A theory of learning simple concepts under simple distributions and average case complexity for the universal distribution
30th Annual Symposium on Foundations of Computer Science
None
1989
It is pointed out that in L.G. Valiant's learning model (Commun. ACM, vol.27, p.1134-42, 1984) many concepts turn out to be too hard to learn, whereas in practice, almost nothing we care to learn appears to be not learnable. To model the intuitive notion of learning more closely, it is assumed that learning happens under an arbitrary distribution, rather than under an arbitrary simple distribution, as assumed by Valiant. A distribution is called simple if it is dominated by a semicomputable distribution. A general theory of learning under simple distributions is developed. In particular, it is shown that one can learn under all simple distributions if one can learn under one fixed simple distribution, called the universal distribution. Interesting learning algorithms and several quite general new learnable classes are presented. It is shown that for essentially all algorithms, if the inputs are distributed according to the universal distribution, then the average-case complexity is of the same order of magnitude as the worst-case complexity.<<ETX>>
[universal distribution, Computer aided software engineering, Educational robots, intuitive notion, learning algorithms, Humans, inputs, Doped fiber amplifiers, worst-case complexity, learnable classes, Sun, average case complexity, learning systems, Computer science, semicomputable distribution, simple concepts, simple distributions, Polynomials, Informatics, learning theory, computational complexity]
Stable maintenance of point set triangulations in two dimensions
30th Annual Symposium on Foundations of Computer Science
None
1989
Geometric algorithms are explored, assuming that arithmetic is done approximately. Stable algorithms are described for two geometric problems. The first algorithm computes two-dimensional convex hulls. The main result is that a triangulation of a set of points in the plane can be maintained stably. The second algorithm deals with line arrangements in the plane.<<ETX>>
[plane, point set triangulations, Robust stability, Error analysis, stable algorithms, error analysis, computational geometry, line arrangements, Floating-point arithmetic, two-dimensional convex hulls, Numerical analysis, epsilon arithmetic, digital arithmetic, geometric problems, Polynomials, Robustness, Numerical stability, stability, Testing]
Galois groups and factoring polynomials over finite fields
30th Annual Symposium on Foundations of Computer Science
None
1989
Let p be a prime and F be a polynomial with integer coefficients. Suppose that the discriminant of F is not divisible by p, and denote by m the degree of the splitting field of F over Q and by L the maximal size of the coefficients of F. Then, assuming the generalized Riemann hypothesis (GRH), it is shown that the irreducible factors of F modulo p can be found in deterministic time polynomial in deg F, m, log p, and L. As an application, it is shown that it is possible under GRH to solve certain equations of the form nP=R, where R is a given and P is an unknown point of an elliptic curve defined over GF(p) in polynomial time (n is counted in unary). An elliptic analog of results obtained recently about factoring polynomials with the help of smooth multiplicative subgroups of finite field is proved.<<ETX>>
[prime, splitting field, elliptic curve, factoring polynomials, integer coefficients, maximal size, elliptic analog, Hydrogen, smooth multiplicative subgroups, Encoding, Galois fields, Equations, group theory, finite fields, irreducible factors, Elliptic curves, Algebra, discriminant, Polynomials, Galois groups, generalized Riemann hypothesis, deterministic time polynomial, computational complexity]
On the complexity of learning from counterexamples
30th Annual Symposium on Foundations of Computer Science
None
1989
The complexity of learning concepts belonging to various concrete concept classes C contained in 2/sup X/ over a finite domain X is analyzed in terms of the number of counterexamples that are needed in the worst case. It turns out that for many interesting concept classes there exist exponential differences between the number of counterexamples that are required by a 'naive' learning algorithm for C (e.g. one that always outputs the minimal consistent hypothesis) and a 'smart' learning algorithm for C, which attempts to make a more sophisticated prediction. theta (log n) bounds are given for the number of counterexamples that are required for learning boxes, balls, and halfspaces in a d-dimensional discrete space X=(1, . . ., n)/sup d/ (for every finite dimension d). Also given are an upper bound of O(d/sup 3/) and a lower bound of Omega (d/sup 2/) for the complexity of learning a threshold function with d input bits (i.e. X=(0, 1)/sup d/). For each of these concept classes one can give learning algorithms that are both optimal (or close to optimal in the case of threshold functions) with regard to the number of counterexamples which they require and computationally feasible. The complexity of learning the concept classes on several variations of the learning model considered is determined. The relationship between these learning models and some related combinatorial invariants is clarified.<<ETX>>
[Algorithm design and analysis, upper bound, Mathematics, Partitioning algorithms, lower bound, d-dimensional discrete space, Statistics, combinatorial invariants, Ellipsoids, Radio access networks, learning systems, Computer science, Upper bound, complexity of learning, Concrete, Mathematical model, finite domain, computational complexity]
Generalizing the continued fraction algorithm to arbitrary dimensions
30th Annual Symposium on Foundations of Computer Science
None
1989
A higher dimensional continued fraction algorithm (CFA) that produces diophantine approximations of more than linear goodness is given. The algorithm is also ideally convergent and detects integer relations.<<ETX>>
[Costs, convergent, Vectors, Zinc, continued fraction algorithm, Convergence, linear goodness, Jacobian matrices, integer relations, algorithm theory, Tires, diophantine approximations, Arithmetic]
Lower bounds for the stable marriage problem and its variants
30th Annual Symposium on Foundations of Computer Science
None
1989
An instance of the stable marriage problem of size n involves n men and n women. Each participant ranks all members of the opposite sex in order of preference. A stable marriage is a complete matching M=((m/sub 1/, w/sub i1/), (m/sub 2/, w/sub i2/), . . ., (m/sub n/, w/sub in/)) such that no unmatched man and woman prefer each other to their partners in M. A pair (m/sub i/, w/sub j/) is stable if it is contained in some stable marriage. The problem of determining whether an arbitrary pair is stable in a given problem instance is studied. It is shown that the problem has a lower bound of Omega (n/sup 2/) in the worst case. As corollaries of the results, the lower bound of Omega (n/sup 2/) is established for several related stable marriage problems, including that of finding a stable marriage for any given problem instance.<<ETX>>
[Algorithm design and analysis, Computational modeling, opposite sex, order of preference, lower bound, Computer science, worst case, ranks, partners, operations research, men, complete matching, Artificial intelligence, stable marriage problem, computational complexity, women]
On the complexity of space bounded interactive proofs
30th Annual Symposium on Foundations of Computer Science
None
1989
Two results on interactive proof systems with two-way probabilistic finite-state verifiers are proved. The first is a lower bound on the power of such proof systems if they are not required to halt with high probability on rejected inputs: it is shown that they can accept any recursively enumerable language. The second is an upper bound on the power of interactive proof systems that halt with high probability on all inputs. The proof method for the lower bound also shows that the emptiness problem for one-way probabilistic finite-state machines is undecidable. In the proof of the upper bound some results of independent interest on the rate of convergence of time-varying Markov chains to their halting states are obtained.<<ETX>>
[Error probability, finite automata, Area measurement, space bounded interactive proofs, finite-state verifiers, interactive proof systems, upper bound, Markov chains, Convergence, Computer science, Upper bound, Time varying systems, decidability, undecidable, Automata, theorem proving, finite-state machines, Contracts, recursively enumerable language, computational complexity]
An analogue of the Myhill-Nerode theorem and its use in computing finite-basis characterizations
30th Annual Symposium on Foundations of Computer Science
None
1989
A theorem that is a graph-theoretic analog of the Myhill-Nerode characterization of regular languages is proved. The theorem is used to establish that for many applications obstruction sets are computable by known algorithms. The focus is exclusively on what is computable (by a known algorithm) in principle, as opposed to what is computable in practice.<<ETX>>
[graph theory, regular languages, Doped fiber amplifiers, Analog computers, computability, Computer science, graph-theoretic analog, obstruction sets, Resists, Polynomials, Argon, Myhill-Nerode characterization, Contracts, computing finite-basis characterizations]
Interior-point methods in parallel computation
30th Annual Symposium on Foundations of Computer Science
None
1989
Interior-point methods for linear programming, developed in the context of sequential computation, are used to obtain a parallel algorithm for the bipartite matching problem. The algorithm runs in O*( square root m) time. The results extend to the weighted bipartite matching problem and to the zero-one minimum-cost flow problem, yielding O*( square root m log C) algorithms. This improves previous bounds on these problems and illustrates the importance of interior-point methods in parallel algorithm design.<<ETX>>
[Algorithm design and analysis, parallel algorithms, Operations research, Costs, interior-point methods, parallel algorithm, Linear programming, linear programming, Parallel algorithms, Sun, Uninterruptible power systems, Concurrent computing, Bipartite graph, Contracts, bipartite matching problem]
The 0-1 law fails for the class of existential second order Godel sentences with equality
30th Annual Symposium on Foundations of Computer Science
None
1989
P. Kolaitis and M. Vardi (see Proc. 19th ACM Symp. on Theory of Computing, p.425-35 (1987), and Proc. 3rd Ann. Symp. on Logic in Computer Science, p.2-11 (1988)) proved that the 0-1 law holds for the second-order existential sentences whose first-order parts are formulas of Bernays-Schonfinkel or Ackermann prefix classes. They also provided several examples of second-order formulas for which the 0-1 law does not hold and noticed that the classification of second-order sentences for which the 0-1 law holds resembles the classification of decidable cases of prenex first-order sentences. The only cases they have not settled were the cases of Godel classes with and without equality. The authors confirm the conjecture of Kolaitis and Vardi that the 0-1 law does not hold for the existential second-order sentences whose first-order part is in the godel prenex form with equality.<<ETX>>
[Out of order, prenex first-order sentences, 0-1 law, Bernays-Schonfinkel, Complexity theory, Application software, Combinatorial mathematics, Computer science, formal logic, Ackermann prefix classes, decidable cases, Upper bound, decidability, second order Godel sentences, Logic]
Space-efficient static trees and graphs
30th Annual Symposium on Foundations of Computer Science
None
1989
Data structures that represent static unlabeled trees and planar graphs are developed. The structures are more space efficient than conventional pointer-based representations, but (to within a constant factor) they are just as time efficient for traversal operations. For trees, the data structures described are asymptotically optimal: there is no other structure that encodes n-node trees with fewer bits per node, as N grows without bound. For planar graphs (and for all graphs of bounded page number), the data structure described uses linear space: it is within a constant factor of the most succinct representation.<<ETX>>
[Tree data structures, graph theory, trees (mathematics), planar graphs, data structure, Data structures, Size measurement, Encoding, static unlabeled trees, Jacobian matrices, Computer science, bounded page number, Tree graphs, bits per node, traversal operations, Binary trees, n-node trees, data structures, Joining processes, Testing, computational complexity]
An upper bound on the number of planar k-sets
30th Annual Symposium on Foundations of Computer Science
None
1989
Given a set S of n points, a subset X of size k is called a k-set if there is a hyperplane II that separates X from X/sup c/. It is proved that O(n square root k/log/sub */k) is an upper bound for the number of k-sets in the plane, thus improving the previous bound of P. Erdos et al. (A Survey of Combinatorial Theory, North-Holland, 1983, p.139-49) by a factor of log/sub */k. The method can be extended to give the bound O(n square root k/(log k)/sup epsilon /). The proof only establishes the weaker result; it uses the geometry and combinatorics together in a stronger way than in the earlier work.<<ETX>>
[combinatorial mathematics, planar k-sets, upper bound, hyperplane II, set theory, Application software, points, Computer science, Geometry, Upper bound, Sampling methods, geometry, computational complexity, combinatorics]
Efficient cryptographic schemes provably as secure as subset sum
30th Annual Symposium on Foundations of Computer Science
None
1989
Very efficient constructions, based on the intractability of the subset sum problem for certain dimensions, are shown for a pseudorandom generator and for a universal one-way hash function. (Pseudorandom generators can be used for private key encryption, and universal one-way hash functions for signature schemes). The increase in efficiency in the construction is due to the fact that many bits can be generated/hashed with one application of the assumed one-way function. All the constructions can be implemented in NC using an optimal number of processors.<<ETX>>
[pseudorandom generator, Computational modeling, intractability, cryptography, cryptographic schemes, History, Cryptographic protocols, Computer hacking, NC, Public key, Chromium, Public key cryptography, file organisation, universal one-way hash function, subset sum]
Network decomposition and locality in distributed computation
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors introduce a concept of network decomposition, a partitioning of an arbitrary graph into small-diameter connected components, such that the graph created by contracting each component into a single node has low chromatic number. They present an efficient distributed algorithm for constructing such a decomposition and demonstrate its use for design of efficient distributed algorithms. The method yields new deterministic distributed algorithms for finding a maximal independent set in an arbitrary graph and for ( Delta +1)-coloring of graphs with maximum degree Delta . These algorithms run in O(n/sup epsilon /) time for epsilon =O((log log n/log n)/sup 1/2/), whereas the best previously known deterministic algorithms required Omega (n) time. The techniques can also be used to remove randomness from the previously known most distributed breadth-first search algorithm.<<ETX>>
[Algorithm design and analysis, arbitrary graph, Computational modeling, graph theory, computer networks, distributed processing, distributed computation, Distributed computing, Parallel algorithms, deterministic algorithms, Concurrent computing, Intelligent networks, distributed algorithm, network decomposition, Computer networks, Global communication, graph partitioning, Distributed algorithms, Contracts]
The parallel complexity of the subgraph connectivity problem
30th Annual Symposium on Foundations of Computer Science
None
1989
It is shown that the problem of testing whether a graph G contains a vertex- (edge-) connected induced subgraph of cardinality k is P-complete for any fixed k>or=3. Moreover, it is shown that approximating within a factor c>1/2 the maximum d for which there is a d-vertex-(d-edge-) connected induced subgraph of G is not in NC, unless P=NC. In contrast, it is known that the problem of finding the Tutte (triconnected) components of G is in NC. On the positive side, it is shown by proving extremal-graph results, that the maximum d for which there is a d-edge-connected induced subgraph of G can be approximated in NC within any factor c<1/2 and that the same is true for vertex connectivity for any C<1/4.<<ETX>>
[Out of order, parallel algorithms, parallel complexity, subgraph connectivity problem, graph theory, induced subgraph, cardinality, Computer science, triconnected components, P-complete, Contracts, Testing, computational complexity]
On the network complexity of selection
30th Annual Symposium on Foundations of Computer Science
None
1989
The sequential complexity of determining the kth largest out of a given set of n keys is known to be linear. Thus, given a p-processor parallel machine, it is asked whether or not an O(n/p) selection algorithm can be devised for that machine. An Omega ((n/p) log log p+log p) lower bound is obtained for selection on any network that satisfies a particular low expansion property. The class of networks satisfying this property includes all of the common network families, such as the tree, multidimensional mesh, hypercube, butterfly, and shuffle-exchange. When n/p is sufficiently large (e.g. greater than log/sup 2/p on the butterfly, hypercube, and shuffle-exchange), this result is matched by the upper bound given previously by the author (Proc. 1st Ann. ACM Symp. on Parallel Algorithms and Architecture p.64-73, 1989).<<ETX>>
[multidimensional mesh, hypercube, Multiprocessor interconnection networks, Parallel machines, sequential complexity, parallel machines, shuffle-exchange, Computer science, Upper bound, network complexity, parallel machine, selection algorithm, butterfly, Communication channels, Hypercubes, computational complexity]
Every polynomial-time 1-degree collapses iff P=PSPACE
30th Annual Symposium on Foundations of Computer Science
None
1989
A set A is m-reducible (or Karp-reducible) to B if and only if there is a polynomial-time computable function f such that for all x, x in A if and only if f(x) in B. Two sets are 1-equivalent if each is m-reducible to the other by one-one reductions; p-invertible equivalent iff each is m-reducible to the other by one-one, polynomial-time invertible reductions; and p-isomorphic iff there is an m-reduction from one set to the other that is one-one, onto, and polynomial-time invertible. It is proved that the following statements are equivalent: (1) P=PSPACE. (2) Every two 1-equivalent sets are p-isomorphic. (3) Every two p-invertible equivalent sets are p-isomorphic.<<ETX>>
[m-reducible, Karp-reducible, equivalent sets, computability, polynomial-time computable function, Polynomials, set theory, computational complexity]
An optimal parallel algorithm for graph planarity
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors present a parallel algorithm based on open ear decomposition which, given a graph G on n vertices, constructs an embedding of G onto the plane or reports that G is nonplanar. This parallel algorithm runs on a concurrent-read, concurrent-write parallel random-access machine (CRCW PRAM) in O(log n) time with the same processor bound as graph connectivity.<<ETX>>
[parallel algorithms, CRCW PRAM, random-access storage, NASA, graph theory, optimal parallel algorithm, open ear decomposition, Very large scale integration, Phase change random access memory, Parallel algorithms, graph planarity, Concurrent computing, Semiconductor device modeling, graph connectivity, Ear, Polynomials, embedding, Contracts, Testing, computational complexity]
Expanders might be practical: fast algorithms for routing around faults on multibutterflies
30th Annual Symposium on Foundations of Computer Science
None
1989
Simple deterministic O(log N)-step algorithms for routing packets on a multibutterfly are described. The algorithms are shown to be robust against faults, even in the worst case, and to be efficient from a practical point of view. As a consequence, the multibutterfly is shown to be an excellent candidate for a high-bandwidth, low-diameter switching network underlying a distributed-memory machine.<<ETX>>
[Context, routing packets, switching network, robust, Merging, packet switching, Switches, Routing, Mathematics, distributed-memory machine, Computer science, high-bandwidth, switching networks, Robustness, Joining processes, multibutterflies, Contracts]
An optimal lower bound on the number of variables for graph identification
30th Annual Symposium on Foundations of Computer Science
None
1989
It is shown that Omega (n) variables are needed for first-order logic with counting to identify graphs on n vertices. This settles a long-standing open problem. The lower bound remains true over a set of graphs of color class size 4. This contrasts sharply with the fact that three variables suffice to identify all graphs of color class size 3, and two variables suffice to identify almost all graphs. The lower bound is optimal up to multiplication by a constant because n variables obviously suffice to identify graphs on n vertices.<<ETX>>
[graph identification, Particle separators, optimal lower bound, graph theory, first-order logic, Orbits, Complexity theory, lower bound, Computer science, formal logic, Upper bound, Tree graphs, Polynomials, Logic, Labeling, Testing]
Pipelining computations in a tree of processors
30th Annual Symposium on Foundations of Computer Science
None
1989
The computational power of a tree of processors is investigated. It is demonstrated that a tree of processors can solve certain problems impressively fast by exploiting the internal pipelining capabilities. Efficient tree algorithms are designed for two different problems: selection and maintaining dictionaries. It is shown that an O(log n)-height tree of processors can find the kth smallest element of n numbers in deterministic O((log n)/sup 2+o(1)/) steps, an impressive improvement over previous results. The main tools are the development of a new sampling technique and an elegant internal pipelining strategy. A lower bound is established for this selection problem. Another variant of the sampling technique reduces the storage requirement of R.M. Karp et al.'s (1986) tree searching algorithm while maintaining its speed. It is established that dictionary operations can be performed with a pipelined interval of O(1) and a response time of O(height of the tree), which again improves a known result and settles an open problem. This is based on being able to make the tree operate like a complete tree view from the root.<<ETX>>
[Algorithm design and analysis, Dictionaries, Computational modeling, trees (mathematics), tree of processors, Phase change random access memory, pipelining computations, lower bound, Delay, Pipeline processing, Computer science, Concurrent computing, Sampling methods, sampling technique, Decision trees, search problems, computational complexity, dictionaries]
Multiparty communication complexity
30th Annual Symposium on Foundations of Computer Science
None
1989
A given Boolean function has its input distributed among many parties. The aim is to determine which parties to talk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. It is shown that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.<<ETX>>
[nondeterministic communication complexity, Boolean functions, Scholarships, information lower bound, Very large scale integration, Boolean function, Polynomials, Complexity theory, Decision trees, communication complexity, Distributed computing, computational complexity]
On obstructions in relation to a fixed viewpoint
30th Annual Symposium on Foundations of Computer Science
None
1989
108 Efficient, randomized algorithms are given for the following problems: (1) construction of levels of order 1 to k in an arrangement of hyperplanes in any dimension; (2) construction of higher order Voronoi diagrams of order 1 to k in any dimension; (3) hidden surface removal for completely general scenes (intersecting and curved faces are allowed). A combinatorial tool in the form of a mathematical series called a theta series is associated with a configuration of polytopes in R/sup d/. It is used to study the combinatorial, as well as algorithmic, complexity of the geometric problems under consideration.<<ETX>>
[complexity, Voronoi diagrams, obstructions, series (mathematics), computational geometry, hyperplanes, Surface roughness, Rough surfaces, fixed viewpoint, combinatorial tool, Computational geometry, mathematical series, Layout, randomized algorithms, algorithm theory, hidden surface removal, geometric problems, computational complexity]
Generalizing the PAC model: sample size bounds from metric dimension-based uniform convergence results
30th Annual Symposium on Foundations of Computer Science
None
1989
The probably approximately correct (PAC) model of learning from examples is generalized. The problem of learning functions from a set X into a set Y is considered, assuming only that the examples are generated by independent draws according to an unknown probability measure on X*Y. The learner's goal is to find a function in a given hypothesis space of functions from X into Y that on average give Y values that are close to those observed in random examples. The discrepancy is measured by a bounded real-valued loss function. The average loss is called the error of the hypothesis. A theorem on the uniform convergence of empirical error estimates to true error rates is given for certain hypothesis spaces, and it is shown how this implies learnability. A generalized notion of VC dimension that applies to classes of real-valued functions and a notion of capacity for classes of functions that map into a bounded metric space are given. These measures are used to bound the rate of convergence of empirical error estimates to true error rates, giving bounds on the sample size needed for learning using hypotheses in these classes. As an application, a distribution-independent uniform convergence result for certain classes of functions computed by feedforward neural nets is obtained. Distribution-specific uniform convergence results for classes of functions that are uniformly continuous on average are also obtained.<<ETX>>
[Virtual colonoscopy, VC dimension, Error analysis, Fractals, Loss measurement, learnability, Distributed computing, bounded metric space, Convergence, hypothesis error, unknown probability, distribution specific uniform convergence, true error rates, independent draws, learning functions, set, PAC model, real-valued functions, probability, empirical error estimates, distribution-independent uniform convergence result, Extraterrestrial measurements, Size measurement, convergence of numerical methods, Computational complexity, metric dimension-based uniform convergence results, learning systems, bounded real-valued loss function, learning from examples, average loss, Machine learning, probably approximately correct, computational complexity, feedforward neural nets]
Double precision geometry: a general technique for calculating line and segment intersections using rounded arithmetic
30th Annual Symposium on Foundations of Computer Science
None
1989
For the first time it is shown how to reduce the cost of performing specific geometric constructions by using rounded arithmetic instead of exact arithmetic. By exploiting a property of floating-point arithmetic called monotonicity, a technique called double-precision geometry can replace exact arithmetic with rounded arithmetic in any efficient algorithm for computing the set of intersections of a set of lines or line segments. The technique reduces the complexity of any such line or segment arrangement algorithm by a constant factor. In addition, double-precision geometry reduces by a factor of N the complexity of rendering segment arrangements on a 2/sup N/*2/sup N/ integer grid such that output segments have grid points as endpoints.<<ETX>>
[Computers, segment arrangement algorithm, Costs, error analysis, computational geometry, geometric constructions, double-precision geometry, monotonicity, Floating-point arithmetic, Equations, Computational geometry, Numerical analysis, digital arithmetic, Failure analysis, roundoff errors, floating-point arithmetic, Robustness, Hardware, Standards development, computational complexity, rounded arithmetic]
Efficient algorithms for independent assignment on graphic and linear matroids
30th Annual Symposium on Foundations of Computer Science
None
1989
Efficient algorithms are presented for the matroid intersection problem and generalizations. The algorithm for weighted intersection works by scaling the weights. The cardinality algorithm is a special case that takes advantage of greater structure. Efficiency of the algorithms is illustrated by several implementations. On graphic matroids the algorithms run close to the best bounds for trivial matroids (i.e. ordinary bipartite graph matching): O( square root nm log n) for cardinality intersection and O( square root nm log/sup 2/n log(nN)) for weighted intersection (n, m, and N denote the number of vertices, edges, and largest edge weight, respectively; weights are assumed integral). Efficient algorithms are also given for linear matroids. These include both algorithms that are practical and algorithms exploiting fast matrix multiplication.<<ETX>>
[Algorithm design and analysis, Costs, Chemical analysis, graph theory, vertices, linear matroids, Continuous time systems, matroid intersection problem, cardinality algorithm, graphic matroids, largest edge weight, Integral equations, cardinality intersection, Computer graphics, Controllability, Bipartite graph, scaling, weighted intersection, edges, Application software, Computer science, bipartite graph matching, trivial matroids, independent assignment, efficient algorithms, fast matrix multiplication, computational complexity]
The equivalence and learning of probabilistic automata
30th Annual Symposium on Foundations of Computer Science
None
1989
It is proved that the equivalence problem for probabilistic automata is solvable in time O((n/sub 1/+n/sub 2/)/sup 4/), where n/sub 1/ and n/sub 2/ are numbers of states of two given probabilistic automata. This result improves the best previous upper bound of coNP. The algorithm has some interesting applications, for example, to the covering and equivalence problems for uninitiated probabilistic automata, the equivalence and containment problems for unambiguous nondeterministic finite automata, and the path-equivalence problem for nondeterministic finite automata. Using the same technique, a polynomial-time algorithm for learning probabilistic automata is developed. The learning protocol is learning by means of queries.<<ETX>>
[coNP, Gold, Protocols, equivalence, finite automata, covering, nondeterministic finite automata, Learning automata, Linear programming, upper bound, Probability distribution, Vectors, learning, learning protocol, Computer science, probabilistic automata, Polynomials, polynomial-time algorithm, Arithmetic]
The complexity of approximating the square root
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors prove upper and lower bounds for approximately computing the square root using a given set of operations. The bounds are extended to hold for approximating the kth root, for any fixed k. Several tools from approximation theory are used to prove the lower bound. These include Markoff inequality, Chebyshev polynomials, and a theorem that relates the degree of a rational function to its deviation from the approximated function over a given interval. The lower bound can be generalized to other algebraic functions. The upper bound can be generalized to obtain an O(1)-step straight-line program for evaluating any rational function with integer coefficients at a given integer point.<<ETX>>
[approximation theory, complexity, Laboratories, Read-write memory, Markoff inequality, Computer science, Upper bound, function approximation, Chebyshev approximation, rational function, Decision trees, square root, Chebyshev polynomials, computational complexity]
Approximation schemes for constrained scheduling problems
30th Annual Symposium on Foundations of Computer Science
None
1989
Several constrained scheduling problems are considered. The first polynomial approximation schemes for the problem of minimizing maximum completion time in a two-machine flow shop with release dates and for the problem of minimizing maximum lateness for the single and parallel-machine problem with release dates are described. All of these algorithms are based upon the notion of an outline, a set of information with which it is possible to compute, with relatively simple procedures and in polynomial time, an optimal or near-optimal solution to the problem instance under consideration. Two related precedence-constrained scheduling problems are discussed, and new approximation results are presented.<<ETX>>
[approximation theory, Job shop scheduling, Operations research, maximum lateness, single machine problem, Mathematics, Sun, Uninterruptible power systems, maximum completion time, outline, Processor scheduling, constrained scheduling problems, release dates, polynomial approximation, scheduling, Approximation algorithms, two-machine flow shop, parallel-machine problem, Polynomials, precedence-constrained scheduling problems, minimisation, Contracts, computational complexity]
Conductance and convergence of Markov chains-a combinatorial treatment of expanders
30th Annual Symposium on Foundations of Computer Science
None
1989
A direct combinatorial argument is given to bound the convergence rate of Markov chains in terms of their conductance (these are statements of the nature 'random walks on expanders converge fast'). In addition to showing that the linear algebra in previous arguments for such results on time-reversible Markov chains was unnecessary, the direct analysis applies to general irreversible Markov chains.<<ETX>>
[random walks, time-reversible Markov chains, Symmetric matrices, combinatorial argument, expanders, Graph theory, State-space methods, conductance, Convergence, general irreversible Markov chains, Linear algebra, Markov processes, Sampling methods, Eigenvalues and eigenfunctions, convergence rate, linear algebra]
A really temporal logic
30th Annual Symposium on Foundations of Computer Science
None
1989
A real-time temporal logic for the specification of reactive systems is introduced. The novel feature of the logic, TPTL, is the adoption of temporal operators as quantifiers over time variables; every modality binds a variable to the time(s) it refers to. TPTL is demonstrated to be both a natural specification language and a suitable formalism for verification and synthesis. A tableau-based decision procedure and model-checking algorithm for TPTL are presented. Several generalizations of TPTL are shown to be highly undecidable.<<ETX>>
[Real time systems, synthesis, temporal operators, formalism, reactive systems, specification, Specification languages, tableau-based decision procedure, formal specification, Computer science, formal logic, TPTL, specification languages, really temporal logic, natural specification language, model-checking algorithm, Logic, quantifiers, Contracts, verification]
Twists, turns, cascades, deque conjecture, and scanning theorem
30th Annual Symposium on Foundations of Computer Science
None
1989
Nearly tight upper and lower bounds on the maximum number of various rotational operations that can be performed on a binary tree are proved. One of the lower bound results refutes D.E. Sleator's turn conjecture for binary trees (see R.E. Tarjan, SIAM J. Alg. Disc. Meth., vol.2, p.306-318, 1985). The upper bound results are used to derive an inverse Ackerman bound for Tarjan's deque conjecture on the performance of the splay tree. Two new proofs of Tarjan's scanning theorem are provided. One proof generalizes the theorem, whereas the other is a simple, potential-based proof.<<ETX>>
[Costs, Dictionaries, binary tree, trees (mathematics), Binary search trees, scanning theorem, splay tree, turns, cascades, deque conjecture, Upper bound, Binary trees, inverse Ackerman bound, search problems]
Upper and lower bounds for routing schemes in dynamic networks
30th Annual Symposium on Foundations of Computer Science
None
1989
An algorithm and two lower bounds are presented for the problem of constructing and maintaining routing schemes in dynamic networks. The algorithm distributively assigns addresses to nodes and constructs routing tables in a dynamically growing tree. The resulting scheme routes data messages over the shortest path between any source and destination, assigns addresses of O(log/sup 2/n) bits to each node, and uses in its routing table O(log/sup 3/n) bits of memory per incident link, where n is the final number of nodes in the tree. The amortized communication cost of the algorithm is O(log n) messages per node. Also given are two lower bounds on the tradeoff between the quality of routing schemes (i.e. their stretch factor) and their amortized communication cost in general dynamic networks.<<ETX>>
[Algorithm design and analysis, Force measurement, routing schemes, computer networks, trees (mathematics), distributed processing, tree, Data structures, communication cost, shortest path, Computer science, Intelligent networks, dynamic networks, Cost function, Communication system traffic control, Routing protocols, Data communication, Size control]
Power of fast VLSI models is insensitive to wires' thinness
30th Annual Symposium on Foundations of Computer Science
None
1989
VLSI f-models which allow the switching time to decrease to f(D) when the length of all wires is restricted by D are called 'fast' if the decrease is slightly superlinear. The fast models are so strong and robust that their computational power cannot be increased by and combination of the following: (1) making zero the width of each wire of length d, except for its log d segment, thus eliminating layout and area considerations; (2) allowing wires to transmit log d bits simultaneously; (3) making the switching time f(d) of each node depend only on the length d of its own input wires, thus enabling small subcircuits to run faster; (4) changing f while preserving Sigma /sub k/ 1/f(k); (5) enabling the nodes to change connections arbitrarily in the run time. The authors construct a kind of operating system link server (linx, for short) that simulates all these powers online. The condition of superlinearity cannot be weakened.<<ETX>>
[operating system link server, VLSI, Circuits, Very large scale integration, fast VLSI models, switching theory, Computer science, Semiconductor device measurement, Runtime, Operating systems, Wires, linx, Automata, computational power, Robustness, Manufacturing, switching time]
Sorting on a parallel pointer machine with applications to set expression evaluation
30th Annual Symposium on Foundations of Computer Science
None
1989
Optimal algorithms for sorting on parallel CREW (concurrent read, exclusive write) and EREW (exclusive read, exclusive write) versions of the pointer machine model are presented. Intuitively, these methods can be viewed as being based on the use of linked lists rather than arrays (the usual parallel data structure). It is shown how to exploit the 'locality' of the approach to solve a problem with applications to database querying and logic programming (set-expression evaluation) in O(log n) time using O(n) processors.<<ETX>>
[linked lists, parallel data structure, information retrieval, Read-write memory, Phase change random access memory, Data structures, optimal algorithms, Graph theory, Application software, Sorting, Databases, set expression evaluation, parallel CREW, parallel pointer machine, sorting, Writing, logic programming, database querying, data structures, Joining processes, Arithmetic, EREW]
Incremental planarity testing
30th Annual Symposium on Foundations of Computer Science
None
1989
The incremental planarity testing problem consists of performing the following operations on a planar graph G with n vertices: (1) testing whether a new edge can be added to G so that the resulting graph is itself planar; (2) adding vertices and edges such that planarity is preserved. An efficient technique for incremental planarity testing that uses O(n) space and supports tests and insertion of vertices and edges in O(log n) time is presented. The bounds for queries and vertex insertions are worst case, and the bound for edge insertions is amortized.<<ETX>>
[Performance evaluation, Embedded computing, graph theory, vertices, edges, Data structures, planar graph, Application software, Circuit testing, queries, Computer science, Space technology, Automatic testing, incremental planarity testing, Layout, supports tests, data structures, insertion, vertex insertions, Assembly, edge insertions]
Output-sensitive hidden surface removal
30th Annual Symposium on Foundations of Computer Science
None
1989
Several output-sensitive algorithms for hidden surface removal in a collection of n horizontal triangles, viewed from a point at z=- infinity , are derived. If k is the combinatorial complexity of the output visibility map, then the result is a simple (deterministic) algorithm that runs in time O(n square root k log n) and several improved and more sophisticated algorithms that use randomization. One of these algorithms runs in time O(n/sup 4/3/log /sup gamma /n+k/sup 3/5/n/sup 4/5+ delta /) for any delta >0, where gamma is some constant less than 3. The performance of the other algorithms is potentially even faster; it depends on other parameters of the structure of the given triangles, as well as on the output size. A variant of the simple algorithm performs hidden surface removal for n (nonintersecting) balls in time O(n/sup 3/2/log n+k).<<ETX>>
[horizontal triangles, computational geometry, Research and development, combinatorial complexity, Computer science, output-sensitive algorithms, Councils, Layout, Computer graphics, hidden surface removal, Slabs, Pixel, output visibility map, computational complexity]
Learning binary relations and total orders
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of designing polynomial prediction algorithms for learning binary relations is studied for an online model in which the instances are drawn by the learner, by a helpful teacher, by an adversary, or according to a probability distribution on the instance space. The relation is represented as an n*m binary matrix, and results are presented when the matrix is restricted to have at most k distinct row types, and when it is constrained by requiring that the predicate form a total order.<<ETX>>
[Algorithm design and analysis, total orders, Laboratories, instance space, Predictive models, Probability distribution, learning, predicate, helpful teacher, probability distribution, Prediction algorithms, Polynomials, binary matrix, Bipartite graph, online model, Size measurement, binary relations, adversary, learning systems, matrix algebra, Computer science, Animals, polynomial prediction algorithms, distinct row types, learner, computational complexity]
Speeding-up linear programming using fast matrix multiplication
30th Annual Symposium on Foundations of Computer Science
None
1989
The author presents an algorithm for solving linear programming problems that requires O((m+n)/sup 1.5/nL) arithmetic operations in the worst case, where m is the number of constraints, n the number of variables, and L a parameter defined in the paper. This result improves on the best known time complexity for linear programming by about square root n. A key ingredient in obtaining the speedup is a proper combination and balancing of precomputation of certain matrices by fast matrix multiplication and low-rank incremental updating of inverses of other matrices. Specializing the algorithm to problems such as minimum-cost flow, flow with losses and gains, and multicommodity flow leads to algorithms whose time complexity closely matches or is better than the time complexity of the best known algorithms for these problems.<<ETX>>
[Costs, time complexity, Linear programming, linear programming, Ellipsoids, matrix algebra, minimum-cost flow, Polynomials, multicommodity flow, fast matrix multiplication, Arithmetic, computational complexity, flow with losses]
Datalog vs. first-order logic
30th Annual Symposium on Foundations of Computer Science
None
1989
The relation between the expressive power of datalog and that of first-order languages, is clarified. It is then proved that every first-order expressible datalog query is bounded. A form of compactness theorem for finite structure implied by this result is examined, and counterexamples to natural generalizations of the above result are given.<<ETX>>
[formal languages, first-order languages, first-order logic, Relational databases, query languages, relational databases, Database languages, bounded, finite structure, database theory, formal logic, first-order expressible datalog query, compactness theorem, Logic]
Lower bounds for constant depth circuits in the presence of help bits
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of how many extra bits of 'help' a constant depth circuit needs in order to compute m functions is considered. Each help bit can be an arbitrary Boolean function. An exponential lower bound on the size of the circuit computing m parity functions in the presence of m-1 help bits is proved. The proof is carried out using the algebraic machinery of A. Razborov (1987) and R. Smolensky (1987). A by-product of the proof is that the same bound holds for circuits with mod/sub p/ gates for a fixed prime p>2. The lower bound implies a random oracle separation for PH and PSPACE, which is optimal in a technical sense.<<ETX>>
[Frequency selective surfaces, Commutation, Circuits, arbitrary Boolean function, Complexity theory, PSPACE, Machinery, Computational complexity, Combinatorial mathematics, exponential lower bound, constant depth circuit, Computer science, Boolean functions, parity functions, constant depth circuits, PH, Polynomials, random oracle separation, help bit, computational complexity]
Full abstraction for nondeterministic dataflow networks
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of finding fully abstract semantic models for nondeterministic dataflow networks is discussed. The author presents a result indicating that there exist nondeterministic networks using only bounded choice for which the input-output relation is not compositional. It is shown that the trace semantics is fully abstract for all nondeterministic as well as deterministic networks.<<ETX>>
[Computer science, Concurrent computing, bounded choice, computer networks, full abstraction, H infinity control, input-output relation, Computer networks, data structures, semantic models, Context modeling, nondeterministic dataflow networks]
Probabilistic communication complexity of Boolean relations
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors demonstrate an exponential gap between deterministic and probabilistic complexity and between the probabilistic complexity of monotonic and nonmonotonic relations. They then prove, as their main result, an Omega ((log n)/sup 2/) bound on the probabilistic communication complexity of monotonic st-connectivity. From this they deduce that every nonmonotonic NC/sup 1/ circuit for st-connectivity requires a constant fraction of negated input variables.<<ETX>>
[Boolean relations, Eyes, Input variables, Circuits, Search problems, Complexity theory, Boolean algebra, communication complexity, Distributed computing, Radio access networks, Communication standards, Boolean functions, Wires, probabilistic complexity, st-connectivity, computational complexity]
Towards optimal distributed consensus
30th Annual Symposium on Foundations of Computer Science
None
1989
In a distributed consensus protocol all processors (of which t may be faulty) are given (binary) initial values; after exchanging messages all correct processors must agree on one of them. The quality of a protocol is measured here using as parameters the total number of processors n, number of rounds of message exchange r, and maximal message length m, with optima, respectively, of 3t+1, t+1, and 1. Although no known protocol is optimal in all these three aspects simultaneously, the protocols that take further steps in this direction are presented. The first protocol has n>4t, r=t+1, and polynomial message size. The second protocol has n>3t, r=3t+3, and m=2, and it is asymptotically optimal in all three quality parameters while using the optimal number of processors. Using these protocols as building blocks, families of protocols with intermediate quality parameters, offering better tradeoffs than previous results, are obtained. All the protocols work in polynomial time and have succinct descriptions.<<ETX>>
[optimal distributed consensus, Protocols, message exchange, Process control, intermediate quality parameters, Length measurement, distributed processing, Computer science, Fault tolerance, distributed consensus protocol, protocol, Voting, Distributed control, Polynomials, Computer networks, polynomial time, protocols, Contracts]
Simulating (log/sup c/n)-wise independence in NC
30th Annual Symposium on Foundations of Computer Science
None
1989
A general framework is developed for removing randomness from randomized NC algorithms whose analysis uses only polylogarithmic independence. Previously, no techniques were known to determinize those RNC algorithms depending on more than constant independence. One application of the techniques is an NC algorithm for the set discrepancy problem, which can be used to obtain many other NC algorithms, including a better NC edge-coloring algorithm. As another application an NC algorithm for the hypergraph coloring problem is provided.<<ETX>>
[Out of order, polylogarithmic independence, Lattices, simulation, randomness removal, (log/sup c/n)-wise independence, RNC algorithms, graph colouring, set discrepancy problem, randomized NC algorithms, NC edge-coloring algorithm, Polynomials, Random variables, computational complexity, hypergraph coloring problem]
Recursive *-tree parallel data-structure
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors introduce a fundamentally novel parallel data structure, called recursive *-tree (star tree). For its definition, they use a generalization of this * functional and apply it to functions other than log. Using recursion in the spirit of the inverse-Akermann function, they derive recursive *-trees. The recursive *-tree data structure leads to a new design paradigm for parallel algorithms. The paradigm allows for unusually fast parallel computations that need only constant time, using an optimal number of processors under the assumption that a very small number of processors can write simultaneously, each into different bits of the same word.<<ETX>>
[Algorithm design and analysis, parallel algorithms, Computational modeling, parallel data structure, recursive *-tree, star tree, Phase change random access memory, Complexity theory, Parallel algorithms, Concurrent computing, Computer science, inverse-Akermann function, Computational geometry, Binary trees, data structures, Pattern matching]
Efficient NC algorithms for set cover with applications to learning and geometry
30th Annual Symposium on Foundations of Computer Science
None
1989
NC approximation algorithms are given for the unweighted and weighted set cover problems. The algorithms use a linear number of processors and give a cover that has at most log n times the optimal size/weight, thus matching the performance of the best sequential algorithms. The set cover algorithm is applied to learning theory, providing an NC algorithm for learning the concept class obtained by taking the closure under finite union or finite intersection of any concept class of finite VC dimension which has an NC hypothesis finder. In addition, a linear-processor NC algorithm is given for a variant of the set cover problem and used to obtain NC algorithms for several problems in computational geometry.<<ETX>>
[Greedy algorithms, Laboratories, graph theory, computational geometry, set cover algorithm, set theory, finite union, Parallel algorithms, finite VC dimension, efficient NC algorithms, linear-processor NC algorithm, algorithm theory, Polynomials, closure, approximation theory, Application software, Uninterruptible power systems, weighted set cover problems, learning systems, finite intersection, Computer science, Bridges, Computational geometry, NC hypothesis finder, unweighted set cover problems, concept class, Approximation algorithms, NC approximation algorithms, learning theory]
Subquadratic simulations of circuits by branching programs
30th Annual Symposium on Foundations of Computer Science
None
1989
Boolean circuits and their simulations by bounded-width branching programs are considered. It is shown that every NC/sup 1/ circuit of size s can be simulated by a constant-width branching program of length s/sup 1.811. . ./. Some related group-theoretic results are presented.<<ETX>>
[simulations of circuits, subquadratic simulations, Circuit simulation, Computational modeling, Computer simulation, Binary decision diagrams, Encoding, Boolean algebra, group-theoretic results, Computer science, Upper bound, switching circuits, branching programs, Hardware, Polynomials, bounded-width, Boolean circuits, Contracts]
Privacy and communication complexity
30th Annual Symposium on Foundations of Computer Science
None
1989
Each of two parties P/sub 1/ and P/sub 2/ holds an n-bit input, x and y, respectively. They wish to compute privately the value of f(x,y). Two questions are considered: (1) Which functions can be privately computed? (2) What is the communication complexity of protocols that privately compute a function f (in the case in which such protocols exist)? A complete combinatorial characterization of privately computable functions is given. This characterization is used to derive tight bounds on the rounds complexity of any privately computable function and to design optimal private protocols that compute these functions. It is shown that for every 1<or=g(n)<or=2*2/sup n/ there are functions that can be privately computed with g(n) rounds of communication, but not with g(n)-1 rounds of communication.<<ETX>>
[Protocols, Costs, computable functions, privacy, Complexity theory, communication complexity, Privacy, Boolean functions, optimal private protocols, Polynomials, protocols, combinatorial characterization, computational complexity]
The probabilistic method yields deterministic parallel algorithms
30th Annual Symposium on Foundations of Computer Science
None
1989
A method is provided for converting randomized parallel algorithms into deterministic parallel algorithms. The approach is based on a parallel implementation of the method of conditional probabilities. Results obtained by applying the method to the set balancing problem, lattice approximation, edge-coloring graphs, random sampling, and combinatorial constructions are presented. The general form in which the method of conditional probabilities is applied sequentially is described. The reason why this form does not lend itself to parallelization are discussed. The general form of the case for which the method of conditional probabilities can be applied in the parallel context is given.<<ETX>>
[Lattices, Ink, random sampling, set theory, Parallel algorithms, graph colouring, conditional probabilities, lattice approximation, combinatorial constructions, deterministic parallel algorithms, Contracts, approximation theory, parallel algorithms, randomized parallel algorithms, Cloning, probability, Color, set balancing problem, Geometry, Computer science, Approximation algorithms, Sampling methods, edge-coloring graphs, probabilistic method, computational complexity]
Computational complexity of roots of real functions
30th Annual Symposium on Foundations of Computer Science
None
1989
An attempt is made to give a more accurate classification of the computational complexity of roots of real functions. Attention is focused on the simplest types of functions, namely, one-to-one and k-to-one functions, and the complexity of their roots is characterized in terms of relations between discrete complexity classes, such as LOGSPACE, P, UP, and NP.<<ETX>>
[Algorithm design and analysis, NP, Computational modeling, LOGSPACE, one-to-one, Complexity theory, k-to-one functions, classification, Computational complexity, Computer science, P, roots of real functions, Polynomials, discrete complexity classes, UP, computational complexity]
Constant depth circuits, Fourier transform, and learnability
30th Annual Symposium on Foundations of Computer Science
None
1989
Boolean functions in AC/sup O/ are studied using the harmonic analysis of the cube. The main result is that an AC/sup O/ Boolean function has almost all of its power spectrum on the low-order coefficients. This result implies the following properties of functions in AC/sup O/: functions in AC/sup O/ have low average sensitivity; they can be approximated well be a real polynomial of low degree; they cannot be pseudorandom function generators and their correlation with any polylog-wide independent probability distribution is small. An O(n/sup polylog(/ /sup sup)/ /sup (n)/)-time algorithm for learning functions in AC/sup O/ is obtained. The algorithm observed the behavior of an AC/sup O/ function on O(n/sup polylog/ /sup (n)/) randomly chosen inputs and derives a good approximation for the Fourier transform of the function. This allows it to predict with high probability the value of the function on other randomly chosen inputs.<<ETX>>
[Fourier transforms, Laboratories, Harmonic analysis, Fourier transform, learnability, Application software, learning systems, Computer science, Boolean functions, Bridge circuits, Approximation algorithms, Signal generators, Polynomials]
Dispersers, deterministic amplification, and weak random sources
30th Annual Symposium on Foundations of Computer Science
None
1989
The use of highly expanding bipartite multigraphs (called dispersers) to reduce greatly the error of probabilistic algorithms at the cost of few additional random bits is treated. Explicit constructions of such graphs are generalized and used to obtain the following results: (1) The error probability of any RP (BPP) algorithm can be made exponentially small at the cost of only a constant factor increase in the number of random bits. (2) RP (BPP) algorithms with some weak bit fixing sources are simulated.<<ETX>>
[deterministic amplification, bipartite multigraphs, Costs, Error probability, Terminology, Computational modeling, graph theory, error probability, weak bit fixing sources, Entropy, Graph theory, Probability distribution, RP algorithm, BPP algorithm, probabilistic algorithms, weak random sources, Sampling methods, Robustness, dispersers, Testing, computational complexity]
On the complexity of fixed parameter problems
30th Annual Symposium on Foundations of Computer Science
None
1989
The authors address the question of why some fixed-parameter problem families solvable in polynomial time seem to be harder than others with respect to fixed-parameter tractability: whether there is a constant alpha such that all problems in the family are solvable in time O(n/sup alpha /). The question is modeled by considering a class of polynomially indexed relations. The main results show that (1) this setting supports notions of completeness that can be used to explain the apparent hardness of certain problems with respect to fixed-parameter tractability, and (2) some natural problems are complete.<<ETX>>
[complexity, polynomials, fixed parameter problems, fixed-parameter tractability, polynomially indexed relations, completeness, Computer science, Councils, Feedback, Polynomials, polynomial time, Contracts, computational complexity]
A note on the power of threshold circuits
30th Annual Symposium on Foundations of Computer Science
None
1989
The author presents a very simple proof of the fact that any language accepted by polynomial-size depth-k unbounded-fan-in circuits of AND and OR gates is accepted by depth-three threshold circuits of size n raised to the power O(log/sup k/n). The proof uses much of the intuition of S. Toda's result that the polynomial hierarchy is contained in P/sup Hash P/ (30th Ann. Symp. Foundations Comput. Sci., p.514-519, 1989).<<ETX>>
[Out of order, depth-three threshold circuits, Circuit simulation, Computational modeling, polynomial hierarchy, AND gates, threshold logic, threshold circuits, switching theory, Computer science, Polynomials, OR gates, unbounded-fan-in circuits]
On universal classes of fast high performance hash functions, their time-space tradeoff, and their applications
30th Annual Symposium on Foundations of Computer Science
None
1989
A mechanism is provided for constructing log-n-wise-independent hash functions that can be evaluated in O(1) time. A probabilistic argument shows that for fixed epsilon <1, a table of n/sup epsilon / random words can be accessed by a small O(1)-time program to compute one important family of hash functions. An explicit algorithm for such a family, which achieves comparable performance for all practical purposes, is also given. A lower bound shows that such a program must take Omega (k/ epsilon ) time, and a probabilistic arguments shows that programs can run in O(k/sup 2// epsilon /sup 2/) time. An immediate consequence of these constructions is that double hashing using these universal functions has (constant factor) optimal performance in time, for suitably moderate loads. Another consequence is that a T-time PRAM algorithm for n log n processors (and n/sup k/ memory) can be emulated on an n-processor machine interconnected by an n*log n Omega network with a multiplicative penalty for total work that, with high probability, is only O(1).<<ETX>>
[Omega network, hash functions, Delay, explicit algorithm, Emulation, probabilistic argument, Cost function, Polynomials, time-space tradeoff, Probes, universal classes, random words, double hashing, probability, Routing, Phase change random access memory, lower bound, Application software, Pipeline processing, Computer science, T-time PRAM algorithm, multiplicative penalty, file organisation, universal functions, computational complexity]
Structure in locally optimal solutions
30th Annual Symposium on Foundations of Computer Science
None
1989
A class of local search problems, PLS (polynomial-time local search), as defined by D.S. Johnson et al. (see J. Comput. Syst. Sci., vol.37, no.1, p.79-100 (1988)) is considered. PLS captures much of the structure of NP problems at the level of their feasible solutions and neighborhoods. It is first shown that CNF (conjunctive normal form) satisfiability is PLS-complete, even with simultaneously bounded size clauses and bounded number of occurrences of variables. This result is used to show that traveling salesman under the k-opt neighborhood is also PLS-complete. It is argued that PLS-completeness is the normal behavior of NP-complete problems.<<ETX>>
[local search problems, Circuits, Search problems, NP problems, Partitioning algorithms, NP-complete problem, locally optimal solution structure, polynomial-time local search, conjunctive normal form, Computer science, PLS, traveling salesman, Cost function, Polynomials, search problems, computational complexity]
The strength of weak learnability
30th Annual Symposium on Foundations of Computer Science
None
1989
The problem of improving the accuracy of a hypothesis output by a learning algorithm in the distribution-free learning model is considered. A concept class is learnable (or strongly learnable) if, given access to a source of examples from the unknown concept, the learner with high probability is able to output a hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce a hypothesis that forms only slightly better than random guessing. It is shown that these two notions of learnability are equivalent. An explicit method is described for directly converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences.<<ETX>>
[learning algorithm, equivalent, Filtering, unknown concept, Laboratories, probability, weak learnability, distribution-free learning model, Boosting, learning systems, Computer science, Upper bound, Boolean functions, concept class, Polynomials, equivalence classes, computational complexity, source of examples]
No better ways to generate hard NP instances than picking uniformly at random
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Distributed NP (DNP) problems are ones supplied with probability distributions of instances. It is shown that every DNP problem complete for P-time computable distributions is also complete for all distributions that can be sampled. This result makes the concept of average-case NP completeness robust and the question of the average-case complexity of complete DNP problems a natural alternative to P=?NP. Similar techniques yield a connection between cryptography and learning theory.
[distributed NP problems, Probability distribution, NP completeness, NP-complete problem, Matrix decomposition, Distributed computing, Sun, Computer science, DNP problem, average-case complexity, Extrapolation, P-time computable distributions, hard NP, probability distributions, Polynomials, Cryptography, computational complexity]
The mixing rate of Markov chains, an isoperimetric inequality, and computing the volume
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A. Sinclair and M. Jerrum (1988) derived a bound on the mixing rate of time-reversible Markov chains in terms of their conductance. The authors generalize this result by not assuming time reversibility and using a weaker notion of conductance. They prove an isoperimetric inequality for subsets of a convex body. These results are combined to simplify an algorithm of M. Dyer et al. (1989) for approximating the volume of a convex body and to improve running-time bounds.<<ETX>>
[Algorithm design and analysis, Law, Lattices, computational geometry, Markov chains, time-reversible, conductance, mixing rate, running-time, Markov processes, isoperimetric inequality, Polynomials, convex body, Legal factors, Testing]
Private computations over the integers
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The possibility of private distributed computations of n-argument functions defined over the integers is considered. A function f is t-private if there exists a protocol for computing f so that no coalition of <or=t participants can infer any additional information from the execution of the protocol. It is shown that certain results for private computation over finite domains cannot be extended to infinite domains. The possibility of privately computing f is shown to be closely related to the communication complexity of f. A complete characterization of t-private Boolean functions over countable domains is given.<<ETX>>
[Protocols, t-private Boolean functions, Complexity theory, Distributed computing, finite domains, Computer science, Privacy, Boolean functions, security of data, Cryptography, protocols, private distributed computations, countable domains]
Decision problems for propositional linear logic
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is shown that, unlike most other propositional (quantifier-free) logics, full propositional linear logic is undecidable. Further, it is provided that without the model storage operator, which indicates unboundedness of resources, the decision problem becomes PSPACE-complete. Also established are membership in NP for the multiplicative fragment, NP-completeness for the multiplicative fragment extended with unrestricted weakening, and undecidability for certain fragments of noncommutative propositional linear logic.<<ETX>>
[Linear systems, decision problem, Scholarships, Laboratories, multiplicative fragment, unrestricted weakening, noncommutative propositional linear logic, propositional linear logic, PSPACE-complete, Calculus, Mathematics, undecidability, Computer science, formal logic, decidability, NP-completeness, Logic, Resource management]
Learning conjunctions of Horn clauses
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
An algorithm for learning the class of Boolean formulas that are expressible as conjunctions of Horn clauses is presented. (A Horn clause is a disjunction of literals, all but at most one of which is a negated variable). The algorithm uses equivalence queries and membership queries to produce a formula that is logically equivalent to the unknown formula to be learned. The amount of time used by the algorithm is polynomial in the number of variables and the number of clauses in the unknown formula.<<ETX>>
[learning algorithm, Protocols, Horn clause conjunctions, negated variable, clauses, equivalence queries, learning systems, Computer science, membership queries, Boolean formulas, Boolean functions, unknown formula, Horn sentence, Polynomials, Inference algorithms, Marine vehicles, polynomial time complexity, disjunction of literals]
A time-space tradeoff for Boolean matrix multiplication
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A time-space tradeoff is established in the branching program model for the problem of computing the product of two n*n matrices over a certain semiring. It is assumed that each element of each n*n input matrix is chosen independently to be 1 with probability n/sup -1/2/ and to be 0 with probability 1-n/sup -1/2/. Letting S and T denote expected space and time of a deterministic algorithm, the tradeoff is ST= Omega (n/sup 3.5/) for T<c/sub 1/n/sup 2.5 /and ST Omega (n/sup 3/) for T<c/sub 2/n/sup 2.5/, where c/sub 1/,/sub /c/sub 2/ >0. The lower bounds are matched to within a logarithmic factor by upper bounds in the branching program model. Thus, the tradeoff possesses a sharp break at T= Theta (n/sup 2.5/). These expected case lower bounds are also the best known lower bounds for the worst case.<<ETX>>
[Computational modeling, Binary decision diagrams, semiring, upper bounds, deterministic algorithm, Equations, lower bounds, Sorting, matrix algebra, Computer science, Boolean functions, Polynomials, time-space tradeoff, Boolean matrix multiplication, branching program model]
Trans-dichotomous algorithms for minimum spanning trees and shortest paths
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The fusion tree method is extended to develop a linear-time algorithm for the minimum spanning tree problem and an O(m+n log n/log log n) implementation of Dijkstra's shortest-path algorithm for a graph with n vertices and m edges. The shortest-path algorithm surpasses information-theoretic limitations. The extension of the fusion tree method involves the development of a new data structure, the atomic heap. The atomic heap accommodates heap (priority queue) operations in constant amortized time under suitable polylog restrictions on the heap size. The linear-time minimum spanning tree algorithm results from a direct application of the atomic heap. To obtain the shortest path algorithm, the atomic heap is used as a building block to construct a new data structure, the AF-heap, which has no size restrictions and surpasses information theoretic limitations. The AF-heap belongs to the Fibonacci heap family.<<ETX>>
[Algorithm design and analysis, Performance evaluation, Costs, heap size, data structure, AF-heap, atomic heap, linear-time algorithm, Fibonacci heap, Concurrent computing, Tree graphs, algorithm theory, information-theoretic limitations, data structures, trans-dichotomous algorithms, Tree data structures, Computational modeling, minimum spanning trees, trees (mathematics), Data structures, fusion tree method, amortized time, shortest paths, shortest-path algorithm, Inference algorithms, Arithmetic]
Efficiently inverting bijections given by straight line programs
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Let K be any field, and let F: K/sup n/ to K/sup n/ be a bijection with the property that both F and F/sup -1/ are computable using only arithmetic operations from K. Motivated by cryptographic considerations, the authors concern themselves with the relationship between the arithmetic complexity of F and the arithmetic complexity of F/sup -1/. They give strong relations between the complexity of F and F/sup -1/ when F is an automorphism in the sense of algebraic geometry (i.e. a formal bijection defined by n polynomials in n variables with a formal inverse of the same form). These constitute all such bijections in the case in which K is infinite. The authors show that at polynomially bounded degree, if an automorphism F has a polynomial-size arithmetic circuit, then F/sup -1/ has a polynomial-size arithmetic circuit. Furthermore, this result is uniform in the sense that there is an efficient algorithm for finding such a circuit for F/sup -1/, given such a circuit for F. This algorithm can also be used to check whether a circuit defines an automorphism F. If K is the Boolean field GF(2), then a circuit defining a bijection does not necessarily define an automorphism. However, it is shown in this case that, given any K/sup n/ to K/sup n/ bijection, there always exists an automorphism defining that bijection. This is not generally true for an arbitrary finite field.<<ETX>>
[inverting bijections, Circuits, Size measurement, cryptography, straight line programs, automorphism, Galois fields, Computer science, Geometry, arithmetic operations, cryptographic considerations, Digital arithmetic, polynomial-size arithmetic circuit, Polynomials, algebraic geometry, Cryptography, Boolean field]
Interpolation of sparse rational functions without knowing bounds on exponents
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The authors present the first algorithm for the (black box) interpolation of t-sparse, n-variate, rational functions without knowing bounds on exponents of their sparse representation, with the number of queries independent of exponents. In fact, the algorithm uses O(nt/sup t/) queries to the black box, and it can be implemented for a fixed t in a polynomially bounded storage (or polynomial parallel time).<<ETX>>
[polynomially bounded storage, sparse rational functions, Mathematics, queries, Computer science, Concurrent computing, Interpolation, interpolation, function approximation, polynomial parallel time, Polynomials, black box, computational complexity, sparse representation]
Permuting
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The fundamental problem of permuting the elements of an array according to some given permutation is addressed. The goal is to perform the permutation quickly using only a polylogarithmic number of bits of extra storage. The main result is an O(n log n)-time, O(log/sup 2/n)-space worst case method. A simpler method is presented for the case in which both the permutation and its inverse can be computed at (amortized) unit cost. This algorithm requires O(n log n) time and O(log n) bits in the worst case. These results are extended to the situation in which a power of the permutation is to be applied. A linear time, O(log n)-bit method is presented for the special case in which the data values are all distinct and are either initially in sorted order or will be when permuted.<<ETX>>
[Tree data structures, Costs, programming theory, permuting, Binary search trees, Organizing, Sorting, Computer science, Councils, array elements, polylogarithmic number of bits, permutation, linear time]
Constructing generalized universal traversing sequences of polynomial size for graphs with small diameter
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A generalized version of universal traversing sequences is constructed. The generalization preserves the features of the universal traversing sequences that make them attractive for applications to derandomizations and space-bounded computation. For every n, there is constructed a sequence that is used by a finite automaton with O(1) states in order to traverse all the n-vertex labeled undirected graphs. The automaton walks on the graph; when it is at a certain vertex, it uses the edge labels and the sequence in order to decide which edge to follow. When it is walking on an edge, the automaton can see the edge labeling. As a corollary, polynomial-size generalized universal traversing sequences constructible in DSpace(log n) are obtained for certain classes of graphs.<<ETX>>
[Legged locomotion, polynomial-size, generalized universal traversing sequences, finite automata, edge labels, graph theory, space-bounded computation, Information retrieval, Mathematics, Graph theory, History, labeled undirected graphs, finite automaton, Automata, polynomial size for graphs, Hypercubes, Polynomials, derandomizations, Labeling, Testing, small diameter]
Deciding properties of nonregular programs
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The problem of deciding the validity of formulas in extensions of propositional dynamic logic (PDL) is considered. The extensions are obtained by adding programs defined by nonregular languages. In the past, a number of very simple languages were shown to render this problem highly undecidable, whereas other very similar-looking languages were shown to retain decidability. Understanding this rather strange phenomenon and generalizing the isolated extensions have remained elusive. The authors provide decision procedures for two wide classes of extensions, thus shedding light on the general problem. The proofs are novel, in that they explicitly consider the machines that accept the languages, in this case special classes of PDAs and stack automata. It is shown that the emptiness problem for stack automata on infinite trees is decidable, a result of independent interest, and the result is combined with the construction of certain tree models for the corresponding formulas.<<ETX>>
[nonregular languages, formal languages, finite automata, PDAs, infinite trees, propositional dynamic logic, stack automata, nonregular programs, Calculus, Mathematics, Computer science, formal logic, decidability, Automata, decision procedures, Page description languages, validity, Logic, Personal digital assistants]
Efficient parallel algorithms for tree-decomposition and related problems
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
An efficient parallel algorithm for the tree-decomposition problem for fixed width w is presented. The algorithm runs in time O(log/sup 3/ n) and uses O(n) processors on a concurrent-read, concurrent-write parallel random access machine (CRCW PRAM). This result can be used to construct efficient parallel algorithms for three important classes of problems: MS (monadic second-order) properties, linear EMS (extended monadic second-order) extremum problems, and enumeration problems for MS properties, for graphs of tree width at most w. The sequential time complexity of the tree-composition problem for fixed w is improved, and some implications for this improvement are stated.<<ETX>>
[parallel algorithms, CRCW PRAM, tree width, Computational modeling, enumeration problems, trees (mathematics), Medical services, tree-decomposition, Phase change random access memory, Parallel algorithms, Stress, Concurrent computing, concurrent-read, graphs, Tree graphs, Numerical analysis, sequential time complexity, linear extended monadic second order extremum problems, concurrent-write parallel random access machine, Polynomials, Logic, monadic second order properties, computational complexity]
Robust separations in inductive inference
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Results in recursion-theoretic inductive inference have been criticized as depending on unrealistic self-referential examples. J.M. Barzdin (1974) proposed a way of ruling out such examples and conjectured that one of the earliest results of inductive inference theory would fall if his method were used. The author refutes Barzdin's conjecture and proposes a new line of research examining robust separations which are defined using a strengthening of Barzdin's original idea. Preliminary results are presented, and the most important open problem is stated as a conjecture. The extension of this work from function learning to formal language learning is discussed.<<ETX>>
[Gold, Formal languages, Machine learning, Robustness, Inference algorithms, inference mechanisms, function learning, learning systems, recursion-theoretic inductive inference, formal language]
On interpolation by analytic functions with special properties and some weak lower bounds on the size of circuits with symmetric gates
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The author investigates the question of whether or not a specific Boolean function in n variables can be interpolated by an analytic function in the same variables whose partial derivatives of all orders span a subspace of low dimension in the space of analytic functions. The upper and lower bounds for this dimension yield some weak circuit lower bounds. For a particular function, an Omega (n/log n)-size lower bound is obtained for its computation by a circuit whose gates are symmetric. For the same function an Omega (n) lower bound is obtained for the circuit with mod/sub k/ gates.<<ETX>>
[analytic functions, Input variables, Circuits, Boolean function, Complexity theory, Machinery, Computer science, Interpolation, weak circuit lower bounds, Boolean functions, interpolation, combinatorial switching, symmetric gates, Polynomials, partial derivatives]
Augmenting graphs to meet edge-connectivity requirements
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The problem of determining the minimum number gamma of edges to be added to a graph G so that in the resulting graph the edge-connectivity between every pair (u,v) of nodes is at least a prescribed value r(u,v) is treated. A min-max formula for gamma is derived, and a polynomial-time algorithm for computing gamma is described. The directed counterpart of the problem is also solved for the case in which r(u,v)=k>or=1. The approach used makes it possible to solve a degree-constrained version of the problem. The minimum-cost augmentation problem can also be solved in polynomial time provided that the edge costs arise from node costs.<<ETX>>
[minimum number, node costs, Operations research, graph theory, edge costs, time complexity, Mathematics, Sun, Computer science, edge-connectivity, algorithm theory, min-max formula, Cost function, Polynomials, polynomial time, minimisation, polynomial-time algorithm, minimum-cost augmentation problem, computational complexity]
Hidden surface removal for axis-parallel polyhedra
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
An efficient, output-sensitive method for computing the visibility map of a set of axis-parallel polyhedra (i.e. polyhedra with their faces and edges parallel to the coordinate axes) as seen from a given viewpoint is introduced. For nonintersecting polyhedra with n edges in total, the algorithm runs in time O((n+k)log n), where k is the complexity of the visibility map. The method can handle cyclic overlap of the polyhedra and perspective views without any problem. For c-oriented polyhedra (with faces and edges in c orientations, for some constant c) the method can be extended to run in the same time bound. The method can be extended even further to deal with intersecting polyhedra with only a slight increase in the time bound.<<ETX>>
[Concurrent computing, complexity, cyclic overlap, computer graphics, visibility map, Layout, Computer graphics, hidden surface removal, computational geometry, axis-parallel polyhedra, c-oriented polyhedra, Pixel]
Faster tree pattern matching
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Recently, R. Kosaraju (Proc. 30th IEEE Symp. on Foundations of Computer Science, 1989, p.178-83) gave an O(nm/sup 0.75/ polylog(m))-step algorithm for tree pattern matching. The authors improve this result by designing a simple O(n square root m polylog (m)) algorithm.<<ETX>>
[Algorithm design and analysis, tail period pairs, Convolution, trees (mathematics), time complexity, Mathematics, Partitioning algorithms, ordered labelled trees, tree pattern matching, Pattern matching, computational complexity, algorithm]
Security preserving amplification of hardness
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The task of transforming a weak one-way function (which may be easily inverted on all but a polynomial fraction of the range) into a strong one-way function (which can be easily inverted only on a negligible function of the range) is considered. The previously known transformation does not preserve the security (i.e. the running time of the inverting algorithm) within any polynomial. Its resulting function, F(x), applies the weak one-way function to many small (of length mod x mod /sup theta /, theta <1) pieces of the input. Consequently, the function can be inverted for reasonable input lengths by exhaustive search. Random walks on constructive expanders are used to transform any regular (e.g. one-to-one) weak one-way function into a strong one, while preserving security. The resulting function, F(x), applies the weak one-way f to strings of length Theta ( mod x mod ). The security-preserving constructions yield efficient pseudorandom generators and signatures based on any regular one-way function.<<ETX>>
[pseudorandom generators, Sun, weak one-way function, signatures, Cryptographic protocols, inverting algorithm, Computer science, hardness, polynomial fraction, security of data, regular one-way function, security preserving amplification, Polynomials, Cryptography, National security]
Inferring evolutionary history from DNA sequences
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Two related problems are considered. The first is determining whether it is possible to triangulate a vertex-colored graph without introducing edges between vertices of the same color. This is related to a fundamental problem for geneticists, that of using character state information to construct evolutionary trees. The polynomial equivalence of these problems is demonstrated. An important subproblem arises when the characters are based on DNA sequences. Such characters assume up to four states. An O(n/sup 2/k) algorithm, where n is the number of species and k is the number of characters, is presented for this case.<<ETX>>
[Sequences, vertex-colored graph, DNA sequences, polynomial equivalence, character state information, Mathematics, Phylogeny, Partitioning algorithms, History, inference mechanisms, graph colouring, Computer science, Tree graphs, DNA, evolutionary trees, Polynomials, Inference algorithms]
On the complexity of learning from counterexamples and membership queries
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is shown that for any concept class C the number of equivalence and membership queries that are needed to learn C is bounded from below by Omega (VC-dimension(C)). Furthermore, it is shown that the required number of equivalence and membership queries is also bounded from below by Omega (LC-ARB(C)/log(1+LC-ARB(C))), where LC-ARB(C) is the required number of steps in a different model where no membership queries but equivalence queries with arbitrary subsets of the domain are permitted. These two relationships are the only relationships between the learning complexities of the common online learning models and the related combinatorial parameters that have remained open. As an application of the first lower bound, the number of equivalence and membership queries that are needed to learn monomials of k out of n variables is determined. Learning algorithms for threshold gates that are based on equivalence queries are examined. It is shown that a threshold gate can learn not only concepts but also nondecreasing functions in polynomially many steps.<<ETX>>
[threshold gates, monomials, equivalence queries, online learning models, Mathematics, set theory, Application specific integrated circuits, membership queries, learning from counterexamples, combinatorial parameters, Statistical distributions, learning complexities, nondecreasing functions, Polynomials, Marine vehicles, Learning automata, Doped fiber amplifiers, lower bound, domain subsets, learning systems, Computer science, logic gates, concept class, Neural networks, computational complexity]
Time-space tradeoffs for undirected graph traversal
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Time-space tradeoffs for traversing undirected graphs are proved. One of these tradeoffs is a quadratic lower bound on a deterministic model that closely matches the probabilistic upper bound of A.Z. Broder et al. (1989). The models used are variants of S.A. Cook and C.W. Rackoff's (1980) jumping automata for graphs. Some open problems are stated.<<ETX>>
[deterministic model, Computational modeling, automata theory, graph theory, Inspection, jumping automata, Computational complexity, undirected graph traversal, Computer science, Upper bound, Turing machines, Councils, Automata, probabilistic upper bound, Polynomials, time-space tradeoff, Contracts, quadratic lower bound]
Computing with snakes in directed networks of automata
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Directed, strongly connected networks of finite-state automata, of bounded in- and outdegree but unknown topology and unbounded size n, are considered. Protocols that are quadratic or linear in n and accomplish the following tasks are provided: waking up and reporting when done, constructing smart spanning trees out from the root and in to the root, conducting breadth-first and depth-first searches, sending a message from the endpoint of a (directed) edge to its startpoint, running a slow clock, and solving the firing squad synchronization problem. The protocols are highly parallel and entail the use of sequences of signals called 'snakes''. All the tasks are accomplished in less time than is possible with any previously known techniques.<<ETX>>
[Protocols, finite automata, smart spanning trees, finite-state automata, Mathematics, Ambient intelligence, Synchronization, directed networks, depth-first, Computer science, Intelligent networks, Network topology, Automata, breadth-first, Computer networks, waking up, protocols, reporting, snakes', Clocks, firing squad synchronization]
Faster circuits and shorter formulae for multiple addition, multiplication and symmetric Boolean functions
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A general theory is developed for constructing the shallowest possible circuits and the shortest possible formulas for the carry-save addition of n numbers using any given basic addition unit. More precisely, it is shown that if BA is a basic addition unit with occurrence matrix N, then the shortest multiple carry-save addition formulas that could be obtained by composing BA units are of size n/sup 1/p+o(1)/, where p is the unique real number for which the L/sub p/ norm of the matrix N equals 1. An analogous result connects the delay matrix M of the basic addition unit BA and the minimal q such that multiple carry-save addition circuits of depth (q+o(1)) log n could be constructed by combining BA units. On the basis of these optimal constructions of multiple carry-save adders, the shallowest known multiplication circuits are constructed.<<ETX>>
[multiplying circuits, Circuits, Very large scale integration, shortest multiple carry-save addition formulas, Delay, Concurrent computing, Boolean functions, adders, digital arithmetic, occurrence matrix, multiplication, shallowest possible circuits, Adders, shortest possible formulas, multiple carry-save adders, multiplication circuits, Computational modeling, multiple addition, symmetric Boolean functions, matrix algebra, Computer science, delay matrix, combinatorial switching, carry-save addition, Context modeling, Arithmetic]
A tree-partitioning technique with applications to expression evaluation and term matching
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A tree-partitioning technique is proposed and applied to expression evaluation and term matching. It was shown recently that the problem of evaluating an arithmetic expression is in NC/sup 1/, and an O(log N)-depth, O(N/sup 2/ log N)-size circuit for this problem was described. The size is reduced to O(N log/sup k/ N) while O(log N) depth is maintained. An O(log N)-time, O(N)-processor CREW (concurrent read, exclusive write) algorithm for term matching, which improves the previous O(log N)-time, N/sup 2/-processor algorithm, is also presented.<<ETX>>
[Algorithm design and analysis, parallel algorithms, term matching, tree-partitioning technique, trees (mathematics), Phase change random access memory, NC/sup 1/ complexity class, Partitioning algorithms, Combinational circuits, Application software, CREW algorithm, Petroleum, Computer science, expression evaluation, Binary trees, arithmetic expression, Arithmetic, computational complexity]
Communication complexity of algebraic computation
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The authors consider a situation in which two processors P/sub 1/ and P/sub 2/ are to evaluate one or more functions f/sub 1/, . . ., f/sub s/ of two vector variables x and y, under the assumption that processor P/sub 1/ (respectively, P/sub 2/) has access only to the value of x (respectively, y) and the functional form of f/sub 1/, . . ., f/sub s/. They consider a continuous model of communication whereby real-valued messages are transmitted, and they study the minimum number of messages required for the desired computation. Tight lower bounds are established for the following three problems: (1) each f/sub i/ is a rational function and only one-way communication is allowed. (2) The variables x and y are matrices and the processors wish to solve the linear system (x+y)z=b for the unknown z. (3) The processors wish to evaluate a particular root of the polynomial equation Sigma (x/sub i/+y/sub i/)z/sup i/=0, where the sum is from i=0 to n-1.<<ETX>>
[Algorithm design and analysis, Linear systems, Protocols, Operations research, Computational modeling, real-valued messages, Complexity theory, communication complexity, model of communication, Equations, lower bounds, symbol manipulation, Polynomials, multiprocessors, Distributed algorithms, Contracts, computational complexity, algebraic computation]
A characterization of Hash P by arithmetic straight line programs
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Hash P functions are characterized by certain straight-line programs of multivariate polynomials. The power of this characterization is illustrated by a number of consequences. These include a somewhat simplified proof of S. Toda's (1989) theorem that PH contained in P/sup Hash P/, as well as an infinite class of potentially inequivalent checkable functions.<<ETX>>
[programming theory, Input variables, polynomials, Hash P functions, inequivalent checkable functions, multivariate polynomials, Wire, Computer aided instruction, infinite class, Polynomials, arithmetic straight line programs, Arithmetic, computational complexity]
IP=PSPACE (interactive proof=polynomial space)
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is proved that, when both randomization and interaction are allowed, the proofs that can be verified in polynomial time are exactly those proofs that can be generated with polynomial space. The interactive proofs introduced use only public coins, are accepted with probability one when the prover is honest, require only logarithmic workspace when the verifier is given a two-way access to his or her random tape, and by the use of known techniques can be turned into zero-knowledge proofs under the sole assumption that one-way functions exist.<<ETX>>
[formal languages, polynomial space, two-way access, IP, proof verification, honest provers, Mathematics, random tape, public coins, PSPACE, logarithmic workspace, Boolean functions, acceptance probability, randomization, interactive proofs, interactive systems, interaction, Polynomials, polynomial time, theorem proving, protocols, one-way functions, computational complexity, zero-knowledge proofs]
On threshold circuits for parity
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Motivated by, the problem of understanding the limitations of neural networks for representing Boolean functions, the authors consider size-depth tradeoffs for threshold circuits that compute the parity function. They give an almost optimal lower bound on the number of edges of any depth-2 threshold circuit that computes the parity function with polynomially bounded weights. The main technique used in the proof, which is based on the theory of rational approximation, appears to be a potentially useful technique for the analysis of such networks. It is conjectured that there are no linear size, bounded-depth threshold circuits for computing parity.<<ETX>>
[Circuits, parity, neural networks, almost optimal lower bound, threshold logic, Approximation methods, threshold circuits, Convergence, Computer science, size-depth tradeoffs, Boolean functions, Neural networks, Prototypes, Computer architecture, rational approximation, Polynomials, Computer networks, polynomially bounded weights, neural nets]
New results on dynamic planar point location
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A point location scheme is presented for an n-vertex dynamic planar subdivision whose underlying graph is only required to be connected. The scheme uses O(n) space and yields an O(log/sup 2/n) query time and an O(log n) update time. Insertion (respectively, deletion) of an arbitrary k-edge chain inside a region can be performed in O(k log(n+k)) (respectively, O(k log n)) time. The scheme is then extended to speed up the insertion/deletion of a k-edge monotone chain to O(log/sup 2/n log log n+k) time (or O(log n log log n+k) time for an alternative model of input), but at the expense of increasing the other time bounds slightly. All bounds are worst case. Additional results include a generalization to planar subdivisions consisting of algebraic segments of bounded degree and a persistent scheme for planar point location.<<ETX>>
[graph theory, query time, time bounds, algebraic segments, chain insertion, Computer science, n-vertex dynamic planar subdivision, chain deletion, k-edge monotone chain, connected graph, persistent scheme, dynamic planar point location, update time, computational complexity]
ON ACC and threshold circuits
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is proved that any language in ACC can be approximately computed by two-level circuits of size 2 raised to the (log n)/sup k/ power, with a symmetric-function gate at the top and only AND gates on the first level. This implies that any language in ACC can be recognized by depth-3 threshold circuits of that size. This result gives the first nontrivial upper bound on the computing power of ACC circuits.<<ETX>>
[Frequency selective surfaces, ACC, formal languages, Input variables, AND gates, language, threshold logic, Character recognition, threshold circuits, Galois fields, symmetric-function gate, Computer science, Upper bound, combinatorial switching, two-level circuits, computing power, depth-3 threshold circuits, Polynomials, Circuit theory]
A fast algorithm for optimally increasing the edge-connectivity
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
An undirected, unweighted graph G=(V, E with n nodes, m edges, and connectivity lambda ) is considered. Given an input parameter delta , the edge augmentation problem is to find the smallest set of edges to add to G so that its edge-connectivity is increased by delta . A solution to this problem that runs in time O( delta /sup 2/nm+nF(n)), where F(n) is the time to perform one maximum flow on G, is given. The solution gives the optimal augmentation for every delta ', 1<or= delta '<or= delta , in the same time bound. A modification of the solution solves the problem without knowing delta in advance. If delta =1, then the solution is particularly simple, running in O(nm) time, and it is a natural generalization of an algorithm of K. Eswaran and R.E. Tarjan (1976) for the case in which lambda + delta =2. The converse problem (given an input number k, increase the connectivity of G as much as possible by adding at most k edges) is solved in the same time bound. The solution makes extensive use of the structure of particular sets of cuts.<<ETX>>
[Data security, graph theory, time complexity, time bound, Graph theory, edge augmentation problem, input parameter, Computer science, undirected unweighted graph, NP-hard problem, edge-connectivity, optimal augmentation, algorithm theory, Polynomials, computational complexity]
Counting and cutting cycles of lines and rods in space
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A number of rendering algorithms in computer graphics sort three-dimensional objects by depth and assume that there is no cycle that makes the sorting impossible. One way to resolve the problem caused by cycles is to cut the objects into smaller pieces. The problem of estimating how many such cuts are always sufficient is addressed. A few related algorithmic and combinatorial geometry problems are considered.<<ETX>>
[computational geometry, cutting cycles, Partitioning algorithms, Sorting, Computational geometry, computer graphics, combinatorial geometry, Layout, Computer graphics, Binary trees, sorting, rendering algorithms, Rendering (computer graphics), rods, lines, three-dimensional objects, Lifting equipment]
On graph-theoretic lemmata and complexity classes
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Several new complexity classes of search problems that lie between the classes FP and FNP are defined. These classes are contained in the class TFNP of search problems that always have a solution. A problem in each of these new classes is defined in terms of an implicitly given, exponentially large graph, very much like PLS (polynomial local search). The existence of the solution sought is established by means of a simple graph-theoretic lemma with an inefficiently constructive proof. Several class containments and collapses, resulting in the two new classes PDLF contained in PLF are shown; the relation of either class of PLS is open. PLF contains several important problems for which no polynomial-time algorithm is presently known.<<ETX>>
[graph-theoretic lemmata, complexity classes, Circuits, graph theory, Search problems, Linear programming, PDLF, PLF, Computer science, polynomial local search, Turing machines, TFNP, PLS, Ear, class containments, search problems, computational complexity]
A dining philosophers algorithm with polynomial response time
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Presents an efficient distributed online algorithm for scheduling jobs that are created dynamically, subject to resource constraints that require that certain pairs of jobs not run concurrently. The focus is on the response time of the system to each job, i.e. the length of the time interval that starts when the job is created or assigned to a processor and ends at the instant the execution of the job begins. The goal is to provide guarantees on the response time to each job j in terms of the density of arrivals of jobs that conflict with j. The model is completely asynchronous and includes various resource allocation problems that have been studied extensively, including the dining philosophers problem and its generalizations to arbitrary networks. In these versions of the problem, the resource requirements of each new job j determines an upper bound delta /sub j/ on the number of jobs that can exist concurrently in the system and conflict with j. Given such upper bounds, no scheduling algorithm can guarantee a response time better than delta /sub j/ times the maximum execution or message transmission time. A simple algorithm that guarantees response time that is essentially polynomial in delta /sub j/ is presented. It is based on the notion of a distribution queue and has a compact implementation.<<ETX>>
[dining philosophers algorithm, distributed processing, Mathematics, Delay, conflicting jobs, Lapping, resource allocation, dynamic job creation, Polynomials, maximum execution time, Contracts, job scheduling, queueing theory, job arrival density, asynchronous model, message transmission time, polynomial response time, distribution queue, resource constraints, resource requirements, Dynamic scheduling, distributed online algorithm, upper bound, Scheduling algorithm, Computer science, Upper bound, Resource management, concurrent jobs, arbitrary networks, computational complexity]
On the exact complexity of string matching
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The maximal number of character comparisons made by a linear-time string matching algorithm, given a text string of length n and a pattern string of length m over a general alphabet, is investigated. The number is denoted by c(n,m) or approximated by (1+C)n, where C is a universal constant. The subscript 'online' is added when attention is restricted to online algorithms, and the superscript '1' is added when algorithms that find only one occurrence of the pattern in the text are considered. It is well known that n<or=c(n,m)<or=2n-m+1 or 0<or=C<or=1. These bounds are improved, and C/sub online/ is determined exactly.<<ETX>>
[Algorithm design and analysis, Mechanical factors, alphabet, Computational complexity, character comparisons, online algorithms, exact complexity, Upper bound, online operation, text string, linear-time string matching algorithm, Pattern matching, computational complexity, pattern recognition, pattern string]
Asynchronous PRAMs are (almost) as good as synchronous PRAMs
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A PRAM (parallel random-access-machine) model that allows processors to have arbitrary asynchronous behavior is introduced. The main result shows that any n-processor CRCW (concurrent-read, concurrent-write) PRAM program can be simulated on an asynchronous CRCW PRAM using O(n) expected work per parallel step and up to n/log n log*n asynchronous processors. It is shown that a synchronization primitive for n parallel instructions can be computed using O(n) expected work by a system of asynchronous processors. Since a special case of asynchronous behavior is a fail-stop error, the simulation technique described above can convert any PRAM program into a PRAM program that is resistant to all fail-stop errors and has the same expected work as the original program.<<ETX>>
[Algorithm design and analysis, parallel step, synchronization primitive, parallel random-access-machine, asynchronous CRCW PRAM, expected work, parallel programming, asynchronous behavior, Concurrent computing, Analytical models, Failure analysis, Computer architecture, Large-scale systems, parallel instructions, programming theory, Computational modeling, simulation technique, synchronous PRAMs, Phase change random access memory, concurrent-write, fail-stop error, Computer science, concurrent-read, Error correction, CRCW]
Multiple non-interactive zero knowledge proofs based on a single random string
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The authors solve the two major open problems associated with noninteractive zero-knowledge proofs: how to enable polynomially many provers to prove in writing polynomially many theorems based on the basis of a single random string, and how to construct such proofs under general (rather than number-theoretic) assumptions. The constructions can be used in cryptographic applications in which the prover is restricted to polynomial time, and they are much simpler than earlier (and less capable) proposals.<<ETX>>
[open problems, single random string, polynomially many theorems, multiple noninteractive zero knowledge proofs, cryptographic applications, cryptography, polynomially many provers, Proposals, Cryptographic protocols, Writing, Polynomials, theorem proving, Cryptography]
Some triply-logarithmic parallel algorithms
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is established that several natural problems have triply logarithmic, or even faster, optimal parallel algorithms. These problems include: merging two sorted lists, where the values are drawn from a large, but restricted, domain on a CREW PRAM; finding all prefix minima, where the values are drawn from a restricted domain; and top-bottom global routing around a rectangle, a well-investigated problem in VLSI routing for which only highly involved serial algorithms were known.<<ETX>>
[Algorithm design and analysis, parallel algorithms, merging, sorted lists, Computational modeling, Merging, Very large scale integration, Phase change random access memory, Educational institutions, Routing, restricted domain, global routing, Parallel algorithms, Concurrent computing, prefix minima, optimal parallel algorithms, CREW PRAM, Polynomials, triply-logarithmic, computational complexity]
Some tools for approximate 3-coloring
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Several tools for use in approximation algorithms to color 3-chromatic graphs are presented. The techniques are used in an algorithm that colors any 3-chromatic graph with O(n/sup 3/8/)+O(n/sup 3/8+O(1)/) colors (or more precisely) O(n/sup 3/8/log/sup 5/8/ n) colors, which improves the previous best bound of O(n/sup 0.4+0(1)/) colors. The techniques are illustrated by considering a problem in which the 3-chromatic graph is created not by a worst-case adversary, but by an adversary each of whose decisions (whether or not to include an edge) is reversed with some small probability or noise rate p. This type of adversary is equivalent to the semirandom source of M. Santha and U.V. Vazirani (1986). An algorithm that will actually 3-color such a graph with high probability even for quite low noise rates (p>or=n/sup -1/2+ epsilon / for constant epsilon >0), is presented.<<ETX>>
[Computer science, noise rate, Laboratories, 3-chromatic graphs, probability, algorithm theory, Polynomials, semirandom source, approximation algorithms, Colored noise, approximate 3-coloring, graph colouring]
Exploring an unknown graph
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is desired to explore all edges of an unknown directed, strongly connected graph. At each point one has a map of all nodes and edges visited, one can recognize these nodes and edges upon seeing them again, and it is known how many unexplored edges emanate from each node visited. The goal is to minimize the ratio of the total number of edges traversed to the optimum number of traversals had the graph been known. For Eulerian graphs this ratio cannot be better than 2, and 2 is achievable by a simple algorithm. In contrast, the ratio is unbounded when the deficiency of the graph (the number of edges that have to be added to make it Eulerian) is unbounded. The main result is an algorithm that achieves a bounded ratio when the deficiency is bounded; unfortunately the ratio is exponential in the deficiency. It is also shown that, when partial information about the graph is available, minimizing the worst-case ratio is PSPACE-complete.<<ETX>>
[Computer science, Pediatrics, Costs, directed graphs, directed graph, PSPACE-complete, Eulerian graphs, strongly connected graph, unknown graph exploration, bounded ratio, worst-case ratio, computational complexity]
Randomized online graph coloring
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is shown that randomization helps in coloring graphs online, and a simple randomized online algorithm is presented. For 3-colorable graphs the expected number of colors the algorithm uses is O((n log n)/sup 1/2/). The algorithm runs in polynomial time and compares well with the best known polynomial-time offline algorithms. A lower bound is proved for the randomized algorithm.<<ETX>>
[Legged locomotion, Computer science, Upper bound, Tree graphs, offline algorithms, randomised online graph colouring, Polynomials, polynomial time, lower bound, graph colouring, 3-colorable graphs]
Tight bounds on the complexity of cascaded decomposition of automata
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Exponential upper and lower bounds on the size of the cascaded (Krohn-Rhodes) decomposition of automata are given. These results are used to obtain elementary algorithms for various translations between automata and temporal logic, where the previously known translations were nonelementary. The relevance of the result is discussed.<<ETX>>
[complexity, automata theory, temporal logic, Mathematics, Counting circuits, Upper bound, Automata, cascaded decomposition, Logic, Books, automata, computational complexity, Krohn-Rhodes decomposition]
Exact identification of circuits using fixed points of amplification functions
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A technique for exactly identifying certain classes of read-once Boolean formulas is introduced. The method is based on sampling the input-output behavior of the target formula on a probability distribution which is determined by the fixed point of the formula's amplification function (defined as the probability that a 1 is output by the formula when each input bit is 1 independently with probability p). By performing various statistical tests on easily sampled variants of the fixed-point distribution, it is possible to infer efficiently all structural information about any logarithmic-depth target family (with high probability). The results are used to prove the existence of short universal identification sequences for large classes of formulas. Extensions of the algorithms to handle high rates of noise and to learn formulas of unbounded depth in L.G. Valiant's (1984) model with respect to specific distributions are described.<<ETX>>
[Performance evaluation, read-once Boolean formulas, circuit identification, Circuits, Laboratories, Probability distribution, amplification functions, statistical tests, Boolean functions, identification, Wires, probability distribution, noise, structural information, Polynomials, Contracts, Testing, unbounded depth, fixed-point distribution, probability, input-output behaviour sampling, formula learning, universal identification sequences, learning systems, Computer science, amplification, logarithmic-depth target family, Sampling methods, statistical analysis, logic circuits]
Communication-space tradeoffs for unrestricted protocols
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Communicating branching programs are introduced, and a general technique for demonstrating communication-space tradeoffs for pairs of communicating branching programs is developed. The technique is used to prove communication-space tradeoffs for any pair of communicating branching programs that hashes according to a universal family of hash functions. Other tradeoffs follow from this result. For example any pair of communicating Boolean branching programs that computes matrix-vector products over GF(2) requires communication-space product Omega (n/sup 2/). These are the first examples of communication-space tradeoffs on a completely general model of communicating processes.<<ETX>>
[communication-space tradeoffs, Protocols, Circuits, Binary decision diagrams, Very large scale integration, communicating branching programs, Complexity theory, hash functions, matrix-vector products, Galois fields, Distributed computing, Concurrent computing, unrestricted protocols, hashes, file organisation, Decision trees, protocols, Contracts]
Approximation through multicommodity flow
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The first approximate max-flow-min-cut theorem for general multicommodity flow is proved. It is used to obtain approximation algorithms for minimum deletion of clauses of a 2-CNF identical to formula, via minimization problems, and other problems. Also presented are approximation algorithms for chordalization of a graph and for register sufficiency that are based on undirected and directed node separators.<<ETX>>
[Upper bound, Minimization methods, Particle separators, Laboratories, algorithm theory, Approximation algorithms, Polynomials, multicommodity flow, approximation algorithms, minimum deletion, computational complexity, max-flow-min-cut theorem]
Efficient distribution-free learning of probabilistic concepts
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>
[p-concepts, weather prediction, distribution-free learning, Laboratories, Weather forecasting, probability, Educational institutions, Random processes, probabilistic concepts, machine learning, learning systems, Computer science, Rain, Current measurement, uncertain behaviour, model, Pressure measurement, Velocity measurement, probabilistic behavior, Meteorology]
Polynomial threshold functions, AC functions and spectrum norms
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The class of polynomial-threshold functions is studied using harmonic analysis, and the results are used to derive lower bounds related to AC/sup 0/ functions. A Boolean function is polynomial threshold if it can be represented as a sign function of a sparse polynomial (one that consists of a polynomial number of terms). The main result is that polynomial-threshold functions can be characterized by means of their spectral representation. In particular, it is proved that a Boolean function whose L/sub 1/ spectral norm is bounded by a polynomial in n is a polynomial-threshold function, and that a Boolean function whose L/sub infinity //sup -1/ spectral norm is not bounded by a polynomial in n is not a polynomial-threshold function. Some results for AC/sup 0/ functions are derived.<<ETX>>
[spectrum norms, Computational modeling, polynomials, Circuits, Boolean function, threshold logic, harmonic analysis, polynomial-threshold functions, Boolean functions, Neural networks, Ear, Polynomials, spectral representation, AC functions]
A (fairly) simple circuit that (usually) sorts
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A natural k-round tournament over n=2/sup k/ players is analyzed, and it is demonstrated that the tournament possesses a surprisingly strong ranking property. The ranking property of this tournament is exploited by being used as a building block for efficient parallel sorting algorithms under a variety of different models of computation. Three important applications are provided. First, a sorting circuit of depth 7.44 log n, which sorts all but a superpolynomially small fraction of the n-factorial possible input permutations, is defined. Secondly, a randomized sorting algorithm that runs in O(log n) word steps with very high probability is given for the hypercube and related parallel computers (the butterfly, cube-connected cycles, and shuffle-exchange). Thirdly, a randomized algorithm that runs in O(m+log n)-bit steps with very high probability is given for sorting n O(m)-bit records on an n log n-node butterfly.<<ETX>>
[Algorithm design and analysis, parallel algorithms, models, hypercube, Computational modeling, Circuits, randomized algorithm, Application software, shuffle-exchange, Sorting, Concurrent computing, Computer science, cube-connected, butterfly, sorting, strong ranking property, parallel sorting, Hypercubes, Contracts]
Specified precision polynomial root isolation is in NC
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Given a polynomial p(z) od degree n with integer coefficients, whose absolute values are bounded above by 2/sup m/, and a specified integer mu , it is shown that the problem of determining all roots of p with error less than 2/sup - mu / is in the parallel complexity class NC. To do this, an algorithm that runs on at most POLY(n+m+ mu ) processors with a parallel time complexity of O(log/sup 3/(n+m+ mu )) is constructed. This algorithm extends the algorithm of M. Ben-Or et al. (SIAM J. Comput., vol.17, p.1081-92, 1988) by removing the severe restriction that all the roots of p(z) should be real.<<ETX>>
[parallel algorithms, parallel complexity class NC, polynomials, polynomial root isolation, precision, parallel time complexity, Mathematics, error, Concurrent computing, Ear, Polynomials, Arithmetic, computational complexity]
Separating distribution-free and mistake-bound learning models over the Boolean domain
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Two of the most commonly used models in computational learning theory are the distribution-free model, in which examples are chosen from a fixed but arbitrary distribution, and the absolute mistake-bound model, in which examples are presented in order by an adversary. Over the Boolean domain
[Heart, distribution-free model, Laboratories, ordered examples, computational learning theory, set theory, polynomial-time learning algorithm, Distributed computing, absolute mistake-bound model, adversary, learning systems, membership queries, Boolean functions, model separation, Voting, concept class, Boolean domain, Polynomials, Cryptography, one-way functions, computational complexity, unlimited computational resources]
Distributed reactive systems are hard to synthesize
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The problem of synthesizing a finite-state distributed reactive system is considered. Given a distributed architecture A, which comprises several processors P/sub 1/, . . ., P/sub k/ and their interconnection scheme, and a propositional temporal specification phi , a solution to the synthesis problem consists of finite-state programs Pi /sub 1/, . . ., Pi /sub k/ (one for each processor), whose joint (synchronous) behavior maintains phi against all possible inputs from the environment. Such a solution is referred to as the realization of the specification phi over the architecture A. Specifically, it is shown that the problem of realizing a given propositional specification over a given architecture is undecidable, and it is nonelementarily decidable for the very restricted class of hierarchical architectures. An extensive characterization of architecture classes for which the realizability problem is elementarily decidable and of classes for which it is undecidable is given.<<ETX>>
[elementarily decidable, distributed processing, nonelementarily decidable, Mathematics, Maintenance, distributed architecture, Research and development, distributed reactive system, decidability, Councils, finite-state, undecidable, Automata, Open systems, propositional specification, Logic]
Bounds on tradeoffs between randomness and communication complexity
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A quantitative investigation of the power of randomness in the context of communication complexity is initiated. The authors prove general lower bounds on the length of the random input of parties computing a function f, depending on the number of bits communicated and the deterministic communication complexity of f. Four standard models for communication complexity are considered: the random input of the parties may be shared or local, and the communication may be one-way or two-way. The bounds are shown to be tight for all the models, for all values of the deterministic communication complexity, and for all possible quantities of bits exchanged. It is shown that it is possible to reduce the number of random bits required by any protocol, without increasing the number of bits exchanged (up to a limit depending on the advantage achieved by the protocol).<<ETX>>
[Context, Protocols, Computational modeling, tradeoffs, Length measurement, Routing, randomness, Complexity theory, communication complexity, deterministic communication complexity, random bits, Radio access networks, Communication standards, Computer science, Measurement standards, quantitative, computational complexity]
Perfectly secure message transmission
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The problem of perfectly secure communication in a general network in which processors and communication lines may be faulty is studied. Lower bounds are obtained on the connectivity required for successful secure communication. Efficient algorithms that operate with this connectivity and rely on no complexity theoretic assumptions are derived. These are the first algorithms for secure communication in a general network to achieve simultaneously the goals of perfect secrecy, perfect resiliency, and a worst case time which is linear in the diameter of the network.<<ETX>>
[Optical fibers, Protocols, Costs, faulty communication lines, network diameter, Surface-mount technology, secure message transmission, perfectly secure communication, faulty processors, general network, perfect secrecy, Privacy, security of data, connectivity, Wires, Bandwidth, Computer networks, efficient algorithms, Communication networks, protocols, Contracts, perfect resiliency, worst case time, computational complexity]
Nondeterministic exponential time has two-prover interactive protocols
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The exact power of two-prover interactive proof systems (MIP) introduced by M. Ben-Or et al. (Proc. 20th Symp. on Theory of Computing, 1988, p.113-31) is determined. In this system, two all-powerful noncommunicating provers convince a randomizing polynomial-time verifier in polynomial time that the input x belongs to the language L. It was previously suspected (and proved in a relativized sense) that coNP-complete languages do not admit such proof systems. In sharp contrast, it is shown that the class of languages having two-prover interactive proof systems is computable in nondeterministic exponential time (NEXP). This represents a further step demonstrating the unexpectedly immense power for randomization and interaction in efficient provability.<<ETX>>
[nondeterministic exponential time, Protocols, formal languages, Circuits, efficient provability, multiple prover interactive proof systems, Graphics, Extrapolation, Upper bound, coNP-complete languages, noncommunicating provers, interactive systems, Polynomials, two-prover interactive protocols, polynomial time, theorem proving, Cryptography, protocols, Joining processes, computational complexity, randomizing polynomial-time verifier]
The computability and complexity of optical beam tracing
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The ray-tracing problem is considered for optical systems consisting of a set of refractive or reflective surfaces. It is assumed that the position and the tangent of the incident angle of the initial light ray are rational. The computability and complexity of the ray-tracing problems are investigated for various optical models. The results show that, depending on the optical model, ray tracing is sometimes undecidable, sometimes PSPACE-hard, and sometimes in PSPACE.<<ETX>>
[complexity, undecidable problem, Optical computing, computability, computational geometry, Geometrical optics, light refraction, optical beam tracing, light reflection, Optical design, decidability, Computer graphics, Ray tracing, PSPACE-hard problem, Mirrors, Optical beams, Contracts, optical models, geometrical optics, incident angle, ray-tracing problem, physics computing, reflective surfaces, refractive surfaces, Optical refraction, Lenses, computational complexity]
Matrix decomposition problem is complete for the average case
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The first algebraic average-case complete problem is presented. The focus of attention is the modular group, i.e., the multiplicative group SL/sub 2/(Z) of two-by-two integer matrices of determinant 1. By default, in this study matrices are elements of the modular group. The problem is arguably the simplest natural average-case complete problem to date.<<ETX>>
[Computer aided software engineering, Ear, Probability distribution, Robustness, Polynomials, matrix decomposition, complete problem, Matrix decomposition, algebraic average-case complete problem, computational complexity, matrix algebra]
An approach for proving lower bounds: solution of Gilbert-Pollak's conjecture on Steiner ratio
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A family of finitely many continuous functions on a polytope X, namely (g/sub i/(x))/sub i in I/, is considered, and the problem of minimizing the function f(x)=max/sub i in I/g/sub i/(x) on X is treated. It is shown that if every g/sub i/(x) is a concave function, then the minimum value of f(x) is achieved at finitely many special points in X. As an application, a long-standing problem about Steiner minimum trees and minimum spanning trees is solved. In particular, if P is a set of n points on the Euclidean plane and L/sub s/(P) and L/sub m/(P) denote the lengths of a Steiner minimum tree and a minimum spanning tree on P, respectively, it is proved that, for any P, L/sub S/(P)>or= square root 3L/sub m/(P)/2, as conjectured by E.N. Gilbert and H.O. Pollak (1968).<<ETX>>
[Steiner trees, minimum spanning trees, Laboratories, trees (mathematics), Minimax techniques, Euclidean plane, Steiner ratio, Mathematics, continuous functions, lower bounds, Computer science, Q measurement, NP-hard problem, function minimization, minimisation, concave function, Steiner minimum trees, polytope]
Uniform memory hierarchies
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The authors introduce a model, called the uniform memory hierarchy (UMH) model, which reflects the hierarchical nature of computer memory more accurately than the RAM (random-access-machine) model, which assumes that any item in memory can be accessed with unit cost. In the model memory occurs as a sequence of increasingly large levels. Data are transferred between levels in fixed-size blocks (the size is level dependent). Within a level blocks are random access. The model is easily extended to handle parallelism. The UMH model is really a family of models parameterized by the rate at which the bandwidth decays as one travels up the hierarchy. A program is parsimonious on a UMH if the leading terms of the program's (time) complexity on the UMH and on a RAM are identical. If these terms differ by more than a constant factor, then the program is inefficient. The authors analyze two standard FFT programs with the same RAM complexity. One is efficient; the other is not.<<ETX>>
[Algorithm design and analysis, Costs, Computational modeling, computer memory, Random access memory, parallelism, parsimonious, Read-write memory, Surface roughness, Rough surfaces, FFT programs, memory architecture, random-access-machine, RAM complexity, Bandwidth, uniform memory hierarchy, Parallel processing, Solids, computational complexity, RAM]
Randomness in interactive proofs
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The quantitative aspects of randomness in interactive proof systems are studied. The result is a randomness-efficient error-reduction technique: given an Arthur-Merlin proof system (error probability <or=1/3) in which Arthur sends l=l(n) random bits per rounds, a proof system that achieves error probability 2/sup -k/ at the cost of Arthur sending only 2l+O(k) random bits per round is constructed. The method maintains the number of rounds in the game. Underlying the transformation is a novel sampling method for approximating the average value of an arbitrary function f:
[error-reduction technique, Costs, Protocols, Error probability, Computational modeling, Laboratories, game theory, randomness, Computer science, Voting, interactive proofs, algorithm theory, Arthur-Merlin proof system, Sampling methods, Polynomials, theorem proving, error handling, sampling method, coin tosses]
Coloring inductive graphs on-line
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Online graph coloring, in which the vertices are presented one at a time, is considered. Each vertex must be assigned a color, different from the colors of its neighbors, before the next vertex is given. The class of d-inductive graphs is treated. A graph G is said to be d-inductive if the vertices of G can be numbered so that each vertex has at most d edges to higher numbered vertices. First Fit (FF) is the algorithm that assigns each vertex the lowest numbered color possible. It is shown that if G is d-inductive, then FF uses O(d log n) colors on G. This yields an upper bound of O(log n) on the performance ratio of FF on chordal and planar graphs. FF does as well as any online algorithm for d-inductive graphs; it is shown that for any d and any online graph-coloring algorithm A, there is a d-inductive graph that forces A to use Omega (d log n) colors to color G. Online graph coloring with lookahead is also investigated.<<ETX>>
[Algorithm design and analysis, online graph colouring, Costs, vertices, upper bound, Registers, Delay, graph colouring, Computer science, d-inductive graphs, lookahead, Processor scheduling, First Fit algorithm]
Finite-memory automata
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A model of computation dealing with infinite alphabets is proposed. The model is based on replacing the equality test by unification. It appears to be a natural generalization of the classical Rabin-Scott finite-state automata and possesses many of their properties.<<ETX>>
[Context, finite automata, Computational modeling, finite-memory automata, Registers, Character recognition, unification, Database languages, equality test, Rabin-Scott finite-state automata, Computer science, Concurrent computing, infinite alphabets, Automatic testing, Automata]
Algebraic methods for interactive proof systems
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
An algebraic technique for the construction of interactive proof systems is proposed. The technique is used to prove that every language in the polynomial-time hierarchy has an interactive proof system. For the proof, a method is developed for reducing the problem of verifying the value of a low-degree polynomial at two points to verifying the value at one new point. The results have implications for program checking, verification, and self-correction.<<ETX>>
[Context, formal languages, programming theory, program verification, polynomials, interactive proof systems, language, program self correction, program checking, algebraic technique, polynomial-time hierarchy, low-degree polynomial, polynomial value verification, interactive systems, Polynomials, theorem proving, protocols, computational complexity]
A Markovian extension of Valiant's learning model
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A model of learning that expands on the Valiant model is introduced. The point of departure from the Valiant model is that the learner is placed in a Markovian environment. The environment of the learner is a (exponentially large) graph, and the examples reside on the vertices of the graph, one example on each vertex. The learner obtains the examples while performing a random walk on the graph. At each step, the learning algorithm guesses the classification of the example on the current vertex using its current hypothesis. If its guess is incorrect, the learning algorithm updates its current working hypothesis. The performance of the learning algorithm in a given environment is judged by the expected number of mistakes made as a function of the number of steps in the random walk. The predictive value of Occam algorithms under this weaker probabilistic model of the learner's environment is studied.<<ETX>>
[random walk, weaker probabilistic model, performance, Predictive models, Markov processes, Valiant learning model, Probability distribution, Classification algorithms, classification, Occam algorithms, learning systems, Markovian extension]
Fault tolerant sorting network
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A general technique for enhancing the reliability of sorting networks and other comparator-based networks is presented. The technique converts any network that uses unreliable comparators to a fault-tolerant network that produces the correct output with overwhelming probability, even if each comparator is faulty with some probability smaller than 1/2, independently of other comparators. The depth of the fault-tolerant network is only a constant times the depth of the original network, and the width of the network is increased by a logarithmic factor.<<ETX>>
[Algorithm design and analysis, Career development, reliability enhancement, Merging, logarithmic factor, Stochastic processes, Mathematics, Registers, Sorting, Fault tolerance, fault tolerant sorting network, sorting, Computer networks, fault tolerant computing, Large-scale systems, comparator-based networks]
Triangulating a simple polygon in linear time
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A linear-time deterministic algorithm for triangulating a simple polygon is developed. The algorithm is elementary in that it does not require the use of any complicated data structures; in particular, it does not need dynamic search trees, finger trees, or fancy point location structures.<<ETX>>
[Tree data structures, triangulation, merging, Merging, computational geometry, Turning, Partitioning algorithms, simple polygon, Sorting, linear-time deterministic algorithm, Fingers, granularity, Computer graphics, visibility maps, Polynomials, data structures, 2D computational geometry, conformality, computational complexity]
The complexity of finding medians
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
PF( Hash P) is characterized in a manner similar to M.W. Krentel's (1988) characterization of Pf(NP). If MidP is the class of functions that give the medians in the outputs of metric Turing machines, then it is shown that every function in PF( Hash P) is polynomial time 1-Turing reducible to a function in MidP and MidP contained in PF( Hash P); that is, PF( Hash P)=PF(MidP(1)). Intuitively, finding medians is as hard computationally as PF( Hash P); this forms a contrast to an intuitive interpretation of Krentel's result that finding maxima (or minima) is as hard as PF(NP). Several applications of the result are shown.<<ETX>>
[finding medians, complexity, 1-Turing reducible, metric Turing machines, Mathematics, History, MidP, Computational complexity, Computer science, Turing machines, PF( Hash P), Polynomials, computational complexity]
Coin-flipping games immune against linear-sized coalitions
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is proved that for every c<1 there are perfect-information coin-flipping and leader-election games on n players in which no coalition of cn players can influence the outcome with probability greater than some universal constant times c. It is shown that a random protocol of a certain length has this property, and an explicit construction is given as well.<<ETX>>
[Protocols, random protocol, game players, Nominations and elections, probability, game theory, distributed processing, coin flipping games, Distributed computing, outcome influencing, Fault tolerance, perfect information games, linear-sized coalitions, Broadcasting, fault tolerant computing, fault tolerant distributed computing, Size control, protocols, leader-election games]
Approximate string matching in sublinear expected time
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The k differences approximate string matching problem specifies a text string of length n, a pattern string of length m, and the number k of differences (insertions, deletions, substitutions) allowed in a match, and asks for every location in the text where a match occurs. Previous algorithms required at least O(nk) time. When k is as large as a fraction of m, no substantial progress has been made over O(nm) dynamic programming. The authors have investigated much faster algorithms for restricted cases of the problem, such as when the text string is random and errors are not too frequent. They have devised an algorithm that, for k<m/log n+O(1), runs in time O((n/m)k log n) on the average. In the worst case their algorithm is O(nk), but it is still an improvement in that it is very practical and uses only O(n) space compared with O(n) or O(n/sup 2/). The authors define an approximate substring matching problem and give efficient algorithms based on their techniques. Special cases include several applications to genetics and molecular biology.<<ETX>>
[molecular biology, insertions, deletions, Humans, differences, random string, infrequent errors, sublinear expected time, genetics, substitutions, approximate string matching, Genetics, Dynamic programming, Performance analysis, Pattern analysis, Assembly, search problems, pattern recognition, Sequences, dynamic programming, Computer science, substring matching, DNA, text string, efficient algorithms, molecular biophysics, Pattern matching, computational complexity, pattern string]
Drawing graphs in the plane with high resolution
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The problem of drawing a graph in the plane so that edges appear as straight lines and the minimum angle formed by any pair of incident edges is maximized is studied. The resolution of a layout is defined to be the size of the minimum angle formed by incident edges of the graph, and the resolution of a graph is defined to be the maximum resolution of any layout of the graph. The resolution R of a graph is characterized in terms of the maximum node degree d of the graph by proving that Omega (1/d/sup 2/)<or=R<or=2 pi /d for any graph. Moreover, it is proved that R= Theta (1/d) for many graphs, including planar graphs, complete graphs, hypercubes, multidimensional meshes and tori, and other special networks. It is also shown that the problem of deciding if R=2 pi /d for a graph is NP-hard for d=4, and a counting argument is used to show that R=O(log d/d/sup 2/) for many graphs.<<ETX>>
[counting, Multidimensional systems, maximum node degree, multidimensional meshes, tori, Laboratories, graph theory, minimum angle, straight line edges, Mathematics, incident edges, Paper technology, high resolution planar graph drawing, hypercubes, Proposals, Computer science, Upper bound, optimisation, NP-hard problem, Cost function, Hypercubes, Contracts, complete graphs]
On the power of small-depth threshold circuits
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The power of threshold circuits of small depth is investigated. In particular, functions that require exponential-size unweighted threshold circuits of depth 3 when the bottom fan-in is restricted are given. It is proved that there are monotone functions f/sub k/ that can be computed on depth k and linear size AND, OR circuits but require exponential-size to be computed by a depth-(k-1) monotone weighted threshold circuit.<<ETX>>
[Protocols, small-depth threshold circuits, functions, Circuits, monotone weighted threshold circuit, Probability distribution, Complexity theory, threshold logic, monotone functions, combinatorial switching, bottom fan-in, AND, Cost function, Polynomials, OR circuits]
Parallel linear programming in fixed dimension almost surely in constant time
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is shown that, for any fixed dimension d, the linear programming problem with n inequality constraints can be solvent on a probabilistic CRCW PRAM (concurrent-read-concurrent-write parallel random-access machine) with O(n) processors almost surely in constant time. The algorithm always finds the correct solution. With nd/log/sup 2/d processors, the probability that the algorithm will not finish within O(d/sup 2/log/sup 2/d) time tends to zero exponentially with n.<<ETX>>
[parallel algorithms, Time of arrival estimation, linear programming problem, probability, Linear programming, Phase change random access memory, linear programming, Linear matrix inequalities, constant time, Failure analysis, fixed dimension, probabilistic CRCW PRAM, computational complexity]
Online algorithms for finger searching
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The technique of speeding up access into search structures by maintaining fingers that point to various locations of the search structure is considered. The problem of choosing, in a large search structure, locations at which to maintain fingers is treated. In particular, a server problem in which k servers move along a line segment of length m, where m is the number of keys in the search structure, is addressed. Since fingers may be arbitrarily copied, a server is allowed to jump, or fork, to a location currently occupied by another server. Online algorithms are presented and their competitiveness analyzed. It is shown that the case in which k=2 behaves differently from the case in which k>or=3, by showing that there is a four-competitive algorithm for k=2 that never forks its fingers. For k>or=3, it is shown that any online algorithm that does not fork its fingers can be at most Omega (m/sup 1/2/)-competitive. The main result is that for k=3 there is an online algorithm that forks and is constant competitive (independent of m, the size of the search structure). The algorithm is simple and implementable.<<ETX>>
[Algorithm design and analysis, Tree data structures, Costs, TV, online algorithm, Binary search trees, Sorting, database theory, search structures, Fingers, algorithm theory, finger searching, data structures, server problem, Artificial intelligence, search problems]
Probabilities of sentences about very sparse random graphs
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The author considers random graphs with edge probability beta n/sup - alpha /, where n is the number of vertices of the graph, beta >0 is fixed, and alpha =1 or alpha =(l+1)/l for some fixed positive integer l. It is proved that, for every first-order sentence, the probability that the sentence is true for the random graph has an asymptotic limit. Also, there is an effective procedure for generating the value of the limit in closed form.<<ETX>>
[Computer science, formal languages, very sparse random graphs, graph theory, edge probability, fixed positive integer, Formal languages, first-order sentence, Probability distribution, Logic, Application software, Combinatorial mathematics]
Asymptotically tight bounds for computing with faulty arrays of processors
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The computational power of 2-D and 3-D processor arrays that contain a potentially large number of faults is analyzed. Both a random and a worst-case fault model are considered, and it is proved that in either scenario low-dimensional arrays are surprisingly fault tolerant. It is also shown how to route, sort, and perform systolic algorithms for problems such as matrix multiplication in optimal time on faulty arrays. In many cases, the running time is the same as if there were no faults in the array (up to constant factors). On the negative side, it is shown that any constant congestion embedding of an n*n fault-free array on an n*n array with Theta (n/sup 2/) random faults (or Theta (log n) worst-case faults) requires dilation Theta (log n). For 3-D arrays, knot theory is used to prove that the required dilation is Omega ( square root log n).<<ETX>>
[knot theory, Military computing, systolic algorithms, congestion embedding, Laboratories, systolic arrays, Routing, 2-D, Mathematics, faulty arrays of processors, 3-D, sort, Bridges, Computer science, Fault tolerance, worst-case fault model, low-dimensional arrays, asymptotically tight bounds, route, random fault model, fault tolerant computing, Contracts, Testing]
Provably good mesh generation
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Several versions of the problem of generating triangular meshes for finite-element methods are studied. It is shown how to triangulate a planar point set or a polygonally bounded domain with triangles of bounded aspect ratio, how to triangulate a planar point set with triangles having no obtuse angles, how to triangulate a point set in arbitrary dimension with simplices of bounded aspect ratio, and how to produce a linear-size Delaunay triangulation of a multidimensional point set by adding a linear number of extra points. All the triangulations have size within a constant factor of optimal and run in optimal time O(n log n+k) with input of size n and output of size k. No previous work on mesh generation simultaneously guarantees well-shaped elements and small total size.<<ETX>>
[finite-element methods, Solid modeling, Design automation, Data analysis, multidimensional point set, computational geometry, Finite element methods, finite element analysis, polygonally bounded domain, Geometry, Computer science, optimal size, Tree graphs, acute angled triangles, bounded aspect ratio, Rendering (computer graphics), triangular meshes, simplices, Polynomials, probably good mesh generation, planar point set, linear-size Delaunay triangulation, well-shaped elements, Mesh generation]
On the predictability of coupled automata: an allegory about chaos
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The authors show a sharp dichotomy between systems of identical automata with symmetric global control whose behavior is easy to predict and those whose behavior is hard to predict. The division pertains to whether the global control rule is invariant with respect to permutations of the states of the automaton. It is also shown that testing whether the global control rule has this invariance property is an undecidable problem. It is argued that there is a natural analog between complexity in the present model and chaos in dynamical systems.<<ETX>>
[Chaos, complexity, undecidable problem, chaos, automata theory, dichotomy, Predictive models, Control systems, symmetric global control, predictability, decidability, Automata, Automatic control, Polynomials, coupled automata, Testing, computational complexity]
Are wait-free algorithms fast?
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The time complexity of wait-free algorithms in so-called normal executions, where no failures occur and processes operate at approximately the same speed, is considered. A lower bound of log n on the time complexity of any wait-free algorithm that achieves approximate agreement among n processes is proved. In contrast, there exists a non-wait-free algorithm that solves this problem in constant time. This implies an Omega (log n)-time separation between the wait-free and non-wait-free computation models. An O(log n)-time wait-free approximate agreement algorithm is presented. Its complexity is within a small constant of the lower bound.<<ETX>>
[Atomic measurements, Algorithm design and analysis, parallel algorithms, execution speed, normal executions, Computational modeling, Read-write memory, distributed processing, time complexity, Time measurement, Delay, Computer science, wait-free algorithms, approximate agreement, algorithm theory, Writing, Time sharing computer systems, computation models, shared memory distributed systems, Contracts, computational complexity]
Towards a DNA sequencing theory (learning a string)
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Mathematical frameworks suitable for massive automated DNA sequencing and for analyzing DNA sequencing algorithms are studied under plausible assumptions. The DNA sequencing problem is modeled as learning a superstring from its randomly drawn substrings. Under certain restrictions, this may be viewed as learning a superstring in L.G. Valiant's (1984) learning model, and in this case the author gives an efficient algorithm for learning a superstring and a quantitative bound on how many samples suffice. A major obstacle to the approach turns out to be a quite well-known open question on how to approximate the shortest common superstring of a set of strings. The author presents the first provably good algorithm that approximates the shortest superstring of length n by a superstring of length O(n log n).<<ETX>>
[Sequences, Machine learning algorithms, merging, efficient algorithm, randomly drawn substrings, Laboratories, superstring learning, Humans, Genomics, shortest common superstring, samples, Postal services, learning systems, biology computing, DNA, Machine learning, Approximation algorithms, Bioinformatics, search problems, DNA sequencing]
Complexity of unification in free groups and free semi-groups
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is proved that the exponent of periodicity of a minimal solution of a word equation is at most 2/sup 2.54n/, where n is the length of the equation. Since the best known lower bound is 2/sup 0.31n/, this upper bound is almost optimal and exponentially better than the original bound. Thus the result implies exponential improvement of known upper bounds on complexity of word-unification algorithms. Evidence is given that, contrary to common belief, the algorithm deciding satisfiability of equations in free groups, given by G.S. Makanin (1977), is not primitive recursive.<<ETX>>
[group theory, complexity, Upper bound, satisfiability, free groups, Mathematics, free semi-groups, unification, Equations, word-unification algorithms, computational complexity]
Reducing the parallel complexity of certain linear programming problems
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The parallel complexity of solving linear programming problems is studied in the context of interior point methods. If n and m, respectively, denote the number of variables and the number of constraints in the given problem, an algorithm that solves linear programming problems in O((mn)/sup 1/4/ (log 1 n)/sup 3/L) time using O(M(n)m/n+1n/sup 3/) processors is given. (M(n) is the number of operations for multiplying two n*n matrices). This gives an improvement in the parallel running time for n=o(m). A typical case in which n=o(m) is the dual of the uncapacitated transportation problem. The algorithm solves the uncapacitated transportation problem in O((VE)/sup 1/4/(log V)/sup 3/ (log V gamma )) time using O(V/sup 3/) processors, where V (E) is the number of nodes (edges) and gamma is the largest magnitude of an edge cost or a demand at a node. As a by-product, a better parallel algorithm for the assignment problem for graphs of moderate density is obtained.<<ETX>>
[parallel algorithms, parallel complexity, Costs, Computational modeling, moderate density, Transportation, parallel algorithm, assignment problem, Linear programming, Phase change random access memory, linear programming, Parallel algorithms, uncapacitated transportation problem, Concurrent computing, Computer science, interior point methods, parallel running time, edge cost, graphs, Iterative algorithms, Erbium, computational complexity]
Communication-optimal maintenance of replicated information
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
It is shown that keeping track of history allows significant improvements in the realistic model of communication complexity of dynamic network protocols. The communication complexity for solving an arbitrary graph problem is improved from Theta (E) to Theta (V), thus achieving the lower bound. Moreover, O(V) is also the amortized complexity of solving an arbitrary function (not only graph functions) defined on the local inputs of the nodes. As a corollary, it is found that amortized communication complexity, i.e. incremental cost of adapting to a single topology change, can be smaller than the communication complexity of solving the problem from scratch. The first stage in the solution is a communication-optimal maintenance of a spanning tree in a dynamic network. The second stage is the optimal maintenance of replicas of databases. An important example of this task is the problem of updating the description of the network's topology at every node. For this problem the message complexity is improved from O(EV) to Theta (V). The improvement for a general database is even larger if the size of the database is larger than E.<<ETX>>
[Protocols, communication-optimal maintenance, Mathematics, Complexity theory, Electronic mail, History, communication complexity, amortized complexity, arbitrary graph problem, message complexity, Databases, Network topology, distributed databases, Cost function, Polynomials, protocols, Contracts, replicated information, databases, trees (mathematics), replicas, dynamic network, database theory, dynamic network protocols, spanning tree, computational complexity]
Deterministic on-line routing on area-universal networks
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
Two deterministic routing networks, the pruned butterfly and the sorting fat-tree, are presented. Both networks are area universal, i.e. they can simulate with polylogarithmic slowdown, any other routing network fitting in similar area. Previous area-universal networks were either for the offline problem, where the message set to be routed is known in advance and substantial precomputation is permitted, or involved randomization, yielding results that hold only with high probability. The present networks are the first that are simultaneously deterministic and online, and they use two substantially different routing techniques. The performance of the routing algorithms depends on the difficulty of the problem instance, which is measured by a quantity lambda , known as the load factor. The pruned butterfly algorithm runs in time O( lambda log/sup 2/N), where N is the number of possible sources and destinations for messages and lambda is assumed to be polynomial in N. The sorting fat-free algorithm runs in O( lambda log N + log/sup 2/N) time for a restricted class of message sets, including partial permutations. Other results include a new type of sorting circuit, an area universal circuit, and an area-time lower bound for routers.<<ETX>>
[Costs, area-universal networks, Computational modeling, Circuits, multiprocessor interconnection networks, Very large scale integration, Routing, polylogarithmic slowdown, Sorting, Computer science, deterministic online routing, sorting fat-tree, routing, Wires, randomization, Bandwidth, sorting, Polynomials, partial permutations, precomputation, pruned butterfly]
The lattice reduction algorithm of Gauss: an average case analysis
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The lattice reduction algorithm of Gauss is shown to have an average-case complexity that is asymptotic to a constant. The analysis makes use of elementary properties of continued fractions and of linear fractional transformations.<<ETX>>
[Algorithm design and analysis, Computer aided software engineering, lattice reduction algorithm, continued fractions, Lattices, Optical wavelength conversion, average case analysis, Gauss, Convergence, Computer science, average-case complexity, Gaussian processes, linear fractional transformations, computational complexity]
Sparse partitions
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
A collection of clustering and decomposition techniques that make possible the construction of sparse and locality-preserving representations for arbitrary networks is presented. The representation method considered is based on breaking the network G(V,E) into connected regions, or clusters, thus obtaining a cover for the network, i.e. a collection of clusters that covers the entire set of vertices V. Several other graph-theoretic structures that are strongly related to covers are discussed. These include sparse spanners, tree covers of graphs and the concepts of regional matchings and diameter-based separators. All of these structures can be constructed by means of one of the clustering algorithms given, and each has proved a convenient representation for handling certain network applications.<<ETX>>
[locality-preserving representations, Protocols, Costs, sparse spanners, graph theory, Communication system control, Mobile communication, Mathematics, polynomial routing, clustering algorithms, Clustering algorithms, decomposition techniques, protocols, Contracts, covers, computer networks, graph-theoretic structures, tree covers, regional matchings, Routing, diameter-based separators, Partitioning algorithms, network applications, sparse partitions, ARPANET, online tracking, arbitrary networks]
Simplifying nested radicals and solving polynomials by radicals in minimum depth
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The notion of pure nested radicals and its field-theoretic counterpart, pure root extensions, are defined and used for investigating exact radical solutions.<<ETX>>
[polynomials, pure root extensions, pure nested radicals, Gaussian processes, Polynomials, Encoding, nested radicals, minimum depth, Equations, computational complexity]
Network synchronization with polylogarithmic overhead
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The synchronizer is a simulation methodology for simulating a synchronous network by an asynchronous one, thus enabling the execution of a synchronous algorithm on an asynchronous network. Previously known synchronizers require each processor in the network to participate in each pulse of the synchronization process. The resulting communication overhead depends linearly on the number n of network nodes. A synchronizer with overhead only polylogarithmically dependent on n is introduced. This synchronizer can also be realized with polylog(n) space. The polylog-overhead synchronizer is based on involving only the relevant portions of the network in the synchronization process.<<ETX>>
[Algorithm design and analysis, polylogarithmic overhead, Career development, Protocols, Pulse measurements, Mathematics, Synchronization, switching theory, communication overhead, synchronisation, Pulse generation, ARPANET, simulation methodology, virtual machines, asynchronous network, synchronous network, data structures, protocols, Contracts, Clocks]
On the diameter of finite groups
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The diameter of a group G with respect to a set S of generators is the maximum over g in G of the length of the shortest word in S union S/sup -1/ representing g. This concept arises in the contexts of efficient communication networks and Rubik's-cube-type puzzles. 'Best' generators are pertinent to networks, whereas 'worst' and 'average' generators seem more adequate models for puzzles. A substantial body of recent work on these subjects by the authors is surveyed. Regarding the 'best' case, it is shown that, although the structure of the group is essentially irrelevant if mod S mod is allowed to exceed (log mod G mod )/sup 1+c/(c>0), it plays a strong role when mod S mod =O(1).<<ETX>>
[group theory, Upper bound, Genetic mutations, communication networks, finite groups, Rubik's-cube, generators, Communication networks, Context modeling]
General weak random sources
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The following model for a weak random source is considered. The source is asked only once for R bits, and the source outputs an R-bit string such that no string has probability more than 2/sup - delta R/ of being output. for some fixed delta >0. A pseudorandom generator that runs in time n/sup O(log n)/ and simulates RP using as a seed a string from such a source is exhibited. Under the generalized Paley graph conjecture, a generator that runs in polynomial time and simulates RP is given, as well as a different generator that produces almost perfectly random bits at a rate arbitrarily close to optimal using as seeds strings from a constant number of independent weak random sources.<<ETX>>
[Paley graph conjecture, pseudorandom generator, Computational modeling, Computer simulation, graph theory, probability, Entropy, Diodes, Computer science, Upper bound, Physics computing, Character generation, algorithm theory, weak random source, Polynomials, polynomial time, Random number generation, computational complexity]
Simple construction of almost k-wise independent random variables
Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science
None
1990
The authors present three alternative simple constructions of small probability spaces on n bits for which any k bits are almost independent. The number of bits used to specify a point in the sample space is O(log log n+k+log 1/ epsilon ), where epsilon is the statistical difference between the distribution induced on any k-bit locations and the uniform distribution. This is asymptotically comparable to the construction recently presented by J. Naor and M. Naor (1990). An advantage of the present constructions is their simplicity. Two of the constructions are based on bit sequences that are widely believed to possess randomness properties, and the results can be viewed as an explanation and establishment of these beliefs.<<ETX>>
[Heart, algorithm efficiency, probability, random processes, Shift registers, Probability distribution, probability spaces, Feedback, Ear, algorithm theory, Sampling methods, statistical difference, Polynomials, Eigenvalues and eigenfunctions, Random variables, independent random variables]
Connected components in O(lg/sup 3/2 mod V/ mod ) parallel time for the CREW PRAM
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Computing the connected components of an undirected graph G=(V, E) on mod V mod =n vertices and mod E mod =m edges is addressed. An efficient and simple algorithm that runs in O(lg/sup 3/2/ n) time using n+m CREW processors is presented.<<ETX>>
[Process design, Algorithm design and analysis, parallel algorithms, parallel time, connected components, graph theory, vertices, CREW processors, edges, Phase change random access memory, Educational institutions, Mathematics, Parallel algorithms, Concurrent computing, Computer science, Writing, CREW PRAM, undirected graph, Joining processes, computational complexity]
Efficient algorithms for the Riemann-Roch problem and for addition in the Jacobian of a curve
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Several computational problems concerning the construction of rational functions and intersecting curves over a given curve are studied. The first problem is to construct a rational function with prescribed zeros and poles over a given curve. More precisely, let C be a smooth projective curve and assume as given an affine plane model F(x,y)=0 for C, a finite set of points P/sub i/=(X/sub i/, Y/sub i/) with F (X/sub i/, Y/sub i/)=0 and natural numbers n/sub i/, and a finite set of points Q/sub i/=(X/sub j/, Y/sub j/) with F(X/sub j/, Y/sub j/)=0 and natural numbers m/sub j/. The problem is to decide whether there is a rational function which has zeros at each point P/sub i/ of order n/sub i/, poles at each Q/sub j/ of order m/sub j/, and no zeros or poles anywhere else on C. One would also like to construct such a rational function if one exists. An efficient algorithm for solving this problem when the given plane curve has only ordinary multiple points is given.<<ETX>>
[natural numbers, smooth projective curve, Terminology, affine plane model, Mathematics, ordinary multiple points, poles and zeros, zeros and poles, Jacobian matrices, Computer science, intersecting curves, Elliptic curves, rational functions, plane curve, Polynomials, curve fitting, Jacobian, Poles and zeros, Riemann-Roch problem, computational complexity, number theory]
Interactive communication: balanced distributions, correlated files, and average-case complexity
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Suppose (X,Y) is a pair of random variables distributed over a support set S. Person P/sub x/ knows X, person P/sub y/ knows Y, and both know S. Using a predetermined protocol, they exchange binary messages in order for P/sub y/ to learn X. P/sub x/ may or may not learn Y. Bounds on communication complexity are obtained and used to obtain efficient protocols for the correlated files problem where X and Y are binary strings (files) within a small edit distance from each other. The average number of bits required for P/sub y/ to learn X when at most m messages are permitted is also determined.<<ETX>>
[Protocols, random variables, binary messages, Complexity theory, communication complexity, balanced distributions, average-case complexity, predetermined protocol, interactive communication, Random variables, protocols, correlated files, computational complexity]
Randomized multidimensional search trees: lazy balancing and dynamic shuffling
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A randomized technique, called dynamic shuffling, is given for multidimensional dynamic search. This technique, when specialized to the problem of searching in sorted lists, yields the previously known randomized binary trees (treaps). The crux of the technique is a multidimensional generalization of the rotation operation on binary search trees. Simultaneously, it is shown how to dynamize the randomized incremental algorithms so as to allow additions as well as deletions of objects. The techniques are based on remembering the history of the actual or imaginary sequence of updates. The techniques are applied to several problems in computational geometry.<<ETX>>
[Multidimensional systems, lazy balancing, sorted lists, trees (mathematics), dynamic shuffling, Binary search trees, computational geometry, Data structures, History, rotation operation, Computational geometry, Binary trees, sorting, Sampling methods, Polynomials, search problems, randomised multidimensional search trees, imaginary sequence]
Competitive algorithms for layered graph traversal
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A layered graph is a connected, weighted graph whose vertices are partitioned into sets L/sub 0/=(s), L/sub 1/, L/sub 2/, . . ., and whose edges run between consecutive layers. Its width is max( mod L/sub i/ mod ). In the online layered graph traversal problem, a searcher starts at s in a layered graph of unknown width and tries to reach a target vertex t; however, the vertices in layer i and the edges between layers i-1 and i are only revealed when the searcher reaches layer i-1. The authors give upper and lower bounds on the competitive ratio of layered graph traversal algorithms. They give a deterministic online algorithm that is O(9w)-competitive on width-w graphs and prove that for no w can a deterministic online algorithm have a competitive ratio better than 2w/sup -2/ on width-w graphs. They prove that for all w, w/2 is a lower bound on the competitive ratio of any randomized online layered graph traversal algorithm. For traversing layered graphs consisting of w disjoint paths tied together at a common source, they give a randomized online algorithm with a competitive ratio of O(log w) and prove that this is optimal up to a constant factor.<<ETX>>
[Algorithm design and analysis, weighted graph, Costs, searcher, competitive algorithms, Length measurement, computational geometry, upper bounds, Mathematics, Partitioning algorithms, lower bounds, Computer science, Shortest path problem, target vertex, Vents, deterministic online algorithm, layered graph traversal, search problems]
A linear time algorithm for triconnectivity augmentation
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The problem of finding the smallest set of edges whose addition triconnects an undirected graph is considered. This is a fundamental graph-theoretic problem that has applications in designing reliable networks and fault-tolerant computing. A linear time sequential algorithm is given for the problem. This is a substantial improvement over the best previous algorithm for this problem, which runs in O(n(n+m)/sup 2/) time on a graph with n vertices and m edges.<<ETX>>
[fault-tolerant computing, graph theory, vertices, edges, Phase change random access memory, Application software, Parallel algorithms, Fault tolerance, linear time sequential algorithm, Computer network reliability, triconnectivity augmentation, reliable networks, Computer networks, Polynomials, undirected graph, computational complexity]
Computing sums of radicals in polynomial time
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
For a certain sum of radicals the author presents a Monte Carlo algorithm that runs in polynomial time to decide whether the sum is contained in some number field Q( alpha ), and, if so, its coefficient representation in Q( alpha ) is computed. As a special case the algorithm decides whether the sum is zero. The main algorithm is based on a subalgorithm which is of interest in its own right. This algorithm uses probabilistic methods to check for an element beta of an arbitrary (not necessarily) real algebraic number field Q( alpha ) and some positive rational integer r whether there exists an rth root of beta in Q( alpha ).<<ETX>>
[Error probability, Monte Carlo algorithm, Traveling salesman problems, probabilistic checking, real algebraic number field, polynomial time algorithm, Radiofrequency interference, sums of radicals, subalgorithm, Computer science, Monte Carlo methods, positive rational integer, Algebra, decidability, coefficient representation, Polynomials, computational complexity, number theory]
Randomized multidimensional search trees: further results in dynamic sampling
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The use of randomization in dynamic search structures by means of a technique called dynamic sampling is investigated. In particular, an efficient algorithm for dynamic (logarithmic time) point location in 3-D partitions induced by a set of possibly interesting polygons in R/sup 3/ is given. The expected running time of the algorithm on a random sequence of updates is close to optimal. Efficient algorithms for dynamic nearest-k-neighbor queries and half space range queries in R/sup d/ are also given.<<ETX>>
[Algorithm design and analysis, dynamic sampling, half space range queries, Multidimensional systems, possibly interesting polygons, random sequence, Heuristic algorithms, trees (mathematics), dynamic nearest-k-neighbor queries, Binary search trees, computational geometry, Data structures, Partitioning algorithms, randomized multidimensional search trees, Computational geometry, running time, dynamic search structures, Sampling methods, 3-D partitions, Performance analysis, search problems, Random sequences, algorithm]
Exact learning of read-twice DNF formulas
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A polynomial-time algorithm is presented for exactly learning the class of read-twice DNF formulas, i.e. Boolean formulas in disjunctive normal form where each variable appears at most twice. The (standard) protocol used allows the learning algorithm to query whether a given assignment of Boolean variables satisfies the DNF formula to be learned (membership queries), as well as to obtain counterexamples to the correctness of its current hypothesis which can be any arbitrary DNF formula (equivalence queries). The formula output by the learning algorithm is logically equivalent to the formula to be learned.<<ETX>>
[Protocols, read-twice DNF formulas, Predictive models, equivalence queries, Educational institutions, Probability distribution, exact learning, learning systems, Computer science, membership queries, Boolean formulas, Boolean functions, protocol, disjunctive normal form, Telephony, Public key cryptography, Prediction algorithms, Polynomials, Inference algorithms, polynomial-time algorithm, computational complexity]
An asynchronous two-dimensional self-correcting cellular automaton
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Earlier work of P. Gacs and J. Reif (see J. Comput. Syst. Sci., vol.36, no.2, p.125-147 (1988)) on reliable computation using cellular automata is extended to asynchronous cellular automata. The goal is to find ways to implement computations of arbitrary size by a homogeneous asynchronous array of unreliable elementary components. An asynchronous two-dimensional cellular automaton is constructed so that given any computation and reliability requirement, a program can be found for such an automaton that performs the computation with probability that meets the reliability requirement. This is the strongest among the published results on reliable computation in an asynchronous environment. It is stronger than its asynchronous counterpart in the sense that it removes the assumption of a fault-free global synchronization clock underlying a synchronous system.<<ETX>>
[finite automata, asynchronous environment, Computational modeling, Lattices, probability, reliability, homogeneous asynchronous array, Synchronization, fault-free global synchronization clock, synchronisation, Concurrent computing, Computer science, Upper bound, arbitrary size, High performance computing, reliable computation, Automata, asynchronous two-dimensional self-correcting cellular automaton, Clocks]
Reliable computation with noisy circuits and decision trees-a general n log n lower bound
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Boolean circuits in which gates independently make errors with probability (at most) epsilon are considered. It is shown that the critical number crit(f) of a function f yields lower bound Omega (crit(f) log crit (f)) for the noisy circuit size. The lower bound is proved for an even stronger computational model, static Boolean decision trees with erroneous answers. A decision tree is static if the questions it asks do not depend on previous answers. The depth of such a tree provides a lower bound on the number of gates that depend directly on some input and hence on the size of a noisy circuit. Furthermore, it is shown that an Omega (n log n) lower bound holds for almost all Boolean n-input functions with respect to the depth of noisy dynamic decision trees. This bound is the best possible and implies that almost all n-input Boolean functions have noisy decision tree complexity Theta (n log n) in the static as well as in the dynamic case.<<ETX>>
[Error probability, decision theory, questions, answers, Boolean functions, noisy decision tree complexity, circuit reliability, Decision trees, noisy dynamic decision trees, Computational modeling, Redundancy, probability, trees (mathematics), lower bound, Circuit faults, critical number, Upper bound, noisy circuits, static Boolean decision trees, erroneous answers, computational model, tree depth, Circuit noise, Boolean circuits, logic circuits, computational complexity]
On ACC (circuit complexity)
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It has been shown by A. Yao (1990) that every language in ACC is recognized by a sequence of depth-2 probabilistic circuits with a symmetric gate at the root and n/sup polylog/(n) AND gates of fan-in polylog (n) at the leaves. The authors simplify Yao's proof and strengthen his results: every language in ACC is recognized by a sequence of depth-2 deterministic circuits with a symmetric gate at the root and n/sup polylog/(n) AND gates of fan-in polylog(n) at the leaves. They also analyze and improve modulus-amplifying polynomials constructed by S. Toda (1989) and Yao: this yields smaller circuits in Yao's and the present results on ACC.<<ETX>>
[ACC, formal languages, Circuit analysis computing, AND gates, fan-in, language, Complexity theory, threshold logic, Galois fields, modulus-amplifying polynomials, Computer science, Boolean functions, symmetric gate, depth-2 deterministic circuits, leaves, Wires, root, depth-2 probabilistic circuits, Polynomials, logic circuits, computational complexity]
A quadratic time algorithm for the minmax length triangulation
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is shown that a triangulation of a set of n points in the plane that minimizes the maximum edge length can be computed in time O(n/sup 2/). The algorithm is reasonably easy to implement and is based on the theorem that there is a triangulation with minmax edge length that contains the relative neighborhood graph of the points as a subgraph. With minor modifications the algorithm works for arbitrary normed metrics.<<ETX>>
[triangulation, maximum edge length, Minimax techniques, computational geometry, arbitrary normed metrics, minimax techniques, Computer science, Upper bound, minmax length triangulation, relative neighborhood graph, Approximation algorithms, quadratic time algorithm, Polynomials]
A unified geometric approach to graph separators
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A class of graphs called k-overlap graphs is proposed. Special cases of k-overlap graphs include planar graphs, k-nearest neighbor graphs, and earlier classes of graphs associated with finite element methods. A separator bound is proved for k-overlap graphs embedded in d dimensions. The result unifies several earlier separator results. All the arguments are based on geometric properties of embedding. The separator bounds come with randomized linear-time and randomized NC algorithms. Moreover, the bounds are the best possible up to the leading term.<<ETX>>
[Statistical analysis, Particle separators, graph theory, planar graphs, Very large scale integration, computational geometry, Partitioning algorithms, Complexity theory, Finite element methods, Computer science, Computational geometry, Image analysis, k-overlap graphs, Numerical analysis, k-nearest neighbor graphs, finite element methods, graph separators, geometric embedding, randomized NC algorithms, separator bound, randomized linear time algorithms, computational complexity]
Dynamic three-dimensional linear programming
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Linear programming optimizations on the intersection of k polyhedra in R/sup 3/, represented by their outer recursive decompositions, are performed in expected time O(k log k log n+ square root k log k log/sup 3/ n). This result is used to derive efficient algorithms for dynamic linear programming problems ill which constraints are inserted and deleted, and queries must optimize specified objective functions. As an application, an improved solution to the planar 2-center problem, is described.<<ETX>>
[Heuristic algorithms, dynamic three dimensional programming, expected time, outer recursive decompositions, objective functions, Linear programming, Data structures, linear programming, planar 2-center problem, Application software, Computer science, Constraint optimization, dynamic 3D linear programming, Dynamic programming, Time factors, computational complexity]
An approximation algorithm for the number of zeros or arbitrary polynomials over GF(q)
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The authors design the first polynomial time (for an arbitrary and fixed field GF(q)) ( in , delta )-approximation algorithm for the number of zeros of arbitrary polynomial f(x/sub 1/. . . x/sub n/) over GF(q). It gives the first efficient method for estimating the number of zeros and nonzeros of multivariate polynomials over small finite fields other than GF(2) (like GF(3)), the case important for various circuit approximation techniques. The algorithm is based on the estimation of the number of zeros of an arbitrary polynomial f(x/sub 1/. . .,x/sub n/) over GF(q) in the function of the number m of its terms. The bounding ratio is proved to be m/sup (q-1)/log/sup q/.<<ETX>>
[Algorithm design and analysis, approximation theory, Codes, polynomials, Circuits, Mathematics, multivariate polynomials, Application software, Galois fields, poles and zeros, polynomial time algorithm, zeros, bounding ratio, Computer science, finite fields, Algebra, approximation algorithm, Approximation algorithms, arbitrary polynomials, Polynomials, computational complexity]
Concentrated regular data streams on grids: sorting and routing near to the bisection bound
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Sorting and routing on r-dimensional n*. . .*n grids of processors is studied. Deterministic algorithms are presented for h-h problems, h>or=1, where each processor initially and finally contains h elements. It is shown that the classical 1-1 sorting can be solved with (2r-1.5)n+o(n) transport steps, i.e. in about 2.5n steps for r=2. The general h-h sorting problem, h>or=4r-4 can be solved within a number of transport steps that asymptotically differs by a factor of at most 3 from the trivial bisection bound. Furthermore, the bisection bound is asymptotically tight for sequences of h permutation routing problems, h=4cr, c>or=1, and for so-called offline routing.<<ETX>>
[permutation routing, multiprocessor interconnection networks, Routing, offline routing, bisection bound, deterministic algorithms, Sorting, h-h sorting, routing, sorting, 1-1 sorting, concentrated regular data streams, Indexing, computational complexity]
Communication complexity towards lower bounds on circuit depth
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
M. Karchmer et al. (1991) considered the circuit depth complexity of n-bit Boolean function constructed by composing up to d=log n/log log n levels of k=log-n-bit Boolean functions. Any such function is in AC/sup 1/. They conjecture that circuit depth is additive under composition, which would imply that any (bounded fan-in) circuit for this problem requires dk in Omega (log/sup 2/ n/log log n) depth. This would separate AC/sup 1/ from NC/sup 1/. They recommend using the communication game characterization of circuit depth. They suggest an intermediate problem which they call the universal composition relation. An almost optimal lower bound of dk-O(d/sup 2/(k log k)/sup 1/2/) is given for this problem. In addition, a proof, directly in terms of communication complexity, that there is a function on k bits requiring Omega (k) circuit depth is presented.<<ETX>>
[communication game characterization, Scholarships, Circuits, universal composition relation, almost optimal lower bound, Complexity theory, circuit depth, lower bounds, Computer science, Boolean functions, circuit depth complexity, Labeling, n-bit Boolean function, computational complexity]
Reporting points in halfspaces
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The author considers the halfspace range reporting problem: Given a finite set P of points in E/sup d/, preprocess it so that given a query halfspace gamma , the points of p intersection gamma can be reported efficiently. It is shown that, with almost linear storage, this problem can be solved substantially more efficiently than the more general simplex range searching problem. A data structure for halfspace range reporting in dimensions d>or=4 is given. It uses O(n log log n) space and O (n log n) deterministic preprocessing time. The query time is also given. Results for the halfspace emptiness problem, where one only wants to know whether P intersection gamma is empty, are also presented.<<ETX>>
[linear storage, Filtering, data structure, query time, computational geometry, Data structures, Mathematics, Computational geometry, halfspace range reporting, query halfspace, simplex range searching problem, Slabs, deterministic preprocessing time, search problems]
Search problems in the decision tree model
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The relative power of determinism, randomness, and nondeterminism for search problems in the Boolean decision tree model is studied. It is shown that the CNF search problem is complete for all the variants of decision trees. It is then shown that the gaps between the nondeterministic, the randomized, and the deterministic complexities can be arbitrarily large for search problems. The special case of nondeterministic complexity is discussed.<<ETX>>
[decision theory, Computational modeling, trees (mathematics), Search problems, randomness, Electronic mail, Boolean algebra, CNF search, Boolean functions, Measurement standards, complexities, Polynomials, determinism, Decision trees, Probes, search problems, Boolean decision tree model, computational complexity, nondeterminism]
On selecting a satisfying truth assignment
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The complexity of certain natural generalizations of satisfiability, in which one of the possibly exponentially many satisfying truth assignments must be selected, is studied. Two natural selection criteria, default preference and minimality (circumscription), are considered. The thrust of the complexity results seems to be that hard problems become harder, while easy problems remain easy. This consideration yields as a byproduct a new and very natural polynomial-time randomized algorithm for 2SAT.<<ETX>>
[circumscription, complexity, default preference, 2SAT, satisfying truth assignment selection, Computer science, natural generalizations, satisfiability, Tail, Hypercubes, Polynomials, selection criteria, polynomial-time randomized algorithm, minimality, computational complexity]
Self-stabilization by local checking and correction
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The first self-stabilizing end-to-end communication protocol and the most efficient known self-stabilizing network reset protocol are introduced. A simple method of local checking and correction, by which distributed protocols can be made self-stabilizing without the use of unbounded counters, is used. The self-stabilization model distinguishes between catastrophic faults that abstract arbitrary corruption of global state, and other restricted kinds of anticipated faults. It is assumed that after the execution starts there are no further catastrophic faults, but the anticipated faults may continue to occur.<<ETX>>
[Educational programs, Protocols, Stability, network reset protocol, distributed protocols, Laboratories, distributed processing, end-to-end communication protocol, Computer crashes, Distributed computing, local checking, Computer science, Fault tolerance, Network topology, catastrophic faults, self stabilisation, fault tolerant computing, protocols, correction, Contracts]
Lower bounds for the complexity of reliable Boolean circuits with noisy gates
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is proved that the reliable computation of any Boolean function with, sensitivity s requires Omega (s log s) gates if the gates of the circuit fail independently with a fixed positive probability. The Omega (s log s) bound holds even if s is the block sensitivity instead of the sensitivity of the Boolean function. Some open problems are mentioned.<<ETX>>
[complexity, reliable Boolean circuits, Error probability, Redundancy, fixed positive probability, Boolean function, noisy gates, lower bounds, Computer science, Boolean functions, Upper bound, logic gates, circuit reliability, block sensitivity, Circuit noise, logic circuits, Signal to noise ratio, computational complexity]
Variation ranks of communication matrices and lower bounds for depth two circuits having symmetric gates with unbounded fan-in
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
An exponential lower bound for depth two circuits with arbitrary symmetric gates in the bottom level and with a MOD/sub m/-gate in the top level is proved. This solves a problem posed by R. Smolensky (1990). The method uses the variation rank of communication matrices. A variant of this method is used for deriving lower bounds for the size of depth-two circuits having a threshold gate at the top.<<ETX>>
[Symmetric matrices, communication matrices, variation rank, Computational modeling, threshold logic, Circuit testing, Zinc, exponential lower bound, matrix algebra, Concurrent computing, logic gates, Wires, threshold gate, unbounded fan-in, symmetric gates, Polynomials, depth two circuits, MOD m gate, logic circuits, computational complexity]
Fully parallelized multi prover protocols for NEXP-time
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A major open problem in the theory of multiprover protocols is to characterize the languages which can be accepted by fully parallelized protocols which achieve an exponentially low probability of cheating in a single round. The problem was motivated by the observation that the probability of cheating the n parallel executions of a multiprover protocol can be exponentially higher than the probability of cheating in n sequential executions of the same protocol. The problem is solved by proving that any language in NEXP-time has a fully parallelized multiprover protocol. By combining this result with a fully parallelized version of the protocol of M. Ben-Or et al. (ACM Symp. on Theory of Computing, 1988), a one-round perfect zero-knowledge protocol (under no cryptographic assumptions) can be obtained for every NEXPTIME language.<<ETX>>
[formal languages, NEXP-time, Mathematics, Iron, fully parallelized multi prover protocols, language, parallel executions, Cryptographic protocols, one-round perfect zero-knowledge protocol, Polynomials, theorem proving, Cryptography, protocols, computational complexity]
Fat triangles determine linearly many holes (computational geometry)
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is shown that for every fixed delta >0 the following holds: if F is a union of n triangles, all of whose angles are at least delta , then the complement of F has O(n) connected components, and the boundary of F consists of O(n log log n) segments. This latter complexity becomes linear if all triangles are of roughly the same size or if they are all infinite wedges. A randomized algorithm that computes F in expected time O(n2/sup alpha (n)/ log n) is given. Several applications of these results are presented.<<ETX>>
[Computational geometry, complexity, Upper bound, connected components, computational geometry, Mathematics, fat triangles, linearly many holes, randomized algorithm, Research and development]
Asymptotically optimal PRAM emulation on faulty hypercubes
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A scheme for emulating the parallel random access machine (PRAM) on a faulty hypercube is presented. All components of the hypercube, including the memory modules, are assumed to be subject to failure. The faults may occur at any time during the emulation and the system readjusts dynamically. The scheme, which rests on L.G. Valiant's BSP model (1990), is the first to achieve optimal and work-preserving PRAM emulation on a dynamically faulty network.<<ETX>>
[memory modules, dynamically faulty network, random-access storage, Random access memory, Phase change random access memory, Routing, hypercube networks, faulty hypercubes, parallel machines, asymptotically optimal PRAM emulation, Computer science, Degradation, Fault tolerance, Emulation, Computer architecture, Hypercubes, fault tolerant computing, parallel random access machine, Large-scale systems]
Simulating BPP using a general weak random source
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is shown how to simulate BPP and approximation algorithms in polynomial time using the output from a delta -source. A delta -source is a weak random source that is asked only once for R bits, and must output an R-bit string according to some distribution that places probability no more than 2/sup - delta R/ on any particular string. Also given are two applications: one to show the difficulty of approximating the size of the maximum clique, and the other to the problem of implicit O(1) probe search.<<ETX>>
[polynomials, Entropy, approximation algorithms, probe search, Application software, History, maximum clique, Diodes, Computer science, R-bit string, general weak random source, Physics computing, Approximation algorithms, file organisation, weak random source, Polynomials, polynomial time, Probes, search problems, BPP simulation, Clocks, computational complexity]
A general approach to removing degeneracies
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Algorithms modeled as algebraic branching programs, with inputs from an infinite ordered field, are studied. Direct perturbations on the input, so that an algorithm designed under the assumption of nondegeneracy can be applied to all inputs, are described. A deterministic method for algorithms with determinant tests and a randomized one for arbitrary test expressions are defined. They both incur extra complexity factors that are constant in several cases. Moreover, polynomial and exponential time algorithms always remain in the same complexity class while being enhanced with the power to execute on arbitrary inputs. Both methods are distinguished by their conceptual elegance and are significantly faster than previous ones.<<ETX>>
[Algorithm design and analysis, direct perturbations, arbitrary test expressions, Input variables, Computational modeling, exponential time algorithms, complexity class, Binary decision diagrams, deterministic method, degeneracies, infinite ordered field, determinant tests, Computer science, complexity factors, Perturbation methods, algebraic branching programs, polynomial time algorithms, Euclidean distance, nondegeneracy, Polynomials, Computational efficiency, Testing, computational complexity]
Shrinkage of de Morgan formulae under restriction
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is shown that a random restriction leaving only a fraction in of the input variables unassigned reduces the expected de Morgan formula size of the induced function by a factor of O( in /sup 1.63/). This is an improvement over previous results. The new exponent yields an increased lower bound of approximately n/sup 2.63/ for the de Morgan formula size of a function in P defined by A.E. Andreev (1987). This is the largest lower bound known, even for functions in NP.<<ETX>>
[Computer science, formal logic, NP, Input variables, trees (mathematics), induced function, Mathematics, de Morgan formulae, lower bound, random restriction, Contracts]
Low contention linearizable counting
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The linearizable counting problem requires asynchronous concurrent processes to assign themselves successive values so that the order of the values assigned reflects the real-time order in which they were requested. It is shown that the problem can be solved without funneling all processes through a common memory location. Two new constructions for linearizable counting networks, data structures that solve the linearizable counting problem, are given. The first construction is nonblocking: some process takes a value after O(n) network gates have been traversed. The second construction is wait-free: it guarantees that each process takes a value after it traverses O(wn) gates, where w is a parameter affecting contention. It is shown that in any nonblocking or wait-free linearizable counting network, processes must traverse an average of Omega (n) gates, and so the constructions are close to optimal. A simpler and more efficient network is constructed by giving up the robustness requirements and allowing processes to wait for one another.<<ETX>>
[Heart, parallel algorithms, Data structures, linearizable counting networks, nonblocking construction, asynchronous concurrent processes, parallel programming, Sorting, Counting circuits, Computer science, Concurrent computing, real-time order, Wires, wait free construction, Robustness, Hardware, data structures, Contracts, network gates, computational complexity]
Polynomial algorithms for LP over a subring of the algebraic integers with applications to LP with circulant matrices
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is shown that a modified variant of the interior point method can solve linear programs (LPs) whose coefficients are real numbers from a subring of the algebraic integers. By defining the encoding size of such numbers to be the bit size of the integers that represent them in the subring, it is proved that the modified algorithm runs in time polynomial in the encoding size of the input coefficients, the dimension of the problem, and the order of the subring. The Tardos scheme is then extended to this case, yielding a running time that is independent of the objective and right-hand side data. As a consequence of these results, it is shown that LPs with real circulant coefficient matrices can be solved in strongly polynomial time. It is also shown how the algorithm can be applied to LPs whose coefficients belong to the extension of the integers by a fixed set of square roots.<<ETX>>
[Operations research, linear programming, Ellipsoids, polynomial algorithms, running time, Turing machines, Tardos scheme, circulant matrices, Polynomials, subring, encoding size, square roots, Computational modeling, strongly polynomial time, Industrial engineering, Linear programming, interior point method, Encoding, Vectors, algebraic integers, matrix algebra, Character generation, bit size, real numbers, computational complexity]
On the complexity of computing the homology type of a triangulation
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
An algorithm for computing the homology type of a triangulation is analyzed. By triangulation is meant a finite simplicial complex; its homology type is given by its homology groups (with integer coefficients). The algorithm could be used in computer-aided design to tell whether two finite-element meshes or Bezier-spline surfaces are of the same topological type, and whether they can be embedded in R/sup 3/. Homology computation is a pure combinatorial problem of considerable intrinsic interest. While the worst-case bounds obtained for this algorithm are poor, it is argued that many triangulations (in general) and virtually all triangulations in design are very sparse in a particular sense. This sparseness measure is formalized, and a probabilistic analysis of the sparse case is performed to show that the expected running time, of the algorithm is roughly quadratic in the geometric complexity (number of simplices) and linear in the dimension.<<ETX>>
[Algorithm design and analysis, Process design, Solid modeling, Design automation, integer coefficients, worst-case bounds, computational geometry, topological type, probabilistic analysis, Finite element methods, quadratic expected running time, geometric complexity, Performance analysis, Embedded computing, homology triangulation, Robot vision systems, computer-aided design, Bezier-spline surfaces, Computer science, sparseness measure, Machine vision, finite-element meshes, finite simplicial complex, linear expected running time, simplices, homology groups, computational complexity]
Scheduling parallel machines on-line
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The authors study the problem of scheduling jobs on parallel machines when the existence of a job is not known until an unknown release date and the processing requirement of a job is not known until the job is processed to completion. They demonstrate two general algorithmic techniques for converting existing polynomial-time algorithms that require complete knowledge about the input data into algorithms that need less advance knowledge. They prove information-theoretic lower bounds on the lengths of online schedules for several basic parallel machine models and then show that the algorithms construct schedules with lengths that either match or come within a constant factor of the lower bounds.<<ETX>>
[algorithmic techniques, information-theoretic lower bounds, online scheduling, Parallel machines, performance evaluation, Sun, parallel machines, Uninterruptible power systems, Scheduling algorithm, scheduling, polynomial-time algorithms, Polynomials, Marine vehicles, Contracts]
Adaptive dictionary matching
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Semiadaptive and fully adaptive dictionary matching algorithms are presented. In the fully adaptive algorithm, the dictionary is processed in time O( mod D mod log mod D mod ). Inserting a new pattern P/sub k+1/ into the dictionary can be done in time O mod P/sub K+1/ mod log mod D mod ). A dictionary pattern can be deleted in time O(log mod D mod ). Text scanning is accomplished in time O( mod T mod log mod D mod ). Also presented is a parallel version of the algorithm with optimal speedup for the dictionary construction and pattern addition phase and a logarithmic overhead in the text scan phase. The method used incorporates a new way of using suffix trees as well as a new data structure in which the suffix tree is embedded for the sequential algorithm.<<ETX>>
[Tree data structures, sequential algorithm, parallel algorithms, Dictionaries, Sequences, programming theory, Hamming distance, pattern deletion, data structure, pattern insertion, Educational institutions, Data structures, adaptive dictionary matching algorithms, dictionary pattern, Parallel algorithms, logarithmic overhead, Concurrent computing, Computer science, text scanning, suffix trees, dictionary construction, data structures, Pattern matching, computational complexity]
Efficient exponentiation in finite field
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Optimal sequential and parallel algorithms for exponentiation in a finite field extension are presented, assuming that a normal basis over the ground field is given.<<ETX>>
[parallel algorithms, Circuits, Natural languages, ground field, Galois fields, Parallel algorithms, Concurrent computing, Computer science, exponentiation, normal basis, Councils, finite field, optimal sequential algorithms, Cryptography, finite field extension, Arithmetic]
Towards a theory of nearly constant time parallel algorithms
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is demonstrated that randomization is an extremely powerful tool for designing very fast and efficient parallel algorithms. Specifically, a running time of O(lg* n) (nearly-constant), with high probability, is achieved using n/lg* n (optimal speedup) processors for a wide range of fundamental problems. Also given is a constant time algorithm which, using n processors, approximates the sum of n positive numbers to within an error which is smaller than the sum by an order of magnitude. A variety of known and new techniques are used. New techniques, which are of independent interest, include estimation of the size of a set in constant time for several settings, and ways for deriving superfast optimal algorithms from superfast nonoptimal ones.<<ETX>>
[Tree data structures, Algorithm design and analysis, parallel algorithms, Dictionaries, positive numbers, Computational modeling, Nominations and elections, nearly constant time parallel algorithms, Phase change random access memory, Parallel algorithms, Gas insulated transmission lines, Concurrent computing, estimation, running time, superfast optimal algorithms, randomization, Load management, computational complexity]
Amortized communication complexity
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The authors study the direct sum problem with respect to communication complexity: Consider a function f: D to (0, 1), where D contained in (0, 1)/sup n/*(0, 1)/sup n/. The amortized communication complexity of f, i.e. the communication complexity of simultaneously computing f on l instances, divided by l is studied. The authors present, both in the deterministic and the randomized model, functions with communication complexity Theta (log n) and amortized communication complexity O(1). They also give a general lower bound on the amortized communication complexity of any function f in terms of its communication complexity C(f).<<ETX>>
[Computer science, Context, Protocols, Costs, Boolean functions, Circuits, amortised communication complexity, Chromium, direct sum problem, Complexity theory, lower bound, computational complexity]
Dynamic maintenance of geometric structures made easy
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The problem of dynamically maintaining geometric structures is considered. A technique is proposed that uses randomized incremental algorithms which are augmented to allow deletions of objects. A model for distributions on the possible input sequences of insertions and deletions is developed and analyzed using R. Seidel's backwards analysis. It is further shown how to apply this to maintain Voronoi diagrams, convex hulls, and planar subdivisions. A strikingly simple algorithm for the maintenance of convex hulls in any dimension is given. The expected running time is determined.<<ETX>>
[Algorithm design and analysis, randomized incremental algorithms, Voronoi diagrams, insertions, dynamic maintenance, deletions, computational geometry, Data structures, geometric structures, convex hulls, planar subdivisions, Computational geometry, backwards analysis, Artificial intelligence, Contracts]
How to learn an unknown environment
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The authors consider the problem faced by a newborn that must explore and learn an unknown room with obstacles in it. They seek algorithms that achieve a bounded ratio of the worst-case distance traversed in order to see all visible points of the environment (thus creating a map), divided by the optimum distance needed to verify the map. The situation is complicated by the fact that the latter offline problem (optimally verifying a map) is NP-hard and thus must be solved approximately. Although the authors show that there is no such competitive algorithm for general obstacle courses, they give a competitive algorithm for the case of a polygonal room with a bounded number of obstacles in it.<<ETX>>
[Pediatrics, Solid modeling, NP-hard, unknown environment, computational geometry, learning, worst-case distance, polygonal room, learning systems, Geometry, Computer science, offline problem, Cities and towns, Polynomials, competitive algorithm, bounded ratio, general obstacle courses, computational complexity]
Finding the hidden path: time bounds for all-pairs shortest paths
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The all-pairs shortest paths problem in weighted graphs is investigated. An algorithm called the hidden paths algorithm, which finds these paths in time O(m*+n n/sup 2/ log n), where m* is the number of edges participating in shortest paths, is presented. It is argued that m* is likely to be small in practice, since m*=O(n log n) with high probability for many probability distributions on edge weights. An Omega (mn) lower bound on the running time of any path-comparison-based algorithm for the all-pairs shortest paths problem is proved.<<ETX>>
[all-pairs shortest paths, Change detection algorithms, graph theory, time bounds, path-comparison-based algorithm, Probability distribution, lower bound, hidden paths algorithm, edge weights, Shortest path problem, Computer science, weighted graphs, computational complexity]
A lower bound for the dictionary problem under a hashing model
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A fundamental open question in data structures concerns the existence of a dictionary data structure that processes the operations in constant amortized time and uses space polynomial in the dictionary size. The complexity of the dictionary problem is studied under a multilevel hashing model that is based on A.C. Yao's (1981) cell probe model, and it is proved that dictionary operations require log-algorithmic amortized time jn this model. The model encompasses many known solutions to the dictionary problem, and the result is the first nontrivial lower bound for the problem in a reasonably general model that takes into account the limited wordsize of memory locations and realistically measures the cost of update operations. This lower bound separates the deterministic and randomized complexities of the problem under this model.<<ETX>>
[Dictionaries, Costs, deterministic complexities, polynomial space, Random access memory, nontrivial lower bound, Read-write memory, Data structures, multilevel hashing model, Partitioning algorithms, log-algorithmic amortized time, memory locations, Computer science, update costs, dictionary problem, constant amortized time, randomized complexities, cell probe model, file organisation, Polynomials, data structures, limited wordsize, Probes, computational complexity]
On-line maintenance of the four-connected components of a graph
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Given a graph G with n vertices and m edges, a k-connectivity query for vertices v' and v" of G asks whether there exist k disjoint paths between v' and v". The authors consider the problem of performing k-connectivity queries for k<or=4. First, they present a static data structure that answers such queries in O(1) time. Next, they consider the problem of performing queries intermixed with online updates that insert vertices and edges. For triconnected graphs they give a dynamic data structure that supports queries and updates in time O( alpha (l,n)) amortized, where n is the current number of vertices of the graph and l is the total number of operations performed ( alpha (l, n) denotes the slowly growing Ackermann function inverse). For general graphs, a sequence of l operations takes total time O(n log n+l). All of the above data structures use space O(n), proportional to the number of vertices of the graph. The results also yield an efficient algorithm for testing whether graph G is four-connected that runs in O(n alpha (n, n)+m) time using O(n+m) space.<<ETX>>
[programming theory, four-connected components, graph theory, vertices, dynamic data structure, k-connectivity query, edges, Ackermann function inverse, Data structures, online updates, graph, Computer science, Fault tolerance, Space technology, Councils, triconnected graphs, data structures, disjoint paths, Communication networks, Contracts, static data structure, Testing, computational complexity]
An optimal convex hull algorithm and new results on cuttings
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
An optimal algorithm for computing hyperplane cuttings is given. It results in a new kind of cutting, which enjoys all the properties of the previous ones and, in addition, can be refined by composition. An optimal algorithm for computing the convex hull of a finite point set in any fixed dimension is also given.<<ETX>>
[Computer science, Bridges, Identity-based encryption, Additives, optimal convex hull algorithm, composition, computational geometry, Polynomials, convex hull, hyperplane cuttings]
Discrepancy and in -approximations for bounded VC-dimension
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Let (X, R) be a set system on an n-point set X. For a two-coloring on X, its discrepancy is defined as the maximum number by which the occurrences of the two colors differ in any set in R. It is shown that if for any m-point subset Y contained in X the number of distinct subsets induced by R on Y is bounded by O(m/sup d/) for a fixed integer d is a coloring with discrepancy bounded by O(n/sup 1/2-1/2d/ (log n)/sup 1+1/2d/). Also, if any subcollection of m sets of R partitions the points into at most O(m/sup d/) classes, then there is a coloring with discrepancy at most O(n/sup 1/2-1/2d/ n). These bounds imply improved upper bounds on the size of in -approximations for (X, R). All of the bounds are tight up to polylogarithmic factors in the worst case. The results allow the generalization of several results of J. Beck (1984) bounding the discrepancy in certain geometric settings to the case when the discrepancy is taken relative to an arbitrary measure.<<ETX>>
[approximation theory, geometric settings, polylogarithmic factors, computational geometry, upper bounds, Extraterrestrial measurements, Mathematics, bounded VC-dimension, Computational geometry, Upper bound, two-coloring, discrepancy, Polynomials, in -approximations, Books, Contracts, m-point subset]
Fast approximation algorithms for fractional packing and covering problems
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Fast algorithms that find approximate solutions for a general class of problems, which are called fractional packing and covering problems, are presented. The only previously known algorithms for solving these problems are based on general linear programming techniques. The techniques developed greatly outperform the general methods in many applications, and are extensions of a method previously applied to find approximate solutions to multicommodity flow problems. The algorithms are based on a Lagrangian relaxation technique, and an important result is a theoretical analysis of the running time of a Lagrangian relaxation based algorithm. Several applications of the algorithms are presented.<<ETX>>
[Algorithm design and analysis, approximation theory, Job shop scheduling, Parallel machines, Linear programming, linear programming, approximation algorithms, multicommodity flow problems, Sun, Lagrangian functions, Uninterruptible power systems, running time, Lagrangian relaxation, operations research, Approximation algorithms, Cost function, fractional covering, fractional packing, Contracts, computational complexity]
Communication complexity for parallel divide-and-conquer
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The relationship between parallel computation cost and communication cost for performing divide-and-conquer (D&C) computations on a parallel system of p processors is studied. The parallel computation cost is the maximal number of the D&C nodes that any processor in the parallel system may expand, whereas the communication cost is the total number of cross nodes (nodes generated by one processor but expanded by another processor). A scheduling algorithm is proposed, and lower bounds on the communication cost are derived. The proposed scheduling algorithm is optimal with respect to the communication cost, since the parallel computation cost of the algorithm is near optimal.<<ETX>>
[parallel algorithms, Costs, parallel system, communication cost, Complexity theory, scheduling algorithm, parallel computation cost, Scheduling algorithm, lower bounds, Sorting, Concurrent computing, Computer science, Processor scheduling, cross nodes, parallel divide-and-conquer, Load management, Computational efficiency, Contracts, computational complexity]
Distributed program checking: a paradigm for building self-stabilizing distributed protocols
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The notion of distributed program checking as a means of making a distributed algorithm self-stabilizing is explored. A compiler that converts a deterministic synchronous protocol pi for static networks into a self-stabilizing version of pi for dynamic networks is described. If T/sub pi / is the time complexity of pi and D is a bound on the diameter of the final network, the compiled version of pi stabilizes in time O(D+T/sub pi /) and has the same space complexity as pi . The general method achieves efficient results for many specific noninteractive tasks. For instance, solutions for the shortest paths and spanning tree problems take O(D) to stabilize, an improvement over the previous best time of O(D/sup 2/).<<ETX>>
[Protocols, Costs, self-stabilizing distributed protocols, program verification, Laboratories, deterministic synchronous protocol, distributed processing, time complexity, Computer crashes, Topology, spanning tree problems, Computer science, Fault tolerance, shortest paths, protocols, compiler, Distributed algorithms, Springs, Contracts, space complexity, noninteractive tasks, computational complexity, distributed program checking]
A parallel algorithmic version of the local lemma
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The Lovasz local lemma (1975) is a tool that enables one to show that certain events hold with positive, though very small probability. It often yields existence proofs of results without supplying any efficient way of solving the corresponding algorithmic problems. J. Beck has recently found a method for converting some of these existence proofs into efficient algorithmic procedures, at the cost of losing a little in the estimates, but his method does not seem to be parallelizable. His technique is modified to achieve an algorithmic version that can be parallelized, thus providing deterministic NC/sup 1/ algorithms for various interesting algorithmic search problems.<<ETX>>
[positive probability, parallel algorithms, Costs, graph theory, H infinity control, Search problems, Mathematics, Microwave integrated circuits, deterministic NC/sup 1/ algorithms, algorithmic search problems, Polynomials, Artificial intelligence, existence proofs, search problems, computational complexity, Lovasz local lemma]
Quantifying knowledge complexity
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Several alternative ways of defining knowledge complexity are presented, and the relationships between them are explored. The discussion covers inclusion results, separation results, properties of knowledge complexity of languages in the Hint sense, and the knowledge complexity of constant round AM proofs.<<ETX>>
[Protocols, inclusion results, proof system, languages, Natural languages, Probability distribution, Complexity theory, Computer science, knowledge complexity, constant round AM proofs, Gain measurement, Hint sense, Polynomials, theorem proving, separation results, computational complexity]
Fault-tolerant computation in the full information model
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Efficient two-party protocols for fault-tolerant computation of any two-argument function are presented. It is proved that the influence of a dishonest player in these protocols is the minimum one possible (up to polylogarithmic factors). Also presented are efficient m-party fault-tolerant protocols for sampling a general distribution (m>or=2). Efficient m-party protocols for computation of any m-argument function are given, and it is proved for these protocols that for most functions, the influence of any t dishonest players on the outcome of the protocol is the minimum one possible (up to polylogarithmic factors).<<ETX>>
[Protocols, Costs, Computational modeling, full information model, Laboratories, polylogarithmic factors, sampling, fault tolerant computation, m-party fault-tolerant protocols, m-argument function, Distributed computing, Computer science, Fault tolerance, two-argument function, Sampling methods, fault tolerant computing, protocols, Joining processes, two-party protocols, Erbium]
Checking the correctness of memories
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The notion of program checking is extended to include programs that alter their environment, in particular, programs that store and retrieve data from memory. The model considered allows the checker a small amount of reliable memory. The checker is presented with a sequence of requests (online) to a data structure which must reside in a large but unreliable memory. The data structure is viewed as being controlled by an adversary. The checker is to perform each operation in the input sequence using its reliable memory and the unreliable data structure so that any error in the operation of the structure will be detected by the checker with high probability. Checkers for various data structures are presented. Lower bounds of log n on the amount of reliable memory needed by these checkers, where n is the size of the structure, are proved.<<ETX>>
[program verification, Random access memory, probability, data structure, Read-write memory, Data structures, Information retrieval, reliable memory, program checking, lower bounds, Computer science, sequence of requests, Turing machines, Software protection, memories correctness checking, data structures]
Size-depth tradeoffs for algebraic formulae
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Some tradeoffs between the size and depth of algebraic formulas are proved. It is shown that, for any fixed in >0, any algebraic formula of size S can be converted into an equivalent formula of depth O(log S) and size O(S/sup 1+ in /). This result is an improvement over previously known results where, to obtain the same depth bound, the formula size is Omega (S/sup alpha /), with alpha >or=2.<<ETX>>
[Computer science, Boolean functions, algebraic formulae, Councils, Circuits, Polynomials, size depth tradeoffs, Electronic mail, Boolean algebra, Parallel algorithms]
On the computational power of sigmoid versus Boolean threshold circuits
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The power of constant depth circuits with sigmoid (i.e., smooth) threshold gates for computing Boolean functions is examined. It is shown that, for depth 2, constant size circuits of this type are strictly more powerful than constant size Boolean threshold circuits (i.e., circuits with Boolean threshold gates). On the other hand it turns out that, for any constant depth d, polynomial size sigmoid threshold circuits with polynomially bounded weights compute exactly the same Boolean functions as the corresponding circuits with Boolean threshold gates.<<ETX>>
[Boolean threshold circuits, Circuits, threshold logic, smooth threshold gates, Concurrent computing, Boolean functions, Boolean threshold gates, Computer networks, Polynomials, depth 2 circuits, polynomially bounded weights, Computational modeling, Neurons, Analog computers, polynomial size sigmoid threshold circuits, logic gates, Neural networks, sigmoid threshold gates, Information processing, constant size circuits, constant depth circuits, computational power, logic circuits, neural nets, computational complexity]
Explicit construction of natural bounded concentrators
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The first known direct construction for linear families of bounded concentrators is given. The construction is explicit, and the results are simple natural bounded concentrators.<<ETX>>
[Computer science, natural bounded concentrators, switching networks, linear families, Mathematics, Graph theory, explicit construction, Bipartite graph, Joining processes, Sorting, line concentrators, direct construction]
Using approximation algorithms to design parallel algorithms that may ignore processor allocation
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A framework is presented for designing parallel algorithms that may ignore processor allocation. A number of fast approximation algorithms are developed, and it is shown how to use these algorithms to simulate any algorithm that fits this framework in a work-preserving fashion on a randomized CRCW PRAM. Several applications of the approach to parallel computational geometry are given.<<ETX>>
[Algorithm design and analysis, approximation theory, parallel algorithms, Costs, Computational modeling, computational geometry, Phase change random access memory, approximation algorithms, processor allocation, Parallel algorithms, Concurrent computing, Computer science, Computational geometry, Approximation algorithms, parallel computational geometry, Artificial intelligence, randomized CRCW PRAM]
Walking an unknown street with bounded detour
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A polygon with two distinguished vertices, s and g, is called a street if the two boundary chains from s to g are mutually weakly visible. For a mobile robot with onboard vision, a strategy for finding a short path from s to g in a street not known in advance is described, and it is proved that the length of the path created does not exceed 1+3 pi /2 times the length of the shortest path from s to g. Experiments suggest that the strategy is much better than this, as no ratio bigger than 1.8 has yet been observed. This is complemented by a lower bound of 1.41 for the relative detour each strategy can be forced to generate.<<ETX>>
[Legged locomotion, onboard vision, Resumes, vertices, boundary chains, computational geometry, mobile robot, mobile robots, lower bound, Mobile robots, Computational geometry, Machine vision, Robot kinematics, polygon, mutually weakly visible, short path, Layout, Tactile sensors, computer vision, Robot sensing systems, Joining processes, bounded detour]
Approximating clique is almost NP-complete
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The computational complexity of approximating omega (G), the size of the largest clique in a graph G, within a given factor is considered. It is shown that if certain approximation procedures exist, then EXPTIME=NEXPTIME and NP=P.<<ETX>>
[almost NP-complete, approximation procedures, Protocols, Upper bound, Approximation algorithms, Polynomials, approximating clique, Data mining, Computational complexity, graph, Testing, computational complexity]
Lower bounds for data structure problems on RAMs
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A technique is described for deriving lower bounds and tradeoffs for data structure problems. Two quantities are defined. The output variability depends only on the model of computation. It characterizes in some sense the power of a model. The problem variability depends only on the problem under consideration. It characterizes in some sense the difficulty of the problem. The first theorem states that if a model's output variability is smaller than the problem variability, a lower bound on the worst case (average case) time for the problem follows. A RAM that can add, subtract and compare unbounded integers is considered. The second theorem gives an upper bound on the output variability of this model. The two theorems are used to derive lower bounds for the union-find problem in this RAM.<<ETX>>
[data structure problems, Computational modeling, average case time, Buildings, Random access memory, Read-write memory, Length measurement, Data structures, upper bound, lower bounds, union-find problem, Upper bound, RAMs, problem variability, unbounded integers, Cost function, output variability, data structures, Probes, worst case time, computational complexity]
Computing planar intertwines
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The proof of Wagner's conjecture by N. Robertson and P. Seymour gives a finite description of any family of graphs which is closed under the minor ordering, called the obstructions of the family. Since the intersection and the union of two minor closed graph families are again a minor closed graph family, an interesting question is that of computing the obstructions of the new family given the obstructions for the original two families. It is easy to compute the obstructions of the intersection, but, until very recently, it was an open problem to compute the obstructions of the union. It is shown that if the original families are planar, then the obstructions of the union are no larger than n to the O(n/sup 2/) power, where n is the size of the largest obstruction of the original family.<<ETX>>
[Computer science, planar intertwines, minor ordering, Embedded computing, Upper bound, graph family obstructions, graph theory, minor closed graph families, Polynomials, Combinatorial mathematics, Testing, computational complexity]
The art gallery theorem for polygons with holes
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Art gallery problems which have been extensively studied over the last decade ask how to station a small (minimum) set of guards in a polygon such that every point of the polygon is watched by at least one guard. The graph-theoretic formulation and solution to the gallery problem for polygons in standard form is given. A complexity analysis is carried out, and open problems are discussed.<<ETX>>
[Algorithm design and analysis, graph-theoretic formulation, open problems, Art, Upper bound, Shape, Subspace constraints, complexity analysis, Watches, computational geometry, polygons with holes, art gallery theorem]
On better heuristic for Euclidean Steiner minimum trees
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Finding a shortest network interconnecting a given set of points in the Euclidean plane (a Steiner minimum tree) is known to be NP-hard. It is shown that there exists a polynomial-time heuristic with a performance ratio bigger than square root 3/2.<<ETX>>
[Steiner trees, Operations research, NP-hard, Surface-mount technology, Euclidean Steiner minimum trees, trees (mathematics), computational geometry, Extraterrestrial measurements, Mathematics, polynomial-time heuristic, Computer science, NP-hard problem, shortest network, Approximation algorithms, Polynomials, Civil engineering, computational complexity]
The maintenance of common data in a distributed system
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A basic task in distributed computation is the maintenance at each processor of the network, of a current and accurate copy of a common database. Such a database must be updated in the wake of locally generated changes to its contents. Due to previous disconnections of parts of the network, a maintenance protocol may need to update processors holding widely varying versions of the database. A deterministic protocol, which has only polylogarithmic overhead in its time and communication complexities, is provided for this problem. Previous deterministic solutions required polynomial overhead in at least one of these measures.<<ETX>>
[polylogarithmic overhead, Protocols, distributed system, Mathematics, Complexity theory, communication complexity, Distributed computing, deterministic protocol, Databases, Network topology, common data maintenance, distributed databases, Broadcasting, protocols, locally generated changes, Contracts, computer networks, time complexity, Routing, distributed computation, software maintenance, database theory, Computer science, common database, maintenance protocol, computational complexity]
Subquadratic zero-knowledge
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The communication complexity of zero-knowledge proof systems is improved. Let C be a Boolean circuit of size n. Previous zero-knowledge proof systems for the satisfiability of C require the use of Omega (kn) bit commitments in order to achieve a probability of undetected cheating not greater than 2/sup -k/. In the case k=n, the communication complexity of these protocols is therefore Omega (n/sup 2/) bit commitments. A zero-knowledge proof is given for achieving the same goal with only O(n/sup m/+k square root n/sup m/) bit commitments, where m=1+ epsilon /sub n/ and epsilon /sub n/ goes to zero as n goes to infinity. In the case k=n, this is O(n square root n/sup m/). Moreover, only O(k) commitments need ever be opened, which is interesting if committing to a bit is significantly less expensive than opening a commitment.<<ETX>>
[Costs, proof systems, Circuits, probability, H infinity control, Complexity theory, communication complexity, Cryptographic protocols, Computer science, subquadratic zero-knowledge, Boolean circuit, satisfiability, Polynomials, theorem proving, Cryptography, protocols, Contracts, computational complexity]
Highly fault-tolerant sorting circuits
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The problem of constructing a sorting circuit that will work well even if a constant fraction of its comparators fail at random is addressed. Two types of comparator failure are considered: passive failures, which result in no comparison being made (i.e., the items being compared are output in the same order that they are input), and destructive failures, which result in the items being output in the reverse of the correct order. In either scenario, it is assumed that each comparator is faulty with some constant probability rho , and a circuit is said to be fault-tolerant if it performs some desired function with high probability given that each comparator fails with probability rho . One passive and two destructive circuits are constructed.<<ETX>>
[Out of order, Genetic mutations, probability, passive failures, highly fault tolerant sorting circuits, Circuit faults, destructive failures, Sorting, Fault tolerance, Failure analysis, sorting, fault tolerant computing, Contracts, comparators]
On-line scheduling in the presence of overload
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The preemptive scheduling of sporadic tasks on a uniprocessor is considered. A task may arrive at any time, and is characterized by a value that reflects its importance, an execution time that is the amount of processor time needed to completely execute the task, and a deadline by which the task is to complete execution. The goal is to maximize the sum of the values of the completed tasks. An online scheduling algorithm that achieves optimal performance when the system is underloaded and provides a nontrivial performance guarantee when the system is overloaded is designed. The algorithm is implemented using simple data structures to run at a cost of O(log n) time per task, where n bounds the number of tasks in the system at any instant. Upper bounds on the best performance guarantee obtainable by an online algorithm in a variety of settings are derived.<<ETX>>
[Real time systems, Algorithm design and analysis, nontrivial performance, preemptive scheduling, Costs, upper bounds, Control systems, NP completeness, processor time, Automatic control, scheduling, data structures, sporadic tasks, deadline, Power generation, optimal performance, performance evaluation, Data structures, Scheduling algorithm, Processor scheduling, online scheduling algorithm, real-time systems, uniprocessor, execution time, Timing, computational complexity]
A new characterization of Mehlhorn's polynomial time functionals
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A. Cobham (1964) presented a machine-independent characterization of computational feasibility, via inductive definition. R. Constable (1973) was apparently the first to consider the notion of feasibility for type 2 functionals. K. Mehlhorn's (1976) study of feasible reducibilities proceeds from Constable's work. Here, a class of polytime operators is defined, using a generalization of Cobham's definition. The authors provide an affirmative answer to the question of whether there is a natural machine based definition of Mehlhorn's class.<<ETX>>
[Computational modeling, polynomials, Mehlhorn's polynomial time functionals, Magnetic heads, Equations, Computer science, feasibility, Turing machines, polytime operators, Set theory, computational feasibility, Polynomials, Robustness, type 2 functionals, Logic, machine-independent characterization, Arithmetic, computational complexity, inductive definition]
Better expansion for Ramanujan graphs
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The expansion properties of regular graphs are investigated. The best previously known expansion of subsets of linear size of explicit k-regular graphs is k/4. This bound is achieved by nonbipartite Ramanujan graphs of degree k=p+1, which have the property that all but the largest eigenvalue have absolute value at most 2 square root p. The expansion coefficient for linear subsets for nonbipartite Ramanujan graphs is improved to 3(k-2)/8. Other results are established, including improved results about random walks on expanders.<<ETX>>
[random walks, Laboratories, graph theory, expanders, Routing, explicit k-regular graphs, Graph theory, regular graphs, Complexity theory, Application software, Sorting, eigenvalues and eigenfunctions, Computer science, expansion properties, Eigenvalues and eigenfunctions, Ramanujan graphs, Cryptography, Contracts, eigenvalue]
A deterministic parallel algorithm for planar graphs isomorphism
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A deterministic parallel algorithm for determining whether two planar graphs are isomorphic is presented. The algorithm needs O(log n) separators that have to be computed one after the other. The running time is T=O(log/sup 3/ n) time for finding separators, and the processors count is n/sup 1.5/ log n/T. It is also shown that every planar graph has a separator, and a parallel algorithm for finding the separator is given.<<ETX>>
[parallel algorithms, Particle separators, Computational modeling, graph theory, deterministic parallel algorithm, Read-write memory, Phase change random access memory, Parallel algorithms, Concurrent computing, running time, Tree graphs, Vegetation mapping, planar graphs isomorphism, separators, processors count, Contracts, Arithmetic, computational complexity]
Better bounds for threshold formulas
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The computation of threshold functions using formulas over the basis (AND, OR, NOT) is considered. It is shown that every monotone formula that computes the threshold function T/sub k//sup n/2<or=k<or=n/2, has size Omega (nk log (n/(k-1))). The same lower bound is shown to hold even in the stronger monotone contact networks model. Nearly optimal bounds on the size of Sigma Pi Sigma formulas computing T/sub k//sup n/ for small k are also shown.<<ETX>>
[Input variables, Circuits, better bounds, threshold logic, threshold formulas, nearly optimal bounds, Computational complexity, monotone formula, Sorting, computation, Computer science, Boolean functions, Upper bound, stronger monotone contact networks model, Computer networks, threshold functions]
Ambivalent data structures for dynamic 2-edge-connectivity and k smallest spanning trees
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Ambivalent data structures are presented for several problems on undirected graphs. They are used in finding the k smallest spanning trees of a weighted undirected graph in O(m log beta (m,n)+min(k/sup 3/2/, km/sup 1/2/)) time, where m is the number of edges and n the number of vertices in the graph. The techniques are extended to find the k smallest spanning trees in an embedded planar graph in O(n+k(log n)/sup 3/) time. Ambivalent data structures are also used to maintain dynamically 2-edge-connectivity information. Edges and vertices can be inserted or deleted in O(m/sup 1/2/) time, and a query as to whether two vertices are in the same 2-edge-connected component can be answered in O(log n) time, where m and n are understood to be the current number of edges and vertices, respectively. Again, the techniques are extended to maintain an embedded planar graph so that edges and vertices can be inserted or deleted in O((log n)/sup 3/) time, and a query answered in O(log n) time.<<ETX>>
[Tree data structures, graph theory, dynamic maintenance, vertices, trees (mathematics), k smallest spanning trees, query, edges, embedded planar graph, Data structures, weighted undirected graph, Computer science, ambivalent data structures, Tree graphs, dynamic 2-edge-connectivity, data structures, Contracts, computational complexity]
Applications of a poset representation to edge connectivity and graph rigidity
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A poset representation for a family of sets defined by a labeling algorithm is investigated. Poset representations are given for the family of minimum cuts of a graph, and it is shown how to compute them quickly. The representations are the starting point for algorithms that increase the edge connectivity of a graph, from lambda to a given target tau = lambda + delta , adding the fewest edges possible. For undirected graphs the time bound is essentially the best-known bound to test tau -edge connectivity; for directed graphs the time bound is roughly a factor delta more. Also constructed are poset representations for the family of rigid subgraphs of a graph, when graphs model structures constructed from rigid bars. The link between these problems is that they all deal with graphic matroids.<<ETX>>
[Sensitivity analysis, graph theory, time bound, Partitioning algorithms, set theory, poset representation, labeling algorithm, graphic matroids, Application software, matrix algebra, Graphics, Computer science, edge connectivity, graph rigidity, directed graphs, rigid subgraphs, rigid bars, minimum cuts, Delta modulation, Labeling, undirected graphs, Bars, Testing, computational complexity]
Optimal file sharing in distributed networks
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Given a distributed network of processors represented by an undirected graph G=(V, E) and a file size k, the problem of distributing an arbitrary file w of k bits among all nodes of the network G is considered. Memory devices are to be assigned to the node of G such that, by accessing the memory of its own and of its adjacent nodes, each node can reconstruct the contents of w. The objective is to minimize the total size memory in the network. A file distribution scheme that realizes this objective for k>>log Delta /sub G/, where Delta /sub G/, stands for the maximum degree in G, is presented. For this range of k, the total size of memory required by the suggested scheme approaches an integer programming lower bound on that size.<<ETX>>
[optimal file sharing, Peer to peer computing, integer programming, graph theory, memory devices, Linear programming, Size measurement, Electronic mail, integer programming lower bound, Computer science, Intelligent networks, memory minimisation, file distribution, file size, file organisation, Polynomials, distributed networks, minimisation, undirected graph, computational complexity]
Efficient algorithms for dynamic allocation of distributed memory
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The problem of dynamically allocating and deallocating local memory resources among multiple users in a parallel or distributed system is considered. The goal is to devise an online allocation algorithm that minimizes both the fraction of unused space due to fragmentation of the memory and the slowdown needed by the system to service user requests. The problem is solved in near-optimal fashion by devising an algorithm that allows the memory to be used to 100% of capacity despite the fragmentation and guarantees that service delays will always be within a constant factor of optimal. The algorithm is completely online (no foreknowledge of user activity is assumed) and can accommodate any sequence of insertions and deletions by the users which does not violate global memory bounds. The results have applications in the domain of parallel disk allocation.<<ETX>>
[algorithms, parallel system, Military computing, Heuristic algorithms, insertions, Laboratories, NASA, deletions, Mathematics, online allocation algorithm, near-optimal fashion, Delay, dynamic allocation, service delays, Concurrent computing, Computer science, distributed memory, local memory resources, file organisation, Resource management, Contracts]
Dynamic scheduling on parallel machines
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The problem of online job scheduling on various parallel architectures is studied. An O((log log n)/sup 1/2/)-competitive algorithm for online dynamic scheduling on an n*n mesh is given. It is proved that this algorithm is optimal up to a constant factor. The algorithm is not greedy, and the lower bound proof shows that no greedy-like algorithm can be very good. The upper bound result can be generalized to any fixed-dimensional meshes. Competitive scheduling algorithms for other architectures are given.<<ETX>>
[Heuristic algorithms, parallel architectures, Parallel machines, performance evaluation, lower bound proof, Dynamic scheduling, Phase change random access memory, upper bound, Parallel architectures, dynamic scheduling, parallel machines, Scheduling algorithm, Computer science, online job scheduling, Upper bound, Processor scheduling, fixed-dimensional meshes, scheduling, Time sharing computer systems]
A theory of using history for equational systems with applications
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A general theory of using a congruence closure based simplifier (CCNS) proposed by P. Chew (1980) for computing normal forms is developed, and several applications are presented. An independent set of postulates is given, and it is proved that CCNS can be used for any system that satisfies them. It is then shown that CCNS can be used for consistent convergent systems and for various kinds of priority rewrite systems. A simple translation scheme for converting priority systems into effectively nonoverlapping convergent systems is presented.<<ETX>>
[rewriting systems, normal forms, congruence closure based simplifier, Data structures, Mathematics, History, Application software, Equations, translation scheme, Computer science, Computer languages, equational systems, Databases, consistent convergent systems, priority rewrite systems]
Approximate representation theory of finite groups
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The asymptotic stability and complexity of floating point manipulation of representations of a finite group G are considered, especially splitting them into irreducible constituents and deciding their equivalence. Using rapid mixing estimates for random walks, the authors analyze a classical algorithm by J. Dixon (1970). They find that both its stability and complexity critically depend on the diameter d=diam(G,S) (S is the set that generates G). They propose a worst-case speedup by using Erdos-Renyi generators and modifying the Dixon averaging method. The overall effect in asymptotic complexity is a guaranteed (n log mod G mod )/sup O(1)/ running time.<<ETX>>
[Algorithm design and analysis, irreducible constituents, rapid mixing estimates, worst-case speedup, set theory, Sparse matrices, asymptotic stability, Asymptotic stability, Monte Carlo methods, diameter, asymptotic complexity, Dixon averaging method, Polynomials, Erbium, equivalence decision, approximation theory, random walks, approximate representation theory, Matrix decomposition, Statistics, Computer science, group theory, Erdos-Renyi generators, generating set, finite groups, Linear algebra, floating point manipulation, computational complexity]
Faster uniquely represented dictionaries
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
The authors present a solution to the dictionary problem where each subset of size n of an ordered universe is represented by a unique structure, containing a (unique) binary search tree. The structure permits the execution of search, insert, and delete operations in O(n/sup 1/3/) time in the worst case. They also give a general lower bound, stating that for any unique representation of a set in a graph of, bounded outdegree, one of the operations search or update must require a cost of Omega (n/sup 1/3/) Therefore, the result sheds new light on previously claimed lower bounds for unique binary search tree representations.<<ETX>>
[Tree data structures, Dictionaries, Costs, Computational modeling, trees (mathematics), Binary search trees, Data structures, insert, update, lower bound, bounded outdegree graph, delete, Computer science, worst case, search, Tree graphs, dictionary problem, ordered universe, binary search tree, data structures, search problems, computational complexity]
Progress measures for complementation omega -automata with applications to temporal logic
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A new approach to complementing omega -automata, which are finite-state automata defining languages of infinite words, is given. Instead of using usual combinatorial or algebraic properties of transition relations, it is shown that a graph-theoretic approach based on the notion of progress measures is a potent tool for complementing omega -automata. Progress measures are applied to the classical problem of complementing Buchi automata, and a simple method is obtained. The technique applies to Streett automata, for which an optimal complementation method is also obtained. As a consequence, it is seen that the powerful temporal logic ETLs is much more tractable than previously thought.<<ETX>>
[finite automata, finite-state automata, temporal logic, State-space methods, graph-theoretic approach, languages of infinite words, Automata, ETLs, Buchi automata, Streett automata, Logic, optimal complementation, complementation omega -automata]
Finding k-cuts within twice the optimal
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
Two simple approximation algorithms are presented for the minimum k-cut problem. Each algorithm finds a k-cut having weight within a factor of (2-2/k) of the optimal. One of the algorithms is particularly efficient, requiring a total of only n-1 maximum flow computations for finding a set of near-optimal k-cuts, one for each value of k between 2 and n.<<ETX>>
[maximum flow computations, approximation theory, graph theory, Very large scale integration, minimum k-cut problem, weight, Partitioning algorithms, approximation algorithms, Helium, Computer science, Approximation algorithms, Polynomials, minimisation, computational complexity]
Optimal prefetching via data compression
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
A form of the competitive philosophy is applied to the problem of prefetching to develop an optimal universal prefetcher in terms of fault ratio, with particular applications to large-scale databases and hypertext systems. The algorithms are novel in that they are based on data compression techniques that are both theoretically optimal and good in practice. Intuitively, in order to compress data effectively, one has to be able to predict feature data well, and thus good data compressors should be able to predict well for purposes of prefetching. It is shown for powerful models such as Markov sources and mth order Markov sources that the page fault rates incurred by the prefetching algorithms presented are optimal in the limit for almost all sequences of page accesses.<<ETX>>
[data compression, buffer storage, Prefetching, Data compression, feature data, competitive philosophy, Compressors, Cache storage, Application software, Hypertext systems, Delay, optimal universal prefetcher, Computer science, storage management, hypertext systems, large-scale databases, Databases, page fault rates, fault ratio, file organisation, Large-scale systems, optimal prefetching, Markov sources]
Tree automata, mu-calculus and determinacy
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is shown that the propositional mu-calculus is equivalent in expressive power to finite automata on infinite trees. Since complementation is trivial in the mu-calculus, the equivalence provides a radically simplified, alternative proof of M.O. Rabin's (1989) complementation lemma for tree automata, which is the heart of one of the deepest decidability results. It is also shown how mu-calculus can be used to establish determinacy of infinite games used in earlier proofs of complementation lemma, and certain games used in the theory of online algorithms.<<ETX>>
[Heart, equivalence, finite automata, infinite trees, trees (mathematics), Calculus, Mathematics, Game theory, complementation, online algorithms, determinacy, Computer science, mu-calculus, decidability, Automata, Logic, Marine vehicles, infinite games]
How to pack better than best fit: tight bounds for average-case online bin packing
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
An O(n log n)-time online algorithm is given for packing items i.i.d. uniform on (0, 1) into bins of size 1 with expected wasted space Theta (n/sup 1/2/ log /sup 1/2/n). This matches the lowest bound that no online algorithm can achieve O(n/sup 1/2/ log /sup 1/2/ n) wasted space. It is done by analyzing another algorithm which involves putting balls into buckets online. The analysis of this second algorithm also gives bound on the stochastic rightward matching problem, which arises in analyzing not only the above online bin packing problem, but also a 2-D problem of packing rectangles into a half-infinite strip. The bounds on rightward matching thus give good bounds for the 2-D strip packing problem.<<ETX>>
[Algorithm design and analysis, Strips, Optimized production technology, ball packing, online algorithm, Harmonic analysis, expected wasted space, NP-complete problem, average-case online bin packing, half-infinite strip, tight bounds, operations research, rectangle packing, stochastic rightward matching problem, 2-D strip packing problem, Polynomials, Performance analysis, lowest bound, Testing, computational complexity]
Lower bounds for polynomial evaluation and interpolation problems
[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science
None
1991
It is shown that there is a set of points p/sub 1/, p/sub 2/,. . .,p/sub n/ such that any algebraic program of depth d for polynomial evaluation (or interpolation) at these points has size Omega (n log n/log d). Moreover, if d is a constant, then a lower bound of Omega (n/sup 1+1/d/) is obtained.<<ETX>>
[Costs, algebraic program, polynomials, Discrete Fourier transforms, lower bound, polynomial evaluation, Computer science, Interpolation, interpolation, Ear, symbol manipulation, Polynomials, Arithmetic, interpolation problems]
Lower bounds on the competitive ratio for mobile user tracking and distributed job scheduling
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors prove a lower bound of Omega (log n/log log n) on the competitive ratio of any (deterministic or randomised) distributed algorithm for solving the mobile user problem on certain networks of n processors. The lower bound holds for various networks, including the hypercube, any network with sufficiently large girth, and any highly expanding graph. A similar Omega (log n/log log n) lower bound is proved for the competitive ratio of the maximum job delay of any distributed algorithm for solving a distributed scheduling problem on any of these networks. The proofs combine combinatorial techniques with tools from linear algebra and harmonic analysis and apply, in particular, a generalization of the vertex isoperimetric problem on the hypercube, which may be of independent interest.<<ETX>>
[multiprocessor interconnection networks, Harmonic analysis, Mathematics, vertex isoperimetric problem, communication complexity, Gas insulated transmission lines, Distributed computing, competitive ratio, scheduling, Hypercubes, Communication networks, Distributed algorithms, linear algebra, hypercube, game theory, mobile user tracking, harmonic analysis, Computer science, distributed algorithm, Processor scheduling, combinatorial techniques, distributed algorithms, Linear algebra, distributed job scheduling, expanding graph, maximum job delay]
The distributed k-server problem-a competitive distributed translator for k-server algorithms
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors consider the k-server problem in a distributed setting. Given a network of n processors, and k identical mobile servers, requests for service appear at the processors and a server must reach the request point. Besides modeling problems in computer networks where k identical mobile resources are shared by the processors of the network, this models a realistic situation where the transfer of information is costly and there is no central control that governs the behavior of servers that move around to satisfy requests for service. The problem is that of devising algorithms that minimize not only the travel of the server but also the communication cost incurred for the transmission of control messages. The main contribution is a general translator to transform any deterministic global-control competitive k-server algorithm into a distributed competitive one. As consequences they get poly(k)-competitive distributed algorithms for the line, trees and the ring.<<ETX>>
[mobile servers, poly(k)-competitive distributed algorithms, Costs, Communication system control, computer networks, game theory, communication cost, competitive distributed translator, communication complexity, mobile resources, Centralized control, Computer science, Network servers, control messages, distributed algorithms, scheduling, Computer networks, Polynomials, Space exploration, distributed k-server problem, Distributed algorithms, Mobile computing, competitive ratios]
Computing a shortest k-link path in a polygon
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors consider the problem of finding a shortest polygonal path from s to t within a simple polygon P, subject to the restriction that the path have at most k links (edges). They give an algorithm to compute a k-link path with length at most (1 + epsilon ) times the length of a shortest k-link path, for any error tolerance epsilon >0. The algorithm runs in time O(n/sup 3/k/sup 3/ log (Hk/ epsilon /sup 1/k/)), where N is the largest integer coordinate among the n vertices of P. They also study the more general problem of approximating shortest k-link paths in polygons with holes. In this case, they give an algorithm that returns a path with at most 2k links and length at most that of a shortest k-link path; the running time is O(kE/sup 2/), where E is the number of edges in the visibility graph. Finally, they study the bicriteria path problem in which the two criteria are link length and 'total turn' (the integral of mod Delta theta mod along a path). They obtain in an exact polynomial-time algorithm for polygons with holes.<<ETX>>
[Laboratories, graph theory, exact polynomial-time algorithm, computational geometry, bicriteria path problem, visibility graph, Computer science, Computational geometry, running time, polygon, shortest k-link path, Polynomials, Contracts, holes, computational complexity]
Undecidability of the Horn-clause implication problem
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors prove that the problem 'given two Horn clauses H/sub 1/=( alpha /sub 1/ V-product alpha /sub 2/ to beta ) and H/sub 2/=( gamma /sub 1/ V-product . . . V-product gamma /sub k/ to delta ), where alpha /sub i/, beta , gamma /sub i/, delta are atomic formulas, decide if H/sub 2/, is a consequence of H/sub 1/' is not recursive. This solves one of the last open decidability problems concerning formulas in pure predicate logic (i.e. without equality symbol). The proof depends on a thorough analysis of derivation trees of one rule of inference with two premisses and one conclusion, and it may have further applications.<<ETX>>
[atomic formulas, Horn clauses, equality symbol, Horn-clause implication, Equations, Computer science, pure predicate logic, Algebra, decidability, decidability problems, Logic, Books, Artificial intelligence, derivation trees]
The power of combining the techniques of algebraic and numerical computing: improved approximate multipoint polynomial evaluation and improved multipole algorithms
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors demonstrate the power of combining the techniques of algebraic computation with ones of numerical computation. They do this by improving the known methods for polynomial evaluation on a set of real points and for simulation of n charged particles on the plane. In both cases they approximate (rather than exactly compute) the solutions and do this by exploiting algebraic techniques of the algorithm design.<<ETX>>
[Algorithm design and analysis, complexity, Costs, Computational modeling, polynomials, multipoint polynomial evaluation, Educational institutions, Mathematics, Application software, poles and zeros, Computer science, multipole algorithms, numerical computing, symbol manipulation, Approximation algorithms, Polynomials, Arithmetic, computational complexity, algebraic computation]
Amplification and percolation (probabilistic Boolean functions)
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors extend R.B. Boppana's results (1989) in two ways. They first show that his two lower bounds hold for general read-once formulae, not necessarily monotone, that may even include exclusive-or gates. They are then able to join his two lower bounds together and show that any read-once, not necessarily monotone, formula that amplifies (p-/sup 1///sub n/,p+/sup 1///sub n/) to (2/sup -n/,1-2/sup -n/) has size of at least Omega (n/sup alpha +2/). This result does not follow from Boppana's arguments and it shows that the amount of amplification achieved by L.G. Valiant (1984) is the maximal achievable using read-once formulae.<<ETX>>
[Computer science, amplification, Boolean functions, percolation, Computer network reliability, Circuit simulation, Polynomials, probabilistic Boolean functions, Relays, lower bounds]
Efficient minimum cost matching using quadrangle inequality
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors present efficient algorithms for finding a minimum cost perfect matching, and for solving the transportation problem in bipartite graphs, G = (Red union Blue, Red * Blue), where mod Red mod = n, mod Blue mod = m, n <or= m, and the cost function obeys the quadrangle inequality. The first results assume that all the red points and all the blue points lie on a curve that is homeomorphic to either a line or a circle and the cost function is given by the Euclidean distance along the curve. They present a linear time algorithm for the matching problem. They generalize the method to solve the corresponding transportation problem in O((m+n)log(m+n)) time. The next result is an O(n log m) algorithm for minimum cost matching when the cost array is a bitonic Monge array. An example of this is when the red points lie on one straight line and the blue points lie on another straight line (that is not necessarily parallel to the first one). Finally, they provide a weakly polynomial algorithm for the transportation problem in which the associated cost array is a bitonic Monge array.<<ETX>>
[minimum cost perfect matching, quadrangle inequality, transportation problem, graph theory, Transportation, computational geometry, time complexity, bitonic Monge array, transportation, red points, Computer science, blue points, bipartite graphs, linear time algorithm, weakly polynomial algorithm, Euclidean distance, matching problem, cost function, Cost function, Polynomials, Bipartite graph, minimisation, Contracts, computational complexity]
Processor-efficient parallel solution of linear systems. II. The positive characteristic and singular cases
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
For pt.I see Proc. 3rd Ann. ACM Symp. Parallel Algms. Architecture, p. 180-91 (1991). The authors show that over any field, the solution set to a system of n linear equations in n unknowns can be computed in parallel with randomization simultaneously in poly-logarithmic time in n and with only as many processors as are utilized to multiply two n * n matrices. A time unit represents an arithmetic operation in the field. For singular systems the parallel timings are asymptotically as fast as those for non-singular systems, due to the avoidance of binary search in the matrix rank problem, except when the field has small positive characteristic; in that case, binary search is avoided at a somewhat higher processor count measure.<<ETX>>
[Linear systems, Algorithm design and analysis, binary search, parallel algorithms, singular systems, Economic indicators, poly-logarithmic time, processor count, time complexity, Educational institutions, parallel timings, Mathematics, linear systems, parallel solution, Parallel algorithms, Equations, Computer science, Concurrent computing, arithmetic operation, matrix rank problem, Arithmetic, computational complexity]
On-line load balancing
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The setup for the authors' problem consists of n servers that must complete a set of tasks. Each task can be handled only by a subset of the servers, requires a different level of service, and once assigned can not be re-assigned. They make the natural assumption that the level of service is known at arrival time, but that the duration of service is not. The on-line load balancing problem is to assign each task to an appropriate server in such a way that the maximum load on the servers is minimized. The authors derive matching upper and lower bounds for the competitive ratio of the on-line greedy algorithm for this problem, namely /sup (3n)2/3///sub 2/(1+o(1)), and derive a lower bound, Omega ( square root n), for any other deterministic or randomized on-line algorithm.<<ETX>>
[Greedy algorithms, queueing theory, upper bounds, local area networks, online load balancing, Multimedia communication, Application software, lower bounds, Bridges, randomised online algorithm, servers, file servers, Bandwidth, Computer graphics, deterministic online algorithm, on-line greedy algorithm, LAN, Load management, Computer networks, Workstations, Local area networks]
Undirected connectivity in O(log/sup 1.5/n) space
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors present a deterministic algorithm for the connectivity problem on undirected graphs that runs in O(log/sup 1.5/n) space. Thus, the recursive doubling technique of Savich (1970) which requires Theta (log/sup 2/n) space is not optimal for this problem.<<ETX>>
[parallel algorithms, graph theory, computational geometry, Phase change random access memory, recursive doubling technique, deterministic algorithm, Parallel algorithms, Sampling methods, Polynomials, undirected graphs, space complexity, connectivity problem, computational complexity]
Zero-knowledge proofs of knowledge without interaction
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
A zero-knowledge proof system of knowledge is a protocol between two parties called the prover and the verifier. The prover wants to convince the verifier that he 'knows' the proof of a given theorem without revealing any additional information. This is different from a zero-knowledge proof system of membership where the prover convinces the verifier only of the veridicity of the statement. Zero-knowledge proofs of knowledge are very useful tools in the design of secure protocols. Though, the concept of a proof of knowledge is a very subtle one and great care is needed to obtain a satisfying formalization. The authors investigate the concept of a zero-knowledge proof of knowledge with a non-interactive model. Here, the prover and the verifier share a short random string and the only communication allowed is from the prover to the verifier. Although this is a simpler model than the interactive one, still formalizing zero-knowledge proofs of knowledge is a delicate task.<<ETX>>
[security of data, Councils, Laboratories, secure protocols, Public key cryptography, Polynomials, zero-knowledge proof system, theorem proving, Security, protocols, Cryptographic protocols, Postal services]
Algebraic decision trees and Euler characteristics
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
For any set S contained in R/sup n/, let chi (S) denote its Euler characteristic. The author shows that any algebraic computation tree or fixed-degree algebraic decision tree must have height Omega (log mod chi (S) mod )for deciding the membership question of a compact semi-algebraic set S. This extends a result by A. Bjorner, L. Lovasz and A. Yao where it was shown that any linear decision tree for deciding the membership question of a closed polyhedron S must have height greater than or equal to log/sub 3/ mod chi (S) mod .<<ETX>>
[algebraic decision trees, decision theory, Computational modeling, trees (mathematics), computational geometry, closed polyhedron, Computational complexity, membership question, Computer science, Computational geometry, algebraic computation tree, Euler characteristics, Decision trees, Integrated circuit modeling, Marine vehicles, Testing]
Sparsification-a technique for speeding up dynamic graph algorithms
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors provide data structures that maintain a graph as edges are inserted and deleted, and keep track of the following properties: minimum spanning forests, best swap, graph connectivity, and graph 2-edge-connectivity, in time O(n/sup 1/2/log(m/n)) per change; 3-edge-connectivity, in time O(n/sup 2/3/) per change; 4-edge-connectivity, in time O(n alpha (n)) per change; k-edge-connectivity, in time O(n log n) per change; bipartiteness, 2-vertex-connectivity, and 3-vertex-connectivity, in time O(n log(m/n)) per change; and 4-vertex-connectivity, in time O(n log(m/n)+n alpha (n)) per change. Further results speed up the insertion times to match the bounds of known partially dynamic algorithms. The algorithms are based on a technique that transforms algorithms for sparse graphs into ones that work on any graph, which they call sparsification.<<ETX>>
[Algorithm design and analysis, Heuristic algorithms, graph theory, dynamic graph algorithms, computational geometry, Data structures, Partitioning algorithms, minimum spanning forests, best swap, Computer science, graph 2-edge-connectivity, graph connectivity, Tree graphs, spatial data structures, Binary trees, sparsification, data structures, computational complexity]
Waste makes haste: tight bounds for loose parallel sorting
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Conventional parallel sorting requires the n input keys to be output in an array of size n, and is known to take Omega (log n/log log n) time using any polynomial number of processors. The lower bound does not apply to the more 'wasteful' convention of padded sorting, which requires the keys to be output in sorted order in an array of size (1+o(1))n. The authors give very fast randomised CRCW PRAM algorithms for several padded-sorting problems. Applying only pairwise comparisons to the input and using kn processors, where 2 <or= k <or= n, they can padded-sort n keys in O(log n/log k) time with high probability (WHP), which is the best possible (expected) run time for any comparison-based algorithm. They also show how to padded-sort n independent random numbers in O(log/sub */n) time WHP with O(n) work, which matches a recent lower bound, and how to padded-sort n integers in the range 1..n in constant time whp using n processors. If the integer sorting is required to be stable, they can still solve the problem in O(log log n/log k) time WHP using kn processors, for any k with 2 <or= k <or= log n. The integer sorting results require the nonstandard OR PRAM. As an application of the padded-sorting algorithms, they can solve approximate prefix summation problems of size n with O(n) work in constant time WHP on the OR PRAM, and in O(log log n) time WHP on standard PRAM variants.<<ETX>>
[parallel algorithms, input keys, padded sorting, fast randomised CRCW PRAM algorithms, time complexity, Phase change random access memory, Large scale integration, Parallel algorithms, Sorting, Concurrent computing, integer sorting, loose parallel sorting, nonstandard OR PRAM, sorting, Polynomials, random numbers, approximate prefix summation problems, Contracts, computational complexity]
A mildly exponential approximation algorithm for the permanent
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
An approximation algorithm for the permanent of an n*n 0,1-matrix is presented. The algorithm is shown to have worst-case time complexity exp (0(n/sup 1/2/ log/sup 2/ n)). Asymptotically, this represents a considerable improvement over the best existing algorithm, which has worst-case time complexity of the form e/sup theta (n)/.<<ETX>>
[worst-case time complexity, matrix permanent, mildly exponential approximation algorithm, Mathematics, matrix algebra, Computer science, Monte Carlo methods, NP-hard problem, Turing machines, permanent, Approximation algorithms, Polynomials, Bipartite graph, Random variables, computational complexity]
Exact analysis of hot-potato routing
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors consider a form of packet routing known as hot potato routing or deflection routing. Its striking feature is that there are no buffers at intermediate nodes. Thus packets are always moving (possibly in the 'wrong' direction), giving rise to the term 'hot potato'. They give a simple deterministic algorithm that on a n*n torus will route a random instance in 2n+O(log n) steps with high probability. They add random delays to this algorithm so that it solves the permutation routing problem on the torus in 9n steps with high probability, on every instance. On a hypercube with N=2/sup n/ nodes, they give a simple deterministic algorithm that will route a random instance in O(n) steps with high probability. Various other results are discussed.<<ETX>>
[Algorithm design and analysis, hypercube, Buffer storage, multiprocessor interconnection networks, packet switching, Parallel machines, Optical fiber networks, Routing, hot-potato routing, permutation routing problem, deterministic algorithm, Added delay, Network topology, Optical buffering, Hypercubes, deflection routing, random delays, packet routing]
Tiling a polygon with rectangles
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors study the problem of tiling a simple polygon of surface n with rectangles of given types (tiles). They present a linear time algorithm for deciding if a polygon can be tiled with 1 * m and k * 1 tiles (and giving a tiling when it exists), and a quadratic algorithm for the same problem when the tile types are m * k and k * m.<<ETX>>
[linear time algorithm, Tiles, polygon, rectangles, quadratic algorithm, tiling, computational geometry, time complexity, geometry, computational complexity]
Optimal parallel hull construction for simple polygons in O(log log n) time
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author proposes an optimal parallel algorithm for computing the convex hull of a simple polygon. The algorithm achieves a runtime of O(log log n) using O(n/log log n) processors of a CRCW-PRAM. The data structure representing the convex hull is not the standard one, i.e. an array storing the vertices of the hull in clockwise order. Indeed, a lower bound of Omega (log n/log log n) on the runtime for any algorithm employing a polynomial number of processors and computing the array-representation is known. Nevertheless, the representation is adequate for further parallel processing; standard queries like computing the intersection of the hull with a given line, etc., can be answered in time O(log n/(log p+1)+1) using p processors. In addition subchain hull queries are supported optimally in time O(log k/(log p+1)+1), where k is the length of the subchain. The algorithm can easily be adapted to other hull-like structures for simple polygons; as e.g. the orthogonal hull, and the visibility region from a point under various definitions of visibility.<<ETX>>
[parallel algorithms, Computational modeling, optimal parallel algorithm, topology, data structure, computational geometry, Phase change random access memory, Data structures, runtime, visibility region, convex hull, simple polygon, Parallel algorithms, simple polygons, Concurrent computing, Bridges, Computational geometry, Runtime, subchain hull queries, Parallel processing, CRCW-PRAM, data structures, Clocks]
A subexponential algorithm for abstract optimization problems
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
An abstract optimization problem (AOP) is a triple (H,<, phi ) where H is a finite set, < a linear order on 2/sup H/ and phi an oracle that, for given F contained in G contained in H, determines whether F=min(2/sup G/), and if not, returns a smaller set. To solve the problem means to find min(2/sup H/). The author presents a randomized algorithm that solves any AOP with an expected number of O(e/sup O( square root mod H mod )/) oracle calls. In contrast, any deterministic algorithm needs to make 2/sup mod H mod /-1 oracle calls in the worst case. The algorithm is applied to the problem of finding the minimum distance of two polyhedra in d-space, which gives the first subexponential bound in d for this problem. Another application is the computation of the smallest ball containing n points in d-space; the previous bounds for this problem were also exponential in d.<<ETX>>
[computational geometry, smallest ball, Linear programming, oracle calls, randomized algorithm, History, optimisation, abstract optimization problems, minimum distance, Computer applications, subexponential bound, subexponential algorithm, polyhedra, computational complexity]
On the randomized complexity of volume and diameter
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors give an O(n/sup 7/log/sup 2/n) randomised algorithm to approximate the volume of a convex body, and an O(n/sup 6/log n) algorithm to sample a point from the uniform distribution over a convex body. For convex polytopes the algorithm runs in O(n/sup 7/log/sup 4/n) steps. Several tools are developed that may be interesting on their own. They extend results of Sinclair-Jerrum (1988) and the authors (1990) on the mixing rate of Markov chains from finite to arbitrary Markov chains. They describe an algorithm to integrate a function with respect to the stationary distribution of a general Markov chain. They also analyze the mixing rate of various random walks on convex bodies, in particular the random walk with steps from the uniform distribution over a unit ball. In several previous positive and negative results, the problem of computing the diameter of a convex body behaved similarly as the volume problem. In contrast to this, they show that there is no polynomial randomized algorithm to compute the diameter within a factor of n/sup 1/4/.<<ETX>>
[Algorithm design and analysis, random walks, unit ball, convex polytopes, computational geometry, Markov chains, randomized complexity, mixing rate, volume, Microwave integrated circuits, diameter, Markov processes, Approximation algorithms, Polynomials, Random variables, convex body, computational complexity]
On the completeness of object-creating query languages
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Recently, various database query languages have been considered that have the ability to create new domain elements. These languages, however, are not complete in the sense of Abiteboul and Kanellakis (1989). They provide a precise characterization for the class of queries that can be expressed in these languages. They call this class the constructive queries and motivate this term by establishing a close correspondence between object creation and the construction of hereditarily finite sets.<<ETX>>
[formal languages, Object oriented databases, object-oriented databases, constructive queries, Relational databases, query languages, Calculus, Database languages, database theory, hereditarily finite sets, Algebra, database query languages, object-oriented languages, Logic functions, object-creating query languages, Data models]
Reconstructing algebraic functions from mixed data
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors consider the task of reconstructing algebraic functions given by black boxes. Unlike traditional settings, they are interested in black boxes which represent several algebraic functions-f/sub 1/, . . ., f/sub k/, where at each input x, the box arbitarrily chooses a subset of f/sub 1/(x), . . ., f/sub k/(x) to output. They show how to reconstruct the functions f/sub 1/,. . ., f/sub k/ from the black box. This allows them to group the same points into sets, such that for each set, all outputs to points in the set are from the same algebraic function. The methods are robust in the presence of errors in the black box. The model and techniques can be applied in the areas of computer vision, machine learning, curve fitting and polynomial approximation, self-correcting programs and bivariate polynomial factorization.<<ETX>>
[mixed data, same points, Computer vision, black boxes, self-correcting programs, Image edge detection, Robot vision systems, computational geometry, Application software, machine learning, Information technology, Image reconstruction, group theory, reconstructing algebraic functions, Layout, polynomial approximation, computer vision, Computer errors, bivariate polynomial factorization, Polynomials, curve fitting, Detection algorithms]
Communication on noisy channels: a coding theorem for computation
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Communication is critical to distributed computing, parallel computing, or any situation in which automata interact-hence its significance as a resource in computation. In view of the likelihood of errors occurring in a lengthy interaction, it is desirable to incorporate this possibility in the model of communication. The author relates the noisy channel and the standard (noise less channel) complexities of a communication problem by establishing a 'two-way' or interactive analogue of Shanon's coding theorem: every noiseless channel protocol can be simulated by a private-coin noisy channel protocol whose time bound is proportional to the original (noiseless) time bound and inversely proportional to the capacity of the channel, while the protocol errs with vanishing probability. The method involves simulating the original protocol while implementing a hierarchical system of progress checks which ensure that errors of any magnitude in the simulation are, with high probability, rapidly eliminated.<<ETX>>
[Protocols, Hierarchical systems, communication complexity, Distributed computing, distributed computing, Communication standards, Concurrent computing, noiseless channel protocol, coding theorem, Parallel processing, noise, information theory, noisy channels, parallel computing, Codes, Channel capacity, encoding, computation, distributed algorithms, complexities, Automata, noisy channel, errors, Capacity planning, private-coin noisy channel protocol]
Safe and effective determinant evaluation
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The problem of evaluating the sign of the determinant of a small matrix aries in many geometric algorithms. Given an n*n matrix A with integer entries, whose columns are all smaller than M in Euclidean norm, the algorithm given evaluates the sign of the determinant det A exactly. The algorithm requires an arithmetic precision of less than 1.5n+2lgM bits. The number of arithmetic operations needed is O(n/sup 3/)+O(n/sup 2/) log OD(A)/ beta , where OD(A) mod det A mod is the product of the lengths of the columns of A, and beta is the number of 'extra' bits of precision, min(lg(1/u)-1.1n-2lgn-2,lgN-lgM-1.5n-1), where u is the roundoff error in approximate arithmetic, and N is the largest representable integer. Since OD(A)<or=M/sup n/, the algorithm requires O(n/sup 3/lgM) time, and O(n/sup 3/) time when beta = Omega (logM).<<ETX>>
[Algorithm design and analysis, Vehicle crash testing, determinant evaluation, computational geometry, Computer crashes, arithmetic precision, small matrix, matrix algebra, geometric algorithms, integer entries, roundoff error, Roundoff errors, Artificial intelligence, Arithmetic]
Improved lower bounds for Shellsort
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors give improved lower bounds for Shellsort based on a new and relatively simple proof idea. The lower bounds obtained are both stronger and more general than the previously known bounds. In particular, they hold for nonmonotone increment sequences and adaptive Shellsort algorithms, as well as for some recently proposed variations of Shellsort.<<ETX>>
[proof idea, Upper bound, sorting, Mathematics, Partitioning algorithms, Shellsort, search problems, lower bounds, nonmonotone increment sequences, Sorting]
The complexity of the Hajos calculus
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The Hajos construction is a simple, nondeterministic procedure for generating the class of graphs that are not 3-colorable. A.J. Mansfield and D.J.A. Welsh have posed the problem of proving whether or not there exists a polynomial-size Hajos construction for every non-3-colorable graph. The main result of this paper is a proof that the Hajos calculus is polynomially-bounded if and only if extended Frege proof systems are polynomially bounded. This result links an open problem in graph theory to an important open problem in the complexity of propositional proof systems. In addition, the authors establish an exponential lower bound for a strong subsystem of the Hajos calculus. Lastly, they discuss an interesting graph-theoretical consequence of this result.<<ETX>>
[complexity, Terminology, graph theory, Frege proof systems, Calculus, Graph theory, nondeterministic procedure, Computer science, formal logic, polynomially-bounded, Variable speed drives, Polynomials, Hajos calculus, theorem proving, Power generation, computational complexity]
Fast algorithms for matrix normal forms
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
A Las Vegas type probabilistic algorithm is presented for computing the Frobenius normal form of an n*n matrix T over any field K. The algorithm requires O/sup approximately /(MM(n))=MM(n)/sup ./(log n)/sup O(1)/ operations in K, where O(MM(n)) operations in K are sufficient to multiply two n*n matrices over K. This nearly matches the lower bound of Omega (MM(n)) operations in K for this problem, and improves on the O(n/sup 4/) operations in K required by the previously best known algorithm. The author applies the algorithm to evaluate a polynominal g in K(x) at T with /sup approximately /(MM(n)) operations in K when deg g<or=n/sup 2/. This nearly matches a lower bound of Omega (MM(n)) operations in K when deg g<or=2. Other applications include algorithms for computing the minimal polynomial of a matrix, the rational Jordan form of a matrix, for testing whether two matrices are similar, and for matrix powering, which are substantially faster than those previously known.<<ETX>>
[Computer science, Las Vegas type probabilistic algorithm, Frobenius normal form, minimal polynomial, rational Jordan form, matrix powering, Polynomials, matrix normal forms, Testing, computational complexity, matrix algebra]
The isomorphism conjecture holds relative to an oracle
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors introduce symmetric perfect generic sets. these sets vary from the usual generic sets by allowing limited infinite encoding into the oracle. They then show that the Berman-Hartmanis (1977) isomorphism conjecture holds relative to any sp-generic oracle, i.e., for any symmetric perfect generic set A, all NP/sup A/-complete sets are polynomial-time isomorphic relative to A. As part of the proof that the isomorphism conjecture holds relative to symmetric perfect generic sets they also show that P/sup A/=FewP/sup A/ for any symmetric perfect generic/sup /A.<<ETX>>
[isomorphism conjecture, oracle, automata theory, Polynomials, Encoding, NP completeness, computational complexity, symmetric perfect generic sets]
Read-thrice DNF is hard to learn with membership and equivalence queries
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
A general technique is developed to obtain nonlearnability results in the model of exact learning from equivalence and membership queries. The technique is applied to show that, assuming NP not=co-NP, there does not exist a polynomial-time membership and equivalence query algorithm for exactly learning read-thrice DNF formulas-boolean formulas in disjunctive normal form where each variable appears at most three times. This result adds evidence to the conjecture that DNF is hard to learn in the membership and equivalence query model.<<ETX>>
[Protocols, disjunctive normal, Switches, equivalence queries, exact learning, Boolean algebra, membership, polynomial-time membership, Computer science, read-thrice DNF formulas, boolean formulas, Telephony, nonlearnability, Polynomials, Standards development, learning (artificial intelligence), computational complexity]
Fast unimodular reduction: planar integer lattices
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author shows that a shortest basis for the 2-dimensional lattice Lambda (u, v) generated by an input pair u, v in Z/sup 2/ can be computed in O(M(n) log n) where n is the bit-size of the input numbers and M(n) is the complexity of multiplying two n-bit integers. This generalizes Schonhage's technique (1971) for fast integer GCD to a higher dimension.<<ETX>>
[Algorithm design and analysis, complexity, Lattices, planar integer lattices, computational geometry, Linear programming, Computational geometry, Approximation algorithms, fast unimodular reduction, Polynomials, Books, computational complexity, number theory, fast integer GCD]
Fault-tolerant wait-free shared objects
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors classify object failures into two broad categories: responsive and non-responsive. They require that wait-free objects subject to responsive failures continue to respond (in finite time) to operation invocations. The responses may be incorrect. In contrast, wait-free objects subject to non-responsive failures are exempt from responding to operation invocations. Such objects may 'hang' on the invoking process. They divide responsive failures into three models: R-crash,R-omission, and R-arbitrary. They divide non-responsive failures into crash, omission, and arbitrary. An object subject to crash failure behaves correctly until it fails, and once it fails, it never responds to operation invocations. An object subject to omission failures may fail to respond to the invocations of an arbitrary subset of processes, but continue to respond to the invocations of the remaining processes (forever).<<ETX>>
[NASA, Laboratories, fault tolerant wait-free shared objects, Data structures, nonresponsive object failures, arbitrary, Computer crashes, crash, Fault tolerance, R-arbitrary, R-omission, responsive, Hardware, data structures, fault tolerant computing, object-oriented methods, object failures, Time factors, R-crash, omission, Testing]
Separating the communication complexities of MOD m and MOD p circuits
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author proves in this paper that it is much harder to evaluate depth-2, size-N circuits with MOD m gates than with MOD p gates by k-party communication protocols: he shows a k-party protocol which communicates O(1) bits to evaluate circuits with MOD p gates, while evaluating circuits with MOD m gates needs Omega (N) bits, where p denotes a prime, and m a composite, non-prime power number. As a corollary, for all m, he shows a function, computable with a depth-2 circuit with MOD m gates, but not with any depth-2 circuit with MOD p gates. He proves in the second part that the GIP function by L. Babai et al. (1989) needs exponential size in n when it is computed by some depth-3 circuits, with threshold, symmetric, and MOD m gates.<<ETX>>
[depth-2, Protocols, Circuits, GIP function, communication complexities, k-party communication protocols, MOD m circuits, Complexity theory, communication complexity, Game theory, Galois fields, size-N circuits, Computer science, Upper bound, MOD p circuits, Cost function, Computational efficiency, protocols]
On four-connecting a triconnected graph
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author considers the problem of finding a smallest set of edges whose addition four-connects a triconnected graph. This is a fundamental graph-theoretic problem that has applications in designing reliable networks. He presents an O(n alpha (m,n)+m) time sequential algorithm for four-connecting an undirected graph G that is triconnected by adding the smallest number of edges, where n and m are the number of vertices and edges in G, respectively, and alpha (m, n) is the inverse Ackermann function. He presents a new lower bound for the number of edges needed to four-connect a triconnected graph. The form of this lower bound is different from the form of the lower bound known for biconnectivity augmentation and triconnectivity augmentation. The new lower bound applies for arbitrary k, and gives a tighter lower bound than the one known earlier for the number of edges needed to k-connect a (k-1)-connect graph. For k=4, he shows that this lower bound is tight by giving an efficient algorithm for finding a set edges with the required size whose addition four-connects a triconnected graph.<<ETX>>
[graph theory, computational geometry, Phase change random access memory, Application software, Parallel algorithms, Fault tolerance, triconnected graph, graph-theoretic problem, Tree graphs, four-connecting, Computer network reliability, reliable networks, inverse Ackermann function, Polynomials, Computer networks, computational complexity]
Towards a computational theory of statistical tests
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors initiate a computational theory of statistical tests. Loosely speaking, an algorithm is a statistical test if it rejects a 'negligible' fraction of strings. A statistical test is universal for a class of algorithms if it rejects all (but finitely many) of the strings rejected by each algorithm in the class. They consider the existence and efficiency of universal statistical tests for various classes of statistical tests. They also consider the relation between ensembles passing statistical tests of particular complexity and ensembles which are indistinguishable from uniform by algorithms of the same complexity. Some results refer to relatively simple statistical tests (e.g. those implemented by counter machines).<<ETX>>
[universal statistical tests, complexity, automata theory, Computational complexity, Statistics, Counting circuits, Computer science, statistical tests, Fault detection, Writing, counter machines, Sampling methods, Polynomials, computational theory, statistical analysis, Testing, computational complexity, algorithm]
The complexity of parallel prefix problems on small domains
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors study the complexity of some prefix problems in the CRCW PRAM model. The main result is an Omega ( alpha (n)) lower bound for chaining, matching a previous upper bound and solving an open problem. They give reductions to show an Omega ( alpha (n)) lower bound on the complexity of the prefix maxima and range maxima problems even when the domain is (1,...,n). An interesting consequence is that prefix maximum is strictly harder than simple maximum. They also give a reduction to show an Omega ( alpha (n)) lower bound on a parenthesis matching problem, matching the upper bound. No lower bounds were previously known for any of these problems. The lower bounds contribute to the study of very fast parallel algorithms by introducing techniques for proving lower bounds for small domain problems.<<ETX>>
[parallel algorithms, complexity, Law, CRCW PRAM model, prefix maxima, Phase change random access memory, very fast parallel algorithms, lower bound, Parallel algorithms, parallel prefix problems, Computer science, Concurrent computing, parenthesis matching problem, Upper bound, range maxima, small domain problems, small domains, chaining, Joining processes, Legal factors, computational complexity]
Competitive analysis of financial games
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
In the unidirectional conversion problem an on-line player is given the task of converting dollars to yen over some period of time. Each day, a new exchange rate is announced and the player must decide how many dollars to convert. His goal is to minimize the competitive ratio. defined as sup/sub E/ (P/sub OPT/(E)/P/sub X/E) where E ranges over exchange rate sequences. P/sub OPT/(E) is the number of yen obtained by an optimal off-line algorithm, and Px(E) is the number of yen obtained by the on-line algorithm X. The authors also consider a continuous version of the problem. in which the exchange rate varies over a continuous time interval. The on-line line players a priori information about the fluctuation of exchange rates distinguishes different variants of the problem. For three variants they show that a simple threat-based strategy is optimal for the on-line player and determine its competitive ratio. They also derive and analyze an optimal policy for the on-line player when he knows the probability distribution of the maximum value that the exchange rate will reach. Finally, they consider a bidirectional conversion problem, which the player may trade dollars for yen or yen for dollars.<<ETX>>
[Algorithm design and analysis, optimal policy, continuous time interval, Decision making, bidirectional conversion problem, game theory, Probability distribution, online player, unidirectional conversion problem, History, financial games, competitive ratio, Exchange rates, Loans and mortgages, a priori information, Investments, threat-based strategy, Cost function, exchange rate, Stock markets, foreign exchange trading, Portfolios]
A theory of wormhole routing in parallel computers
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Virtually all theoretical work on message routing in parallel computers has dwelt on packet routing: messages are conveyed as packets, an entire packet can reside at a node of the network, and a packet is sent from the queue of one node to the queue of another node until its reaches its destination. The current trend in multicomputer architecture, however, is to use wormhole routing. In wormhole routing a message is transmitted as a contiguous stream of bits, physically occupying a sequence of nodes/edges in the network. Thus, a message resembles a worm burrowing through the network. The authors give theoretical analyses of simple wormhole routing algorithms, showing them to be nearly optimal for butterfly and mesh connected networks. The analysis requires initial random delays in injecting messages to the network. They report simulation results suggesting that the idea of random initial delays is not only useful for theoretical analysis but may actually improve the performance of wormhole routing algorithms.<<ETX>>
[Algorithm design and analysis, Computer worms, multiprocessor interconnection networks, initial random delays, Routing, message routing, Delay, Concurrent computing, Analytical models, wormhole routing, telecommunication network routing, butterfly, simulation results, Computer architecture, parallel computers, mesh connected networks, Computer networks, multicomputer architecture, Performance analysis, message switching, Queueing analysis, packet routing]
Fault tolerant graphs, perfect hash functions and disjoint paths
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Given a graph G on n nodes the authors say that a graph T on n + k nodes is a k-fault tolerant version of G, if one can embed G in any n node induced subgraph of T. Thus T can sustain k faults and still emulate G without any performance degradation. They show that for a wide range of values of n, k and d, for any graph on n nodes with maximum degree d there is a k-fault tolerant graph with maximum degree O(kd). They provide lower bounds as well: there are graphs G with maximum degree d such that any k-fault tolerant version of them has maximum degree at least Omega (d square root k).<<ETX>>
[graph theory, computational geometry, Mathematics, Graph theory, perfect hash functions, performance degradation, k-fault tolerant graph, History, Degradation, Fault tolerance, file organisation, fault tolerant computing, disjoint paths, Joining processes]
Truly alphabet-independent two-dimensional pattern matching
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
A. Amir, G. Benson and M. Farach (see Proc. 24th STOC, p.59-68 (1992)) gave an algorithm for two-dimensional pattern matching (ABF for short) whose text processing is independent of the alphabet and takes O(n/sup 2/) time, but whose pattern processing is dependent on the alphabet and takes O(m/sup 2/log mod Sigma mod ) time. The authors present an algorithm that is truly independent of the alphabet and takes linear O(m/sup 2/+n/sup 2/) time. As in the Knuth-Morris-Pratt algorithm, the only operation on the alphabet is the equality test of two symbols. All previous algorithms except the ABF algorithm reduce the two-dimensional problem into one-dimensional string matching, and use known techniques in string matching. The ABF algorithm uses two-dimensional periodicity for text processing, but their pattern processing resorts to one-dimensional techniques. The authors present a two-dimensional technique for both pattern processing and text processing.<<ETX>>
[Computer science, text processing, Knuth-Morris-Pratt algorithm, Birds, Educational institutions, character recognition, string matching, truly alphabet-independent 2D pattern matching, Pattern matching, Text processing, Testing]
Markov paging
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
This paper considers the problem of paging under the assumption that the sequence of pages accessed is generated by a Markov chain. The authors use this model to study the fault-rate of paging algorithms, a quantity of interest to practitioners. They first draw on the theory of Markov decision processes to characterize the paging algorithm that achieves optimal fault-rate on any Markov chain. They address the problem of efficiently devising a paging strategy with low fault-rate for a given Markov chain. They show that a number of intuitively good approaches fail. Their main result is an efficient procedure that, on any Markov chain, will give a paging algorithm with fault-rate at most a constant times optimal. Their techniques also show that some algorithms that do poorly in practice fail in the Markov setting, despite known (good) performance guarantees when the requests are generated independently from a probability distribution.<<ETX>>
[Algorithm design and analysis, Markov chain, Markov decision processes, virtual storage, fault-rate, Markov processes, Probability distribution, Markov paging, Paging strategies]
Proof verification and hardness of approximation problems
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The class PCP(f(n),g(n)) consists of all languages L for which there exists a polynomial-time probabilistic oracle machine that used O(f(n)) random bits, queries O(g(n)) bits of its oracle and behaves as follows: If x in L then there exists an oracle y such that the machine accepts for all random choices but if x not in L then for every oracle y the machine rejects with high probability. Arora and Safra (1992) characterized NP as PCP(log n, (loglogn)/sup O(1)/). The authors improve on their result by showing that NP=PCP(logn, 1). The result has the following consequences: (1) MAXSNP-hard problems (e.g. metric TSP, MAX-SAT, MAX-CUT) do not have polynomial time approximation schemes unless P=NP; and (2) for some epsilon >0 the size of the maximal clique in a graph cannot be approximated within a factor of n/sup epsilon / unless P=NP.<<ETX>>
[approximation problems, MAX-CUT, formal languages, polynomial-time probabilistic oracle machine, NP, metric TSP, MAX-SAT, Traveling salesman problems, proof verification, time complexity, MAXSNP-hard, stochastic automata, random bits, Computer science, Approximation algorithms, Polynomials, theorem proving, Logic, computational complexity]
Randomized consensus in expected O(n log/sup 2/ n) operations per processor
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The paper presents a new randomized algorithm for achieving consensus among asynchronous processors that communicate by reading and writing shared registers. The fastest previously known algorithm requires a processor to perform an expected O(n/sup 2/ log n) read and write operations in the worst case. In the algorithm, each processor executes at most an expected O(n log/sup 2/ n) read and write operations, which is close to the trivial lower bound of Omega (n). All previously known polynomial-time consensus algorithms were structured around a shared coin protocol in which each processor repeatedly adds random +or-1 votes to a common pool. Consequently, in all of these protocols, the worst case expected bound on the number of read and write operations done by a single processor is asymptotically no better than the bound on the total number of read and write operations done by all of the processors together. The authors succeed in breaking this tradition by allowing the processors to cast votes of increasing weights. This grants the adversary greater control since he can choose from up to n different weights (one for each processor) when determining the w i ht of the next vote to be cast. They prove that the shared coin protocol is correct nevertheless using martingale arguments.<<ETX>>
[Heart, Protocols, multiprocessor interconnection networks, game theory, Data structures, Control systems, reading, worst case expected bound, Registers, randomized algorithm, History, martingale arguments, synchronisation, consensus, Processor scheduling, Voting, shared coin protocol, writing, Writing, asynchronous processors, Polynomials, shared registers]
Fully dynamic biconnectivity in graphs
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author presents an algorithm for maintaining the bi-connected components of a graph during a sequence of edge insertions and deletions. It requires linear storage and preprocessing time. The amortized running time for insertions and for deletions is O(m/sup 2/3/), where m is the number of edges in the graph. Each query of the form 'Are the vertices u and v biconnected?' can be answered in time O(1). This is the first sublinear algorithm for this problem. If the input is a planar embedded graph, the amortized running time for insertions and deletions drops to O( square root nlogn) and the worst case query time is O((logn)/sup 2/), where n is the number of vertices in the graph. The best previously known solution takes time O(n/sup 2/3/) per update or query.<<ETX>>
[linear storage, Heuristic algorithms, graph theory, deletions, query time, computational geometry, time complexity, Data structures, amortized running time, Computer science, Bridges, graphs, dynamic biconnectivity, planar embedded graph, computational complexity, edge insertions]
Mick gets some (the odds are on his side) (satisfiability)
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Consider a randomly generated boolean formula F (in the conjunctive normal form) with m clauses of size k over n variables; k is fixed at any value greater than 1, but n tends to infinity and m = (1 + o(1))cn for some c depending only on k. It is easy to see that F is unsatisfiable with probability 1-o(1) whenever c>(ln 2)2/sup k/; the authors complement this observation by proving that F is satisfiable with probability 1-o(1) whenever c<(0.25)2/sup k//k; in fact, they present a linear-time algorithm that satisfies F with probability 1-o(1). In addition, they establish a threshold for 2-SAT: if k = 2 then F is satisfiable with probability 1-o(1) whenever c<1 and unsatisfiable with probability 1-o(1) whenever c>1.<<ETX>>
[Chaos, unsatisfiability, probability, H infinity control, computability, time complexity, randomly generated boolean formula, Boolean algebra, conjunctive normal form, linear-time algorithm, Computer science, truth assignment, satisfiability, Random variables, computational complexity]
On the bit extraction problem
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Consider a coloring of the n-dimensional Boolean cube with c=2/sup s/ colors in such a way that every k-dimensional subcube is equicolored, i.e. each color occurs the same number of times. The author shows that for such a coloring one necessarily has (k-1)/n>or= theta /sub c/=(c/2-1)/(c-1). This resolves the 'bit extraction' or 't-resilient functions' problem (also a special case of the privacy amplification problem) in many cases, such as c-1/n, proving that XOR type colorings are optimal, and always resolves this question to within c/4 in determining the optimal value of k (for any fixed n and c). He also studies the problem of finding almost equicolored colorings when (k-1)/n< theta /sub c/, and of classifying all optimal colorings.<<ETX>>
[Computer science, Privacy, XOR type colorings, Boolean functions, bit extraction problem, privacy amplification problem, almost equicolored colorings, coloring, Cryptography, n-dimensional Boolean cube, graph colouring]
On the fault tolerance of some popular bounded-degree networks
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors analyze the fault-tolerance properties of several bounded-degree networks that are commonly used for parallel computation. Among other things, they show that an N-node butterfly containing N/sup 1- epsilon / worst-case faults (for any constant epsilon >0) can emulate a fault-free butterfly of the same size with only constant slowdown. Similar results are proved for the shuffle-exchange graph. Hence, these networks become the first connected bounded-degree networks known to be able to sustain more than a constant number of worst-case faults without suffering more than a constant-factor slowdown in performance. They also show that an N-node butterfly whose nodes fail with some constant probability p can emulate a fault-free version of itself with a slowdown of 2/sup O(log* N)/, which is a very slowly increasing function of N. The proofs of these results combine the technique of redundant computation with new algorithms for routing packets around faults in hypercubic networks. Techniques for reconfiguring hypercubic networks around faults that do not rely on redundant computation are also presented. These techniques tolerate fewer faults but are more widely applicable since they can be used with other networks such as binary trees and meshes of trees.<<ETX>>
[shuffle-exchange graph, binary trees, parallel computation, routing packets, fault tolerance, Laboratories, multiprocessor interconnection networks, Routing, Mathematics, worst-case faults, Computer science, Concurrent computing, Fault tolerance, bounded-degree networks, hypercubic networks, National electric code, Binary trees, constant-factor slowdown, Computer networks, fault tolerant computing, meshes of trees, Contracts]
Tighter bounds on the exact complexity of string matching
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The paper considers how many character comparisons are needed to find all occurrences of a pattern of length m in a text of length n. The main contribution is to show an upper bound of the form n + O(n/m) character comparisons, following preprocessing. Specifically, the authors show an upper bound of n+8/3(m+1)(n-m) character comparisons. This bound is achieved by an online algorithm which performs O(n) work in total, requires O(m) space and O(m/sup 2/) time for preprocessing. In addition the following lower bounds are shown: for online algorithms, a bound of n+11/5(m+1) (n-m) character comparisons for m = 10 + 11 k, for any integer k >or= 1, and for general algorithms, a bound of n+2(n-m)/m+3 character comparisons, for m=2 k+l, for any integer k>or=1.<<ETX>>
[Algorithm design and analysis, Costs, online algorithm, Time measurement, character comparisons, exact complexity, Upper bound, Automata, Polynomials, string matching, Pattern matching, search problems, computational complexity, pattern recognition]
On efficient band matrix arithmetic
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
An efficient parallel Las Vegas algorithm is presented for computation of the determinant of a non-singular band matrix and for the solution of a system of linear equations with a band matrix as coefficient matrix. The algorithm can be implemented using time polylogarithmic in n with O(nm/sup omega -1/) processors, in order to process an input matrix with order n and band width m, provided that n*n matrices can be multiplied in logarithmic time with O(n/sup omega /) processors. If asymptotically efficient matrix multiplication is used ( omega <3), then the time-processor product for the resulting algorithm is less than the number of steps used to solve these problems via Gaussian elimination. The algorithm is more general than previous processor efficient parallel algorithms for band matrix computations, since it can be applied to invert arbitrary nonsingular band matrices over arbitrary fields.<<ETX>>
[parallel algorithms, Costs, determinants, nonsingular band matrices, parallel Las Vegas algorithm, coefficient matrix, Sparse matrices, Parallel algorithms, logarithmic time, Equations, asymptotically efficient matrix multiplication, matrix algebra, Concurrent computing, Computer science, Councils, Gaussian elimination, Clustering algorithms, arbitrary fields, Polynomials, band matrix arithmetic, Arithmetic, computational complexity, determinant, linear equations]
The algorithmic aspects of the regularity lemma
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The regularity lemma of Szemeredi (1978) is a result that asserts that every graph can be partitioned in a certain regular way. This result has numerous applications, but its known proof is not algorithmic. The authors first demonstrate the computational difficulty of finding a regular partition; they show that deciding if a given partition of an input graph satisfies the properties guaranteed by the lemma is co-NP-complete. However, they also prove that despite this difficulty the lemma can be made constructive; they show how to obtain, for any input graph, a partition with the properties guaranteed by the lemma, efficiently. The desired partition, for an n-vertex graph, can be found in time O(M(n)), where M(n)=O(n/sup 2.376/) is the time needed to multiply two n by n matrices with 0,1-entries over the integers. The algorithm can be parallelized and implemented in NC/sup 1/.<<ETX>>
[regularity lemma, parallel algorithms, graph theory, parallelism, computational geometry, Graph theory, Partitioning algorithms, Parallel algorithms, Combinatorial mathematics, regular partition, Computer science, Microwave integrated circuits, decidability, partition, computational difficulty, input graph, computational complexity]
Apple tasting and nearly one-sided learning
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
In the standard on-line model the learning algorithm tries to minimize the total number of mistakes made in a series of trials. On each trial the learner sees an instance, either accepts or rejects that instance, and then is told the appropriate response. The authors define a natural variant of this model ('apple tasting') where the learner gets feedback only when the instance is accepted. They use two transformations to relate the apple tasting model to an enhanced standard model where false acceptances are counted separately from false rejections. They present a strategy for trading between false acceptances and false rejections in the standard model. From one perspective this strategy is exactly optimal, including constants. They apply the results to obtain a good general purpose apple tasting algorithm as well as nearly optimal apple tasting algorithms for a variety of standard classes, such as conjunctions and disjunctions of n boolean variables. They also present and analyze a simpler transformation useful when the instances are drawn at random rather than selected by an adversary.<<ETX>>
[learning algorithm, false acceptances, false rejections, conjunctions, Predictive models, Loss measurement, apple tasting model, Computer science, disjunctions, Feedback, nearly one-sided learning, National electric code, Sampling methods, learning (artificial intelligence), boolean variables]
Enumerating the k closest pairs optimally
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Let S be a set of n points in D-dimensional space, where D is a constant, and let k be an integer between 1 and (/sub 2//sup n/) An algorithm is given that computes the k closest pairs in the set S in O(nlogn+k) time, using O(n+k) space. The algorithm fits in the algebraic decision tree model and is, therefore, optimal.<<ETX>>
[Algorithm design and analysis, computational geometry, time complexity, algebraic decision tree model, k closest pairs, Contracts, D-dimensional space, Testing, computational complexity]
Back to the future: towards a theory of timed regular languages
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors introduce two-way timed automata-timed automata that can move back and forth while reading a timed word. Two-wayness in its unrestricted form leads, like nondeterminism, to the undecidability of language inclusion. However, if they restrict the number of times an input symbol may be revisited, then two-wayness is both harmless and desirable. The authors show that the resulting class of bounded two-way deterministic timed automata is closed under all boolean operations, has decidable (PSPACE-complete) emptiness and inclusion problems, and subsumes all decidable real-time logics we know. They obtain a strict hierarchy of real-time properties: deterministic timed automata can accept more languages as the bound on the number of times an input symbol may be revisited is increased. This hierarchy is also enforced by the number of alternations between past and future operators in temporal logic. The combination of the results leads to a decision procedure for a real-time logic with past operators.<<ETX>>
[formal languages, Military computing, Computational modeling, Instruments, boolean operations, theory of timed regular languages, two-way timed automata, temporal logic, PSPACE-complete, undecidability, Computer science, Boolean functions, deterministic automata, Automata, Robustness, Timing, Logic, Contracts]
Computing in solvable matrix groups
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author announces methods for efficient management of solvable matrix groups over finite fields. He shows that solvability and nilpotence can be tested in polynomial-time. Such efficiency seems unlikely for membership-testing, which subsumes the discrete-log problem. However, assuming that the primes in mod G mod (other than the field characteristic) are polynomially-bounded, membership-testing and many other computational problems are in polynomial time. These problems include finding stabilizers of vectors and of subspaces and finding centralizers and intersections of subgroups. An application to solvable permutation groups puts the problem of finding normalizers of subgroups into polynomial time. Some of the results carry over directly to finite matrix groups over algebraic number fields; thus, testing solvability is in polynomial time, as is testing membership and finding Sylow subgroups.<<ETX>>
[Poles and towers, Sylow subgroups, solvability, centralizers, finite matrix groups, Galois fields, intersections, algebraic number fields, matrix algebra, group theory, Information science, finite fields, solvable permutation groups, subgroups, Polynomials, Matrices, Libraries, nilpotence, solvable matrix groups, polynomial-time, discrete-log problem, membership-testing, Testing, computational complexity]
On the exact learning of formulas in parallel
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors investigate the parallel complexity of learning formulas from membership and equivalence queries. They consider a number of learning problems that can be solved sequentially in polynomial time. They prove some upper and lower bounds on the number of parallel steps required to solve these problems with a polynomial number of processors.<<ETX>>
[parallel algorithms, parallel complexity, learning problems, equivalence queries, Phase change random access memory, exact learning, membership, Parallel algorithms, Computer science, Upper bound, parallel, Parallel processing, Polynomials, polynomial time, learning (artificial intelligence), Arithmetic, computational complexity]
Clock construction in fully asynchronous parallel systems and PRAM simulation
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors discuss the question of simulating synchronous computations on asynchronous systems. They consider an asynchronous system with very weak, or altogether lacking any, atomicity assumptions. The first contribution of this paper is a novel clock for asynchronous systems. The clock is a basic tool for synchronization in the asynchronous environment. It is a very robust construction and can operate in a system with no atomicity assumptions, and in the presence of a dynamic scheduler. The behavior of the clock is obtained with overwhelming probability (1-2/sup - alpha n/, alpha >0). The authors show how to harness this clock to drive a PRAM simulation on an asynchronous system. The resulting simulation scheme is more efficient than existing ones, while actually relaxing the assumptions on the underlying asynchronous system.<<ETX>>
[parallel algorithms, random-access storage, atomicity, Computational modeling, probability, Phase change random access memory, Dynamic scheduling, Mathematics, clock construction, Synchronization, Parallel algorithms, Computer science, Concurrent computing, clocks, fully asynchronous parallel systems, dynamic scheduler, Robustness, Clocks, computational complexity, PRAM simulation]
On minimum and maximum spanning trees of linearly moving points
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors investigate the upper bounds on the numbers of transitions of minimum and maximum spanning trees (MinST and MaxST for short) for linearly moving points. Suppose that one is given a set of n points in general d-dimensional space, S=(p/sub 1/,p/sub 2/, . . ., p/sub n/), and that all points move along different straight lines at different but fixed speeds, i.e., the position of p/sub i/ is a linear function of a real parameter. They investigate the numbers of transitions of MinST and MaxST when t increases from - infinity to + infinity . They assume that the dimension d is a fixed constant. Since there are O(n/sup 2/) distances among n points, there are naively O(n/sup 4/) transitions of MinST and MaxST. They improve these trivial upper bounds for L/sub 1/ and L/sub infinity / distance metrics. Let c/sub p/(n, min) (resp. c/sub p/(n, max)) be the number of maximum possible transitions of MinST (resp. MaxST) in L/sub p/ metric for n linearly moving points. They give the following results; c/sub 1/(n, min)=O(n/sup 5/2/a(n)), c/sub infinity /(n, min)=O(n/sup 5/2/a(n)), c/sub 1/(n, max)=O(n/sup n/) and c/sub infinity /(n, max)=O(n/sup 2/) where O(n) is the inverse Ackermann function. They also investigate two restricted cases.<<ETX>>
[trees (mathematics), computational geometry, real parameter, distance metrics, spanning trees, straight lines, Postal services, Motion planning, Computational geometry, trivial upper bounds, Upper bound, inverse Ackermann function, linearly moving points, Robots, Business]
Halvers and expanders (switching)
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors investigate the asymptotic efficiency of certain combinatorial networks called halvers, which are basic building blocks of many parallel algorithms. They improve the efficiency of halvers in terms of their depth. The novelty is the use of combinatorial circuits whose basic units are k-sorter switches.<<ETX>>
[parallel algorithms, Delay effects, combinatorial circuits, expanders, combinatorial networks, Switches, Registers, asymptotic efficiency, k-sorter switches, Parallel algorithms, Sorting, Switching circuits, halvers, Binary trees, combinatorial switching, comparator switch, Bipartite graph, building blocks]
The asymptotic complexity of merging networks
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Let M(m,n) be the minimum number of comparators needed in a comparator network that merges m elements x/sub 1/<or=x/sub 2/<or=. . .<or=x/sub m/ and n elements y/sub 1/<or=y/sub 2/. . .<or=y/sub n/, where n>or=m. Batcher's odd-even merge yields the following upper bound: M(m,n)<or=/sup 1///sub 2/(m+n)log/sub 2/(m+1)+O(n); in particular, M(n,n)<or=nlog/sub 2/n+O(n). The authors prove the following lower bound that matches the upper bound above asymptotically as n>or=m to infinity :M(m,n)>or=/sup 1///sub 2/(m+n)log/sub 2/(m+1)-O(m); in particular, M(n,n)>or=nlog/sub 2/n-O(n). The authors' proof technique extends to give similarly tight lower bounds for the size of monotone Boolean circuits for merging, and for the size of switching networks capable of realizing the set of permutations that arise from merging.<<ETX>>
[Merging, computer networks, Switches, upper bound, lower bound, permutations, monotone Boolean circuits, Sorting, Switching circuits, Computer science, Upper bound, switching networks, comparator network, asymptotic complexity, sorting, merging networks, Contracts, comparators (circuits), computational complexity, comparators]
A decomposition theorem and bounds for randomized server problems
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors prove a lower bound of Omega ( square root logk/loglogk) for the competitive ratio of randomized algorithms for the k-server problem against an oblivious adversary. The bound holds for arbitrary metric spaces (of at least k+1 points) and provides a new lower bound for the metrical task system problem as well. This improves the previous best lower bound of Omega (loglogk) for arbitrary metric spaces, more closely approaching the conjectured lower bound of Omega (logk). They also prove a lower bound of Omega (/sup logk///sub loglogk/) for the server problem on k+1 equally-spaced points on a line, which corresponds to some natural motion-planning problems.<<ETX>>
[decomposition theorem, queueing theory, Motion-planning, arbitrary metric spaces, Extraterrestrial measurements, Mathematics, motion-planning, lower bound, Motion measurement, randomized server problems, k-server problem, Game theory, Postal services, Computer science, competitive ratio, Upper bound, Current measurement, file servers, algorithm theory, bounds, Cost function]
Probabilistic checking of proofs; a new characterization of NP
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors give a new characterization of NP: the class NP contains exactly those languages L for which membership proofs (a proof that an input x is in L) can be verified probabilistically in polynomial time using logarithmic number of random bits and sub-logarithmic number of queries to the proof. This is a non-relativizing characterization of NP. They discuss implications of this characterization; specifically, they show that approximating clique (or independent set) is NP-hard.<<ETX>>
[Protocols, formal languages, NP, languages, time complexity, NP-complete problem, independent set, Microwave integrated circuits, membership proofs, clique, Turing machines, Polynomials, polynomial time, theorem proving, Cryptography, Marine vehicles, computational complexity]
Improved parallel polynomial division and its extensions
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors compute the first N coefficients of the reciprocal r(x) of a given polynomial p(x), (r(x)p(x)=1 mod x/sup N/, p(0) not=0), by using, under the PRAM arithmetic models, O(h log N) time-steps and O((N/h)(1+2/sup -h/log/sup (h)/ N)) processors, for any h, h=1,2, . . .,log/sup */ N, provided that O(logm) steps and m processors suffice to perform DFT on m points and that log/sup (0)/ N=N, log/sup (h)/ N=log/sub 2/log/sup (h-1)/N, h=1, . . .,log/sup */N, log/sup */N=max(h:log/sup (h)/N>0). The same complexity estimates apply to some other computations, such as the division with a remainder of two polynomials of degrees O(N) and the inversion of an N*N triangular Toeplitz matrix. They also show how to extend the techniques to parallel implementation of other recursive processes, such as the evaluation modulo x/sup N/ of the m-th root, p(x)/sup 1/m/, of p(x) (for any fixed natural m), for which we need O(log N log log N) time-steps and O(N/log log N) processors. The paper demonstrates some new techniques of supereffective slowdown of parallel algebraic computations, which they combine with a technique of stream contraction.<<ETX>>
[supereffective slowdown, parallel algorithms, Costs, polynomials, complexity estimates, parallel polynomial division, recursive processes, Phase change random access memory, Educational institutions, Mathematics, reciprocal, PRAM arithmetic models, stream contraction, Concurrent computing, triangular Toeplitz matrix, Processor scheduling, parallel algebraic computations, polynomial, Polynomials, Mathematical model, State estimation, evaluation modulo, Arithmetic, computational complexity]
Data structural bootstrapping, linear path compression, and catenable heap ordered double ended queues
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors provide an efficient implementation of catenable mindeques. To prove that the resulting data structure achieves constant amortized time per operation, they consider order preserving path compression. They prove a linear bound on deque ordered spine-only path compression, a case of order persevering path compression employed by the data structure.<<ETX>>
[programming theory, queueing theory, catenable mindeques, data structure, Data structures, Computer science, catenable heap ordered double ended queues, Content addressable storage, Linearity, sorting, data structures, order preserving path compression, Contracts, linear path compression]
Quadratic dynamical systems
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The paper promotes the study of computational aspects, primarily the convergence rate, of nonlinear dynamical systems from a combinatorial perspective. The authors identify the class of symmetric quadratic systems. Such systems have been widely used to model phenomena in the natural sciences, and also provide an appropriate framework for the study of genetic algorithms in combinatorial optimisation. They prove several fundamental general properties of these systems, notably that every trajectory converges to a fixed point. They go on to give a detailed analysis of a quadratic system defined in a natural way on probability distributions over the set of matchings in a graph. In particular, they prove that convergence to the limit requires only polynomial time when the graph is a tree. This result demonstrates that such systems, though nonlinear, are amenable to quantitative analysis.<<ETX>>
[convergence, graph theory, computability, Nonlinear dynamical systems, nonlinear dynamical systems, combinatorial optimisation, Convergence, optimisation, Tree graphs, probability distributions, Genetics, Polynomials, polynomial time, Extraterrestrial phenomena, tree, Educational institutions, computational aspects, State-space methods, genetic algorithms, graph, Computer science, nonlinear equations, Ear, symmetric quadratic systems, convergence rate]
Efficient self-embedding of butterfly networks with random faults
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author studies the embedding of the butterfly network in a faulty version of itself where each node is independently faulty with some constant probability. He shows that such a self-embedding of the N-node butterfly with O(1) load, O((log logN)/sup 2.6/) dilation, and 0((log log N)/sup 8.2/) congestion is possible with high probability, assuming sufficiently small node-failure probability. This embedding is level-preserving in the sense that each node is mapped to a node in the same level of the butterfly. He also derives a lower bound of log log log N-c on the dilation of a level-preserving embedding with O(log/sup alpha / N) load, for any alpha , 0< alpha <1, any node-failure probability p>0, and some constant c depending on alpha and p.<<ETX>>
[random faults, Multiprocessor interconnection networks, level-preserving embedding, multiprocessor interconnection networks, Routing, node-failure probability, Land mobile radio, self-embedding, Computer science, Upper bound, dilation, butterfly networks, Hypercubes, Robustness, fault tolerant computing, congestion]
How to denest Ramanujan's nested radicals
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author presents a simple condition when nested radical expressions of depth two can be denested using real radicals or radicals of some bounded degree. He describes the structure of these denestings and determines an upper bound on the maximum size of a denesting. Also for depth two radicals he describes an algorithm that will find such a denesting whenever one exists. Unlike all previous denesting algorithms the algorithm does not use Galois theory. In particular, he avoids the construction of the minimal polynomial and splitting field of a nested radical expression. Thus he can obtain the first denesting algorithm whose run time is at most, and in general much less, than polynomial in description size of the minimal polynomial. The algorithm can be used to determine non-trivial denestings for expressions of depth larger than two.<<ETX>>
[Upper bound, nested radical expressions, Polynomials, Virtual manufacturing, denestings, Equations, Contracts, computational complexity, number theory, run time]
Hierarchies in transitive closure logic, stratified Datalog and infinitary logic
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors establish a general hierarchy theorem for quantifier classes in the infinitary logic L/sub infinity omega //sup omega / on finite structures. In particular, it is shown that no infinitary formula with bounded number of universal quantifiers can express the negation of a transitive closure. This implies the solution of several open problems in finite model theory: On finite structures, positive transitive closure logic is not closed under negation. More generally the hierarchy defined by interleaving negation and transitive closure operators is strict. This proves a conjecture of N. Immerman (1987). The authors also separate the expressive power of several extensions of Datalog, giving new insight in the fine structure of stratified Datalog.<<ETX>>
[transitive closure logic, stratified Datalog, Analog computers, query languages, Mathematics, universal quantifiers, Complexity theory, Database languages, formal logic, general hierarchy theorem, Logic, quantifier classes, infinitary logic, computational complexity]
Drawing planar graphs using the lmc-ordering
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author introduces a method to optimize the required area, minimum angle and number of bends of planar drawings of graphs on a grid. The main tool is a new type of ordering on the vertices and faces of triconnected planar graphs. With this method linear time and space algorithms can be designed for many graph drawing problems. He shows that every triconnected planar graph G can be drawn convexly with straight lines on an (2n-4)*(n-2) grid. If G has maximum degree four (three), then G can be drawn orthogonal with at most (/sup 3n///sub 2/)+3 (at most (/sup n///sub 2/)+1) bends on an n*n grid ((/sup n///sub 2/)*(/sup n///sub 2/) grid, respectively). If G has maximum degree d, then G can be drawn planar on an (2n-6)*(3n-6) grid with minimum angle larger than /sup 1///sub d-2/ radians and at most 5n-15 bends. These results give in some cases considerable improvements over previous results, and give new bounds in other cases. Several other results, e.g. concerning visibility representations, are included.<<ETX>>
[Computer aided software engineering, Heuristic algorithms, graph theory, minimum angle, vertices, Optimization methods, planar graphs, Very large scale integration, computational geometry, lmc-ordering, planar drawings, Computer science, Constraint optimization, Graphics, visibility representations, number of bends, triconnected planar graphs, Animation, Cost function, required area, faces]
Maximizing non-linear concave functions in fixed dimension
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Consider a convex set P in R/sup d/ and a piece wise polynomial concave function F: P to R. Let A be an algorithm that given a point x in IR/sup d/ computes F(x) if x in P, or returns a concave polynomial p such that p(x) <0 but for any y in P, p(y) >or= 0. The author assumes that d is fixed and that all comparisons in A depend on the sign of polynomial functions of the input point. He shows that under these conditions, one can find max/sub P/ F in time which is polynomial in the number of arithmetic operations of A. Using this method he gives the first strongly polynomial algorithms for many nonlinear parametric problems in fixed dimension, such as the parametric max flow problem, the parametric minimum s-t distance, the parametric spanning tree problem and other problems. In addition he shows that in one dimension, the same result holds even if one only knows how to approximate the value of F. Specifically, if one can obtain an alpha -approximation for F(x) then one can alpha -approximate the value of maxF. He thus obtains the first polynomial approximation algorithms for many NP-hard problems such as the parametric Euclidean traveling salesman problem.<<ETX>>
[nonlinear programming, Laboratories, Piecewise linear approximation, parametric spanning tree, convex set, concave polynomial, NP-hard problems, arithmetic operations, algorithm theory, polynomial functions, Polynomials, concave programming, fixed dimension, parametric max flow problem, Piecewise linear techniques, Traveling salesman problems, Linear programming, nonlinear parametric problems, parametric minimum s-t distance, piece wise polynomial concave function, Computer science, NP-hard problem, input point, Approximation algorithms, parametric Euclidean traveling salesman problem, Arithmetic, computational complexity]
On the second eigenvalue and linear expansion of regular graphs
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors investigate the relation between the second eigen-value and the linear expansion of regular graphs. The spectral method is the best currently known technique to prove lower bounds on the expansion. He improves this technique by showing that the expansion coefficient of linear-sized subsets of a k-regular graph G is at least k/2(1- square root max(0,1-/sub lambda 1(G)2//sup 4k-4/))/sup -/ , where lambda /sub 1/(G) is the second largest eigenvalue of the graph. In particular, the linear expansion of Ramanujan graphs, which have the property that the second largest eigenvalue is at most 2 square root k-1, is at least (k/2)/sup -/. This improves upon the best previously known lower bound of 3(k-2)/8. For any integer k such that k-1 is prime, he explicitly constructs an infinite family of k-regular graphs G/sub n/ on n vertices whose linear expansion is k/2 and such that lambda /sub 1/(G/sub n/)<or= square root k-1+0(1). Since the graphs G/sub n/ have asymptotically optimal second eigenvalue, this essentially shows the (k/2) is the best bound one can obtain using the second eigenvalue method.<<ETX>>
[asymptotically optimal second eigenvalue, Laboratories, Circuits, graph theory, computational geometry, linear expansion, Graph theory, regular graphs, Complexity theory, Sorting, eigenvalues and eigenfunctions, Computer science, Concurrent computing, spectral method, second eigenvalue, expansion coefficient, Eigenvalues and eigenfunctions, Polynomials, Ramanujan graphs, Contracts]
Randomized geometric algorithms and pseudo-random generators
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The so called randomized incremental algorithms in computational geometry can be thought of as a generalization of Quicksort to higher dimensional geometric problems. They all construct the geometric complex in the given problem, such as a Voronoi diagram or a convex polytope, by adding the objects in the input set, one at a time, in a random order. The author shows that the expected running times of most of the randomized incremental algorithms in computational geometry do not change (up to a constant factor), when the sequence of additions is not truly random but is instead generated using only O(log n) random bits. The pseudo-random generator used is a generalization of the well known linear congruential generator.<<ETX>>
[randomized incremental algorithms, Quicksort, expected running times, computational geometry, Linear programming, Partitioning algorithms, History, random number generation, Computational geometry, Voronoi diagram, Veins, Iterative algorithms, Polynomials, pseudo-random generators, Random variables, Random number generation, convex polytope]
A class of logic problems solvable by linear programming
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Several problems of propositional logic, such as satisfiability, MAXSAT and logical inference, can be formulated as integer programs. The authors consider sets of clauses for which these integer programs can be solved as linear programs. They prove that balanced sets of clauses have this property.<<ETX>>
[MAXSAT, Logic programming, propositional logic, integer programming, integer programs, computability, logical inference, Linear programming, linear programming, Linear matrix inequalities, inference mechanisms, formal logic, Sufficient conditions, logic problems, truth assignment, satisfiability, Polynomials, balanced sets of clauses]
Lower bounds on the depth of monotone arithmetic computations
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
Consider an arithmetic expression of length n involving only the operations (+,*) and non-negative constants. The authors prove lower bounds on the depth of any binary computation tree over the same set of operations and constants that computes such an expression. In their main result they exhibit a family of arithmetic expressions that requires computation trees of depth at least 1.5 log/sub 2/n-O(1). The authors also consider the family of arithmetic expressions defined by alternating 5-3 trees. For this family they show a tight bound of 5/(log/sub 2/15)log/sub 2/n+O(1) on the depth of any computation tree. This is the best known tight bound for any family of arithmetic expressions.<<ETX>>
[Computational modeling, lower bounds, monotone arithmetic computations, alternating 5-3 trees, Concurrent computing, Upper bound, depth, binary computation tree, tight bound, Binary trees, arithmetic expression, Arithmetic, computational complexity]
Dynamic half-space reporting, geometric optimization, and minimum spanning trees
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors describe dynamic data structures for half-space range reporting and for maintaining the minima of a decomposable function. Using these data structures, they obtain efficient dynamic algorithms for a number of geometric problems, including closest/farthest neighbor searching, fixed dimension linear programming, bi-chromatic closest pair, diameter, and Euclidean minimum spanning tree.<<ETX>>
[bi-chromatic closest pair, Heuristic algorithms, graph theory, Very large scale integration, computational geometry, Mathematics, decomposable function, fixed dimension linear programming, dynamic data structures, minima, optimisation, Tree graphs, spatial data structures, diameter, Computer graphics, geometric problems, geometric optimization, closest/farthest neighbor searching, Tree data structures, minimum spanning trees, Data structures, Application software, Computer science, half-space range reporting, Euclidean minimum spanning tree]
Newton's method for fractional combinatorial optimization
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The authors considers Newton's method for the linear fractional combinatorial optimization. He proves a strongly polynomial bound on the number of iterations for the general case. He considers the maximum mean-weight cut problem, which is a special case of the linear fractional combinatorial optimization. This problem is closely related to the parametric flow problem and the flow problem when the maximum arc cost is being minimised. He proves that Newton's method runs in O(m) iterations for the maximum mean-weight cut problem. One iteration is dominated by the maximum flow computation. This gives the best known strongly polynomial bound of O(m/sup 2/n) for all three problems mentioned.<<ETX>>
[iterative methods, Piecewise linear techniques, graph theory, Optimization methods, computational geometry, parametric flow problem, spanning trees, iterations, optimisation, Tree graphs, maximum arc cost, Search methods, Cost function, Polynomials, geometry, maximum mean-weight cut problem, Newton method, fractional combinatorial optimization, strongly polynomial bound]
Witnesses for Boolean matrix multiplication and for shortest paths
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The subcubic (O(n/sup w/) for w(3) algorithms to multiply Boolean matrices do not provide the witnesses; namely, they compute C=A.B but if C/sub ij/=1 they do not find an index k (a witness) such that A/sub ik/=B/sub kj/=1. The authors design a deterministic algorithm for computing the matrix of witnesses that runs in O(n/sup w/) time, where here O(n/sup w/) denotes O(n/sup w/(log n)/sup O(1)/). The subcubic methods to compute the shortest distances between all pairs of vertices also do not provide for witnesses; namely they compute the shortest distances but do not generate information for computing quickly the paths themselves. A witness for a shortest path from v/sub i/ to v/sub j/ is an index k such that v/sub k/ is the first vertex on such a path. They describe subcubic methods to compute such witnesses for several versions of the all pairs shortest paths problem. As a result, they derive shortest paths algorithms that provide characterization of the shortest paths in addition to the shortest distances in the same time (up to a polylogarithmic factor) needed for computing the distances; namely O(n/sup (3+w)/2/) time in the directed case and O(n/sup w/) time in the undirected case. They also design an algorithm that computes witnesses for the transitive closure in the same time needed to compute witnesses for Boolean matrix multiplication.<<ETX>>
[Algorithm design and analysis, graph theory, computational geometry, Mathematics, Boolean algebra, deterministic algorithm, matrix algebra, Computer science, Shortest path problem, subcubic methods, shortest paths, algorithm theory, Random variables, Boolean matrix multiplication, transitive closure, computational complexity, witnesses]
Approximate max flow on small depth networks
Proceedings., 33rd Annual Symposium on Foundations of Computer Science
None
1992
The author considers the maximum flow problem on directed acyclic networks with m edges and depth r (length of the longest s-t path). The main result is a new deterministic algorithm for solving the relaxed problem of computing an s-t flow of value at least (1- epsilon ) of the maximum flow. For instances where r and epsilon /sup -1/ are small (i.e., O(polylog(m))), this algorithm is in NC and uses only O(m) processors, which is a significant improvement over existing parallel algorithms. As one consequence, he obtains an NC O(m) processor algorithm to find a bipartite matching of cardinality (1- epsilon ) of the maximum (for epsilon /sup -1/ = O(polylog(m))). The parallel bounds are based on a novel approach to the blocking flow problem that produces fractional valued flow augmentations even when capacities are integral. She shows that a fractional flow on any network with integral capacities can be rounded in polylogarithmic time to an integral flow of no smaller value using O(m) processors. Hence, within the same resource bounds, an integral flow can be obtained when desired.<<ETX>>
[directed acyclic networks, fractional valued flow augmentations, parallel algorithms, computational geometry, Phase change random access memory, deterministic algorithm, Parallel algorithms, fractional flow, resource bounds, Integral equations, directed graphs, maximum flow problem, Polynomials, geometry, small depth networks, polylogarithmic time, NC algorithm, computational complexity]
Near-quadratic bounds for the motion planning problem for a polygon in a polygonal environment
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We consider the problem of planning the motion of an arbitrary k-sided polygonal robot B, free to translate and rotate in a polygonal environment V bounded by n edges. We show that the combinatorial complexity of a single connected component of the free configuration space of B is k/sup 3/n/sup 2/2/sup O(log(2/3)/ n). This is a significant improvement of the naive bound O((kn)/sup 3/); when k is constant, which is often the case in practice, this yields a near-quadratic bound on the complexity of such a component, which almost settles (in this special case) a long-standing conjecture regarding the complexity of a single cell in a three-dimensional arrangement of surfaces. We also present an algorithm that constructs a single component of the free configuration space of B in time O(n/sup 2+/spl epsi//), for any /spl epsi/>0, assuming B has a constant number of sides. This algorithm, combined with some standard techniques in motion planning, yields a solution to the underlying motion planning problem, within the same asymptotic running time.<<ETX>>
[Pulp manufacturing, computational geometry, polygonal environment, mobile robots, path planning, Research and development, Orbital robotics, combinatorial complexity, Motion planning, Computer science, Upper bound, single connected component, Robot kinematics, polygon, position control, arbitrary k-sided polygonal robot, computational complexity, near-quadratic bounds, motion planning problem]
Top-down lower bounds for depth 3 circuits
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We present a top-down lower bound method for depth 3 AND-OR-NOT circuits which is simpler than the previous methods and in some cases gives better lower bounds. In particular we prove that depth 3 AND-OR-NOT circuits that compute PARITY resp. MAJORITY require size at least 2/sup 0.618/ .../spl radic/n/ resp. 2/sup 0.849/.../spl radic/n/. This is the first simple proof of a strong lower bound by a top-down argument for non-monotone circuits.<<ETX>>
[depth 3 AND-OR-NOT circuits, Costs, Computational modeling, depth 3 circuits, Circuit analysis, Combinatorial mathematics, nonmonotone circuits, Switching circuits, Computer science, top-down lower bounds, strong lower bound, top-down argument, Boolean functions, logic circuits, computational complexity]
A linear-processor polylog-time algorithm for shortest paths in planar graphs
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We give an algorithm requiring polylog time and a linear number of processors to solve single-source shortest paths in directed planar graphs, bounded-genus graphs, and 2-dimensional overlap graphs. More generally, the algorithm works for any graph provided with a decomposition tree constructed using size-O(/spl radic/n polylog n) separators.<<ETX>>
[Transmission line matrix methods, bounded-genus graphs, Particle separators, decomposition tree, planar graphs, computational geometry, Parallel algorithms, Concurrent computing, Tree graphs, directed graphs, 2-dimensional overlap graphs, shortest paths, linear-processor polylog-time algorithm, directed planar graphs, separators, Contracts]
Geometric discrepancy revisited
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Discrepancy theory addresses the general issue of approximating one measure by another one. Originally an offshoot of diophantine approximation theory, the area has expanded into applied mathematics, and now, computer science. Besides providing the theoretical foundation for sampling, it holds some of the keys to understanding the computational power of randomization. A few applications of discrepancy theory are listed. We give elementary algorithms for estimating the discrepancy between various measures arising in practice. We also present a general technique for proving discrepancy lower bounds.<<ETX>>
[Atomic measurements, Algorithm design and analysis, diophantine approximation theory, geometric discrepancy, sampling, computational geometry, Mathematics, Approximation methods, Application software, Finite element methods, randomised algorithms, discrepancy theory, Computer science, elementary algorithms, randomization, Computer graphics, discrepancy lower bounds, Computer errors, Sampling methods, computational power, computational complexity]
On representations by low-degree polynomials
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
In the first part of the paper we show that a subset S of a boolean cube B/sub n/ embedded in the projective space P/sup n/ can be approximated by a subset of B/sub n/ defined by nonzeroes of a low-degree polynomial only if the values of the Hilbert function of S are sufficiently small relative to the size of S. The use of this property provides a simple and direct technique for proving lower bounds on the size of ACC[p/sup r/] circuits. In the second part we look at the problem of computing many-output function by ACC[p/sup r/] circuit and give an example when such a circuit can be correct only at exponentially small fraction of assignments.<<ETX>>
[Embedded computing, polynomials, Circuits, nonzeroes, Complexity theory, Computer science, low-degree polynomials, projective space, low-degree polynomial, Computational geometry, Boolean functions, boolean cube, Hilbert function, Polynomials, Hilbert space, many-output function]
Highly efficient asynchronous execution of large-grained parallel programs
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
An n-thread parallel program p is large-grained if in every parallel step the computations on each of the threads are complex procedures requiring numerous processor instructions. This practically relevant style of programs differs from PRAM programs in its large granularity and the possibility that within a parallel step the computations on different threads may considerably vary in size. Let M be an n-processor asynchronous parallel system, with no restriction on the degree of asynchrony and without any specialized synchronization mechanisms. It is a challenging theoretical as well as practically important problem to ensure correct execution of P on such a parallel machine. Let P be a large-grained program requiring total work W for its execution on a synchronous a-processor parallel system. We present a transformation (compilation) of P into a program C(P) which correctly and efficiently effects the computation of P on the asynchronous machine M. Under moderate assumptions on the granularity of threads and the size of the program variables, execution of C(P) requires just O(Wlog* n) expected total work, and the memory space overhead is a small multiplicative constant.<<ETX>>
[PRAM programs, Parallel machines, Phase change random access memory, n-processor asynchronous parallel system, Application software, synchronization mechanisms, Yarn, processor instructions, parallel programming, synchronisation, Concurrent computing, Computer science, Bridges, Computer aided instruction, memory space overhead, granularity, large-grained parallel programs, highly efficient asynchronous execution, Error correction codes, Contracts]
Product range spaces, sensitive sampling, and derandomization
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We introduce the concept of a sensitive /spl epsi/-approximation, and use it to derive a more efficient algorithm for computing /spl epsi/-nets. We define and investigate product range spaces, for which we establish sampling theorems analogous to the standard finite VC-dimensional case. This generalizes and simplifies results from previous works. We derive a simpler optimal deterministic convex hull algorithm, and by extending the method to the intersection of a set of balls with the same radius, we obtain an O(nlog/sup 3/ n) deterministic algorithm for computing the diameter of an n-point set in 3-dimensional space.<<ETX>>
[sensitive sampling, US Department of Energy, n-point set, computational geometry, convex programming, Mathematics, derandomization, deterministic algorithm, deterministic algorithms, Computer science, Geometry, optimal deterministic convex hull algorithm, 3-dimensional space, standard finite VC-dimensional case, sampling theorems, Sampling methods, Polynomials, product range spaces]
Space bounds for graph connectivity problems on node-named JAGs and node-ordered JAGs
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Two new models, NO-JAG and NN-JAG in order of increasing computation power, are introduced as extensions to the conventional JAG model. A space lower bound of /spl Omega/(log/sup 2/ n/log log n) is proved for the problem of directed st-connectivity on a probabilistic NN-JAG and a space upper bound of O(log n) is proved for the problem of directed st-nonconnectivity on a nondeterministic NO-JAG. It is also shown that a nondeterministic NO-JAG is nearly as powerful as a nondeterministic Turing machine.<<ETX>>
[automata theory, graph theory, Binary decision diagrams, Search problems, node-ordered JAGs, nondeterministic NO-JAG, Computer science, graph connectivity problems, Upper bound, probabilistic automata, Turing machines, node-named JAGs, Automata, space bounds, computational complexity, nondeterministic Turing machine]
Fast algorithms for constructing t-spanners and paths with stretch t
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
The distance between two vertices in a weighted graph is the weight of a minimum-weight path between them. A path has stretch t if its weight is at most t times the distance between its end points. We consider a weighted undirected graph G=(V, E) and present algorithms that compute paths with stretch 2/spl les/t/spl les/log n. We present a O/spl tilde/((m+k)n/sup (2+/spl epsiv///t)) time randomized algorithm that finds paths between k specified pairs of vertices and a O/spl tilde/((m+ns)n/sup 2(1+log(n)/ /sup m+/spl epsiv/)/t/) deterministic algorithm that finds paths from s specified sources to all other vertices (for any fixed /spl epsiv/>0), where n=|V| and m=|E|. This improves significantly over the slower O/spl tilde/(min{k, n}m) exact shortest paths algorithms and a previous O/spl tilde/(mn/sup 64/t/+kn/sup 32/t/) time algorithm by Awerbuch et al. A t-spanner of a graph G is a set of weighted edges on the vertices of G such that distances in the spanner are not smaller and within a factor of t from the corresponding distances in G. Previous work was concerned with bounding the size and efficiently constructing t-spanners. We construct t-spanners of size O/spl tilde/(n/sup 1+(2+/spl epsiv///t)) in O/spl tilde/(mn/sup (2+/spl epsiv///t)) expected time (for any fixed /spl epsiv/>0), what constitutes a faster construction (by a factor of n/sup (3+2//t)) of sparser spanners than was previously attainable. We also provide efficient parallel constructions. Our algorithms are based on new structures called pairwise-covers and a novel approach to construct them efficiently.<<ETX>>
[weighted graph, Costs, t-spanners constructions, paths with stretch, graph theory, vertices, exact shortest paths algorithms, computational geometry, minimum-weight path, randomized algorithm, weighted undirected graph, deterministic algorithm, deterministic algorithms, Leg, randomised algorithms, IEL, Upper bound, pairwise-covers, Joining processes]
On the value of information in coordination games
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We discuss settings where several "agents" combine efforts to solve problems. This is a well-known setting in distributed artificial intelligence. Our work addresses theoretical questions in this model which are motivated by the work of X. Deng and C.H. Papadimitriou (1992). We consider optimization problems, in particular load balancing and virtual circuit routing, in which the input is divided among the agents. An underlying directed graph, whose nodes are the agents, defines the constraints on the information each agent may have about the portion of the input held by other agents. The questions we discuss are: Given a bound on the maximum out-degree in this graph, which is the best graph? What is the quality of the solution obtained as a function of the maximum out-degree?.<<ETX>>
[Algorithm design and analysis, parallel algorithms, load balancing, Circuits, game theory, Routing, Loss measurement, maximum out-degree, artificial intelligence, distributed artificial intelligence, Information analysis, virtual circuit routing, Computer science, optimisation, Ear, directed graph, coordination games, Context modeling]
A polynomial-time algorithm for the perfect phylogeny problem when the number of character states is fixed
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We present a polynomial-time algorithm for determining whether a set of species, described by the characters they exhibit, has a perfect phylogeny, assuming the maximum number of possible states for a character is fixed. This solves a longstanding open problem. Our result should be contrasted with the proof by Steel and Bodlaender, Fellows, and Warnow that the perfect phylogeny problem is NP-complete in general.<<ETX>>
[Art, character states, perfect phylogeny problem, NP-complete, Phylogeny, Steel, History, Computer science, Evolution (biology), Biology computing, Polynomials, polynomial-time algorithm, computational complexity]
Better lower bounds on detecting affine and spherical degeneracies
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We show that in the worst case, /spl Omega/(n/sup d/) sidedness queries are required to determine whether a set of n points in R/sup d/ is affinely degenerate, i.e., whether it contains d+1 points on a common hyperplane. This matches known upper bounds. We give a straightforward adversary argument, based on the explicit construction of a point set containing /spl Omega/(n/sup d/) "collapsible" simplices, any one of which can be made degenerate without changing the orientation of any other simplex. As an immediate corollary, we have an /spl Omega/(n/sup d/) lower bound on the number of sidedness queries required to determine the order type of a set of n points in R/sup d/. Using similar techniques, we also show that /spl Omega/(n/sup d+1/) in-sphere queries are required to decide the existence of spherical degeneracies in a set of n points in R/sup d/.<<ETX>>
[Computational modeling, Lattices, computational geometry, upper bounds, Valves, lower bound, lower bounds, sidedness queries, Computer science, Computational geometry, Upper bound, affine detection, common hyperplane, Page description languages, spherical degeneracies, point set, Decision trees]
External-memory computational geometry
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
In this paper we give new techniques for designing efficient algorithms for computational geometry problems that are too large to be solved in internal memory. We use these techniques to develop optimal and practical algorithms for a number of important large-scale problems. We discuss our algorithms primarily in the context of single processor/single disk machines, a domain in which they are not only the first known optimal results but also of tremendous practical value. Our methods also produce the first known optimal algorithms for a wide range of two-level and hierarchical multilevel memory models, including parallel models. The algorithms are optimal both in terms of I/O cost and internal computation.<<ETX>>
[Algorithm design and analysis, parallel algorithms, hierarchical multilevel memory models, Object oriented databases, Object oriented modeling, computational geometry, optimal algorithms, Spatial databases, Computer science, Computational geometry, Design engineering, large-scale problems, Disk drives, Cost function, parallel models, Large-scale systems, internal computation, efficient algorithms, computational complexity, I/O cost]
Using difficulty of prediction to decrease computation: fast sort, priority queue and convex hull on entropy bounded inputs
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Studies have indicated that sorting comprises about 20% of all computing on mainframes. Perhaps the largest use of sorting in computing (particularly business computing) is the sort required for large database operations (e.g. required by joint operations). In these applications the keys are many words long. Since our sorting algorithm hashes the key (rather than compare entire keys as in comparison sorts such as quicksort), our algorithm is even more advantageous in the case of large key lengths; in that case the cutoff is much lower. In case that the compression ratio is high, which can be determined after building the dictionary, we just adopt the previous sorting algorithm, e.g. quick sort. The same techniques can be extended to other problems (e.g. computational geometry problems) to decrease computation by learning the distribution of the inputs.<<ETX>>
[Prefetching, priority queue, Stochastic processes, Predictive models, computational geometry, Data structures, Entropy, sort, convex hull, Distributed computing, Sorting, compression ratio, Computer science, entropy bounded inputs, sorting, data structures, computational geometry problems, Queueing analysis, Contracts, large database operations, computational complexity]
Synchronization power depends on the register size
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Though it is common practice to treat synchronization primitives for multiprocessors as abstract data types, they are in reality machine instructions on registers. A crucial theoretical question with practical implications is the relationship between the size of the register and its computational power. The authors study this question and choose as a first target the popular compare and swap operation (which is the basis for many modern multiprocessor architectures). The results of this paper suggest that a complexity hierarchy for multiprocessor synchronization operations should be based on the space complexity of synchronization registers and not on the number of so called "synchronization objects".<<ETX>>
[synchronization primitives, multiprocessing systems, Read-write memory, Registers, compare and swap, synchronisation, Computer science, multiprocessor architectures, register size, Computer architecture, complexity hierarchy, Hardware, machine instructions, multiprocessors, space complexity, synchronization registers, Testing, computational complexity]
Efficient computation of Euclidean shortest paths in the plane
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We propose a new algorithm for a classical problem in plane computational geometry: computing a shortest path between two points in the presence of polygonal obstacles. Our algorithm runs in worst-case time O(nlog/sup 2/ n) and requires O(nlog n) space, where n is the total number of vertices in the obstacle polygons. Our algorithm actually computes a planar map that encodes shortest paths from a fixed source point to all other points of the plane; the map can be used to answer single-source shortest path queries in O(log n) time. The time complexity of our algorithm is a significant improvement over all previous results known for the shortest path problem.<<ETX>>
[efficient computation, Euclidean shortest paths, computational geometry, time complexity, Routing, polygonal obstacles, worst-case time, Shortest path problem, Computational geometry, shortest paths, plane computational geometry, Robots, computational complexity]
On choosing a dense subgraph
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
This paper concerns the problem of computing the densest k-vertex subgraph of a given graph, namely, the subgraph with the most edges, or with the highest edges-to-vertices ratio. A sequence of approximation algorithms is developed for the problem, with each step yielding a better ratio at the cost of a more complicated solution. The approximation ratio of our final algorithm is O/spl tilde/(n/sup 0.3885/). We also present a method for converting an approximation algorithm for an unweighted graph problem (from a specific class of maximization problems) into one for the corresponding weighted problem, and apply it to the densest subgraph problem.<<ETX>>
[approximation ratio, approximation theory, Costs, edges-to-vertices ratio, Density measurement, Engineering profession, densest k-vertex subgraph, graph theory, dense subgraph, Mathematics, Topology, approximation algorithms, maximization problems, Computer science, optimisation, unweighted graph problem, weighted problem, Approximation algorithms, Polynomials, most edges]
A compact piecewise-linear Voronoi diagram for convex sites in the plane
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
In the plane, the post-office problem, which asks for the closest site to a query site, and retraction motion planning, which asks for a one-dimensional retract of the free space of a robot, are both classically solved by computing a Voronoi diagram. When the sites are k disjoint convex sets, we give a compact representation of the Voronoi diagram, using O(k) line segments, that is sufficient for logarithmic time post-office location queries and motion planning. If these sets are polygons with n total vertices, we compute this diagram optimally in O(klog n) deterministic time for the Euclidean metric and in O(klog nlog m) deterministic time for the convex distance function defined by a convex m-gon.<<ETX>>
[one-dimensional retract, Piecewise linear techniques, Scholarships, convex m-gon, polygons, retraction motion planning, computational geometry, Data structures, convex sites, mobile robots, path planning, deterministic time, Orbital robotics, Computer science, Motion planning, Computational geometry, compact piecewise-linear Voronoi diagram, k disjoint convex sets, Euclidean distance, post-office problem, query site, Lifting equipment, Euclidean metric]
General bounds on statistical query learning and PAC learning with noise via hypothesis boosting
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We derive general bounds on the complexity of learning in the statistical query model and in the PAC model with classification noise. We do so by considering the problem of boosting the accuracy of weak learning algorithms which fall within the statistical query model. This new model was introduced by M. Kearns (1993) to provide a general framework for efficient PAC learning in the presence of classification noise.<<ETX>>
[complexity, Machine learning algorithms, PAC learning, Laboratories, Boosting, Extraterrestrial measurements, Size measurement, Noise measurement, Computer science, Upper bound, hypothesis boosting, Machine learning, noise, general bounds, learning (artificial intelligence), statistical query learning, Contracts, computational complexity]
Signal propagation, with application to a lower bound on the depth of noisy formulas
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We study the decay of an information signal propagating through a series of noisy channels. We obtain exact bounds on such decay, and as a result provide a new lower bound on the depth of formulas with noisy components. This improves upon previous work of N. Pippenger (1988) and significantly decreases the gap between his lower bound and the classical upper bound of von Neumann. We also discuss connections between our work and the study of mixing rates of Markov chains.<<ETX>>
[information signal, Codes, signal propagation, exact bounds, signal processing, Acoustic propagation, upper bound, Markov chains, decay, lower bound, Application software, Wire, Computer science, noisy formulas depth, Upper bound, Acoustic noise, Markov processes, Random variables, Circuit noise, information theory, Signal to noise ratio]
Logical reducibility and monadic NP
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
It is shown that, by choosing appropriate encodings of instances as relational structures, several known polynomial-time many-one reductions can he described in first-order logic, and furthermore they are monadic. As a corollary, several known NP-complete problems in monadic NP are shown not to be in monadic co-NP. It is further shown that there is no monadic first-order reduction from connectivity to directed reachability, even in the presence of successor. Finally, some classes of syntactically restricted first-order reductions are shown to be incomparable.<<ETX>>
[encodings, first-order logic, relational structures, Encoding, NP-complete problem, polynomial-time many-one reductions, logical reducibility, formal logic, monadic NP, directed reachability, NP-complete problems, Polynomials, Logic, syntactically restricted first-order reductions, computational complexity]
The complexity of the theory of p-adic numbers
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
This paper addresses the question of the complexity of the decision problem for the theory Th(Q/sub p/) of p-adic numbers. The best known lower bound for the theory is double exponential alternating time with a linear number of alternations. I have designed an algorithm that determines the truth value of sentences of the theory requiring double exponential space. My algorithm is based on techniques used by G.E. Collins (1975) for the theory Th(R) of the reals, and on J. Denef's work (1986) on semi-algebraic sets and cell decomposition for p-adic fields. No elementary upper bound had been previously established.<<ETX>>
[Vocabulary, complexity, decision problem, decision theory, Control systems, elementary upper bound, lower bound, Application software, Matrix decomposition, double exponential alternating time, cell decomposition, Computer science, p-adic numbers, Sufficient conditions, Upper bound, Algebra, process algebra, double exponential space, symbol manipulation, Digital arithmetic, Roundoff errors, semi-algebraic sets, computational complexity]
Exact learning via the Monotone theory
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We study the learnability of concept classes from membership and equivalence queries. We develop the Monotone theory that proves (1) Any boolean function is learnable as decision tree. (2) Any boolean function is either learnable as DNF or as CNF (or both). The first result solves the open problem of the learnability of decision trees and the second result gives more evidence that DNFs are not "very hard" to learn.<<ETX>>
[Circuits, boolean function, Monotone theory, concept classes, equivalence queries, exact learning, learnability, membership, Noise measurement, Boolean functions, Machine learning, decision trees, Polynomials, CNF, Decision trees, learning (artificial intelligence), DNF]
Time-space lower bounds for directed s-t connectivity on JAG models
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Directed s-t connectivity is the problem of detecting whether there is a path from a distinguished vertex s to a distinguished vertex t in a directed graph. We prove time-space lower bounds of ST=/spl Omega/(n/sup 2//log n) and S/sup 1/2/T /spl Omega/(mn/sup 1/2/) for Cook and Rackoff's JAG model (1980), where n is the number of vertices and m the number of edges in the input graph, and S is the space and T the time used by the JAG. We also prove a time-space lower bound of S/sup 1/3/T=/spl Omega/(m/sup 2/3/n(2/3)) on the more powerful node-named JAG model of Poon (1993). These bounds approach the known upper bound of T=O(m) when S=/spl Theta/(n log n).<<ETX>>
[distinguished vertex, Computational modeling, s-t connectivity, upper bound, Encoding, Computational complexity, Computer science, directed graphs, Prototypes, directed s-t connectivity, directed graph, time-space lower bound, Tires, JAG models, computational complexity]
Counting rational points on curves over finite fields
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We consider the problem of counting the number of points on a plane curve, given by a homogeneous polynomial F/spl isin/F/sub p/[x, y, z], which is rational over the ground field F/sub p/. More precisely, we show that if we are given a projective plane curve C of degree n, and if C has only ordinary multiple points, then one can compute the number of F/sub p/-rational points on C in randomized time (log p)/sup /spl Delta// where /spl Delta/=(degF)/sup O(1/). The complexity of this construction improves previously known bounds for this problem by at least an order of magnitude.<<ETX>>
[Algorithm design and analysis, complexity, rational points counting, rational points, computational geometry, projective plane curve, randomized time, ordinary multiple points, Galois fields, Postal services, Computer science, Jacobian matrices, finite fields, curves, Elliptic curves, homogeneous polynomial, Gaussian processes, Polynomials, Testing, computational complexity]
Parallel computable higher type functionals
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
The primary aim of this paper is to introduce higher type analogues of some familiar parallel complexity classes, and to show that these higher type classes can be characterised in significantly different ways. Recursion-theoretic, proof-theoretic and machine-theoretic characterisations are given for various classes, providing evidence of their naturalness.<<ETX>>
[recursion-theoretic characterisations, Educational institutions, Calculus, Computational complexity, higher type classes, parallel complexity classes, Concurrent computing, Computer science, Microwave integrated circuits, parallel computable higher type functionals, Turing machines, machine-theoretic characterisations, proof-theoretic characterisations, Polynomials, Logic, Arithmetic, computational complexity]
A sub-linear time distributed algorithm for minimum-weight spanning trees
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
This paper considers the question of identifying the parameters governing the behavior of fundamental global network problems. Many papers on distributed network algorithms consider the task of optimizing the running time successful when an O(n) bound is achieved on an n-vertex network. We propose that a more sensitive parameter is the network's diameter Diam. This is demonstrated in the paper by providing a distributed minimum-weight spanning tree algorithm whose time complexity is sub-linear in n, but linear in Diam (specifically, O(Diam+n/sup 0.614/)). Our result is achieved through the application of graph decomposition and edge elimination techniques that may be of independent interest.<<ETX>>
[Career development, edge elimination techniques, graph decomposition, Nominations and elections, trees (mathematics), computational geometry, global network problems, time complexity, Mathematics, distributed minimum-weight spanning tree algorithm, n-vertex network, Global Positioning System, Diam, minimum-weight spanning trees, Tree graphs, USA Councils, sublinear time distributed algorithm, distributed algorithms, Cities and towns, Distributed algorithms, computational complexity]
Heat and Dump: competitive distributed paging
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
This paper gives a randomized competitive distributed paging algorithm called Heat and Dump, The competitive ratio is logarithmic in the total storage capacity of the network, this is optimal to within a constant factor. This is in contrast to the linear optimal deterministic competitive ratio.<<ETX>>
[Algorithm design and analysis, paged storage, total storage capacity, File servers, Phase change random access memory, Electronic mail, Programming profession, Computer science, Runtime, Memory management, competitive distributed paging, Cost function, Heat and Dump, Contracts]
Genome rearrangements and sorting by reversals
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Sequence comparison in molecular biology is in the beginning of a major paradigm shift-a shift from gene comparison based on local mutations to chromosome comparison based on global rearrangements. In the simplest form the problem of gene rearrangements corresponds to sorting by reversals, i.e. sorting of an array using reversals of arbitrary fragments. Kececioglu and Sankoff gave the first approximation algorithm for sorting by reversals with guaranteed error bound and identified open problems related to chromosome rearrangements. One of these problems is Gollan's conjecture on the reversal diameter of the symmetric group. We prove this conjecture and further study the problem of expected reversal distance between two random permutations. We demonstrate that the expected reversal distance is very close to the reversal diameter thereby indicating that reversal distance provides a good separation between related and non-related sequences. The gene rearrangement problem forces us to consider reversals of signed permutations, as the genes in DNA are oriented. Our approximation algorithm for signed permutation provides a 'performance guarantee' of 3/2. Finally, we devise an approximation algorithm for sorting by reversals with a performance ratio of 7/4.<<ETX>>
[molecular biology, Sequences, error bound, Genetic mutations, Genomics, Biology, performance ratio, signed permutations, Sorting, Computer science, sorting by reversals, Evolution (biology), DNA, sequence comparison, sorting, algorithm theory, gene comparison, approximation algorithm, Approximation algorithms, reversal distance, gene rearrangement problem, molecular biophysics, Bioinformatics]
Dynamic word problems
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Let M be a fixed finite monoid. We consider the problem of implementing a data type containing a vector x=(x/sub 1/,x/sub 2/,...,x/sub n/)/spl isin/M/sup n/, initially (1,1,...,1) with two kinds of operations, for each i/spl isin/{1,...,n}, a/spl isin/M, an operation change/sub i,a/ which changes x/sub i/ to a and a single operation product returning /spl Pi//sub i=1//sup n/x/sub i/. This is the dynamic word problem. If we in addition for each j/spl isin/{1,...,n} have an operation prefix/sub j/ returning /spl Pi//sub i=1//sup j/x/sub i/, we talk about the dynamic prefix problem. We analyze the complexity of these problems in the cell probe or decision assignment tree model for two natural cell sizes, 1 bit and log n bits. We obtain a classification of the complexity based on algebraic properties of M.<<ETX>>
[complexity, Costs, algebraic properties, Random access memory, dynamic word problems, data type, Computer science, group theory, fixed finite monoid, decision assignment tree model, Probes, Contracts, computational complexity]
What can we sort in o(nlog n) time?
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We define two conditions on a random access machine (RAM) with arithmetic and Boolean instructions and possible bounds on word and memory sizes. One condition asserts that we either restrict attention to short words or allow non-uniform programs. The second asserts that we either allow a large memory or a double-precision multiplication. Our main theorem shows that the RAM can sort in o(nlog n) time if and only if both of these conditions hold. This theorem breaks down into four upper bounds only one of which has been known before, and two lower bounds neither of which has been known.<<ETX>>
[random-access storage, Computational modeling, Random access memory, Read-write memory, upper bounds, Registers, random access machine, lower bounds, Sorting, Computer science, nonuniform programs, Upper bound, double-precision multiplication, sorting, Boolean instructions, Decision trees, arithmetic instructions, Arithmetic, computational complexity]
Las Vegas algorithms for matrix groups
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We consider algorithms in finite groups, given by a list of generators. We give polynomial time Las Vegas algorithms (randomized, with guaranteed correct output) for basic problems for finite matrix groups over the rationals (and over algebraic number fields): testing membership, determining the order, finding a presentation (generators and relations), and finding basic building blocks: center, composition factors, and Sylow subgroups. These results extend previous work on permutation groups into the potentially more significant domain of matrix groups. Such an extension has until recently been considered intractable. In case of matrix groups G of characteristic p, there are two basic types of obstacles to polynomial-time computation: number theoretic (factoring, discrete log) and large Lie-type simple groups of the same characteristic p involved in the group. The number theoretic obstacles are inherent and appear already in handling abelian groups. They can be handled by moderately efficient (subexponential) algorithms. We are able to locate all the nonabelian obstacles in a normal subgroup N and solve all problems listed above for G/N.<<ETX>>
[matrix groups, Lie algebras, Sylow subgroups, Mathematics, Complexity theory, algebraic number fields, Lie-type simple groups, polynomial matrices, permutation groups, Polynomials, Marine vehicles, Testing, Statistical analysis, testing membership, composition factors, Las Vegas algorithms, center, Computational Intelligence Society, randomized algorithm, number theoretic obstacles, Galois fields, randomised algorithms, Computer science, abelian groups, finite groups, Packaging]
The hardness of approximate optima in lattices, codes, and systems of linear equations
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We prove the following about the Nearest Lattice Vector Problem (in any l/sub p/ norm), the Nearest Code-word Problem for binary codes, the problem of learning a halfspace in the presence of errors, and some other problems. 1. Approximating the optimum within any constant factor is NP-hard. 2. If for some /spl epsiv/>0 there exists a polynomial time algorithm that approximates the optimum within a factor of 2/sup log(0.5-/spl epsiv/)/ /sup n/ then NP is in quasi-polynomial deterministic time: NP/spl sube/DTIME(n/sup poly(log/ /sup n)/). Moreover, we show that result 2 also holds for the Shortest Lattice Vector Problem in the l/sub /spl infin// norm. Improving the factor 2/sup log(0.5-/spl epsiv/)/ /sup n/ to /spl radic/(dim) for either of the lattice problems would imply the hardness of the Shortest Vector Problem in l/sub 2/ norm; an old open problem. Our proofs use reductions from few-prover, one-round interactive proof systems, either directly, or through a set-cover problem.<<ETX>>
[codes, binary codes, NP-hard, Laboratories, Lattices, Linear programming, interactive proof systems, Vectors, polynomial time algorithm, set-cover problem, Geometry, Linear code, hardness, Integral equations, Binary codes, Approximation algorithms, approximate optima, Polynomials, theorem proving, lattices, linear algebra, computational complexity, linear equations]
Primal-dual RNC approximation algorithms for (multi)-set (multi)-cover and covering integer programs
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We build on the classical greedy sequential set cover algorithm, in the spirit of the primal-dual schema, to obtain simple parallel approximation algorithms for the set cover problem and its generalizations. Our algorithms use randomization, and our randomized voting lemmas may be of independent interest. Fast parallel approximation algorithms were known before for set cover, though not for any of its generalizations.<<ETX>>
[Greedy algorithms, Algorithm design and analysis, parallel algorithms, primal-dual RNC approximation algorithms, computational geometry, Linear programming, randomized voting, Parallel algorithms, Voting, covering integer programs, randomization, parallel approximation algorithms, Approximation algorithms, Cost function, set cover problem, classical greedy sequential set cover algorithm]
Solving systems of set constraints with negated subset relationships
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We present a decision procedure, based on tree automata techniques, for satisfiability of systems of set constraints including negated subset relationships. This result extends all previous works on set constraints solving and solves a problem which was left open by L. Bachmair et al. (1993). We prove in a constructive way that a non empty set of solutions always contains a regular solution, that is a tuple of regular tree languages. Moreover, we think that the new class of tree automata described here could be interesting in its own.<<ETX>>
[Algorithm design and analysis, formal languages, Logic programming, automata theory, systems of set constraints, Algebra, decidability, tree automata techniques, satisfiability, Automata, decision procedure, Constraint theory, Inference algorithms, negated subset relationships, constraint handling, regular tree languages]
The shrinkage exponent is 2
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We prove that if we hit a formula of size L with a random restriction from R/sub p/ then the expected remaining size is at most O(p/sup 2/(log p)/sup 3/2/L). As a corollary we obtain a R(n/sup 3-O(1)/) formula size lower bound for an explicit function in NP.<<ETX>>
[Upper bound, explicit function, NP, shrinkage exponent, Computational modeling, Inspection, random restriction, computational complexity]
Optimally fast parallel algorithms for preprocessing and pattern matching in one and two dimensions
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
All algorithms below are optimal alphabet-independent parallel CRCW PRAM algorithms. In one dimension: Given a pattern string of length m for the string-matching problem, we design an algorithm that computes a deterministic sample of a sufficiently long substring in constant time. This problem used to be a bottleneck in the pattern preprocessing for one- and two-dimensional pattern matching. The best previous time bound was O(log/sup 2/ m/log log m). We use this algorithm to obtain the following results. 1. Improving the preprocessing of the constant-time text search algorithm from O(log/sup 2/ m/log log m) to n(log log m), which is now best possible. 2. A constant-time deterministic string-matching algorithm in the case that the text length n satisfies n=/spl Omega/(m/sup 1+/spl epsiv//) for a constant /spl epsiv/>0. 3. A simple probabilistic string-matching algorithm that has constant time with high probability for random input. 4. A constant expected time Las-Vegas algorithm for computing the period of the pattern and all witnesses and thus string matching itself, solving the main open problem remaining in string matching.<<ETX>>
[parallel algorithms, pattern matching, preprocessing, probability, Phase change random access memory, Educational institutions, Data structures, parallel CRCW PRAM algorithms, time bound, Parallel algorithms, Las-Vegas algorithm, Text processing, Runtime, constant-time text search algorithm, probabilistic string-matching algorithm, string matching, optimally fast parallel algorithms, Pattern matching]
The complexity and distribution of hard problems
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Measure-theoretic aspects of the /spl les//sub m//sup P/-reducibility structure of exponential time complexity classes E=DTIME(2/sup linear/) and E/sub 2/=DTIME(2/sup polynomial/) are investigated. Particular attention is given to the complexity (measured by the size of complexity cores) and distribution (abundance in the sense of measure) of languages that are /spl les//sub m//sup P/-hard for E and other complexity classes. Tight upper and lower bounds on the size of complexity cores of hard languages are derived. The upper bounds say that the /spl les//sub m//sup P/-hard languages for E are unusually simple in, the sense that they have smaller complexity cores than most languages in E. It follows that the /spl les//sub m//sup P/-complete languages for E form a measure 0 subset of E (and similarly in E/sub 2/). This latter fact is seen to be a special case of a more general theorem, namely, that every /spl les//sub m//sup P/-degree (e.g. the degree of all /spl les//sub m//sup P/-complete languages for NP) has measure 0 in E and in E/sub 2/.<<ETX>>
[complexity, complete languages, formal languages, hard problems, Size measurement, Time measurement, distribution, Computer science, Upper bound, reducibility, time complexity classes, hard languages, Particle measurements, Polynomials, Lifting equipment, computational complexity]
A tight lower bound for k-set agreement
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We prove tight bounds on the time needed to solve k-set agreement, a natural generalization of consensus. We analyze this problem in a synchronous, message-passing model where processors fail by crashing. We prove a lower bound of [f/k]+1 rounds of communication for solutions to k-set agreement that tolerate f failures. This bound is tight, and shows that there is an inherent tradeoff between the running time, the degree of coordination required, and the number of faults tolerated, even in idealized models like the synchronous model. The proof of this result is interesting because it is a geometric combination of other well-known proof techniques.<<ETX>>
[Protocols, message passing, fault tolerance, Computational modeling, Laboratories, Communication system control, Computer crashes, Transaction databases, tight lower bound, proof techniques, tradeoff, Computer science, Concurrent computing, consensus, Fault tolerance, message-passing model, Failure analysis, k-set agreement, fault tolerant computing, communication, computational complexity]
Near-linear cost sequential and distributed constructions of sparse neighborhood covers
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
This paper introduces the first near-linear (specifically, O(Elog n+nlog/sup 2/ n)) time algorithm for constructing a sparse neighborhood cover in sequential and distributed environments. This automatically implies analogous improvements (from quadratic to near-linear) to all the results in the literature that rely on network decompositions, both in sequential and distributed domains, including adaptive routing schemes with O/spl tilde/(1) stretch and memory, small edge cuts in planar graphs, sequential algorithms for dynamic approximate shortest paths with O/spl tilde/(E) cost for edge insertion/deletion and O/spl tilde/(1) time to answer shortest-path queries, weight and distance-preserving graph spanners with O/spl tilde/(E) running time and space, and distributed asynchronous "from-scratch" breadth-first-search and network synchronizer constructions with O/spl tilde/(1) message and space overhead (down from O(n)).<<ETX>>
[Career development, Costs, Heuristic algorithms, network decompositions, adaptive routing, computational geometry, Routing, Data structures, Mathematics, breadth-first-search, Stress, Computer science, near-linear cost sequential constructions, network synchronizer constructions, distributed constructions, dynamic approximate shortest paths, sparse neighborhood covers, Standards development, Contracts, distance-preserving graph spanners, sequential algorithms]
An on-line algorithm for improving performance in navigation
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Recent papers have shown optimally-competitive on-line strategies for a robot traveling from a point s to a point t in certain unknown geometric environments. We consider the question: Having gained some partial information about the scene on its first trip from s to t, can the robot improve its performance on subsequent trips it might make? This is a type of on-line problem where a strategy must exploit partial information about the future (e.g., about obstacles that lie ahead). For scenes with axis-parallel rectangular obstacles where the Euclidean distance between s and t is n, we present a deterministic algorithm whose average trip length after t trips, k/spl les/n, is O(/spl radic/n/k) times the length of the shortest s-t path in the scene. We also show that this is the best a deterministic strategy can do. This algorithm can be thought of as performing an optimal tradeoff between search effort and the goodness of the path found. We improve this algorithm so that for every i/spl les/n, the robot's ith trip length is O(/spl radic/n/t) times the shortest s-t path length. A key idea of the paper is that a tree structure can be defined in the scene, where the nodes are portions of certain obstacles and the edges are "short" paths from a node to its children. The core of our algorithms is an on-line strategy for traversing this tree optimally.<<ETX>>
[Computer science, Tree data structures, Navigation, performance, Robot kinematics, Layout, online algorithm, Euclidean distance, Cities and towns, computational geometry, deterministic algorithm, deterministic algorithms]
The union of convex polyhedra in three dimensions
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We show that the number of vertices, edges, and faces of the union of k convex polyhedra in 3-space, having a total of n faces, is O(k/sup 3/+knlog/sup 2/ k). This bound is almost tight in the worst case. We also describe a rather simple randomized incremental algorithm for computing the boundary of the union in O(k/sup 3/+knlog/sup 3/ k) expected time.<<ETX>>
[Algorithm design and analysis, three dimensions, vertices, union of convex polyhedra, computational geometry, edges, almost tight, Research and development, randomised algorithms, Computer science, Robot motion, Motion planning, faces, randomized incremental algorithm, computational complexity]
Efficient out-of-core algorithms for linear relaxation using blocking covers
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
When a numerical computation fails to fit in the primary memory of a serial or parallel computer, a so-called "out-of-core" algorithm must be used which moves data between primary and secondary memories. In this paper, we study out-of-core algorithms for sparse linear relaxation problems in which each iteration of the algorithm updates the state of every vertex in a graph with a linear combination of the states of its neighbors. We give a general method that can save substantially on the I/O traffic for many problems. For example, our technique allows a computer with M words of primary memory to perform T=/spl Omega/(M/sup 1/5/) cycles of a multigrid algorithm for a two-dimensional elliptic solver over an n-point domain using only /spl Theta/(nT/M/sup 1/5/) I/O transfers, as compared with the naive algorithm which requires /spl Omega/(nT) I/O's.<<ETX>>
[iterative methods, out-of-core algorithms, Transmission line matrix methods, Costs, elliptic solver, Vectors, Equations, Computer science, Concurrent computing, Jacobian matrices, multigrid algorithm, sparse linear relaxation problems, blocking covers, numerical computation, relaxation theory, National electric code, Gaussian processes, iteration, Iterative algorithms, primary memory, computational complexity, linear relaxation]
Simulated annealing for graph bisection
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We resolve in the affirmative a question of R.B. Boppana and T. Bui: whether simulated annealing can with high probability and in polynomial time, find the optimal bisection of a random graph an G/sub npr/ when p-r=(/spl Theta/n/sup /spl Delta/-2/) for /spl Delta//spl les/2. (The random graph model G/sub npr/ specifies a "planted" bisection of density r, separating two n/2-vertex subsets of slightly higher density p.) We show that simulated "annealing" at an appropriate fixed temperature (i.e., the Metropolis algorithm) finds the unique smallest bisection in O(n/sup 2+/spl epsi//) steps with very high probability, provided /spl Delta/>11/6. (By using a slightly modified neighborhood structure, the number of steps can be reduced to O(n/sup 1+/spl epsi//).) We leave open the question of whether annealing is effective for /spl Delta/ in the range 3/2</spl les/11/6, whose lower limit represents the threshold at which the planted bisection becomes lost amongst other random small bisections. It also remains open whether hillclimbing (i.e. annealing at temperature 0) solves the same problem.<<ETX>>
[Temperature distribution, Metropolis algorithm, Costs, simulated annealing, Computational modeling, Stochastic processes, computational geometry, Computer science, graph bisection, optimisation, unique smallest bisection, Simulated annealing, Energy states, Polynomials, Temperature control, Power engineering and energy]
Almost tight upper bounds for lower envelopes in higher dimensions
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We show that the combinatorial complexity of the lower envelope of n surfaces or surface patches in d-space (d/spl ges/3), all algebraic of constant maximum degree, and bounded by algebraic surfaces of constant maximum degree, is O(n/sup d-1+/spl epsi//), for any /spl epsi/>0; the constant of proportionality depends on /spl epsi/, d, and the shape and degree of the surface patches and of their boundaries. This is the first nontrivial general upper bound for this problem, and it almost establishes a long-standing conjecture that the complexity of the envelope is O(n/sup d-2//spl lambda//sub q/(n)) for some constant q depending on the shape and degree of the surfaces (where /spl lambda//sub q/(n) is the maximum length of (n,q) Davenport-Schinzel sequences). We also present a randomized algorithm for computing the envelope in three dimensions, with expected running time O(n/sup 2+/spl epsi//), and give several applications of the new bounds.<<ETX>>
[Shape, randomized algorithm, Research and development, randomised algorithms, combinatorial complexity, constant of proportionality, surface patches, Davenport-Schinzel sequences, higher dimensions, Polynomials, almost tight upper bounds, lower envelopes, constant maximum degree, computational complexity]
A Chernoff bound for random walks on expander graphs
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We consider a finite random walk on a weighted graph G; we show that the sample average of visits to a set of vertices A converges to the stationary probability /spl pi/(A) with error probability exponentially small in the length of the random walk and the square of the size of the deviation from /spl pi/(A). The exponential bound is in terms of the expansion of G and improves previous results. We show that the method of taking the sample average from one trajectory is a more efficient estimate of /spl pi/(A) than the standard method of generating independent sample points from several trajectories. Using this more efficient sampling method, we improve the algorithms of Jerrum and Sinclair (1989) for approximating the number of perfect matchings in a dense graph and for approximating the partition function of an Ising system. We also give a fast estimate of the entropy of a random walk on an unweighted graph.<<ETX>>
[random walks, finite random walk, weighted graph, Error probability, graph theory, probability, Chernoff bound, error probability, Graph theory, Mathematics, Entropy, Iron, Partitioning algorithms, Convergence, entropy, algorithm theory, partition function, Writing, Sampling methods, Random variables, expander graphs, Ising system]
A polynomial time algorithm for counting integral points in polyhedra when the dimension is fixed
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We prove that for any dimension d there exists a polynomial time algorithm for counting integral points in polyhedra in the d-dimensional Euclidean space. Previously such algorithms were known for dimensions d=1,2,3, and 4 only.<<ETX>>
[Algorithm design and analysis, Space technology, integral points counting, Lattices, d-dimensional Euclidean space, computational geometry, Polynomials, Mathematics, polynomial time algorithm, polyhedra]
Refining a triangulation of a planar straight-line graph to eliminate large angles
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We show that any planar straight line graph (PSLG) with v vertices can be triangulated with no angle larger than 7/spl pi//8 by adding O(v/sup 2/log v) Steiner points in O(v/sup 2/log/sup 2/ v) time. We first triangulate the PSLG with an arbitrary constrained triangulation and then refine that triangulation by adding additional vertices and edges. We follow a lazy strategy of starting from an obtuse angle and exploring the triangulation in search of a sequence of Steiner points that will satisfy a local angle condition. Explorations may either terminate successfully (for example at a triangle vertex), or merge. Some PSLGs require /spl Omega/(v/sup 2/) Steiner points in any triangulation achieving any largest angle bound less than /spl pi/. Hence the number of Steiner points added by our algorithm is within a log v factor of worst case optimal. For most inputs the number of Steiner points and running time would be considerably smaller than in the worst case.<<ETX>>
[Steiner trees, triangulation, Shape, Laboratories, computational geometry, Mathematics, Finite element methods, Postal services, Convergence, Interpolation, arbitrary constrained triangulation, Numerical analysis, worst case optimal, Computer graphics, local angle condition, planar straight line graph, obtuse angle, Steiner points]
NP trees and Carnap's modal logic
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We consider problems and complexity classes definable by interdependent queries to an oracle in NP. How the queries depend on each other is specified by a directed graph G. We first study the class of problems where G is a general dag and show that this class coincides with /spl Delta//sub 2//sup P/. We then consider the class where G is a tree. Our main result states that this class is identical to P/sup NP/ [O(log n)], the class of problems solvable in polynomial time with a logarithmic number of queries to an oracle in NP. Using this result we show that the following problems are all P/sup NP/[O(logn)] complete: validity-checking of formulas in Carnap's modal logic, checking whether a formula is almost surely valid over finite structures in modal logics K, T, and S4, and checking whether a formula belongs to the stable set of beliefs generated by a propositional theory.<<ETX>>
[Carnap's modal logic, complexity classes, trees (mathematics), propositional theory, formal logic, Turing machines, Tree graphs, NP trees, directed graph, general dag, Polynomials, polynomial time, Internet, Logic, logarithmic number of queries, computational complexity]
Scale-sensitive dimensions, uniform convergence, and learnability
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Learnability in Valiant's PAC learning model has been shown to be strongly related to the existence of uniform laws of large numbers. These laws define a distribution-free convergence property of means to expectations uniformly over classes of random variables. Classes of real-valued functions enjoying such a property are also known as uniform Gliveako-Cantelli classes. In this paper we prove, through a generalization of Sauer's lemma that may be interesting in its own right, a new characterization of uniform Glivenko-Cantelli classes. Our characterization yields Dudley, Gine, and Zinn's previous characterization as a corollary. Furthermore, it is the first based on a simple combinatorial quantity generalizing the Vapnik-Chervonenkis dimension. We apply this result to characterize PAC learnability in the statistical regression framework of probabilistic concepts, solving an open problem posed by Kearns and Schapire. Our characterization shows that the accuracy parameter plays a crucial role in determining the effective complexity of the learner's hypothesis class.<<ETX>>
[Minimization methods, Weather forecasting, scale-sensitive dimensions, Predictive models, Size measurement, Mathematics, learnability, probabilistic concepts, distribution-free convergence property, Convergence, statistical regression framework, Computer science, uniform Gliveako-Cantelli classes, Power measurement, uniform convergence, PAC learning model, Mathematical model, learning (artificial intelligence), Meteorology, computational complexity]
Directed vs. undirected monotone contact networks for threshold functions
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We consider the problem of computing threshold functions using directed and undirected monotone contact networks. Our main results are the following. First, we show that there exist directed monotone contact networks that compute T/sub k//sup n/, 2/spl les/k/spl les/n-1, of size O(k(n-k+2)log(n-k+2)). This bound is almost optimal for small thresholds, since there exists an /spl Omega/(knlog (n/(k-1))) lower bound. Our networks are described explicitly; the previously best upper bound known, obtained from the undirected networks of Dubiner and Zwick, used non-constructive arguments and gave directed networks of size O(k/sup 3.99/nlog n). Second, we show a lower bound of O(nlogloglog n) on the size of undirected monotone contact networks computing T/sub n-1//sup n/, improving the 2(n-1) lower bound of Markov. Combined with our upper bound result, this shows that directed monotone contact networks compute some threshold functions more easily than undirected networks.<<ETX>>
[Circuits, upper bound, threshold logic, monotone contact networks, lower bound, Computer science, Information science, Upper bound, Boolean functions, Boolean functions complexity, almost optimal, Computer networks, threshold functions, computational complexity]
Gates accept concurrent behavior
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We represent concurrent processes as Boolean propositions or gates, cast in the role of accepters of concurrent behavior. This properly extends other mainstream representations of concurrent behavior such as event structures, yet is defined more simply. It admits an intrinsic notion of duality that permits processes to be viewed as either schedules or automata. Its algebraic structure is essentially that of linear logic, with its morphisms being consequence-preserving renamings of propositions, and with its operations forming the core of a natural concurrent programming language.<<ETX>>
[natural concurrent programming language, Automatic programming, Logic programming, automata theory, linear logic, consequence-preserving renamings, Formal languages, Linear programming, Mathematics, duality, morphisms, Concurrent computing, Computer science, gates, concurrent behavior, algebraic structure, Automata, Ear, intrinsic notion, Logic functions, accepters, event structures, automata, Boolean propositions]
A simple local-control approximation algorithm for multicommodity flow
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
In this paper, we describe a very simple (1+/spl epsi/)-approximation algorithm for the multicommodity flow problem. The algorithm runs in time that is polynomial in N (the number of nodes in the network) and /spl epsiv//sup -1/ (the closeness of the approximation to optimal). The algorithm is remarkable in that it is much simpler than all known polynomial time flow algorithms (including algorithms for the special case of one-commodity flow). In particular, the algorithm does not rely on augmenting paths, shortest paths, min-cost paths, or similar techniques to push flow through a network. In fact, no explicit attempt is ever made to push flow towards a sink during the algorithm. Because the algorithm is so simple, it can be applied to a variety of problems for which centralized decision making and flow planning is not possible. For example, the algorithm can be easily implemented with local control in a distributed network and it can be made tolerant to link failures. In addition, the algorithm appears to perform well in practice. Initial experiments using the DIMACS generator of test problems indicate that the algorithm performs as well as or better than previously known algorithms, at least for certain test problems.<<ETX>>
[Performance evaluation, approximation theory, local-control approximation algorithm, Laboratories, Mathematics, DIMACS generator, polynomial time flow algorithms, Computer science, centralized decision making, Distributed control, Approximation algorithms, Polynomials, multicommodity flow, Marine vehicles, Contracts, Testing, computational complexity, distributed network]
Testing equalities of multiplicative representations in polynomial time
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
For multiplicative representations /spl Pi//sub i=1//sup k//spl alpha//sub i//sup n(i)/ and /spl Pi//sub j=1//sup l//spl beta//sub j//sup m(j)/ where /spl alpha//sub i/, /spl beta//sub j/ are non-zero elements of some algebraic number field K and n/sub i/, m/sub j/ are rational integers, we present a deterministic polynomial time algorithm that decides whether /spl Pi//sub i=1//sup k//spl alpha//sub i//sup n(i)/ equals /spl Pi//sub j=1//sup l//spl beta//sub j//sup m(j)/. The running time of the algorithm is polynomial in the number of bits required to represent the number field K, the elements /spl alpha//sub i/, /spl beta//sub j/ and the integers n/sub i/, m/sub j/.<<ETX>>
[nonzero elements, Error probability, deterministic polynomial time algorithm, algebraic number field, Mathematics, testing equalities, Galois fields, deterministic algorithms, polynomial matrices, multiplicative representations, rational integers, Polynomials, polynomial time, Testing]
Learning an intersection of k halfspaces over a uniform distribution
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We present a polynomial-time algorithm to learn an intersection of a constant number of halfspaces in n dimensions, over the uniform distribution on an n-dimensional ball. The algorithm we present in fact can learn an intersection of an arbitrary (polynomial) number of halfspaces over this distribution, if the subspace spanned by the normal vectors to the bounding hyperplanes has constant dimension. This generalizes previous results for this distribution, in particular a result of E.B. Baum (1990) who showed how to learn an intersection of 2 halfspaces defined by hyperplanes that pass through the origin (his results in fact held for a variety of symmetric distributions). Our algorithm uses estimates of second moments to find vectors in a low-dimensional "relevant subspace". We believe that the algorithmic techniques studied here may be useful in other geometric learning applications.<<ETX>>
[Machine learning algorithms, Predictive models, computational geometry, uniform distribution, geometric learning, Computer science, Neural networks, Machine learning, intersection, Prediction algorithms, Polynomials, Computer networks, learning (artificial intelligence), polynomial-time algorithm, bounding hyperplanes, k halfspaces]
A quantum bit commitment scheme provably unbreakable by both parties
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We describe a complete protocol for bit commitment based on the transmission of polarized photons. We show that under the laws of quantum physics, this protocol cannot be cheated by either party except with exponentially small probability (exponential in the running time needed to implement the honest protocol). A more thorough analysis is required to adjust all the constants used in this paper to get the best performance from our construction. Better performances may probably be achieved by using a third conjugate transmission-reception basis of circular polarization.<<ETX>>
[Polarization, circular polarization, Optical computing, cryptography, complete protocol, Security, Computational complexity, Cryptographic protocols, Microwave integrated circuits, Quantum computing, Power measurement, Physics computing, quantum bit commitment scheme, polarized photons, Cryptography, protocols, conjugate transmission-reception basis]
A randomized time-space tradeoff of O/spl tilde/(mR/spl circ/) for USTCON
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We present a randomized time space tradeoff of O/spl tilde/(mR/spl circ/) for undirected S-T-connectivity, where R/spl circ/ /spl Sigma//sub /spl upsi//spl epsiv/V/ 1/d/sub /spl upsi// is the virtual resistance of the graph. This solves an open question of Broder et al. (1989) (implicit also in Aleliunas et al. (1979)) who asked whether a tradeoff of O/spl tilde/(mn) is achievable, and also improves upon a tradeoff of O/spl tilde/(mn/d/sub min/) conjectured by Barnes and Feige (1993). Our algorithm is a modification of the Broder et al. algorithm. In passing, we also improve a result from Barnes and Feige regarding the rate at which a random walk discovers new vertices in a graph.<<ETX>>
[Algorithm design and analysis, random walk, Error probability, virtual resistance, graph theory, Scattering, algorithm theory, USTCON, Mathematics, time-space tradeoff, Joining processes, computational complexity]
On the "log rank"-conjecture in communication complexity
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We show the existence of a non-constant gap between the communication complexity of a function and the logarithm of the rank of its input matrix. We consider the following problem: each of two players gets a perfect matching between two n-element sets of vertices. Their goal is to decide whether or not the union of the two matchings forms a Hamiltonian cycle. We prove: (1) The rank of the input matrix over the reals for this problem is 2/sup O(n)/. (2) The non-deterministic communication complexity of the problem is /spl Omega/(n log log n). Our result also supplies a superpolynomial gap between the chromatic number of a graph and the rank of its adjacency matrix. Another conclusion from the second result is an /spl Omega/(n log log n). Lower bound for the graph connectivity problem in the non-deterministic case. We make use of the theory of group representations for the first result. The second result is proved by an information theoretic argument.<<ETX>>
[non-constant gap, Operations research, Protocols, Hamiltonian cycle, group representations, log rank, game theory, chromatic number, Mathematics, Complexity theory, communication complexity, Galois fields, Radio access networks, matrix algebra, Computer science, input matrix, nondeterministic, information theoretic argument, superpolynomial gap]
An O(nlog/sup 3/ n) algorithm for the real root problem
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Given a univariate complex polynomial f(x) of degree n with rational coefficients expressed as a ratio of two integers <2/sup m/, the root problem is to find all the roots of f(x) up to specified precision 2/sup -/spl mu//. In this paper we assume the arithmetic model for computation. We give an algorithm for the real root problem: where all the roots of the polynomial are real. Our real root algorithm has time cost of O(nlog/sup 2/ n(log n+log b)), where b=m+/spl mu/, thus has time bound O(nlog/sup 3/ n) even in the case of high precision m+/spl mu//spl les/n/sup O(1/). This is within a small polylog factor of optimality, thus (perhaps surprisingly) upper bounding the arithmetic complexity of our real root problem to nearly the same as basic arithmetic operations on polynomials. We require only /spl pi/=O(n(/spl mu/+m+n)) bits of precision to carry out our computations. The Boolean complexity of our algorithm is a multiplicative factor of M(/spl pi/)=O(/spl pi/(log /spl pi/)loglog /spl pi/) more.<<ETX>>
[univariate complex polynomial, Symmetric matrices, arithmetic complexity, Computational modeling, polynomials, O(nlog/sup 3/ n) algorithm, Read-write memory, arithmetic model for computation, real root algorithm, multiplicative factor, polylog factor of optimality, Postal services, Computer science, time cost, Boolean functions, Boolean complexity, Algebra, Eigenvalues and eigenfunctions, Polynomials, rational coefficients, real root problem, Contracts, Arithmetic, computational complexity]
Random sampling in matroids, with applications to graph connectivity and minimum spanning trees
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Random sampling is a powerful way to gather information about a group by considering only a small part of it. We give a paradigm for applying this technique to optimization problems, and demonstrate its effectiveness on matroids. Matroids abstractly model many optimization problems that can be solved by greedy methods, such as the minimum spanning tree (MST) problem. Our results have several applications. We give an algorithm that uses simple data structures to construct an MST in O(m+n log n) time. We give bounds on the connectivity (minimum cut) of a graph suffering random edge failures. We give fast algorithms for packing matroid bases, with particular attention to packing spanning trees in graphs.<<ETX>>
[Greedy algorithms, Data analysis, minimum spanning trees, Optimization methods, computational geometry, Data structures, random sampling, Application software, Statistics, matrix algebra, Computer science, Graphics, graph connectivity, matroids, Tree graphs, connectivity, optimization, random edge failures, Sampling methods, greedy methods, data structures, tree data structures]
A weak version of the Blum, Shub and Smale model
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We propose a weak version of the Blum-Shub-Smale model (1989) of computation over the real numbers. In this weak model only a "moderate" usage of multiplications and divisions is allowed. The class of languages recognizable in polynomial time as shown to be the complexity class P/poly. This implies under a standard complexity-theoretic assumption that P/spl ne/NP in the weak model, and that problems such as the real traveling salesman problem cannot be solved in polynomial time. As an application, we generalize recent results of H.T. Siegelmann and E.D. Sontag (1993) on recurrent neural networks, and of W. Maass (1993) on feedforward nets.<<ETX>>
[real traveling salesman problem, Computational modeling, Circuits, complexity class, recurrent neural nets, Analog computers, Traveling salesman problems, feedforward nets, Feedforward neural networks, Electronic mail, Complexity theory, divisions, Blum-Shub-Smale model of computation, Turing machines, recurrent neural networks, Neural networks, multiplications, real numbers, Polynomials, polynomial time, computational complexity, feedforward neural nets]
Eavesdropping games: a graph-theoretic approach to privacy in distributed systems
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We initiate a graph-theoretic approach to study the (information-theoretic) maintenance of privacy in distributed environments in the presence of a bounded number of mobile eavesdroppers ("bugs"). For two fundamental privacy problems-secure message transmission and distributed database maintenance-we assume an adversary is "playing eavesdropping games," coordinating the movement of the bugs among the sites to learn the current memory contents. We consider various mobility settings (adversaries), motivated by the capabilities (strength) of the bugging technologies (e.g., how fast can a bug be reassigned). We combinatorially characterize and compare privacy maintenance problems, determine their feasibility (under numerous bug models), suggest protocols for the feasible cases, and analyze their computational complexity.<<ETX>>
[Data privacy, Data security, secure message transmission, Telecommunication traffic, Switches, mobile eavesdroppers, eavesdropping games, privacy, distributed database maintenance, graph-theoretic approach, Cryptographic protocols, bugs, privacy maintenance problems, security of data, Computer bugs, Distributed databases, Information security, distributed databases, distributed systems, data privacy, protocols, Pattern analysis, Protection, computational complexity]
The NC equivalence of planar integer linear programming and Euclidean GCD
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We show NC-reduction of integer linear programming with two variables to the evaluation of the remainder sequence arising in the application of the Euclidean algorithm to two positive integers. Due to the previous result of X. Deng (1989), this implies NC-equivalence of both of these problems, whose membership in NC, as well as P-completeness, remain unresolved open problems.<<ETX>>
[NC equivalence, integer programming, planar integer linear programming, Lattices, computational geometry, Euclidean GCD, NC-reduction, Educational institutions, Mathematics, linear programming, remainder sequence, Computational complexity, Equations, Computer science, Concurrent computing, Euclidean algorithm, Integer linear programming, Cities and towns, Polynomials, positive integers, computational complexity, P-completeness]
Throughput-competitive on-line routing
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We develop a framework that allows us to address the issues of admission control and routing in high-speed networks under the restriction that once a call is admitted and routed, it has to proceed to completion and no reroutings are allowed. The "no rerouting" restriction appears in all the proposals for future high-speed networks and stems from current hardware limitations, in particular the fact that the bandwidth-delay product of the newly developed optical communication links far exceeds the buffer capacity of the network. In case the goal is to maximize the throughput, our framework yields an on-line O(log nT)-competitive strategy, where n is the number of nodes in the network and T is the maximum call duration. In other words, our strategy results in throughput that is within O(log nT) factor of the highest possible throughput achievable by an omniscient algorithm that knows all of the requests in advance. Moreover, we show that no on-line strategy can achieve a better competitive ratio. Our framework leads to competitive strategies applicable in several more general settings. Extensions include assigning each connection an associated "profit" that represents the importance of this connection, and addressing the issue of call-establishment costs.<<ETX>>
[bandwidth-delay product, optical communication links, throughput-competitive on-line routing, Routing, Throughput, admission control, Proposals, Delay, high-speed networks, Computer science, High-speed networks, Admission control, telecommunication networks, Bandwidth, maximum call duration, Hardware, optical communication, Contracts]
A framework for cost-scaling algorithms for submodular flow problems
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
The submodular flow problem includes such problems as minimum-cost network flow, dijoin, edge-connectivity orientation and others. We present a cost-scaling algorithm for submodular flow problems. The algorithm applies to these problems in general; we also examine its efficiency for the dijoin and edge-connectivity orientation problems. A minimum-cost dijoin is found in time O(min{m/sup 1/2/, n/sup 2/3/}nmlog(nN)), where n, m and N denote the number of vertices, number of edges and largest magnitude of an integral edge cost. The previous best-known bound is O(n/sup 2/m) if fast matrix multiplication is not used. A k-edge-connected orientation is found in time O(kn/sup 2/(/spl radic/(kn)+k/sup 2/log(n/k))). A minimum-cost k-edge-connected orientation is found on the above time bound for dijoins when k=O(1) (and a more complicated bound for general k). The scaling algorithm uses a transformation that eliminates vertex weights in edge-capacitated graphs. It also incorporates a scheme to limit the growth in the size of intermediate solutions, using a dual minimum-cost network flow problem.<<ETX>>
[Costs, Transmission line matrix methods, integer linear programming, integer programming, minimum-cost k-edge-connected orientation, vertices, computational geometry, Data structures, linear programming, minimum-cost network flow, vertex weights, cost-scaling algorithms, Computer science, Degradation, submodular flow problems, matrix multiplication, dijoin, edge-connectivity orientation, Feedback, Polynomials, k-edge-connected orientation]
Quantum circuit complexity
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We propose a complexity model of quantum circuits analogous to the standard (acyclic) Boolean circuit model. It is shown that any function computable in polynomial time by a quantum Turing machine has a polynomial-size quantum circuit. This result also enables us to construct a universal quantum computer which can simulate, with a polynomial factor slowdown, a broader class of quantum machines than that considered by E. Bernstein and U. Vazirani (1993), thus answering an open question raised by them. We also develop a theory of quantum communication complexity, and use it as a tool to prove that the majority function does not have a linear-size quantum formula.<<ETX>>
[quantum Turing machine, Computational modeling, Computer simulation, Circuit simulation, Boolean circuit model, quantum communication complexity, quantum circuit complexity, Complexity theory, communication complexity, Computer science, Quantum computing, Boolean functions, Turing machines, Quantum mechanics, Polynomials, Computer networks, polynomial time, computational complexity]
Approximating shortest superstrings
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
The Shortest Superstring Problem is to find a shortest possible string that contains every string in a given set as substrings. This problem has applications to data compression and DNA sequencing. As the problem is NP-hard and MAX SNP-hard, approximation algorithms are of interest. We present a new algorithm which always finds a superstring that is at most 2.89 times as long as the shortest superstring. Our result improves the 3-approximation result of Blum, Jiang, Li, Tromp, and Yannakakis (1991).<<ETX>>
[Greedy algorithms, data compression, NP-hard, Data compression, Length measurement, Shortest Superstring Problem, Mathematics, approximation algorithms, shortest superstrings, DNA, Linear approximation, algorithm theory, Approximation algorithms, Polynomials, MAX SNP-hard, superstring, computational complexity, DNA sequencing]
Optimal bi-weighted binary trees and the complexity of maintaining partial sums
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
Let A be an array. The partial sum problem concerns the design of a data structure for implementing the following operations. The operation update(j,x) has the effect, A[j]/spl larr/A[j]+x, and the query operation sum(j) returns the partial sum, /spl Sigma//sub i=1//sup j/A[i]. Our interest centers upon the optimal efficiency with which sequences of such operations can be performed, and we derive new upper and lower bounds in the semi-group model of computation. Our analysis relates the optimal complexity of the partial sum problem to optimal binary trees relative to a type of weighting scheme that defines the notion of bi-weighted binary tree.<<ETX>>
[Availability, complexity, optimal bi-weighted binary trees, Computational modeling, data structure, upper bounds, query operation, Registers, lower bounds, Binary trees, weighting scheme, Cost function, maintaining partial sums, tree data structures, computational complexity]
On bounded queries and approximation
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
This paper investigates the computational complexity of approximating NP-optimization problems using the number of queries to an NP oracle as a complexity measure. The results show a trade-off between the closeness of the approximation and the number of queries required. For an approximation factor k(n), loglog/sub k(n/) n queries to an NP oracle can be used to approximate the maximum clique size of a graph within a factor of k(n). However, this approximation cannot be achieved using fewer than loglog/sub k(n/) n-c queries to any oracle unless P=NP, where c is a constant that does not depend on k. These results hold when k(n) belongs to a class of functions which include any integer constant function, log n, log/sup a/ n and n/sup 1/a/. Similar results are obtained for graph coloring, set cover and other NP-optimization problems.<<ETX>>
[NP oracle, approximation, computational geometry, NP-optimization problems, complexity measure, Educational institutions, Size measurement, graph coloring, Computational complexity, graph colouring, Computer science, approximation factor, set cover, Approximation algorithms, Polynomials, maximum clique size, computational complexity, bounded queries]
Faster algorithms for the generalized network flow problem
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We consider the generalized network flow problem. Each arc e in the network has a gain factor /spl gamma/(e). If f(e) units of flow enter arc e, then f(e)/spl gamma/(e) units arrive at the other end of e. The generalized network flow problem is to maximize the net flow into one specific node, the sink. We give an algorithm which solves this problem in O/spl tilde/(m/sup 2/(m+nloglog B)log B) time, where B is the largest integer used to represent the gain factors, the capacities, and the initial supplies at the nodes. If m is O(n/sup (4/3/-/spl epsiv/) and B is not extremely large, then our bound improves the previous best bound O(m/sup 1.5/n/sup 2/log B) given by P.M. Vaidya (1989). Our algorithm is an approximation scheme which in each iteration reduces by a constant factor the difference between the current net flow into the sink and the optimal one. The solution which is within a factor of 1+/spl xi/ from the optimum can be computed in O/spl tilde/(m/sup 2/n+min{m/sup 2/n, m(m+nloglog B)}log(1//spl xi/)) time. This improves the previous bounds on the approximate generalized flow problem.<<ETX>>
[generalized network flow problem, Costs, Operations research, approximate generalized flow problem, Transportation, Linear programming, Educational institutions, linear programming, Computer science, Shortest path problem, Manufacturing processes, faster algorithms, Approximation algorithms, gain factors, capacities, approximation scheme]
Breaking the /spl Theta/(nlog/sup 2/ n) barrier for sorting with faults
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We study the problem of constructing a sorting circuit, network, or PRAM algorithm that is tolerant to faults. For the most part, we focus on fault patterns that are random, e.g., where the result of each comparison is independently faulty with probability upper-bounded by some constant. All previous fault-tolerant sorting circuits, networks, and parallel algorithms require /spl Omega/(log/sup 2/ n) depth (time) and/or /spl Omega/(nlog/sup 2/ n) comparisons to sort n items. In this paper, we construct a passive-fault-tolerant sorting circuit with O(nlog nloglog n) comparators, a reversal-fault-tolerant sorting network with O(n log/sup log(2)/ /sup 3/ n) comparators, and a deterministic O(log n)-step O(n)-processor EREW PRAM fault-tolerant sorting algorithm. The results are based on a new analysis of the AKS circuit, which uses a much weaker notion of expansion that can be preserved in the presence of faults. Previously, the AKS circuit was not believed to be fault-tolerant because the expansion properties that were believed to be crucial for the performance of the circuit are destroyed by random faults. Extensions of our results for worst-case faults are also presented.<<ETX>>
[parallel algorithms, sorting with faults, Packet switching, Merging, sorting circuit, reversal-fault-tolerant sorting network, Phase change random access memory, Circuit faults, sorting network, Parallel algorithms, Sorting, Leg, Switching circuits, passive-fault-tolerant sorting circuit, Fault tolerance, sorting, PRAM algorithm, fault tolerant computing, fault-tolerant sorting algorithm, Contracts, fault-tolerant, computational complexity]
Universal emulations with sublogarithmic slowdown
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
The existence of bounded degree networks which can emulate the computation of any bounded degree network of the same size with logarithmic slowdown is well-known. The butterfly is an example of such a universal network. Leiserson was the first to introduce the concept of an area-universal network: a network with VLSI layout area A which can emulate any network of the same size and layout area with logarithmic slowdown. His results imply the existence of an N-node network with layout area O(N log/sup 2/ N) which can emulate any N-node planar network with O(log N) slowdown. The main results of this paper are: There exists an N-node network with layout area O(N log/sup 2/ N) which can emulate any N-node planar network with O(loglogN) slowdown. The N-node butterfly (and hypercube) can emulate any network with VLSI layout area N/sup 2-/spl epsiv// (/spl epsiv/>0) with O(loglogN) slowdown. We also discuss sublogarithmic bounds for the slowdown of emulations of arbitrary bounded degree networks.<<ETX>>
[sublogarithmic bounds, hypercube, arbitrary bounded degree networks, Multiprocessor interconnection networks, Computational modeling, VLSI, bounded degree networks, Very large scale integration, hypercube networks, VLSI layout area, sublogarithmic slowdown, Computer science, Design engineering, universal emulations, Emulation, butterfly, National electric code, Grid computing, Hypercubes, Computer networks, area-universal network, circuit layout CAD, N-node network]
Optimal parallel all-nearest-neighbors using the well-separated pair decomposition
Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science
None
1993
We present an optimal parallel algorithm to construct the well-separated pair decomposition of a point set P in R/sup d/. We show how this leads to a deterministic optimal O(log n) time parallel algorithm for finding the k nearest neighbors of each point in P, where k is a constant. We discuss several additional applications of the well-separated pair decomposition for which we can derive faster parallel algorithms.<<ETX>>
[parallel algorithms, k nearest neighbors, Computational modeling, computational geometry, Phase change random access memory, Parallel algorithms, Nearest neighbor searches, optimal parallel all-nearest-neighbors, Computer science, Concurrent computing, Computational geometry, well-separated pair decomposition, pair decomposition, Binary trees, point set]
Scheduling multithreaded computations by work stealing
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
This paper studies the problem of efficiently scheduling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is "work stealing," in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies. Specifically, our analysis shows that the expected time T/sub P/ to execute a fully strict computation on P processors using our work-stealing scheduler is T/sub P/=O(T/sub 1//P+T/sub /spl infin//), where T/sub 1/ is the minimum serial execution time of the multithreaded computation and T/sub /spl infin// is the minimum execution time with an infinite number of processors. Moreover, the space S/sub P/ required by the execution satisfies S/sub P//spl les/S/sub 1/P. We also show that the expected total communication of the algorithm is at most O(T/sub /spl infin//S/sub max/P), where S/sub max/ is the size of the largest activation record of any thread, thereby justifying the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.<<ETX>>
[Algorithm design and analysis, work stealing, Laboratories, Dynamic scheduling, Data structures, Yarn, parallel processing, Scheduling algorithm, processor scheduling, parallel programming, Concurrent computing, Computer science, folk wisdom, Processor scheduling, dynamic MIMD-style computation, multithreaded computations scheduling, Load management, parallel computers, minimum serial execution time]
Markov chains and polynomial time algorithms
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
This paper outlines the use of rapidly mixing Markov Chains in randomized polynomial time algorithms to solve approximately certain counting problems. They fall into two classes: combinatorial problems like counting the number of perfect matchings in certain graphs and geometric ones like computing the volumes of convex sets.<<ETX>>
[Markov Chains, perfect matchings, Lattices, Steady-state, counting problems, Convergence, randomised algorithms, Computer science, Upper bound, approximately certain, polynomial time algorithms, randomized algorithms, Markov processes, Sampling methods, Approximation algorithms, Polynomials, computational complexity]
Fast and feasible periodic sorting networks of constant depth
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
A periodic comparator network has depth (or period) k, if for every t>k, the compare-exchange operations performed at step t are executed between exactly the same registers as at step t-k. We introduce a general method that converts an arbitrary comparator network that sorts n items in time T(n) and that has layout area A into a periodic sorting network of depth 5 that sorts /spl Theta/(n/spl middot/T(n)) items in time O(T(n)/spl middot/log n) and has layout area O(A/spl middot/T(n)). This scheme applied to the AKS network yields a depth 5 periodic comparator network that sorts in time O(log/sup 2/ n). More practical networks with runtime O(log/sup 3/ n) can be obtained from Batcher's networks. Developing the techniques for the main result, we improve some previous results: Let us fix a d/spl isin/N. Then we can construct a network of depth 3 based on a d-dimensional mesh sorting n items in time O(n/sup 1/d//spl middot/log/sup O(d/) n).<<ETX>>
[parallel algorithms, Circuits, layout area, Very large scale integration, Mathematics, Registers, d-dimensional mesh, Communication switching, Sorting, Computer science, Concurrent computing, Runtime, Batcher's networks, comparator network, periodic sorting networks, sorting, compare-exchange operations, AKS network, Web sites, constant depth, computational complexity]
A spectral approach to lower bounds
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We establish a nonlinear lower bound for halfplane range searching over a group. Specifically, we show that summing up the weights of n (weighted) points within n halfplanes requires /spl Omega/(n log n) additions and subtractions. This is the first nontrivial lower bound for range searching over a group. By constrast, range searching over a semigroup (which forbids subtractions) is almost completely understood. Our proof has two parts: First, we develop a general, entropy-based, method for relating the linear circuit complexity of a linear map A to the spectrum of A/sup T/A. In the second part of the proof, we design a "high-spectrum" geometric set system and, using techniques from discrepancy theory, we estimate the median eigenvalue of its associated map. Interestingly, the method also shows that using up to a linear number of help gates cannot help; these are gates that can compute any bivariate function. The best feature of our method is that it is very general. With any instance of range searching we associate a quadratic form: any lower bound on the mid-range of its spectrum implies a lower bound on the complexity of that range searching problem. The main drawback of our approach is that it (probably) yields weak lower bounds. Another shortcoming is that the method does not seem to generalize to range searching over rings or fields.<<ETX>>
[linear map, median eigenvalue, Computational modeling, halfplane range searching, nontrivial lower bound, Estimation theory, computational geometry, Encoding, Complexity theory, Table lookup, nonlinear lower bound, lower bounds, linear circuit complexity, Sorting, Computer science, Upper bound, Eigenvalues and eigenfunctions, geometric set system, range searching, computational complexity]
Program result-checking: a theory of testing meets a test of theory
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We review the field of result-checking, discussing simple checkers and self-correctors. We argue that such checkers could profitably be incorporated in software as an aid to efficient debugging and reliable functionality. We consider how to modify traditional checking methodologies to make them more appropriate for use in real-time, real-number computer systems. In particular, we suggest that checkers should be allowed to use stored randomness: i.e., that they should be allowed to generate, pre-process, and store random bits prior to run-time, and then to use this information repeatedly in a series of run-time checks. In a case study of checking a general real-number linear transformation (for example, a Fourier Transform), we present a simple checker which uses stored randomness, and a self-corrector which is particularly efficient if stored randomness is allowed.<<ETX>>
[Software testing, real-number computer systems, program debugging, program testing, program result-checking, software reliability, checkers, reliable functionality, Software performance, Software reliability, Software debugging, real-number linear transformation, Programming profession, run-time checks, Computer science, Runtime, self-correctors, Automatic testing, Computer bugs, debugging, Error correction codes, Fourier Transform, stored randomness]
Parallel algorithms for higher-dimensional convex hulls
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We give fast randomized and deterministic parallel methods for constructing convex hulls in R/sup d/, for any fixed d. Our methods are for the weakest shared-memory model, the EREW PRAM, and have optimal work bounds (with high probability for the randomized methods). In particular, we show that the convex hull of n points in R/sup d/ can be constructed in O(log n) time using O(n log n+n/sup [d/2]/) work, with high probability. We also show that it can be constructed deterministically in O(log/sup 2/ n) time using O(n log n) work for d=3 and in O(log n) time using O(n/sup [d/2]/ log/sup c([d/2]-[d/2]/) n) work for d/spl ges/4, where c>0 is a constant which is optimal for even d/spl ges/4. We also show how to make our 3-dimensional methods output-sensitive with only a small increase in running time. These methods can be applied to other problems as well.<<ETX>>
[weakest shared-memory model, parallel algorithms, 3-dimensional methods, computational geometry, Phase change random access memory, Data structures, Size measurement, convex hulls, Parallel algorithms, Radio access networks, Geometry, EREW PRAM, higher-dimensional convex hulls, shared memory systems, output-sensitive]
Beyond competitive analysis [on-line algorithms]
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The competitive analysis of on-line algorithms has been criticized as being too crude and unrealistic. We propose two refinements of competitive analysis an two directions: The first restricts the power of the adversary by allowing only certain input distributions, while the other allows for comparisons between information regimes for on-line decision-making. We illustrate the first with an application to the paging problem; as a by product we characterize completely the work functions of this important special case of the k-server problem. We use the second refinement to explore the power of lookahead in server systems, and the power of visual sensors in robot navigation.<<ETX>>
[Algorithm design and analysis, robot navigation, paged storage, on-line decision-making, Navigation, work functions, server systems, Decision making, competitive algorithms, Sensor phenomena and characterization, Minimax techniques, on-line algorithms, Sensor systems, Complexity theory, k-server problem, Information analysis, lookahead, paging problem, visual sensors, Robot sensing systems, Performance analysis, computational complexity, competitive analysis]
More output-sensitive geometric algorithms
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
A simple idea for speeding up the computation of extrema of a partially ordered set turns out to have a number of interesting applications in geometric algorithms; the resulting algorithms generally replace an appearance of the input size n in the running time by an output size A/spl les/n. In particular, the A coordinate-wise minima of a set of n points in R/sup d/ can be found by an algorithm needing O(nA) time. Given n points uniformly distributed in the unit square, the algorithm needs n+O(n/sup 5/8/) point comparisons on average. Given a set of n points in R/sup d/, another algorithm can find its A extreme points in O(nA) time. Thinning for nearest-neighbor classification can be done in time O(n log n)/spl Sigma//sub i/ A/sub i/n/sub i/, finding the A/sub i/ irredundant points among n/sub i/ points for each class i, where n=/spl Sigma//sub i/ n/sub i/ is the total number of input points. This sharpens a more obvious O(n/sup 3/) algorithm, which is also given here. Another algorithm is given that needs O(n) space to compute the convex hull of n points in O(nA) time. Finally, a new randomized algorithm finds the convex hull of n points in O(n log A) expected time, under the condition that a random subset of the points of size r has expected hull complexity O(r). All but the last of these algorithms has polynomial dependence on the dimension d, except possibly for linear programming.<<ETX>>
[computational geometry, Linear programming, set theory, coordinate-wise minima, Sorting, geometric algorithms, partially ordered set, irredundant points, Polynomials, output-sensitive, extrema, computational complexity, hull complexity]
Expander codes
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We present a new class of asymptotically good, linear error-correcting codes based upon expander graphs. These codes have linear time sequential decoding algorithms, logarithmic time parallel decoding algorithms with a linear number of processors, and are simple to understand. We present both randomized and explicit constructions for some of these codes. Experimental results demonstrate the extremely good performance of the randomly chosen codes.<<ETX>>
[sequential decoding, error correction codes, Laboratories, Graph theory, Encoding, Mathematics, Decoding, encoding, logarithmic time parallel decoding algorithms, Satellites, CD recording, Ear, expander codes, linear error-correcting codes, Error correction codes, linear time sequential decoding algorithms, expander graphs, Contracts]
An efficient membership-query algorithm for learning DNF with respect to the uniform distribution
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We present a membership-query algorithm for efficiently learning DNF with respect to the uniform distribution. In fact, the algorithm properly learns the more general class of functions that are computable as a majority of polynomially-many parity functions. We also describe extensions of this algorithm for learning DNF over certain nonuniform distributions and from noisy examples as well as for learning a class of geometric concepts that generalizes DNF. The algorithm utilizes one of Freund's boosting techniques and relies on the fact that boosting does not require a completely distribution-independent weak learner. The boosted weak learner is a nonuniform extension of a Fourier-based algorithm due to Kushilevitz and Mansour (1991).<<ETX>>
[learning DNF, Government, Circuits, Boosting, nonuniform distributions, noisy examples, Distributed computing, uniform distribution, membership-query algorithm, Computer science, Boolean functions, Polynomials, boosting techniques, Cryptography, learning (artificial intelligence), boosted weak learner]
Randomized and deterministic algorithms for geometric spanners of small diameter
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Let S be a set of n points in IR/sup d/ and let t>1 be a real number. A t-spanner for S is a directed graph having the points of S as its vertices, such that for any pair p and q of points there is a path from p to q of length at most t times the Euclidean distance between p and p. Such a path is called a t-spanner path. The spanner diameter of such a spanner is defined as the smallest integer D such that for any pair p and q of points there is a t-spanner path from p to q containing at most D edges. Randomized and deterministic algorithms are given for constructing t-spanners consisting of O(n) edges and having O(log n) diameter. Also, it is shown how to maintain the randomized t-spanner under random insertions and deletions. Previously, no results were known for spanners with low spanner diameter and for maintaining spanners under insertions and deletions.<<ETX>>
[insertions, deletions, geometric spanners, computational geometry, Data structures, deterministic algorithms, randomised algorithms, Computer science, Tree graphs, directed graphs, randomized algorithms, Euclidean distance, directed graph, Contracts]
The geometry of graphs and some of its algorithmic applications
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We explore some implications of viewing graphs as geometric objects. This approach offers a new perspective on a number of graph-theoretic and algorithmic problems. There are several ways to model graphs geometrically and our main concern here is with geometric representations that respect the metric of the (possibly weighted) graph. Given a graph G we map its vertices to a normed space in an attempt to (i) Keep down the dimension of the host space and (ii) Guarantee a small distortion, i.e., make sure that distances between vertices in G closely match the distances between their geometric images. We develop efficient algorithms for embedding graphs low-dimensionally with a small distortion.<<ETX>>
[Solid modeling, Embedded computing, graph theory, vertices, geometry of graphs, computational geometry, Extraterrestrial measurements, Mathematics, Pattern recognition, algorithmic applications, Application software, host space, Geometry, Computer science, viewing graphs, Polynomials, geometric representations]
On learning discretized geometric concepts
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We present a polynomial time online learning algorithm that learns any discretized geometric concept generated from any number of halfspaces with any number of known (to the learner) slopes in a constant dimensional space. In particular, our algorithm learns (from equivalence queries only) unions of discretized axis-parallel rectangles in a constant dimensional space in polynomial time. The algorithm also runs in polynomial time in l if the teacher lies on l counterexamples. We then show a PAC-learning algorithm for the above discretized geometric concept when the example oracle lies on the labels of the examples with a fixed probability p/spl les/ 1/2 -1/r that runs in polynomial time also with r. We use these methods, as well as a bounded version of the finite injury priority method, to construct algorithms for learning several classes of rectangles. In particular we design efficient algorithms for learning several classes of unions of discretized axis-parallel rectangles in either arbitrary dimensional spaces or constant dimensional spaces.<<ETX>>
[Algorithm design and analysis, geometric concept, computational geometry, equivalence queries, classes of rectangles, finite injury priority method, Computer science, PAC-learning, halfspaces, online learning algorithm, Approximation algorithms, Polynomials, polynomial time, learning (artificial intelligence), Injuries, discretized geometric concepts, computational complexity]
Algebraic computation trees in characteristic p>0
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We provide a simple and powerful combinatorial method for proving lower bounds for algebraic computation trees over algebraically closed fields of characteristic p>0. We apply our method to prove, for example, an /spl Omega/(n log n) lower bound for the n element distinctness problem, an /spl Omega/(n log(n/k)) lower bound to the "k-equal problem"-that is deciding whether there are k identical elements out of n input elements, and more. The proof of the main theorem relies on the deep work of B.M. Dwork, P. Deligne, and E. Bombieri on the Weil conjectures. In particular we make use of Bombieri's bound on the degree of the Zeta function of algebraic varieties over finite fields. Our bounds provide a natural extension to the recent topological lower bounds obtained by A. Bjorner, L. Lovasz and A.C. Yao for algebraic computation trees over the real numbers. For the special cases of real subspace arrangements and general complex varieties we can reformulate their specific results using our combinatorial approach without mentioning any topological invariants.<<ETX>>
[element distinctness problem, algebraically closed fields, Computational modeling, algebraic computation trees, Weil conjectures, Galois fields, Computational complexity, lower bounds, Computer science, algebraic varieties, algebraic geometric codes, Zeta function, Polynomials, Marine vehicles, Arithmetic, computational complexity, combinatorial method]
On-line admission control and circuit routing for high performance computing and communication
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
This paper considers the problems of admission control and virtual circuit routing in high performance computing and communication systems. Admission control and virtual circuit routing problems arise in numerous applications, including video-servers, real-lime database servers, and the provision of permanent virtual channel in large-scale communications networks. The paper describes both upper and lower bounds on the competitive ratio of algorithms for admission control and virtual circuit routing in trees, arrays, and hypercubes (the networks most commonly used in conjunction with nigh performance computing and communication). Our results include optimal algorithms for admission control and virtual circuit routing in trees, as well as the first competitive algorithms for these problems on non-tree networks. A key result of our research is the development of on-line algorithms that substantially outperform the greedy-based approaches that are used in practice.<<ETX>>
[on-line admission control, high performance communication, Circuits, upper bounds, hypercube networks, hypercubes, real-lime database servers, parallel processing, circuit routing, trees, virtual circuit routing, Network servers, Databases, Hypercubes, Computer networks, Large-scale systems, Communication networks, competitive algorithms, performance evaluation, video-servers, greedy-based approaches, Routing, optimal algorithms, lower bounds, High performance computing, Admission control, high performance computing, arrays]
A new efficient radix sort
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We present new improved algorithms for the sorting problem. The algorithms are not only efficient but also clear and simple. First, we introduce Forward Radix Sort which combines the advantages of traditional left-to-right and right-to-left radix sort in a simple manner. We argue that this algorithm will work very well in practice. Adding a preprocessing step, we obtain an algorithm with attractive theoretical properties. For example, n binary strings can be sorted in /spl Theta/ (n log(B/(n log n)+2)) time, where B is the minimum number of bits that have to be inspected to distinguish the strings. This is an improvement over the previously best known result by Paige and Tarjan (1987). The complexity may also be expressed in terms of H, the entropy of the input: n strings from a stationary ergodic process can be sorted in /spl Theta/ (n log(1/H+1)) time an improvement over the result recently presented by Chen and Reif (1993).<<ETX>>
[Algorithm design and analysis, complexity, Costs, Gaussian distribution, Entropy, sorting problem, Partitioning algorithms, stationary ergodic process, Sorting, Computer science, Interpolation, preprocessing step, entropy, Forward Radix Sort, sorting, radix sort, computational complexity]
A lower bound for the monotone depth of connectivity
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We show that any monotone circuit for computing graph connectivity must have a depth greater than /spl Omega/((log n)/sup 3/2// log log n). This proves that UCONN/sub n/ is not in monotone NC/sup 1/. The proof technique, which is an adaptation of Razborov's approximation method, is also used to derive lower bounds for a general class of graph problems.<<ETX>>
[Error analysis, Circuit analysis computing, graph problems, graph theory, computational geometry, lower bound, proof technique, Approximation methods, Game theory, lower bounds, Computer science, graph connectivity, Boolean functions, Razborov's approximation method, Polynomials, monotone depth of connectivity, monotone circuit, computational complexity]
Tail bounds for occupancy and the satisfiability threshold conjecture
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The classical occupancy problem is concerned with studying the number of empty bins resulting from a random allocation of m balls to n bins. We provide a series of tail bounds on the distribution of the number of empty bins. These tail bounds should find application in randomized algorithms and probabilistic analysis. Our motivating application is the following well-known conjecture on threshold phenomenon for the satisfiability problem. Consider random 3-SAT formulas with cn clauses over n variables, where each clause is chosen uniformly and independently from the space of all clauses of size 3. It has been conjectured that there is a sharp threshold for satisfiability at c*/spl ap/4.2. We provide the first non-trivial upper bound on the value of c*, showing that for c>4.758 a random 3-SAT formula is unsatisfiable with high probability. This result is based on a structural property, possibly of independent interest, whose proof needs several applications of the occupancy tail bounds.<<ETX>>
[Algorithm design and analysis, Logic programming, occupancy, Very large scale integration, computability, satisfiability threshold conjecture, upper bound, Probability distribution, probabilistic analysis, Logic testing, randomised algorithms, Computer science, structural property, Upper bound, decidability, Statistical distributions, randomized algorithms, Tail, algorithm theory, Random variables, random 3-SAT formulas, random allocation, tail bounds]
Optimizing static calendar queues
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The calendar queue is an important implementation of a priority queue which is particularly useful in discrete event simulators. In this paper we present an analysis of the static calendar queue which maintains N active events. A step of the discrete event simulator removes and processes the event with the smallest associated time and inserts a new event whose associated time is the time of the removed event plus a random increment with mean /spl mu/. We demonstrate that for the infinite bucket calendar queue the optimal bucket width is approximately /spl delta//sub opt/=/spl radic/(2b/c)/spl mu//N where b is the time to process an empty bucket and c the incremental time to process a list element. With bucket width chosen to be /spl delta//sub opt/, the expected time to process an event is approximately minimized at the constant c+/spl radic/(2bc)+d, where d is the fixed time to process an event. We show that choosing the number of buckets to be O(N) yields a calendar queue with performance equal to or almost equal to the performance of the infinite bucket calendar queue.<<ETX>>
[queueing theory, programming theory, Computational modeling, priority queue, Calendars, Data structures, Mathematics, Discrete event simulation, calendar queue, Computer science, Concurrent computing, optimisation, discrete event simulators, optimal bucket, infinite bucket calendar queue, data structures, Random variables, discrete event simulation, static calendar queues, Queueing analysis]
PAC learning with irrelevant attributes
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We consider the problem of learning in the presence of irrelevant attributes in Valiant's PAC model (1984). In the PAC model, the goal of the learner is to produce an approximately correct hypothesis from random sample data. If the number of relevant attributes in the target function is small, it may be desirable to produce a hypothesis that also depends on only a small number of variables. Haussler (1988) previously considered the problem of learning monomials of a small number of variables. He showed that the greedy set cover approximation algorithm can be used as a polynomial-time Occam algorithm for learning monomials on r of n variables. A outputs a monomial on r(ln q+1) variables, where q is the number of negative examples in the sample. We extend this result by showing that there is a polynomial-time Occam algorithm for learning k-term DNF formulas depending on r of n variables that outputs a DNF formula depending on O(r/sup k/log/sup k/q) variables, where q is the number of negative examples in the sample. We also give a polynomial-time Occam algorithm for learning decision lists (sometimes called 1-decision lists) with k alternations.<<ETX>>
[PAC learning, decision lists, Medical diagnosis, polynomial-time Occam algorithm, Diseases, greedy set cover, Approximation algorithms, Polynomials, Occam, learning (artificial intelligence), polynomial-time, Occam algorithm, Testing, computational complexity, irrelevant attributes]
Measure on small complexity classes, with applications for BPP
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We present a notion of resource-bounded measure for P and other subexponential-time classes. This generalization is based on Lutz's notion of measure, but overcomes the limitations that cause Lutz's definitions to apply only to classes at least as large as E. We present many of the basic properties of this measure, and use it to explore the class of sets that are hard for BPP. Bennett and Gill showed that almost all sets are hard for BPP; Lutz improved this from Lebesgue measure to measure on ESPACE. We use our measure to improve this still further, showing that for all /spl epsiv/>0, almost every set in E/sub /spl epsiv// is hard for BPP, where E/sub /spl epsiv//=/spl cup//sub /spl delta/</spl epsiv//DTIME(2(n/sup /spl delta//)), which is the best that can be achieved without showing that BPP is properly contained in E. A number of related results are also obtained in this way.<<ETX>>
[small complexity classes, Density measurement, BPP, subexponential-time classes, resource-bounded measure theory, Mathematics, Time measurement, Complexity theory, Application software, resource-bounded measure, Computer science, Q measurement, Power measurement, class of sets, Polynomials, Robustness, computational complexity]
The load, capacity and availability of quorum systems
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
A quorum system is a collection of sets (quorums) every two of which have a nonempty intersection. Quorum systems have been used for a number of applications in the area of distributed systems. We investigate the load, capacity and availability of quorum systems. We present four novel constructions of quorum system, all featuring optimal or near optimal load, and high availability. These desirable properties of the constructions translate into improvements of any protocol using them: a low work load on the processors and a high resilience to processor failures. The best construction, based on paths in a grid, has a load of O(1//spl radic/n), and a failure probability of exp(-O(/spl radic/n)) when the elements fail with probability p< 1/2 . Moreover, even in the presence of faults, with exponentially high probability the load of this system is still O(1//spl radic/n). The analysis of this scheme is based on Percolation Theory.<<ETX>>
[Availability, low work load, Access protocols, processor failures, Reliability theory, distributed processing, Control systems, Mathematics, availability, Resilience, capacity, quorum systems, Databases, Percolation Theory, load, Permission, Grid computing, distributed systems, Nuclear magnetic resonance, computational complexity]
Products and help bits in decision trees
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We investigate two problems concerning the complexity of evaluating a function f at k-tuple of unrelated inputs by k parallel decision tree algorithms. In the product problem, for some fixed depth bound d, we seek to maximize the fraction of input k-tuples for which all k decision trees are correct. Assume that for a single input to f, the best decision tree algorithm of depth d is correct on a fraction p of inputs. We prove that the maximum fraction of k-tuples on which k depth d algorithms are all correct is at most p/sup k/, which is the trivial lower bound. We show that if we replace the depth d restriction by "expected depth d\
[complexity, decision theory, Computational modeling, Circuits, trees (mathematics), parallel decision tree algorithms, Length measurement, Size measurement, Mathematics, lower bound, arbitrary binary questions, Computer science, help bits, fixed depth bound, decision trees, k-tuple, Polynomials, Decision trees, multivariate polynomial, Contracts, maximum fraction, Context modeling, computational complexity]
An O(n/sup 1+/spl epsiv// log b) algorithm for the complex roots problem
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Given a univariate polynomial f(z) of degree n with complex coefficients, whose real and imaginary parts can be expressed as a ratio of two integers less than 2/sup m/ in magnitude, the root problem is to find all the roots of f(z) up to specified precision 2/sup -/spl mu//. Assuming the arithmetic model for computation, we provide, for any /spl epsiv/>0, an algorithm which has complexity O(n/sup 1+/spl epsiv// log b), where b=m+/spl mu/. This improves on the previous best known algorithm for the problem which has complexity O(n/sup 2/ log b). We claim it that it follows from the fact that we can bound the precision required in all the arithmetic computations, that the complexity of our algorithm in the Boolean model of computation is O(n/sup 2+/spl epsiv//(n+b) log/sup 2/ b log log b).<<ETX>>
[complexity, Computational modeling, real parts, Fasteners, Read-write memory, Subcontracting, arithmetic computations, univariate polynomial, Computer science, complex coefficients, Boolean functions, Boolean model of computation, Information services, imaginary parts, arithmetic model, Polynomials, Internet, complex roots problem, Web sites, Arithmetic, computational complexity]
Randomized simplex algorithms on Klee-Minty cubes
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We investigate the behavior of randomized simplex algorithms on special linear programs. For this, we develop combinatorial models for the Klee-Minty cubes (1972) and similar linear programs with exponential decreasing paths. The analysis of two most natural randomized pivot rules on the Klee-Minty cubes leads to (nearly) quadratic lower bounds for the complexity of linear programming with random pivots. Thus we disprove two bounds conjectured in the literature. At the same lime, we establish quadratic upper bounds for random pivots on the linear programs under investigation. This motivates the question whether some randomized pivot rules possibly have quadratic worst-case behavior on general linear programs.<<ETX>>
[Klee-Minty cubes, linear programs, complexity, quadratic worst-case behavior, quadratic lower bounds, quadratic upper bounds, Linear programming, linear programming, combinatorial models, Upper bound, randomized simplex algorithms, Polynomials, computational complexity]
IP over connection-oriented networks and distributional paging
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Next generation wide area network are very likely to use connection-oriented protocols such as Asynchronous Transfer Mode (ATM). For the huge existing investment in current IP networks such as the Internet to remain useful, me must devise mechanisms to carry IP traffic over connection-oriented networks. A basic issue is to devise holding policies for virtual circuits carrying datagrams. In this paper we consider two variants of the paging problem that arise in the design of such holding policies. In the IP-paging problem the page inter-request times are chosen according to independent distributions. For this model we construct a very simple deterministic algorithm whose page fault rate is at most 5 times that of the best online algorithm (that knows the inter-request time distributions). We also show that some natural algorithms for this problem do not have constant competitive ratio. In distributional paging the inter-request time distributions may be dependent, and hence any probabilistic model of page request sequences can be represented. We construct a simple randomized algorithm whose page fault rate is at most 4 times that of the best online algorithm.<<ETX>>
[Protocols, wide area networks, Telecommunication traffic, asynchronous transfer mode, page inter-request times, probabilistic model, Frame relay, Asynchronous Transfer Mode, Investments, datagrams, Pricing, Traffic control, Computer networks, IP networks, protocols, Wide area networks, paged storage, Internet protocol, virtual circuits, randomized algorithm, Circuit faults, deterministic algorithm, deterministic algorithms, randomised algorithms, holding policies, distributional paging, Internet, connection-oriented networks, wide area network]
Maximum (s,t)-flows in planar networks in O(|V|log|V|) time
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Let G=(V, A) be a directed, planar graph, let s, t /spl isin/ V, s/spl ne/t, and let c/sub a/>0 be the capacity of an arc a/spl isin/A. The problem is to find a maximum flow from s to t in G: subject to these capacities. The fastest algorithm known so far requires /spl Oscr/(|V|/spl middot//sup 3//spl radic/|V|/spl middot/log|V|) time, whereas the algorithm introduced in this paper requires only /spl Oscr/(|V|log|V|) time.<<ETX>>
[Tree data structures, Algorithm design and analysis, flow graphs, Telecommunication traffic, computational geometry, planar graph, t)-flows, Shortest path problem, Intelligent networks, Upper bound, directed graphs, planar networks, maximum flow, maximum (s, tree data structures]
On the complexity of bounded-interaction and noninteractive zero-knowledge proofs
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We consider the basic cryptographic primitive known as zero-knowledge proofs on committed bits. In this primitive, a prover P commits to a set of bits, and then at a later time convinces a verifier V that some property /spl Pscr/ holds for a subset of these bits. It is known how to implement this primitive based on an ordinary bit-committal primitive, but the standard implementations involve a great deal of interaction between the prover and the verifier. We introduce new implementations that require markedly less interaction. We implement bounded-interaction proofs on committed bits, generalizing a model of A. De Micali et al. (1988). For all security parameters, our implementations require only a lg/sup 2/ (n) overhead over the best known circuit-based interactive implementations; for sufficiently large security parameters this gap drops to a lg(n) factor.<<ETX>>
[complexity, Error probability, Circuits, committed bits, cryptography, Time measurement, Security, Cryptographic protocols, bounded-interaction, cryptographic primitive, noninteractive zero-knowledge proofs, Cryptography, protocols, bounded-interaction proofs, computational complexity]
Efficient average-case algorithms for the modular group
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The modular group occupies a central position in many branches of mathematical sciences. In this paper we give average polynomial-time algorithms for the unbounded and bounded membership problems for finitely generated subgroups of the modular group. The latter result affirms a conjecture of Y. Gurevich (1990).<<ETX>>
[Algorithm design and analysis, modular group, Lattices, bounded membership, mathematical sciences, finitely generated subgroups, Linear programming, Geometry, average-case algorithms, Elliptic curves, Optical design, Character generation, Computer applications, unbounded membership, Polynomials, average polynomial-time algorithms, computational complexity]
Graph connectivity and monadic NP
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Ehrenfeucht games are a useful tool in proving that certain properties of finite structures are not expressible by formulas of a certain type. In this paper a new method is introduced that allows the extension of a local winning strategy for Duplicator, one of the two players in Ehrenfeucht games, to a global winning strategy. As an application it is shown that graph connectivity cannot be expressed by existential second-order formulas, where the second-order quantification is restricted to unary relations (monadic NP), even, in the presence of a built-in linear order. As a second application it is stated, that, on the other hand, the presence of a linear order increases the power of monadic NP more than the presence of a successor relation.<<ETX>>
[graph connectivity, monadic NP, local winning strategy, finite structures, Duplicator, game theory, Complexity theory, Ehrenfeucht games, Game theory, computational complexity, second-order quantification]
Maximum agreement subtree in a set of evolutionary trees-metrics and efficient algorithms
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
In this paper we prove that the maximum homeomorphic agreement subtree problem is /spl Nscr//spl Pscr/-complete for three trees with unbounded degrees. We then show an approximation algorithm of time O(kn/sup 5/) for choosing the species that are not in a maximum agreement subtree of a set of k trees. Our approximation is guaranteed to provide a set that is no more than 4 times the optimum solution. While the set of evolutionary trees may be large in practice, the trees usually have very small degrees, typically no larger than three. We develop a new method for finding a maximum agreement subtree of k trees, of which one has degree bounded by d. This new method enables us to find a maximum agreement subtree in time O(kn/sup d+1/).<<ETX>>
[Educational institutions, homeomorphic agreement subtree, maximum agreement subtree, Tree graphs, evolutionary trees, approximation algorithm, Approximation algorithms, Polynomials, Dynamic programming, tree data structures, metrics, efficient algorithms, Classification tree analysis, computational complexity]
Algorithmic number theory-the complexity contribution
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Though algorithmic number theory is one of man's oldest intellectual pursuits, its current vitality is perhaps unrivalled in history. This is due in part to the injection of new ideas from computational complexity. In this paper, a brief history of the symbiotic relationship between number theory and complexity theory will be presented. In addition, some of the technical aspects underlying 'modern' methods of primality testing and factoring will be described. Finally, an extensive lists of open problems in algorithmic number theory will be provided.<<ETX>>
[Symbiosis, open problems, Complexity theory, History, Least squares methods, Computer science, Earth, factoring, Gaussian processes, primality testing, Marine vehicles, Pursuit algorithms, Testing, computational complexity, number theory, algorithmic number theory]
On rank vs. communication complexity
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
This paper concerns the open problem of Lovasz and Saks (1988) regarding the relationship between the communication complexity of a Boolean function and the rank of the associated matrix. We first give an example exhibiting the largest gap known. We then prove two related theorems.<<ETX>>
[Computer science, associated function, Boolean functions, Protocols, Boolean function, rank, Complexity theory, associated matrix, communication complexity, matrix, deterministic communication complexity, matrix algebra]
Local optimization of global objectives: competitive distributed deadlock resolution and resource allocation
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The work is motivated by deadlock resolution and resource allocation problems, occurring in distributed server-client architectures. We consider a very general setting which includes, as special cases, distributed bandwidth management in communication networks, as well as variations of classical problems in distributed computing and communication networking such as deadlock: resolution and "dining philosophers". In the current paper, we exhibit first local solutions with globally-optimum performance guarantees. An application of our method is distributed bandwidth management in communication networks. In this setting, deadlock resolution (and maximum fractional independent set) corresponds to admission control maximizing network throughput. Job scheduling (and minimum fractional coloring) corresponds to route selection that minimizes load.<<ETX>>
[competitive distributed deadlock resolution, distributed processing, distributed bandwidth management, admission control, network throughput, Distributed computing, global objectives, distributed server-client architectures, distributed computing, resource allocation, telecommunication networks, Computer architecture, Bandwidth, scheduling, Communication networks, Contracts, job scheduling, globally-optimum performance, client-server systems, minimum fractional coloring, communication networks, local optimization, Computer science, Admission control, maximum fractional independent set, System recovery, Resource management, Computer network management, communication networking]
Nearly tight bounds for wormhole routing
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We present nearly tight bounds for wormhole muting on Butterfly networks which indicate it is fundamentally different from store-and-forward packet routing. For instance, consider the problem of routing N log N (randomly generated) log N length messages from the inputs to the outputs of an N input Butterfly. We show that with high probability that this must take time at least /spl Omega/(log/sup 3/N/(log log N)/sup 2/). The best lower bound known earlier was /spl Omega/(log/sup 2/ N), which is simply the flit congestion an each link. Thus our lower bound shows that wormhole routing (unlike store-and-forward-routing) is very ineffective in utilizing communication links. We also give a routing algorithm which nearly matches our lower bound. That is, we show that with high probability the time is O(log/sup 3/ N log log N), which improves upon, the previous best bound of O(log/sup 4/ N). Our method also extends to other networks such as the two-dimensional mesh, where it is nearly optimal. Finally, we consider the problem of offline wormhole routing, where we give optimal algorithms for trees and multidimensional meshes.<<ETX>>
[Algorithm design and analysis, Computer worms, Buffer storage, multidimensional meshes, trees (mathematics), probability, two-dimensional mesh, high probability, Routing, hypercube networks, optimal algorithms, Mathematics, lower bound, trees, Scheduling algorithm, Concurrent computing, Computer science, wormhole routing, Processor scheduling, routing algorithm, Hardware, Butterfly networks, nearly tight bounds]
Set constraints with projections are in NEXPTIME
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Systems of set constraints describe relations between sets of ground terms. They have been successfully used in program analysis and type inference. In this paper we prove that the problem of existence of a solution of a system of set constraints with projections is in NEXPTIME, and thus that it is NEXPTIME-complete. This extends the result of A. Aiken, D. Kozen, and E.L. Wimmers (1993) and R. Gilleron, S. Tison, and M. Tommasi (1990) on decidability of negated set constraints and solves a problem that was open for several years.<<ETX>>
[Algorithm design and analysis, projections, Logic programming, computability, inference mechanisms, Computer science, negated set constraints, decidability, Automata, program analysis, set constraints, Constraint theory, Inference algorithms, NEXPTIME, computational complexity, type inference]
On the computation of Boolean functions by analog circuits of bounded fan-in
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We consider the complexity of computing Boolean functions by analog circuits of bounded fan-in, i.e. by circuits of gates computing real-valued functions, either exactly or as a sign-representation. Sharp upper bounds are obtained for the complexity of the most difficult n-variable function over certain bases (sign-representation by arithmetic circuits and exact computation by piecewise linear circuits). Bounds are given for the computational power gained by adding discontinuous gate functions and nondeterminism. We also prove explicit nonlinear lower bounds for the formula size of analog circuits over bases containing addition, subtraction, multiplication, the sign function and all real constants.<<ETX>>
[complexity, piecewise linear circuits, Piecewise linear techniques, real-valued functions, Piecewise linear approximation, Analog computers, Analog circuits, upper bounds, Mathematics, sign-representation, explicit nonlinear lower bounds, Statistics, n-variable function, Boolean functions, Upper bound, bounded fan-in, Neural networks, computation of Boolean functions, analog circuits, Arithmetic, computational complexity, nondeterminism]
A note on the /spl theta/ number of Lovasz and the generalized Delsarte bound
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The /spl theta/ number of Lovasz and the /spl theta//sub 1/2/ of Schrijver, McEliece, Rodemich and Rumsey are convex (semidefinite) programming upper bounds on /spl alpha/(G), the size of a maximal independent set of G. It is known that /spl alpha/(G)/spl les//spl theta//sub 1/2/(G)/spl les//spl theta/(G)/spl les//spl chi/~(G), where /spl chi/~(G) is the clique cover number of G. The above inequalities suggest that perhaps /spl theta//sub 1/2/(G) approximates /spl alpha/(G) from above, and /spl theta/(G) approximates /spl chi/~(G) from below for every graph G. Can this approximation be to within a factor of at most n/sup 1-/spl epsiv// for some fixed /spl epsiv/>0? We show, that the following three conjectures are equivalent: 1. /spl exist//spl epsiv/>0 : /spl theta/(G) approximates /spl alpha/(G) for every G within a factor of n/sup n-/spl epsiv// 2. /spl exist//spl epsiv/>0 : /spl theta/(G) approximates /spl chi/~(G) for every G within a factor of n/sup n-/spl epsiv// 3. /spl exist//spl epsiv/>0 : /spl theta//sub 1/2/(G) approximates /spl alpha/(G) for every G within a factor of n/sup n-/spl epsiv// It is not impossible that /spl theta//sub 1/2/ approximates /spl chi/~(G), but the latter conjecture looks strictly stronger than 1-3. We give however a simple combinatorial reformulation of this one (we cannot find such for 1-3). We rule out some likely candidates for counterexamples to 1-3 by showing that /spl theta/(G) approximates /spl alpha/(G) and /spl chi/~(G) for those graphs G that come from the Hamming scheme.<<ETX>>
[generalized Delsarte bound, Upper bound, Symmetric matrices, programming theory, convex programming upper bounds, convex programming, Eigenvalues and eigenfunctions, programming upper bounds, /spl theta/ number, semidefinite]
The localization problem for mobile robots
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
A fundamental task for an autonomous mobile robot is that of localization-determining its location in a known environment. This problem arises in settings that range from the computer analysis of aerial photographs to the design of autonomous Mars rovers. L. Guibas et al. ((1992) have given geometric algorithms for the problem of enumerating locations for a robot consistent with a given view of the environment. We provide an on-line algorithm for a robot to move within its environment so as to uniquely determine its location. The algorithm improves asymptotically on strategies based purely on the "spiral search" technique of R. Baeza-Yates et al. (1993); an interesting feature of our approach is the way in which the robot is able to identify "critical directions" in the environment which allow it to perform late stages of the search more efficiently.<<ETX>>
[autonomous mobile robot, Navigation, autonomous Mars rovers, geometric programming, mobile robots, path planning, Mobile robots, Computer science, Remotely operated vehicles, geometric algorithms, Mars, on-line algorithm, Veins, localization problem, Prototypes, Robot sensing systems]
On the robustness of functional equations
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Given a functional equation, such as /spl forall/x, y f(x)+f(y)=f(x+y), we study the following general question: When can the "for all" quantifiers be replaced by "for most" quantifiers without essentially changing the functions that are characterized by the property? When "for most" quantifiers are sufficient, we say that the functional equation is robust. We show conditions on functional equations of the form /spl forall/x, y F[f(x-y), f(x+y), f(x), f(y)]=0, where F is an algebraic function, that imply robustness. We then initiate a general study aimed at characterizing properties of functional equations that determine whether or not they are robust. Our results have applications to the area of self-testing/correcting programs-this paper provides results which show that the concept of self-testing/correcting has much broader applications than we previously understood. We show that self-testers and self-correctors can be found for many functions satisfying robust functional equations, including tan x, 1/1+cot x, Ax/1-Ax', cosh x.<<ETX>>
[self-testers, self correcting programs, self-testing programs, Built-in self-test, Mathematics, Equations, Physics, functional equations robustness, self-correctors, functional equations, Automatic testing, Linearity, Prototypes, Robustness, quantifiers, computational complexity]
Multi-index hashing for information retrieval
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We describe a technique for building hash indices for a large dictionary of strings. This technique permits robust retrieval of strings from the dictionary even when the query pattern has a significant number of errors. This technique is closely related to the classical Turan problem for hypergraphs. We propose a general method of multi-index construction by generalizing certain Turan hypergraphs. We also develop an accompanying theory for analyzing such hashing schemes. The resulting algorithms have been implemented and can be applied to a wide variety of recognition and retrieval problems.<<ETX>>
[algorithms, Dictionaries, Hamming distance, robust retrieval, information retrieval, Information retrieval, Data structures, Turan problem, Application software, hash indices, Handwriting recognition, multi-index construction, multi-index hashing, Computer errors, Speech, file organisation, Robustness, data structures, Error correction, query formulation]
Lower bounds on Hilbert's Nullstellensatz and propositional proofs
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The weak form of the Hilbert's Nullstellensatz says that a system of algebraic equations over a field, Q/sub i/(x~)=0, does not have a solution in the algebraic closure iff 1 is in the ideal generated by the polynomials Q/sub i/(x~). We shall prove a lower bound on the degrees of polynomials P/sub i/(x~) such that /spl Sigma//sub i/ P/sub i/(x~)Q/sub i/(x~)=1. This result has the following application. The modular counting principle states that no finite set whose cardinality is not divisible by q can be partitioned into q-element classes. For each fixed cardinality N, this principle can be expressed as a propositional formula Count/sub q//sup N/. Ajtai (1988) proved recently that, whenever p, q are two different primes, the propositional formulas Count/sub q//sup qn+1/ do not have polynomial size, constant-depth Frege proofs from instances of Count/sub p//sup m/, m/spl ne/0 (mod p). We give a new proof of this theorem based on the lower bound for the Hilbert's Nullstellensatz. Furthermore our technique enables us to extend the independence results for counting principles to composite numbers p and q. This results in an exact characterization of when Count/sub q/ can be proven efficiently from Count/sub p/, for all p and q.<<ETX>>
[modular counting principle, fixed cardinality, propositional proofs, computability, Mathematics, Galois fields, Equations, Computer science, Geometry, Upper bound, algebraic equations, propositional formulas, Polynomials, Concrete, Hilbert's Nullstellensatz, Testing, computational complexity]
Rapid rumor ramification: approximating the minimum broadcast time
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Given an undirected graph representing a network of processors, and a source node containing a message that must be broadcast to all the nodes, find a scheme that accomplishes the broadcast in the minimum number of time steps. At each time step, any processor that has received the message is allowed to communicate the message to at most one of its neighbors in the network, i.e. can communicate via a telephone call to a neighbor. This has been termed the minimum broadcast time problem under the telephone model and is known to be NP-complete. The minimum broadcast time in a graph is closely related to the poise of the graph. The poise of a tree is defined to be the quantity (maximum degree of any node in the tree+diameter of the tree). The poise of a graph is the minimum poise of any of its spanning trees. Computing the poise of a graph is shown to be NP-hard and an algorithm for computing a spanning tree of approximately minimum poise is derived. This algorithm is then used to derive an O(log/sup 2/n/log log n)-approximation for the minimum broadcast time problem on an n-node graph. Our algorithm extends to many generalizations of the problem such as the multicast problem, a telephone model allowing conference calls, and to the closely related minimum gossip time problem.<<ETX>>
[parallel algorithms, NP-hard, computational geometry, NP-complete, minimum broadcast time approximation, rapid rumor ramification, multicast problem, Computer science, conference calls, Telegraphy, Multicast algorithms, telephone model, Tree graphs, Broadcasting, Telephony, Approximation algorithms, minimum gossip time problem, Call conference, Communication networks, undirected graph, minimum broadcast time problem, Clocks, computational complexity]
Efficient oblivious branching programs for threshold functions
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
In his survey paper on branching programs, A.A. Razborov (1991) asked the following question: Does every rectifier-switching network computing the majority of n bits have size n/sup 1+/spl Omega/(1/)? We answer this question in the negative by constructing a simple oblivious branching program of size O(n log/sup 3/ n/log log n log log log n) for computing any threshold function. This improves the previously best known upper bound of O(n/sup 3/2/) due to O.B. Lupanov (1965).<<ETX>>
[programming theory, threshold function, Input variables, Circuits, Binary decision diagrams, Length measurement, Size measurement, upper bound, threshold logic, Computer science, Upper bound, Boolean functions, oblivious branching programs, branching programs, rectifier-switching network, Computer networks, Labeling, threshold functions]
"Go with the winners" algorithms
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We can view certain randomized optimization algorithms as rules for randomly moving a particle around in a state space; each state might correspond to a distinct solution to the optimization problem, or more generally, the state space might express some other structure underlying the optimization algorithm. In this setting, a general paradigm for designing heuristics is to run several simulations of the algorithm simultaneously, and every so often classify the particles as "doing well" or "doing badly\
[Algorithm design and analysis, Temperature, randomized optimization algorithms, trees (mathematics), searching, tree, deep leaf, worst case running time, Fractals, Go with the winners, State-space methods, Statistics, rigorous analysis, randomised algorithms, Computer science, Analytical models, optimisation, Simulated annealing, probability of success, Polynomials, Concrete, computational complexity, worst-case]
Long tours and short superstrings
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
This paper considers weight-maximizing variants of the classical symmetric and asymmetric traveling-salesman problems. Like their weight-minimizing counterparts, these variants are MAX SNP-hard. We present the first nontrivial approximation algorithms for these problems. Our algorithm for directed graphs finds a tour whose weight is at least 38/63/spl ap/0.603 times the weight of a maximum-weight tour, and our algorithm for undirected graphs finds a tour whose weight is at least 5/7/spl ap/0.714 times optimal. These bounds compare favorably with the 1/2 and 2/3 bounds that can be obtained for undirected and directed graphs, respectively, by simply deleting the minimum-weight edge from each cycle of a maximum-weight cycle cover. Our algorithm for directed graphs can be used to improve several recent approximation results for the shortest-superstring problem.<<ETX>>
[Algorithm design and analysis, shortest-superstring problem, Data compression, computational geometry, nontrivial approximation algorithms, Educational institutions, long tours, weight-maximizing variants, Computer science, traveling-salesman problems, Upper bound, NP-hard problem, directed graphs, DNA, operations research, Approximation algorithms, Polynomials, short superstrings, weight-minimizing counterparts, MAX SNP-hard, undirected graphs, computational complexity]
On monotone formula closure of SZK
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We investigate structural properties of statistical zero knowledge (SZK) both in the interactive and in the non-interactive model. Specifically, we look into the closure properties of SZK languages under monotone logical formula composition. This gives rise to new protocol techniques. We show that interactive SZK for random self reducible languages (RSR) (and for co-RSR) is closed under monotone Boolean operations. Namely, we give SZK proofs for monotone Boolean formulae whose atoms are statements about an SZK language which is RSR (or a complement of RSR). All previously known languages in SZK are in these classes. We then show that if a language L has a non-interactive SZK proof system then honest-verifier interactive SZK proof systems exist for all monotone Boolean formulae whose atoms are statements about the complement of L. We also discuss extensions and generalizations.<<ETX>>
[Boolean operations, honest-verifier interactive SZK proof systems, Protocols, Acoustic testing, SZK, cryptography, closure properties, Computer science, monotone formula closure, structural properties, Boolean functions, monotone logical formula, file organisation, random self reducible languages, statistical zero knowledge, theorem proving, protocol techniques]
The complexity of the membership problem for 2-generated commutative semigroups of rational matrices
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We present a deterministic polynomial-time algorithm for the ABC problem, which is the membership problem for 2-generated commutative linear semigroups over an algebraic number field. We also obtain a polynomial time algorithm, for the (easier) membership problem, for 2-generated abelian linear groups. Furthermore, we provide a polynomial-sized encoding for the set of all solutions.<<ETX>>
[rational matrices, deterministic polynomial-time algorithm, polynomial-sized encoding, 2-generated abelian linear groups, algebraic number field, Encoding, encoding, Galois fields, deterministic algorithms, polynomial time algorithm, Physics, matrix algebra, Computer science, 2-generated commutative semigroups, Polynomials, ABC problem, Testing, computational complexity, membership problem complexity]
Priority encoding transmission
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We introduce a novel approach for sending messages over lossy packet-based networks. The new method, called Priority Encoding Transmission, allows a user to specify a different priority on each segment of the message. Based on the priorities, the sender uses the system to encode the segments into packets for transmission. The system ensures recovery of the segments in order of their priority. The priority of a segment determines the minimum number of packets sufficient to recover the segment. We define a measure for a set of priorities, called the rate, which dictates how much information about the message must be contained in each bit of the encoding. We develop systems for implementing any set of priorities with rate equal to one. We also give an information-theoretic proof that there is no system that implements a set of priorities with rate greater than one. This work has applications to multi-media and high speed networks applications, especially in those with bursty sources and multiple receivers with heterogeneous capabilities.<<ETX>>
[multimedia systems, Encoding, Decoding, Application software, encoding, lossy packet-based networks, segments, Computer science, multimedia, High-speed networks, information-theoretic proof, Bandwidth, Propagation losses, Error correction, Buffer overflow, Positron emission tomography, priority encoding transmission, high speed networks, multiple receivers]
Fully dynamic cycle-equivalence in graphs
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Two edges e/sub 1/ and e/sub 2/ of an undirected graph are cycle-equivalent iff all cycles that contain e/sub 1/ also contain e/sub 2/, i.e., iff e/sub 1/ and e/sub 2/ are a cut-edge pair. The cycle-equivalence classes of the control-flow graph are used in optimizing compilers to speed up existing control-flow and data-flow algorithms. While the cycle-equivalence classes can be computed in linear time, we present the first fully dynamic algorithm for maintaining the cycle-equivalence relation. In an n-node graph our data structure executes an edge insertion or deletion in O(/spl radic/n log n) time and answers the query whether two given edges are cycle-equivalent in O(log/sup 2/ n) time. We also present an algorithm for plane graphs with O(log n) update and query time and for planar graphs with O(log n) insertion time and O(log/sup 2/ n) query and deletion time. Additionally, we show a lower bound of /spl Omega/(log n/log log n) for the amortized time per operation for the dynamic cycle-equivalence problem in the cell probe model.<<ETX>>
[Algorithm design and analysis, cycle-equivalence, Optimizing compilers, Heuristic algorithms, graph theory, planar graphs, data structure, computational geometry, cycle-equivalence problem, Centralized control, graphs, data structures, undirected graph, Probes, Testing, Availability, Data analysis, optimising compilers, control-flow graph, query time, Data structures, optimizing compilers, plane graphs, Computer science, equivalence classes]
The power of team exploration: two robots can learn unlabeled directed graphs
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We show that two cooperating robots can learn exactly any strongly-connected directed graph with n indistinguishable nodes in expected time polynomial in n. We introduce a new type of homing sequence for two robots which helps the robots recognize certain previously-seen nodes. We then present an algorithm in which the robots learn the graph and the homing sequence simultaneously by wandering actively through the graph. Unlike most previous learning results using homing sequences, our algorithm does not require a teacher to provide counterexamples. Furthermore, the algorithm can use efficiently any additional information available that distinguishes nodes. We also present an algorithm in which the robots learn by taking random walks. The rate at which a random walk converges to the stationary distribution is characterized by the conductance of the graph. Our random-walk algorithm learns in expected time polynomial in n and in the inverse of the conductance and is more efficient than the homing-sequence algorithm for high-conductance graphs.<<ETX>>
[Legged locomotion, strongly-connected directed graph, random walks, intelligent control, Learning automata, Roads, Laboratories, homing sequence, cooperating robots, Robotics and automation, Computer science, team exploration, teacher, directed graphs, Cities and towns, random-walk algorithm, Polynomials, unlabeled directed graphs, learning (artificial intelligence), Robots, Radio communication]
On syntactic versus computational views of approximability
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We attempt to reconcile the two distinct views of approximation classes: syntactic and computational. Syntactic classes such as MAX SNP permit structural results and have natural complete problems, while computational classes such as APX allow us to work with classes of problems whose approximability is well-understood. Our results provide a syntactic characterization of computational classes, and give a computational framework for syntactic classes.<<ETX>>
[computational views, approximation theory, computational classes, MAX SNP, approximation classes, Computer science, Couplings, syntactic, Approximation algorithms, Polynomials, structural results, computational complexity, approximability]
Fast and lean self-stabilizing asynchronous protocols
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We consider asynchronous general topology dynamic networks of identical nameless nodes with worst-case transient faults. Starting from any faulty configuration, our protocols self-stabilize any computation in time polynomial in the (unknown) network diameter. This version sacrifices some diversity of tasks and efficiency for simplicity and clarity of details. Appendix gives more efficient procedures in less detail.<<ETX>>
[Protocols, Nominations and elections, self-stabilizing asynchronous protocols, Distributed computing, Counting circuits, identical nameless nodes, asynchronous general topology dynamic networks, Network topology, Algorithms, worst-case transient faults, Resists, faulty configuration, Computer networks, Polynomials, time polynomial, protocols, Clocks]
On the design of reliable Boolean circuits that contain partially unreliable gates
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We investigate a model of gate failure for Boolean circuits in which a faulty gate is restricted to output one of its input values. For some types of gates, the model (which we call the short-circuit model of gate failure) is weaker than the traditional von Neumann model where faulty gates always output precisely the wrong value. Our model has the advantage that it allows us to design Boolean circuits that can tolerate worst-case faults, as well as circuits that have arbitrarily high success probability in the case of random faults. Moreover, the short-circuit model captures a particular type of fault that commonly appears in practice, and it suggests a simple method for performing post-test alterations to circuits that have more severe types of faults. A variety of bounds on the size of fault-tolerant circuits are proved in the paper. Perhaps, the most important is a proof that any k-fault-tolerant circuit for any input-sensitive function using any type of gates (even arbitrarily powerful, multiple-input gates) must have size at least /spl Omega/(k log k/log log k). Obtaining a tight bound on the size of a circuit for computing the AND of two values if up to k of the gates are faulty is one of the central questions left open in the paper.<<ETX>>
[post-test alterations, fault-tolerant circuits, random faults, Laboratories, Mathematics, Decoding, Circuit faults, Wire, worst-case faults, Computer science, Fault tolerance, Boolean functions, partially unreliable gates, von Neumann model, reliable Boolean circuits design, tight bound, Mathematical model, short-circuit model, Integrated circuit modeling, Contracts, logic design, gate failure]
On the combinatorial and algebraic complexity of quantifier elimination
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
In this paper we give a new algorithm for performing quantifier elimination from first order formulae over real closed fields. This algorithm improves the complexity of the asymptotically fastest algorithm for this problem, known to this date. A new feature of our algorithm is that the role of the algebraic part (the dependence on the degrees of the input polynomials) and the combinatorial part (the dependence on the number of polynomials) are separated, making possible our improved complexity bound. Another new feature is that the degrees of the polynomials in the equivalent quantifier-free formula that we output, are independent of the number of input polynomials. As special cases of this algorithm, we obtain new and improved algorithms for deciding a sentence in the first order theory over real closed fields, and also for solving the existential problem in the first order theory over real closed fields. Using the theory developed in this paper, we also give an improved bound on the radius of a ball centered at the origin, which is guaranteed to intersect every connected component of the sign partition induced by a family of polynomials. We also use our methods to obtain algorithms for solving certain decision problems in real and complex geometry which improves the complexity of the currently known algorithms for these problems.<<ETX>>
[asymptotically fastest algorithm, Computer vision, sign partition, polynomials, computational geometry, Mathematics, Partitioning algorithms, Application software, input polynomials, Orbital robotics, Robot programming, combinatorial complexity, first order theory, Computer science, Computational geometry, first order formulae, Polynomials, algebraic complexity, quantifier elimination, real closed fields, complexity bound, complex geometry, Mathematical programming, computational complexity]
Complexity lower bounds for computation trees with elementary transcendental function gates
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We consider computation trees which admit as gate functions along with the usual arithmetic operations also algebraic or transcendental functions like exp, log, sin, square root (defined in the relevant domains) or much more general, Pfaffian functions. A new method for proving lower bounds on the depth of these trees is developed which allows to prove a lower bound /spl Omega/(/spl radic/(log N)) for testing membership to a convex polyhedron with N facets of all dimensions, provided that N is large enough. This method differs essentially from the previous approaches adopted for algebraic computation trees.<<ETX>>
[complexity lower bounds, Computational modeling, polynomials, Mathematics, membership, Computer science, transcendental functions, computation trees, arithmetic operations, Differential equations, Digital arithmetic, computation theory, Polynomials, elementary transcendental function gates, convex polyhedron, Testing, computational complexity, algebraic functions]
Polynomial time randomised approximation schemes for the Tutte polynomial of dense graphs
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The Tutte-Grothendieck polynomial T(G; x, y) of a graph G encodes numerous interesting combinatorial quantities associated with the graph. Its evaluation in various points in the (x,y) plane gave the number of spanning forests of the graph, the number of its strongly connected orientations, the number of its proper k-colorings, the (all terminal) reliability probability of the graph, and various other invariants the exact computation of each of which is well known to be P-hard. Here we develop a general technique that supplies fully polynomial randomised approximation schemes for approximating the valve of T(G; x,, y) for any dense graph G, that is, any graph on n vertices whose minimum degree is /spl Omega/(n), whenever x/spl ges/1 and y/spl ges/1, and in various additional points. This region includes evaluations of reliability and partition functions of the ferromagnetic Q-state Potts model. Extensions to linear matroids where T specialises to the weight enumerator of linear codes are considered as well.<<ETX>>
[weight enumerator, Tutte polynomial, spanning forests, linear codes, polynomial time randomised approximation schemes, graph theory, Tutte-Grothendieck polynomial, P-hard, partition functions, dense graphs, linear matroids, Educational institutions, k-colorings, Mathematics, Potts model, strongly connected orientations, randomised algorithms, Linear code, polynomial matrices, ferromagnetic Q-state Potts model, Polynomials, combinatorial quantities, Bipartite graph, reliability probability]
Motion planning on a graph
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We are given a connected, undirected graph G on n vertices. There is a mobile robot on one of the vertices; this vertex is labeled s. Each of several other vertices contains a single movable obstacle. The robot and the obstacles may only reside at vertices, although they may be moved across edges. A vertex may never contain more than one object (robot/obstacle). In one step, we may move either the robot or one of the obstacles from its current position /spl upsi/ to a vacant vertex adjacent to v. Our goal is to move the robot to a designated vertex t using the smallest number of steps possible. The problem is a simple abstraction of a robot motion planning problem, with the geometry replaced by the adjacencies in the graph. We point out its connections to robot motion planning. We study its complexity, giving exact and approximate algorithms for several cases.<<ETX>>
[single movable obstacle, Strips, complexity, motion planning, vertex, Laboratories, vertices, mobile robot, Production facilities, path planning, mobile robots, Mobile robots, Motion planning, Robot motion, Computer science, Computational geometry, Layout, adjacencies, position control, undirected graph, Floors, simple abstraction, computational complexity]
Randomness-efficient oblivious sampling
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We introduce a natural notion of obliviousness of a sampling procedure, and construct a randomness-efficient oblivious sampler. Our sampler uses O(l+log /spl delta//sup -1//spl middot/log l) coins to output m=poly(/spl epsiv//sup -1/, log /spl delta//sup -1/, log l) sample points x/sub 1/, ..., x/sub m/, /spl isin/ {0, 1}/sup 1/ such that Pr[|1/m/spl Sigma//sub i=1//sup m/f(x/sub i/)-E[f]|</spl epsiv/]/spl ges/1-/spl delta/ for any function f: {0, 1}/sup 1//spl rarr/[0, 1]. We apply this sampling procedure to reduce the randomness required to halve the number of rounds of interaction in an Arthur Merlin proof system. Given a 2g(n) round AM proof for L in which Arthur sends l(n) coins per round and Merlin responds with a q(n) bit string, we construct a g(n) round AM proof for L in which Arthur sends O(l+(q+log g)/spl middot/log l) coins per round and Merlin responds with a poly(n) bit string.<<ETX>>
[Boolean functions, sampling procedure, Tail, Sampling methods, Polynomials, randomness-efficient oblivious sampler, Electronic mail, Random variables, Security, randomness-efficient oblivious sampling, computational complexity, Arthur Merlin proof system]
Tractability of parameterized completion problems on chordal and interval graphs: minimum fill-in and physical mapping
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We study the parameterized complexity of several NP-Hard graph completion problems: The minimum fill-in problem is to decide if a graph can be triangulated by adding at most k edges. We develop an O(k/sup 5/ mn+f(K)) algorithm for the problem on a graph with n vertices and m edges. In particular, this implies that the problem is fixed parameter tractable (FPT). proper interval graph completion problems, motivated by molecular biology, ask for adding edges in order to obtain a proper interval graph, so that a parameter in that graph does not exceed k. We show that the problem is FPT when k is the number of added edges. For the problem where k is the clique size, we give an O(f(k)n/sup k-1/) algorithm, so it is polynomial for fixed k. On the other hand, we prove its hardness in the parameterized hierarchy, so it is probably not FPT. Those results are obtained even when a set of edges which should not be added is given. That set can be given either explicitly or by a proper vertex coloring which the added edges should respect.<<ETX>>
[Art, Symmetric matrices, added edges, graph theory, parameterized complexity, Sparse matrices, tractability, minimum fill-in, vertex coloring, Computer science, NP-Hard graph completion problem, NP-hard problem, physical mapping, parameterized completion problems, National electric code, Bandwidth, Polynomials, interval graphs, Contracts, computational complexity]
Estimating the size of the transitive closure in linear time
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Computing transitive closure and reachability information in directed graphs is a fundamental graph problem with many applications. The fastest known algorithms run in O(sm) time for computing all nodes reachable from each of 1/spl les/s/spl les/n source nodes, or, using fast matrix multiplication, in O(n/sup 2.38/) time for computing the transitive closure, where n is the number of nodes and m the number of edges in the graph. In query optimization in database applications it is often the case that only estimates on the size of the transitive closure and on the number of nodes reachable from certain nodes are needed. We present an O(m) time randomized algorithm that estimates the number of nodes reachable from every node and the size of the transitive closure. We also obtain a O/spl tilde/(m) time algorithm for estimating sizes of neighborhoods in directed graphs with nonnegative weights, avoiding the O/spl tilde/(mn) time bound of explicitly computing these neighborhoods. Our size-estimation algorithms are much faster than performing the actual computations and improve significantly over previous estimation methods.<<ETX>>
[neighborhoods, computational geometry, reachability information, randomized algorithm, database applications, IEL, matrix multiplication, optimisation, query optimization, Query processing, directed graphs, size-estimation algorithms, Database systems, fast matrix multiplication, transitive closure, linear time]
Reducibility and completeness in multi-party private computations
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We define the notions of reducibility and completeness in multi-party private computations. Let g be an n-argument function. We say that a function f is reducible to g if n honest-but-curious players can compute the function f n-privately, given a black-box for g (for which they secretly give inputs and get the result of operating g on these inputs). We say that g is complete (for multi-party private computations) if every function f is reducible to g. In this paper, we characterize the complete Boolean functions: we show that a Boolean function g is complete if and only if g itself cannot be computed n-privately (when there is no black-box available). Namely, for Boolean functions, the notions of completeness and n-privacy are complementary. This characterization gives a huge collection of complete functions (any non-private Boolean function!) compared to very few examples given (implicitly) in previous work. On the other hand, for non-Boolean functions, we show that these two notions are not complementary. Our results can be viewed as a generalization (for multi-party protocols and for (n/spl ges/2)-argument functions) of the two-party case, where it was known that Oblivious Transfer protocol (and its variants) are complete.<<ETX>>
[Protocols, black-box, completeness, Computer science, Privacy, Boolean functions, security of data, n-privacy, Information security, multi-party private computations, Communication channels, reducibility, data privacy, Oblivious Transfer protocol, protocols, Marine vehicles, multi-party protocols, Contracts]
Finding the k shortest paths
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We give algorithms for finding the k shortest paths (not required to be simple) connecting a pair of vertices in a digraph. Our algorithms output an implicit representation of these paths in a digraph with n vertices and m edges, in time O(m+n log n+k). We can also find the k shortest paths from a given source s to each vertex in the graph, in total time O(m+n log n+kn). We describe applications to dynamic programming problems including the knapsack problem, sequence alignment, and maximum inscribed polygons.<<ETX>>
[algorithms, dynamic programming problems, knapsack problem, sequence alignment, vertices, maximum inscribed polygons, dynamic programming, computational geometry, Routing, Path planning, Shortest path problem, Computer science, Robot motion, Motion planning, Road transportation, implicit representation, digraph, operations research, Biology computing, Dynamic programming, Joining processes, k shortest paths]
CS proofs
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
This paper puts forward a computationally-based notion of proof and explores its implications to computation at large. In particular, given a random oracle or a suitable cryptographic assumption, we show that every computation possesses a short certificate vouching its correctness, and that, under a cryptographic assumption, any program for a /spl Nscr//spl Pscr/-complete problem is checkable in polynomial time. In addition, our work provides the beginnings of a theory of computational complexity that is based on "individual inputs" rather than languages.<<ETX>>
[computationally sound proof, Laboratories, cryptography, Complexity theory, NP-complete problem, Computational complexity, Computer science, computationally-based notion, cryptographic assumption, CS proofs, random oracle, NP complete problem, Polynomials, polynomial time, proof, Cryptography, computational complexity]
Algorithms for quantum computation: discrete logarithms and factoring
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in computation time of at most a polynomial factor: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their computational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. We thus give the first examples of quantum cryptanalysis.<<ETX>>
[parallel algorithms, quantum computer, Costs, Computational modeling, Computer simulation, Circuit simulation, Las Vegas algorithms, Mechanical factors, physical computational device, polynomial factor, factoring, Quantum computing, Physics computing, cryptosystems, discrete logarithms, Quantum mechanics, Polynomials, quantum computation algorithms, Cryptography, computational complexity]
A polynomial-time algorithm for deciding equivalence of normed context-free processes
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
A polynomial-time procedure is presented for deciding bisimilarity of normed context-free processes. It follows as a corollary that language equivalence of simple context-free grammars is decidable in polynomial time.<<ETX>>
[Context, equivalence, normed context-free processes, Mathematics, bisimilarity, Computer science, Algebra, decidability, Councils, Production, context-free grammars, Polynomials, Carbon capture and storage, polynomial-time algorithm, equivalence classes, language equivalence]
Finding separator cuts in planar graphs within twice the optimal
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
Building on the works of S.B. Rao (1987, 1992) and J.K. Park and C.A. Phillips (1993), we present a factor 2 approximation algorithm for the problem of finding a minimum cost b-balanced cut in planar graphs, for b/spl les/1/3, if the vertex weights are given in unary (using scaling, a psuedo-approximation algorithm is also presented for the case of binary vertex weights). This problem is of considerable practical significance, especially in VLSI design.<<ETX>>
[minimum cost b-balanced cut, Particle separators, Circuits, graph theory, planar graphs, separator cuts, Very large scale integration, computational geometry, VLSI design, Partitioning algorithms, factor 2 approximation algorithm, vertex weights, Computer science, Approximation algorithms, Cost function, Iterative algorithms, psuedo-approximation algorithm, binary vertex weights]
Computing with very weak random sources
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
For any fixed /spl epsiv/>0, we show how to simulate RP algorithms in time n/sup O(log n/) using the output of a /spl delta/-source with min-entropy R(/spl epsiv/). Such a weak random source is asked once for R(/spl epsiv/) bits; it outputs an R-bit string such that any string has probability at most 2/sup -R/(/spl epsiv//). If /spl epsiv/>1-1/(k+1), our BPP simulations take time n/sup O(log(k/ n)) (log/sup (k/) is the logarithm iterated k times). We also give a polynomial-time BPP simulation using Chor-Goldreich sources of min-entropy R/sup /spl Omega/(1/), which is optimal. We present applications to time-space tradeoffs, expander constructions, and the hardness of approximation. Also of interest is our randomness-efficient Leftover Hash Lemma, found independently by Goldreich and Wigderson.<<ETX>>
[Computational modeling, Computer simulation, BPP simulations, probability, min-entropy, Mathematics, very weak random sources, Application software, time-space tradeoffs, RP algorithms simulation, Computer science, R-bit string, hardness, randomness-efficient Leftover Hash Lemma, Physics computing, expander constructions, Polynomials, Chor-Goldreich sources, Cryptography, Distributed algorithms, Testing, computational complexity]
Optimal evolutionary tree comparison by sparse dynamic programming
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
In computational biology one is often interested in finding the concensus between different evolutionary trees for the same set of species. A popular formalizations is the Maximum Agreement Subtree Problem (MAST) defined as follows: given a set A and two rooted trees /spl Tscr//sub 0/ and /spl Tscr//sub 1/ leaf-labeled by the elements of A, find a maximum cardinality subset B of A such that the restrictions of /spl Tscr//sub 0/ and /spl Tscr//sub 1/ to B are topologically isomorphic. Polynomial time solutions exist, but they rely on a dynamic program with /spl Theta/(n/sup 2/) nodes-and /spl Theta/(n/sup 2/) running time. We sparsify this dynamic program and show that MAST is equivalent to Unary Weighted Bipartite Matching (UWBM) modulo an O(nc/sup /spl radic/(log n/) additive overhead. Applying the best bound for UWBM, we get an O(n/sup 1.5/ log n) algorithm for MAST. From our sparsification follows an O(nc/sup /spl radic/(log n/)) time algorithm for the special case of bounded degrees. Also here the best previous bound was /spl Theta/(n/sup 2/).<<ETX>>
[computational biology, Additives, programming theory, evolutionary tree comparison, evolution (biological), trees (mathematics), dynamic programming, Phylogeny, History, Computer science, Maximum Agreement Subtree Problem, dynamic program, Evolution (biology), biology computing, Unary Weighted Bipartite Matching, Genetic programming, Polynomials, Dynamic programming, tree data structures, sparsification follows, sparse dynamic programming, Contracts, Computational biology]
On the power of quantum computation
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
The quantum model of computation is a probabilistic model, similar to the probabilistic Turing Machine, in which the laws of chance are those obeyed by particles on a quantum mechanical scale, rather than the rules familiar to us from the macroscopic world. We present here a problem of distinguishing between two fairly natural classes of function, which can provably be solved exponentially faster in the quantum model than in the classical probabilistic one, when the function is given as an oracle drawn equiprobably from the uniform distribution on either class. We thus offer compelling evidence that the quantum model may have significantly more complexity theoretic power than the probabilistic Turing Machine. In fact, drawing on this work, Shor (1994) has recently developed remarkable new quantum polynomial-time algorithms for the discrete logarithm and integer factoring problems.<<ETX>>
[complexity theory, Computational modeling, Computer simulation, quantum model of computation, integer factoring, probabilistic model, Quantum computing, probabilistic automata, Turing machines, Physics computing, probabilistic Turing Machine, Quantum mechanics, Computer errors, Lakes, discrete logarithm, Polynomials, Relativistic quantum mechanics, quantum computation, computational complexity]
Approximate graph coloring by semidefinite programming
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
We consider the problem of coloring k-colorable graphs with the fewest possible colors. We give a randomized polynomial time algorithm which colors a 3-colorable graph on n vertices with min {O(/spl Delta//sup 1/3/log/sup 4/3//spl Delta/), O(n/sup 1/4/ log n)} colors where /spl Delta/ is the maximum degree of any vertex. Besides giving the best known approximation ratio in terms of n, this marks the first non-trivial approximation result as a function of the maximum degree /spl Delta/. This result can be generalized to k-colorable graphs to obtain a coloring using min {O/spl tilde/(/spl Delta//sup 1-2/k/), O/spl tilde/(n/sup 1-3/(k+1/))} colors. Our results are inspired by the recent work of Goemans and Williamson who used an algorithm for semidefinite optimization problems, which generalize linear programs, to obtain improved approximations for the MAX CUT and MAX 2-SAT problems. An intriguing outcome of our work is a duality relationship established between the value of the optimum solution to our semidefinite program and the Lovasz /spl thetav/-function. We show lower bounds on the gap between the optimum solution of our semidefinite program and the actual chromatic number; by duality this also demonstrates interesting new facts about the /spl thetav/-function.<<ETX>>
[Algorithm design and analysis, Greedy algorithms, Law, vertices, duality relationship, computational geometry, linear programming, graph colouring, semidefinite programming, optimisation, approximate graph coloring, MAX CUT, semidefinite optimization problems, duality (mathematics), Polynomials, k-colorable graphs, approximation ratio, linear programs, randomized polynomial time algorithm, chromatic number, Scheduling, lower bounds, randomised algorithms, MAX 2-SAT problems, Computer science, Approximation algorithms, Legal factors]
(De)randomized construction of small sample spaces in /spl Nscr//spl Cscr/
Proceedings 35th Annual Symposium on Foundations of Computer Science
None
1994
D. Koller and N. Megiddo (1993) introduced the paradigm of constructing compact distributions that satisfy a given set of constraints, and showed how it can be used to efficiently derandomize certain types of algorithm. In this paper, we significantly extend their results in two ways. First, we show how their approach can be applied to deal with more general expectation constraints. More importantly, we provide the first parallel (/spl Nscr//spl Cscr/) algorithm for constructing a compact distribution that satisfies the constraints up to a small relative error. This algorithm deals with constraints over any event that can be verified by finite automata, including all independence constraints as well as constraints over events relating to the parity or sum of a certain set of variables. Our construction relies on a new and independently interesting parallel algorithm for converting a solution to a linear system into an almost basic approximate solution to the same system. We use these techniques in the first /spl Nscr//spl Cscr/ derandomization of an algorithm for constructing large independent sets in d-uniform hypergraphs for arbitrary d. We also show how the linear programming perspective suggests new proof techniques which might be useful in general probabilistic analysis.<<ETX>>
[Linear systems, Automatic frequency control, parallel algorithms, finite automata, derandomized construction, probability, parallel algorithm, d-uniform hypergraphs, Linear programming, Probability distribution, linear programming, compact distributions, Parallel algorithms, randomised algorithms, Computer science, Automata, Polynomials, constraint handling, small sample spaces, Contracts, general probabilistic analysis]
Lower bounds for monotone span programs
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Span programs provide a linear algebraic model of computation. Lower Bounds for span programs imply lower bounds for formula size, symmetric branching programs and for contact schemes. Monotone span programs correspond also to linear secret-sharing schemes. We present a technique for proving lower bounds for monotone span programs, and prove a lower bound of &#x003A9;(m/sup 2.5/) for the 6-clique function. Our results improve on the previously known bounds for explicit functions.
[secret-sharing, programming theory, Computational modeling, linear algebraic model, Binary decision diagrams, span programs, cryptography, monotone span programs, formula size, lower bounds, symmetric branching, computation theory, Polynomials, Cryptography]
Routing on butterfly networks with random faults
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We show that even if every node or edge in an N-node butterfly network fails independently with some constant probability, p, it is still possible to identify a set of /spl Theta/(N) nodes between which packets can be routed in any permutation in O(logN) steps, with high probability. Although the analysis as complicated, the routing algorithm itself is relatively simple.
[random faults, probability, packet switching, reliability, Routing, hypercube networks, Computer science, Concurrent computing, Fault detection, routing algorithm, National electric code, butterfly networks, Computer networks, fault tolerant computing, constant probability, Books, Contracts, packet routing]
Efficient access to optical bandwidth wavelength routing on directed fiber trees, rings, and trees of rings
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We address efficient access to bandwidth in WDM (wavelength division multiplexing) optical networks. We consider tree topologies, ring topologies, as well as trees of rings. These are topologies of concrete practical relevance for which undirected underlying graph models have been studied before by P. Raghavan and E. Upfal (1993). As opposed to previous studies (A. Aggarwal et al., 1993; R. Pankaj, 1992; P. Raghavan and E. Upfal, 1993), we consider directed graph models. Directedness of fiber links is dictated by physical directedness of optical amplifiers. For trees, we give a polynomial time routing algorithm that satisfies requests of maximum load L/sub max/ per fiber link using no more than 15L/sub max//8/spl les/15OPT/8 optical wavelengths. This improves a 2L/sub max/ scheme that is implicit by P. Raghavan and E. Upfal by extending their undirected methods to our directed model. Alternatively stated, for fixed W wavelength technology, we can load the network up to L,, 8W/15 rather than W/2. In engineering terms, this is a so called "6.66% increase of bandwidth" and it is considered substantial. For rings, the approximation factor is 2OPT. For trees of rings, the approximation factor is 15OPT/4. Technically, optical routing requirements give rise to novel coloring paradigms. Our algorithms involve matchings and multicolored alternating cycles, combined with detailed potential and averaging analysis.
[trees of rings, wavelength division multiplexing, optical routing requirements, directed fiber trees, Optical fiber networks, physical directedness, fiber links, Wavelength routing, graph colouring, fixed W wavelength technology, coloring paradigms, Semiconductor optical amplifiers, optical wavelengths, approximation factor, ring topologies, Tree graphs, Network topology, Bandwidth, optical communication, directed graph models, multicolored alternating cycles, averaging analysis, trees (mathematics), optical bandwidth wavelength routing, Wavelength division multiplexing, WDM optical networks, efficient access, Optical fiber amplifiers, Stimulated emission, directed graphs, polynomial time routing algorithm, optical amplifiers, Concrete, tree topologies]
An optimal algorithm for Monte Carlo estimation
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
A typical approach to estimate an unknown quantity /spl mu/ is to design an experiment that produces a random variable Z distributed in [O,1] with E[Z]=/spl mu/, run this experiment independently a number of times and use the average of the outcomes as the estimate. In this paper, we consider the case when no a priori information about Z is known except that is distributed in [0,1]. We describe an approximation algorithm AA which, given /spl epsiv/ and /spl delta/, when running independent experiments with respect to any Z, produces an estimate that is within a factor 1+/spl epsiv/ of /spl mu/ with probability at least 1-/spl delta/. We prove that the expected number of experiments ran by AA (which depends on Z) is optimal to within a constant factor for every Z.
[Algorithm design and analysis, parallel algorithms, Estimation theory, Probability, Design for experiments, Silicon compounds, Computer science, Monte Carlo estimation, Monte Carlo methods, Upper bound, a priori information, approximation algorithm, Approximation algorithms, Random variables, optimal algorithm]
Minimum coloring random and semi-random graphs in polynomial expected time
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We present new algorithms for k-coloring and minimum (/spl chi/(G)-) coloring random and semi-random k-colorable graphs in polynomial expected time. The random graphs are drawn from the G(n,p,k) model and the semi-random graphs are drawn from the G/sub SB/(n,p,k) model. In both models, an adversary initially splits the n vertices into k color classes, each of size /spl Theta/(n). Then the edges between vertices in different color classes are chosen one by one, according to some probability distribution. The model G/sub SB/(n,p,k) was introduced by A. Blum (1991) and with respect to randomness, it lies between the random model G(n,p,k) where all edges are chosen with equal probability and the worst-case model.
[Algorithm design and analysis, Greedy algorithms, Automation, Design methodology, graph theory, vertices, worst-case model, Probability distribution, Partitioning algorithms, graph colouring, minimum coloring random graphs, semi-random graphs, Computer science, polynomial expected time, Approximation algorithms, Polynomials, Integrated circuit modeling, computational complexity, k-coloring]
Disjoint paths in densely embedded graphs
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We consider the following maximum disjoint paths problem (MDPP). We are given a large network, and pairs of nodes that wish to communicate over paths through the network-the goal is to simultaneously connect as many of these pairs as possible in such a way that no two communication paths share an edge in the network. This classical problem has been brought into focus recently in papers discussing applications to routing in high-speed networks, where the current lack of understanding of the MDPP is an obstacle to the design of practical heuristics. We consider the class of densely embedded, nearly-Eulerian graphs, which includes the two-dimensional mesh and other planar and locally planar interconnection networks. We obtain a constant-factor approximation algorithm for the maximum disjoint paths problem for this class of graphs; this improves on an O(log n)-approximation for the special case of the two-dimensional mesh due to Aumann-Rabani and the authors. For networks that are not explicitly required to be "high-capacity," this is the first constant-factor approximation for the MDPP in any class of graphs other than trees. We also consider the MDPP in the on-line setting, relevant to applications in which connection requests arrive over time and must be processed immediately. Here we obtain an asymptptically optimal O(log n)competitive on-line algorithm for the same class of graphs; this improves on an O(log n log log n) competitive algorithm for the special case of the mesh due to B. Awerbuch et al (1994).
[Operations research, Multiprocessor interconnection networks, multiprocessor interconnection networks, Optical fiber networks, computational geometry, high-speed networks, Intelligent networks, routing, High-speed networks, Tree graphs, heuristics, densely embedded graphs, disjoint paths, on-line setting, nearly-Eulerian graphs, constant-factor approximation algorithm, trees (mathematics), two-dimensional mesh, Routing, locally planar interconnection networks, NP-complete problem, operations research, maximum disjoint paths, communication paths, Approximation algorithms, High speed optical techniques]
Hard-core distributions for somewhat hard problems
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Consider a decision problem that cannot be 1-/spl delta/ approximated by circuits of a given size in the sense that any such circuit fails to give the correct answer on at least a /spl delta/ fraction of instances. We show that for any such problem there is a specific "hard core" set of inputs which is at least a /spl delta/ fraction of all inputs and on which no circuit of a slightly smaller size can get even a small advantage over a random guess. More generally, our argument holds for any non uniform model of computation closed under majorities. We apply this result to get a new proof of the Yao XOR lemma (A.C. Yao, 1982), and to get a related XOR lemma for inputs that are only k wise independent.
[decision problem, hard problems, decision theory, Computational modeling, Circuits, probability, hard core distributions, Drives, Boolean function, Complexity theory, computational problem, Distributed computing, random guess, Computer science, Boolean functions, non uniform mode, Yao XOR lemma, Polynomials, k wise independent, hard-core distributions, computational complexity]
Approximating the volume of definable sets
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
The first part of this paper deals with finite-precision arithmetic. We give an upper bound on the precision that should be used in a Monte-Carlo integration method. Such bounds have been known only for convex sets; our bound applies to almost any "reasonable" set. In the second part of the paper, we show how to construct in polynomial time first-order formulas that approximately define the volume of definable sets. This result is based on a VC dimension hypothesis, and is inspired from the well-known complexity-theoretic result "BPP/spl sube//sub 2/". Finally, we show how these results can be applied to sets defined by systems of inequalities involving polynomial or exponential functions. In particular, we describe an application to a problem of structural complexity in the Blum-Shub-Smale model of computation over the reals.
[definable sets, Virtual colonoscopy, Computational modeling, convex sets, upper bound, polynomial time first-order formulas, Equations, finite-precision arithmetic, complexity-theoretic result, Blum-Shub-Smale model, Upper bound, Monte Carlo methods, structural complexity, Grid computing, Polynomials, VC dimension hypothesis, Random number generation, Monte-Carlo integration method, Arithmetic, computational complexity]
Faster approximate agreement with multi-writer registers
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We consider the complexity of the wait-free approximate agreement problem in an asynchronous shared memory comprised of only single-bit multi-writer multi-reader registers. For real-valued inputs x/sub 1/,...,x/sub n/ and /spl epsiv/ we show matching upper and lower bounds of /spl Theta/(log(ma.
[complexity, Uncertainty, wait-free approximate agreement problem, upper bounds, Phase change random access memory, Registers, Synchronization, Distributed computing, Scheduling algorithm, lower bounds, wait-free single-writer multi-reader, Computer science, single-bit multi-writer multi-reader registers, Processor scheduling, multi-writer registers, Message passing, distributed algorithms, asynchronous shared memory, wait-free multi-writer multi-reader, shared memory systems, shared registers, Clocks, computational complexity]
Computing simulations on finite and infinite graphs
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We present algorithms for computing similarity relations of labeled graphs. Similarity relations have applications for the refinement and verification of reactive systems. For finite graphs, we present an O(mn) algorithm for computing the similarity relation of a graph with n vertices and m edges (assuming m/spl ges/n). For effectively presented infinite graphs, we present a symbolic similarity-checking procedure that terminates if a finite similarity relation exists. We show that 2D rectangular automata, which model discrete reactive systems with continuous environments, define effectively presented infinite graphs with finite similarity relations. It follows that the refinement problem and the /spl forall/CTL* model-checking problem are decidable for 2D rectangular automata.
[Algorithm design and analysis, Engineering profession, Computational modeling, automata theory, graph theory, O(mn) algorithm, similarity relations, infinite graphs, State-space methods, continuous environments, Application software, System analysis and design, reactive systems verification, Computer science, labeled graphs, Analytical models, decidability, 2D rectangular automata, model-checking problem, Automata, simulations computing, finite graphs, symbolic similarity-checking procedure, Contracts]
Sparse P-hard sets yield space-efficient algorithms
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
J. Hartmanis (1978) conjectured that there exist no sparse complete sets for P under logspace many-one reductions. In this paper, in support of the conjecture, it is shown that if P has sparse hard sets under logspace many-one reductions, then P/spl sube/DSPACE[log/sup 2/n]. The result follows from a more general statement: if P has 2/sup polylog/ sparse hard sets under poly-logarithmic space-computable many-one reductions, then P/spl sube/DSPACE[polylog].
[Computer science, logspace many-one reductions, poly-logarithmic space-computable many-one reductions, sparse P-hard sets, Density functional theory, Polynomials, computational complexity, space-efficient algorithms]
Private information retrieval
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We describe schemes that enable a user to access k replicated copies of a database (k/spl ges/2) and privately retrieve information stored in the database. This means that each individual database gets no information on the identity of the item retrieved by the user. For a single database, achieving this type of privacy requires communicating the whole database, or n bits (where n is the number of bits in the database). Our schemes use the replication to gain substantial saving. In particular, we have: A two database scheme with communication complexity of O(n/sup 1/3/). A scheme for a constant number, k, of databases with communication complexity O(n/sup 1/k/). A scheme for 1/3 log/sub 2/ n databases with polylogarithmic (in n) communication complexity.
[replication, Data privacy, replicated databases, replicated copies, Distributed databases, information retrieval, Information retrieval, privacy, Complexity theory, communication complexity, Protection, database theory]
Gambling in a rigged casino: The adversarial multi-armed bandit problem
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
In the multi-armed bandit problem, a gambler must decide which arm of K non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines. In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the expected per-round payoff of our algorithm approaches that of the best arm at the rate O(T/sup -1/3/), and we give an improved rate of convergence when the best arm has fairly low payoff. We also consider a setting in which the player has a team of "experts" advising him on which arm to play; here, we give a strategy that will guarantee expected payoff close to that of the best expert. Finally, we apply our result to the problem of learning to play an unknown repeated matrix game against an all-powerful adversary.
[Costs, Stochastic processes, Process control, game theory, rate of convergence, Routing, well-behaved stochastic process, Statistics, slot machines, Convergence, matrix game, bandit problem, Machine learning, multi-armed bandit problem, Communication networks, Arm, stochastic games]
On one-dimensional quantum cellular automata
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Since Richard Feynman introduced the notion of quantum computation in 1982, various models of "quantum computers" have been proposed (R. Feynman, 1992). These models include quantum Turing machines and quantum circuits. We define another quantum computational model, one dimensional quantum cellular automata, and demonstrate that any quantum Turing machine can be efficiently simulated by a one dimensional quantum cellular automaton with constant slowdown. This can be accomplished by consideration of a restricted class of one dimensional quantum cellular automata called one dimensional partitioned quantum cellular automata. We also show that any one dimensional partitioned quantum cellular automaton can be simulated by a quantum Turing machine with linear slowdown, but the problem of efficiently simulating an arbitrary one dimensional quantum cellular automaton with a quantum Turing machine is left open. From this discussion, some interesting facts concerning these models are easily deduced.
[simulation, 1D partitioned quantum cellular automaton simulation, cellular automata, one-dimensional quantum cellular automata, constant slowdown, Quantum computing, Turing machines, Physics computing, physics, Polynomials, quantum computation, quantum Turing machine, Computational modeling, Circuit simulation, Computer simulation, quantum theory, Magnetic heads, quantum computers, one dimensional quantum cellular automata, one dimensional partitioned quantum cellular automata, physics computing, Quantum cellular automata, Quantum mechanics, quantum computational model, linear slowdown]
Reconstructing strings from substrings in rounds
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We establish a variety of combinatorial bounds on the tradeoffs inherent in reconstructing strings using few rounds of a given number of substring queries per round. These results lead us to propose a new approach to sequencing by hybridization (SBH), which uses interaction to dramatically reduce the number of oligonucleotides used for de novo sequencing of large DNA fragments, while preserving the parallelism which is the primary advantage of SBH.
[Sequences, Costs, pattern matching, combinatorial bounds, de novo sequencing, parallelism, string reconstruction, oligonucleotides, substrings, sequencing by hybridization, Chip scale packaging, Computer science, Upper bound, biology computing, DNA, Detectors, interaction, SBH, string matching, large DNA fragments, substring queries, rounds]
Efficient parallel solution of sparse eigenvalue and eigenvector problems
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
This paper gives a new algorithm for computing the characteristic polynomial of a symmetric sparse matrix. We derive an interesting algebraic version of nested dissection, which constructs a sparse factorization the matrix A-/spl lambda/ where A is the input matrix. While nested dissection is commonly used to minimize the fill-in in the solution of sparse linear systems, our innovation is to use the separator structure to bound also the work for manipulation of rational polynomials in the recursively factored matrices. We compute the characteristic polynomial sparse symmetric matrix in polylog time using O(n(n+P(s(n))))/spl les/O(n(n+s(n)/sup 2.376/)) processors, where the sparsity graph of the matrix has separator size s(n). Our method requires only that the matrix be symmetric and nonsingular (it need not be positive definite as usual for nested dissection techniques); we use perturbation methods to avoid singularities. For the frequently occurring case where the matrix has small separator size our polylog parallel algorithm requires work bounds competitive with the best known sequential algorithms (i.e. sparse Lanczos methods), for example: (1) when the sparsity graph is a planar graph, s(n)/spl les//spl radic/n, and we require only n/sup 2.188/ processors, and (2) in the case where the input matrix is b-banded, we require only O(nP(b))=O(n) processors, for constant b.
[Linear systems, Transmission line matrix methods, planar graph, Sparse matrices, parallel solution, Parallel algorithms, eigenvalues and eigenfunctions, characteristic polynomial, Perturbation methods, sparsity graph, Eigenvalues and eigenfunctions, Polynomials, Technological innovation, parallel algorithms, eigenvector problems, Symmetric matrices, Particle separators, symmetric sparse matrix, polylog parallel algorithm, matrix algebra, sparse eigenvalue, algebraic version, sparse Lanczos methods, work bounds, perturbation methods]
Tight fault locality
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
The notion of fault local mending was suggested as a paradigm for designing fault tolerant algorithms that scale to large networks. For such algorithms the complexity of recovering is proportional to the number of faults. We refine this notion by introducing the concept of tight fault locality to deal with problems whose complexity (in the absence of faults) is sublinear in the size of the network. For a function whose complexity on an n-node network is f(n), a tightly fault local algorithm recovers a legal global state in O(f(x)) time when the (unknown) number of faults is x. We illustrate this concept by presenting a general transformation for MIS algorithms to make them fault local. In particular, our transformation yields an O(logx) randomized mending algorithm and a 2/sup /spl radic//spl beta/logx/ deterministic mending algorithm for MIS. Similar results are obtained for other local functions such as a /spl Delta/+1 coloring. We also present the first tight fault local mending algorithm for global functions, using our results for MIS. This improves (by a logarithmic factor) the complexity of a previous fault-local mending algorithm for global functions.
[Algorithm design and analysis, Career development, complexity, Law, deterministic mending algorithm, Mathematics, system recovery, fault local mending, Computer science, Fault tolerance, global functions, Information processing, tightly fault local algorithm, Computer networks, fault tolerant computing, fault tolerant algorithms, IP networks, Legal factors, computational complexity]
Lower bounds on arithmetic circuits via partial derivatives
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We describe a new technique for obtaining lower bounds on restricted classes of non-monotone arithmetic circuits. The heart of this technique is a complexity measure for multivariate polynomials, based on the linear span of their partial derivatives. We use the technique to obtain new lower bounds for computing symmetric polynomials and iterated matrix products.
[Heart, Symmetric matrices, arithmetic circuits, Circuit simulation, polynomials, Analog computers, complexity measure, multivariate polynomials, lower bounds, Computer science, Upper bound, minimisation of switching nets, digital arithmetic, Digital arithmetic, Polynomials, logic circuits, partial derivatives, restricted classes, computational complexity]
3-coloring in time 0(1.3446/sup n/): a no-MIS algorithm
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We consider worst case time bounds for NP-complete problems including 3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a common generalization of these problems, called symbol-system satisfiability or, briefly, SSS. 3-SAT is equivalent to (2,3)-SSS while the other problems above are special cases of (3,2)-SSS; there is also a natural duality transformation from (a,b)-SSS to (b,a)-SSS. We give a fast algorithm for (3,2)-SSS and use it to improve the time bounds for solving the other problems listed above.
[3-edge-coloring, time bounds, computability, 3-SAT, Search problems, NP-complete problem, worst case time bounds, graph colouring, Computer science, 3-list-coloring, symbol-system satisfiability, decidability, common generalization, 3-coloring, NP-complete problems, duality (mathematics), Polynomials, duality transformation, Testing, computational complexity]
The loading time scheduling problem
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
In this paper we study precedence constrained scheduling problems, where the tasks can only be executed on a specified subset of the machines. Each machine has a loading time that is incurred only for the first task that is scheduled on the machine in a particular run. This basic scheduling problem arises in the context of machining on numerically controlled machines, query optimization in databases, and in other artificial intelligence applications. We give the first non-trivial approximation algorithm for this problem. We also prove non-trivial lower bounds on best possible approximation ratios for these problems. These improve on the non-approximability results that are implied by the non-approximability results for the shortest common supersequence problem. We use the same algorithmic technique to obtain approximation algorithms for a problem arising in the context of code generation for parallel machines, and for the weighted shortest common supersequence problem.
[Drilling, databases, weighted shortest common supersequence problem, Machining, Educational institutions, numerically controlled machines, parallel machines, Scheduling algorithm, artificial intelligence, shortest common supersequence problem, Milling machines, Computer science, algorithmic technique, Processor scheduling, Databases, query optimization, code generation, Query processing, precedence constrained scheduling problems, machining, loading time scheduling problem, scheduling, Approximation algorithms, constraint handling]
RSPACE(S)/spl sube/DSPACE(S/sup 3/2/)
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We prove that any language that can be recognized by a randomized algorithm (with possibly two-sided error) that runs in space S and expected time 2/sup 0(s)/ can be recognized by a deterministic algorithm running in space S/sup 3/2/. This improves over the best previously known result that such algorithms have deterministic space S/sup 2/ simulations which, for one-sided error algorithms, follows from Savitch's Theorem and for two-sided error algorithms follows by reduction to recursive matrix powering. Our result includes as a special case the result due to N. Nisan et al. (1992), that undirected connectivity can be computed in space log/sup 3/2/n. It is obtained via a new algorithm for repeated squaring of a matrix we show how to approximate the 2/sup /spl tau// power of a d/spl times/d matrix in space /spl tau//sup 1/2/ log d, improving on the bo und of /spl tau/ log d that comes from the natural recursive algorithm. The algorithm employs Nisan's pseudorandom generator for space bounded computation, together with some new techniques for reducing the number of random bits needed by an algorithm.
[Terminology, Stochastic processes, two-sided error algorithms, Mathematics, Probability distribution, recursive matrix powering, RSPACE(S), Turing machines, USA Councils, two-sided error, one-sided error algorithms, formal languages, pseudorandom generator, DSPACE(S/sup 3/2/), Computational modeling, probability, Extraterrestrial measurements, space bounded computation, Magnetic heads, randomized algorithm, deterministic algorithm, deterministic algorithms, natural recursive algorithm, randomised algorithms, Computer science]
Counting bottlenecks to show monotone P/spl ne/NP
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
The method of proving lower bounds by bottleneck counting is illustrated for monotone Boolean circuits. This paper gives another proof of the result of Razborov (1985) and Andreev (1985), that monotone Boolean circuits must have exponential size when solving a problem in NP. More specifically, the paper defines a graph recognition problem called BMS. Any monotone circuit that solves BMS, must contain a quantity of gates that is exponential in the eighth root of the input size. The actual instances of the BMS problem used to prove the lower bound are easy to separate for non-monotone circuits. The proof is self-contained and uses only elementary combinatorics.
[Circuit simulation, Lattices, BMS, bottleneck counting, Combinatorial mathematics, lower bounds, monotone Boolean circuits, Boolean functions, Turing machines, monotone circuit, logic circuits, graph recognition problem, computational complexity]
Tracking the best disjunction
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
N. Littlestone developed a simple deterministic on-line learning algorithm for learning k-literal disjunctions. This algorithm (called Winnow) keeps one weight for each of the n variables and does multiplicative updates to its weights. We develop a randomized version of Winnow and prove bounds for an adaptation of the algorithm for the case when the disjunction may change over time. In this case a possible target disjunction schedule T is a sequence of disjunctions (one per trial) and the shift size is the total number of literals that are added/removed from the disjunctions as one progresses through the sequence. We develop an algorithm that predicts nearly as well as the best disjunction schedule for an arbitrary sequence of examples. This algorithm that allows us to track the predictions of the best disjunction is hardly more complex than the original version. However the amortized analysis needed for obtaining worst-case mistake bounds requires new techniques. In some cases our lower bounds show that the upper bounds of our algorithm have the right constant in front of the leading term in the mistake bound and almost the right constant in front of the second leading term. By combining the tracking capability with existing applications of Winnow we are able to enhance these applications to the shifting case as well.
[Algorithm design and analysis, computational linguistics, worst-case mistake bounds, upper bounds, Winnow, Scheduling algorithm, randomised algorithms, Computer science, Upper bound, best disjunction tracking, deterministic on-line learning algorithm, Prediction algorithms, amortized analysis, learning (artificial intelligence), k-literal disjunctions]
Using autoreducibility to separate complexity classes
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
A language is autoreducible if it can be reduced to itself by a Turing machine that does not ask its own input to the oracle. We use autoreducibility to separate exponential space from doubly exponential space by showing that all Turing complete sets for exponential space are autoreducible but there exists some Turing complete set for doubly exponential space that is not. We immediately also get a separation of logarithmic space from polynomial space. Although we already know how to separate these classes using diagonalization, our proofs separate classes solely by showing they have different structural properties, thus applying Post's Program (E. Pos, 1944) to complexity theory. We feel such techniques may prove unknown separations in the future. In particular if we could settle the question as to whether all complete sets for doubly exponential time were autoreducible we would separate polynomial time from either logarithmic space or polynomial space. We also show several other theorems about autoreducibility.
[complexity theory, TV, autoreducibility, complexity classes, oracle, polynomial space, Turing machine, doubly exponential space, Complexity theory, set theory, Noise measurement, Computer science, Turing complete sets, structural properties, Turing machines, diagonalization, logarithmic space, Polynomials, Contracts, computational complexity]
Learning polynomials with queries: The highly noisy case
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Given a function f mappping n-variate inputs from a finite field F into F, we consider the task of reconstructing a list of all n-variate degree d polynomials which agree with f on a tiny but non-negligible fraction, /spl delta/, of the input space. We give a randomized algorithm for solving this task which accesses f as a black box and runs in time polynomial in 1//spl delta/, n and exponential in d, provided /spl delta/ is /spl Omega/(/spl radic/(d.
[Computer aided software engineering, polynomials, highly noisy case, Random processes, randomized algorithm, explanation, Galois fields, polynomials with queries learning, n-variate degree d polynomials, Read only memory, randomised algorithms, Computer science, running time, n-variate inputs, finite field, Polynomials, Argon, learning (artificial intelligence)]
Coding for computing
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
A sender communicates with a receiver who wishes to reliably evaluate a function of their combined data. We show that if only the sender can transmit, the number of bits required is a conditional entropy of a naturally defined graph. We also determine the number of bits needed when the communicators exchange two messages.
[conditional entropy, Entropy, encoding, communication complexity, Communication standards, Sorting, coding, Upper bound, Rate-distortion, Robustness, Random variables, naturally defined graph, Distortion measurement, Information theory]
Optimal on-line search and sublinear time update in string matching
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We study in a dynamic setting the problem of online searching for the occurrences of an arbitrary pattern string P[1,p] in an indexed text string T[1,n]. That is, we assume that the text T may be updated by inserting or deleting an arbitrary string Y[1,y]. Our main contribution is presenting the first dynamic algorithm that achieves optimal time, i.e. /spl Theta/(p+occ), to find the occ occurrences of P, and sublinear time per update, i.e. O(/spl radic/(n+y)), in the worst case. The required space is optimal /spl Theta/(n).
[pattern matching, sublinear time per update, optimal time, Encoding, Statistics, dynamic setting, word processing, arbitrary string, sublinear time update, Automata, arbitrary pattern string, optimal on-line search, dynamic algorithm, string matching, optimal online search, occ occurrences, indexed text string, online searching, search problems, computational complexity]
A unified analysis of paging and caching
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Paging (caching) is the problem of managing a two-level memory hierarchy in order to minimise the time required to process a sequence of memory accesses. In order to measure this quantity, we define the system parameter miss penalty to represent the extra time required to access slow memory. In the context of paging, miss penalty is large, so most previous studies of on-line paging have implicitly set miss penalty=/spl infin/ in order to simplify the model. We show that this seemingly insignificant simplification substantially alters the precision of derived results. Consequently, we reintroduce miss penalty to the paging problem and present a more accurate analysis of on-line paging (and caching). We validate using this more accurate model by deriving intuitively appealing results for the paging problem which cannot be derived using the simplified model.
[Algorithm design and analysis, paged storage, Costs, Terminology, miss penalty, unified analysis, Time measurement, cache storage, paging, caching, Counting circuits, Computer science, Memory management, Performance analysis, two-level memory hierarchy, Context modeling]
Tight bounds for a distributed selection game with applications to fixed-connection machines
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We define a distributed selection game that generalizes a selection problem considered by S.R. Kosaraju (1989). We offer a tight analysis of our distributed selection game, and show that the lower bound for this abstract communication game directly implies near-tight lower bounds for certain selection problems on fixed-connection machines. For example, we prove that any deterministic comparison-based selection algorithm on an (n/log n)-processor bounded-degree hypercubic machine requires /spl Omega/(log/sup 3/2/n) steps in the worst case. This lower bound implies a non-trivial separation between the power of bounded-degree hypercubic and expander-based machines. Furthermore, we show that the algorithm underlying our tight upper bound for the distributed selection game can be adapted to run in O((log/sup 3/2/n) (log log n)/sup 2/) steps on any (n/log n)-processor hypercubic machine.
[Algorithm design and analysis, fixed-connection machines, abstract communication game, Computational modeling, distributed selection game, Phase change random access memory, hypercube networks, lower bound, Application software, hypercubic machine, Sorting, Concurrent computing, Computer science, Upper bound, tight bounds, computational complexity]
Improved depth lower bounds for small distance connectivity
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We consider the problem of determining, given a graph G and specified nodes s and t, whether or not there is a path of at most k edges in G from s to t. We show that solving this problem on polynomial-size unbounded fan-in circuits, requires depth /spl Omega/(loglogk), improving on a depth lower bound of n(log*k) when k=log/sup O(1/) n. In addition we show that there is a constant c such that for k/spl les/logn, any depth d unbounded fan-in circuit for this problem requires size at least n/sup ck/spl epsiv/d/ where /spl epsiv//sub d/=/spl phi//sup -2d//3 and /spl phi/ is the golden mean. This latter result improves on an n/sup /spl Omega/(log(d+3/k)) bound where log/sup (i/) is the i-fold composition of log with itself. The key to our technique is a new form of switching lemma which combines some of the features of iteratively shortening terms due to Furst, Saxe, and Sipser (1981) and Ajtai (1983) with the kinds of switching lemma arguments introduced by Yao (1985), Hastad (1986), and Cai (1986) that have been the methods of choice for subsequent results.
[depth lower bounds, Computational modeling, Circuits, graph theory, H infinity control, Drives, switching lemma, depth lower bound, Computational complexity, graph, Computer science, Turing machines, small distance connectivity, Polynomials, Matrix converters, computational complexity]
Spectral methods for matrix rigidity with applications to size-depth tradeoffs and communication complexity
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
The rigidity of a matrix measures the number of entries that must be changed in order to reduce its rank below a certain value. The known lower bounds on the rigidity of explicit matrices are very weak. It is known that stronger lower bounds would have implications to complexity theory. We consider weaker forms of the rigidity problem over the complex numbers. Using spectral methods, we derive lower bounds on these variants. We then give two applications of such weaker forms. First, we show that our lower bound on a variant of rigidity implies lower bounds on size-depth tradeoffs for arithmetic circuits with bounded coefficients computing linear transformations. These bounds generalize a recent result of Nisan and Wigderson. The second application is conditional; we show that it would suffice to prove lower bounds on certain weaker forms of rigidity to conclude several separation results in communication complexity theory. Our results complement and strengthen a result of Razborov.
[complexity theory, arithmetic circuits, matrix rigidity, Binary decision diagrams, Size measurement, Complexity theory, explicit matrices, Application software, communication complexity, Galois fields, Computational complexity, lower bounds, matrix algebra, Computer science, size-depth tradeoffs, Linear circuits, Communication networks, Arithmetic]
Sublogarithmic searching without multiplications
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We show that a unit-cost RAM with word length w can maintain an ordered set of w-bit integers (or binary strings) under the operations search, insert, delete, nearest neighbour in O(/spl radic/(logn)) worst-case time and range queries in O(/spl radic/(logn)+size of output) worst-case time. The operations rely on AC/sup 0/ instructions only, thereby solving an open problem posed by Fredman and Willard. The data structure is simple. We also present a static data structure that can process a set of /spl Theta/O(logn) searches in O(lognloglogn) time.
[Tree data structures, sublogarithmic searching, Costs, data structure, Read-write memory, Binary search trees, Data structures, insert, worst-case time, Table lookup, delete, nearest neighbour, Sorting, Computer science, search, tree data structures, search problems, unit-cost RAM, computational complexity]
Integral geometry of higher-dimensional polytopes and the average case in combinatorial optimization
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We consider the average case behavior of a linear optimization problem on various series of combinatorially interesting polytopes. From general results of integral geometry it follows that for all but an asymptotically negligible fraction of linear functions a polytope can be replaced by a pair of concentric balls with asymptotically equal radii so that the optimal value of the linear function on the polytope is in the interval between the optimal values of the linear function on these balls. In particular, we show that the average case behavior of the assignment problem, traveling salesman problem, and, generally speaking, of any optimization problem on a polynomial fraction of all permutations is the same.
[Computer aided software engineering, traveling salesman problem, Traveling salesman problems, computational geometry, higher-dimensional polytopes, assignment problem, Linear programming, Mathematics, Geometry, travelling salesman problems, combinatorial optimization, integral geometry, Ear, Polynomials, linear functions, Space exploration, average case behavior, linear optimization problem]
Linearity testing in characteristic two
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Let Dist(f,g)=Pr/sub u/ [f(u)/spl ne/g(u)] denote the relative distance between functions f,g mapping from a group G to a group H, and let Dist(f) denote the minimum, over all linear functions (homomorphisms) g, of Dist(f,g). Given a function f:G/spl rarr/H we let Err(f)=Pr/sub u/,v[f(u)+f(v)/spl ne/f(u+v)] denote the rejection probability of the BLR (Blum-Luby-Rubinfeld) linearity test. Linearity testing is the study of the relationship between Err(f) and Dist(f), and in particular the study of lower bounds on Err(f) in terms of Dist(f). The case we are interested in is when the underlying groups are G=GF(2)/sup n/ and H=GF(2). The corresponding test is used in the construction of efficient PCPs and thence in the derivation of hardness of approximation results, and, in this context, improved analyses translate into better non-approximability results. However, while several analyses of the relation of Err(f) to Dist(f) are known, none is tight. We present a description of the relationship between Err(f) and Dist(f) which is nearly complete in all its aspects, and entirely complete (i.e. tight) in some. In particular we present functions L,U:[0,1]/spl rarr/[0,1] such that for all x/spl isin/[0,1] we have L(x)<Err(f)/spl les/U(x) whenever Dist(f)=x, with the upper bound being tight on the whole range, and the lower bound tight on a large part of the range and close on the rest. Part of our strengthening is obtained by showing a new connection between the linearity testing problem and Fourier analysis, a connection which may be of independent interest. Our results are used by M. Bellare et al. (1995) to present the best known hardness results for Max3SAT and other MaxSNP problems.
[homomorphisms, characteristic two, relative distance, Scholarships, MaxSNP problems, probability, rejection probability, Drives, Fourier analysis, linearity testing, upper bound, Mathematics, lower bound, lower bounds, Max3SAT, Postal services, Computer science, Upper bound, Linearity, linear functions, Error correction codes, theorem proving, Testing]
Fault diagnosis in a flash
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Consider a set of n processors that can communicate with each other. Assume that each processor can be either "good" or "faulty". Also assume that the processors can test each other. We consider how to use parallel testing rounds to identify the faulty processors, given an upper bound t on their number. We prove that 4 rounds are necessary and sufficient when 2/spl radic/(2n)/spl les/0.03n (for n sufficiently large). Furthermore, at least 5 rounds are necessary when t/spl ges/0.49n (for n sufficiently large), and 10 rounds are sufficient when t<0.5n (for all n). (It is well known that no general solution is possible when t/spl ges/0.5n).
[Performance evaluation, parallel algorithms, Protocols, fault diagnosis, computer testing, reliability, faulty processors, built-in self test, Fault diagnosis, Computer science, Fault tolerance, Upper bound, Computed tomography, parallel testing rounds, fault tolerant computing, deterministic preprocessing, Robots, Testing, Accidents, randomised preprocessing]
Derandomizing semidefinite programming based approximation algorithms
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Remarkable breakthroughs have been made recently in obtaining approximate solutions to some fundamental NP-Complete problems, namely Max-Cut, Max k-Cut, Max-Sat, Max-Dicut, Max-Bisection, k Vertex Coloring, Independent Set, etc. These breakthroughs all involve polynomial time randomized algorithms based upon semidefinite programming, a technique pioneered by M. Goemans and D. Williamson (1994). In this paper, we give techniques to derandomize the above class of randomized algorithms, thus obtaining polynomial time deterministic algorithms with the same approximation ratios for the above problems. Note that Goemans and Williamson also gave an elegant method to derandomize their Max-Cut algorithm. We show here that their technique has a fatal flaw. The techniques we subsequently develop are very different from theirs. At the heart of our technique is the use of spherical symmetry to convert a nested sequence of n integrations, which cannot be approximated sufficiently well in polynomial time, to a nested sequence of just a constant number of integrations, which can be approximated sufficiently well in polynomial time.
[Heart, programming theory, Max k-Cut, Plasma welding, Max-Sat, NP-complete problem, deterministic algorithms, Organizing, randomised algorithms, Max-Cut, semidefinite programming, Max-Bisection, randomized algorithms, Max-Dicut, k Vertex Coloring, Independent Set, polynomial time randomized algorithms, Approximation algorithms, Polynomials, semidefinite programming based approximation algorithms, NP-Complete problems, computational complexity, polynomial time deterministic algorithms]
Improved algorithms and analysis for secretary problems and generalizations
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
In the classical secretary problem, n objects from an ordered set arrive in random order, and one has to accept k of them so that the final decision about each object is made only on the basis of its rank relative to the ones already seen. Variants of the problem depend on the goal: either maximize the probability of accepting the best k objects, or minimize the expectation of the sum of the ranks (or powers of ranks) of the accepted objects. The problem and its generalizations are at the core of tasks with a large data set, in which it may be impractical to backtrack and select previous choices. Optimal algorithms for the special case of k=1 are well known. Partial solutions for the first variant with general k are also known. In contrast, an explicit solution for the second variant with general k has not been known; even the question of whether or not the expected sum of powers of the ranks of selected items tends to infinity with n has been unresolved. We answer these open questions by obtaining explicit algorithms. For each z/spl ges/1, the resulting expected sum of the zth powers of the ranks of the selected objects is at most k/sup z+1//(z+1)+C(z)/spl middot/k/sup z+0.5/log k, whereas the best possible value at all is k/sup z+1//(z+1)+O(k/sup z/). Our methods are very intuitive and apply to some generalizations. We also derive a lower bound on the trade-off between the probability of selecting the best object and its expected rank.
[Algorithm design and analysis, expected rank, probability, H infinity control, optimal algorithms, lower bound, Postal services, Computer science, random order, generalizations, operations research, zth powers, large data set, secretary problems, Contracts, expected sum]
Divide-and-conquer approximation algorithms via spreading metrics
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We present a novel divide-and-conquer paradigm for approximating NP-hard graph optimization problems. The paradigm models graph optimization problems that satisfy two properties: First, a divide-and-conquer approach is applicable. Second, a fractional spreading metric is computable in polynomial time. The spreading metric assigns fractional lengths to either edges or vertices of the input graph, such that all subgraphs on which the optimisation problem is non-trivial have large diameters. In addition, the spreading metric provides a lower bound, /spl tau/, on the cost of solving the optimization problem. We present a polynomial time approximation algorithm for problems modelled by our paradigm whose approximation factor is O (mi.
[graph optimization problems, divide and conquer methods, vertices, Optimization methods, NP-hard graph optimization problems, storage-time product, optimisation problem, spreading metrics, optimisation, Feedback, National electric code, Cost function, Polynomials, polynomial time, polynomial time approximation algorithm, Particle separators, fractional lengths, spreading metric, interval graph completion, directed graphs, Approximation algorithms, fractional spreading metric, multicuts, divide-and-conquer approximation algorithms, computational complexity]
Resolving message complexity of Byzantine Agreement and beyond
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Byzantine Agreement among processors is a basic primitive in distributed computing. It comes in a number of basic fault models: "Crash\
[Protocols, reliability, workstation network, History, communication complexity, Distributed computing, distributed computing, Malicious, message complexity, optimisation, fault models, early stopping agreement protocols, Parallel processing, Broadcasting, Workstations, message passing, Crash, agreement, Omission, Computer crashes, Partitioning algorithms, Computer science, Byzantine Agreement, Message passing, distributed algorithms, fault tolerant computing, distributed work performance, adaptive parallelism, linear time]
The resolution of a Hartmanis conjecture
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Building on the recent breakthrough by M. Ogihara (1995), we resolve a conjecture made by J. Hartmanis (1978) regarding the (non) existence of sparse sets complete for P under logspace many-one reductions. We show that if there exists a sparse hard set for P under logspace many-one reductions, then P=LOGSPACE. We further prove that if P has a sparse hard set under many-one reductions computable in NC/sup 1/, then P collapses to NC/sup 1/.
[Computer science, logspace many-one reductions, sparse sets, sparse hard set, Circuits, Polynomials, Hartmanis conjecture, Complexity theory, NP-complete problem, computational complexity]
Efficient algorithms for learning to play repeated games against computationally bounded adversaries
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We examine the problem of learning to play various games optimally against resource-bounded adversaries, with an explicit emphasis on the computational efficiency of the learning algorithm. We are especially interested in providing efficient algorithms for games other than penny-matching (in which payoff is received for matching the adversary's action in the current round), and for adversaries other than the classically studied finite automata. In particular, we examine games and adversaries for which the learning algorithm's past actions may strongly affect the adversary's future willingness to "cooperate" (that is, permit high payoff), and therefore require carefully planned actions on the part of the learning algorithm. For example, in the game we call contract, both sides play O or 1 on each round, but our side receives payoff only if we play 1 in synchrony with the adversary; unlike penny-matching, playing O in synchrony with the adversary pays nothing. The name of the game is derived from the example of signing a contract, which becomes valid only if both parties sign (play 1).
[learning algorithm, penny-matching, finite automata, computationally bounded adversaries, Learning automata, game theory, Minimax techniques, repeated games playing, computational efficiency, Time measurement, Game theory, Computer science, classically studied finite automata, Particle measurements, Computational efficiency, learning (artificial intelligence), Contracts]
Pseudorandom generators, measure theory, and natural proofs
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We prove that if strong pseudorandom number generators exist, then the class of languages that have polynomial-sized circuits (P/poly) is not measurable within exponential time, in terms of the resource-bounded measure theory of Lutz. We prove our result by showing that if P/poly has measure zero in exponential time, then there is a natural proof against P/poly, in the terminology of Razborov and Rudich (1994). We also provide a partial converse of this result.
[formal languages, partial converse, Terminology, Circuits, Natural languages, Area measurement, resource-bounded measure theory, measure theory, Size measurement, Time measurement, Complexity theory, random number generation, Computer science, Turing machines, Polynomials, theorem proving, natural proofs, computational complexity, pseudorandom number generators]
Approximability of flow shop scheduling
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Shop scheduling problems are notorious for their intractability, both in theory and practice. In this paper, we demonstrate the existence of a polynomial approximation scheme for the flow shop scheduling problem with an arbitrary fixed number of machines. For the three common shop models (open, flow, and job), this result is the only known approximation scheme. Since none of the three models can be approximated arbitrarily closely in the general case (unless P=NP), the result demonstrates the approximability gap between the models in which the number of machines is fixed, and those in which it is part of the input of the instance. The result can be extended to flow shops with job release dates and delivery times and to flow shops with a fixed number stages, where the number of machines at any stage is fixed. We also describe a related polynomial approximation scheme for the problem of scheduling an open shop with a single bottleneck machine and an arbitrary number of non-bottleneck machines.
[shop models, Job shop scheduling, resource allocation, single bottleneck machine, scheduling, Approximation algorithms, flow shop scheduling, Polynomials, Scheduling algorithm, polynomial approximation scheme, open shop]
Simple learning algorithms for decision trees and multivariate polynomials
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
In this paper we develop a new approach for learning decision trees and multivariate polynomials via interpolation of multivariate polynomials. This new approach yields simple learning algorithms for multivariate polynomials and decision trees over finite fields under any constant bounded product distribution. The output hypothesis is a (single) multivariate polynomial that is an /spl epsiv/-approximation of the target under any constant bounded product distribution. The new approach demonstrates the learnability of many classes under any constant bounded product distribution and using membership queries, such as j-disjoint DNF and multivariate polynomial with bounded degree over any field. The technique shows how to interpolate multivariate polynomials with bounded term size from membership queries only. This in particular gives a learning algorithm for O(log n)-depth decision tree from membership queries only and a new learning algorithm of any multivariate polynomial over sufficiently large fields from membership queries only. We show that our results for learning from membership queries only are the best possible.
[j-disjoint DNF, Fourier transforms, decision theory, polynomials, learning algorithms, Lattices, bounded term size, learnability, multivariate polynomials, constant bounded product distribution, Galois fields, Computer science, membership queries, finite fields, interpolation, bounded degree, decision trees, /spl epsiv/-approximation, Polynomials, Decision trees, learning (artificial intelligence)]
Linear time erasure codes with nearly optimal recovery
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
An (n,c,l,r) erasure code consists of an encoding algorithm and a decoding algorithm with the following properties. The encoding algorithm produces a set of l-bit packets of total length cn from an n-bit message. The decoding algorithm is able to recover the message from any set of packets whose total length is r, i.e., from any set of r/l packets. We describe erasure codes where both the encoding and decoding algorithms run in linear time and where r is only slightly larger than n.
[error correction codes, linear codes, probability, packet switching, nearly optimal recovery, Encoding, Mathematics, Decoding, Application software, encoding, encoding algorithm, decoding, Computer science, Multicast algorithms, Communication system traffic control, Error correction codes, linear time erasure codes, IP networks, Protection, erasure code, computational complexity, decoding algorithm]
Application-controlled paging for a shared cache
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We consider a cache shared by several concurrently running application processes and propose a provably efficient application-controlled global strategy for the shared cache. Using future information implicitly in the form of good decisions by application processes, we are able to break through the H/sub k/ lower bound on competitive ratio proved for classical paging for a k-sized cache in [FKL/sup +/91]. For a size-k cache shared by P application processes that always make good cache replacement decisions, we develop an online application-controlled paging algorithm with and competitive ratio of 2H/sub P-1/+2 Typically, P is much smaller than k, perhaps by several orders of magnitude. Our competitive ratio improves upon the 2P+2 competitive ratio achieved by [CFL94a]. We show for this problem that no on-line algorithm A can have a competitive ratio better than H/sub P-1/ even if the application processes aiding A have perfect knowledge of individual request sequences. Our results are with respect to a worst-case interleaving of the individual request sequences of the P applications. We introduce a notion of fairness in the more realistic situation when application processes do not always make good cache replacement decisions. We show that our algorithm ensures that no application process needs to evict one of its cached pages to service some page fault caused by a mistake of some other application. Our algorithm is not only fair, but remains efficient; the global paging performance can be bounded in terms of the number of mistakes that application processes make.
[paged storage, application-controlled paging, shared cache, worst-case interleaving, cache storage, application processes, Application software, Delay, Paging strategies, Computer science, Concurrent computing, page fault, global paging performance, Aging, Interleaved codes, Time sharing computer systems, shared memory systems]
Fully dynamic biconnectivity and transitive closure
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
This paper presents an algorithm for the fully dynamic biconnectivity problem whose running time is exponentially faster than all previously known solutions. It is the first dynamic algorithm that answers biconnectivity queries in time O(log/sup 2/n) in a n-node graph and can be updated after an edge insertion or deletion in polylogarithmic time. Our algorithm is a Las-Vegas style randomized algorithm with the update time amortized update time O(log/sup 4/n). Only recently the best deterministic result for this problem was improved to O(/spl radic/nlog/sup 2/n). We also give the first fully dynamic and a novel deletions-only transitive closure (i.e. directed connectivity) algorithms. These are randomized Monte Carlo algorithms. Let n be the number of nodes in the graph and let m/spl circ/ be the average number of edges in the graph during the whole update sequence: The fully dynamic algorithms achieve (1) query time O(n/logn) and update time O(m/spl circ//spl radic/nlog/sup 2/n+n); or (2) query time O(n/logn) and update time O(nm/spl circ//sup /spl mu/-1/)log/sup 2/n=O(nm/spl circ//sup 0.58/log/sup 2/n), where /spl mu/ is the exponent for boolean matrix multiplication (currently /spl mu/=2.38). The deletions-only algorithm answers queries in time O(n/logn). Its amortized update time is O(nlog/sup 2/n).
[Algorithm design and analysis, System testing, Engineering profession, Heuristic algorithms, graph theory, n-node graph, edge insertion, fully dynamic biconnectivity, randomized algorithm, randomised algorithms, Computer science, Monte Carlo methods, Las-Vegas style, deletion, algorithm theory, Approximation algorithms, dynamic algorithm, polylogarithmic time, transitive closure, computational complexity]
Finding points on curves over finite fields
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We solve two computational problems concerning plane algebraic curves over finite fields: generating an (approximately) uniform random point, and finding all points deterministically in amortized polynomial time (over a prime field, for non-exceptional curves).
[computational geometry, plane algebraic curves, Galois fields, computational problems, finite fields, curves, Elliptic curves, Approximation algorithms, Polynomials, uniform random point, Australia, Power generation, amortized polynomial time, Testing]
Transforming men into mice (polynomial algorithm for genomic distance problem)
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Many people believe that transformations of humans into mice happen only in fairy tales. However, despite some differences in appearance and habits, men and mice are genetically very similar. In the pioneering paper, J.H. Nadeau and B.A. Taylor (1984) estimated that surprisingly few genomic rearrangements (178/spl plusmn/39) happened since the divergence of human and mouse 80 million years ago. However, their analysis is nonconstructive and no rearrangement scenario for human-mouse evolution has been suggested yet. The problem is complicated by the fact that rearrangements in multi chromosomal genomes include inversions, translocations, fusions and fissions of chromosomes, a rather complex set of operations. As a result, at first glance, a polynomial algorithm for the genomic distance problem with all these operations looks almost as improbable as the transformation of a (real) man into a (real) mouse. We prove a duality theorem which expresses the genomic distance in terms of easily computable parameters reflecting different combinatorial properties of sets of strings. This theorem leads to a polynomial time algorithm for computing most parsimonious rearrangement scenarios. Based on this result and the latest comparative physical mapping data we have constructed a scenario of human-mouse evolution with 131 reversals/translocaitons/fusions/fissions. A combination of the genome rearrangement algorithm with the recently proposed experimental technique called ZOO FISH suggests a new constructive approach to the 100 year old problem of reconstructing mammalian evolution.
[Chaos, pattern matching, computable parameters, duality theorem, Genomics, Humans, polynomial algorithm, set theory, Biological cells, genetics, biology computing, strings, comparative physical mapping data, ZOO FISH, sorting, duality (mathematics), Polynomials, Bioinformatics, genomic distance problem, evolution (biological), genomic rearrangements, genome rearrangement algorithm, combinatorial properties, human-mouse evolution, polynomial time algorithm, parsimonious rearrangement scenarios, Computer science, multi chromosomal genomes, mammalian evolution, Ear, Mice, string matching, Power engineering and energy]
Synthesizers and their application to the parallel construction of pseudo-random functions
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We present a new cryptographic primitive called pseudo-random synthesizer and show how to use it in order to get a parallel construction of a pseudo-random function. We show an NC/sup 1/ implementation of pseudo-random synthesizers based on the RSA or the Diffie-Hellman assumptions. This yields the first parallel (NC/sup 2/) pseudo-random function and the only alternative to the original construction of Goldreich, Gold-wasser and Micali (GGM). The security of our constructions is similar to the security of the underling assumptions. We discuss the connection with problems in computational learning theory.
[Modular construction, RSA, Diffie-Hellman assumptions, Engineering profession, Synthesizers, computational linguistics, pseudo-random functions, cryptography, computational learning theory, Mathematics, parallel construction, Security, Delay, cryptographic primitive, pseudo-random synthesizer, Writing, Polynomials, Nuclear magnetic resonance, Cryptography]
Controllability, recognizability, and complexity issues in robot motion planning
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Path planning has been widely studied by computer scientists. However, it is a very simplified version of the motion planning problems occurring in robotics. This paper examines extensions yielding two important issues: controllability and recognizability. The controllability issue arises when the number of controls is smaller than the number of independent parameters defining the robot's configuration: Can the motions span the configuration space? The recognizability issue occurs when there are errors in control and sensing: Can the robot recognize goal achievement? Both issues have interesting impact on the computational complexity of motion planning. This paper will also discuss a new path planning scheme based on random sampling of configuration space, to deal with many-degree-of-freedom robots. The blend of controllability, recognizability, and complexity issues discussed in this paper is unique to robotics and its study is key to the development of autonomous robots.
[autonomous robots, random sampling, Path planning, path planning, mobile robots, Computational complexity, robot motion planning, Orbital robotics, Robot motion, Motion planning, Computer science, controllability, complexity issues, Controllability, Robot sensing systems, Error correction, Motion control, recognizability, many-degree-of-freedom robots, computational complexity]
A scheduling model for reduced CPU energy
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
The energy usage of computer systems is becoming an important consideration, especially for battery-operated systems. Various methods for reducing energy consumption have been investigated, both at the circuit level and at the operating systems level. In this paper, we propose a simple model of job scheduling aimed at capturing some key aspects of energy minimization. In this model, each job is to be executed between its arrival time and deadline by a single processor with variable speed, under the assumption that energy usage per unit time, P, is a convex function, of the processor speed s. We give an off-line algorithm that computes, for any set of jobs, a minimum-energy schedule. We then consider some on-line algorithms and their competitive performance for the power function P(s)=s/sup p/ where p/spl ges/2. It is shown that one natural heuristic, called the Average Rate heuristic, uses at most a constant times the minimum energy required. The analysis involves bounding the largest eigenvalue in matrices of a special type.
[Energy consumption, Portable computers, Circuits, scheduling model, on-line algorithms, minimum-energy schedule, power function, competitive performance, Scheduling algorithm, power consumption, reduced CPU energy, computer power supplies, Computer displays, Processor scheduling, energy usage, Operating systems, Energy conservation, scheduling, Central Processing Unit, Personal digital assistants, job scheduling]
A representation of cuts within 6/5 times the edge connectivity with applications
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Let G be an undirected c-edge connected graph. In this paper we give an O(n/sup 2/)-sized planar geometric representation for all edge cuts with capacity less than 6/5c. The representation can be very efficiently built, by using a single run of the Karger-Stein algorithm for finding near-mincuts. We demonstrate that the representation provides an efficient query structure for near-mincuts, as well as a new proof technique through geometric arguments. We show that in algorithms based on edge splitting, computing our representation O(log n) times substitute for one, or sometimes even /spl Omega/(n), u-/spl nu/ mincut computations; this can lead to significant savings, since our representation can be computed /spl theta//spl tilde/(m/n) times faster than the currently best known u-/spl nu/ mincut algorithm. We also improve the running time of the edge augmentation problem, provided the initial edge weights are polynomially bounded.
[Terminology, computational geometry, Data structures, Mathematics, edge augmentation problem, Computer science, Geometry, edge connectivity, edge connected graph, query structure, planar geometric representation, edge cuts, data structures, edge splitting, Joining processes, Contracts, computational complexity, cuts]
Speed is as powerful as clairvoyance [scheduling problems]
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We consider several well known nonclairvoyant scheduling problems, including the problem of minimizing the average response time, and best-effort firm real-time scheduling. It is known that there are no deterministic online algorithms for these problems with bounded (or even polylogarithmic in the number of jobs) competitive ratios. We show that moderately increasing the speed of the processor used by the non-clairvoyant scheduler effectively gives this scheduler the power of clairvoyence. Furthermore, we show that there exist online algorithms with bounded competitive ratios on all inputs that are not closely correlated with processor speed.
[Kirk field collapse effect, Optimal scheduling, average response time, Time measurement, online algorithms, Scheduling algorithm, Delay, best-effort firm real-time scheduling, Computer science, Processor scheduling, Measurement standards, nonclairvoyant scheduling problems, bounded competitive ratios, scheduling, Cost function, Velocity measurement]
Algorithms for matrix groups and the Tits alternative
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
J. Tits (1972) has shown that a finitely generated linear group either contains a nonabelian free group or has a solvable subgroup of finite index. We give a polynomial time algorithm for deciding which of these two conditions holds for a given finitely generated matrix group over an algebraic number field. Noting that many computational problems are undecidable for groups with nonabelian free subgroups, we investigate the complexity of problems relating to linear groups with solvable subgroups of finite index. For such a group G, we are able in polynomial time to compute a homomorphism /spl phi/ such that /spl phi/(G) is a finite matrix group and the kernel of /spl phi/ is solvable. If in addition G has a nilpotent subgroup of finite index, we obtain much stronger results. These include an effective encoding of elements of G such that the encoding length of an element obtained as a product of length /spl les/l over the generators is O(logl) times a polynomial in the input length. This result is the best possible. For groups with abelian subgroups of finite index, we obtain a Las Vegas algorithm for several basic computational tasks including membership testing and computing a presentation. This generalizes recent work of R. Beals and L. Babai (1993), who give a Las Vegas algorithm for the case of finite groups.
[Algorithm design and analysis, matrix groups, complexity, nonabelian free subgroups, finite index, algebraic number field, encoding length, Mathematics, formal logic, finitely generated matrix group, decidability, nonabelian free group, Polynomials, Kernel, Testing, solvable subgroup, nilpotent subgroup, finitely generated linear group, Codes, Las Vegas algorithm, Encoding, Application software, encoding, polynomial time algorithm, matrix algebra, Computer science, group theory, finite groups, Packaging, homomorphism, membership testing, computational complexity]
Splitters and near-optimal derandomization
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We present a fairly general method for finding deterministic constructions obeying what we call k-restrictions; this yields structures of size not much larger than the probabilistic bound. The structures constructed by our method include (n,k)-universal sets (a collection of binary vectors of length n such that for any subset of size k of the indices, all 2/sup k/ configurations appear) and families of perfect hash functions. The near-optimal constructions of these objects imply the very efficient derandomization of algorithms in learning, of fixed-subgraph finding algorithms, and of near optimal /spl Sigma/II/spl Sigma/ threshold formulae. In addition, they derandomize the reduction showing the hardness of approximation of set cover. They also yield deterministic constructions for a local-coloring protocol, and for exhaustive testing of circuits.
[k-restrictions, Protocols, computational linguistics, Mathematics, learning, Circuit testing, Parallel algorithms, deterministic constructions, Information systems, set cover, exhaustive testing, probabilistic bound, near-optimal constructions, Contracts, Engineering profession, splitters, local-coloring protocol, probability, hardness of approximation, Educational institutions, Boosting, derandomization, randomised algorithms, Computer science, fairly general method, fixed-subgraph finding algorithms, near-optimal derandomization, computational complexity]
Load balancing in the L/sub p/ norm
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
In the load balancing problem, there is a set of servers, and jobs arrive sequentially. Each job can be run on some subset of the servers, and must be assigned to one of them in an online fashion. Traditionally, the assignment of jobs to servers is measured by the L/sub /spl infin// norm; in other words, an assignment of jobs to servers is quantified by the maximum load assigned to any server. In this measure the performance of the greedy load balancing algorithm may be a logarithmic factor higher than the offline optimal. In many applications, the L/sub /spl infin// norm is not a suitable way to measure how well the jobs are balanced, If each job sees a delay that is proportional to the number of jobs on its server, then the average delay among all jobs is proportional to the sum of the squares of the numbers of jobs assigned to the servers. Minimizing the average delay is equivalent to minimizing the Euclidean (or L/sub 2/) norm. For any fixed p, 1/spl les/p</spl infin/, we show that the greedy algorithm performs within a constant factor of the offline optimal with respect to the L/sub p/ norm. The constant grows linearly with p, which is best possible, but does not depend on the number of servers and jobs.
[Greedy algorithms, Algorithm design and analysis, average job delay, load balancing, Delay, resource allocation, L/sub p/ norm, Cost function, greedy load balancing algorithm, Contracts, queueing theory, L/sub /spl infin// norm, Optimized production technology, competitive algorithms, offline optimal, deterministic algorithm, deterministic algorithms, Euclidean norm, Computer science, sum of the squares, Cellular phones, online operation, delays, job assignment, Load management, maximum load]
The bit vector intersection problem
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
This paper introduces the bit vector intersection problem: given a large collection of sparse bit vectors, find all the pairs with at least t ones in common for a given input parameter t. The assumption is that the number of ones common to any two vectors is significantly less than t, except for an unknown set of O(n) pairs. This problem has important applications in DNA physical mapping, clustering, and searching for approximate dictionary matches. We present two randomized algorithms that solve this problem with high probability and in sub-quadratic expected time. One of these algorithms is based on a recursive tree-searching procedure, and the other on hashing. We analyze the tree scheme in terms of branching processes, while our analysis of the hashing scheme is based on Markov chains. Since both algorithms have similar asymptotic performance, we also examine experimentally their relative merits in practical situations. We conclude by showing that a fundamental problem arising in the Human Genome Project is captured by the bit vector intersection problem described above and hence can be solved by our algorithms.
[Dictionaries, Cloning, Humans, Genomics, hashing, Fingerprint recognition, high probability, Electronic mail, sparse bit vectors, Biological cells, randomised algorithms, recursive tree-searching, bit vector intersection, DNA, Clustering algorithms, randomized algorithms, algorithm theory, tree data structures, dictionary matches, Bioinformatics]
Contention resolution with bounded delay
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
When distributed processes contend for a shared resource, we need a good distributed contention resolution protocol, e.g., for multiple-access channels (ALOHA, Ethernet), PRAM emulation, and optical routing. Under a stochastic model of request generation from n synchronous processes, Raghavan & Upfal (1995) have shown a protocol which is stable for a positive request rate; their main result is that for every resource request, its expected delay (time to get serviced) is O(log n). Assuming that the initial clock times of the processes are within a known bound of each other, we present a stable protocol, wherein the expected delay for each request is O(1). We derive this by showing an analogous result for can infinite number of processes, assuming that all processes agree on the time.
[Ethernet networks, Delay effects, Stochastic processes, Optical computing, Access protocols, resource request, distributed processing, contention resolution, Phase change random access memory, Computer science, bounded delay, resource allocation, Emulation, distributed contention resolution, stable protocol, Routing protocols, protocols, Clocks]
Perspectives on database theory
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Database management systems address the need to store, retrieve, and manipulate large amounts of data in an organized fashion. The database held has grown tremendously in the last 25 years. It is reported that the database industry generated $7 billion in revenue in 1994 and is growing at a rate of 35% per year. Industrial and academic research have been instrumental to this growth. Theory has played an important role in defining the right abstractions and concepts, and providing a firm foundation for the field. In order to access effectively a large volume of data, one needs an abstract logical view of the data, which must be separate from the physical storage of data. The important first component of a database is therefore an abstract view of data (called the data model) and the accompanying specialized high-level language that is used to access the data. The second important component is the data structures that are used to store the data along with the algorithms to support the efficient translation from the logical to the physical world. The third important component is the mechanisms that allow the database to be accessed concurrently by many users, without violating its integrity. Theory has contributed to all three fronts, starting with what is undoubtedly the cornerstone of the area, the introduction and formal definition of the relational model by F.P. Codd (1970). It is a highly unusual compliment for theory when the major commercial products in the field have at their core a mathematically rigorous, formal model. Our primary aims in this paper will be to give a flavor of the types of problems that database theory addresses, and to review how research in the area has evolved over the years. At the end we will try to point to some topics that may be of interest to people in the FOCS community tempted to work in database theory.
[Vocabulary, Instruments, database industry, Relational databases, Information retrieval, Calculus, database management systems, High level languages, database theory, abstractions, Database systems, Data models, data structures, relational model, Mathematical model, Logic]
On computing Boolean functions by sparse real polynomials
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We investigate the complexity of Boolean functions f with respect to realizations by real polynomials p (voting polynomials) in the sense that the sign of p(x) determines the value f(x). Considerable research has been done on determining the minimal degree needed for realizing or approximating particular functions. In this paper we focus our interest on estimating the minimal number of monomials, i.e. the length of realizing polynomials. Our main observation is that, in contrast to the degree, the minimal length essentially depends on whether we realize f over the domain.
[complexity, polynomials, Circuits, threshold-and circuits, Switches, Feedforward neural networks, threshold-parity circuits, Boolean functions, Voting, Neural networks, sparse real polynomials, Polynomials, computational complexity]
Faster algorithms for the construction of parameterized suffix trees
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Parameterized strings were introduced by Baker to solve the problem of identifying blocks of code that get duplicated in a large software system. Parameter symbols capture the notion of code identity while permitting renaming of variables. The code duplication problem was solved by first constructing a generalized suffix tree for the corresponding parameterized strings. The fastest known generalized suffix tree algorithm has an O(n.
[Algorithm design and analysis, pattern matching, Software algorithms, trees (mathematics), parameterized suffix trees, suffix tree algorithm, Computer science, code duplication problem, algorithm theory, Software systems, string matching, suffix tree, computational complexity]
Competitive access time via dynamic storage rearrangement
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We model the problem of storing items in some warehouse (modeled as an undirected graph) where a server has to visit items over time, with the goal of minimizing the total distance traversed by the server. Special cases of this problem include the management of a real industrial stacker crane warehouse, automatic robot run warehouses, disk track optimization to minimize access time, managing two dimensional memory (bubble memory and mass storage systems), doubly linked list management, and the process migration problem. The static version of this problem assumes some known probability distribution on the access patterns. We initiate the study of the dynamic version of the problem, where the robot may rearrange the warehouse to deal efficiently with future events. We require no statistical assumptions on the access pattern, and give competitive algorithms that rearrange the warehouse over time to deal efficiently with the true access patterns. We give non-trivial upper bounds for the general problem, along with some interesting lower bounds. In addition, we model realistic data access patterns on disk storage by considering two practically significant scenarios: access to some database via dynamically changing alternative indices and access patterns derived from root to leaf traversals of some (unknown) tree structure. In both cases we give greatly improved competitive ratios.
[storage allocation, server, mass storage systems, graph theory, tree structure, upper bounds, Probability distribution, database management systems, disk track optimization, process migration, optimisation, database, Service robots, Databases, Storage automation, bubble memory, probability distribution, tree data structures, automatic robot run warehouse, undirected graph, Tree data structures, Cranes, industrial stacker crane warehouse, competitive algorithms, disk storage, warehouse, Robotics and automation, lower bounds, Computer science, Upper bound, two dimensional memory management, Memory management, operations research, competitive access time, doubly linked list management, dynamic storage rearrangement, computational complexity]
Cognitive computation
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Cognitive computation is discussed as a discipline that links together neurobiology, cognitive psychology and artificial intelligence.
[Heart, cognitive systems, cognitive psychology, Computational modeling, Psychology, Humans, Cognition, artificial intelligence, Computer science, psychology, Fires, neurobiology, Brain modeling, Digital arithmetic, Artificial intelligence, cognitive computation]
Algebraic decomposition of non-convex polyhedra
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Any arbitrary polyhedron P/spl sube/R/sup d/ can be written as algebraic sum of simple terms, each an integer multiple of the intersection of d or fewer half-spaces defined by facets of P. P can be non-convex and can have holes of any kind. Among the consequences of this result are a short boolean formula for P, a fast parallel algorithm for point classification, and a new proof of the Gram-Sommerville angle relation.
[Solid modeling, parallel algorithms, Piecewise linear approximation, parallel algorithm, half-spaces, point classification, computational geometry, Data structures, Topology, boolean formula, Parallel algorithms, Computational complexity, arbitrary polyhedron, Gram-Sommerville angle relation, Computer science, Computational geometry, algebraic sum, algebraic decomposition, nonconvex polyhedra]
An approximation scheme for planar graph TSP
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We consider the special case of the traveling salesman problem (TSP) in which the distance metric is the shortest-path metric of a planar unweighted graph. We present a polynomial-time approximation scheme (PTAS) for this problem.
[approximation theory, distance metric, traveling salesman problem, Particle separators, graph theory, Traveling salesman problems, shortest-path metric, Linear programming, Combinatorial mathematics, Genetic algorithms, Computer science, travelling salesman problems, planar unweighted graph, operations research, Approximation algorithms, Polynomials, Dynamic programming, approximation scheme, polynomial-time, Testing, computational complexity]
Reductions, codes, PCPs, and inapproximability
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Many recent results show the hardness of approximating NP-hard functions. We formalize, in a very simple way, what these results involve: a code-like Levin reduction. Assuming a well-known complexity assumption, we show that such reductions cannot prove the NP-hardness of the following problems, where /spl epsiv/ is any positive fraction: (i) achieving an approximation ratio n/sup 1/2+/spl epsiv// for Clique, (ii) achieving an approximation ratio 1.5+/spl epsiv/ for Vertex Cover, and (iii) coloring a 3-colorable graph with O(logn) colors. In fact, we explain why current reductions cannot prove the NP-hardness of coloring 3-colorable graphs with 9 colors. Our formalization of a code-like reduction, together with our justification of why such reductions are natural, also clarifies why current proofs of inapproximability results use error-correcting codes.
[approximation ratio, codes, Engineering profession, error correction codes, graph theory, code-like Levin reduction, Displays, inapproximability, 3-colorable graph, code-like reduction, NP-hardness, hardness, error-correcting codes, PCPs, complexity assumption, NP-hard functions, Approximation algorithms, Polynomials, Error correction codes, positive fraction, computational complexity]
Improved lower bound on testing membership to a polyhedron by algebraic decision trees
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We introduce a new method of proving lower bounds on the depth of algebraic decision trees of degree d and apply it to prove a lower bound /spl Omega/(log N) for testing membership to an n-dimensional convex polyhedron having N faces of all dimensions, provided that N>(nd)/sup /spl Omega//(n). This weakens considerably the restriction on N previously imposed by the authors and opens a possibility to apply the bound to some naturally appearing polyhedra.
[algebraic decision trees, decision theory, polyhedron, Computational modeling, computational geometry, Mathematics, lower bound, lower bounds, Computer science, n-dimensional convex polyhedron, Concrete, Decision trees, naturally appearing polyhedra, Marine vehicles, membership testing, Testing]
Amortization, lazy evaluation, and persistence: lists with catenation via lazy linking
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
Amortization has been underutilized in the design of persistent data structures, largely because traditional accounting schemes break down in a persistent setting. Such schemes depend on saving "credits" for future use, but a persistent data structure may have multiple "futures\
[Data analysis, Costs, catenation, Delay effects, persistent data structures, Electrostatic discharge, Data structures, amortization, History, lazy evaluation, Computer science, list processing, persistence, lazy linking, data structures, Functional programming, Joining processes, Contracts]
Improved hardness results for approximating the chromatic number
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
First, a simplified geometric proof is presented for the result of C. Lund and M. Yannakakis (1994) saying that for some /spl epsiv/>0 it is NP-hard to approximate the chromatic number of graphs with N vertices by a factor of N/sup /spl epsiv//. Then, more sophisticated techniques are employed to improve the exponent. A randomized twisting method allows us to completely pack a certain space with copies of a graph without much affecting the independence number. Together with the newest results of M. Bellare et al. (1995), on the number of amortized free bits, it is shown that for every /spl epsiv/>0 the chromatic number cannot be approximated by a factor of N/sup 1/5-/spl epsiv// unless NP=ZPP. Finally, we get polynomial lower bounds in terms of /spl chi/. Unless NP=ZPP, the performance ratio of every polynomial time algorithm approximating the chromatic number of /spl chi/-colorable graphs (i.e., the chromatic number is at most /spl chi/) is at least /spl chi//sup 1/5-o(1/) (where the o-notation is with respect to /spl chi/).
[Minimization methods, NP-hard, polynomial lower bounds, Error probability, chromatic number approximation, graph theory, chromatic number, Mathematics, hardness results, randomized twisting method, NP-complete problem, Noise measurement, polynomial time algorithm, amortized free bits, Computer science, geometric proof, Microscopy, Approximation algorithms, Polynomials, computational complexity]
Optimal algorithms for curves on surfaces
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
We describe an optimal algorithm to decide if one closed curve on a triangulated 2-manifold can be continuously transformed to another, i.e., if they are homotopic. Our algorithm runs in O(n+k/sub 1/+k/sub 2/) time and space, where closed curves C/sub 1/ and C/sub 2/ of lengths k/sub 1/ and k/sub 2/, resp., on a genus g surface M (g/spl ne/2 if M orientable, and g/spl ne/3,4 if M is non-orientable) are presented as edge-vertex sequences in a triangulation T of size n of M. This also implies an optimal algorithm to decide if a closed curve on a surface can be continuously contracted to a point. Except for three low genus cases, our algorithm completes an investigation into the computational complexity of the two classical problems for surfaces posed by the mathematician Max Dehn at the beginning of this century. However, we make novel applications of methods from modern combinatorial group theory for an approach entirely different from previous ones, and much simpler to implement.
[Sequences, computational geometry, edge-vertex sequences, Data structures, optimal algorithms, Topology, combinatorial group theory, group theory, curves, triangulated 2-manifold, Physics computing, surfaces, Biology computing, Computational biology, DNA computing, computational complexity]
Free bits, PCPs and non-approximability-towards tight results
Proceedings of IEEE 36th Annual Foundations of Computer Science
None
1995
The first part of this paper presents new proof systems and improved non-approximability results. In particular we present a proof system for NP using logarithmic randomness and two amortized free bits, so that Max clique is hard within N/sup 1/3/ and chromatic number within N/sup 1/5/. We also show hardness of 38/37 for Max-3-SAT, 27/26 for vertex cover, 82/81 for Max-cut, and 94/93 for Max-2-SAT. The second part of this paper presents a "reverse" of the FGLSS connection by showing that an NP-hardness result for the approximation of Max clique to within a factor of N/sup 1/(g+1/) would imply a probabilistic verifier for NP with logarithmic randomness and amortized free-bit complexity g. We also show that "existing techniques" won't yield proof systems of less than two bits in amortized free bit complexity. Finally, we initiate a comprehensive study of PCP and FPCP parameters, proving several triviality results and providing several useful transformations.
[proof systems, Drives, computational geometry, History, Max Clique, amortized free bits, Postal services, Max-cut, free bits, FGLSS connection, Max-2-SAT, Cities and towns, triviality results, Polynomials, theorem proving, NP complete problems, nonapproximability, chromatic number, Computer science, NP-hardness, amortized free bit complexity, PCPs, logarithmic randomness, amortized free-bit complexity, FPCP parameters, Error correction codes, computational complexity]
Approximate strip packing
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We present an approximation scheme for strip-packing, or packing rectangles into a rectangle of fixed width and minimum height, a classical NP-hard cutting-stock problem. The algorithm finds a packing of n rectangles whose total height is within a factor of (1+/spl epsiv/) of optimal, and has running time polynomial both in n and in 1//spl epsiv/. It is based on a reduction to fractional bin-packing, and can be performed by 5 stages of guillotine cuts.
[Algorithm design and analysis, Strips, NP-hard cutting-stock problem, strip-packing, fractional bin-packing, Application software, guillotine cuts, Computer science, Processor scheduling, Integer linear programming, packing rectangles, Polynomials, computational complexity]
An efficient algorithm for constructing minimal trellises for codes over finite Abelian groups
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We present an efficient algorithm for computing the minimal trellis for a group code over a finite Abelian group, given a generator matrix for the code. We also show how to compute a succinct representation of the minimal trellis for such a code, and present algorithms that use this information to efficiently compute local descriptions of the minimal trellis. This extends the work of Kschischang and Sorokine (1995), who handled the case of linear codes over fields. An important application of our algorithms is to the construction of minimal trellises for lattices. A key step in our work is handling codes over cyclic groups C/sub p//spl alpha/, where p is a prime. Such a code can be viewed as a submodule over the ring Z/sub p//spl alpha/. Because of the presence of zero-divisors in the ring, submodules do not share the useful properties of vector spaces. We get around this difficulty by restricting the notion of linear combination to p-linear combination, and introducing the notion of a p-generator sequence, which enjoys properties similar to that of a generator matrix for a vector space.
[cyclic groups, zero-divisors, efficient algorithm, Lattices, linear combination, Educational institutions, Vectors, Decoding, p-linear combination, Computer science, group theory, Linear code, minimal trellis, p-generator sequence, minimal trellises, Bandwidth, finite Abelian groups, Modems, submodule, Block codes, Modulation coding, group code]
Faster deterministic sorting and searching in linear space
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We present a significant improvement on linear space deterministic sorting and searching. On a unit-cost RAM with word size w, an ordered set of n w-bit keys (viewed as binary strings or integers) can be maintained in O(min{[/spl radic/(logn)][logn/logw+loglogn][logwloglogn]}) time per operation, including insert, delete, member search, and neighbour search. The cost for searching is worst-case while the cost for updates is amortized. As an application, n keys can be sorted in linear at O(n/spl radic/(logn)) worst-case cost. The best previous method for deterministic sorting and searching in linear space has been the fusion trees which supports updates and queries in O(logn/loglogn) amortized time and sorting in O(nlogn/loglogn) worst-case time. We also make two minor observations on adapting our data structure to the input distribution and on the complexity of perfect hashing.
[Tree data structures, neighbour search, complexity, Costs, Dictionaries, linear space, fusion trees, searching, data structure, Read-write memory, Data structures, insert, member search, delete, Sorting, Computer science, Upper bound, deterministic sorting, sorting, Polynomials, perfect hashing, unit-cost RAM, Arithmetic, worst-case]
On the applications of multiplicity automata in learning
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The learnability of multiplicity automata has attracted a lot of attention, mainly because of its implications on the learnability of several classes of DNF formulae. The authors further study the learnability of multiplicity automata. The starting point is a known theorem from automata theory relating the number of states in a minimal multiplicity automaton for a function f to the rank of a certain matrix F. With this theorem in hand they obtain the following results: a new simple algorithm for learning multiplicity automata with a better query complexity. As a result, they improve the complexity for all classes that use the algorithms of Bergadano and Varricchio (1994) and Ohnishi et al. (1994) and also obtain the best query complexity for several classes known to be learnable by other methods such as decision trees and polynomials over GF(2). They prove the learnability of some new classes that were not known to be learnable before. Most notably, the class of polynomials over finite fields, the class of bounded-degree polynomials over infinite fields, the class of XOR of terms, and a certain class of decision trees. While multiplicity automata were shown to be useful to prove the learnability of some subclasses of DNF formulae and various other classes, they study the limitations of this method. They prove that this method cannot be used to resolve the learnability of some other open problems such as the learnability of general DNF formulae or even K-term DNF for k=/spl omega/ (log n) or satisfy-s DNF formulae for s=/spl omega/(1). These results are proven by exhibiting functions in the above classes that require multiplicity automata with superpolynomial number of states.
[multiplicity automata, Learning automata, automata theory, query complexity, matrix, DNF formulae learnability, Postal services, states, Computer science, infinite field bounded-degree polynomials, minimal multiplicity automaton, decision trees, multiplicity automata learning, Polynomials, finite field polynomials]
Approximate checking of polynomials and functional equations
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors show how to check programs that compute polynomials and functions defined by addition theorems-in the realistic setting where the output of the program is approximate instead of exact. They present results showing how to perform approximate checking, self-testing, and self-correcting of polynomials, settling in the affirmative a question raised by Gemmell et al. (1991), and Rubinfeld and Sudan (1992, 1996). They then show how to perform approximate checking, self-testing, and self-correcting for those functions that satisfy addition theorems, settling a question raised by Rubinfeld (1994]) In both cases, they show that the properties used to test programs for these functions are both robust (in the approximate sense) and stable. Finally, they explore the use of reductions between functional equations in the context of approximate self-testing. Their results have implications to the stability theory of functional equations.
[Performance evaluation, Stability, program testing, polynomials, approximate checking, functions, reductions, Built-in self-test, program checking, self-correcting, Equations, Computer science, approximate program output, stability theory, Automatic testing, functional equation computation, addition theorems, Polynomials, Robustness, polynomial computation, self-testing, Finite wordlength effects, Fixed-point arithmetic]
Sampling according to the multivariate normal density
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
This paper deals with the normal density of n dependent random variables. This is a function of the form: ce(-x/sup T/Ax) where A is an n/spl times/n positive definite matrix, a: is the n-vector of the random variables and c is a suitable constant. The first problem we consider is the (approximate) evaluation of the integral of this function over the positive orthant /spl int/(x/sub 1/=0)/sup /spl infin///spl int/(x/sub 2/=0)/sup /spl infin///spl middot//spl middot//spl middot//spl int/(x/sub n/=0)/sup /spl infin//ce(-x/sup T/Ax). This problem has a long history and a substantial literature. Related to it is the problem of drawing a sample from the positive orthant with probability density (approximately) equal to ce(-x/sup T/Ax). We solve both these problems here in polynomial time using rapidly mixing Markov Chains. For proving rapid convergence of the chains to their stationary distribution, we use a geometric property called the isoperimetric inequality. Such an inequality has been the subject of recent papers for general log-concave functions. We use these techniques, but the main thrust of the paper is to exploit the special property of the normal density to prove a stronger inequality than for general log-concave functions. We actually consider first the problem of drawing a sample according to the normal density with A equal to the identity matrix from a convex set K in R/sup n/ which contains the unit ball. This problem is motivated by the problem of computing the volume of a convex set in a way we explain later. Also, the methods used in the solution of this and the orthant problem are similar.
[positive definite matrix, probability density, random variables, positive orthant, Gaussian distribution, Mathematics, Probability distribution, Steady-state, rapidly mixing Markov Chains, Convergence, log-concave functions, convex set, n dependent random variables, Statistical distributions, Tin, Markov processes, geometric property, Sampling methods, isoperimetric inequality, Polynomials, polynomial time, multivariate normal density]
Static dictionaries on AC/sup 0/ RAMs: query time /spl theta/(/spl radic/log n/log log n) is necessary and sufficient
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
In this paper we consider solutions to the static dictionary problem on AC/sup 0/ RAMs, i.e. random access machines where the only restriction on the finite instruction set is that all computational instructions are in AC/sup 0/. Our main result is a tight upper and lower bound of /spl theta/(/spl radic/log n/log log n) on the time for answering membership queries in a set of size n when reasonable space is used for the data structure storing the set; the upper bound can be obtained using O(n) space, and the lower bound holds even if we allow space 2/sup polylog n/. Several variations of this result are also obtained. Among others, we show a tradeoff between time and circuit depth under the unit-cost assumption: any RAM instruction set which permits a linear space, constant query time solution to the static dictionary problem must have an instruction of depth /spl Omega/(log w/log log to), where w is the word size of the machine (and log the size of the universe). This matches the depth of multiplication and integer division, used in the perfect hashing scheme by M.L. Fredman, J. Komlos and E. Szemeredi (1984).
[Dictionaries, Costs, Circuits, data structure, Registers, finite instruction set, integer division, membership queries, unit-cost assumption, AC/sup 0/ RAMs, Robustness, data structures, Computational efficiency, multiplication, computational instructions, Computational modeling, query time, Read-write memory, upper bound, static dictionaries, Computer aided instruction, constant query time solution, Councils, perfect hashing scheme, random access machines]
Median selection requires (2+/spl epsiv/)n comparisons
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Improving a long standing result of Bent and John (1985), we obtain a (2+/spl epsiv/)n lower bound (for some fixed /spl epsiv/>0) on the number of comparisons required, in the worst case, for selecting the median of n elements. The new lower bound is obtained using a weight function that allows us to combine leaf counting and adversary arguments.
[Computer science, Chaos, median selection, Upper bound, weight function, adversary arguments, Binary trees, algorithm theory, lower bound, leaf counting]
New lower bounds for halfspace emptiness
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The author derives a lower bound of /spl Omega/(n/sup 4/3/) for the halfspace emptiness problem: given a set of n points and n hyperplanes in R/sup 5/, is every point above every hyperplane? This matches the best known upper bound to within polylogarithmic factors, and improves the previous best lower bound of /spl Omega/(nlogn). The lower bound applies to partitioning algorithms in which every query region is a polyhedron with a constant number of facets.
[halfspace emptiness, polyhedron, Computational modeling, Buildings, polylogarithmic factors, H infinity control, hyperplanes, Data structures, Linear programming, Partitioning algorithms, query region, lower bounds, points, facets, Computer science, Upper bound, partitioning algorithms, Ear, Marine vehicles, computational complexity]
An 8-approximation algorithm for the subset feedback vertex set problem
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We present an 8-approximation algorithm for the problem of finding a minimum weight subset feedback vertex set. The input in this problem consists of an undirected graph G=(V,E) with vertex weights w(v) and a subset of vertices S called special vertices. A cycle is called interesting if it contains at least one special vertex. A subset of vertices is called a subset feedback vertex set with respect to S if it intersects every interesting cycle The goal is to find a minimum weight subset feedback vertex set. The best pervious algorithm for the general case provided only a logarithmic approximation factor. The minimum weight subset feedback vertex set problem generalizes two NP-Complete problems: the minimum weight feedback vertex set problem in undirected graphs and the minimum weight multiway vertex cut problem. The main tool that we use in our algorithm and its analysis is a new version of multi-commodity flow which we call relaxed multi-commodity flow. Relaxed multi-commodity flow is a hybrid of multi-commodity flow and multi-terminal flow.
[Algorithm design and analysis, State feedback, subset feedback vertex set problem, multi-commodity flow, 8-approximation algorithm, multi-terminal flow, relaxed multi-commodity flow, NP-complete problem, vertex weights, logarithmic approximation factor, Computer science, Couplings, Approximation algorithms, Genetics, minimum weight, special vertices, undirected graph, NP-Complete problems, Joining processes, computational complexity]
Property testing and its connection to learning and approximation
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors study the question of determining whether an unknown function has a particular property or is /spl epsiv/-far from any function with that property. A property testing algorithm is given a sample of the value of the function on instances drawn according to some distribution, and possibly may query the function on instances of its choice. First, they establish some connections between property testing and problems in learning theory. Next, they focus on testing graph properties, and devise algorithms to test whether a graph has properties such as being k-colorable or having a /spl rho/-clique (clique of density /spl rho/ w.r.t. the vertex set). The graph property testing algorithms are probabilistic and make assertions which are correct with high probability utilizing only poly(1//spl epsiv/) edge-queries into the graph, where /spl epsiv/ is the distance parameter. Moreover, the property testing algorithms can be used to efficiently (i.e., in time linear in the number of vertices) construct partitions of the graph which correspond to the property being tested, if it holds for the input graph.
[/spl rho/-clique, graph property testing, approximation, testing, k-colorable graph, Partitioning algorithms, property testing, Application software, querying, Programming profession, property testing algorithm, Computer science, probabilistic algorithm, assertions, input graph, unknown function, learning theory, Testing]
Fault tolerant data structures
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors consider the tolerance of data structures to memory faults. They observe that many pointer-based data structures (e.g. linked lists, trees, etc.) are highly nonresilient to faults. A single fault in a linked list or tree may result in the loss of the entire set of data. They present a formal framework for studying the fault tolerance properties of pointer-based data structures, and provide fault tolerant versions of the stack, the linked list, and the dictionary tree.
[Tree data structures, Dictionaries, stack, Laboratories, fault tolerant data structures, Data structures, Mathematics, linked list, Application software, trees, Resilience, Computer science, Fault tolerance, pointer-based data structures, memory faults, Tail, dictionary tree, tree data structures]
Universal data compression and portfolio selection
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors consider universal data compression, universal portfolio selection (online portfolio algorithms) and the relationship of both to information theory. Apparently the fundamental minimax redundancy game in data compression and the minimax regret game for the growth rate of wealth in investment have the same answer. There is also a duality between entropy rate and the growth rate of wealth.
[data compression, online portfolio algorithms, entropy rate, Data compression, portfolio selection, Minimax techniques, investment, Entropy, universal data compression, wealth growth rate, Game theory, Information systems, minimax regret game, Investments, universal portfolio selection, fundamental minimax redundancy game, Random variables, information theory, Stock markets, Portfolios, Information theory]
Factoring graphs to bound mixing rates
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
This paper develops a new technique for bounding the mixing rate of a Markov chain by decomposing the state space into factors. The first application is an efficient Monte Carlo Markov chain algorithm for generating random three-colorings of 2-dimensional lattice regions. This provides a rigorous tool for studying some properties of the 3-state Potts model and the ice model from statistical mechanics. As a second application, we develop similar techniques to bound the mixing rate of a Metropolis sampling algorithm by a type of "temperature factorization". Both factorization theorems work by using known mixing properties of related Markov chains to establish the efficiency of a new sampling algorithm.
[Lattices, mixing rate bounding, Monte Carlo Markov chain algorithm, 3-state Potts model, Metropolis sampling algorithm, Educational institutions, Mathematics, State-space methods, Physics, state space decomposition, Markov chain, Thermodynamics, Monte Carlo methods, Lapping, Councils, random three-colorings, Markov processes, factorization theorems, 2-dimensional lattice regions, Artificial intelligence, ice model, statistical mechanics]
Potential of the approximation method
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Developing some techniques for the approximation method, we establish precise versions of the following statements concerning lower bounds for circuits that detect cliques of size s in a graph with m vertices. For 5/spl les/s/spl les/m/4, a monotone circuit computing CLIQUE(m, s) contains at least (1/2) 1.8/sup min(/spl radic/s-1/2,m/(4s))/ gates. If a non-monotone circuit computes CLIQUE using a "small" amount of negation, then the circuit contains an exponential number of gates. The former is proved very simply using so called bottleneck counting argument within the framework of approximation, whereas the latter is verified introducing a notion of restricting negation and generalizing the sunflower contraction.
[precise versions, cliques, Input variables, Circuits, vertices, Analog computers, Complexity theory, Approximation methods, Combinatorial mathematics, lower bounds, Boolean functions, approximation method, bottleneck counting argument, Polynomials, sunflower contraction, monotone circuit]
Solving systems of polynomial congruences modulo a large prime
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We consider the following polynomial congruences problem: given a prime p, and a set of polynomials f/sub 1/,...,f/sub m//spl isin/F/sub p/[x/sub 1/,...,x/sub n/] of total degree at most d, solve the system f/sub 1/=...=f/sub m/=0 for solution(s) in F/sub p//sup n/. We give a randomized algorithm for the decision version of this problem. When the system has F/sub p/-rational solutions our algorithm finds one of them as well as an approximation of the total number of such solutions. For a fixed number of variables, the algorithm runs in random polynomial time with parallel complexity poly-logarithmic in d, m and p, using a polynomial number of processors. As an essential step of the algorithm, we also formulate an algebraic homotopy method for extracting components of all dimensions of an algebraic set. The method is efficiently parallelizable.
[Solid modeling, parallel complexity, decision, polynomial congruences, H infinity control, randomized algorithm, Equations, Computer science, Algebra, algebraic homotopy method, Gaussian processes, Approximation algorithms, Polynomials, algebraic set, Robots, Arithmetic, computational complexity]
Simplified and improved resolution lower bounds
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We give simple new lower bounds on the lengths of resolution proofs for the pigeonhole principle and for randomly generated formulas. For random formulas, our bounds significantly extend the range of formula sizes for which non-trivial lower bounds are known. For example, we show that with probability approaching 1, any resolution refutation of a randomly chosen 3-CNF formula with at most n/sup 6/5-/spl epsiv// clauses requires exponential size. Previous bounds applied only when the number of clauses was at most linear in the number of variables. For the pigeonhole principle our bound is a small improvement over previous bounds. Our proofs are more elementary than previous arguments, and establish a connection between resolution proof size and maximum clause size.
[Computer science, resolution lower bounds, computability, randomly chosen 3-CNF formula, pigeonhole principle, randomly generated formulas, random formulas, lower bounds, Testing]
Fault-tolerant quantum computation
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
It has recently been realized that use of the properties of quantum mechanics might speed up certain computations dramatically. Interest in quantum computation has since been growing. One of the main difficulties in realizing quantum computation is that decoherence tends to destroy the information in a superposition of states in a quantum computer making long computations impossible. A further difficulty is that inaccuracies in quantum state transformations throughout the computation accumulate, rendering long computations unreliable. However, these obstacles may not be as formidable as originally believed. For any quantum computation with t gates, we show how to build a polynomial size quantum circuit that tolerates O(1/log/sup c/t) amounts of inaccuracy and decoherence per gate, for some constant c; the previous bound was O(1/t). We do this by showing that operations can be performed on quantum data encoded by quantum error-correcting codes without decoding this data.
[Computational modeling, Circuits, long computations, Interference, quantum theory, quantum error-correcting codes, Mechanical factors, Decoding, Fault tolerance, Quantum computing, quantum mechanics, decoherence, quantum circuit, Quantum mechanics, Polynomials, Error correction codes, quantum computation, fault-tolerant]
Computing vertex connectivity: new bounds from old techniques
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The vertex connectivity /spl kappa/ of a graph is the smallest number of vertices whose deletion separates the graph or makes it trivial. We present the fastest known deterministic algorithm for finding the vertex connectivity and a corresponding separator. The time for a digraph having n vertices and m edges is O(min{/spl kappa//sup 3/+n,/spl kappa/n}m); for an undirected graph the term m can be replaced by /spl kappa/n. A randomized algorithm finds /spl kappa/ with error probability 1/2 in time O(nm). If the vertices have nonnegative weights the weighted vertex connectivity is found in time O(/spl kappa//sub 1/nmlog(n/sup 2//m)) where /spl kappa//sub 1//spl les/m/n is the unweighted vertex connectivity, or in expected time O(nm log(n/sup 2//m)) with error probability 1/2. The main algorithm combines two previous vertex connectivity algorithms and a generalization of the preflow push algorithm of J. Hao and J.B. Orlin (1994) that computes edge connectivity.
[Error probability, Engineering profession, Particle separators, error probability, computational geometry, Graph theory, smallest number of vertices, deterministic algorithm, Computer science, preflow push algorithm, Monte Carlo methods, vertex connectivity, digraph, National electric code, Computer networks]
A 3-approximation for the minimum tree spanning k vertices
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
In this paper we give a 3-approximation algorithm for the problem of finding a minimum tree spanning any k-vertices in a graph. Our algorithm extends to a 3-approximation algorithm for the minimum tour that visits any k-vertices.
[Tree graphs, minimum tree, trees (mathematics), Approximation algorithms, Cost function, minimum tour, 3-approximation, k-vertices, Delay, graph, spanning k vertices]
A polynomial-time algorithm for learning noisy linear threshold functions
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors consider the problem of learning a linear threshold function (a halfspace in n dimensions, also called a "perceptron"). Methods for solving this problem generally fall into two categories. In the absence of noise, this problem can be formulated as a linear program and solved in polynomial time with the ellipsoid algorithm (or interior point methods). On the other hand, simple greedy algorithms such as the perceptron algorithm seem to work well in practice and can be made noise tolerant; but, their running time depends on a separation parameter (which quantifies the amount of "wiggle room" available) and can be exponential in the description length of the input. They show how simple greedy methods can be used to find weak hypotheses (hypotheses that classify noticeably more than half of the examples) in polynomial time, without dependence on any separation parameter. This results in a polynomial-time algorithm for learning linear threshold functions in the PAC model in the presence of random classification noise. The algorithm is based on a new method for removing outliers in data. Specifically, for any set S of points in R/sup n/, each given to b bits of precision, they show that one can remove only a small fraction of S so that in the remaining set T, for every vector v, max/sub x/spl epsiv/T/(v/spl middot/x)/sup 2//spl les/poly(n,b)|T|/sup -1//spl Sigma//sub x/spl epsiv/T/(v/spl middot/x)/sup 2/. After removing these outliers, they are able to show that a modified version of the perceptron learning algorithm works in polynomial time, even in the presence of random classification noise.
[Greedy algorithms, weak hypothesis finding, Machine learning algorithms, greedy algorithms, PAC model, perceptron algorithm, separation parameter, Linear programming, linear program, Vectors, data outlier removal, Ellipsoids, Computer science, ellipsoid algorithm, random classification noise, Ear, Machine learning, noisy linear threshold function learning, Polynomials, learning (artificial intelligence), polynomial-time algorithm, input description length, noise tolerance]
New algorithms for the disk scheduling problem
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Processor speed and memory capacity are increasing several times faster than disk speed. This disparity suggests that disk I/O performance will become an important bottleneck. Methods are needed for using disks more efficiently. Past analysis of disk scheduling algorithms has largely been experimental and little attempt has been made to develop algorithms with provable performance guarantees. We consider the following disk scheduling problem. Given a set of requests on a computer disk and a convex reachability function which determines how fast the disk head travels between tracks, our goal is to schedule the disk head so that it services all the requests in the shortest time possible. We present a 3/2-approximation algorithm (with a constant additive term). For the special case in which the reachability function is linear we present an optimal polynomial-time solution. The disk scheduling problem is related to the special case of the asymmetric Traveling Salesman Problem with the triangle inequality (ATSP-/spl Delta/) in which all distances are either 0 or some constant /spl alpha/. We show how to find the optimal tour in polynomial time and describe how this gives another approximation algorithm for the disk scheduling problem. Finally we consider the on-line version of the problem in which uniformly-distributed requests arrive over time. We present an algorithm (related to the above ATSP-/spl Delta/) that appears to give higher throughput than previously existing head scheduling algorithms.
[Algorithm design and analysis, asymmetric Traveling Salesman Problem, head scheduling, Laboratories, Traveling salesman problems, Throughput, Scheduling algorithm, disk head, Computer science, 3/2-approximation algorithm, Processor scheduling, convex reachability function, scheduling, optimal tour, Polynomials, polynomial time, Performance analysis, disk I/O performance, disk scheduling, Contracts]
Fast fault-tolerant concurrent access to shared objects
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors consider a synchronous model of distributed computation in which n nodes communicate via point-to-point messages, subject to the following constraints: (i) in a single "step\
[message loss, wide area networks, distributed file systems, File servers, expected steps, Concurrent computing, Fault tolerance, Network servers, Network topology, File systems, servers, object request, Broadcasting, fast fault-tolerant concurrent access, unreliable communication, node communication, local protocol, Computational modeling, hashing-based method, Access protocols, shared objects, words, object replication, Multicast protocols, distributed computation, faulty network environment, point-to-point messages, distributed memory systems, link failures, synchronous model]
The geometry of coin-weighing problems
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Given a set of m coins out of a collection of coins of k unknown distinct weights, the authors wish to decide if all the m given coins have the same weight or not using the minimum possible number of weighings in a regular balance beam. Let m(n,k) denote the maximum possible number of coins for which the above problem can be solved in n weighings. They show that m(n,2)=n/sup ( 1/2 +o(1))n/, whereas for all 3/spl les/k/spl les/n+1, m(n,k) is much smaller than m(n,2) and satisfies m(n,k)=/spl Theta/(n log n/log k). The proofs have an interesting geometric flavour; and combine linear algebra techniques with geometric probabilistic and combinatorial arguments.
[regular balance beam, linear algebra techniques, coin weighing problems, Lattices, H infinity control, computational geometry, geometric probabilistic arguments, Combinatorial mathematics, coin collection, Geometry, proofs, Linear algebra, geometry, unknown distinct weights, geometric combinatorial arguments, Arithmetic]
Universal stability results for greedy contention-resolution protocols
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
In this paper we analyze the behavior of communication networks in which packets are generated dynamically at the nodes and routed in discrete time steps across the edges. We focus on a basic adversarial model of packet generation and path determination for which the time-averaged injection rate of packets requiring the use of any edge is limited to be less than 1. A crucial issue that arises in such a setting is that of stability-will the number of packets in the system remain bounded, as the system runs for an arbitrarily long period of time? Among other things, we show: (i) There exist simple greedy protocols that are stable for all networks. (ii) There exist other commonly-used protocols (such as FIFO) and networks (such as arrays and hypercubes) that are not stable. (iii) The n-node ring is stable for all greedy routing protocols (with maximum queue-size and packet delay that is linear in n). (iv) There exists a simple distributed randomized greedy protocol that is stable for all networks and requires only polynomial queue size. Our results resolve several questions posed by Borodin et al. and provide the first examples of (i) a protocol that is stable for all networks, and (ii) a protocol that is not stable for all networks.
[Stability, communication networks, distributed randomized greedy protocol, path determination, commonly-used protocols, greedy contention-resolution protocols, adversarial model, hypercubes, time-averaged injection rate, FIFO, Computer science, universal stability results, packet generation, telecommunication networks, n-node ring, Delay lines, Hypercubes, Routing protocols, Polynomials, Computer science education, Communication networks, packet delay, Queueing analysis, Contracts]
Gadgets, approximation, and linear programming
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors present a linear-programming based method for finding "gadgets\
[computer-constructed gadgets, MAX DICUT, approximation ratio, Optimization methods, Linear programming, linear programming, Remuneration, constraint reduction, duality, Constraint optimization, gadget optimality proof, hardness, finite search space, MAX CUT, Linear approximation, approximation algorithm, Cost function, Approximation algorithms, Concrete, combinatorial structures, MAX 3SAT, gadgets, optimization problems]
Incoercible multiparty computation
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Current secure multiparty protocols have the following deficiency. The public transcript of the communication can be used as an involuntary commitment of the parties to their inputs and outputs. Thus parties can be later coerced by some authority to reveal their private data. Previous work that has pointed this interesting problem out contained only partial treatment. The authors present the first general treatment of the coercion problem in secure computation. They first present a general definition of protocols that provide resilience to coercion. Their definition constitutes a natural extension of the general paradigm used for defining secure multiparty protocols. They next show that if trapdoor permutations exist then any function can be incoercibly computed (i.e., computed by a protocol that provides resilience to coercion) in the presence of computationally bounded adversaries and only public communication channels. This holds as long as less than half the parties are coerced (or corrupted). In particular, theirs are the first incoercible protocols without physical security assumptions. Also, the protocols constitute an alternative solution to the recently solved adaptive security problem. Their techniques are quite surprising and include non-standard use of deniable encryptions.
[incoercible multiparty computation, secure multiparty protocols, Protocols, computationally bounded adversaries, Data security, coercion resilience, public communication channels, Communication system control, Resilience, Radio access networks, Computer science, Privacy, Voting, Communication channels, trapdoor permutations, deniable encryptions, adaptive security problem, Cryptography, protocols]
Probabilistic approximation of metric spaces and its algorithmic applications
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
This paper provides a novel technique for the analysis of randomized algorithms for optimization problems on metric spaces, by relating the randomized performance ratio for any, metric space to the randomized performance ratio for a set of "simple" metric spaces. We define a notion of a set of metric spaces that probabilistically-approximates another metric space. We prove that any metric space can be probabilistically-approximated by hierarchically well-separated trees (HST) with a polylogarithmic distortion. These metric spaces are "simple" as being: (1) tree metrics; (2) natural for applying a divide-and-conquer algorithmic approach. The technique presented is of particular interest in the context of on-line computation. A large number of on-line algorithmic problems, including metrical task systems, server problems, distributed paging, and dynamic storage rearrangement are defined in terms of some metric space. Typically for these problems, there are linear lower bounds on the competitive ratio of deterministic algorithms. Although randomization against an oblivious adversary has the potential of overcoming these high ratios, very little progress has been made in the analysis. We demonstrate the use of our technique by obtaining substantially improved results for two different on-line problems.
[Algorithm design and analysis, distributed paging, randomized performance ratio, server problems, Extraterrestrial measurements, Probability distribution, Mathematics, metric spaces, metrical task systems, Distributed computing, deterministic algorithms, randomised algorithms, Computer science, competitive ratio, randomized algorithms, Approximation algorithms, Polynomials, Performance analysis, dynamic storage rearrangement, Contracts, optimization problems]
The Boolean isomorphism problem
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We investigate the computational complexity of the Boolean isomorphism problem (BI): on input of two Boolean formulas F and G decide whether there exists a permutation of the variables of G such that F and G become equivalent. Our main result is a one-round interactive proof for BI, where the verifier has access to an NP oracle. To obtain this, we use a recent result from learning theory by N. Bshouty et al. (1995), that Boolean formulas can be learned probabilistically with equivalence queries and access to an NP oracle. As a consequence, BI cannot be /spl Sigma//sub 2//sup p/ complete unless the polynomial hierarchy collapses. This solves an open problem posed previously. Further properties of BI are shown: BI has And- and Or-functions, the counting version, BI, can be computed in polynomial time relative to BI, and BI is self-reducible.
[NP oracle, Computational modeling, Circuits, polynomial hierarchy, Binary decision diagrams, equivalence queries, Computational complexity, Computer science, Boolean formulas, Boolean functions, Turing machines, one-round interactive proof, Bismuth, Boolean isomorphism problem, Polynomials, learning theory, Context modeling, computational complexity]
Computing permanents over fields of characteristic 3: where and why it becomes difficult
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
In this paper we consider the complexity of computing permanents over fields of characteristic 3. We present a polynomial time algorithm for computing per(A) for a matrix A such that the rank rg(AA/sup T/-I)/spl les/1. On the other hand, we show that existence of a polynomial-time algorithm for computing per(A) for a matrix A such that rg(AA/sup T/-I)/spl ges/2 implies NP=R. As a byproduct we obtain that computing per(A) for a matrix A such that rg(AA/sup T/-I)/spl ges/2 is P(mod3) complete.
[complexity, Symmetric matrices, fields of characteristic 3, World Wide Web, Complexity theory, matrix, polynomial time algorithm, Computer science, Linear algebra, permanents, Polynomials, Internet, polynomial-time algorithm, computational complexity]
Equivalence in finite-variable logics is complete for polynomial time
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
How difficult is it to decide whether two finite structures can be distinguished in a given logic? For first order logic, this question is equivalent to the graph isomorphism problem with its well-known complexity theoretic difficulties. Somewhat surprisingly, the situation is much clearer when considering the fragments L/sup k/ of first-order logic whose formulae contain at most k (free or bound) variables (for some k/spl ges/1). We show that for each k/spl ges/2, equivalence in the k-variable logic L/sup k/ is complete for polynomial time under quantifier-free reductions (a weak form of NC/sub 0/ reductions). Moreover, we show that the same completeness result holds for the powerful extension C/sup k/ of L/sup k/ with counting quantifiers (for every k/spl ges/2).
[complexity theoretic difficulties, counting quantifiers, equivalence, finite structures, completeness result, finite-variable logics, Sun, graph isomorphism problem, Councils, first order logic, Polynomials, polynomial time, quantifier-free reductions, Logic, Labeling, computational complexity]
Polynomial simulations of decohered quantum computers
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Recently it has become clear, that a key issue in quantum computation is understanding how interaction with the environment, or "decoherence\
[Costs, Computational modeling, Computer simulation, Circuit simulation, automata theory, Time of arrival estimation, parallelism, phase transitions, Power system modeling, probabilistic Turing machine, density matrices, Concurrent computing, Quantum computing, decoherence, Physics computing, decohered sequential quantum computers, computational power, Polynomials, quantum computation]
Highly fault-tolerant parallel computation
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We re-introduce the coded model of fault-tolerant computation in which the input and output of a computational device are treated as words in an error-correcting code. A computational device correctly computes a function in the coded model if its input and output, once decoded, are a valid input and output of the function. In the coded model, it is reasonable to hope to simulate all computational devices by devices whose size is greater by a constant factor but which are exponentially reliable even if each of their components can fail with some constant probability. We consider fine-grained parallel computations in which each processor has a constant probability of producing the wrong output at each time step. We show that any parallel computation that runs for time t on w processors can be performed reliably on a faulty machine in the coded model using wlog/sup 0(1/)w processors and time tlog/sup 0(1)/w. The failure probability of the computation will be at most t/spl middot/exp(-w/sup 1/4 /). The codes used to communicate with our fault-tolerant machines are generalized Reed-Solomon codes and can thus be encoded and decoded in O(nlog/sup 0(1)/n) sequential time and are independent of the machine they are used to communicate with. We also show how coded computation can be used to self-correct many linear functions in parallel with arbitrarily small overhead.
[parallel computation, Computational modeling, Circuit simulation, Mathematics, Decoding, Circuit faults, coded computation, Concurrent computing, Reed-Solomon codes, Fault tolerance, fault-tolerant computation, coded model, fault tolerant computing, Error correction codes, failure probability, Protection, error-correcting code, fault-tolerant parallel computation, generalized Reed-Solomon codes]
Learning linear transformations
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We present a polynomial time algorithm to learn (in Valiant's PAC model) an arbitrarily oriented cube in n-space, given uniformly distributed sample points from it. In fact, we solve the more general problem of learning, in polynomial time, a linear (affine) transformation of a product distribution.
[linear transformations, Independent component analysis, Gaussian distribution, Mathematics, Probability distribution, learning, Valiant's PAC model, distributed sample points, polynomial time algorithm, arbitrarily oriented cube, Computer science, Collaboration, Linear algebra, Polynomials, Computer networks, Random variables, learning (artificial intelligence)]
Efficient self-testing/self-correction of linear recurrences
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors consider the problem of designing self-testers/self-correctors for functions defined by linear recurrences. They present the first complete package of efficient and simple self-testers, self-correctors, and result-checkers for such functions. The results are proved by demonstrating an efficient reduction from this problem to the problem of testing linear functions over certain matrix groups. The tools include spectral analysis of matrices over finite fields, and various counting arguments that extend known techniques. The matrix twist yields simple and efficient self-testers for all linear recurrences. They also show a technique of using convolution identities to obtain very simple self-testers and self correctors. Their techniques promise new and efficient ways of testing VLSI chips for applications in control engineering, signal processing, etc. An interesting consequence of their methods is a completely new and randomness-efficient self-tester for polynomials over finite fields and rational domains. In particular the self-tester for polynomials over rational domains overcomes a main drawback of the result of Rubinfeld and Sudan (1992)-the need for a test domain of much larger size and of much finer precision.
[matrix twist, matrix groups, linear function testing, rational domains, program testing, functions, randomness-efficient self-tester, signal processing, linear recurrences, Very large scale integration, spectral analysis, self-tester design, finite fields, Convolution, self-corrector design, Polynomials, counting arguments, efficient self-testing, control engineering, Testing, efficient self-correction, polynomials, Built-in self-test, result-checkers, Galois fields, Spectral analysis, Control engineering, Packaging, Signal processing, VLSI chip testing, convolution identities, reduction]
Load balancing and density dependent jump Markov processes
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We provide a new approach for analyzing both static and dynamic randomized load balancing strategies. We demonstrate the approach by providing the first analysis of the following model: customers arrive as a Poisson stream of rate /spl lambda//sub n/, /spl lambda/<1, at a collection of n servers. Each customer chooses some constant d servers independently and uniformly at random from the n servers, and waits for service at the one with the fewest customers. Customers are served according to the first-in first-out (FIFO) protocol, and the service time for a customer is exponentially distributed with mean 1. We call this problem the supermarket model. We wish to know how the system behaves, and in particular we are interested in the expected time a customer spends in the system in equilibrium. The model provides a good abstraction of a simple, efficient load balancing scheme in the setting where jobs arrive at a large system of parallel processors. This model appears more realistic than similar models studied previously, in that it is both dynamic and open: that is, customers arrive over time, and the number of customers is not fixed.
[Protocols, load balancing, supermarket model, service time, Predictive models, Poisson stream, Computer science, servers, Computer applications, dynamic randomized load balancing, Markov processes, Load management, parallel processors, Resource management, protocols, first-in first-out protocol, Queueing analysis, density dependent jump Markov processes]
All pairs almost shortest paths
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Let G=(V,E) be an unweighted undirected graph on n vertices. A simple argument shows that computing all distances in G with an additive one-sided error of at most 1 is as hard as Boolean matrix multiplication. Building on recent work of D. Aingworth et al. (1996), we describe an O/spl tilde/(min{n/sup 3/2/m/sup 1/2/,n/sup 7/3/}) time algorithm APASP/sub 2/ for computing all distances in G with an additive one-sided error of at most 2. The algorithm APASP/sub 2/ is simple, easy to implement, and faster than the fastest known matrix multiplication algorithm. Furthermore, for every even k>2, we describe an O/spl tilde/(min{n/sup 2-(2)/(k+2)/m/sup (2)/(k+2)/, n/sup 2+(2)/(3k-2)/}) time algorithm APASP/sub k/ for computing all distances in G with an additive one-sided error of at most k. We also give an O/spl tilde/(n/sup 2/) time algorithm APASP/sub /spl infin// for producing stretch 3 estimated distances in an unweighted and undirected graph on n vertices. No constant stretch factor was previously achieved in O/spl tilde/(n/sup 2/) time. We say that a weighted graph F=(V,E') k-emulates an unweighted graph G=(V,E) if for every u, v/spl isin/V we have /spl delta//sub G/(u,v)/spl les//spl delta//sub F/(u,v)/spl les//spl delta//sub G/(u,v)+k. We show that every unweighted graph on n vertices has a 2-emulator with O/spl tilde/(n/sup 3/2/) edges and a 4-emulator with O/spl tilde/(n/sup 4/3/) edges. These results are asymptotically tight. Finally, we show that any weighted undirected graph on n vertices has a 3-spanner with O/spl tilde/(n/sup 3/2/) edges and that such a 3-spanner can be built in O/spl tilde/(mn/sup 1/2/) time. We also describe an O/spl tilde/(n(m/sup 2/3/+n)) time algorithm for estimating all distances in a weighted undirected graph on n vertices with a stretch factor of at most 3.
[Computer science, matrix multiplication, Upper bound, vertices, matrix multiplication algorithm, Computer errors, all pairs almost shortest paths, 3-spanner, Boolean matrix multiplication, weighted undirected graph, unweighted undirected graph]
Approximating minimum-size k-connected spanning subgraphs via matching
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
An efficient heuristic is presented for the problem of finding a minimum-size k-connected spanning subgraph of a given (undirected or directed) graph G=(V,E). There are four versions of the problem, depending on whether G is undirected or directed, and whether the spanning subgraph is required to be k-node connected (k-NCSS) or k-edge connected (k-ECSS). The approximation guarantees are as follows: min-size k-NCSS of an undirected graph 1+[1/k], min-size k-NCSS of a directed graph 1+[1/k], min-size k-ECSS of an undirected graph 1+[7/k], & min-size k-ECSS of a directed graph 1+[4//spl radic/k]. The heuristic is based on a subroutine for the degree-constrained subgraph (b-matching) problem. It is simple, deterministic, and runs in time O(k|E|/sup 2/). For undirected graphs and k=2, a (deterministic) parallel NC version of the heuristic finds a 2-node connected (or a-edge connected) spanning subgraph whose size is within a factor of (1.5+/spl epsiv/) of minimum, where /spl epsiv/>0 is a constant.
[k-NCSS, graph theory, Optimized production technology, Fasteners, heuristic, k-connected spanning subgraphs, matching, Computer science, k-ECSS, Approximation algorithms, Polynomials, minimum-size, undirected graphs]
Binary space partitions for fat rectangles
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors consider the practical problem of constructing binary space partitions (BSPs) for a set S of n orthogonal, nonintersecting, two-dimensional rectangles in R/sup 3/ such that the aspect ratio of each rectangle in S is at most /spl alpha/, for some constant a /spl alpha//spl ges/1. They present an n2/sup O(/spl radic/logn)/-time algorithm to build a binary space partition of size n2/sup O(/spl radic/logn)/ for S. They also show that if m of the n rectangles in S have aspect ratios greater than /spl alpha/, they can contact a BSP of size n/spl radic/m2/sup O(/spl radic/logn)/ for S in n/spl radic/2/sup O(/spl radic/logn)/ time. The constants of proportionality in the big-oh terms are linear in log /spl alpha/. They extend these results to cases in which the input contains non-orthogonal or intersecting objects.
[aspect ratio, Costs, Military computing, binary space partitions, orthogonal nonintersecting two-dimensional rectangles, nonorthogonal objects, Engines, Computer science, Computational geometry, fat rectangles, intersecting objects, computation time, Layout, Computer graphics, hidden surface removal, Rendering (computer graphics), Hardware, proportionality constants, Pixel, computational complexity, algorithm]
Efficient approximate and dynamic matching of patterns using a labeling paradigm
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
A key approach in string processing algorithmics has been the labeling paradigm which is based on assigning labels to some of the substrings of a given string. If these labels are chosen consistently, they can enable fast comparisons of substrings. Until the first optimal parallel algorithm for suffix tree construction was given by the authors in 1994 the labeling paradigm was considered not to be competitive with other approaches. They show that this general method is also useful for several central problems in the area of string processing: approximate string matching, dynamic dictionary matching, and dynamic text indexing. The approximate string matching problem deals with finding all substrings of a text which match a pattern "approximately\
[Tree data structures, Dictionaries, pattern matching, labeling paradigm, replaced characters, Heuristic algorithms, optimal parallel algorithm, efficient dynamic pattern matching, Educational institutions, dynamic text indexing, string processing algorithmics, substrings, Parallel algorithms, dynamic dictionary matching, deleted characters, approximate string matching, inserted characters, Labeling, efficient approximate pattern matching, Pattern matching, Indexing, suffix tree construction]
Near-optimal parallel prefetching and caching
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors consider algorithms for integrated prefetching and caching in a model with a fixed-size cache and any number of backing storage devices (disks). Previously, the single disk case was considered by Cao et al. (1995). They show that the natural extension of their aggressive algorithm to the parallel disk case is suboptimal by a factor near the number of disks in the worst case. The main result is a new algorithm, reverse aggressive, with near-optimal performance in the presence of multiple disks.
[magnetic disc storage, algorithms, near-optimal performance, Costs, Prefetching, reverse aggressive algorithm, Cache storage, near-optimal parallel prefetching/caching, Scheduling algorithm, Computer science, fixed-size cache, parallel disk case, multiple disks, backing storage devices, Bandwidth, Parallel processing, Polynomials, integrated prefetching/caching, model]
Single-source unsplittable flow
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The max-flow min-cut theorem of Ford and Fulkerson is based on an even more foundational result, namely Menger's theorem on graph connectivity Menger's theorem provides a good characterization for the following single-source disjoint paths problem: given a graph G, with a source vertex s and terminals t/sub 1/,...,t/sub k/, decide whether there exist edge-disjoint s-t/sub i/ paths for i=1,...,k. We consider a natural, NP-hard generalization of this problem, which we call the single-source unsplittable flow problem. We are given a source and terminals as before; but now each terminal t/sub i/ has a demand p/sub i//spl les/1, and each edge e of G has a capacity c/sub e//spl ges/1. The problem is to decide whether one can choose a single s-t/sub i/ path for each i, so that the resulting set of paths respects the capacity constraints-the total amount of demand routed across any edge e must be bounded by the capacity c/sub e/. The main results of this paper are constant-factor approximation algorithms for three natural optimization versions of this problem, in arbitrary directed and undirected graphs. The development of these algorithms requires a number of new techniques for rounding fractional solutions to network flow problems; for two of the three problems we consider, there were no previous techniques capable of providing an approximation in the general case, and for the third, the randomized rounding algorithm of Raghavan and Thompson provides a logarithmic approximation. Our techniques are also of interest from the perspective of a family of NP-hard load balancing and machine scheduling problems that can be reduced to the single-source unsplittable flow problem.
[unsplittable flow, NP-hard, load balancing, Routing, Graph theory, capacity constraints, Scheduling algorithm, machine scheduling, Postal services, Computer science, Constraint optimization, Processor scheduling, Admission control, generalization, scheduling, max-flow min-cut, Approximation algorithms, Load management]
Optimal dynamic interval management in external memory
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors present a space- and I/O-optimal external-memory data structure for answering stabbing queries on a set of dynamically maintained intervals. The data structure settles an open problem in databases and I/O algorithms by providing the first optimal external-memory solution to the dynamic interval management problem, which is a special case of 2-dimensional range searching and a central problem for object-oriented and temporal databases and for constraint logic programming. The data structure simultaneously uses optimal linear space (that is, O(N/B) blocks of disk space) and achieves the optimal O(log/sub B/ N+T/B) I/O query bound and O(log/sub B/ N) I/O update bound, where B is the I/O block size and T the number of elements in the answer to a query. The structure is also the first optimal external data structure for a 2-dimensional range searching problem that has worst-case as opposed to amortized update bounds. Part of the data structure uses a novel balancing technique for efficient worst-case manipulation of balanced trees, which is of independent interest.
[I/O algorithms, Earth Observing System, space-optimal external-memory data structure, 2D range searching, balancing technique, storage management, worst-case balanced tree manipulation, I/O-optimal external-memory data structure, stabbing query answering, optimal I/O query bound, dynamically maintained intervals, constraint logic programming, Dynamic programming, optimal linear space, worst-case update bounds, Tree data structures, databases, Object oriented databases, Logic programming, Object oriented modeling, object-oriented databases, Data structures, optimal I/O update bound, external memory, Computer science, optimal dynamic interval management, temporal databases, Memory management, Indexing]
A general approach to dynamic packet routing with bounded buffers
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We prove a sufficient condition for the stability of dynamic packet routing algorithms. Our approach reduces the problem of steady state analysis to the easier and better understood question of static routing. We show that certain high probability and worst case bounds on the quasistatic (finite past) performance of a routing algorithm imply bounds on the performance of the dynamic version of that algorithm. Our technique is particularly useful in analyzing routing on networks with bounded buffers where complicated dependencies make standard queuing techniques inapplicable. We present several applications of our approach. In all cases we start from a known static algorithm, and modify it to fit our framework. In particular we give the first dynamic algorithm for routing on a butterfly with bounded buffers. Both the injection rate for which the algorithm is stable, and the expected time a packet spends in the system are optimal up to constant factors. Our approach is also applicable to the recently introduced adversarial input model.
[Algorithm design and analysis, Stability, Digital systems, Heuristic algorithms, packet switching, Routing, Mathematics, Steady-state, worst case bounds, static routing, general approach, steady state analysis, routing algorithm, butterfly, bounded buffers, Performance analysis, injection rate, Communication networks, sufficient condition, Queueing analysis, dynamic packet routing, stability]
Clique is hard to approximate within n/sup 1-/spl epsiv//
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The author proves that unless NP=coR, Max Clique is hard to approximate in polynomial time within a factor n/sup 1-/spl epsiv// for any /spl epsiv/>0. This is done by, for any /spl delta/>0, constructing a proof system for NP which uses /spl delta/ amortized free bits. A central lemma, which might be of independent interest, gives sufficient conditions (in the form of a certain type of agreement) for creating a global function from local functions certain local consistency conditions.
[Microwave integrated circuits, proof system, Approximation algorithms, local functions, Polynomials, polynomial time, local consistency conditions, History, Max Clique approximation, amortized free bits, computational complexity, global function]
Pseudorandom functions revisited: the cascade construction and its concrete security
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Pseudorandom function families are a powerful cryptographic primitive, yielding, in particular simple solutions for the main problems in private key cryptography. Their existence based on general assumptions (namely the existence of one-way functions) has been established. The authors investigate new ways of designing pseudorandom function families. The goal is to find constructions that are both efficient and secure, and thus eventually to bring the benefits of pseudorandom functions to practice. The basic building blocks in the design are certain limited versions of pseudorandom function families, called finite length input pseudorandom function families, for which very efficient realizations exist impractical cryptography. Thus rather than starting from one-way functions, they propose constructions of "full-fledged" pseudorandom function families from these limited ones. In particular they propose the cascade construction, and provide a concrete security analysis which relates the strength of the cascade to that of the underlying finite pseudorandom function family in a precise and quantitative way.
[Data security, pseudorandom functions, Drives, cryptography, Radio access networks, Postal services, Cryptographic protocols, finite length input pseudorandom function families, Graphics, Computer science, cryptographic primitive, private key cryptography, Concrete, Cryptography, concrete security, Message authentication, cascade construction]
Maximum likelihood decoding of Reed Solomon codes
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We present a randomized algorithm which takes as input n distinct points {(x/sub i/,y/sub i/)}/sub i=1//sup n/ from F/spl times/F (where F is a field) and integer parameters t and d and returns a list of all univariate polynomials f over F in the variable a of degree at most d which agree with the given set of points in at least t places (i.e., y/sub i/=f(x/sub i/) for at least t values of i), provided t=/spl Omega/(/spl radic/(nd)). The running time is bounded by a polynomial in n. This immediately provides a maximum likelihood decoding algorithm for Reed Solomon Codes, which works in a setting with a larger number of errors than any previously known algorithm. To the best of our knowledge, this is the first efficient (i.e., polynomial time bounded) algorithm which provides some maximum likelihood decoding for any efficient (i.e., constant or even polynomial rate) code.
[Reed Solomon codes, Hamming distance, Terminology, polynomial time bounded, Complexity theory, randomized algorithm, Galois fields, maximum likelihood decoding, Maximum likelihood decoding, Reed-Solomon codes, running time, Polynomials, Error correction codes, Argon]
Deterministic routing with bounded buffers: turning offline into online protocols
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
In this paper we present a deterministic protocol for routing arbitrary permutations in arbitrary networks. The protocol is analyzed in terms of the size of the network and the routing number of the network. Given a network H of size n, the routing number of H is defined as the maximum over all permutations /spl pi/ on [n] of the minimal number of steps to route /spl pi/ offline in H. We can show that for any network H of size n with routing number R our protocol needs O(log/sub R/ n/spl middot/R) time to route any permutation in H using only constant size edge buffers. This significantly improves all previously known results on deterministic routing. In particular our result yields optimal deterministic routing protocols for arbitrary networks with diameter /spl Omega/(n/sup /spl epsiv//) or bisection width O(n/sup 1-/spl epsiv//), /spl epsiv/>0 constant. Furthermore we can extend our result to deterministic compact routing. This yields, e.g., a deterministic routing protocol with runtime O((log n)/(log log n) R) for arbitrary bounded degree networks if only O(log n) bits are available at each node for storing routing information. Our proofs use a new protocol for routing arbitrary r/spl middot/s-relations in r-replicated s-ary Multibutterflies in optimal time O(log, n).
[Algorithm design and analysis, Multiprocessor interconnection networks, Buffer storage, multiprocessor interconnection networks, Turning, Mathematics, deterministic protocol, Computer science, Runtime, online protocols, arbitrary permutations, bounded buffers, Routing protocols, r-replicated s-ary Multibutterflies, Standards development, arbitrary networks, deterministic routing]
Verifying identities
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors provide an O/spl tilde/(n/sup 2/) time randomized algorithm to check whether a given operation f:S/spl times/S/spl rarr/S is associative (letting n=|S|). They prove this performance is optimal (up to polylogarithmic factors) even in case the operation is "cancellative". No sub-n/sup 3/ algorithm was previously known for this task. More generally they give an O(n/sup c/) time randomized algorithm to check whether a collection of c-ary operations satisfy any given "read-once" identity.
[identity verification, Error probability, c-ary operations, Automata, associative operation, optimal performance, time randomized algorithm, read-once identity, Testing, randomised algorithms]
Tree data structures for N-body simulation
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
In this paper, we study data structures for use in N-body simulation. We concentrate on the spatial decomposition tree used in particle-cluster force evaluation algorithms such as the Barnes-Hut algorithm. We prove that a k-d tree is asymptotically inferior to a spatially balanced tree. We show that the worst case complexity of the force evaluation algorithm using a k-d tree is /spl Theta/(nlog/sup 3/nlogL) compared with /spl Theta/(nlogL) for an oct-tree. (L is the separation ratio of the set of points.) We also investigate improving the constant factor of the algorithm, and present several methods which improve over the standard oct-tree decomposition. Finally, we consider whether or not the bounding box of a point set should be "tight\
[Tree data structures, Performance evaluation, particle-cluster force evaluation algorithms, Force measurement, Computational modeling, worst case complexity, k-d tree, standard oct-tree decomposition, Data structures, Extraterrestrial measurements, N-body simulation, Computer science, force evaluation algorithm, Barnes-Hut algorithm, spatially balanced tree, spatial decomposition tree, Clustering algorithms, bounding box, Large-scale systems, tree data structures, Gravity]
On the knowledge complexity of /spl Nscr//spl Pscr/
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The authors show that if a language has an interactive proof of logarithmic statistical knowledge-complexity, then it belongs to the class /spl Ascr//spl Mscr//spl cap/co-/spl Ascr//spl Mscr/. Thus, if the polynomial time hierarchy does not collapse, then /spl Nscr//spl Pscr/-complete languages do not have logarithmic knowledge complexity. Prior to this work, there was no indication that would contradict /spl Nscr//spl Pscr/ languages being proven with even one bit of knowledge. Next, they consider the relation between the error probability and the knowledge complexity of an interactive proof. They show that if the error probability /spl epsiv/(n) is less than 2/sup -3k(n)/ (where k(n) is the knowledge complexity) then the language proven has to be in the third level of the polynomial time hierarchy. In order to prove their main result, they develop an /spl Ascr//spl Mscr/ protocol for checking that a samplable distribution has a given entropy. They believe that this protocol is of independent interest.
[logarithmic statistical knowledge complexity, Protocols, Error probability, Natural languages, error probability, polynomial time hierarchy, NP-complete languages, Entropy, language, Computational complexity, interactive proof, Gain measurement, Polynomials, samplable distribution entropy, computational complexity]
The optimal path-matching problem
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We describe a common generalization of the weighted matching problem and the weighted matroid intersection problem. In this context we present results implying the polynomial-time solvability of the two problems. We also use our results to give the first strongly polynomial separation algorithm for the convex hull of matchable sets of a graph, and the first polynomial-time algorithm to compute the rank of a certain matrix of indeterminates. Our algorithmic results are based on polyhedral characterizations, and on the equivalence of separation and optimization.
[Algorithm design and analysis, equivalence, polynomial-time solvability, convex hull, Combinatorial mathematics, matrix algebra, Sufficient conditions, generalization, separation, optimization, path-matching, weighted matroid intersection, Polynomials, polynomial-time algorithm, Joining processes, Testing]
Path coloring on the mesh
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
In the minimum path coloring problem, we are given a list of pairs of vertices of a graph. We are asked to connect each pair by a colored path. Paths of the same color must be edge disjoint. Our objective is to minimize the number of colors used. This problem was raised by A. Aggarwal et al. (1994) and P. Raghavan and E. Upfal (1994) as a model for routing in all-optical networks. It is also related to questions in circuit routing. In this paper, we improve the O(ln N) approximation result of J. Kleinberg and E. Tardos (1995) for path coloring on the N/spl times/N mesh. We give an O(1) approximation algorithm to the number of colors needed, and a poly(ln ln N) approximation algorithm to the choice of paths and colors. To the best of our knowledge, these are the first sub-logarithmic bounds for any network other than trees, rings, or trees of rings. Our results are based on developing new techniques for randomized rounding. These techniques iteratively improve a fractional solution until it approaches integrality. They are motivated by the method used by F.T. Leighton, B.M. Maggs, and S.B. Rao (1994) for packet routing.
[minimum path coloring problem, Optical interconnections, Optical switches, vertices, Integrated circuit interconnections, Routing, Wavelength division multiplexing, Supercomputers, Telecommunications, randomized rounding, circuit routing, graph colouring, All-optical networks, path coloring, Approximation algorithms, Iterative algorithms, all-optical networks, mesh, packet routing]
New coding techniques for improved bandwidth utilization
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The introduction of parallel models that account for communication between processors has shown that interprocessor bandwidth is often the limiting factor in parallel computing. In this paper, we introduce a new coding technique for transmitting the XOR of carefully selected patterns of bits to be communicated which greatly reduces bandwidth requirements in some settings. This technique has broader applications. For example, we demonstrate that the coding technique has a surprising application to a simple I/O (Input/Output) complexity problem related to finding the transpose of a matrix. Our main results are developed in the PRAM(M) model, a limited bandwidth PRAM model where P processors communicate through a small globally shared memory of M bits. We provide new algorithms for the problems of sorting and permutation routing. For the concurrent read PRAM(M), as P grows with M held constant, our sorting algorithm outperforms any previous algorithm by /spl Omega/(log/sup c/ P) for any constant c. The combination of a known lower bound for sorting in the exclusive read PRAM(M) model and this algorithm implies that the concurrent read PRAM(M) is strictly more powerful than the exclusive read PRAM(M).
[PRAM(M) model, Phase change random access memory, Application software, Sorting, Computer science, interprocessor bandwidth, I/O complexity problem, coding technique, Bandwidth, bandwidth utilization, parallel models, shared memory, computational complexity]
Efficient information gathering on the Internet
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
The Internet offers unprecedented access to information. At present most of this information is free, but information providers ore likely to start charging for their services in the near future. With that in mind this paper introduces the following information access problem: given a collection of n information sources, each of which has a known time delay, dollar cost and probability of providing the needed information, find an optimal schedule for querying the information sources. We study several variants of the problem which differ in the definition of an optimal schedule. We first consider a cost model in which the problem is to minimize the expected total cost (monetary and time) of the schedule, subject to the requirement that the schedule may terminate only when the query has been answered or all sources have been queried unsuccessfully. We develop an approximation algorithm for this problem and for an extension of the problem in which more than a single item of information is being sought. We then develop approximation algorithms for a reward model in which a constant reward is earned if the information is successfully provided, and we seek the schedule with the maximum expected difference between the reward and a measure of cost. The monetary and time costs may either appear in the cost measure or be constrained not to exceed a fixed upper bound; these options give rise to four different variants of the reward model.
[Job listing service, Delay effects, Genomics, Optimal scheduling, Time measurement, cost model, Scheduling algorithm, information providers, information sources, reward model, Councils, information access problem, approximation algorithm, Cost function, Approximation algorithms, Internet, information gathering]
Short paths in expander graphs
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Graph expansion has proved to be a powerful general tool for analyzing the behavior of routing algorithms and the inter-connection networks on which they run. We develop new routing algorithms and structural results for bounded-degree expander graphs. Our results are unified by the fact that they are all based upon, and extend, a body of work: asserting that expanders are rich in short, disjoint paths. In particular, our work has consequences for the disjoint paths problem, multicommodity flow, and graph minor containment. We show: (i) A greedy algorithm for approximating the maximum disjoint paths problem achieves a polylogarithmic approximation ratio in bounded-degree expanders. Although our algorithm is both deterministic and on-line, its performance guarantee is an improvement over previous bounds in expanders. (ii) For a multicommodity flow problem with arbitrary demands on a bounded-degree expander there is a (1+/spl epsi/)-optimal solution using only flow paths of polylogarithmic length. It follows that the multicommodity flow algorithm of Awerbuch and Leighton runs in nearly linear time per commodity in expanders. Our analysis is based on establishing the following: given edge weights on an expander G, one can increase some of the weights very slightly so the resulting shortest-path metric is smooth-the min-weight path between any pair of nodes uses a polylogarithmic number of edges. (iii) Every bounded-degree expander on n nodes contains every graph with O(n/log/sup 0(1)/n) nodes and edges as a minor.
[Algorithm design and analysis, network routing, Routing, Graph theory, disjoint paths problem, inter-connection networks, Computer science, routing algorithms, Intelligent networks, graph minor containment, multicommodity flow, greedy algorithm, expander graphs, polylogarithmic approximation]
Discrepancy sets and pseudorandom generators for combinatorial rectangles
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
A common subproblem of DNF approximate counting and derandomizing RL is the discrepancy problem for combinatorial rectangles. We explicitly construct a poly(n)-size sample space that approximates the volume of any combinatorial rectangle in [n]/sup n/ to within o(1) error. The construction extends the previous techniques for the analogous hitting set problem, most notably via discrepancy preserving reductions.
[discrepancy problem, pseudorandom generators, Circuits, combinatorial rectangles, Mathematics, random number generation, Distributed computing, discrepancy sets, Computer science, DNF approximate counting, Polynomials, common subproblem, poly(n)-size sample space]
Approximate option pricing
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
As increasingly large volumes of sophisticated options are traded in world financial markets, determining a "fair" price for these options has become an important and difficult computational problem. Many valuation codes use the binomial pricing model, in which the stock price is driven by a random walk. In this model, the value of an n-period option on a stock is the expected time-discounted value of the future cash flow on an n-period stock price path. Path-dependent options are particularly difficult to value since the future cash flow depends on the entire stock price path rather than on just the final stock price. Currently such options are approximately priced by Monte Carlo methods with error bounds that hold only with high probability and which are reduced by increasing the number of simulation runs. In this paper we show that pricing an arbitrary path-dependent option is #-P hard. We show that certain types of path-dependent options can be valued exactly in polynomial time. Asian options are path-dependent options that are particularly hard to price, and for these we design deterministic polynomial-time approximate algorithms. We show that the value of a perpetual American put option (which can be computed in constant time) is in many cases a good approximation to the value of an otherwise identical n-period American put option. In contrast to Monte Carlo methods, our algorithms have guaranteed error bounds that are polynomially small (and in some cases exponentially small) in the maturity n. For the error analysis we derive large-deviation results for random walks that may be of independent interest.
[Algorithm design and analysis, perpetual American put option, Error analysis, Laboratories, Stochastic processes, stock price, Security, computational problem, Cost accounting, random walk, Monte Carlo methods, Investments, Pricing, Polynomials, polynomial time, approximate option pricing, #-P hard, Contracts, random walks, path-dependent options, error bounds, world financial markets, deterministic polynomial-time approximate algorithms, error analysis, binomial pricing model]
Spectral partitioning works: planar graphs and finite element meshes
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Spectral partitioning methods use the Fiedler vector-the eigenvector of the second-smallest eigenvalue of the Laplacian matrix-to find a small separator of a graph. These methods are important components of many scientific numerical algorithms and have been demonstrated by experiment to work extremely well. In this paper, we show that spectral partitioning methods work well on bounded-degree planar graphs and finite element meshes-the classes of graphs to which they are usually applied. While active spectral bisection does not necessarily work, we prove that spectral partitioning techniques can be used to produce separators whose ratio of vertices removed to edges cut is O(/spl radic/n) for bounded-degree planar graphs and two-dimensional meshes and O(n/sup 1/d/) for well-shaped d-dimensional meshes. The heart of our analysis is an upper bound on the second-smallest eigenvalues of the Laplacian matrices of these graphs: we prove a bound of O(1/n) for bounded-degree planar graphs and O(1/n/sup 2/d/) for well-shaped d-dimensional meshes.
[Transmission line matrix methods, Laplace equations, Particle separators, planar graphs, Vectors, Mathematics, Finite element methods, Matrix decomposition, Sparse matrices, finite element analysis, Computer science, finite element meshes, Eigenvalues and eigenfunctions, numerical algorithms, spectral partitioning, bounded-degree planar graphs]
Temporal logic and semidirect products: an effective characterization of the until hierarchy
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We reveal an intimate connection between semidirect products of finite semigroups and substitution of formulas in linear temporal logic. We use this connection to obtain an algebraic characterization of the 'until' hierarchy of linear temporal logic; the k-th level of that hierarchy is comprised of all temporal properties that are expressible by a formula of nesting depth at most k in the 'until' operator. Applying deep results from finite semigroup theory we are able to prove that each level of the until hierarchy is decidable.
[algebraic characterization, semidirect products, finite semigroups, temporal logic, Mathematics, Spatial databases, nesting depth, finite semigroup theory, Complexity theory, Computer science, Ear, Logic functions, Deductive databases, until hierarchy]
Polynomial time approximation schemes for Euclidean TSP and other geometric problems
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We present a polynomial time approximation scheme for Euclidean TSP in /spl Rfr//sup 2/. Given any n nodes in the plane and /spl epsiv/>0, the scheme finds a (1+/spl epsiv/)-approximation to the optimum traveling salesman tour in time n/sup 0(1//spl epsiv/)/. When the nodes are in /spl Rfr//sup d/, the running time increases to n(O/spl tilde/(log/sup d-2/n)//spl epsiv//sup d-1/) The previous best approximation algorithm for the problem (due to Christofides (1976)) achieves a 3/2-approximation in polynomial time. We also give similar approximation schemes for a host of other Euclidean problems, including Steiner Tree, k-TSP, Minimum degree-k, spanning tree, k-MST, etc. (This list may get longer; our techniques are fairly general.) The previous best approximation algorithms for all these problems achieved a constant-factor approximation. All our algorithms also work, with almost no modification, when distance is measured using any geometric norm (such as l/sub p/ for p/spl ges/1 or other Minkowski norms).
[Algorithm design and analysis, polynomial time approximation, Euclidean TSP, Operations research, Engineering profession, best approximation, Steiner Tree, Traveling salesman problems, computational geometry, Complexity theory, History, Euclidean problems, Minimum degree-k, k-MST, k-TSP, spanning tree, Approximation algorithms, Cost function, geometric problems, Polynomials, optimum traveling salesman tour, Testing]
The regularity lemma and approximation schemes for dense problems
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
There are two main contributions of the present paper. In the first, we use the constructive version of the Regularity Lemma to give directly simple polynomial time approximation schemes for several graph "subdivision" problems in dense graphs including the Max Cut problem, the Graph Bisection problem, the Min l-way cut problem and Graph Separator problem. Arora, Karger and Karpinski (1992) gave the first PTASs for these problems whose running time is O(n/sup o(1/e2)/). Our PTASs have running time where the exponent of n is a constant independent of e. The central point here is that the Regularity Lemma provides an explanation of why these Max-SNP hard problems turn out to be easy in dense graphs. We also give a simple PTAS for dense versions of a special case of the Quadratic Assignment Problem (QAP).
[regularity lemma, polynomial time approximation, Min l-way cut problem, Particle separators, Max-SNP hard problems, graph theory, dense graphs, Graph Separator problem, Partitioning algorithms, dense problems, Max Cut problem, Quadratic Assignment Problem, Computer science, approximation schemes, Graph Bisection problem, Polynomials, Mirrors]
A new rounding procedure for the assignment problem with applications to dense graph arrangement problems
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
We present a randomized procedure for rounding fractional perfect matchings to (integral) matchings. If the original fractional matching satisfies any linear inequality, then with high probability, the new matching satisfies that linear inequality in an approximate sense. This extends the well-known LP rounding procedure of Raghavan and Thompson (1987), which is usually used to round fractional solutions of linear programs. It also solves an open problem of Luby and Nisan (1993) ("Design an NC procedure for converting near-optimum fractional matchings to near-optimum matchings.") We use the rounding procedure to design n/sup 0(logn//spl epsiv/(2)/) time algorithms for the following: (i) an additive approximation to the 0-1 Quadratic Assignment problem (QAP); (ii) a (1+E)-approximation for "dense" instances of many well-known NP-hard problems, including (an optimization formulation of) GRAPH-ISOMORPHISM, MIN-CUT-LINEAR-ARRANGEMENT, MAX-ACYCLIC-SUBGRAPH, MIN-LINEAR-ARRANGEMENT, and BETWEENNESS. (A "dense" graph is one in which the number of edges is /spl Omega/(n/sup 2/); denseness for the other problems is defined in an analogous way).
[Operations research, assignment problem, Vectors, randomised algorithms, Constraint optimization, dense graph arrangement, linear inequality, fractional perfect matchings, rounding procedure, randomized procedure, Integer linear programming, Cost function, Approximation algorithms, Polynomials, LP rounding procedure, Contracts]
A decision procedure for unitary linear quantum cellular automata
Proceedings of 37th Conference on Foundations of Computer Science
None
1996
Linear quantum cellular automata were introduced recently as one of the models of quantum computing. A basic postulate of quantum mechanics imposes a strong constraint on any quantum machine: it has to be unitary, that is its time evolution operator has to be a unitary transformation. In this paper we give an efficient algorithm to decide if a linear quantum cellular automaton is unitary. The complexity of the algorithm is O(n(4r-3)/(r+1))=O(n/sup 4/) if the automaton has a continuous neighborhood of size r.
[complexity, linear quantum, Computational modeling, Circuit simulation, unitary, Magnetic heads, Physics, cellular automata, Quantum computing, Content addressable storage, Turing machines, Quantum cellular automata, Quantum mechanics, quantum computing, decision procedure, Polynomials]
Truly online paging with locality of reference
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
The access graph model for paging, defined by (Borodin et al., 1991) and studied in (Irani et al., 1992) has a number of troubling aspects. The access graph has to be known in advance to the paging algorithm and the memory required to represent the access graph itself may be very large. We present a truly online strongly competitive paging algorithm in the access graph model that does not have any prior information on the access sequence. We give both strongly competitive deterministic and strongly competitive randomized algorithms. Our algorithms need only O(k log n) bits of memory, where k is the number of page slots available and n is the size of the virtual address space, i.e., no more memory than needed to store the virtual translation tables for pages in memory. In fact, we can reduce this to O(k log k) bits using appropriate probabilistic data structures. We also extend the locality of reference concept captured by the access graph model to allow changes in the behavior of the underlying process. We formalize this by introducing the concept of an "extended access graph". We consider a graph parameter /spl Delta/ that captures the degree of change allowed. We study this new model and give algorithms that are strongly competitive for the (unknown) extended access graph. We can do so for almost all values of /spl Delta/ for which it is possible.
[Algorithm design and analysis, storage allocation, memory, page slots, graph theory, competitive deterministic algorithm, online paging, virtual translation tables, data structures, Performance analysis, virtual address space, probabilistic data structures, access sequence, paged storage, extended access graph, competitive randomized algorithm, competitive algorithms, Data structures, Partitioning algorithms, deterministic algorithms, randomised algorithms, Computer science, locality of reference, strongly competitive paging algorithm, access graph model]
Improved approximations for edge-disjoint paths, unsplittable flow, and related routing problems
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We present improved approximation algorithms for a family of problems involving edge-disjoint paths and unsplittable flow, and for some related routing problems. The central theme of all our algorithms is the underlying multi-commodity flow relaxation.
[unsplittable flow, network routing, graph theory, multiprocessor interconnection networks, edge-disjoint paths, multi-commodity flow relaxation, Optical fiber networks, Routing, High speed integrated circuits, Information systems, Computer science, Image motion analysis, IEL, routing problems, Bandwidth, Channel allocation, Approximation algorithms, computational complexity]
New directions in cryptography: twenty some years later (or cryptograpy and complexity theory: a match made in heaven)
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Diffie and Hellman (1976) published their fundamental paper on new directions in cryptography, in which they announced that "we stand on the brink of a revolution in cryptography". Twenty some years later, we survey some of the progress made in cryptography during this time. We especially focus on the successful interplay between complexity theory and cryptography, witnessed perhaps most vividly by the developments in interactive and probabilistic proof systems and in pseudo random number generation.
[complexity theory, Law, Laboratories, cryptography, interactive proof systems, Complexity theory, Security, random number generation, Galois fields, probabilistic proof systems, Computer science, Bibliographies, pseudo random number generation, Polynomials, theorem proving, Cryptography, Legal factors, computational complexity]
Global optimization using local information with applications to flow control
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Flow control in high speed networks requires distributed routers to make fast decisions based only on local information in allocating bandwidth to connections. While most previous work on this problem focuses on achieving local objective functions, in many cases it may be necessary to achieve global objectives such as maximizing the total flow. This problem illustrates one of the basic aspects of distributed computing: achieving global objectives using local information. Papadimitriou and Yannakakis (1993) initiated the study of such problems in a framework of solving positive linear programs by distributed agents. We take their model further, by allowing the distributed agents to acquire more information over time. We therefore turn attention to the tradeoff between the running time and the quality of the solution to the linear program. We give a distributed algorithm that obtains a (1+/spl epsiv/) approximation to the global optimum solution and runs in a polylogarithmic number of distributed rounds. While comparable in running time, our results exhibit a significant improvement on the logarithmic ratio previously obtained by Awerbuch and Azar (1994). Our algorithm, which draws from techniques developed by Luby and Nisan (1993) is considerably simpler than previous approximation algorithms for positive linear programs, and thus may have practical value in both centralized and distributed settings.
[linear programming, approximation algorithms, Distributed computing, distributed computing, High-speed networks, Bit rate, Bandwidth, logarithmic ratio, Communication system traffic control, cooperative systems, Distributed algorithms, high speed networks, distributed agents, local information, distributed routers, software agents, global optimization, quality, Computer science, bandwidth allocation, distributed algorithm, computer network management, distributed algorithms, telecommunication network routing, local objective functions, Strain control, positive linear programs, network flow control]
Buy-at-bulk network design
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
The essence of the simplest buy-at-bulk network design problem is buying network capacity "wholesale" to guarantee connectivity from all network nodes to a certain central network switch. Capacity is sold with "volume discount": the more capacity is bought, the cheaper is the price per unit of bandwidth. We provide O(log/sup 2/n) randomized approximation algorithm for the problem. This solves the open problem in Salman et al. (1997). The only previously known solutions were restricted to special cases (Euclidean graphs). We solve additional natural variations of the problem, such as multi-sink network design, as well as selective network design. These problems can be viewed as generalizations of the the Generalized Steiner Connectivity and Prize-collecting salesman (K-MST) problems. In the selective network design problem, some subset of /spl kappa/ wells must be connected to the (single) refinery, so that the total cost is minimized.
[buy-at-bulk network design, multi-sink network design, network routing, graph theory, central network switch, Switches, Petroleum, selective network design, Computer science, travelling salesman problems, connectivity, Refining, network capacity, Pricing, Bandwidth, Generalized Steiner Connectivity, Economies of scale, Cost function, K-MST, Contracts, Prize-collecting salesman, Capacity planning]
A concrete security treatment of symmetric encryption
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We study notions and schemes for symmetric (ie. private key) encryption in a concrete security framework. We give four different notions of security against chosen plaintext attack and analyze the concrete complexity of reductions among them, providing both upper and lower bounds, and obtaining tight relations. In this way we classify notions (even though polynomially reducible to each other) as stronger or weaker in terms of concrete security. Next we provide concrete security analyses of methods to encrypt using a block cipher, including the most popular encryption method, CBC. We establish tight bounds (meaning matching upper bounds and attacks) on the success of adversaries as a function of their resources.
[complexity, block cipher, Engineering profession, Drives, cryptography, symmetric encryption, plaintext attack, Security, private key, Computer science, Uniform resource locators, computational complexity5816570, matching upper bounds, Upper bound, public key cryptography, tight bounds, attacks, Concrete, Polynomials, Cryptography, Marine vehicles, concrete security]
General dynamic routing with per-packet delay guarantees of O(distance+1/session rate)
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
A central issue in the design of modern communication networks is that of providing performance guarantees. This issue is particularly important if the networks support read-time traffic such as voice and video. The most critical performance parameter to bound is the delay experienced by a packet as it travels from its source to its destination. We study dynamic routing in a connection-oriented packet-switching network. We consider a network with arbitrary topology on which a set of sessions is defined. For each session i, packets are injected at a rate r/sub i/ to follow a predetermined path of length d/sub i/. Due to limited bandwidth, only one packet at a time may advance on an edge. Session paths may overlap subject to the constraint that the total rate of sessions using any particular edge is less than 1. We address the problem of scheduling the sessions at each switch, so as to minimize worst-case packet delay and queue buildup at the switches. We show the existence of an asymptotically-optimal schedule that achieves a delay bound of O(1/r/sub i/+d/sub i/) with only constant-size queues at the switches. We also present a simple distributed algorithm that, with high probability, delivers every session-i packet to its destination within O(1/r/sub i/+d/sub i/ log(m/r/sub min/)) steps of its injection, where r/sub min/ is the minimum session rate, and m is the number of edges in the network. Our results can be generalized to (leaky-bucket constrained) bursty traffic, where session i tolerates a burst size of b/sub i/. In this case, our delay bounds become O(b/sub i//r/sub i/+d/sub i/) and O(b/sub i//r/sub i/+d/sub i/ log(m/r/sub min/)), respectively.
[queue buildup, packet switching, Switches, Telecommunication traffic, communication complexity, Delay, Network topology, arbitrary topology, telecommunication networks, performance guarantees, Bandwidth, Traffic control, scheduling, Communication networks, Distributed algorithms, per-packet delay, Packet switching, packet-switching, communication networks, Routing, telecommunication network routing, dynamic routing, packet delay, bursty traffic, delay bounds]
Deterministic superimposed coding with applications to pattern matching
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
A superimposed code is a set of binary vectors having the property that no vector is contained in a boolean sum (i.e. bitwise OR) of a small number of others. Such codes are used in information retrieval for constructing so-called signature files; they also have applications in other areas. In this paper we introduce a new notion of data-dependent superimposed codes and give a deterministic algorithm for constructing short such codes. We then show that these codes can be used to achieve an almost optimal de-randomization of several pattern matching algorithms, including the almost-linear algorithm for tree pattern matching developed recently. Thus, we give the first almost-linear time deterministic algorithms for these problems.
[Algorithm design and analysis, pattern matching, information retrieval, Information retrieval, almost-linear time, Application software, deterministic algorithms, Computer science, Upper bound, Databases, Computer architecture, superimposed code, Hardware, tree pattern matching, Pattern matching, Random number generation]
No feasible interpolation for TC/sup 0/-Frege proofs
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
The interpolation method has been one of the main tools for proving lower bounds for propositional proof systems. Loosely speaking, if one can prove that a particular proof system has the feasible interpolation property, then a generic reduction can (usually) be applied to prove lower bounds for the proof system, sometimes assuming a (usually modest) complexity-theoretic assumption. In this paper, we show that this method cannot be used to obtain lower bounds for Frege systems, or even for TC/sup 0/-Frege systems. More specifically, we show that unless factoring is feasible, neither Frege nor TC/sup 0/-Frege has the feasible interpolation property. In order to carry out our argument, we show how to carry out proofs of many elementary axioms/theorems of arithmetic in polynomial-size TC/sup 0/-Frege. In particular, we show how to carry out the proof for the Chinese Remainder Theorem, which may be of independent interest. As a corollary, we obtain that TC/sup 0/-Frege as well as any proof system that polynomially simulates it, is not automatizable (under a hardness assumption).
[proof systems, feasible interpolation, Circuits, Chinese Remainder Theorem, Large scale integration, Business process re-engineering, lower bounds, Radio access networks, Computer science, Interpolation, hardness assumption, Boolean functions, interpolation, Polynomials, theorem proving, complexity-theoretic assumption, Artificial intelligence, Arithmetic, computational complexity, TC/sup 0/-Frege proofs]
Improved approximations for shallow-light spanning trees
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We consider the bicriteria optimization problem of computing a shallow-light tree. Given a directed graph with two unrelated cost functions defined on its edges: weight and length, and a designated root vertex, the goal is to find a minimum weight spanning tree such that the path lengths from its root to the rest of the vertices are bounded. This problem has several applications in network and VLSI design, and information retrieval. We give a polynomial time algorithm for finding a spanning tree whose weight is O(log |V|) times the weight of an optimal shallow-light tree, where the path lengths from the root to the rest of the vertices are at most twice the given bounds. We extend our technique to handle two variants of the problem: one in which the length bound is given on the average length of a path from the root to a vertex, and another tricriteria budgeted version. Our paper provides the first non-trivial approximation factors for directed graphs, and improves on previous results for undirected graphs.
[Weight measurement, bicriteria optimization, Integrated circuit interconnections, information retrieval, Very large scale integration, Length measurement, VLSI design, polynomial time algorithm, Delay, Computer science, shallow-light spanning trees, optimisation, Tree graphs, directed graphs, directed graph, Tin, Cost function, Polynomials, non-trivial approximation factors, computational complexity]
An improved algorithm for quantifier elimination over real closed fields
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We give a new algorithm for quantifier elimination in the first order theory of real closed fields that improves the complexity of the best known algorithm for this problem till now. Unlike previously known algorithms the combinatorial part of the complexity of this new algorithm is independent of the number of free variables. Moreover, under the assumption that each polynomial in the input depend only on a constant number of the free variables, the algebraic part of the complexity can also be made independent of the number of free variables. This new feature of our algorithm allows us to obtain a new algorithm for a variant of the quantifier elimination problem. We give an almost optimal algorithm for this new problem, which we call the uniform quantifier elimination problem and apply it to solve a problem arising in the field of constraint databases. No algorithm with reasonable complexity bound was known for this latter problem till now. We also point out interesting logical consequences of this algorithmic result, concerning the expressive power of a constraint query language over the reals. Moreover, our improved algorithm for performing quantifier elimination immediately leads to improved algorithms for several problems for which quantifier elimination is a basic step, for example, the problem of computing the closure of a given semi-algebraic set.
[complexity, Costs, History, Database languages, constraint databases, uniform quantifier elimination problem, Polynomials, quantifier elimination, real closed fields, complexity bound, Fellows, Arithmetic, computational complexity]
Nearly linear time approximation schemes for Euclidean TSP and other geometric problems
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We present a randomized polynomial time approximation scheme for Euclidean TSP in R/sup 2/ that is substantially more efficient than our earlier scheme (1996) (and the scheme of Mitchell (1996)). For any fixed c>1 and any set of n nodes in the plane, the new scheme finds a (1+1/c)-approximation to the optimum traveling salesman tour in O(n(logn)/sup O(c)/) time. (Our earlier scheme ran in n/sup O(C)/ time.) For points in R/sup d/ the algorithm runs in O(n(logn)/sup (O(/spl radic/dc)/d-1)) time. This time is polynomial (actually nearly linear) for every fixed c, d. Designing such a polynomial-time algorithm was an open problem (our earlier algorithm (1996) ran in superpolynomial time for d/spl ges/3). The algorithm generalizes to the same set of Euclidean problems handled by the previous algorithm, including Steiner Tree, /spl kappa/-TSP, /spl kappa/-MST, etc, although for /spl kappa/-TSP and /spl kappa/-MST the running time gets multiplied by /spl kappa/. We also use our ideas to design nearly-linear time approximation schemes for Euclidean versions of problems that are known to be in P, such as Minimum Spanning Tree and Min Cost Perfect Matching. All our algorithms can be derandomized, though the running time then increases by O(n/sup d/) in R/sup d/. They also have simple parallel implementations (say, in NC/sup 2/).
[Algorithm design and analysis, Euclidean TSP, Engineering profession, Steiner Tree, Traveling salesman problems, computational geometry, Linear programming, randomized polynomial time approximation, Euclidean problems, Radio access networks, randomised algorithms, travelling salesman problems, nearly-linear time approximation, Approximation algorithms, Cost function, geometric problems, Polynomials, Libraries, randomized, optimum traveling salesman tour, parallel implementations, Testing, computational complexity]
Two Decades of Temporal Logic: Achievements and Challenges
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
false
[Computer science, Computational modeling, Embedded system, Automata, Writing, Control systems, Mathematics, Robustness, Maintenance, Logic]
Reliable cellular automata with self-organization
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
In a noisy cellular automaton, even if it is infinite, it is non-trivial to keep a bit of information for more than a constant number of steps. A clever solution in 2 dimensions has been applied to a simple 3-dimensional fault-tolerant cellular automaton. This technique did not solve the following problems: remembering a bit of information in 1 dimension; computing in dimensions lower than 3, or with non-synchronized transitions. With a more complex technique using a hierarchy of simulations, we construct an asynchronous one-dimensional reliable cellular automaton, which is also "self-organizing". This means that if the input information has constant size, the initial configuration can be homogenous: the hierarchy organizes itself. An application to information storage in positive-temperature Gibbs states is also given.
[Error probability, self-organization, error-correction, reliability, cellular automata, Gibbs states, Concurrent computing, Fault tolerance, probabilistic automata, Storage automation, probabilistic cellular automata, noisy cellular automaton, fault tolerance, Computational modeling, Redundancy, fault-tolerant cellular automaton, interacting particle system, self-adjusting systems, Circuit faults, Computer science, Integrated circuit noise, Automata, renormalization, self organization, fault tolerant computing, ergodicity]
Contention resolution with guaranteed constant expected delay
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We study contention resolution in multiple-access channels such as the Ethernet. Under a stochastic model of continuous packet generation from a set of n processors, we construct a protocol which guarantees constant expected delay for generation rates up to a fixed constant /spl lambda//sub 0/<1. Previous protocols which are stable for constant arrival rates do not guarantee constant expected delay. The two protocols that achieved results closest to this are one by Raghavan and Upfal, which only guarantees logarithmic (in n) expected delay, and one by Paterson and Srinivasan, which only guarantees constant expected delay with high probability. (In the latter protocol, there is a non-zero probability that the initial clock synchronization might fail and cause the expected delay to grow unboundedly.) Although those protocols do not guarantee constant expected delay, we have used ideas from them in the construction of our protocol, which does guarantee constant expected delay. We achieve our results using a technique called Robust Synchronization which is applied periodically in our protocol. The introduction of this technique and the analysis of this technique are the main contributions of the paper.
[Protocols, Ethernet networks, Stochastic processes, contention resolution, local area networks, multiple-access channels, Synchronization, Delay, Computer science, Algorithms, Robust Synchronization, Ethernet, constant expected delay, Robustness, protocols, Contracts, Clocks]
An improved worst-case to average-case connection for lattice problems
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We improve a connection of the worst-case complexity and the average-case complexity of some well-known lattice problems. This fascinating connection was first discovered by Ajtai (1995). We improve the exponent of this connection from 8 to 3.5+/spl epsiv/.
[Lattices, worst-case complexity, Security, Cryptographic protocols, Geometry, Computer science, Bridges, group theory, average-case complexity, Microwave integrated circuits, lattice problems, Gaussian processes, discrete additive subgroup, Polynomials, Cryptography, computational complexity]
Beyond the flow decomposition barrier
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We introduce a new approach to the maximum flow problem. This approach is based on assigning arc lengths based on the residual flow value and the residual are capacities. Our approach leads to an O(min(n/sup 2/3/, m/sup 1/2/)m log(n/sup 2//m) log U) time bound for a network with n vertices, m arcs, and integral arc capacities in the range [1,...,U]. This is a fundamental improvement over the previous time bounds. We also improve bounds for the Gomory-Hu tree problem, the parametric flow problem, and the approximate s-t cut problems.
[combinatorial mathematics, automata theory, Transportation, computational geometry, flow decomposition barrier, History, computational complexity5816537, arc lengths, Tree graphs, Integral equations, National electric code, algorithm theory, Gomory-Hu tree problem, Polynomials, Books, learning (artificial intelligence), Tree data structures, programming theory, time bounds, cryptography, time bound, parametric flow problem, maximum flow problem, Manipulator dynamics, computational complexity]
Optimal-resilience proactive public-key cryptosystems
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We introduce new efficient techniques for sharing cryptographic functions in a distributed dynamic fashion. These techniques dynamically and securely transform a distributed function (or secret sharing) representation between t-out-of-l (polynomial sharing) and t-out-of-t (additive sharing). We call the techniques poly-to-sum and sum-to-poly, respectively. Employing these techniques, we solve a number of open problems in the area of cryptographic function sharing. We design a threshold function sharing scheme with proactive security for general functions with a "homomorphic property" (a class which includes all RSA variants and Discrete logarithm variants). The sharing has "optimal resilience" (server redundancy) and enables computation of the function by the servers assuring high availability, security and efficiency. Proactive security enables function sharing among servers while tolerating an adversary which is mobile and which dynamically corrupts and abandons servers (and perhaps visits all of them over the lifetime of the system, as long as the number of corruptions (faults) is bounded within a time period). Optimal resilience assures that the adversary can corrupt any minority of servers at any time-period.
[Availability, Additives, Protocols, Redundancy, distributed function, Resilience, public-key cryptosystems, threshold function sharing, proactive security, cryptographic function sharing, public key cryptography, Public key, Public key cryptography, Robustness, Polynomials, cryptographic functions, secret sharing, Protection]
Randomized allocation processes
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We investigate various randomized processes allocating balls into bins that arise in applications in dynamic resource allocation and on-line load balancing. We consider the scenario when m balls arriving sequentially are to be allocated into n bins on-line and without using a global controller. Traditionally, the main aim of allocation processes is to place the balls into bins to minimize the maximum load in bins. However in many applications it is equally important to minimize the number of trails performed by the balls (the allocation time). We study adaptive allocation schemes that achieve optimal tradeoffs between the maximum load, the maximum allocation time, and the average allocation time. We investigate allocation processes that may reallocate the balls. We provide a tight analysis of the maximum load of processes that during placing a new ball may reassign the balls in up to d randomly chosen bins. We study infinite processes, in which in each step a random ball is removed and a new ball is placed according to some scheduling rule. We present a novel approach that establishes a tight estimation of the time needed for the infinite process to be in the state near to its equilibrium. Finally, we provide a tight analysis of the maximum load of the off-line process in which each ball may be placed into one of d randomly chosen bins. We apply this result to competitive analysis of on-line load balancing processes.
[System testing, Protocols, dynamic resource allocation, World Wide Web, Mathematics, HTML, on-line load balancing, randomised algorithms, Computer science, Network servers, resource allocation, tight estimation, allocation processes, Load management, time, Resource management, State estimation, randomized, computational complexity, competitive analysis]
A complete promise problem for statistical zero-knowledge
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We present a complete promise problem for SZK, the class of languages possessing statistical zero-knowledge proofs (against an honest verifier). The problem is to decide whether two efficiently samplable distributions are either statistically close or far apart. This characterizes SZK with no reference to interaction or zero-knowledge. From this theorem and its proof we are able to establish several other results about SZK, knowledge complexity, and efficiently samplable distributions.
[Protocols, samplable distributions, complexity classes, statistical zero-knowledge proofs, Mathematics, Distributed computing, Computer science, statistical zero-knowledge, knowledge complexity, honest verifier, complete promise problem, Polynomials, Tires, theorem proving, Cryptography, computational complexity]
Exploiting locality for data management in systems of limited bandwidth
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
This paper deals with data management in computer systems in which the computing nodes are connected by a relatively sparse network. We consider the problem of placing and accessing a set of shared objects that are read and written from the nodes in the network. These objects are, e.g., global variables in a parallel program, pages or cache lines in a virtual shared memory system, shared files in a distributed file system, or pages in the World Wide Web. A data management strategy consists of a placement strategy that maps the objects (possibly dynamically and with redundancy) to the nodes, and an access strategy that describes how reads and writes are handled by the system (including the routing). We investigate static and dynamic data management strategies.
[data management, limited bandwidth, distributed file system, distributed processing, Delay, shared files, Intelligent networks, storage management, File systems, Bandwidth, shared memory systems, Computer networks, Contracts, computing nodes, shared objects, Routing, virtual shared memory system, placement strategy, access strategy, data management strategy, Computer science, Computer network management, Web sites, parallel program, relatively sparse network]
Tight bounds for depth-two superconcentrators
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We show that the minimum size of a depth-two N-superconcentrator is /spl Theta/(Nlog/sup 2/N/loglogN). Before this work, optimal bounds were known for all depths except two. For the upper bound, we build superconcentrators by putting together a small number of disperser graphs; these disperser graphs are obtained using a probabilistic argument. We present two different methods for showing lower bounds. First, we show that superconcentrators contain several disjoint disperser graphs. When combined with the lower bound for disperser graphs due to Kovari, Sos and Turan, this gives an almost optimal lower bound of /spl Omega/(N(log N/loglog N)/sup 2/) on the size of N-superconcentrators. The second method, based on the work of Hansel (1964), gives the optimal lower bound. The method of the Kovari, Sos and Turan can be extended to give tight lower bounds for extractors, both in terms of the number of truly random bits needed to extract one additional bit and in terms of the unavoidable entropy loss in the system. If the input is an n-bit source with min-entropy /spl kappa/ and the output is required to be within a distance of E from uniform distribution, then to extract even a constant number of additional bits, one must invest at least log(n-/spl kappa/)+2 log(1//spl epsiv/)-O(1) truly random bits; to obtain m output bits one must invest at least m-/spl kappa/+2 log(1//spl epsiv/)-O(1). Thus, there is a loss of 2 log(1//spl epsiv/) bits during the extraction. Interestingly in the case of dispersers this loss in entropy is only about loglog(1//spl epsiv/).
[Art, optimal bounds, superconcentrator, edges, disperser graphs, Entropy, Graph theory, depth-two superconcentrators, Computer science, Upper bound, entropy, tight bounds, directed graphs, directed graph, entropy loss, Error correction codes, Bipartite graph, computational complexity]
Succinct representation of balanced parentheses, static trees and planar graphs
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We consider the implementation of abstract data types for the static objects: binary tree, rooted ordered tree and balanced parenthesis expression. Our representations use an amount of space within a lower order term of the information theoretic minimum and support, in constant time, a richer set of navigational operations than has previously been considered in similar work. In the case of binary trees, for instance, we can move from a node to its left or right child or to the parent in constant time while retaining knowledge of the size of the subtree at which we are positioned. The approach is applied to produce succinct representation of planar graphs in which one can test adjacency in constant time.
[Tree data structures, Costs, binary tree, Navigation, abstract data types, planar graphs, Inspection, adjacency, Encoding, rooted ordered tree, Computer science, Jacobian matrices, Tree graphs, balanced parentheses, Binary trees, static trees, tree data structures, succinct representation, Testing]
Making nondeterminism unambiguous
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We show that in the context of nonuniform complexity, nondeterministic logarithmic space bounded computation can be made unambiguous. An analogous result holds for the class of problems reducible to context-free languages. In terms of complexity classes, this can be stated as: NL/poly=UL/poly LogCFL/poly=UAuxPDA(log n, n/sup O(1)/)/poly.
[Computer science, Microwave integrated circuits, nondeterministic logarithmic space bounded computation, context-free languages, complexity classes, nonuniform complexity, Computer networks, Complexity theory, Switching circuits, computational complexity, nondeterminism]
Hamiltonian cycles in solid grid graphs
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
A grid graph is a finite node induced subgraph of the infinite two dimensional integer grid. A solid grid graph is a grid graph without holes. For general grid graphs, the Hamiltonian cycle problem is known to be NP complete. We give a polynomial time algorithm for the Hamiltonian cycle problem in solid grid graphs, resolving a longstanding open question posed by A. Itai et al. (1982). In fact, our algorithm can identify Hamiltonian cycles in quad quad graphs, a class of graphs that properly includes solid grid graphs.
[Algorithm design and analysis, Strips, NP complete, Hamiltonian cycles, Merging, graph theory, Educational institutions, polynomial time algorithm, Computer science, infinite two dimensional integer grid, graphs, finite node induced subgraph, quad quad graphs, Solids, Polynomials, solid grid graphs, computational complexity]
Nearly tight bounds on the learnability of evolution
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Evolution is often modeled as a stochastic process which modifies DNA. One of the most popular and successful such processes are the Cavender-Farris (CF) trees, which are represented as edge weighted trees. The Phylogeny Construction Problem is that of, given /spl kappa/ samples drawn from a CF tree, output a CF tree which is close to the original. Each CF tree naturally defines a random variable, and the gold standard for reconstructing such trees is the maximum likelihood estimator of this variable. This approach is notoriously computationally expensive. We show that a very simple algorithm, which is a variant on one of the most popular algorithms used by practitioners, converges on the true tree at a rate which differs from the optimum by a constant. We do this by analyzing upper and lower bounds for the convergence rate of learning very simple CF trees, and then show that the learnability of each CF tree is sandwiched between two such simpler trees. Our results rely on the fact that, if the right metric is used, the likelihood space of CF trees is smooth.
[Stochastic processes, random variable, Mathematics, Phylogeny, edge weighted trees, maximum likelihood estimation, Convergence, maximum likelihood estimator, Evolution (biology), CF tree, stochastic processes, learning (artificial intelligence), nearly tight bounds, Phylogeny Construction Problem, Maximum likelihood estimation, trees (mathematics), stochastic process, genetic algorithms, computationally expensive, evolution learnability, Computer science, DNA, Morphology, Random variables, convergence rate, Cavender-Farris trees]
Replication is not needed: single database, computationally-private information retrieval
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We establish the following, quite unexpected, result: replication of data for the computational private information retrieval problem is not necessary. More specifically, based on the quadratic residuosity assumption, we present a single database, computationally private information retrieval scheme with O(n/sup /spl epsiv//) communication complexity for any /spl epsiv/>0.
[computational private information retrieval problem, Data privacy, information retrieval, Information retrieval, Complexity theory, History, Indexes, communication complexity, database management systems, computationally private information retrieval, Postal services, Computer science, Upper bound, Databases, data replication, quadratic residuosity assumption, Polynomials]
The computational complexity of knot and link problems
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We consider the problem of deciding whether a polygonal knot in 3-dimensional Euclidean space is unknotted (that is, whether it is capable of being continuously deformed without self-intersection so that it lies in a plane). We show that this problem, UNKNOTTING PROBLEM, is in NP. We also consider the problem, SPLITTING PROBLEM, of determining whether two or more such polygons can be split (that is, whether they are capable of being continuously deformed without self-intersection so that they occupy both sides of a plane without intersecting it), and show that it also is in NP. Finally, we show that the problem of determining the genus of a polygonal knot (a generalization of the problem of determining whether it is unknotted) is in PSPACE.
[NP, Piecewise linear techniques, SPLITTING PROBLEM, topology, computational geometry, link, Mathematics, Topology, 3-dimensional Euclidean space, PSPACE, Computational complexity, Computer science, knot, polygonal knot, computational complexity, UNKNOTTING PROBLEM]
Deciding properties of polynomials without factoring
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
The polynomial time algorithm of Lenstra, Lenstra, and Lovasz (1982) for factoring integer polynomials and variants thereof have been widely used to show that various computational problems in number theory have polynomial time solutions. Among them is the problem of factoring polynomials over algebraic number fields, which is used itself as a major subroutine for several other algorithms. Although a theoretical breakthrough, algorithms based on factorization of polynomials are notoriously slow and hard to implement, with running times ranging between O(n/sup 12/) and O(n/sup 18/) depending on which variant of the lattice basis reduction is used. Here, n is an upper bound for the maximum of the degrees and the bit-lengths of the coefficients of the polynomials involved. On the other hand, in many situations one does not need the full power of factorization, so one may ask whether there exist faster algorithms in these cases. In this paper we develop more efficient Monte Carlo algorithms to decide certain properties of roots of integer polynomials, without factoring them. Such problems arise, e.g., when solving systems of algebraic equations. Our methods applied to this situation thus give information about the solutions of such systems of equations.
[Algorithm design and analysis, Monte Carlo algorithms, factoring polynomials, polynomials, Lattices, Galois fields, polynomial time algorithm, Equations, algebraic number fields, Computer science, Upper bound, Monte Carlo methods, algebraic equations, algorithm theory, Approximation algorithms, Polynomials, Testing, number theory, computational complexity]
The competitive analysis of risk taking with applications to online trading
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Competitive analysis is concerned with minimizing a relative measure of performance. When applied to financial trading strategies, competitive analysis leads to the development of strategies with minimum relative performance risk. This approach is too inflexible. Many investors are interested in managing their risk: they may be willing to increase their risk for some form of reward. They may also have some forecast of the future. We extend competitive analysis to provide a framework in which investors can develop optimal trading strategies based on their risk tolerance and forecast. We first define notions of risk and reward that are smooth extensions of classical competitive analysis. We then illustrate our ideas using the ski-rental problem. Finally, we analyze a financial game, the unidirectional conversion problem. In particular, we present an optimal risk-tolerant algorithm for the forecast that prices will reach a certain level at some point during the game, and give numerical results of the investor's reward for making such a forecast.
[Algorithm design and analysis, Probability distribution, unidirectional conversion problem, financial trading, Investments, Performance analysis, financial data processing, stock markets, financial game, online trading, risk taking, risk management, ski-rental problem, electronic trading, game theory, competitive algorithms, investment, optimal trading strategies, Risk analysis, Application software, risk-tolerant algorithm, Computer science, Exchange rates, performance, minimization, Optimal control, minimum relative performance risk, Risk management, minimisation, competitive analysis]
Improved approximation algorithms for unsplittable flow problems
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
In the single-source unsplittable flow problem we are given a graph G, a source vertex s and a set of sinks t/sub 1/, ..., t/sub k/ with associated demands. We seek a single s-t/sub i/ flow path for each commodity i so that the demands are satisfied and the total flow routed across any edge e is bounded by its capacity c/sub e/. The problem is an NP-hard variant of max flow and a generalization of single-source edge-disjoint paths with applications to scheduling, load balancing and virtual-circuit routing problems. In a significant development, Kleinberg gave recently constant-factor approximation algorithms for several natural optimization versions of the problem. In this paper we give a generic framework, that yields simpler algorithms and significant improvements upon the constant factors. Our framework, with appropriate subroutines applies to all optimization versions previously considered and treats in a unified manner directed and undirected graphs.
[Costs, NP-hard, Engineering profession, load balancing, graph theory, Parallel machines, single-source, Routing, Educational institutions, unsplittable flow problems, approximation algorithms, Concurrent computing, virtual-circuit routing, optimization versions, Processor scheduling, directed graphs, scheduling, Approximation algorithms, Load management, undirected graphs, Testing, computational complexity]
A 2-approximation algorithm for the directed multiway cut problem
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
A directed multiway cut separates a set of terminals s/sub 1/,...,s/sub /spl kappa// in a directed capacitated graph G=(V, E). Finding a minimum capacity directed multiway cut is an NP-complete problem. We give a polynomial-time algorithm that achieves an approximation factor of 2 for this problem. This improves the result of Garg, Vazirani and Yannakakis (1994) who gave an algorithm that achieves an approximation factor of 2 log /spl kappa/. Our approximation algorithm uses a novel technique for relaxing a multiway flow function in order to find a directed multiway cut. It also implies that the integrality gap of the linear program for the directed multiway cut problem is at most 2.
[Algorithm design and analysis, multiway flow function, NP-complete, NP-complete problem, Computer science, approximation factor, directed graphs, Ear, directed multiway cut problem, 2-approximation algorithm, Approximation algorithms, Polynomials, polynomial-time algorithm, Marine vehicles, computational complexity]
Optimal suffix tree construction with large alphabets
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
The suffix tree of a string is the fundamental data structure of combinatorial pattern matching. Weiner (1973), who introduced the data structure, gave an O(n)-time algorithm for building the suffix tree of an n-character string drawn from a constant size alphabet. In the comparison model, there is a trivial /spl Omega/(n log n)-time lower bound based on sorting, and Weiner's algorithm matches this bound trivially. For integer alphabets, a substantial gap remains between the known upper and lower bounds, and closing this gap is the main open question in the construction of suffix trees. There is no super-linear lower bound, and the fastest known algorithm was the O(n log n) time comparison based algorithm. We settle this open problem by closing the gap: we build suffix trees in linear time for integer alphabet.
[Tree data structures, pattern matching, large alphabets, Engineering profession, integer alphabets, Buildings, data structure, Data structures, World Wide Web, tree data structures5816551, encoding, deterministic algorithms, integer alphabet, Sorting, Computer science, Upper bound, combinatorial pattern matching, sorting, suffix tree, Labeling, Pattern matching, computational complexity]
Weak random sources, hitting sets, and BPP simulations
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We show how to simulate any BPP algorithm in polynomial time using a weak random source of min-entropy r/sup /spl gamma// for any /spl gamma/>0. This follows from a more general result about sampling with weak random sources. Our result matches an information-theoretic lower bound and solves a question that has been open for some years. The previous best results were a polynomial time simulation of RP (Saks et al., 1995) and a n(log/sup (k)/n)-time simulation of BPP for fixed k (Ta-Shma, 1996). Departing significantly from previous related works, we do not use extractors; instead we use the OR-disperser of (Saks et al., 1995) in combination with a tricky use of hitting sets borrowed from Andreev et al. (1996). Of independent interest is our new (simplified) proof of the main result of Andreev et al., (1996). Our proof also gives some new hardness/randomness trade-offs for parallel classes.
[BPP simulations, sampling, parallel classes, min-entropy, Entropy, lower bound, randomised algorithms, hitting sets, weak random sources, Sampling methods, weak random source, Polynomials, polynomial time, Bipartite graph, Testing, computational complexity]
On the power of quantum finite state automata
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
In this paper, we introduce 1-way and 2-way quantum finite state automata (1qfa's and 2qfa's), which are the quantum analogues of deterministic, nondeterministic and probabilistic 1-way and 2-way finite state automata. We prove the following facts regarding 2qfa's. 1. For any /spl epsiv/>0, there is a 2qfa M which recognizes the non-regular language L={a/sup m/b/sup m/|m/spl ges/1} with (one-sided) error bounded by E, and which halts in linear time. Specifically, M accepts any string in L with probability 1 and rejects any string not in L with probability at least 1-/spl epsiv/. 2. For every regular language L, there is a reversible (and hence quantum) 2-way finite state automaton which recognizes L and which runs in linear time. In fact, it is possible to define 2qfar's which recognize the non-context-free language {a/sup m/b/sup m/c/sup m/|m/spl ges/1}, based on the same technique used for 1. Consequently, the class of languages recognized by linear time, bounded error 2qfa's properly includes the regular languages. Since it is known that 2-way deterministic, nondeterministic and polynomial expected time, bounded error probabilistic finite automata can recognize only regular languages, it follows that 2qfa's are strictly more powerful than these "classical" models. In the case of 1-way automata, the situation is reversed. We prove that the class of languages recognizable by bounded error 1qfa's is properly contained in the class of regular languages.
[quantum finite state automata, Error probability, finite automata, Computational modeling, Circuits, regular languages, finite state automata, finite state machines, database theory, Computer science, query processing, Quantum computing, Turing machines, Physics computing, Automata, Automatic control, Polynomials, non-regular language, constraint handling, formal languages5816544, computational complexity]
Satisfiability Coding Lemma
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We present and analyze two simple algorithms for finding satisfying assignments of /spl kappa/-CNFs (Boolean formulae in conjunctive normal form with at most /spl kappa/ literals per clause). The first is a randomized algorithm which, with probability approaching 1, finds a satisfying assignment of a satisfiable /spl kappa/-CNF formula F in time O(n/sup 2/|F|2/sup n-n//spl kappa//). The second algorithm is deterministic, and its running time approaches 2/sup n-n/2/spl kappa// for large n and /spl kappa/. The randomized algorithm is the best known algorithm for /spl kappa/>3; the deterministic algorithm is the best known deterministic algorithm for /spl kappa/>4. We also show an /spl Omega/(n/sup 1/4/2/sup /spl radic/n/) lower bound on the size of depth 3 circuits of AND and OR gates computing the parity function. This bound is tight up to a constant factor. The key idea used in these upper and lower bounds is what we call the Satisfiability Coding Lemma. This basic lemma shows how to encode satisfying solutions of a /spl kappa/-CNF succinctly.
[Algorithm design and analysis, satisfying assignments, Satisfiability Coding Lemma, Circuit analysis computing, Boolean formulae, Input variables, computability, Boolean algebra, randomized algorithm, deterministic algorithm, Galois fields, deterministic algorithms, conjunctive normal form, Switching circuits, randomised algorithms, Computer science, Upper bound, satisfiability, Polynomials, satisfying assignment, computational complexity]
Computable obstructions to wait-free computability
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Effectively computable obstructions are associated to a distributed decision task (/spl Iscr/,/spl Oscr/,/spl Delta/) in the asynchronous, wait-free, read-write shared-memory model. The key new ingredient of this work is the association of a simplicial complex /spl Tscr/, the task complex, to the input-output relation d. The task determines a simplicial map /spl alpha/ from /spl Tscr/ to the input complex /spl Iscr/. The existence of a wait-free protocol solving the task implies that the map /spl alpha//sub */ induced in homology must surject, and thus elements of H/sub */(/spl Iscr/) that are not in the image of /spl alpha//sub */, are obstructions to solvability of the task. These obstructions are effectively computable when using suitable homology theories, such as mod-2 simplicial homology. We also extend Herlihy and Shavit's Theorem on Spans to the case of protocols that are anonymous relative to the action of a group, provided the action is suitably rigid. For such rigid actions, the quotients of the input complex and the task complex by the group are well-behaved, and obstructions to anonymous solvability of the task are obtained analogously, using the homology of the quotient complexes.
[Heart, Protocols, shared-memory model, computable obstructions, distributed decision task, computability, Computer crashes, Topology, asynchronous distributed systems, Distributed computing, parallel programming, wait-free computability, Computer science, Geometry, Fault tolerance, wait-free protocol, fault-tolerant computation, Fault tolerant systems, Writing, shared memory systems, fault tolerant computing]
Alternating-time temporal logic
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Temporal logic comes in two varieties: linear-time temporal logic assumes implicit universal quantification over all paths that are generated by system moves; branching-time temporal logic allows explicit existential and universal quantification over all paths. We introduce a third, more general variety of temporal logic: alternating-time temporal logic offers selective quantification over those paths that are possible outcomes of games, such as the game in which the system and the environment alternate moves. While linear-time and branching-time logics are natural specification languages for closed systems, alternating-time logics are natural specification languages for open systems. For example, by preceding the temporal operator "eventually" with a selective path quantifier, we can specify that in the game between the system and the environment, the system has a strategy to reach a certain state. Also the problems of receptiveness, realizability, and controllability can be formulated as model-checking problems for alternating-time formulas.
[Costs, alternating-time temporal logic, Engineering profession, open systems, Debugging, temporal logic, receptiveness, Specification languages, Power system modeling, Information science, controllability, Open systems, specification languages, Controllability, selective quantification, outcomes of games, model-checking problems, Logic, Contracts, realizability]
Path coupling: A technique for proving rapid mixing in Markov chains
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
The main technique used in algorithm design for approximating #P-hard counting problems is the Markov chain Monte Carlo method. At the heart of the method is the study of the convergence (mixing) rates of particular Markov chains of interest. In this paper we illustrate a new approach to the coupling technique, which we call path coupling, for bounding mixing rates. Previous applications of coupling have required detailed insights into the combinatorics of the problem at hand, and this complexity can make the technique extremely difficult to apply successfully. Path coupling helps to minimize the combinatorial difficulty and in all cases provides simpler convergence proofs than does the standard coupling method. However the true power of the method is that the simplification obtained may allow coupling proofs which were previously unknown, or provide significantly better bounds than those obtained using the standard method. We apply the path coupling method to several hard combinatorial problems, obtaining new or improved results. We examine combinatorial problems such as graph colouring and TWICE-SAT, and problems from statistical physics, such as the antiferromagnetic Potts model and the hard-core lattice gas model. In each case we provide either a proof of rapid mixing where none was known previously, or substantial simplification of existing proofs with consequent gains in the performance of the resulting algorithms.
[Algorithm design and analysis, Heart, complexity, Lattices, Markov chains, P-hard counting, path coupling, graph colouring, Convergence, combinatorial difficulty, algorithm theory, rapid mixing, TWICE-SAT, Polynomials, theorem proving, Potts model, Combinatorial mathematics, Physics, Stress, Computer science, Markov chain Monte Carlo method, hard combinatorial problems, Markov processes, computational complexity, algorithm design]
Finding an even hole in a graph
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
A hole in a graph is a chordless cycle of length greater than three. In this paper we present a decomposition theorem for graphs that contain no even hole. This theorem yields a polytime algorithm to recognize whether a graph contains an even hole.
[chordless cycle, decomposition theorem, graph theory, Bismuth, Mathematics, polytime algorithm, Zinc, even hole, graph, computational complexity]
Undirected single source shortest paths in linear time
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
The single source shortest paths problem (SSSP) is one of the classic problems in algorithmic graph theory: given a weighted graph G with a source vertex s, find the shortest path from s to all other vertices in the graph. Since 1959 all theoretical developments in SSSP have been based on Dijkstra's algorithm, visiting the vertices in order of increasing distance from s. Thus, any implementation of Dijkstra's algorithm sorts the vertices according to their distances from s. However, we do not know how to sort in linear time. Here, a deterministic linear time and linear space algorithm is presented for the undirected single source shortest paths problem with integer weights. The algorithm avoids the sorting bottle-neck by building a hierarchical bucketing structure, identifying vertex pairs that may be visited in any order.
[weighted graph, linear space, Buildings, graph theory, Random access memory, deterministic linear time, Read-write memory, algorithmic graph theory, Graph theory, Registers, deterministic algorithms, Sorting, Shortest path problem, Computer science, Computer languages, single source shortest paths problem, sorting, Dijkstra's algorithm, Assembly, computational complexity, linear time, hierarchical bucketing]
The analysis of a list-coloring algorithm on a random graph
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We introduce a natural k-coloring algorithm and analyze its performance on random graphs with constant expected degree c (G/sub n,p=c/n/). For k=3 our results imply that almost all graphs with n vertices and 1.923 n edges are 3-colorable. This improves the lower bound on the threshold for random 3-colorability significantly and settles the last case of a long-standing open question of Bollobas. We also provide a tight asymptotic analysis of the algorithm. We show that for all k/spl ges/3, if c/spl les/k In k-3/2k then the algorithm almost surely succeeds, while for any /spl epsiv/>0, and k sufficiently large, if c/spl ges/(1+/spl epsiv/)k In k then the algorithm almost surely fails. The analysis is based on the use of differential equations to approximate the mean path of certain Markov chains.
[Algorithm design and analysis, list-coloring algorithm, random graph, Scholarships, tight asymptotic analysis, Color, Markov chains, Distributed computing, Physics, graph colouring, differential equations, Computer science, Sufficient conditions, performance, Councils, Differential equations, Performance analysis, computational complexity, k-coloring algorithm]
Number-theoretic constructions of efficient pseudo-random functions
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We describe efficient constructions for various cryptographic primitives (both in private-key and in public-key cryptography). We show these constructions to be at least as secure as the decisional version of the Diffie-Hellman assumption or as the assumption that factoring is hard. Our major result is a new construction of pseudo-random functions such that computing their value at any given point involves two multiple products. This is much more efficient than previous proposals. Furthermore, these functions have the advantage of being in TC/sup 0/ (the class of functions computable by constant depth circuits consisting of a polynomial number of threshold gates) which has several interesting applications. The simple algebraic structure of the functions implies additional features. In particular, we show a zero-knowledge proof for statements of the form "y=f/sub s/(x)" and "y/spl ne/f(x)" given a commitment to a key s of a pseudo-random function f/sub s/.
[Engineering profession, Circuits, pseudo-random functions, cryptographic primitives, threshold gates, number-theoretic constructions, cryptography, Mathematics, Security, Distributed computing, Cryptographic protocols, Computer science, DH-HEMTs, constant depth circuits, Polynomials, Cryptography, computational complexity, number theory]
Constant depth circuits and the Lutz hypothesis
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Resource-bounded measure theory is a study of complexity classes via an adaptation of the probabilistic method. The central hypothesis in this theory is the assertion that NP does not have measure zero in Exponential Time. This is a quantitative strengthening of NP/spl ne/P. We show that the analog in P of this hypothesis fails dramatically. In fact, we show that NTIME[n/sup 1/11/] has measure zero in P. These follow as consequences of our main theorem that the collection of languages accepted by constant-depth nearly exponential-size circuits has measure zero at polynomial time. In contrast, we show that the class AC/sup 0//sub 4/[/spl oplus/] of languages accepted by depth-4 polynomial-size circuits with AND, OR, NOT, and PARITY gates does not have measure zero at polynomial time. Our proof is based on techniques from circuit complexity theory and pseudorandom generators.
[NP, complexity classes, pseudorandom generators, Circuits, Natural languages, Size measurement, Time measurement, Complexity theory, Computer science, constant-depth, circuit complexity theory, Boolean functions, Lutz hypothesis, minimisation of switching nets, constant depth circuits, Particle measurements, Exponential Time, Polynomials, nearly exponential-size circuits, computational complexity]
A random sampling based algorithm for learning the intersection of half-spaces
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We present an algorithm for learning the intersection of half spaces in n dimensions. Over nearly uniform distributions, it runs in polynomial time for up to O(logn/loglogn) half spaces or, more generally for any number of half spaces whose normal vectors lie in an O(log n/log log n) dimensional subspace. Over less restricted "non-concentrated" distributions it runs in polynomial time for a constant number of half spaces. This generalizes an earlier result of A. Blum and R. Kannan (1993). The algorithm is simple and is based on random sampling.
[subspace, normal vectors, random processes, computational geometry, nearly uniform distributions, Linear programming, half space intersection, learning, Computer science, non concentrated distributions, Neural networks, Machine learning, random sampling based algorithm, Sampling methods, Polynomials, polynomial time, learning (artificial intelligence), Integrated circuit modeling, computational complexity]
Does parallel repetition lower the error in computationally sound protocols?
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Whether or not parallel repetition lowers the error has been a fundamental question in the theory of protocols, with applications in many different areas. It is well known that parallel repetition reduces the error at an exponential rate in interactive proofs and Arthur-Merlin games. It seems to have been taken for granted that the same is true in arguments, or other proofs where the soundness only holds with respect to computationally bounded parties. We show that this is not the case. Surprisingly, parallel repetition can actually fail in this setting. We present four-round protocols whose error does not decrease under parallel repetition. This holds for any (polynomial) number of repetitions. These protocols exploit non-malleable encryption and can be based on any trapdoor permutation. On the other hand we show that for three-round protocols the error does go down exponentially fast. The question of parallel error reduction is particularly important when the protocol is used in cryptographic settings like identification, and the error represents the probability that an intruder succeeds.
[protocol theory, Arthur-Merlin games, combinatorial mathematics, Drives, Complexity theory, non-malleable encryption, cryptographic settings, Postal services, Concurrent computing, four-round protocols, Polynomials, theorem proving, computationally sound protocols, Cryptography, protocols, three-round protocols, parallel algorithms, computationally bounded parties, parallel error reduction, Engineering profession, soundness, probability, cryptography, Cryptographic protocols, Computer science, Graphics, interactive proofs, trapdoor permutation, parallel repetition, computational complexity]
Approximating shortest paths on a nonconvex polyhedron
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We present an approximation algorithm that, given the boundary P of a simple, nonconvex polyhedron in R/sup 3/, and two points s and t on P, constructs a path on P between s and t whose length is at most 7(1+/spl epsi/)d/sub P/(s,t), where d/sub P/(s,t) is the length of the shortest path between s and t on P, and /spl epsi/>0 is an arbitrarily small positive constant. The algorithm runs in O(n/sup 5/3/ log/sup 5/3/ n) time, where n is the number of vertices in P. We also present a slightly faster algorithm that runs in O(n/sup 8/5/ log/sup 8/5/ n) time and returns a path whose length is at most 15(1+/spl epsi/)d/sub P/(s,t).
[Geographic Information Systems, Costs, Military computing, polyhedron, computational geometry, time complexity, nonconvex polyhedron, Information analysis, Computer science, Shortest path problem, Image analysis, shortest paths, faster algorithm, Aerospace simulation, approximation algorithm, Approximation algorithms, Robots, computational complexity]
Minimizing flow time nonclairvoyantly
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We consider the problem of scheduling a collection of dynamically arriving jobs with unknown execution times so as to minimize the average response/flow time. This is the classic CPU scheduling problem faced by time sharing operating systems. In the standard 3-field scheduling notation this is the nonclairvoyant version of 1|pmtn, r/sub j/|/spl Sigma/F/sub j/. Its easy to see that every algorithm that doesn't unnecessarily idle the processor is at worst n-competitive, where n is the number of jobs. Yet there is no known nonclairvoyant algorithm, deterministic or randomized, with a competitive ratio provably o(n). We present a randomized nonclairvoyant algorithm, RMLF, that has competitive ratio /spl theta/(lognloglogn) against an adaptive adversary. RMLF is a slight variation of the multi level feedback (MLF) algorithm used by the Unix operating system, further justifying the adoption of this algorithm. R. Motwani et al. (1994) showed that every randomized nonclairvoyant algorithm is /spl Omega/2(log n)competitive, and that every deterministic nonclairvoyant algorithm is /spl Omega/2(n/sup 1/3/)-competitive.
[Kirk field collapse effect, deterministic nonclairvoyant algorithm, processor scheduling, flow time minimization, adaptive adversary, competitive ratio, average response/flow time, RMLF, Operating systems, Feedback, scheduling, dynamically arriving jobs, unknown execution times, Dynamic scheduling, standard 3-field scheduling notation, Scheduling algorithm, nonclairvoyant version, randomised algorithms, Computer science, multi level feedback algorithm, Processor scheduling, randomized nonclairvoyant algorithm, Time sharing computer systems, Unix operating system, Random variables, Time factors, minimisation, classic CPU scheduling problem, time sharing operating systems, computational complexity]
Lower bounds for the signature size of incremental schemes
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We show lower bounds for the signature size of incremental schemes which are secure against substitution attacks and support single block replacement. We prove that for documents of n blocks such schemes produce signatures of /spl Omega/(n/sup 1/(2+c)/) bits for any constant c>0. For schemes accessing only a single block resp. A constant number of blocks for each replacement this bound can be raised to /spl Omega/(n) resp. /spl Omega/(/spl radic/n). Additionally, we show that our technique yields a new lower bound for memory checkers.
[Uniform resource locators, single block replacement, security of data, incremental schemes, substitution attacks, Security, signature size, Message authentication, lower bounds, computational complexity]
Pattern matching with swaps
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Let a text string T of n symbols and a pattern string P of m symbols from alphabet /spl Sigma/ be given. A swapped version T' of T is a length n string derived from T by a series of local swaps, (i.e. t/sup '//sub l//spl larr/t/sub l+1/ and t'/sub l+1//spl larr/t/sub l/) where each element can participate in no more than one swap. The Pattern Matching with Swaps problem is that of finding all locations i for which there exists a swapped version T' of T where there is an exact matching of P in location i of T'. It has been an open problem whether swapped matching can be done in less than O(mn) time. In this paper we show the first algorithm that solves the pattern matching with swaps problem in time O(mn). We present an algorithm whose time complexity is O(nm/sup 1/3/ log m log/sup 2/ /spl sigma/) for a general alphabet /spl Sigma/, where /spl sigma/=min(m, |/spl Sigma/|).
[Art, pattern matching, Heuristic algorithms, Discrete Fourier transforms, swapped matching, O(mn) time, time complexity, Mathematics, Tellurium, Computer science, Software libraries, DNA, text string, Dynamic programming, string matching, Pattern matching, computational complexity]
Parallelizing elimination orders with linear fill
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
This paper presents an algorithm for finding parallel elimination orders for Gaussian elimination. Viewing a system of equations as a graph, the algorithm can be applied directly to interval graphs and chordal graphs. For general graphs, the algorithm can be used to parallelize the order produced by some other heuristic such as minimum degree. In this case, the algorithm is applied to the chordal completion that the heuristic generates from the input graph. In general, the input to the algorithm is a chordal graph G with n nodes and m edges. The algorithm produces an order with height at most O(log/sup 3/ n) times optimal, fill at most O(m), and work at most O(W*(G)), where W*(G) is the minimum possible work over all elimination orders for G. Experimental results show that when applied after some other heuristic, the increase in work and fill is usually small. In some instances the algorithm obtains an order that is actually better, in terms of work and fill, than the original one. We also present an algorithm that produces an order with a factor of log n less height, but with a factor of O(/spl radic/log n) more fill.
[elimination orders, parallel algorithms, Engineering profession, Particle separators, Government, Sparse matrices, Sun, Equations, linear fill, Concurrent computing, Gaussian elimination, parallelize, National electric code, Computer industry, chordal completion, Contracts, linear algebra, computational complexity]
The minimization problem for Boolean formulas
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We investigate the computational complexity of the minimization problem for Boolean formulas. Depending on the definition, these problems are trivially in /spl Sigma//sub 2//sup P/ or II/sub 2//sup P/, and these are the best upper bounds known. The only previously known lower bounds are also trivial, and are coNP lower bounds at best, thus leaving quite a large gap between the upper and lower bounds. In this paper, we prove much better lower bounds: hardness for parallel access to NP for those cases in which coNP was the best previously known lower bound, and coNP-hardness for the case in which no lower bound was previously known.
[parallel access, Minimization, Educational institutions, Mathematics, Boolean algebra, Computational complexity, lower bounds, coNP-hardness, Boolean formulas, hardness, Upper bound, coNP lower bounds, minimization problem, Polynomials, Robustness, minimisation, computational complexity]
On the complexity of a set-union problem
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We consider a simple data structure supporting the following operations: (i) create a new singleton set; (ii) create a new set which is the union of two pre-existing sets; (iii) determine whether a given element is in a particular set. We prove both lower and upper bounds for an implementation of such a data structure. In a restricted model we show that no deterministic implementation can be better than the "trivial" one that takes O(n/sup 2/) time. In a parallel model where the operations come in at most O(1g n) stages we exhibit a sub-quadratic implementation.
[complexity, Costs, Data analysis, parallel model, data structure, Data structures, Mathematics, set theory, parallel programming, Computer science, Upper bound, singleton set, bounds, set-union problem, data structures, sub-quadratic implementation, computational complexity]
Separation of the monotone NC hierarchy
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We prove tight lower bounds, of up to n/sup /spl epsiv//, for the monotone depth of functions in monotone-P. As a result we achieve the separation of the following classes. 1. Monotone-NC/spl ne/monotone-P. 2. /spl forall/i/spl ges/1, monotone-NC/sup i//spl ne/monotone-NC/sup i+1/. 3. More generally: For any integer function D(n), up to n/sup /spl epsiv// (for some /spl epsiv/>0), we give an explicit example of a monotone Boolean function, that can be computed by polynomial size monotone Boolean circuits of depth D(n), but that cannot be computed by any (fan-in 2) monotone Boolean circuits of depth less than Const/spl middot/D(n) (for some constant Const). Only a separation of monotone-NC/sup 1/ from monotone-NC/sup 2/ was previously known. Our argument is more general: we define a new class of communication complexity search problems, referred to below as DART games, and we prove a tight lower bound for the communication complexity of every member of-this class. As a result we get lower bounds for the monotone depth of many functions. In particular, we get the following bounds: 1. For st-connectivity, we get a tight lower bound of /spl Omega/(log/sup 2/ n). That is, we get a new proof for Karchmer-Wigderson's theorem, as an immediate corollary of our general result. 2. For the k-clique function, with k/spl les/n/sup /spl epsiv//, we get a tight lower bound of /spl Omega/(k log n). Only a bound of /spl Omega/(k) was previously known.
[tight lower bounds, DART games, Circuits, monotone Boolean function, Search problems, Mathematics, Complexity theory, monotone depth, communication complexity, Radio access networks, Boolean functions, monotone NC hierarchy, Polynomials, search problems]
Edge-connectivity augmentation preserving simplicity
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Given a simple graph G=(V, E), the goal is to find a smallest set F of new edges such that G=(V, E/spl cup/F) is /spl kappa/ edge connected and simple. Very recently this problem was shown to be NP hard by T. Jordan (1997). We prove that if OPT/sub P//sup /spl kappa// is high enough-depending on /spl kappa/ only-then OPT/sub S//sup /spl kappa//=OPT/sub P//sup /spl kappa// holds, where OPT/sub S//sup /spl kappa// (OPT/sub P//sup /spl kappa//) is the size of an optimal solution of the augmentation problem with (without) the simplicity preserving requirement, respectively. Furthermore, OPT/sub S//sup /spl kappa//-OPT/sub P//sup /spl kappa///spl les/g(/spl kappa/) holds for a certain (quadratic) function of /spl kappa/. Based on these results an algorithm is given which computes an optimal solution in time O(n/sup 4/) for any fixed /spl kappa/. Most of these results are extended to the case of non-uniform demands, as well.
[simple graph, graph theory, Optimized production technology, simplicity preserving requirement, Mathematics, set theory, optimal solution, augmentation problem, non uniform demands, Computer science, Councils, NP hard, edge connectivity augmentation preserving simplicity, Telephony, Polynomials, Computer networks, quadratic function, computational complexity]
A faster deterministic algorithm for minimum spanning trees
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
A deterministic algorithm for computing a minimum spanning tree of a connected graph is presented. Its running time is O(m /spl alpha/ log /spl alpha/), where /spl alpha/=/spl alpha/(m,n) is a functional inverse of Ackermann's function and n (resp. m) is the number of vertices (resp. edges). This improves on the previous, ten-year old bound of (roughly) O(m log log* m).
[Costs, Computational modeling, minimum spanning trees, trees (mathematics), Partitioning algorithms, History, deterministic algorithm, deterministic algorithms, Computer science, running time, Tree graphs, connected graph, computational complexity]
Learning noisy perceptrons by a perceptron in polynomial time
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Learning perceptrons (linear threshold functions) from labeled examples is an important problem in machine learning. We consider the problem where labels are subjected to random classification noise. The problem was known to be PAC learnable via a hypothesis that consists of a polynomial number of linear thresholds (due to A. Blum, A. Frieze, R. Kannan, and S. Vempala (1996)). The question of whether a hypothesis that is itself a perceptron (a single threshold function) can be found in polynomial time was open. We show that indeed, noisy perceptrons are PAC learnable with a hypothesis that is a perceptron.
[perceptrons, random processes, Linear programming, random noise, Vectors, Probability distribution, noisy perceptron learning, single threshold function, Noise generators, machine learning, Zinc, Noise level, random classification noise, linear threshold functions, linear thresholds, Machine learning, Ear, Polynomials, polynomial time, labeled examples, PAC learnable, hypothesis, learning by example, computational complexity]
Storage management for evolving databases
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
The problem of maintaining data that arrives continuously over time is increasingly prevalent in databases and digital libraries. Building on a model for sliding window indices developed by N. Shivakumar and H. Garcia-Molina (1997), we devise efficient algorithms for some of the central problems that arise. We also show connections between the problems in this model and some fundamental problems in optimization and graph theory.
[Costs, graph theory, digital libraries, Graph theory, database management systems, Computer science, storage management, evolving databases, Software libraries, Disk drives, optimisation, Databases, optimization, sliding window indices, Polynomials, Computer networks, Hardware, efficient algorithms, Computer network management]
Improved bounds on planar k-sets and k-levels
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We prove an O(nk/sup 1/3/) upper bound for planar k-sets. This is the first considerable improvement on this bound after its early solutions approximately twenty seven years ago. Our proof technique also applies to improve the current bounds on the combinatorial complexities of k-levels in arrangements of line segments, k convex polygons in the union of n lines, parametric minimum spanning trees and parametric matroids in general.
[Algorithm design and analysis, planar k-sets, parametric matroids, computational geometry, parametric minimum spanning trees, set theory, Geometry, Computer science, combinatorial complexities, Upper bound, Tree graphs, line segments, convex polygons, k-levels, computational complexity]
Flows in undirected unit capacity networks
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We describe an O(min(m, n/sup 3/2/)m/sup 1/2/)-time algorithm for finding maximum flows in undirected networks with unit capacities and no parallel edges. This improves upon the previous bound of Karzanov and Even and Tarjan when m=/spl omega/(n/sup 3/2/), and upon a randomized bound of Karger when /spl upsi/=/spl Omega/(n/sup 7/4//m/sup 1/2/).
[Intelligent networks, IEL, undirected unit capacity networks, computation time, graph theory, National electric code, Partitioning algorithms, undirected networks, maximum flows, computational complexity]
Computing integral points in convex semi-algebraic sets
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
Let Y be a convex set in R/sup k/ defined by polynomial inequalities and equations of degree at most d/spl ges/2 with integer coefficients of binary length l. We show that if Y/spl cap/Z/sup k//spl ne//spl theta/, then Y contains an integral point of binary length ld/sup O/((k/sup 4/)). For fixed k, our bound implies a polynomial-time algorithm for computing an integral point y/spl isin/Y. In particular, we extend Lenstra's theorem on the polynomial-time solvability of linear integer programming in fixed dimension to semidefinite integer programming.
[polynomial inequalities, integer programming, polynomial-time solvability, computability, Linear programming, Encoding, semidefinite integer programming, linear integer programming, Computer science, Boolean functions, Integral equations, Integer linear programming, Polynomials, polynomial-time algorithm, Lenses, computational complexity, convex semi-algebraic sets]
Randomized and deterministic algorithms for the dimension of algebraic varieties
Proceedings 38th Annual Symposium on Foundations of Computer Science
None
1997
We prove old and new results on the complexity of computing the dimension of algebraic varieties. In particular, we show that this problem is NP-complete in the Blum-Shub-Smale model of computation over C, that it admits a s/sup O(1)/D/sup O(n)/ deterministic algorithm, and that for systems with integer coefficients it is in the Arthur-Merlin class under the Generalized Riemann Hypothesis. The first two results are based on a general derandomization argument.
[complexity, Computational modeling, algebra, NP-complete, deterministic algorithms, randomised algorithms, algebraic varieties, Upper bound, Turing machines, randomized algorithms, Arthur-Merlin class, Generalized Riemann Hypothesis, Polynomials, computational complexity]
Geometric computation and the art of sampling (tutorial)
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
false
[Computer science, Tutorial, Computational geometry, Subspace constraints, Computer graphics, Sampling methods, Mathematics, Electrical capacitance tomography, Books, Application software]
A tutorial on theoretical issues in probabilistic artificial intelligence
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
false
[Learning, Tutorial, Physics computing, Process planning, Probability, World Wide Web, Sampling methods, Inference algorithms, Computer networks, Artificial intelligence]
Which problems have strongly exponential complexity?
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
For several NP-complete problems, there have been a progression of better but still exponential algorithms. In this paper we address the relative likelihood of sub-exponential algorithms for these problems. We introduce a generalized reduction which we call sub-exponential reduction family (SERF) that preserves sub-exponential complexity. We show that Circuit-SAT is SERF-complete for all NP-search problems, and that for any fixed k, k-SAT, k-Colorability, k-Set Cover Independent Set, Clique, Vertex Cover are SERF-complete for the class SNP of search problems expressible by second order existential formulas whose first order part is universal. In particular, sub-exponential complexity for any one of the above problems implies the same for all others. We also look at the issue of proving strongly exponential lower bounds (that is, bounds of the form 2/sup /spl Omega/(n)/) for AC/sup 0/. This problem is even open far depth-3 circuits. In fact, such a bound for depth-3 circuits with even limited (at most n/sup /spl epsiv//) fan-infer bottom-level gates would imply a nonlinear size lower bound for logarithmic depth circuits. We show that with high probability even degree 2 random GF(2) polynomials require strongly exponential site for /spl Sigma//sub 3//sup k/ circuits for k=o(loglogn). We thus exhibit a much smaller space of 2(0(/sup n2/)) functions such that almost every function in this class requires strongly exponential size /spl Sigma//sub 3//sup k/ circuits. As a corollary, we derive a pseudorandom generator (requiring O(n/sup 2/) bits of advice) that maps n bits into a larger number of bits so that computing parity on the range is hard for /spl Sigma//sub 3//sup k/ circuits. Our main technical lemma is an algorithm that, for any fixed /spl epsiv/>0, represents an arbitrary k-CNF formula as a disjunction of 2/sup /spl epsiv/n/ k-CNF formulas that are sparse, e.g., each having O(n) clauses.
[polynomials, strongly exponential lower bounds, Circuits, computational geometry, Electronic switching systems, k-Colorability, NP-complete problem, sub-exponential algorithms, exponential complexity, sub-exponential reduction family, Ear, Writing, NP-complete problems, Polynomials, Vertex Cover, sub-exponential complexity, k-SAT, search problems, NP-search problems, computational complexity]
Tseitin's tautologies and lower bounds for Nullstellensatz proofs
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We use the known linear lower bound for Tseitin's tautologies for establishing linear lower bounds on the degree of Nullstellensatz proofs (in the usual boolean setting) for explicitly constructed systems of polynomials of a constant (in our construction 6) degree. It holds over any field of characteristic distinct from 2. Previously, a linear lower bound was proved for an explicitly constructed system of polynomials of a logarithmic degree.
[Design methodology, polynomials, Tseitin's tautologies, explicitly constructed systems, Mathematics, Calculus, logarithmic degree, lower bounds, Computer science, Upper bound, Boolean functions, Nullstellensatz proofs, Polynomials, theorem proving, boolean setting]
Exponential separations between restricted resolution and cutting planes proof systems
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We prove an exponential lower bound for tree-like cutting planes refutations of a set of clauses which has polynomial size resolution refutations. This implies an exponential separation between tree-like and dag-like proofs for both cutting planes and resolution; in both cases only superpolynomial separations were known before. In order to prove this, we extend the lower bounds on the depth of monotone circuits of R. Raz and P. McKenzie (1997) to monotone real circuits. In the case of resolution, we further improve this result by giving an exponential separation of tree-like resolution front (dag-like) regular resolution proofs. In fact, the refutation provided to give the upper bound respects the stronger restriction of being a Davis-Puatam resolution proof. Finally, we prove an exponential separation between Davis-Putnam resolution and unrestricted resolution proofs; only a superpolynomial separations was previously known.
[Circuits, computational geometry, dag-like proofs, upper bound, Mathematics, restricted resolution, Davis-Puatam resolution proof, Electrical capacitance tomography, Read only memory, exponential lower bound, Reactive power, Upper bound, polynomial size resolution refutations, Polynomials, theorem proving, cutting planes proof systems, exponential separations, Informatics, monotone real circuits, monotone circuits, computational complexity]
An improved exponential-time algorithm for k-SAT
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We propose and analyze a simple new algorithm for finding satisfying assignments of Boolean formulae in conjunctive normal form. The algorithm, ResolveSat, is a randomized variant of the DDL procedure by M. Davis et al. (1962) or Davis-Putnam procedure. Rather than applying the DLL procedure to the input formula F, however; ResolveSat enlarges F by adding additional clauses using limited resolution before performing DLL. The basic idea behind our analysis is the same as by R. Paturi (1997): a critical clause for a variable at a satisfying assignment gives rise to a unit clause in the DLL procedure with sufficiently high probability, thus increasing the probability of finding a satisfying assignment. In the current paper, we analyze the effect of multiple critical clauses (obtained through resolution) in producing unit clauses. We show that, for each k, the running time of ResolveSat on a k-CNF formula is significantly better than 2/sup n/, even in the worst case. In particular we show that the algorithm finds a satisfying assignment of a general 3-CNF in time O(2/sup .446n/) with high probability; where the best previous algorithm has running time O(2/sup .582n/). We obtain a better upper bound of O(2/sup (2ln2-1)/n+0(n))=O(2/sup 0.387n/) for 3-CNF that have at most one satisfying assignment (unique k-SAT). For each k, the bounds for general k-CNF are the best known for the worst-case complexity of finding a satisfying solution for k-SAT, the idea of succinctly encoding satisfying solutions can be applied to obtain lower bounds on circuit site. Here, we exhibit a function f such that any depth-3 AND-OR circuit with bottom fan-in bounded by k requires /spl Omega/(2(c/sub k/n/k)) gates (with c/sub k/>1). This is the first such lower bound with c/sub k/>1.
[Algorithm design and analysis, ResolveSat, depth-3 AND-OR circuit, satisfying assignments, Surface-mount technology, exponential-time algorithm, Boolean formulae, randomized variant, clauses, computational geometry, upper bound, Mathematics, worst-case complexity, conjunctive normal form, Radio access networks, randomised algorithms, Upper bound, Boolean functions, Chromium, k-SAT, computational complexity]
The finite capacity dial-a-ride problem
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We give the first non-trivial approximation algorithm for the Capacitated Dial-a-Ride problem: given a collection of objects located at points in a metric space, a specified destination point for each object, and a vehicle with a capacity of at most k objects, the goal is to compute a shortest tour for the vehicle in which all objects can be delivered to their destinations while ensuring that the vehicle carries at most k objects at any point in time. The problem is known under several names, including the Stacker Crane problem and the Dial-a-Ride problem. No theoretical approximation guarantees were known for this problem other than for the cases k=1, /spl infin/ and the trivial O(k) approximation for general capacity k. We give an algorithm with approximation ratio O(/spl radic/k) for special instances on a class of tree metrics called height-balanced trees. Using Bartal's recent results on the probabilistic approximation of metric spaces by tree metrics, we obtain an approximation ratio of O(/spl radic/k log n log log n) for arbitrary n point metric spaces. When the points lie on a line (line metric), we provide a 2-approximation algorithm. We also consider the Dial-a-Ride problem in another framework: when the vehicle is allowed to leave objects at intermediate locations and pick them up at a later time and deliver them. For this model, we design an approximation algorithm whose performance ratio is O(1) for tree metrics and O(log n log log n) for arbitrary metrics. We also study the ratio between the values of the optimal solutions for the two versions of the problem. We show that unlike in k-delivery TSP in which all the objects are identical, this ratio is not bounded by a constant for the Dial-a-Ride problem, and it could be as large as R(k/sup 2/3/).
[approximation ratio, approximation theory, height-balanced trees, Operations research, probabilistic approximation, Transportation, trees (mathematics), computational geometry, Extraterrestrial measurements, Electrical capacitance tomography, tree metrics, Computer science, Space vehicles, Stacker Crane problem, Chromium, approximation algorithm, shortest tour, finite capacity dial-a-ride problem, Books]
Lower bounds for (MOD p-MOD m) circuits
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Modular gates are known to be immune for the random restriction techniques of previous authors. We demonstrate here a random clustering technique which overcomes this difficulty and is capable to prove generalizations of several known modular circuit lower bounds, characterizing symmetric functions computable by small (MOD/sub p/, AND/sub t/, MOD/sub m/) circuits. Applying a degree-decreasing technique together with random restriction methods for the AND gates at the bottom level, we also prove a hard special case of the constant degree hypothesis and other related lower bounds for certain (MOD/sub p/, MOD/sub m/, AND) circuits. Most of the previous lower bounds on circuits with modular gates used special definitions of the modular gates (i.e., the gate outputs one if the sum of its inputs is divisible by m, or is not divisible by m), and were not valid for more general MOD/sub m/ gates. Our methods are applicable-and our lower bounds are valid-for the most general modular gates as well.
[circuit complexity, degree-decreasing technique, Input variables, Computational modeling, Circuits, Very large scale integration, random clustering technique, random restriction techniques, Complexity theory, Electronic mail, random restriction methods, lower bounds, Concurrent computing, Computer science, modular gates, logic gates, Polynomials, Mathematical model, modular circuit lower bounds]
Approximation of diameters: randomization doesn't help
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We describe a deterministic polynomial-time algorithm which, for a convex body K in Euclidean n-space, finds upper and lower bounds on K's diameter which differ by a factor of O(/spl radic/n/logn). We show that this is, within a constant factor, the best approximation to the diameter that a polynomial-time algorithm can produce even if randomization is allowed. We also show that the above results hold for other quantities similar to the diameter-namely; inradius, circumradius, width, and maximization of the norm over K. In addition to these results for Euclidean spaces, we give tight results for the error of deterministic polynomial-time approximations of radii and norm-maxima for convex bodies in finite-dimensional l/sub p/ spaces.
[deterministic polynomial-time algorithm, computational geometry, upper bounds, Mathematics, deterministic algorithms, Tellurium, lower bounds, Concurrent computing, Computer science, Cyclic redundancy check, optimisation, Euclidean spaces, polynomial approximation, Approximation algorithms, Euclidean n-space, Polynomials, polynomial-time algorithm, deterministic polynomial-time approximations, diameters approximation]
A unified superfast algorithm for boundary rational tangential interpolation problems and for inversion and factorization of dense structured matrices
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The classical scalar Nevanlinna-Pick interpolation problem has a long and distinguished history, appearing in a variety of applications in mathematics and electrical engineering. There is a vast literature on this problem and on its various far reaching generalizations. It is widely known that the now classical algorithm for solving this problem proposed by Nevanlinna in 1929 can be seen as a way of computing the Cholesky factorization for the corresponding Pick matrix. Moreover; the classical Nevanlinna algorithm takes advantage of the special structure of the Pick matrix to compute this triangular factorization in only O(n/sup 2/) arithmetic operations, where n is the number of interpolation points, or equivalently, the size of the Pick matrix. Since the structure-ignoring standard Cholesky algorithm [though applicable to the wider class of general matrices] has much higher complexity O(n/sup 3/), the Nevanlinna algorithm is an example of what is now called fast algorithms. In this paper we use a divide-and-conquer approach to propose a new superfast O(n log/sup 3/ n) algorithm to construct solutions for the more general boundary tangential Nevanlinna-Pick problem. This dramatic speed-up is achieved via a new divide-and-conquer algorithm for factorization of rational matrix functions; this superfast algorithm seems to have a practical and theoretical significance itself. It can be used to solve similar rational interpolation problems [e.g., the matrix Nehari problem], and a variety, of engineering problems. It can also be used for inversion and triangular factorization of matrices with displacement structure, including Hankel-like, Vandermonde-like, and Cauchy-like matrices.
[boundary rational tangential interpolation problems, Pick matrix, divide and conquer methods, divide-and-conquer approach, Mathematics, Digital filters, Cauchy-like matrices, unified superfast algorithm, Polynomials, classical scalar Nevanlinna-Pick interpolation problem, Cholesky factorization, Codes, matrix Nehari problem, Transfer functions, Nevanlinna algorithm, rational interpolation problems, Educational institutions, Ice, matrix algebra, Interpolation, interpolation, dense structured matrices, triangular factorization, Filtering theory, Arithmetic, computational complexity]
Decidability of bisimulation equivalence for equational graphs of finite out-degree
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The bisimulation problem for equational graphs of finite out-degree is shown to be decidable. We reduce this problem to the /spl eta/-bisimulation problem for deterministic rational (vectors of) Boolean series on the alphabet of a dpda M. We then exhibit a complete formal system for deducing equivalent pairs of such vectors.
[Formal languages, pushdown automata, Vectors, finite out-degree, Equations, equivalent pairs, Concurrent computing, deterministic rational, decidability, Automata, equational graphs, bisimulation equivalence, Boolean series]
The complexity of acyclic conjunctive queries
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We show that the problem of evaluating acylic Boolean database-queries is LOGCFL-complete and thus highly parallelizable. We present a parallel database algorithm solving this problem with a logarithmic number of parallel join operations. It follows from our main result that the acylic versions of the following important database and Al problems are LOGCFL-complete: The query output tuple problem for conjunctive queries, conjunctive query containment, clause subsumption, and constraint satisfaction.
[parallel algorithms, clause subsumption, Relational databases, Electrical capacitance tomography, LOGCFL-complete, parallel join operations, Read only memory, database theory, acyclic conjunctive queries complexity, query processing, Microwave integrated circuits, Boolean functions, conjunctive queries, constraint satisfaction, acylic Boolean database-queries, parallel database algorithm, query output tuple problem, Database systems, Books, Artificial intelligence, computational complexity]
Which crossing number is it, anyway? [computational geometry]
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
A drawing of a graph G is a mapping which assigns to each vertex a point of the plane and to each edge a simple continuous arc connecting the corresponding two points. The crossing number of G is the minimum number of crossing points in any drawing of G. We define two new parameters, as follows. The pairwise crossing number (resp. the odd-crossing number) of G is the minimum number of pairs of edges that cross (resp. cross an odd number of times) over all drawings of G. We prove that the determination of each of these parameters is an NP-complete problem. We also prove that the largest of these numbers (the crossing number) cannot exceed twice the square of the smallest (the odd-crossing number). Our proof is based on the following generalization of an old result of Hanani, which is of independent interest. Let G be a graph and let E/sub 0/ be a subset of its edges such that there is a drawing of G, in which every edge belonging E/sub 0/ crosses any other edge an even number of times. Then G can be redrawn so that the element of E/sub 0/ are not involved in any crossing.
[mapping, Terminology, computational geometry, graph drawing, crossing points, Bipartite graph, NP-complete problem, Joining processes, computational complexity]
A factor 2 approximation algorithm for the generalized Steiner network problem
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We present a factor 2 approximation algorithm for finding a minimum-cost subgraph having at least a specified number of edges in each cut. This class of problems includes, among others, the generalized Steiner network problem, which is also known as the survivable network design problem. Our algorithm first solves the linear relaxation of this problem, and then iteratively rounds off the solution. The key idea in rounding off is that in a basic solution of the LP relaxation, at least one edge gets included at least to the extent of half. We include this edge into our integral solution and solve the residual problem.
[Steiner trees, integer programming, graph theory, survivable network design problem, computational geometry, Educational institutions, Electrical capacitance tomography, factor 2 approximation algorithm, minimum-cost subgraph, Approximation algorithms, Cost function, Computer networks, generalized Steiner network problem]
A divide-and-conquer algorithm for min-cost perfect matching in the plane
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Given a set V of 2n points in the plane, the min-cost perfect matching problem is to pair up the points (into n pairs) so that the sum of the Euclidean distances between the paired points is minimized. We present an O(n/sup 3/2/log/sup 5/ n)-time algorithm for computing a min-cost perfect matching in the plane, which is an improvement over the previous best algorithm of Vaidya [1989) by nearly a factor of n. Vaidya's algorithm is an implementation of the algorithm of Edmonds (1965), which runs in n phases, and computes a matching with i edges at the end of the i-th phase. Vaidya shows that geometry can be exploited to implement a single phase in roughly O(n/sup 3/2/) time, thus obtaining an O(n/sup 5/2/log/sup 4/ n)-time algorithm. We improve upon this in two major ways. First, we develop a variant of Edmonds algorithm that uses geometric divide-and-conquer, so that in the conquer step we need only O(/spl radic/n) phases. Second, we show that a single phase can be implemented in O(n log/sup 5/ n) time.
[Costs, Operations research, divide and conquer methods, graph theory, computational geometry, divide-and-conquer algorithm, min-cost perfect matching, Electrical capacitance tomography, Pattern recognition, Application software, Statistics, Geometry, Computer science, Programmable logic arrays, Euclidean distance, perfect matching in the plane, geometric divide-and-conquer]
The security of individual RSA bits
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We study the security of individual bits in an RSA encrypted message E/sub N/(X). We show that given E/sub N/(X), predicting any single bit in x with only a non-negligible advantage over the trivial guessing strategy is (through a polynomial time reduction) as hard as breaking RSA. We briefly discuss a related result for bit security of the discrete logarithm.
[individual RSA bits security, bit security, trivial guessing strategy, polynomial time reduction, cryptography, Electronic mail, Information analysis, Jacobian matrices, Information security, RSA encrypted message, discrete logarithm, Polynomials, Cryptography]
Bivariate polynomial multiplication
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We study the multiplicative complexity and the rank of the multiplication in the local algebras R/sub m,n/=k[x,y]/(x/sup m+1/,y/sup n+1/) and T/sub n/=k[x,y]/(x/sup n+1/,x/sup n/y,...,y/sup n+1/) of bivariate polynomials. We obtain the lower bounds (21/3-0(1))/spl middot/dim R/sub m,n/, and (2 1/2 -0(1))/spl middot/dim T/sub n/ for the multiplicative complexity of the multiplication in R/sub m,n/ and T/sub n/, respectively. On the other hand, we derive the upper bounds 3/spl middot/dim T/sub n/-2n-2 and 3/spl middot/dim R/sub m.n/-m-n-3 for the rank of the multiplication in T/sub n/ and R/sub m,n/, respectively, provided that the ground field k admits "fast" univariate polynomial multiplication mod x/sup N/-1. Our results are also applicable to arbitrary finite dimensional algebras of truncated bivariate polynomials k[x,y]/I, where the ideal I=(x(d/sub 0/+1),x(d/sub 1/+1)y,...,x(d/sub n/+1)y/sup n/,y/sup n+1/) is described by a degree pattern d/sub 0//spl ges/d/sub 1//spl ges//spl middot//spl middot//spl middot//spl ges/d/sub n//spl ges/0.
[Performance evaluation, Costs, Shape, polynomials, upper bounds, Tellurium, lower bounds, Radio access networks, multiplicative complexity, Upper bound, Algebra, bivariate polynomials, local algebras, Polynomials, bivariate polynomial multiplication, univariate polynomial multiplication, arbitrary finite dimensional algebras, computational complexity, truncated bivariate polynomials]
Satisfiability of word equations with constants is in exponential space
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
In this paper we study solvability of equations over free semigroups, known as word equations, particularly G.S. Makanin's algorithm (1977), a general procedure to decide if a word equation has a solution. The upper bound time-complexity of Makanin's original decision procedure was quadruple exponential in the length of the equation, as shown by Jaffar. A. Koscielski and L. Pacholski (1996) reduced it to triple exponential, and conjectured that it could be brought down to double exponential. The present paper proves this conjecture. In fact we prove the stronger fact that its space-complexity is single exponential.
[space-complexity, Length measurement, computability, word equations, exponential space, free semigroups, Computer science, Upper bound, Algebra, satisfiability, Automata, Differential equations, Polynomials, Logic, upper bound time-complexity, Artificial intelligence, Informatics, computational complexity]
Heuristics for finding large independent sets, with applications to coloring semi-random graphs
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We study a semi-random graph model for finding independent sets. For /spl alpha/>0, an n-vertex graph with an independent set S of site /spl alpha/n is constructed by blending random and adversarial decisions. Randomly and independently with probability p, each pair of vertices, such that one is in S and the other is not, is connected by an edge. An adversary can then add edges arbitrarily (provided that S remains an independent set). The smaller p is, the larger the control the adversary has over the semi-random graph. We design heuristics that with high probability recover S when p>(1+/spl epsiv/)ln n/|S|, for any constant /spl epsiv/>0. We show that when p<(1-/spl epsiv/) In n/|S|, an independent set of size |S| cannot be recovered, unless NP/spl sube/BPP. We use our remits to obtain greatly improved coloring algorithms for the model of k-colorable semi-random graphs introduced by A. Blum and J. Spencer (1995).
[Performance evaluation, Career development, computational geometry, Electronic switching systems, graph colouring, independent set, Computer science, k-colorable semi-random graphs, heuristics, National electric code, semi-random graphs colouring, n-vertex graph, large independent sets, Polynomials]
Semidefinite relaxations for parallel machine scheduling
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We consider the problem of scheduling unrelated parallel machines so as to minimize the total weighted completion time of jobs. Whereas the best previously known approximation algorithms for this problem are based on LP relaxations, we give a 3/2-approximation algorithm that relies on a convex quadratic programming relaxation. For the special case of two machines we present a further improvement to a 1.2752-approximation; we introduce a more sophisticated semidefinite programming relaxation and apply the random hyperplane technique introduced by M.X. Goemans and D.P. Williamson (1995) for the MAXCUT problem and its refined version of U. Feige and M.X. Goemans (1995). To the best of our knowledge, this is the first time that convex and semidefinite programming techniques (apart from LPs) are used in the area of scheduling.
[Algorithm design and analysis, random hyperplane technique, Parallel machines, computational geometry, Linear programming, convex programming, parallel machine scheduling, convex quadratic programming, Electrical capacitance tomography, semidefinite relaxations, approximation algorithms, Quadratic programming, quadratic programming, Scheduling algorithm, Read only memory, processor scheduling, total weighted completion time, 3/2-approximation algorithm, MAXCUT problem, LP relaxations, Approximation algorithms, Polynomials, Functional programming]
1-way quantum finite automata: strengths, weaknesses and generalizations
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We study 1-way quantum finite automata (QFAs). First, we compare them with their classical counterparts. We show that, if an automaton is required to give the correct answer with a large probability (greater than 7/9), then any 1-way QFAs can be simulated by a 1-way reversible automaton. However, quantum automata giving the correct answer with smaller probabilities are more powerful than reversible automata. Second, we show that 1-way QFAs can be very space-efficient. We construct a 1-way QFA that is exponentially smaller than any equivalent classical (even randomized) finite automaton. We think that this construction may be useful for design of other space-efficient quantum algorithms. Third, we consider several generalizations of 1-way QFAs. Here, our goal is to find a model which is more powerful than 1-way QFAs keeping the quantum part as simple as possible.
[finite automata, 1-way quantum finite automata, Mathematics, Power system modeling, Tellurium, finite automaton, Postal services, Computer science, quantum automata, Quantum computing, Councils, generalizations, 1-way QFAs, Automata, Quantum mechanics, quantum computing]
On the single-source unsplittable flow problem
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Let G=(V,E) be a capacitated directed graph with a source s and k terminals t/sub i/ with demands d/sub i/, 1/spl les/i/spl les/k. We would like to concurrently route every demand on a single path from s to the corresponding terminal without violating the capacities. There are several interesting and important variations of this unsplittable flow problem. If the necessary cut condition is satisfied, we show how to compute an unsplittable flow satisfying the demands such that the total flow through any edge exceeds its capacity by at most the maximum demand. For graphs in which all capacities are at least the maximum demand, we therefore obtain an unsplittable flow with congestion at most 2, and this result is best possible. Furthermore, we show that all demands can be routed unsplittable in 5 rounds, i.e., all demands can be collectively satisfied by the union of 5 unsplittable flows. Finally, we show that 22.6% of the total demand can be satisfied unsplittably. These results are extended to the case when the cut condition is not necessarily satisfied. We derive a 2-approximation algorithm for congestion, a 5-approximation algorithm for the number of rounds and a 4.43=1/0.226-approximation algorithm for the maximum routable demand.
[Computer science, unsplittable flow problem, directed graphs, capacitated directed graph, approximation algorithm, Routing, cut condition, maximum routable demand, Read only memory, Contracts, computational complexity]
Time-space tradeoffs for branching programs
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We obtain the first non-trivial time-space tradeoff lower bound for functions f: {0,1}/sup n//spl rarr/{0,1} on general branching programs by exhibiting a Boolean function f that requires exponential size to be computed by any branching program of length (1+/spl epsiv/)n, for some constant /spl epsiv/>0. We also give the first separation result between the syntactic and semantic read-k models for k>1 by showing that polynomial-size semantic read-twice branching programs can compute functions that require exponential size on any syntactic read-k branching program. We also show a time-space tradeoff result on the more general R-way branching program model: for any k, we give a function that requires exponential size to be computed by length kn q-way branching programs, for some q=q(k).
[Input variables, Computational modeling, exponential size, Binary decision diagrams, Boolean function, Mathematics, Complexity theory, lower bound, time-space tradeoffs, semantic read-k models, Sorting, Computer science, Microwave integrated circuits, Boolean functions, polynomial-size semantic read-twice branching programs, branching programs, R-way branching program model, Polynomials, Pattern matching, computational complexity]
Multiplicative complexity of Taylor shifts and a new twist of the substitution method
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Let C/sub n/=C/sub n/(K) denote the minimum number of essential multiplications/divisions required for shifting a general n-th degree polynomial A(t)=/spl Sigma/a/sub i/t/sup i/ to some new origin x, which means to compute the coefficients b/sub k/ of the Taylor expansion A(x+t)=B(t)=/spl Sigma/b/sub k/t/sup k/ as elements of K(x,a/sub 0/,...,a/sub n/) with indeterminates a/sub i/ and x over some ground field K. For K of characteristic zero, a new refined version of the substitution method combined with a dimension argument enables us to prove C/sub n//spl ges/n+[n/2]-1 opposed to an upper bound of C/sub n//spl les/2n+[n/2]-4 valid for all n/spl ges/3.
[multiplicative complexity, Upper bound, Costs, substitution method, Ear, upper bound, Polynomials, nth degree polynomial, Taylor shifts, computational complexity]
Algorithms to tile the infinite grid with finite clusters
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We say that a subset T of Z/sup 2/, the two dimensional infinite grid, tiles Z/sup 2/ if we can cover Z/sup 2/ with non-overlapping translates of T. No algorithm is known to decide whether a finite T/spl sube/Z/sup 2/ tiles Z/sup 2/. Here we present two algorithms, one for the case when |T| is prime, and another for the case when |T|=4. Both algorithms generalize to the case, where we replace Z/sup 2/ with all arbitrary finitely generated Abelian group. As a by-product of our results we partially settle the Periodic Tiling Conjecture raised by J. Lagarias and Y. Wang (1997), and we also get the following generalization of a theorem of L.Redei (1965): Let G be a (finite or infinite) Abelian group G with a generator set T of prime cardinality such, that 0/spl isin/T, and there is a set T'/spl sube/G with the property that for every g/spl isin/G there are unique t/spl isin/T, t'/spl isin/T' such that g=t+t'. Then T' can be replaced with a subgroup of G, that also has the above property.
[infinite grid, finitely generated Abelian group, computational geometry, Electronic switching systems, Reflection, Electrical capacitance tomography, Tellurium, Computer science, Lapping, Turing machines, decidability, Tiles, Clustering algorithms, finite clusters, Books]
A linguistic characterization of bounded oracle computation and probabilistic polynomial time
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We present a higher-order functional notation for polynomial-time computation with an arbitrary 0, 1-valued oracle. This formulation provides a linguistic characterization for classes such as NP and BPP, as well as a notation for probabilistic polynomial-time functions. The language is derived from Hofmann's adaptation of Bellantoni-Cook safe recursion, extended to oracle computation via work derived from that of Kapron and Cook. Like Hofmann's language, ours is an applied typed lambda calculus with complexity bounds enforced by a type system. The type system uses a modal operator to distinguish between two sorts of numerical expressions. Recursion can take place on only one of these sorts. The proof that the language captures precisely oracle polynomial time is model-theoretic, using adaptations of various techniques from category theory.
[linguistic characterization, lambda calculus, Adaptation model, higher-order functional notation, modal operator, polynomial-time computation, complexity bounds, Read only memory, Information analysis, bounded oracle computation, Computer languages, Reactive power, typed lambda calculus, probabilistic polynomial time, type system, 1-valued oracle, category theory, Bellantoni-Cook safe recursion, Polynomials, Clocks, computational complexity]
Local divergence of Markov chains and the analysis of iterative load-balancing schemes
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We develop a general technique for the quantitative analysis of iterative distributed load balancing schemes. We illustrate the technique by studying two simple, intuitively appealing models that are prevalent in the literature: the diffusive paradigm, and periodic balancing circuits (or the dimension exchange paradigm). It is well known that such load balancing schemes can be roughly modeled by Markov chains, but also that this approximation can be quite inaccurate. Our main contribution is an effective way of characterizing the deviation between the actual loads and the distribution generated by a related Markov chain, in terms of a natural quantity which we call the local divergence. We apply this technique to obtain bounds on the number of rounds required to achieve coarse balancing in general networks, cycles and meshes in these models. For balancing circuits, we also present bounds for the stronger requirement of perfect balancing, or counting.
[counting, Computational modeling, iterative load-balancing schemes, dimension exchange paradigm, Markov chains, Mathematics, Electrical capacitance tomography, perfect balancing, Application software, Finite element methods, local divergence, Computer science, Processor scheduling, resource allocation, Physics computing, periodic balancing circuits, Markov processes, Load management, quantitative analysis, Context modeling, diffusive paradigm]
On the combinatorial and topological complexity of a single cell
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The problem of bounding the combinatorial complexity of a single connected component (a single cell) of the complement of a set of a geometric objects in R/sup k/, each object of constant description complexity, is an important problem in computational geometry which has attracted much attention over the past decade. It has been conjectured that the combinatorial complexity of a single cell is bounded by a function much closer to O(n/sup k-1/) rather than O(n/sup k/) which is the bound for the combinatorial complexity of the whole arrangement. Till now, this was known to be rule only for k/spl les/3 and only for some special cases in higher dimensions. A classic result in real algebraic geometry due to Oleinik-Petrovsky, Thom and Milnor, bounds the topological complexity (the sum of the Betti numbers) of basic semi-algebraic sets. However, till now no better bounds were known if we restricted attention to a single connected component of a basic semi-algebraic set. In this paper, we show how these two problems are related. We prove a new bound on the sum of the Betti numbers of one connected component of a basic semi-algebraic set which is an improvement over the Oleinik-Petrovsky-Thom-Milnor bound. This also implies that the topological complexity of a single cell, measured by the sum of the Betti numbers, is bounded by O(n/sup k-1/).
[topological complexity, constant description complexity, computational geometry, Mathematics, Electrical capacitance tomography, combinatorial complexity, Betti numbers, Computational geometry, single connected component, Polynomials, geometric objects, Fellows, computational complexity]
The quantum communication complexity of sampling
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Sampling is an important primitive in probabilistic and quantum algorithms. In the spirit of communication complexity, given a function f: X/spl times/Y/spl rarr/{0,1} and a probability distribution D over X/spl times/Y, we define the sampling complexity of (f,D) as the minimum number of bits Alice and Bob must communicate for Alice to pick x/spl isin/X and Bob to pick y/spl isin/Y as well as a valve z s.t. the resulting distribution of (x,y,z) is close to the distribution (D,f(D)). In this paper we initiate the study of sampling complexity, in both the classical and quantum model. We give several variants of the definition. We completely characterize some of these tasks, and give upper and lower bounds on others. In particular this allows us to establish an exponential gap between quantum and classical sampling complexity, for the set disjointness function. This is the first exponential gap for any task where the classical probabilistic algorithm is allowed to err.
[Protocols, Information retrieval, quantum algorithm, Probability distribution, Complexity theory, communication complexity, sampling complexity, Computer science, Quantum computing, Quantum mechanics, classical probabilistic algorithm, Sampling methods, Random variables, quantum communication, Information theory]
Random sampling, halfspace range reporting, and construction of (/spl les/k)-levels in three dimensions
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Given n points in three dimensions, we show how to answer halfspace range reporting queries in O(log n+k) expected time for an output size k. Our data structure can be preprocessed in optimal O(n log n) expected time. We apply this result to obtain the first optimal randomized algorithm for the construction of the (/spl les/k)-level in an arrangement of n planes in three dimensions. The algorithm runs in O(n log n+nk/sup 2/) expected time. Our techniques are based on random sampling. Applications in two dimensions include an improved data structure for "k nearest neighbors" queries, and an algorithm that constructs the order-k Voronoi diagram in O(n log n+nk log k) expected time.
[Particle separators, data structure, computational geometry, Data structures, random sampling, nearest neighbors, Mathematics, optimal randomized algorithm, Nearest neighbor searches, randomised algorithms, order-k Voronoi diagram, Computer science, Computational geometry, Monte Carlo methods, Sampling methods, halfspace range reporting, data structures]
Evolutionary trees can be learned in polynomial time in the two-state general Markov model
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The j-State General Markov Model of evolution M. Steel (1994) is a stochastic model concerned with the evolution of strings over an alphabet of size j. In particular, the Two-State General Markov Model of evolution generalises the well-known Cavender-Farris-Neyman model of evolution by removing the symmetry restriction (which requires that the probability that a '0'' turns into a '1' along an edge is the same as the probability that a '1' turns into a '0' along the edge). M. Farach and S. Kannan (1996) showed how to PAC-learn Markov Evolutionary Trees in the Cavender-Farris-Neyman model provided that the target tree satisfies the additional restriction that all pairs of leaves have a sufficiently high probability of being the same. We show how to remove both restrictions and thereby obtain the first polynomial-time PAC-learning algorithm (in the sense of Kearns et al.) for the general class of Two-State Markov Evolutionary Trees.
[two-state general Markov model, Stochastic processes, trees (mathematics), State-space methods, Cavender-Farris-Neyman model, Steel, Computer science, PAC-learning, DNA, evolutionary trees, Markov processes, Polynomials, polynomial time, learning (artificial intelligence), stochastic model]
The access network design problem
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We consider the problem of designing a minimum cost access network to carry traffic from a set of endnodes to a core network. A set of trunks of K differing types are available for leasing or buying. Some trunk-types have a high initial overhead cost but a low cost per unit bandwidth. Others have a low overhead cost but a high cost per unit bandwidth. When the central core is given, we show how to construct an access network whose cost is within O(K/sup 2/) of optimal, under weak assumptions on the cost structure. In contrast with previous bounds, this bound is independent of the network and the traffic. Typically, the value of K is small. Our approach uses a linear programming relaxation and is motivated by a rounding technique of Shmoys, Tardos and Aardal (1997). Our techniques extend to a more complex situation in which the core is not given a priori. In this case we aim to minimize the switch cost of the core in addition to the trunk cost of the access network. We provide the same performance bound.
[access network design, Switches, Telecommunication traffic, Telecommunication switching, Linear programming, linear programming, Demultiplexing, minimum cost access network, telecommunication networks, rounding technique, Bandwidth, Traffic control, Economies of scale, performance bound, Cost function, core network, Communication networks]
Quantum cryptography with imperfect apparatus
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Quantum key distribution, first proposed by C.H. Bennett and G. Brassard (1984), provides a possible key distribution scheme whose security depends only on the quantum laws of physics. So far the protocol has been proved secure even under channel noise and detector faults of the receiver but is vulnerable if the photon source used is imperfect. In this paper we propose and give a concrete design for a new concept, self-checking source, which requires the manufacturer of the photon source to provide certain tests; these tests are designed such that, if passed, the source is guaranteed to be adequate for the security of the quantum key distribution protocol, even though the testing devices may not be built to the original specification. The main mathematical result is a structural theorem which states that, for any state in a Hilbert space, if certain EPR-type equations are satisfied, the state must be essentially the orthogonal sum of EPR pairs.
[imperfect apparatus, Pulp manufacturing, Optical receivers, Security, key distribution scheme, Physics, Cryptographic protocols, self-checking source, protocol, Automatic testing, Fault detection, quantum cryptography, Detectors, Concrete, Hilbert space, Cryptography, protocols]
Concurrent reachability games
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
An open system can be modeled as a two-player game between the system and its environment. At each round of the game, player 1 (the system) and player 2 (the environment) independently and simultaneously choose moves, and the two choices determine the next state of the game. Properties of open systems can be modeled as objectives of these two-player games. For the basic objective of reachability-can player 1 force the game to a given set of target states?-there are three types of winning states, according to the degree of certainty with which player 1 can reach the target. From type-1 states, player 1 has a deterministic strategy to always reach the target. From type-2 states, player 1 has a randomized strategy to reach the target with probability 1. From type-3 states, player 1 has for every real /spl epsi/>0 a randomized strategy to reach the target with probability greater than 1-/spl epsi/. We show that for finite state spaces, all three sets of winning states can be computed in polynomial time: type-1 states in linear time, and type-2 and type-3 states in quadratic time. The algorithms to compute the three sets of winning states also enable the construction of the winning and spoiling strategies. Finally, we apply our results by introducing a temporal logic in which all three kinds of winning conditions can be specified, and which can be model checked in polynomial time. This logic, called Randomized ATL, is suitable for reasoning about randomized behavior in open (two-agent) as well as multi-agent systems.
[randomized ATL, finite state spaces, multi-agent systems, Engineering profession, open systems, NASA, concurrent reachability games, game theory, temporal logic, open system, State-space methods, deterministic strategy, deterministic algorithms, Tellurium, randomised algorithms, two-player game, Open systems, Ear, Polynomials, Contracts, randomized strategy]
Geometric separator theorems and applications
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We find a large number of "geometric separator theorems" such as: I: Given N disjoint isooriented squares in the plane, there exists a rectangle with /spl les/2N/3 squares inside, /spl les/2N/3 squares outside, and /spl les/(4+0(1))/spl radic/N partly in & out. II: There exists a rectangle that is crossed by the minimal spanning tree of N sites in the plane at /spl les/(4/spl middot/3/sup 1/4/+0(1))/spl radic/N points, having /spl les/2N/3 sites inside and outside. These theorems yield a large number of applications, such as subexponential algorithms for traveling salesman tour and rectilinear Steiner minimal tree in R/sup d/, new point location algorithms, and new upper and lower bound proofs for "planar separator theorems".
[Steiner trees, Particle separators, geometric separator theorems, minimal spanning tree, Traveling salesman problems, computational geometry, Statistics, rectilinear Steiner minimal tree, traveling salesman tour, travelling salesman problems, Tree graphs, planar separator theorems, point location algorithms, subexponential algorithms, lower bound proofs]
Overcoming the memory bottleneck in suffix tree construction
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The suffix tree of a string is the fundamental data structure of string processing. Recent focus on massive data sets has sparked interest in overcoming the memory bottlenecks of known algorithms for building suffix trees. Our main contribution is a new algorithm for suffix tree construction in which we choreograph almost all disk accesses to be via the sort and scan primitives. This algorithm achieves optimal results in a variety of sequential and parallel computational models. Two of our results are: In the traditional external memory model, in which only the number of disk accesses is counted, we achieve an optimal algorithm, both for single and multiple disk cases. This is the first optimal algorithm known for either model. Traditional disk page access counting does not differentiate between random page accesses and block transfers involving several consecutive pages. This difference is routinely exploited by expert programmers to get fast algorithms on real machines. We adopt a simple accounting scheme and show that our algorithm achieves the same optimal tradeoff for block versus random page accesses as the one we establish for sorting.
[Tree data structures, Algorithm design and analysis, Computational modeling, Buildings, data structure, Data warehouses, external memory model, random page accesses, memory bottleneck, Data mining, Programming profession, Sorting, Concurrent computing, Software libraries, sorting, tree data structures, suffix tree construction]
Orchestrating quartets: approximation and data correction
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Inferring evolutionary trees has long been a challenging problem both for biologists and computer scientists. In recent years research has concentrated on the quartet method paradigm for inferring evolutionary trees. Quartet methods proceed by first inferring the evolutionary history for every set of four species (resulting in a set Q of inferred quarter topologies) and then recombining these inferred quarter topologies to form an evolutionary tree. This paper presents two results on the quartet method paradigm. The first is a polynomial time approximation scheme (PTAS) for recombining the inferred quartet topologies optimally. This is an important result since, to date, there have been no polynomial time algorithms with performance guarantees for quartet methods. In fact, this is the first known PTAS for inferring evolutionary trees under any paradigm. To achieve this result the natural denseness of the set Q is exploited. The second result is a new technique, called quartet cleaning, that detects and corrects errors in the set Q with performance guarantees. This result has particular significance since quartet methods are usually very sensitive to errors in the data. It is shown how quartet cleaning can dramatically increase the accuracy of quartet methods.
[quartets, trees (mathematics), Topology, History, Surges, Computer science, Microwave integrated circuits, data correction, polynomial approximation, performance guarantees, evolutionary trees, polynomial time approximation scheme, Biology computing, Error correction, Computational biology, computational complexity]
Probabilistically checkable proofs with low amortized query complexity
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The error probability of Probabilistically Checkable Proof (PCP) systems can be made exponentially small in the number of queries by using sequential repetition. In this paper we are interested in determining the precise rate at which the error goes down in an optimal protocol, and we make substantial progress toward a tight resolution of this question. A PCP verifier uses q~ amortized query bits if, for some t, it makes q~t queries and has error probability at most 2/sup -t/. A PCP characterization of NP using 2.5 amortized query bits is known, and, unless P=NP, no such characterization is possible using 1 amortized query bits. We present a PCP characterization of NP that uses roughly 1.5 amortized query bits. Our result has two main implications. Separating PCP from 2-Provers 1-Round: In the 2-Provers 1-Round (2P1R) model the verifier has access to two oracles (or provers) and can make one query to each oracle. Each answer is a string of l bits (l is called the answer size). A 2P1R protocol with answer size l can be simulated by a PCP that reads 21 bits; we show that the converse does not hold for l/spl ges/7, unless P=NP. No such separation was known before. The Max kCSP problem: The Boolean constraint satisfaction problem with constraints involving at most k variables, usually called Max kCSP, is known to be hard to approximate within a factor 2/sup -4k/, and a 2.2/sup -k/-approximation algorithm is also known. We prove that Max kCSP is NP-hard to approximate within a factor of roughly 2/sup -2k/3/.
[low amortized query complexity, NP-hard, Error probability, Laboratories, 2P1R protocol, Access protocols, error probability, query complexity, Electrical capacitance tomography, Computer science, Bridges, Probabilistically Checkable Proof, Computer errors, Approximation algorithms, theorem proving, error statistics, computational complexity]
Approximating-CVP to within almost-polynomial factors is NP-hard
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
This paper shows the closest vector in a lattice to be NP-hard to approximate to within any factor up to 2/sup (logn)1-4/ where /spl epsiv/=(loglogn)/sup -c/ for any constant c< 1/2.
[almost-polynomial factors, NP-hard, combinatorial mathematics, approximating-CVP, closest vector, Lattices, Vectors, Polynomials, Cryptography, NP-complete problem, computational complexity]
Approximating a finite metric by a small number of tree metrics
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Y. Bartal (1996, 1998) gave a randomized polynomial time algorithm that given any n point metric G, constructs a tree T such that the expected stretch (distortion) of any edge is at most O (log n log log n). His result has found several applications and in particular has resulted in approximation algorithms for many graph optimization problems. However approximation algorithms based on his result are inherently randomized. In this paper we derandomize the use of Bartal's algorithm in the design of approximation algorithms. We give an efficient polynomial time algorithm that given a finite n point metric G, constructs O(n log n) trees and a probability distribution /spl mu/ on them such that the expected stretch of any edge of G in a tree chosen according to /spl mu/ is at most O(log n log log n). Our result establishes that finite metrics can be probabilistically approximated by a small number of tree metrics. We obtain the first deterministic approximation algorithms for buy-at-bulk network design and vehicle routing; in addition we subsume results from our earlier work on derandomization. Our main result is obtained by a novel view of probabilistic approximation of metric spaces as a deterministic optimization problem via linear programming.
[graph optimization problems, trees (mathematics), deterministic approximation algorithms, randomized polynomial time algorithm, Extraterrestrial measurements, Routing, Linear programming, linear programming, Tellurium, Read only memory, randomised algorithms, tree metrics, Computer science, Tree graphs, polynomial approximation, Ear, probability distribution, finite metric, Contracts]
Recommendation systems: a probabilistic analysis
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
A recommendation system tracks past actions of a group of users to make recommendations to individual members of the group. The growth of computer-mediated marketing and commerce has led to increased interest in such systems. We introduce a simple analytical framework for recommendation systems, including a basis for defining the utility of such a system. We perform probabilistic analyses of algorithmic methods within this framework. These analyses yield insights into how much utility can be derived from the memory of past actions and on how this memory can be exploited.
[Algorithm design and analysis, algorithmic methods, Random access memory, probability, probabilistic analysis, Information filtering, Electrical capacitance tomography, marketing data processing, computer-mediated marketing, Microwave integrated circuits, recommendation systems, Collaboration, Filtering algorithms, Information filters, Books, Business]
A randomized approximation scheme for metric MAX-CUT
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Metric MAX-CUT is the problem of dividing a set of points in metric space into two parts so as to maximize the sum of the distances between points belonging to distinct parts. We show that metric MAX-CUT has a polynomial time randomized approximation scheme.
[Algorithm design and analysis, metric MAX-CUT, Traveling salesman problems, computational geometry, World Wide Web, Extraterrestrial measurements, Partitioning algorithms, randomised algorithms, randomized approximation scheme, polynomial approximation, Clustering algorithms, Web pages, Approximation algorithms, Polynomials, Space exploration, polynomial time randomized approximation scheme]
Marked ancestor problems
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Consider a rooted tree whose nodes can be in two states: marked or unmarked. The marked ancestor problem is to maintain a data structure with the following operations: mark(v) marks node v: unmark(v) removes any marks from node v; firstmarked(v) returns the first marked node on the path from v to the root. We show tight upper and lower bounds for the marked ancestor problem. The lower bounds are proved in the cell probe model, the algorithms run on a unit-cost RAM. As easy corollaries we prove (often optimal) lower bounds on a number of problems. These include planar range searching, including the existential or emptiness problem, priority search trees static tree union-find, and several problems from dynamic computational geometry, including segment intersection, interval maintenance, and ray shooting in the plane. Our upper bounds improve algorithms from various fields, including coloured ancestor problems and maintenance of balanced parentheses.
[trees (mathematics), marked ancestor problems, data structure, computational geometry, upper bounds, Data structures, rooted tree, ray shooting, dynamic computational geometry, Read only memory, lower bounds, Computational geometry, coloured ancestor problems, Upper bound, planar range searching, balanced parentheses, segment intersection, DNA, cell probe model, interval maintenance, Probes, computational complexity]
Unsatisfiable systems of equations, over a finite field
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The properties of any system of k simultaneous equations in n variables over GF(q), are studied, with a particular emphasis on unsatisfiable systems. A general formula for the number of solutions is given, which can actually be useful for computing that number in the special case where all the equations are of degree 2. When such a quadratic system has no solution, there is always a proof of unsatisfiability of size q/sup n/2/ times a polynomial in n and q, which can be checked deterministically in time satisfying a similar bound. Such a proof can be found by a probabilistic algorithm in time asymptotic to that required to test, by substitution in k quadratic equations, all q/sup n/ potential solutions.
[System testing, Costing, computability, Mathematics, Galois fields, Equations, unsatisfiable systems of equations, Reactive power, finite field, probabilistic algorithm, Ash, Polynomials, Australia, computational complexity]
A primitive recursive algorithm for the general Petri net reachability problem
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
E. Mayr and R. Kosaraju (1981) proved the decidability of the general Petri net reachability problem. However their algorithms are non primitive recursive. Since then the primitive recursiveness of this problem was stated as an open problem. In this paper we give a double exponential space algorithm for the general Petri net reachability problem.
[Computational modeling, Petri nets, Equations, Concurrent computing, Turing machines, Algebra, decidability, double exponential space algorithm, Automata, primitive recursive algorithm, System recovery, general Petri net reachability problem, Polynomials]
Faster and simpler algorithms for multicommodity flow and other fractional packing problems
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
This paper considers the problem of designing fast, approximate, combinatorial algorithms for multicommodity flows and other fractional packing problems. We provide a different approach to these problems which yields faster and much simpler algorithms. Our approach also allows us to substitute shortest path computations for min-cost flow computations in computing maximum concurrent flow and min-cost multicommodity flow; this yields much faster algorithms when the number of commodities is large.
[Concurrent computing, Computer science, min-cost flow computations, Design engineering, combinatorial algorithms, graph theory, multicommodity flows, Throughput, Polynomials, fractional packing problems, shortest path computations, computational complexity]
Optimal time-space trade-offs for sorting
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We study the fundamental problem of sorting in a sequential model of computation and in particular consider the time-space trade-off (product of time and space) for this problem. P. Beame (1991) has shown a lower bound of /spl Omega/(n/sup 2/) for this product leaving a gap of a logarithmic factor up to the previously best known upper bound of O(n/sup 2/ log n) due to G.N. Frederickson (1987). Since then, no progress has been made towards tightening this gap. The main contribution of this paper is a comparison based sorting algorithm which closes the gap by meeting the lower bound of Beame. The time-space product O(n/sup 2/) upper bound holds for the full range of space bounds between log n and n/log n. Hence in this range our algorithm is optimal for comparison based models as well as for the very powerful general models considered by Beame.
[logarithmic factor, Extraterrestrial measurements, upper bound, Time measurement, lower bound, Sorting, Postal services, Computer science, time-space product, optimal time-space trade-offs, Upper bound, sorting, Chromium, sequential model, comparison based models, computational complexity]
A characterization of NC by tree recurrence
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We show that a boolean valued function is in NC if it is defined by ramified schematic recurrence over trees. This machine-independent characterization uses no initial functions other than basic tree operations, and no bounding conditions on the recurrence. Aside from its technical interest, our result evidences the foundational nature of NC, thereby illustrating the merits of implicit (i.e. machine independent) computational complexity theory.
[NC characterisation, trees (mathematics), Programming, Computational complexity, Computer science, Concurrent computing, Computer languages, Boolean functions, Algebra, Databases, ramified schematic recurrence, Parallel processing, boolean valued function, Logic, tree recurrence, machine-independent characterization, Arithmetic, computational complexity]
Improved bounds and algorithms for hypergraph two-coloring
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We show that for all large n, every n-uniform hypergraph with at most 0.7/spl radic/(n/lnn)/spl times/2/sup n/ edges can be two-colored. We, in fact, present fast algorithms that output a proper two-coloring with high probability for such hypergraphs. We also derandomize and parallelize these algorithms, to derive NC/sup 1/ versions of these results. This makes progress on a problem of Erdos (1963), improving the previous-best bound of n/sup 1/3-0(1)//spl times/2/sup n/ due to Beck (1978). We further generalize this to a "local" version, improving on one of the first applications of the Lovasz Local Lemma.
[algorithms, probability, computational geometry, Mathematics, Application software, History, Parallel algorithms, graph colouring, Computer science, bounds, n-uniform hypergraph, Approximation algorithms, Lab-on-a-chip, hypergraph two-coloring, Polynomials, Erbium, Contracts]
Lower bounds for zero knowledge on the Internet
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We consider zero knowledge interactive proofs in a richer, more realistic communication environment. In this setting, one may simultaneously engage in many interactive proofs, and these proofs may take place in an asynchronous fashion. It is known that zero-knowledge is not necessarily preserved in such an environment; we show that for a large class of protocols, it cannot be preserved. Any 4 round (computational) zero-knowledge interactive proof (or argument) for a non-trivial language L is not black-box simulatable in the asynchronous setting.
[communication environment, Protocols, Costs, zero knowledge interactive proofs, cryptography, lower bounds, Concurrent computing, Computer errors, Computer networks, Internet, Workstations, Cryptography, IP networks, protocols, Local area networks]
Local search in smooth convex sets
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
In this paper we analyse two very simple techniques to minimize a linear function over a convex set. The first is a deterministic algorithm based on gradient descent. The second is a randomized algorithm which makes a small local random change at every step. The second method can be used when the convex set is presented by just a membership oracle whereas the first requires something similar to a separation oracle. We define a simple notation of smoothness of convex sets and show that both algorithms provide a near optimal solution for smooth convex sets in polynomial time. We describe several application examples from linear and stochastic programming where the relevant sets are indeed smooth and thus our algorithms apply. The main point of the paper is that such simple algorithms yield good running time bounds for natural problems.
[Level set, smooth convex sets, membership oracle, Optimized production technology, Stochastic processes, stochastic programming, time bounds, computational geometry, Linear programming, linear programming, Electrical capacitance tomography, randomized algorithm, deterministic algorithm, deterministic algorithms, local search, Read only memory, randomised algorithms, Computer science, Ear, Marine vehicles, near optimal solution, Arithmetic]
On approximate nearest neighbors in non-Euclidean spaces
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The nearest neighbor search (NNS) problem is the following: Given a set of n points P={p/sub 1/,...,p/sub n/} in some metric space X, preprocess P so as to efficiently answer queries which require finding a point in P closest to a query point q/spl isin/X. The approximate nearest neighbor search (c-NNS) is a relaxation of NNS which allows to return any point within c times the distance to the nearest neighbor (called c-nearest neighbor). This problem is of major and growing importance to a variety of applications. In this paper we give an algorithm for (4log/sub 1+/spl rho//log4d+3)-NNS algorithm in l/sub /spl infin///sup d/ with O(dn/sup 1+/spl rho//logn) storage and O(dlogn) query time. In particular this yields the first algorithm for O(1)-NNS for l/sub /spl infin// with subexponential storage. The preprocessing time is linear in the size of the data structure. The algorithm can be also used (after simple modifications) to output the exact nearest neighbor in time bounded bounded O(dlogn) plus the number of (4log/sub 1+/spl rho//log4d+3)-nearest neighbors of the query point. Building on this result, we also obtain an approximation algorithm for a general class of product metrics. Finally: we show that for any c<3 the c-NNS problem in l/sub /spl infin// is provably hard for a version of the indexing model introduced by Hellerstein et al. (1997).
[Bridges, data structure, computational geometry, Extraterrestrial measurements, Cost function, Time measurement, data structures, approximate nearest neighbor search, Topology, non-Euclidean spaces, Nearest neighbor searches, nearest neighbors approximation]
Randomness vs. time: de-randomization under a uniform assumption
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We prove that if BPP/spl ne/EXP, then every problem in BPP can be solved deterministically in subexponential time on almost every input (on every samplable ensemble for infinitely many input sizes). This is the first derandomization result for BPP based on uniform, noncryptographic hardness assumptions. It implies the following gap in the average-instance complexities of problems in BPP: either these complexities are always sub-exponential or they contain arbitrarily large exponential functions. We use a construction of a small "pseudorandom" set of strings from a "hard function" in EXP which is identical to that used in the analogous non-uniform results described previously. However, previous proofs of correctness assume the "hard function" is not in P/poly. They give a non-constructive argument that a circuit distinguishing the pseudo-random strings from truly random strings implies that a similarly-sized circuit exists computing the "hard function". Our main technical contribution is to show that, if the "hard function" has certain properties, then this argument can be made constructive. We then show that, assuming ESP/spl sube/P/poly, there are EXP-complete functions with these properties.
[Algorithm design and analysis, noncryptographic hardness, randomness, derandomization, average-instance complexities, History, uniform assumption, pseudo-random strings, Counting circuits, randomised algorithms, Computer science, proofs of correctness, Polynomials, Concrete, Cryptography, EXP-complete functions, computational complexity]
Parametric and kinetic minimum spanning trees
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We consider the parametric minimum spanning tree problem, in which we are given a graph with edge weights that are linear functions of a parameter /spl lambda/ and wish to compute the sequence of minimum spanning trees generated as /spl lambda/ varies. We also consider the kinetic minimum spanning tree problem, in which /spl lambda/ represents time and the graph is subject in addition to changes such as edge insertions, deletions, and modifications of the weight functions as time progresses. We solve both problems in time O(n/sup 2/3/log/sup 4/3/) per combinatorial change in the tree (or randomized O(n/sup 2/3/log/sup 4/3/ n) per change). Our time bounds reduce to O(n/sup 1/2/log/sup 3/2/ n) per change (O(n/sup 1/2/log n) randomized) for planar graphs or other minor-closed families of graphs, and O(n/sup 1/4/log/sup 3/2/ n) per change (O(n/sup 1/4/ log n) randomized) for planar graphs with weight changes but no insertions or deletions.
[Costs, Military computing, deletions, Stochastic processes, trees (mathematics), computational geometry, edge weights, parametric minimum spanning tree problem, Computer science, optimisation, Tree graphs, weight functions, kinetic minimum spanning trees, linear functions, Kinetic theory, edge insertions]
Jitter control in QoS networks
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We study jitter control in networks guaranteeing quality of service (QoS). Jitter measures variability of delivery times in packet streams. We propose on-line algorithms that control jitter and compare their performance to the best possible (by an off-line algorithm) for any given arrival sequence. For delay jitter, where the goal is to minimize the difference between delay times of different packets, we give an on-line algorithm using buffer size of 2B which guarantees the same delay-jitter as an off-line algorithm using buffer space B. We show that 2B space is the minimum space required by any on-line algorithm to provide delay-jitter related to the best possible delay-jitter using B buffer space. We also show that the guarantees made by our online algorithm hold even for distributed implementations, where the total buffer space is distributed along the path of the connection, provided that the input stream satisfies a certain simple property. For rate jitter, where the goal is to minimize the difference between inter-arrival times, we develop an on-line algorithm using a buffer of size 2B+h for any h/spl ges/1, and compare its jitter to the jitter of an optimal off-line algorithm using buffer size B. Our algorithm guarantees that the difference is bounded by a term proportional to B/h. We also prove that 2B space is necessary for on-line algorithms with non trivial guarantees for rate-jitter control.
[Packet switching, Government, packet switching, Quality of service, Switches, Jitter, on-line algorithms, jitter control, quality of service, 2B space, Computer science, Intelligent networks, on-line algorithm, jitter, packet streams, telecommunication network reliability, IP networks, Propagation delay, Asynchronous transfer mode]
Perfect information leader election in log*n+O(1) rounds
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
In the leader election problem, n players wish to elect a random leader. The difficulty is that some coalition of players may conspire to elect one of its own members. We adopt the perfect information model: all communication is by broadcast, and the bad players have unlimited computational power. Within a round, they may also wait to see the inputs of the good players. A protocol is called resilient if a good leader is elected with probability bounded away from 0. We give a simple, constructive leader election protocol that is resilient against coalitions of size /spl beta/n, for any /spl beta/<1/2. Our protocol takes log*n+O(1) rounds, each player sending at most log n bits per round. For any constant k, our protocol can be modified to take k rounds and be resilient against coalitions of size /spl epsi/n(log/sup (k)/n)/sup 3/, where /spl epsi/ is a small enough constant and log(k) denotes the logarithm iterated k times. This is constructive for k/spl ges/3.
[Protocols, Upper bound, protocol, Nominations and elections, Polynomials, protocols, Resilience, computational complexity, perfect information leader election, random leader]
Testing monotonicity
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We present a (randomized) test for monotonicity of Boolean functions. Namely, given the ability to query an unknown function f: {0, 1}/sup n/-{0, 1} at arguments of its choice, the test always accepts a monotone f, and rejects f with high probability if it is /spl epsiv/-far from being monotone (i.e., every monotone function differs from f on more than an /spl epsiv/ fraction of the domain). The complexity of the test is poly(n//spl epsiv/). The analysis of our algorithm relates two natural combinatorial quantities that can be measured with respect to a Boolean function; one being global to the function and the other being local to it. We also consider the problem of testing monotonicity based only on random examples labeled by the function. We show an /spl Omega/(/spl radic/2/sup n///spl epsiv/) lower bound on the number of required examples, and provide a matching upper bound (via an algorithm).
[Algorithm design and analysis, Gold, complexity, Laboratories, monotone function, probability, upper bound, Mathematics, monotonicity, Electrical capacitance tomography, lower bound, randomized test, Postal services, Computer science, Upper bound, Boolean functions, Lab-on-a-chip, combinatorial quantities, Testing, computational complexity]
Improved decoding of Reed-Solomon and algebraic-geometric codes
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Given an error-correcting code over strings of length n and an arbitrary input string also of length n, the list decoding problem is that of finding all codewords within a specified Hamming distance from the input string. We present an improved list decoding algorithm for decoding Reed-Solomon codes. The list decoding problem for Reed-Solomon codes reduces to the following "curve-fitting" problem over a field F: Given n points {(x/sub i/.y/sub i/)}/sub i=1//sup n/, x/sub i/,y/sub i//spl isin/F, and a degree parameter k and error parameter e, find all univariate polynomials p of degree at most k such that y/sub i/=p(x/sub i/) for all but at most e values of i/spl isin/{1....,n}. We give an algorithm that solves this problem for e<n-/spl radic/(kn), which improves over the previous best result, for every choice of k and n. Of particular interest is the case of k/n>1/3, where the result yields the first asymptotic improvement in four decades. The algorithm generalizes to solve the list decoding problem for other algebraic codes, specifically alternant codes (a class of codes including BCH codes) and algebraic-geometric codes. In both cases, we obtain a list decoding algorithm that corrects up to n-/spl radic/(n-d-) errors, where n is the block length and d' is the designed distance of the code. The improvement for the case of algebraic-geometric codes extends the methods of Shokrollahi and Wasserman (1998) and improves upon their bound for every choice of n and d'. We also present some other consequences of our algorithm including a solution to a weighted curve fitting problem, which is of use in soft-decision decoding algorithms for Reed-Solomon codes.
[Hamming distance, Decoding, Read only memory, decoding, Reed-Solomon codes, algebraic geometric codes, Space technology, list decoding, Computer errors, Polynomials, Error correction, Error correction codes, Curve fitting, error-correcting code, algebraic-geometric codes]
Random projection: a new approach to VLSI layout
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We show that random projection, the technique of projecting a set of points to a randomly chosen low-dimensional subspace, can be used to solve problems in VLSI layout. Specifically, for the problem of laying out a graph on a 2-dimensional grid so as to minimize the maximum edge length, we obtain an O(log/sup 3.5/ n) approximation algorithm (this is the first o(n) approximation), and for the bicriteria problem of minimizing the total edge length while keeping the maximum length bounded, we obtain an O(log/sup 3/ n, log/sup 3.5/ n) approximation. Our algorithms also work for d-dimensional versions of these problems (for any fixed d) with polylog approximation guarantees. Besides random projection, the main components of the algorithms are a linear programming relaxation, and volume-respecting Euclidean embeddings.
[Algorithm design and analysis, bicriteria problem, Particle separators, VLSI, Laboratories, total edge length, Very large scale integration, computational geometry, Linear programming, Mathematics, linear programming, volume-respecting Euclidean embeddings, Wire, Computer science, randomly chosen low-dimensional subspace, Euclidean distance, VLSI layout, Approximation algorithms, circuit layout, random projection]
Towards an optimal bit-reversal permutation program
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The speed of many computations is limited not by the number of arithmetic operations but by the time it takes to move and rearrange data in the increasingly complicated memory hierarchies of modern computers. Array transpose and the bit-reversal permutation-trivial operations on a RAM-present non-trivial problems, when designing highly-tuned scientific library functions, particular for the Fast Fourier Transform. We prove a precise bound for RoCol, a simple pebble-type game that is relevant to implementing these permutations. We use RoCol to give lower bounds on the amount of memory traffic in a computer with four-levels of memory (registers, cache, TLB, and memory), taking into account such "messy" features as block moves and set-associative caches. The insights from this analysis lead to a bit-reversal algorithm whose performance is close to the theoretical minimum. Experiments show that it performs significantly better than every program in a comprehensive study of 30 published algorithms.
[Costs, Computational modeling, pebble-type game, Random access memory, game theory, Read-write memory, Drives, memory hierarchies, Registers, Read only memory, fast Fourier transform, Computer science, RoCol, arithmetic operations, bit-reversal algorithm, Libraries, Argon, optimal bit-reversal permutation program, computational complexity]
All pairs shortest paths in weighted directed graphs-exact and almost exact algorithms
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We present two new algorithms for solving the All Pairs Shortest Paths (APSP) problem for weighted directed graphs. Both algorithms use fast matrix multiplication algorithms. The first algorithm solves the APSP problem for weighted directed graphs in which the edge weights are integers of small absolute value in O/spl tilde/(n/sup 2+/spl mu//) time, where /spl mu/ satisfies the equation /spl omega/(1,/spl mu/,1)=1+2/spl mu/ and /spl omega/(1,/spl mu/,1) is the exponent of the multiplication of an n/spl times/n/sup /spl mu// matrix by an n/sup /spl mu///spl times/n matrix. The currently best available bounds on /spl omega/(1,/spl mu/,1), obtained by Coppersmith and Winograd, and by Huang and Pan, imply that /spl mu/<0.575. The running time of our algorithm is therefore O(n/sup 2.575/). Our algorithm improves on the O/spl tilde/(n/sup (3+/spl omega/)/2/) time algorithm, where /spl omega/=/spl omega/(1,1,1)<2.376 is the usual exponent of matrix multiplication, obtained by Alon, Galil and Margalit, whose running time is only known to be O(n/sup 2.688/). The second algorithm solves the APSP problem almost exactly for directed graphs with arbitrary non-negative real weights. The algorithm runs in O/spl tilde/((n/sup /spl omega////spl epsiv/)/spl middot/log(W//spl epsiv/)) time, where /spl epsiv/>0 is an error parameter and W is the largest edge weight in the graph, after the edge weights are scaled so that the smallest non-zero edge weight in the graph is 1. It returns estimates of all the distances in the graph with a stretch of at most 1+/spl epsiv/. Corresponding paths can also be found efficiently.
[Computer science, All Pairs Shortest Paths, matrix multiplication, weighted directed graphs, directed graphs, Copper, APSP problem, Equations, computational complexity]
Exponential complexity lower bounds for depth 3 arithmetic circuits in algebras of functions over finite fields
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
A depth 3 arithmetic circuit can be viewed as a sum of products of linear functions. We prove an exponential complexity lower bound on depth 3 arithmetic circuits computing some natural symmetric functions over a finite field F. Also, we study the complexity of the functions f: D/sup n//spl rarr/F for subsets D/spl sub/F. In particular, we prove an exponential lower bound on the complexity of a depth 3 arithmetic circuit which computes the determinant or the permanent of a matrix considered as functions f:(F*)n/sup 2//spl rarr/F.
[symmetric functions, polynomials, Circuits, depth 3 arithmetic circuits, Mathematics, Complexity theory, algebras of functions, Galois fields, Computer science, finite fields, Algebra, exponential complexity lower bounds, Linear approximation, Digital arithmetic, Polynomials, linear functions, computational complexity]
Oblivious transfer with a memory-bounded receiver
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We propose a protocol for oblivious transfer that is unconditionally secure under the sole assumption that the memory size of the receiver is bounded. The model assumes that a random bit string slightly larger than the receiver's memory is broadcast (either by the sender or by a third party). In our construction, both parties need memory of size in /spl theta/(n/sup 2-2/spl alpha//) for some /spl alpha/< 1/2 , when a random string of size N=n/sup 2-/spl alpha/-/spl beta// is broadcast, for /spl alpha/>/spl beta/>0, whereas a malicious receiver can have up to /spl gamma/N bits of memory for any /spl gamma/<1. In the course of our analysis, we provide a direct study of an interactive hashing protocol closely related to that of M. Naor et al. (1998).
[Laboratories, cryptography, Security, Cryptographic protocols, Computer science, random bit string, Quantum computing, protocol, Collaboration, memory-bounded receiver, Broadcasting, oblivious transfer, memory size, interactive hashing protocol, Cryptography, protocols]
Pattern matching for spatial point sets
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Two sets of points in d-dimensional space are given: a data set D consisting of N points, and a pattern set or probe P consisting of k points. We address the problem of determining whether there is a transformation, among a specified group of transformations of the space, carrying P into or near (meaning at a small directed Hausdorff distance of) D. The groups we consider are translations and rigid motions. Runtimes of approximately O(nlogn) and O(n/sup d/logn) respectively are obtained (letting n=max{N,k} and omitting the effects of several secondary parameters). For translations, a runtime of approximately O(n(ak+1)log/sup 2/n) is obtained for the case that a constant fraction /spl alpha/<1 of the points of the probe is allowed to fail to match.
[pattern matching, d-dimensional space, computational geometry, Educational institutions, Electronic switching systems, Electrical capacitance tomography, spatial point sets, Runtime, Constellation diagram, Space technology, rigid motions, Probes, directed Hausdorff distance, Pattern matching]
Stability of adversarial queues via fluid models
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The subject of this paper is stability properties of adversarial queueing networks. Such queueing systems are used to model packet switch communication networks, in which packets are generated and routed dynamically, and have become a subject of research focus recently. Adversarial queueing networks are defined to be stable, if the number of packets stays bounded over time. A central question is determining which adversarial queueing networks are stable, when an arbitrary greedy packet routing policy is implemented. In this paper we show how stability of a queueing network can be determined by considering an associated fluid models. Our main result is that the stability of the fluid model implies the stability of an underlying adversarial queueing network. This opens an opportunity for analyzing stability of adversarial networks, using established stability methods from continuous time processes, for example, the method of Lyapunov function or trajectory decomposition. We demonstrate the use of these methods on several examples.
[queueing theory, greedy packet routing, communication networks, packet switching, Switches, Routing, adversarial queueing networks, Stability analysis, queueing systems, Electrical capacitance tomography, Noise measurement, Surges, fluid models, Communication switching, telecommunication networks, Communication networks, packet switch]
Map graphs in polynomial time
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Z. Chen et al. (1997, 1998) have introduced a modified notion of planarity, where two faces are considered adjacent if they share at least one point. The corresponding abstract graphs are called map graphs. Chen et al. raised the question of whether map graphs can be recognized in polynomial time. They showed that the decision problem is in NP and presented a polynomial time algorithm for the special case where we allow at most 4 faces to intersect in any point-for only 3 are allowed to intersect in a point, we get the usual planar graphs. Chen et al. conjectured that map graphs can be recognized in polynomial time, and in this paper, their conjecture is settled affirmatively.
[Computer science, decision problem, Lapping, Face recognition, graph theory, computational geometry, planarity, Polynomials, polynomial time, History, map graphs]
The minimum equivalent DNF problem and shortest implicants
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We prove that the Minimum Equivalent DNF problem is /spl Sigma//sub 2//sup p/-complete, resolving a conjecture due to L.J. Stockmeyer (1976). The proof involves as an intermediate step a variant of a related problem in logic minimization, namely, that of finding the shortest implicant of a Boolean function. We also obtain certain results concerning the complexity of the shortest implicant problem that may be of independent interest. When the input is a formula, the shortest implicant problem is /spl Sigma//sub 2//sup p/-complete, and /spl Sigma//sub 2//sup p/-hard to approximate to within an n/sup 1/2-/spl epsiv// factor. When the input is a circuit, approximation is /spl Sigma//sub 2//sup p/-hard to within an n/sup 1-/spl epsiv// factor. However, when the input is a DNF formula, the shortest implicant problem cannot be /spl Sigma//sub 2//sup p/-complete unless /spl Sigma//sub 2//sup p/=NP[log/sup 2/n]/sup NP/.
[shortest implicants, complexity, Circuits, minimum equivalent DNF problem, Minimization, Boolean function, Electronic switching systems, Computer science, Boolean functions, minimisation of switching nets, logic minimization, /spl Sigma//sub 2//sup p/-complete, Polynomials, Logic, shortest implicant problem, computational complexity]
A TDI system and its application to approximation algorithms
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We obtain a necessary and sufficient condition for tournaments to possess a min-max relation on packing and covering directed cycles, together with strongly polynomial time algorithms for the feedback vertex set problem and the cycle packing problem in this class of tournaments; the condition and the algorithms are all based on a totally dual integral (TDI) system, a theoretical framework introduced by J. Edmonds and R. Giles (1994) for establishing min-max results. As a consequence, we find a 2.5-approximation polynomial time algorithm for the feedback vertex set problem in any tournament.
[tournaments, feedback vertex set problem, strongly polynomial time algorithms, Very large scale integration, computational geometry, Mathematics, approximation algorithms, Application software, Computer science, cycle packing problem, Operating systems, Feedback, directed graphs, TDI system, totally dual integral system, System recovery, Approximation algorithms, Polynomials, min-max relation, necessary and sufficient condition, directed cycles, Manufacturing systems]
Faster algorithms for string matching problems: matching the convolution bound
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
In this paper we give a randomized O(nlogn)-time algorithm for the string matching with don't cares problem. This improves the Fischer-Paterson bound from 1974 and answers the open problem posed (among others) by Weiner and Galil. Using the same technique, we give an O(nlogn)-time algorithm for other problems, including subset matching, tree pattern matching, (general) approximate threshold matching and point set matching. As this bound essentially matches the complexity of computing of the fast Fourier transform which is the only known technique for solving problems of this type, it is likely that the algorithms are in fact optimal. Additionally the technique used for the threshold matching problem can be applied to the on-line version of this problem, in which we are allowed to preprocess the text and require to process the pattern in time sublinear in the text length. This result involves an interesting variant of the Karp-Rabin fingerprint method in which hash functions are locality-sensitive, i.e. the probability of collision of two words depends on the distance between them.
[fast Fourier transforms, complexity, Karp-Rabin fingerprint method, hash functions, point set matching, threshold matching problem, Convolution, Fast Fourier transforms, subset matching, convolution bound, Hamming distance, randomized algorithm, Communication switching, fast Fourier transform, randomised algorithms, Bridges, Automata, don't cares problem, Sampling methods, string matching, tree pattern matching, Pattern matching, string matching problems, approximate threshold matching, computational complexity]
Delayed information and action in on-line algorithms
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Most on-line analysis assumes that, at each time-step, all relevant information up to that time step is available and a decision has an immediate effect. In many on-line problems, however, the time relevant information is available and the time a decision has an effect may be decoupled. For example, when making an investment, one might not have completely up-to-date information on market prices. Similarly, a buy or sell order might only be executed some time later in the future. We introduce and explore natural delayed models for several well-known on-line problems. Our analyses demonstrate the importance of considering timeliness in determining the competitive ratio of an on-line algorithm. For many problems, we demonstrate that there exist algorithms with small competitive ratios even when large delays affect the timeliness of information and the effect of decisions.
[Delay effects, Snow, competitive algorithms, natural delayed models, Electronic switching systems, Production facilities, Electrical capacitance tomography, on-line problems, Postal services, Information analysis, competitive ratio, online operation, Investments, timeliness, Cost function, on-line analysis, Marine vehicles]
On learning monotone Boolean functions
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We consider the problem of learning monotone Boolean functions over {0, 1}/sup n/ under the uniform distribution. Specifically, given a polynomial number of uniform random samples for an unknown monotone Boolean function f, and given polynomial completing time, we would like to approximate f as well as possible. We describe a simple algorithm that we prove achieves error at most 1/2-/spl Omega/(1//spl radic/n), improving on the previous best bound of 1/2-/spl Omega/((log/sup 2/ n)/n). We also prove that no algorithm, given a polynomial number of samples, can guarantee error 1/2-/spl omega/((log n)//spl radic/n), improving on the previous best hardness bound of O(1//spl radic/n). These lower bounds hold even if the learning algorithm is allowed membership queries. Thus this paper settles to an O(log n) factor the question of the best achievable error for learning the class of monotone Boolean functions with respect to the uniform distribution.
[polynomial number, Identity-based encryption, polynomials, Circuits, polynomial completing time, Hip, monotone Boolean functions learning, lower bounds, Radio access networks, membership queries, Boolean functions, Upper bound, Approximation algorithms, Polynomials, uniform random samples]
Quantum lower bounds by polynomials
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We examine the number T of queries that a quantum network requires to compute several Boolean functions on {0,1}/sup N/ in the black-box model. We show that, in the black-box model, the exponential quantum speed-up obtained for partial functions (i.e. problems involving a promise on the input) by Deutsch and Jozsa and by Simon cannot be obtained for any total function: if a quantum algorithm computes some total Boolean function f with bounded-error using T black-box queries then there is a classical deterministic algorithm that computes f exactly with O(T/sup 6/) queries. We also give asymptotically tight characterizations of T for all symmetric f in the exact, zero-error, and bounded-error settings. Finally, we give new precise bounds for AND, OR, and PARITY. Our results are a quantum extension of the so-called polynomial method, which has been successfully applied in classical complexity theory, and also a quantum extension of results by Nisan about a polynomial relationship between randomized and deterministic decision tree complexity.
[decision tree complexity, Computational modeling, quantum network, Laboratories, classical complexity, polynomial relationship, Mathematics, quantum extension, characterizations, Hip, Postal services, Computer science, black-box model, Quantum computing, Boolean functions, quantum computing, US Department of Transportation, Polynomials, Mathematical model, partial functions, randomized, computational complexity]
Informatin Retrieval on the Web
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
false
[]
The complexity of the approximation of the bandwidth problem
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
The bandwidth problem has a long history and a number of important applications. It is the problem of enumerating the vertices of a given graph G such that the maximum difference between the numbers of adjacent vertices is minimal. We will show for any constant k/spl epsiv/N that there is no polynomial time approximation algorithm with an approximation factor of k. Furthermore, we will show that this result holds also for caterpillars, a class of restricted trees. We construct for any x,/spl epsiv//spl isin/R with x>1 and /spl epsiv/>0 a graph class for which an approximation algorithm with an approximation factor of x+/spl epsiv/ exists, but the approximation of the bandwidth problem within a factor of x-/spl epsiv/ is NP-complete. The best previously known approximation factors for the intractability of the bandwidth approximation problem were 1.5 for general graphs and 4/3 for trees.
[complexity, restricted trees, Symmetric matrices, approximation, intractability, Spine, graph theory, trees (mathematics), bandwidth problem, NP-complete, Sparse matrices, History, graph, approximation factor, Tree graphs, caterpillars, graph class, Bandwidth, approximation algorithm, Approximation algorithms, Polynomials, computational complexity]
Quantum oracle interrogation: getting all information for almost half the price
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
Consider a quantum computer in combination with a binary oracle of domain size N. It is shown how N/2+/spl radic/N calls to the oracle are sufficient to guess the whole content of the oracle (being an N bit string) with probability greater than 95%. This contrasts the power of classical computers which would require N calls to achieve the same task. From this result it follows that any function with the N bits of the oracle as input can be calculated using N/2+/spl radic/N queries if we allow a small probability of error. It is also shown that this error probability can be made arbitrary small by using N/2+O(/spl radic/N) oracle queries. In the second part of the article 'approximate interrogation' is considered. This is when only a certain fraction of the N oracle bits are requested. Also for this scenario does the quantum algorithm outperform the classical protocols. An example is given where a quantum procedure with N/10 queries returns a string of which 80% of the bits are correct. Any classical protocol would need 6N/10 queries to establish such a correctness ratio.
[quantum computer, Quantum computing, Upper bound, oracle interrogation, High performance computing, Laboratories, quantum computing, binary oracle, error probability, Computer errors, correctness ratio, computational complexity]
A tight characterization of NP with 3 query PCPs
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
It is known that there exists a PCP characterization of NP where the verifier makes 3 queries and has a one-sided error that is bounded away from 1; and also that 2 queries do not suffice for such a characterization. Thus PCPs with 3 queries possess non-trivial verification power and motivate the task of determining the lowest error that can be achieved with a 3-query PCP. Recently, Hastad (1997) has shown a tight characterization of NP by constructing a 3-query PCP verifier with "error" arbitrarily close to 1/2. Unfortunately this verifier makes two-sided error and Hastad makes essential use of this feature. One-sided error, on the other hand, is a natural notion to associate with a proof system, since it has the desirable property that every rejected proof has a short counterexample. The question of determining the smallest error for which there exists a 3-query PCP verifier making one-sided error and accepting an NP-complete language, however, remained open. We resolve this question by showing that NP has a 3-query PCP with a one-sided error that is arbitrarily close to 1/2. This characterization is tight, i.e., the error cannot be lower. This result is in seeming contradiction with the results of Trevisan (1997) and Zwick (1998) who show that in order to recognize an NP-complete language, the error probability of a PCP verifier making 3 non-adaptive queries and having one-sided error must be at least 5/8. We get around this bottleneck by designing an adaptive 3-query PCP for NP. Our result yields the first tight analysis of an adaptive PCP; and reveals a previously unsuspected separation between the powers of adaptive and non-adaptive PCPs. Our design and analysis of adaptive PCPs can be extended to higher number of queries as well and we give an example of such a proof system with 5 queries. Our adaptive verifiers yield proof systems whose error probabilities match those of previous constructions, while also achieving one-sidedness in the error. This raises new questions about the power of adaptive PCPs, which deserve further study.
[NP, Error probability, proof systems, Laboratories, 3-query PCP verifier, 3-query PCP, Electrical capacitance tomography, error probabilities, Read only memory, Computer science, Bridges, National electric code, Computer errors, tight characterization, theorem proving, NP-complete language, PCP characterization, computational complexity]
The shortest vector in a lattice is hard to approximate to within some constant
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
We show the shortest vector problem in the l/sub 2/ norm is NP-hard (for randomized reductions) to approximate within any constant factor less than /spl radic/2. We also give a deterministic reduction under a reasonable number theoretic conjecture. Analogous results hold in any l/sub p/ norm (p/spl ges/1). In proving our NP-hardness result, we give an alternative construction satisfying Ajtai's probabilistic variant of Sauer's lemma, that greatly simplifies Ajtai's original proof.
[shortest vector, shortest vector problem, NP-hard, Laboratories, randomized reductions, Lattices, Linear programming, deterministic reduction, Electronic switching systems, Mathematics, randomised algorithms, Computer science, NP-hardness, Reactive power, Polynomials, Contracts, computational complexity]
Fast Monte-Carlo algorithms for finding low-rank approximations
Proceedings 39th Annual Symposium on Foundations of Computer Science
None
1998
In several applications, the data consists of an m/spl times/n matrix A and it is of interest to find an approximation D of a specified rank k to A where, k is much smaller than m and n. Traditional methods like the Singular Value Decomposition (SVD) help us find the "best" such approximation. However, these methods take time polynomial in m, n which is often too prohibitive. In this paper, we develop an algorithm which is qualitatively faster provided we may sample the entries of the matrix according to a natural probability distribution. Indeed, in the applications such sampling is possible. Our main result is that we can find the description of a matrix D* of rank at most k so that /spl par/A-D*/spl par//sub F//spl les/min/D,rank(D)/spl les/k/spl par/A-D/spl par//sub F/+/spl epsiv//spl par/A/spl par//sub F/ holds with probability at least 1-/spl delta/. (For any matrix M, /spl par/M/spl par//sub F//sup 2/ denotes the sum of the squares of all the entries of M.) The algorithm takes time polynomial in k, 1//spl epsiv/, log(1//spl delta/) only, independent of m, n.
[low-rank approximations, Laboratories, probability, Mathematics, Electrical capacitance tomography, natural probability distribution, Matrix decomposition, Application software, Monte-Carlo algorithms, Computer science, Monte Carlo methods, Numerical analysis, Sampling methods, Polynomials, singular value decomposition, Singular value decomposition]
Near-optimal conversion of hardness into pseudo-randomness
40th Annual Symposium on Foundations of Computer Science
None
1999
Various efforts have been made to derandomize probabilistic algorithms using the assumption that there exists a problem in E=dtime(2/sup O(n)/) that requires circuits of size s(n) (for some function s). These results are based on the NW (Nisan & Wigderson, 1997) generator. For the strong lower bound s(n)=2/sup &#x003F5;n/, the optimal derandomization is P=BPP. However, for weaker lower bound functions s(n), these constructions fall short of the natural conjecture for optimal derandomization that bptime(t)&#x02286; dtime(2O[s/sup -1/(t)]). The gap is due to an inherent efficiency limitation in NW-style pseudorandom generators. We are able to obtain derandomization in almost optimal time using any lower bound s(n). We do this by using the NW-generator in a more sophisticated way. We view any failure of the generator as a reduction from the given hard function to its restrictions on smaller input sizes. Thus, either the original construction works optimally or one of the restricted functions is as hard as the original. Any such restriction can then be plugged into the NW-generator recursively. This process generates many candidate generators, and at least one is guaranteed to be good. To perform the approximation of the acceptance probability of the given circuit, we run a tournament between the candidate generators which yields an accurate estimate. We explore information theoretic analogs of our new construction. The inherent limitation of the NW-generator makes the extra randomness required by that extractor suboptimal. However, applying our construction, we get an almost optimal disperser.
[hard function, pseudorandom generators, Drives, Complexity theory, Read only memory, Reactive power, almost optimal disperser, NW-generator, information theory, Network address translation, complexity theory, near-optimal conversion, Computational modeling, Circuit simulation, lower bound, candidate generator tournament, randomised algorithms, Computer science, efficiency limitation, circuit size, optimal derandomization, probabilistic algorithm derandomization, input size, Artificial intelligence, computational complexity, circuit acceptance probability]
Approximation algorithms for classification problems with pairwise relationships: metric labeling and Markov random fields
40th Annual Symposium on Foundations of Computer Science
None
1999
In a traditional classification problem, we wish to assign one of k labels (or classes) to each of n objects, in a way that is consistent with some observed data that we have about the problem. An active line of research in this area is concerned with classification when one has information about pairwise relationships among the objects to be classified; this issue is one of the principal motivations for the framework of Markov random fields, and it arises in areas such as image processing, biometry: and document analysis. In its most basic form, this style of analysis seeks a classification that optimizes a combinatorial function consisting of assignment costs-based on the individual choice of label we make for each object-and separation costs-based on the pair of choices we make for two "related" objects. We formulate a general classification problem of this type, the metric labeling problem; we show that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields. From the perspective of combinatorial optimization, our problem can be viewed as a substantial generalization of the multiway cut problem, and equivalent to a type of uncapacitated quadratic assignment problem. We provide the first non-trivial polynomial-time approximation algorithms for a general family of classification problems of this type. Our main result is an O(log k log log k)-approximation algorithm for the metric labeling problem, with respect to an arbitrary metric on a set of k labels, and an arbitrary weighted graph of relationships on a set of objects. For the special case in which the labels are endowed with the uniform metric-all distances are the same-our methods provide a 2-approximation.
[pattern classification, Image processing, multiway cut problem, Classification algorithms, Electrical capacitance tomography, Read only memory, Markov random fields, Computer science, classification problem, combinatorial optimization, Image analysis, pairwise relationships, Markov processes, Approximation algorithms, Cost function, Labeling, uncapacitated quadratic assignment problem, metric labeling]
Taking a walk in a planar arrangement
40th Annual Symposium on Foundations of Computer Science
None
1999
We present a randomized algorithm for computing portions of an arrangement of n arcs in the plane, each pair of which intersect in at most t points. We use this algorithm to perform online walks inside such an arrangement (i.e., compute all the faces that a curve, given in an online manner, crosses), and to compute a level in an arrangement, both in an output-sensitive manner. The expected running time of the algorithm is O(/spl lambda//sub t+2/(m+n) log n), where m is the number of intersections between the walk and the given arcs. No similarly efficient algorithm is known for the general case of arcs. For the case of lines and for certain restricted cases involving line segments, our algorithm improves the best known algorithm of (Overmars and van Leeuwen, 1981) by almost a logarithmic factor.
[Microwave integrated circuits, Computational geometry, line segments, algorithm running time, curve, computational geometry, online walks, randomized algorithm, intersections, planar arrangement, randomised algorithms, computational complexity]
Dynamic planar convex hull operations in near-logarithmic amortized time
40th Annual Symposium on Foundations of Computer Science
None
1999
We give a data structure that allows arbitrary insertions and deletions on a planar point set P and supports basic queries on the convex hull of P, such as membership and tangent-finding. Updates take O(log/sup 1+/spl epsiv// n) amortized time and queries take O(log n) time each, where n is the maximum size of P and /spl epsiv/ is any fixed positive constant. For some advanced queries such as bridge-finding, both our bounds increase to O(log/sup 3/2/ n). The only previous fully dynamic solution was by Overmars and van Leeuwen (1981) and required O(log/sup 2/ n) time per update.
[Tree data structures, tangent-finding, insertions, deletions, data structure, computational geometry, Data structures, Mathematics, dynamic planar convex hull operations, membership, queries, Computer science, Computational geometry, near-logarithmic amortized time, Algorithms, Sampling methods, data structures, planar point set, bridge-finding, computational complexity]
Markovian coupling vs. conductance for the Jerrum-Sinclair chain
40th Annual Symposium on Foundations of Computer Science
None
1999
We show that no Markovian coupling argument can prove rapid mixing of the Jerrum-Sinclair Markov chain for sampling almost uniformly from the set of perfect and near perfect matchings of a given graph. In particular, we show that there exists a bipartite graph G such that any Markovian coupling argument on the Jerrum-Sinclair Markov chain for G must necessarily take time exponential in the number of vertices in G. This holds even when the coupling argument is time-variant, i.e., the transition probabilities used by the coupling process depend upon the history of the process. In contrast, the above Markov chain on G has been shown to mix in polynomial time using conductance arguments.
[perfect matchings, graph theory, vertices, approximate counting, computational geometry, coupling process, Jerrum-Sinclair Markov chain, Electrical capacitance tomography, transition probabilities, conductance arguments, function approximation, time-variant argument, rapid mixing, Polynomials, polynomial time, Markovian coupling, sampling methods, Ducts, sampling, State-space methods, bipartite graph, Computer science, Upper bound, Computer errors, Markov processes, Sampling methods, computational complexity]
Finding double Euler trails of planar graphs in linear time [CMOS VLSI circuit design]
40th Annual Symposium on Foundations of Computer Science
None
1999
The paper answers an open question in the design of complimentary metal-oxide semiconductor (CMOS) VLSI circuits. It asks whether a polynomial-time algorithm can decide if a given planar graph has a plane embedding /spl epsiv/ such that /spl epsiv/ has a Euler trail P=e/sub 1/e/sub 2/...e/sub m/ and its dual graph has a Euler trail P*=e/sub 1/*e/sub 2/*...e/sub m/* where e/sub i/* is the dual edge of e/sub i/ for i=1, 2, ..., m. The paper answers this question in the affirmative by presenting a linear-time algorithm.
[Visualization, double Euler trails, VLSI, Circuits, graph theory, planar graphs, Very large scale integration, integrated circuit layout, Helium, plane embedding, CMOS VLSI circuits, MOSFETs, linear-time algorithm, dual graph, complimentary metal-oxide semiconductor VLSI circuit design, Boolean functions, CMOS technology, Polynomials, CMOS integrated circuits, polynomial-time algorithm, circuit layout CAD, dual edge, linear time]
Improved combinatorial algorithms for the facility location and k-median problems
40th Annual Symposium on Foundations of Computer Science
None
1999
We present improved combinatorial approximation algorithms for the uncapacitated facility location and k-median problems. Two central ideas in most of our results are cost scaling and greedy improvement. We present a simple greedy local search algorithm which achieves an approximation ratio of 2.414+/spl epsiv/ in O/spl tilde/(n/sup 2///spl epsiv/) time. This also yields a bicriteria approximation tradeoff of (1+/spl gamma/, 1+2//spl gamma/) for facility cost versus service cost which is better than previously known tradeoffs and close to the best possible. Combining greedy improvement and cost scaling with a recent primal dual algorithm for facility location due to K. Jain and V. Vazirani (1999), we get an approximation ratio of 1.853 in O/spl tilde/(n/sup 3/) time. This is already very close to the approximation guarantee of the best known algorithm which is LP-based. Further combined with the best known LP-based algorithm for facility location, we get a very slight improvement in the approximation factor for facility location, achieving 1.728. We present improved approximation algorithms for capacitated facility location and a variant. We also present a 4-approximation for the k-median problem, using similar ideas, building on the 6-approximation of Jain and Vazirani. The algorithm runs in O/spl tilde/(n/sup 3/) time.
[k-median problems, primal dual algorithm, Nonlinear filters, Electrical capacitance tomography, approximation algorithms, facility location, approximation factor, optimisation, greedy local search algorithm, capacitated facility location, bicriteria approximation tradeoff, Filtering algorithms, Bismuth, Cost function, duality (mathematics), cost scaling, search problems, approximation ratio, greedy improvement, k-median problem, best known algorithm, facility cost, best known LP-based algorithm, combinatorial approximation algorithms, Linear programming, service cost, uncapacitated facility location, approximation guarantee, Approximation algorithms, computational complexity]
Fully dynamic algorithms for maintaining all-pairs shortest paths and transitive closure in digraphs
40th Annual Symposium on Foundations of Computer Science
None
1999
This paper presents the first fully dynamic algorithms for maintaining all-pairs shortest paths in digraphs with positive integer weights less than b. For approximate shortest paths with an error factor of (2+/spl epsiv/), for any positive constant /spl epsiv/, the amortized update time is O(n/sup 2/ log/sup 2/ n/log log n); for an error factor of (1+/spl epsiv/) the amortized update time is O(n/sup 2/ log/sup 3/ (bn)//spl epsiv//sup 2/). For exact shortest paths the amortized update time is O(n/sup 2.5/ /spl radic/(b log n)). Query time for exact and approximate shortest distances is O(1); exact time and approximate paths can be generated in time proportional to their lengths. Also presented is a fully dynamic transitive closure algorithm with update time O(n/sup 2/ log n) and query time O(1). The previously known fully dynamic transitive closure algorithm with fast query time has one-sided error and update time O(n/sup 2.28/). The algorithms use simple data structures, and are deterministic.
[all-pairs shortest paths, Heuristic algorithms, shortest distance, query time, Birth disorders, error factor, Data structures, Electronic switching systems, deterministic algorithms, dynamic transitive closure algorithm, Computer science, Upper bound, directed graphs, dynamic algorithms, data structures, Error correction, positive integer weights, digraph transitive closure, update time, computational complexity]
Random walks on truncated cubes and sampling 0-1 knapsack solutions
40th Annual Symposium on Foundations of Computer Science
None
1999
We solve an open problem concerning the mixing time of a symmetric random walk on an n-dimensional cube truncated by a hyperplane, showing that it is polynomial in n. As a consequence, we obtain a full-polynomial randomized approximation scheme for counting the feasible solutions of a 0-1 knapsack problem. The key ingredient in our analysis is a combinatorial construction we call a "balanced almost uniform permutation\
[approximation theory, balanced almost uniform permutation, sampling methods, combinatorial mathematics, symmetric random walk, 0-1 knapsack problem, full-polynomial randomized approximation scheme, sampling, combinatorial construction, Statistics, Machinery, knapsack problems, randomised algorithms, Computer science, polynomial complexity, Sampling methods, Approximation algorithms, truncated n-dimensional cube, hyperplane, feasible solutions counting, computational complexity, mixing time]
A theoretical framework for memory-adaptive algorithms
40th Annual Symposium on Foundations of Computer Science
None
1999
External memory algorithms play a key role in database management systems and large scale processing systems. External memory algorithms are typically tuned for efficient performance given a fixed, statically allocated amount of internal memory. However, with the advent of real-time database system and database systems based upon administratively defined goals, algorithms must increasingly be able to adapt in an online manner when the amount of internal memory allocated to them changes dynamically and unpredictably. We present a theoretical and applicable framework for memory-adaptive algorithms (or simply MA algorithms). We define the competitive worst-case notion of what it means for an MA algorithm to be dynamically optimal and prove fundamental lower bounds on the performance of MA algorithms for problems such as sorting, standard matrix multiplication, and several related problems. Our main tool for proving dynamic optimality is the notion of resource consumption, which measures how efficiently an MA algorithm adapts itself to memory fluctuations. We present the first dynamically optimal algorithm for sorting (based upon mergesort), permuting, FFT, permutation networks, buffer trees, (standard) matrix multiplication, and LU decomposition. In each case, dynamic optimality is demonstrated via a potential function argument showing that the algorithm's resource consumption is within a constant factor of optimal.
[Algorithm design and analysis, fast Fourier transforms, dynamic optimality proof, Military computing, Random access memory, permuting, dynamically optimal algorithm, competitive worst-case notion, database management systems, memory fluctuations, Road transportation, FFT, algorithm theory, sorting, LU decomposition, internal memory, Database systems, administratively defined goals, standard matrix multiplication, mergesort, memory-adaptive algorithms, Fluctuations, real-time database system, permutation networks, theoretical framework, buffer trees, potential function argument, Sorting, database theory, external memory algorithms, Computer science, matrix multiplication, Memory management, real-time systems, Computer applications, large scale processing systems, MA algorithm, resource consumption]
Setting parameters by example
40th Annual Symposium on Foundations of Computer Science
None
1999
We introduce a class of "inverse parametric optimization" problems, in which one is given both a parametric optimization problem and a desired optimal solution; the task is to determine parameter values that lead to the given solution. We describe algorithms for solving such problems for minimum spanning trees, shortest paths, and other "optimal subgraph" problems, and discuss applications in multicast routing, vehicle path planning, resource allocation, and board game programming.
[parametric optimization problem, Roads, vehicle path planning, vehicles, parameter values, Vehicles, Network servers, optimisation, resource allocation, computer games, multicast communication, inverse problems, parameter estimation, minimum spanning trees, Software algorithms, trees (mathematics), desired optimal solution, parameter setting by example, Routing, Path planning, path planning, Application software, inverse parametric optimization, randomised algorithms, Multicast algorithms, optimal subgraph problems, Web pages, telecommunication network routing, shortest paths, board game programming, Resource management, multicast routing]
Optimal lower bounds for quantum automata and random access codes
40th Annual Symposium on Foundations of Computer Science
None
1999
Consider the finite regular language L/sub n/={w0|w/spl isin/{0,1}*,|w|/spl les/n}. A. Ambainis et al. (1999) showed that while this language is accepted by a deterministic finite automaton of size O(n), any one-way quantum finite automaton (QFA) for it has size 2/sup /spl Omega/(n/logn)/. This was based on the fact that the evolution of a QFA is required to be reversible. When arbitrary intermediate measurements are allowed, this intuition breaks down. Nonetheless, we show a 2/sup /spl Omega/(n)/ lower bound for such QFA for L/sub n/, thus also improving the previous bound. The improved bound is obtained from simple entropy arguments based on A.S. Holevo's (1973) theorem. This method also allows us to obtain an asymptotically optimal (1-H(p))n bound for the dense quantum codes (random access codes) introduced by A. Ambainis et al. We then turn to Holevo's theorem, and show that in typical situations, it may be replaced by a tighter and more transparent in-probability bound.
[finite automata, simple entropy arguments, Entropy, Electronic switching systems, Tellurium, Read only memory, Postal services, Quantum computing, Power measurement, deterministic automata, optimal lower bounds, asymptotically optimal bound, formal languages, one-way quantum finite automaton, arbitrary intermediate measurements, Doped fiber amplifiers, random processes, random access codes, finite regular language, Computer science, random codes, quantum automata, Automata, quantum computing, deterministic finite automaton, transparent in-probability bound, dense quantum codes, QFA evolution]
Derandomizing Arthur-Merlin games using hitting sets
40th Annual Symposium on Foundations of Computer Science
None
1999
We prove that AM (and hence Graph Nonisomorphism) is in NP if for some /spl epsiv/>0, some language in NE/spl cap/ coNE requires nondeterministic circuits of size 2/sup en/. This improves results of Arvind and Kobler (1997) and of Klivans and Van Melkebeek (1999) who have proven the same conclusion, but under stronger hardness assumptions, namely, either the existence of a language in NE/spl cap/ coNE which cannot be approximated by nondeterministic circuits of size less than 2/sup en/ or the existence of a language in NE/spl cap/ coNE which requires oracle circuits of size 2/sup en/ with oracle gates for SAT (satisfiability). The previous results on derandomizing AM were based on pseudorandom generators. In contrast, our approach is based on a strengthening of Andreev, Clementi and Rolim's (1996) hitting set approach to derandomization. As a spin-off we show that this approach is strong enough to give an easy (if the existence of explicit dispersers can be assumed known) proof of the following implication: for some /spl epsiv/>0, if there is a language in E which requires nondeterministic circuits of size 2/sup en/, then P=BPP. This differs from Impagliazzo and Wigderson's (1995) theorem "only" by replacing deterministic circuits with nondeterministic ones.
[graph nonisomorphism, Arthur-Merlin games, Computational modeling, pseudorandom generators, Circuits, graph theory, computability, Electronic switching systems, derandomization, Complexity theory, Read only memory, randomised algorithms, Computer science, nondeterministic circuits, satisfiability, hitting sets, Polynomials, computational complexity]
Torpid mixing of some Monte Carlo Markov chain algorithms in statistical physics
40th Annual Symposium on Foundations of Computer Science
None
1999
Studies two widely used algorithms, Glauber dynamics and the Swendsen-Wang (1987) algorithm, on rectangular subsets of the hypercubic lattice Z/sup d/. We prove that, under certain circumstances, the mixing time in a box of side length L with periodic boundary conditions can be exponential in L/sup d-1/. In other words, under these circumstances, the mixing in these widely used algorithms is not rapid; instead it is torpid. The models we study are the independent set model and the q-state Potts model. For both models, we prove that Glauber dynamics is torpid in the region with phase coexistence. For the Potts model, we prove that the Swendsen-Wang mixing is torpid at the phase transition point.
[Algorithm design and analysis, Temperature, rectangular subsets, Lattices, mixing, Mathematics, Steady-state, Glauber dynamics, Monte Carlo Markov chain algorithms, Microwave integrated circuits, Monte Carlo methods, phase transformations, Swendsen-Wang algorithm, phase coexistence, phase transition point, periodic boundary conditions, Potts model, hypercubic lattice, Physics, Computer science, physics computing, statistical physics, independent set model, q-state Potts model, Markov processes, mixing time, torpid mixing]
Algorithmic aspects of protein structure similarity
40th Annual Symposium on Foundations of Computer Science
None
1999
We show that calculating contact map overlap (a measure of similarity of protein structures) is NP-hard, but can be solved in polynomial time for several interesting and relevant special cases. We identify an important special case of this problem corresponding to self-avoiding walks, and prove a decomposition theorem and a corollary approximation result for this special case. These are the first approximation algorithms with guaranteed error bounds, and NP-completeness results in the literature in the area of protein structure alignment/fold recognition for measures of structure similarity of practical interest.
[decomposition theorem, US Department of Energy, NP-hard, molecular configurations, Laboratories, Area measurement, graph theory, Prediction methods, NP-completeness results, Mathematics, approximation algorithms, guaranteed error bounds, protein structure similarity, protein structure alignment, Proteins, Microwave integrated circuits, optimisation, Databases, biology computing, proteins, Polynomials, polynomial time, theorem proving, algorithmic aspects, Time measurement, contact map overlap, self-avoiding walks, scientific information systems, computational complexity, corollary approximation result]
Efficient testing of large graphs
40th Annual Symposium on Foundations of Computer Science
None
1999
Let P be a property of graphs. An /spl epsiv/-test for P is a randomized algorithm which, given the ability to make queries whether a desired pair of vertices of an input graph G with n vertices are adjacent or not, distinguishes, with high probability, between the case of G satisfying P and the case that it has to be modified by adding and removing more than /spl epsiv/n/sup 2/ edges to make it satisfy P. The property P is called testable, if for every /spl epsiv/ there exists an /spl epsiv/-test for P whose total number of queries is independent of the size of the input graph. O. Goldreich et al. (1996) showed that certain graph properties admit an /spl epsiv/-test. In this paper we make a first step towards a logical characterization of all testable graph properties, and show that properties describable by a very general type of coloring problem are testable. We use this theorem to prove that first order graph properties not containing a quantifier alternation of type "/spl forall//spl exist/" are always testable, while we show that some properties containing this alternation are not. Our results are proven using a combinatorial lemma, a special case of which, that may be of independent interest, is the following. A graph H is called /spl epsiv/-unavoidable in G if all graphs that differ from G in no more than /spl epsiv/|G|/sup 2/ places contain an induced copy of H. A graph H is called /spl delta/-abundant in G if G contains at least /spl delta/|G|/sup |H|/ induced copies of H. If H is /spl epsiv/-unavoidable in G then it is also /spl delta/(/spl epsiv/, |H|)-abundant.
[Geometry, /spl epsiv/-test, graph theory, logical characterization, probability, Mathematics, large graphs testing, Logic testing, Read only memory, Radio access networks, randomised algorithms]
All pairs shortest paths in undirected graphs with integer weights
40th Annual Symposium on Foundations of Computer Science
None
1999
We show that the all pairs shortest paths (APSP) problem for undirected graphs with integer edge weights taken from the range {1, 2, ..., M} can be solved using only a logarithmic number of distance products of matrices with elements in the range (1, 2, ..., M). As a result, we get an algorithm for the APSP problem in such graphs that runs in O~(Mn/sup /spl omega//) time, where n is the number of vertices in the input graph, M is the largest edge weight in the graph, and /spl omega/<2.376 is the exponent of matrix multiplication. This improves, and also simplifies, an O~(M/sup (/spl omega/+1)/2/n/sup /spl omega//) time algorithm of Galil and Margalit (1997).
[Computer science, matrix multiplication, Upper bound, all pairs shortest paths problem, matrix distance products, graph theory, Matrices, integer edge weights, undirected graphs, input graph, Read only memory, computational complexity]
Approximation schemes for minimizing average weighted completion time with release dates
40th Annual Symposium on Foundations of Computer Science
None
1999
We consider the problem of scheduling n jobs with release dates on m machines so as to minimize their average weighted completion time. We present the first known polynomial time approximation schemes for several variants of this problem. Our results include PTASs for the case of identical parallel machines and a constant number of unrelated machines with and without preemption allowed. Our schemes are efficient: for all variants the running time for /spl alpha/(1+/spl epsiv/) approximation is of the form f(1//spl epsiv/, m)poly(n).
[polynomial time approximation, Engineering profession, minimizing average weighted completion time, Laboratories, Power generation economics, Parallel machines, average weighted completion time, Educational institutions, Mathematics, parallel machines, Computer science, scheduling, Approximation algorithms, Informatics, Contracts, computational complexity]
Long-lived adaptive collect with applications
40th Annual Symposium on Foundations of Computer Science
None
1999
A distributed algorithm is adaptive if the worst case step complexity of its operations is bounded by a function of the number of processes that are concurrently active during the operation (rather than a function of N, the total number of processes, which is usually much larger). We present long-lived and adaptive algorithms for collect in the read/write shared-memory model. Replacing the reads and writes in long-lived shared memory algorithms with our adaptive collect results in many cases in a corresponding long-lived algorithm which is adaptive. Examples of such applications, which are discussed are atomic-snapshots, and l-exclusion. Following the long-lived and adaptive collect we present a more pragmatic version of collect, called active set. This algorithm is slightly weaker than the collect but has several advantages. We employ this algorithm to transform algorithms, such as the Bakery algorithm, into their corresponding adaptive long-lived version, which is more efficient than the version that was obtained with the collect. Previously, long-lived and adaptive algorithms in this model were presented only for the renaming problem.
[l-exclusion, worst case step complexity, Adaptive algorithm, Read-write memory, Electrical capacitance tomography, long-lived adaptive collect, Application software, Computer science, distributed algorithm, read/write shared-memory model, atomic-snapshots, active set, distributed algorithms, Particle measurements, shared memory systems, Bakery algorithm, adaptive algorithms, computational complexity]
On quantum and classical space-bounded processes with algebraic transition amplitudes
40th Annual Symposium on Foundations of Computer Science
None
1999
We define a class of stochastic processes based on evolutions and measurements of quantum systems, and consider the complexity of predicting their long term behavior. It is shown that a very general class of decision problems regarding these stochastic processes can be efficiently solved classically in the space-bounded case. The following corollaries are implied by our main result for any space-constructible space bound s satisfying s(n)=/spl Omega/(log n): (i) any space O(s) uniform family of quantum circuit acting on s qubits and consisting of unitary gates and measurement gates defined in a typical way by matrices of algebraic numbers can be simulated by an unbounded error space O(s) ordinary (i.e., fair-coin flipping) probabilistic Turing machine, and hence by space O(s) uniform classical (deterministic) circuits of depth O(s/sup 2/) and size 2/sup 0/(s); (2) any quantum Turing machine running in space s, having arbitrary algebraic transition amplitudes, allowing unrestricted measurements during its computation, and having no restrictions on running time can be simulated by a space O(s) ordinary probabilistic Turing machine in the unbounded error setting. We also obtain the following classical result: any unbounded error probabilistic Turing machine running in space s that allows algebraic probabilities and algebraic cut-point can be simulated by a space O(s) ordinarily probabilistic Turing machine with cut-point 1/2. Our technique for handling algebraic numbers in the above simulations may be of independent interest. It is shown that any real algebraic number can be accurately approximated by a ratio of GapL functions.
[complexity, unbounded error probabilistic Turing machine, quantum systems, GapL functions, unbounded error setting, Stochastic processes, unitary gates, algebraic transition amplitudes, probabilistic Turing machine, measurement gates, Quantum computing, Measurement units, running time, Turing machines, ordinary probabilistic Turing machine, qubits, algebraic cut-point, Matrices, stochastic processes, matrices, space-constructible space bound, quantum Turing machine, Circuit simulation, Computational modeling, probability, fair-coin flipping, quantum space-bounded processes, Extraterrestrial measurements, Size measurement, algebraic probabilities, Time measurement, deterministic circuits, long term behavior, quantum circuit, classical space-bounded processes, algebraic numbers, quantum computing, unbounded error space, decision problems, unrestricted measurements, computational complexity, arbitrary algebraic transition amplitudes]
Cuts, trees and l/sub 1/-embeddings of graphs
40th Annual Symposium on Foundations of Computer Science
None
1999
Motivated by many recent algorithmic applications, the paper aims to promote a systematic study of the relationship between the topology of a graph and the metric distortion incurred where the graph is embedded into l/sub 1/ space. The main results are: 1. Explicit constant-distortion embeddings of all series parallel graphs, and all graphs with bounded Euler number. These are thus the first natural families known to have constant distortion (strictly greater than 1). Using the above embeddings, we obtain algorithms to approximate the sparsest cut in such graphs to within a constant factor. 2) A constant-distortion embedding of outerplanar graphs into the restricted class of l/sub 1/-metrics known as "dominating tree metrics". We also show a lower bound of /spl Omega/(log n) on the distortion for embeddings of series-parallel graphs into (distributions over) dominating tree metrics. This shows, surprisingly, that such metrics approximate distances very poorly even for families of graphs with low tree width, and excludes the possibility of using them to explore the finer structure of l/sub 1/-embeddability.
[Algorithm design and analysis, low tree width, graph theory, explicit constant-distortion embeddings, l/sub 1/-embeddability, constant-distortion embedding, algorithmic applications, series-parallel graphs, Application specific integrated circuits, Tree graphs, constant factor, Distortion measurement, dominating tree metrics, graph topology, probability, series parallel graphs, Extraterrestrial measurements, restricted class, Hip, outerplanar graphs, tree metrics, Computer science, graph embeddings, l/sub 1/-embeddings, metric distortion, natural families, bounded Euler number, sparsest cut, Approximation algorithms, constant distortion, computational complexity]
The Directed Steiner Network problem is tractable for a constant number of terminals
40th Annual Symposium on Foundations of Computer Science
None
1999
We consider the Directed Steiner Network (DSN) problem, also called the Point-to-Point Connection problem, where given a directed graph G and p pairs {(s/sub 1/,t/sub 1/), ..., (s/sub p/,t/sub p/)} of nodes in the graph, one has to find the smallest subgraph H of G that contains paths from s/sub i/ to t/sub i/ for all i. The problem is NP-hard for general p, since the Directed Steiner Tree problem is a special case. Until now, the complexity was unknown for constant p/spl ges/3. We prove that the problem is polynomially solvable if p is any constant number, even if nodes and edges in G are weighted and the goal is to minimize the total weight of the subgraph H. In addition, we give an efficient algorithm for the Strongly Connected Steiner Subgraph problem for any constant p, where given a directed graph and p nodes in the graph, one has to compute the smallest strongly connected subgraph containing the p nodes.
[complexity, Costs, NP-hard, Laboratories, trees (mathematics), computability, subgraph weight minimization, tractable, Directed Steiner Tree problem, Point-to-Point Connection problem, Read only memory, Postal services, Computer science, Directed Steiner Network problem, directed graphs, directed graph, Polynomials, minimisation, Strongly Connected Steiner Subgraph problem, smallest strongly connected subgraph, computational complexity, polynomial solvability]
A 5/2n/sup 2/-lower bound for the rank of n/spl times/n-matrix multiplication over arbitrary fields
40th Annual Symposium on Foundations of Computer Science
None
1999
We prove a lower bound of 5/2n/sup 2/-3n for the rank of n/spl times/n-matrix multiplication over an arbitrary field. Similar bounds hold for the rank of the multiplication in noncommutative division algebras and for the multiplication of upper triangular matrices.
[Gold, Vectors, Ice, noncommutative division algebras, lower bound, Tellurium, Radio access networks, matrix multiplication, Upper bound, Algebra, Character generation, upper triangular matrices, Ear, computational complexity]
Bounds for small-error and zero-error quantum algorithms
40th Annual Symposium on Foundations of Computer Science
None
1999
We present a number of results related to quantum algorithms with small error probability and quantum algorithms that are zero-error. First, we give a tight analysis of the trade-offs between the number of queries of quantum search algorithms, their error probability, the size of the search space, and the number of solutions in this space. Using this, we deduce new lower and upper bounds for quantum versions of amplification problems. Next, we establish nearly optimal quantum-classical separations for the query complexity of monotone functions in the zero-error model (where our quantum zero-error model is defined so as to be robust when the quantum gates are noisy). Also, we present a communication complexity problem related to a total function for which there is a quantum-classical communication complexity gap in the zero-error model. Finally, we prove separations for monotone graph properties in the zero-error and other error models which imply that the evasiveness conjecture for such properties does not hold for quantum computers.
[Algorithm design and analysis, Error probability, zero-error quantum algorithms, Laboratories, graph theory, quantum versions, query complexity, communication complexity, Postal services, small error probability, monotone functions, Quantum computing, Monte Carlo methods, Polynomials, amplification problems, search space, nearly optimal quantum-classical separations, search problems, quantum zero-error model, error statistics, quantum-classical communication complexity gap, monotone graph properties, error models, error probability, quantum gates, quantum computers, evasiveness conjecture, zero-error model, Computer science, Upper bound, communication complexity problem, quantum computing, Computer errors, tight analysis, quantum search algorithms]
A non-linear time lower bound for Boolean branching programs
40th Annual Symposium on Foundations of Computer Science
None
1999
We prove that for all positive integer k and for all sufficiently small /spl epsiv/>0 if n is sufficiently large then there is no Boolean (or 2-way) branching program of size less than 2/sup em/ which for all inputs X/spl sube/{0, 1, ..., n-1} computes in time kn the parity of the number of elements of the set of all pairs (x,y) with the property x/spl isin/X, y/spl isin/X, x<y, x+y/spl isin/X. For the proof of this fact we show that if A=(/spl alpha//sub i,j/)/sub i=0, j=0//sup n/ is a random n by n matrix over the field with 2 elements with the condition that "/spl forall/, j, k, l/spl isin/{0, 1, ..., n-1}, i+j=k+l implies /spl alpha//sub i,j/=/spl alpha//sub k,l/" then with a high probability the rank of each /spl delta/n by /spl delta/n submatrix of A is at least c/spl delta/|log /spl delta/|/sup -2/n, where c>0 is an absolute constant and n is sufficiently large with respect to /spl delta/.
[Performance evaluation, nonlinear time lower bound, positive integer, Input variables, parity, probability, Binary decision diagrams, Size measurement, Time measurement, Registers, matrix, Content addressable storage, directed graphs, directed graph, Boolean branching programs, computational complexity]
On counting independent sets in sparse graphs
40th Annual Symposium on Foundations of Computer Science
None
1999
We prove two results concerning approximate counting of independent sets in graphs with constant maximum degree /spl Delta/. The first result implies that the Monte-Carlo Markov chain technique is likely to fail if /spl Delta//spl ges/6. The second shows that no fully polynomial randomized approximation scheme can exist for /spl Delta//spl ges/25, unless P=NP under randomized reductions.
[approximation theory, graph theory, approximate counting, polynomial randomized approximation scheme, randomized reductions, sparse graphs, independent set counting, Mathematics, Electronic switching systems, set theory, Radio access networks, randomised algorithms, Computer science, Monte-Carlo Markov chain technique, Monte Carlo methods, Markov processes, Polynomials, Bipartite graph, constant maximum degree, computational complexity]
How asymmetry helps load balancing
40th Annual Symposium on Foundations of Computer Science
None
1999
This paper deals with balls and bins processes related to randomized load balancing, dynamic resource allocation and hashing. Suppose n balls have to be assigned to n bins, where each ball has to be placed without knowledge about the distribution of previously placed balls. The goal is to achieve an allocation that is as even as possible so that no bin gets much more balls than the average. A well known and good solution for this problem is to choose d possible locations for each ball at random, to look into each of these bins, and to place the ball into the least full among these bins. This class of algorithms has been investigated intensively in the past but almost all previous analyses assume that the d locations for each ball are chosen uniform and independently at random from the set of all bins. We investigate whether a non-uniform and possibly dependent choice of the d locations for a ball can improve the load balancing. Three types of selections are distinguished: 1) uniform and independent 2) non-uniform and independent 3) non-uniform and dependent. Our first result shows that choosing the locations in a non-uniform way (type 2) results in a better load balancing than choosing the locations uniformly (type 1). Surprising, this smooth load balancing is obtained by an algorithm called "Always-Go-Left" which creates an asymmetric assignment of the balls to the bins. Our second result is a lower bound on the smallest-possible maximum load that can be achieved by any allocation algorithm of type 1, 2, or 3.
[Algorithm design and analysis, Always-Go-Left algorithm, Nominations and elections, dynamic resource allocation, hashing, lower bound, Parallel algorithms, randomised algorithms, Computer science, Upper bound, resource allocation, Load management, balls and bins processes, Resource management, randomized load balancing, computational complexity]
Non-malleable non-interactive zero knowledge and adaptive chosen-ciphertext security
40th Annual Symposium on Foundations of Computer Science
None
1999
We introduce the notion of non-malleable non-interactive zero-knowledge (NIZK) proof systems. We show how to transform any ordinary NIZK proof system into one that has strong non-malleability properties. We then show that the elegant encryption scheme of Naor and Yung (1990) can be made secure against the strongest form of chosen-ciphertext attack by using a non-malleable NIZK proof instead of a standard NIZK proof. Our encryption scheme is simple to describe and works in the standard cryptographic model under, general assumptions. The encryption scheme can be realized assuming the existence of trapdoor permutations.
[Laboratories, strong nonmalleability properties, cryptography, adaptive chosen-ciphertext security, Security, encryption scheme, Postal services, Computer science, Privacy, Public key, nonmalleable noninteractive zero-knowledge proof systems, trapdoor permutations, Public key cryptography, US Department of Defense, theorem proving]
A study of proof search algorithms for resolution and polynomial calculus
40th Annual Symposium on Foundations of Computer Science
None
1999
The paper is concerned with the complexity of proofs and of searching for proofs in two propositional proof systems: Resolution and Polynomial Calculus (PC). For the former system we show that the recently proposed algorithm of E. Ben-Sasson and A. Wigderson (1999) for searching for proofs cannot give better than weakly exponential performance. This is a consequence of showing optimality of their general relationship, referred to as size-width trade-off. We moreover obtain the optimality of the size width trade-off for the widely used restrictions of resolution: regular, Davis-Putnam, negative, positive and linear. As for the second system, we show that the direct translation to polynomials of a CNF formula having short resolution proofs, cannot be refuted in PC with degree less than /spl Omega/ (log n). A consequence of this is that the simulation of resolution by PC of M. Clegg, J. Edmonds and R. Impagliazzo (1996) cannot be improved to better than quasipolynomial in the case where we start with small resolution proofs. We conjecture that the simulation of M. Clegg et al. is optimal.
[proof complexity, small resolution proofs, Calculus, Electronic switching systems, Electrical capacitance tomography, resolution simulation, Postal services, propositional proof systems, Polynomials, theorem proving, Cryptography, Informatics, short resolution proofs, search problems, polynomial calculus, polynomials, optimality, direct translation, size-width trade-off, Hip, proof search algorithms, process algebra, weakly exponential performance, Ear, CNF formula, quasipolynomial, computational complexity]
Efficient regular data structures and algorithms for location and proximity problems
40th Annual Symposium on Foundations of Computer Science
None
1999
Investigates data structures obtained by a recursive partitioning of the input domain into regions of equal size. One of the most well-known examples of such a structure is the quadtree, which is used in this paper as a basis for more complex data structures; we also provide multidimensional versions of the stratified tree of P. van Emde Boas (1997). We show that, under the assumption that the input points have limited precision (i.e. are drawn from an integer grid of size u), these data structures yield efficient solutions to many important problems. In particular, they allow us to achieve O(log log u) time per operation for finding the dynamic approximate nearest neighbor (under insertions and deletions) and the exact online closest pair (under insertions only) in any constant dimension. They allow O(log log u) point location in a given planar shape or in its expansion (dilation by a ball of a given radius). Finally, we provide a linear-time (optimal) algorithm for computing the expansion of a shape represented by a quadtree. This result shows that the spatial order imposed by this regular data structure is sufficient to optimize the dilation by a ball operation.
[shape expansion, Shape, input domain recursive partitioning, insertions, ball operation, deletions, spatial order, computational geometry, linear-time algorithm, spatial data structures, proximity problems, complex data structures, Contracts, Tree data structures, dilation optimization, Multidimensional systems, multidimensional stratified tree, time complexity, Data structures, Educational institutions, Data processing, Partitioning algorithms, input point precision, Nearest neighbor searches, point location problems, Computer science, exact online closest pair, dynamic approximate nearest neighbor, planar shape, equal-size regions, quadtrees, integer grid, regular data structures, quadtree, computational complexity]
Error reduction for extractors
40th Annual Symposium on Foundations of Computer Science
None
1999
An extractor is a function which extracts (almost) truly random bits from a weak random source, using a small number of additional random bits as a catalyst. We present a general method to reduce the error of any extractor. Our method works particularly well in the case that the original extractor extracts up to a constant function of the source min-entropy and achieves a polynomially small error. In that case, we are able to reduce the error to (almost) any /spl epsiv/, using only O(log(1//spl epsiv/)) additional truly random bits (while keeping the other parameters of the original extractor more or less the same). In other cases (e.g. when the original extractor extracts all the min-entropy or achieves only a constant error), our method is not optimal but it is still quite efficient and leads to improved constructions of extractors. Using our method, we are able to improve almost all known extractors in the case where the error required is relatively small (e.g. less than a polynomially small error). In particular, we apply our method to the new extractors of L. Trevisan (1999) and R. Raz et al. (1999) to obtain improved constructions in almost all cases. Specifically, we obtain extractors that work for sources of any min-entropy on strings of length n which (a) extract any 1/n/sup /spl gamma// fraction of the min-entropy using O[log n+log(1//spl epsiv/)] truly random bits (for any /spl gamma/>0), (b) extract any constant fraction of the min-entropy using O[log/sup 2/n+log(1//spl epsiv/)] truly random bits, and (c) extract all the min-entropy using O[log/sup 3/n+log n/spl middot/log(1//spl epsiv/)] truly random bits.
[polynomially small error, functions, Laboratories, extractor error reduction, extractor functions, source min-entropy, Electronic switching systems, Mathematics, Radio access networks, Postal services, additional random bits, Computer science, Uniform resource locators, truly random bits, strings, weak random source, US Department of Defense, Random variables, extractor constructions, errors, minimum entropy methods, computational complexity]
An approximate L/sup 1/-difference algorithm for massive data streams
40th Annual Symposium on Foundations of Computer Science
None
1999
We give a space-efficient, one-pass algorithm for approximating the L/sup 1/ difference /spl Sigma//sub i/|a/sub i/-b/sub i/| between two functions, when the function values a/sub i/ and b/sub i/ are given as data streams, and their order is chosen by an adversary. Our main technical innovation is a method of constructing families {V/sub j/} of limited independence random variables that are range summable by which we mean that /spl Sigma//sub j=0//sup c-1/ V/sub j/(s) is computable in time polylog(c), for all seeds s. These random variable families may be of interest outside our current application domain, i.e., massive data streams generated by communication networks. Our L/sup 1/-difference algorithm can be viewed as a "sketching" algorithm, in the sense of (A. Broder et al., 1998), and our algorithm performs better than that of Broder et al., when used to approximate the symmetric difference of two sets with small symmetric difference.
[space-efficient one-pass algorithm, Technological innovation, application domain, Computerized monitoring, Instruments, communication networks, random processes, range summable, limited independence random variables, set theory, Statistics, approximate L/sup 1/-difference algorithm, function values, sketching algorithm, Reactive power, symmetric difference, random variable families, technical innovation, Internet, data handling, massive data streams, L/sup 1/-difference algorithm, computational complexity]
Regular languages are testable with a constant number of queries
40th Annual Symposium on Foundations of Computer Science
None
1999
We continue the study of combinatorial property testing, initiated by Goldreich, Goldwasser and Ron (1996). The subject of this paper is testing regular languages. Our main result is as follows. For a regular language L/spl isin/{0, 1}* and an integer n there exists a randomized algorithm which always accepts a word w of length n if w/spl isin/L, and rejects it with high probability if w has to be modified in at least En positions to create a word in L. The algorithm queries O~(1//spl epsiv/) bits of w. This query complexity is shown to be optimal up to a factor poly-logarithmic in 1//spl epsiv/. We also discuss testability of more complex languages and show, in particular, that the query complexity required for testing context free languages cannot be bounded by any function of /spl epsiv/. The problem of testing regular languages can be viewed as a part of a very general approach, seeking to probe testability of properties defined by logical means.
[Performance evaluation, Gold, formal languages, context free language testing, testing, regular language testability, rejection probability, query complexity, Mathematics, Electrical capacitance tomography, combinatorial property testing, randomized algorithm, Logic testing, Radio access networks, randomised algorithms, Computer science, Geometry, Microwave integrated circuits, constant query number, Approximation algorithms, word, computational complexity]
Finding maximal repetitions in a word in linear time
40th Annual Symposium on Foundations of Computer Science
None
1999
A repetition in a word w is a subword with the period of at most half of the subword length. We study maximal repetitions occurring in w, that is those for which any extended subword of w has a bigger period. The set of such repetitions represents in a compact way all repetitions in w. We first prove a combinatorial result asserting that the sum of exponents of all maximal repetitions of a word of length n is bounded by a linear function in n. This implies, in particular that there is only a linear number of maximal repetitions in a word. This allows us to construct a linear-time algorithm for finding all maximal repetitions. Some consequences and applications of these results are discussed, as well as related works.
[Educational programs, formal languages, Electrical capacitance tomography, Combinatorial mathematics, extended subword, linear time algorithm, maximal repetition finding, Ear, DH-HEMTs, linear function, subword, combinatorial result, Informatics, word, computational complexity]
Hardness of approximating the minimum distance of a linear code
40th Annual Symposium on Foundations of Computer Science
None
1999
We show that the minimum distance of a linear code (or equivalently, the weight of the lightest codeword) is not approximable to within any constant factor in random polynomial time (RP), unless NP equals RP. Under the stronger assumption that NP is not contained in RQP (random quasi-polynomial time), we show that the minimum distance is not approximable to within the factor 2/sup log(1-/spl epsiv/)n/, for any /spl epsiv/>0, where n denotes the block length of the code. Our results hold for codes over every finite field, including the special case of binary codes. In the process we show that the nearest codeword problem is hard to solve even under the promise that the number of errors is (a constant factor) smaller than the distance of the code. This is a particularly meaningful version of the nearest codeword problem. Our results strengthen (though using stronger assumptions) a previous result of A. Vardy (1997) who showed that the minimum distance is NP-hard to compute exactly. Our results are obtained by adapting proofs of analogous results for integer lattices due to M. Ajtai (1998) and D. Micciancio (1998). A critical component in the adaptation is our use of linear codes that perform better than random (linear) codes.
[NP-hard, NP, linear codes, linear code, nearest codeword problem, Electronic switching systems, Tellurium, Linear code, Reactive power, random polynomial time, constant factor, Polynomials, theorem proving, integer lattices, minimum distance approximation, lightest codeword, binary codes, stronger assumption, Educational institutions, Vectors, code block length, RQP, hardness, finite field, Ear, random quasi-polynomial time, Computer errors, Error correction codes, RP, computational complexity]
Approximating fractional multicommodity flow independent of the number of commodities
40th Annual Symposium on Foundations of Computer Science
None
1999
We describe fully polynomial time approximation schemes for various multicommodity flow problems in graphs with m edges and n vertices. We present the first approximation scheme for maximum multicommodity flow that is independent of the number of commodities k, and our algorithm improves upon the runtime of previous algorithms by this factor of k, running in O*(/spl epsiv//sup -2/ m/sup 2/) time. For maximum concurrent flow, and minimum cost concurrent flow, we present algorithms that are faster than the current known algorithms when the graph is sparse or the number of commodities k is large, i.e. k>m/n. Our algorithms build on the framework proposed by Garg and Konemann (1998). They are simple, deterministic, and for the versions without costs, they are strongly polynomial. Our maximum multicommodity flow algorithm extends to an approximation scheme for the maximum weighted multicommodity flow, which is faster than those implied by previous algorithms by a factor of k/log W where W is the maximum weight of a commodity.
[polynomial time approximation, Identity-based encryption, Operations research, Industrial engineering, deterministic, multicommodity flow problems, deterministic algorithms, maximum concurrent flow, optimisation, graphs, strongly polynomial, directed graphs, operations research, Cost function, Approximation algorithms, Polynomials, Econometrics, minimum cost concurrent flow]
PSPACE has constant-round quantum interactive proof systems
40th Annual Symposium on Foundations of Computer Science
None
1999
We introduce quantum interactive proof systems, which are interactive proof systems in which the prover and verifier may perform quantum computations and exchange quantum messages. It is proved that every language in PSPACE has a quantum interactive proof system that requires a total of only three messages to be sent between the prover and verifier and has exponentially small (one-sided) probability of error. It follows that quantum interactive proof systems are strictly more powerful than classical interactive proof systems in the constant-round case unless the polynomial time hierarchy collapses to the second level.
[error probability, polynomial time hierarchy, Electronic switching systems, PSPACE, Power system modeling, Cryptographic protocols, Computer science, Quantum computing, Physics computing, Quantum mechanics, quantum computing, Polynomials, theorem proving, quantum computations, Cryptography, quantum messages, constant-round proof systems, computational complexity, quantum interactive proof systems]
A near-tight lower bound on the time complexity of distributed MST construction
40th Annual Symposium on Foundations of Computer Science
None
1999
This paper presents a lower bound of /spl Omega/~(D+/spl radic/n) on the time required for the distributed construction of a minimum-weight spanning tree (MST) in n-vertex networks of diameter D=/spl Omega/(log n), in the bounded message model. This establishes the asymptotic near-optimality of existing time-efficient distributed algorithms for the problem, whose complexity is O(D+/spl radic/nlog* n).
[Art, time-efficient distributed algorithms, near-tight lower bound, message-optimal algorithm, distributed MST construction, Nominations and elections, trees (mathematics), computational geometry, time complexity, Mathematics, communication complexity, Gallium nitride, minimum-weight spanning tree, bounded message model, Computer science, Network topology, distributed algorithms, mailing problem, distributed construction, Broadcasting, Computer networks, Distributed algorithms]
Edge-disjoint routing in plane switch graphs in linear time
40th Annual Symposium on Foundations of Computer Science
None
1999
By a switch graph we mean an undirected graph G=(P/spl cup//spl dot/W,E) such that all vertices in P (the plugs) have degree one and all vertices in W (the switches) have even degrees. We call G plane if G is planar and can be embedded such that all plugs are in the outer face. Given a set (s/sub 1/,t/sub 1/), ..., (s/sub k/,t/sub k/) of pairs of plugs, the problem is to find edge-disjoint paths p/sub 1/, ..., p/sub k/ such that every p/sub i/ connects s/sub i/ with t/sub i/. The best asymptotic worst case complexity known so far is quadratic in the number of vertices. A linear, and thus asymptotically optimal algorithm is introduced. This result may be viewed as a concluding "key-stone" for a number of previous results on various special cases of the problem.
[best asymptotic worst case complexity, graph theory, edge-disjoint paths, Switches, Routing, set theory, plane switch graphs, outer face, Ear, edge-disjoint routing, Polynomials, asymptotically optimal algorithm, undirected graph, Plugs, computational complexity, linear time]
Lovasz's lemma for the three-dimensional K-level of concave surfaces and its applications
40th Annual Symposium on Foundations of Computer Science
None
1999
We show that for any line l in space, there are at most k(k+1) tangent planes through l to the k-level of an arrangement of concave surfaces. This is a generalization of L. Lovasz's (1971) lemma, which is a key constituent in the analysis of the complexity of k-level of planes. Our proof is constructive, and finds a family of concave surfaces covering the "laminated at-most-k level". As consequences, (1): we have an O((n-k)/sup 2/3/n/sup 2/) upper bound for the complexity of the k-level of n triangle of space, and (2): we can extend the k-set result in space to the k-set of a system of subsets of n points.
[complexity, combinatorial mathematics, Laboratories, concave surfaces, Geometry, Upper bound, three-dimensional K-level, k-set result, laminated at-most-k level, Lovasz lemma, constructive proof, geometry, computational complexity, tangent planes, subsets]
Cache-oblivious algorithms
40th Annual Symposium on Foundations of Computer Science
None
1999
This paper presents asymptotically optimal algorithms for rectangular matrix transpose, FFT, and sorting on computers with multiple levels of caching. Unlike previous optimal algorithms, these algorithms are cache oblivious: no variables dependent on hardware parameters, such as cache size and cache-line length, need to be tuned to achieve optimality. Nevertheless, these algorithms use an optimal amount of work and move data optimally among multiple levels of cache. For a cache with size Z and cache-line length L where Z=/spl Omega/(L/sup 2/) the number of cache misses for an m/spl times/n matrix transpose is /spl Theta/(1+mn/L). The number of cache misses for either an n-point FFT or the sorting of n numbers is /spl Theta/(1+(n/L)(1+log/sub Z/n)). We also give an /spl Theta/(mnp)-work algorithm to multiply an m/spl times/n matrix by an n/spl times/p matrix that incurs /spl Theta/(1+(mn+np+mp)/L+mnp/L/spl radic/Z) cache faults. We introduce an "ideal-cache" model to analyze our algorithms. We prove that an optimal cache-oblivious algorithm designed for two levels of memory is also optimal for multiple levels and that the assumption of optimal replacement in the ideal-cache model. Can be simulated efficiently by LRU replacement. We also provide preliminary empirical results on the effectiveness of cache-oblivious algorithms in practice.
[Algorithm design and analysis, fast Fourier transforms, rectangular matrix transpose, cache misses, Laboratories, Banking, cache faults, cache-oblivious algorithms, resource-oblivious algorithms, Sorting, matrix algebra, Strontium, ideal-cache, LRU replacement, FFT, sorting, Hardware, Central Processing Unit, asymptotically optimal algorithms, computational complexity, cache obliviousness]
Magic functions
40th Annual Symposium on Foundations of Computer Science
None
1999
In this paper we show that three apparently unrelated problems are in fact very closely related. We sketch these problems at a high level. The selective decommitment problem first arose in a slightly different form, selective decryption, in the context of Byzantine agreement, no later than 1985. Instead of seeing encryptions of plaintexts the adversary is given commitments to the plaintexts. This problem is poorly understood even in strong-receiver commitments, which leak no information about the plaintext values information-theoretically. The second problem is in complexity theory: what can be proved in (a possibly weakened form of) zero-knowledge in a 3-round argument (interactive proof in which the prover is polynomial-time bounded)? The Fiat-Shamir Methodology is cryptographic, and addresses a methodology suggested by Fiat and Shamir (1987) to construct a (non-interactive) signature scheme from any 3-round (not necessarily zero-knowledge) public-coin identification scheme.
[Roads, signature scheme, Switches, Mathematics, Complexity theory, selective decommitment problem, Security, Postal services, Microwave integrated circuits, plaintexts, Polynomials, information theory, theorem proving, Cryptography, magic functions, selective decryption, complexity theory, Byzantine agreement, 3-round weak zero-knowledge arguments, cryptography, strong-receiver commitments, Computer science, 3-round public-coin identification scheme, Fiat-Shamir Methodology, computational complexity]
An algorithmic theory of learning: robust concepts and random projection
40th Annual Symposium on Foundations of Computer Science
None
1999
We study the phenomenon of cognitive learning from an algorithmic standpoint. How does the brain effectively learn concepts from a small number of examples despite the fact that each example contains a huge amount of information? We provide a novel analysis for a model of robust concept learning (closely related to "margin classifiers"), and show that a relatively small number of examples are sufficient to learn rich concept classes (including threshold functions, Boolean formulae and polynomial surfaces). As a result, we obtain simple intuitive proofs for the generalization bounds of Support Vector Machines. In addition, the new algorithm has several advantages-they are faster conceptually simpler and highly resistant to noise. For example, a robust half-space can be PAC-learned in linear time using only a constant number of training examples, regardless of the number of attributes. A general (algorithmic) consequence of the model, that "more robust concepts are easier to learn\
[cognitive systems, Boolean formulae, Psychology, algorithmic theory of learning, cognitive learning, Cognition, Mathematics, Read only memory, Support vector machines, robust concepts, Boolean functions, Animals, intuitive proofs, psychological studies, Ear, polynomial surfaces, Robustness, Polynomials, learning (artificial intelligence), random projection, threshold functions]
A better lower bound for quantum algorithms searching an ordered list
40th Annual Symposium on Foundations of Computer Science
None
1999
We show that any quantum algorithm searching an ordered list of n elements needs to examine at least (log,n)/12-O(1) of them. Classically, log/sub 2/ n queries are both necessary and sufficient. This shows that quantum algorithms can achieve only a constant speedup for this problem.
[searching, Search problems, lower bound, queries, Postal services, Computer science, Quantum computing, Algorithms, quantum computing, National electric code, quantum algorithms, constant speedup, search problems, ordered list searching, computational complexity]
A probabilistic algorithm for k-SAT and constraint satisfaction problems
40th Annual Symposium on Foundations of Computer Science
None
1999
We present a simple probabilistic algorithm for solving k-SAT and more generally, for solving constraint satisfaction problems (CSP). The algorithm follows a simple local search paradigm (S. Minton et al., 1992): randomly guess an initial assignment and then, guided by those clauses (constraints) that are not satisfied, by successively choosing a random literal from such a clause and flipping the corresponding bit, try to find a satisfying assignment. If no satisfying assignment is found after O(n) steps, start over again. Our analysis shows that for any satisfiable k-CNF-formula with n variables this process has to be repeated only t times, on the average, to find a satisfying assignment, where t is within a polynomial factor of (2(1-1/k))/sup n/. This is the fastest (and also the simplest) algorithm for 3-SAT known up to date. We consider also the more general case of a CSP with n variables, each variable taking at most d values, and constraints of order l, and analyze the complexity of the corresponding (generalized) algorith m. It turns out that any CSP can be solved with complexity at most (d/spl middot/(1-1/l)+/spl epsiv/)/sup n/.
[Algorithm design and analysis, simple probabilistic algorithm, complexity, CSP, Error probability, constraint theory, probability, random literal, 3-SAT, satisfiable k-CNF-formula, polynomial factor, initial assignment, Read only memory, generalized algorithm, constraint satisfaction problems, probabilistic algorithm, simple local search paradigm, satisfying assignment, k-SAT, search problems, computational complexity]
Improved bounds for sampling colorings
40th Annual Symposium on Foundations of Computer Science
None
1999
We consider the problem of sampling uniformly from the set of proper k-colorings of a graph with maximum degree /spl Delta/. Our main result is the design Markov chain that converges in O(nk log n) time to the desired distribution when k>11/6 /spl Delta/.
[Temperature, Lattices, Antiferromagnetic materials, sampling, k-colorings, sampling colorings, design Markov chain, Physics, graph colouring, Radio access networks, Computer science, Microwave integrated circuits, O(nk log n), Sampling methods, Polynomials, Labeling, computational complexity]
Noncryptographic selection protocols
40th Annual Symposium on Foundations of Computer Science
None
1999
Selection tasks generalize some well studied problems, such as collective coin flipping and leader election. We present new selection protocols in the full information model, and new negative results. In particular when there are (1+/spl delta/)n/2 good players, we show a protocol that chooses a good leader with probability /spl Omega/(/spl delta//sup 1.65/), and show that every leader election protocol has success probability O(/spl delta//sup 1-/spl epsiv//), for every /spl epsiv/>0. Previously known protocols for this problem have success probability that is exponentially small in 1//spl delta/, and no nontrivial upper bounds on the success probability were known.
[leader election, Computational modeling, noncryptographic selection protocols, collective coin flipping, Nominations and elections, probability, upper bounds, Cryptographic protocols, Communication standards, Upper bound, Boolean functions, Communication channels, Broadcasting, protocols, information model, computational complexity]
Non-interactive cryptocomputing for NC/sup 1/
40th Annual Symposium on Foundations of Computer Science
None
1999
The area of "computing with encrypted data" has been studied by numerous authors in the past twenty years since it is fundamental to understanding properties of encryption and it has many practical applications. The related fundamental area of "secure function evaluation" has been studied since the mid 80's. In its basic two-party case, two parties (Alice and Bob) evaluate a known circuit over private inputs (or a private input and a private circuit). Much attention has been paid to the important issue of minimizing rounds of computation in this model. Namely, the number of communication rounds in which Alice and Bob need to engage in to evaluate a circuit on encrypted data securely. Advancements in these areas have been recognized as open problems and have remained open for a number of years. In this paper we give a one round, and thus round optimal, protocol for secure evaluation of circuits which is in polynomial time for NC/sup 1/ circuits. The protocol involves an input party sending encrypted input to a second party, a cryptocomputer, which evaluates the circuit (or a known circuit over its additional private input) non-interactively, securely and obliviously, and provides the output to the input party without learning it. This improves on previous (general) results that are specialized to the case of NC/sup 1/ circuits and require a constant number of communication rounds. We further suggest applications to network and mobile computing.
[Protocols, noninteractive cryptocomputing, NC/sup 1/, Circuits, network computing, cryptography, NC/sup 1/ circuits, input party, one round protocol, secure function evaluation, Computer science, mobile computing, encrypted input, communication rounds, encryption, Public key, round optimal protocol, Public key cryptography, Polynomials, Computer networks, polynomial time, cryptocomputer, secure circuit evaluation]
Online scheduling to minimize average stretch
40th Annual Symposium on Foundations of Computer Science
None
1999
We consider the classical problem of online job scheduling on uniprocessor and multiprocessor machines. For a given job, we measure the quality of service provided by an algorithm by the stretch of the job, which is defined as the ratio of the amount of time that the job spends in the system to the processing time of the job. For a given sequence of jobs, we measure the performance of an algorithm by the average stretch achieved by the algorithm over all the jobs in the sequence. The average stretch metric has been used to evaluate the performance of scheduling algorithms in many applications arising in databases, networks and systems; however no formal analysis of scheduling algorithms is known for the average stretch metric. The main contribution of the paper is to show that the shortest remaining processing time algorithm (SRPT) is O(l)-competitive with respect to average stretch for both uniprocessors as well as multiprocessors. For uniprocessors, we prove that SRPT is 2-competitive; we also establish an essentially matching lower bound on the competitive ratio of SRPT. For multiprocessors, we show that the competitive ratio of SRPT is at most 14. Furthermore, we establish constant-factor lower bounds on the competitive ratio of any online algorithm for both uniprocessors and multiprocessors.
[SRPT, Throughput, multiprocessor machines, matching lower bound, Delay, shortest remaining processing time algorithm, Application specific integrated circuits, online job scheduling, competitive ratio, scheduling algorithms, Databases, average stretch metric, scheduling, formal analysis, theorem proving, software performance evaluation, constant-factor lower bounds, multiprocessing systems, average stretch minimization, online algorithm, Time measurement, quality of service, Computer science, 2-competitive, Processor scheduling, algorithm performance, processing time, computational complexity, uniprocessors]
Approximate nearest neighbor algorithms for Hausdorff metrics via embeddings
40th Annual Symposium on Foundations of Computer Science
None
1999
Hausdorff metrics are used in geometric settings for measuring the distance between sets of points. They have been used extensively in areas such as computer vision, pattern recognition and computational chemistry. While computing the distance between a single pair of sets under the Hausdorff metric has been well studied, no results are known for the nearest-neighbor problem under Hausdorff metrics. Indeed, no results were known for the nearest-neighbor problem for any metric without a norm structure, of which the Hausdorff is one. We present the first nearest-neighbor algorithm for the Hausdorff metric. We achieve our result by embedding Hausdorff metrics into l/sub /spl infin// and by using known nearest-neighbor algorithms for this target metric. We give upper and lower bounds on the number of dimensions needed for such an l/sub /spl infin// embedding. Our bounds require the introduction of new techniques based on superimposed codes and non-uniform sampling.
[embeddings, codes, computational geometry, computational chemistry, point distance measurement, search problems, pattern recognition, Computer vision, sampling methods, approximate nearest neighbor algorithms, Engineering profession, superimposed codes, nonuniform sampling, Extraterrestrial measurements, Pattern recognition, Application software, norm structure, Nearest neighbor searches, Computer science, dimensional bounds, Chemistry, Hausdorff metrics, computer vision, Sampling methods, geometry, Pattern matching]
Primality and identity testing via Chinese remaindering
40th Annual Symposium on Foundations of Computer Science
None
1999
Gives a simple and new primality testing algorithm by reducing primality testing for a number n to testing if a specific univariate identity over Z/sub n/ holds. We also give new randomized algorithms for testing if a multivariate polynomial, over a finite field or over rationals, is identically zero. The first of these algorithms also works over Z/sub n/ for any n. The running time of the algorithms is polynomial in the size of the arithmetic circuit representing the input polynomial and the error parameter. These algorithms use fewer random bits and work for a larger class of polynomials than all the previously known methods, e.g. the Schwartz-Zippel test (J.T. Schwartz, 1980; R.E. Zippel, 1979), the Chen-Kao (1997) test and the Lewin-Vadhan (1998) test. Our algorithms first transform the input polynomial to a univariate polynomial and then use Chinese remaindering over univariate polynomials to effectively test if it is zero.
[Circuits, Chen-Kao test, Binary decision diagrams, random bits, input polynomial, polynomial-time algorithms, Polynomials, identity testing algorithm, Testing, arithmetic circuit size, rational numbers, Chinese remainder theorem, univariate identity, testing, error parameter, Schwartz-Zippel test, randomised algorithms, univariate polynomial, Computer science, primality testing algorithm, Lewin-Vadhan test, finite field, randomized algorithms, multivariate polynomial, Arithmetic, number theory, computational complexity]
On universal and fault-tolerant quantum computing: a novel basis and a new constructive proof of universality for Shor's basis
40th Annual Symposium on Foundations of Computer Science
None
1999
A novel universal and fault-tolerant basis (set of gates) for quantum computation is described. Such a set is necessary to perform quantum computation in a realistic noisy environment. The new basis consists of two single-qubit gates (Hadamard and /spl sigma//sub z//sup 1/4 /) and one double-qubit gate (Controlled-NOT). Since the set consisting of Controlled-NOT and Hadamard gates is not universal, the new basis achieves universality by including only one additional elementary (in the sense that it does not include angles that are irrational multiples of /spl pi/) single-qubit gate, and hence, is potentially the simplest universal basis that one can construct. We also provide an alternative proof of universality for the only other known class of universal and fault-tolerant basis proposed by P.W. Shor (1996) and A.Y. Kitaev (1997).
[universality, Shor basis, Subcontracting, Error-free operation, set theory, quantum computation gates, alternative proof, double-qubit gate, Fault tolerance, Quantum computing, Controlled-NOT, theorem proving, Contracts, Hadamard, Computational modeling, quantum gates, Circuit faults, irrational multiples, realistic noisy environment, single-qubit gates, Working environment noise, Quantum mechanics, constructive proof, fault-tolerant basis, simplest universal basis, fault tolerant computing, Error correction, fault-tolerant quantum computing]
Verifiable random functions
40th Annual Symposium on Foundations of Computer Science
None
1999
We efficiently combine unpredictability and verifiability by extending the Goldreich-Goldwasser-Micali (1986) construction of pseudorandom functions f/sub s/ from a secret seed s, so that knowledge of s not only enables one to evaluate f/sub s/ at any point x, but also to provide an NP-proof that the value f/sub s/(x) is indeed correct without compromising the unpredictability of f/sub s/ at any other point for which no such a proof was provided.
[NP-proof, verifiable random functions, Terminology, Laboratories, unpredictability, pseudorandom functions, Electrical capacitance tomography, Postal services, Computer science, Uniform resource locators, random functions, Polynomials, US Department of Defense, Contracts, Testing, computational complexity]
Limits on the efficiency of one-way permutation-based hash functions
40th Annual Symposium on Foundations of Computer Science
None
1999
Naor and Yung (1989) show that a one-bit-compressing universal one-way hash function (UOWHF) can be constructed based on a one-way permutation. This construction can be iterated to build a UOWHF which compresses by /spl epsiv/n bits, at the cost of /spl epsiv/n invocations of the one-way permutation. The show that this construction is not far from optimal, in the following sense, there exists an oracle relative to which there exists a one-way permutation with inversion probability 2/sup -p(n)/ (for any p(n)/spl isin//spl omega/(log n)), but any construction of an /spl epsiv/n-bit-compressing UOWHF. Requires /spl Omega/(/spl radic/n/p(n)) invocations of the one-way permutation, on average. (For example, there exists in this relativized world a one-way permutation with inversion probability n/sup -/spl omega/(1)/, but no UOWHF that involves it fewer than /spl Omega/(/spl radic/n/log n) times.) Thus any proof that a more efficient UOWHF can be derived from a one-way permutation is necessarily non-relativizing; in particular, no provable construction of a more efficient UOWHF can exist based solely on a "black box" one-way permutation. This result can be viewed as a partial justification for the practice of building efficient UOWHFs from stronger primitives (such as collision intractable hash functions), rather than from weaker primitives such as one-way permutations.
[oracle, efficiency limits, Buildings, Circuits, one-way permutation-based hash functions, probability, cryptography, Complexity theory, inversion probability, primitives, one-bit-compressing universal one-way hash function, iteration, Polynomials, Cryptography, Digital signatures, computational complexity]
Boosting and hard-core sets
40th Annual Symposium on Foundations of Computer Science
None
1999
This paper connects two fundamental ideas from theoretical computer science hard-core set construction, a type of hardness amplification from computational complexity, and boosting, a technique from computational learning theory. Using this connection we give fruitful applications of complexity-theoretic techniques to learning theory and vice versa. We show that the hard-core set construction of R. Impagliazzo (1995), which establishes the existence of distributions under which boolean functions are highly inapproximable, may be viewed as a boosting algorithm. Using alternate boosting methods we give an improved bound for hard-core set construction which matches known lower bounds from boosting and thus is optimal within this class of techniques. We then show how to apply techniques from R. Impagliazzo to give a new version of Jackson's celebrated Harmonic Sieve algorithm for learning DNF formulae under the uniform distribution using membership queries. Our new version has a significant asymptotic improvement in running time. Critical to our arguments is a careful analysis of the distributions which are employed in both boosting and hard-core set constructions.
[boosting, Circuits, Boosting, computational learning theory, Mathematics, Application software, hardness amplification, Read only memory, boolean functions, learning systems, Harmonic Sieve algorithm, Computer science, membership queries, Boolean functions, R. Impagliazzo, hard-core sets, Impagliazzo, Marine vehicles, computational complexity]
Random CNFs are hard for the polynomial calculus
40th Annual Symposium on Foundations of Computer Science
None
1999
We show a general reduction that derives lower bounds on degrees of polynomial calculus proofs of tautologies, over any field of characteristic (other than 2) from lower bounds for resolution proofs of a related set of linear equations module 2. We apply this to derive linear lower bounds on the degrees of PC proofs of randomly generated tautologies.
[randomly generated tautologies, module 2, random processes, computability, Size measurement, Calculus, random CNFs, linear lower bounds, Equations, Read only memory, PC proofs, Computer science, resolution proofs, polynomial calculus proofs, Layout, general reduction, Ear, Chromium, Polynomials, Concrete, theorem proving, computational complexity, linear equations]
A sublinear time approximation scheme for clustering in metric spaces
40th Annual Symposium on Foundations of Computer Science
None
1999
The metric 2-clustering problem is defined as follows: given a metric (or weighted graph) (X,d), partition X into two sets S(1) and S(2) in order to minimize the value of /spl Sigma//sub i//spl Sigma//sub {u,v}/spl sub/S(i)/d(u,v). In this paper, we show an approximation scheme for this problem.
[approximation theory, weighted graph, graph theory, Extraterrestrial measurements, metric spaces, set theory, set partitioning, pattern clustering, minimization, Clustering algorithms, metric 2-clustering problem, sublinear-time approximation scheme, Polynomials, clustering, minimisation, computational complexity]
Fairness in routing and load balancing
40th Annual Symposium on Foundations of Computer Science
None
1999
We consider the issue of network routing subject to explicit fairness conditions. The optimization of fairness criteria interacts in a complex fashion with the optimization of network utilization and throughput; in this work, we undertake an investigation of this relationship through the framework of approximation algorithms. In this work we consider the problem of selecting paths for routing so as to provide a bandwidth allocation that is as fair as possible (in the max-min sense). We obtain the first approximation algorithms for this basic optimization problem, for single-source unsplittable routings in an arbitrary directed graph. Special cases of our model include several fundamental load balancing problems, endowing them with a natural fairness criterion to which our approach can be applied. Our results form an interesting counterpart to the work of Megiddo (1974), who considered max-min fairness for single-source fractional flow. The optimization problems in our setting become NP-complete, and require the development of new techniques for relating fractional relaxations of routing to the equilibrium constraints imposed by the fairness criterion.
[equilibrium constraints, Career development, load balancing, Throughput, approximation algorithms, fractional relaxations, Constraint optimization, optimisation, network utilization optimization, resource allocation, single-source fractional flow, Bandwidth, network throughput optimization, single-source unsplittable routings, Contracts, network routing, Routing, explicit fairness conditions, NP-complete problem, Computer science, bandwidth allocation, fairness criteria optimization, telecommunication network routing, Ear, max-min fairness, Load management, selecting paths, Internet, computational complexity, arbitrary directed grap]
Weak adversaries for the k-server problem
40th Annual Symposium on Foundations of Computer Science
None
1999
We study the k-server problem when the offline algorithm has fewer than k servers. We give two upper bounds of the cost WFA(/spl rho/) of the Work Function Algorithm. The first upper bound is kOPT/sub h/(/spl rho/)+(h-1)OPT/sub k/(/spl rho/), where OPT/sub m/(/spl rho/) denotes the optimal cost to service /spl rho/ by m servers. The second upper bound is 2hOPTh(/spl rho/)-OPT/sub k/(/spl rho/) for h/spl les/k. Both bounds imply that the Work Function Algorithm is (2k-1)-competitive. Perhaps more important is our technique which seems promising for settling the k-server conjecture. The proofs are simple and intuitive and they do not involve potential functions. We also apply the technique to give a simple condition for the Work Function Algorithm to be k-competitive; this condition results in a new proof that the k-server conjecture holds for k=2.
[Algorithm design and analysis, weak adversaries, k-server conjecture, Optimized production technology, competitive algorithms, Extraterrestrial measurements, upper bound, k-server problem, optimal cost, Upper bound, optimisation, potential functions, proofs, Cost function, Work Function Algorithm, Performance analysis, theorem proving, offline algorithm, k-competitive, computational complexity]
Satisfiability of word equations with constants is in PSPACE
40th Annual Symposium on Foundations of Computer Science
None
1999
We prove that the satisfiability problem for word equations is in PSPACE. The satisfiability problem for word equations has a simple formulation: find out whether or not an input word equation has a solution. The decidability of the problem was proved by G.S. Makanin (1977). His decision procedure is one of the most complicated algorithms existing in the literature. We propose an alternative algorithm. The full version of the algorithm requires only a proof of the upper bound for index of periodicity of a minimal solution (A. Koscielski and L. Pacholski, see Journal of ACM, vol.43, no.4. p.670-84). Our algorithm is the first one which is proved to work in polynomial space.
[computability, upper bound, PSPACE, input word equation, Equations, Upper bound, NP-hard problem, decidability, satisfiability problem, decision procedure, Polynomials, theorem proving, word equation satisfiability, index of periodicity, Informatics, minimal solution, computational complexity]
Learning mixtures of Gaussians
40th Annual Symposium on Foundations of Computer Science
None
1999
Mixtures of Gaussians are among the most fundamental and widely used statistical models. Current techniques for learning such mixtures from data are local search heuristics with weak performance guarantees. We present the first provably correct algorithm for learning a mixture of Gaussians. This algorithm is very simple and returns the true centers of the Gaussians to within the precision specified by the user with high probability. It runs in time only linear in the dimension of the data and polynomial in the number of Gaussians.
[Astrophysics, Geology, Psychology, probability, local search heuristics, Probability, Electrical capacitance tomography, History, Statistics, Read only memory, learning systems, Clustering algorithms, Gaussian processes, statistical models, mixtures of Gaussians learning]
Stochastic load balancing and related problems
40th Annual Symposium on Foundations of Computer Science
None
1999
We study the problems of makespan minimization (load balancing), knapsack, and bin packing when the jobs have stochastic processing requirements or sizes. If the jobs are all Poisson, we present a two approximation for the first problem using Graham's rule, and observe that polynomial time approximation schemes can be obtained for the last two problems. If the jobs are all exponential, we present polynomial time approximation schemes for all three problems. We also obtain quasi-polynomial time approximation schemes for the last two problems if the jobs are Bernoulli variables.
[Poisson jobs, knapsack problem, Stochastic processes, exponential jobs, Bernoulli variables, stochastic processing requirements, bin packing, stochastic load balancing, Computer science, makespan minimization, resource allocation, Graham's rule, Character generation, quasi-polynomial time approximation schemes, scheduling, Load management, Approximation algorithms, stochastic sizes, Random variables, minimisation, stochastic processes, polynomial time approximation schemes, computational complexity]
Finely-competitive paging
40th Annual Symposium on Foundations of Computer Science
None
1999
We construct an online algorithm for paging that achieves an O(r+log k) competitive ratio when compared to an offline strategy that is allowed the additional ability to "rent" pages at a cost of 1/r. In contrast, the competitive ratio of the Marking algorithm for this scenario is O(r log k). Our algorithm can be thought of in the standard setting as having a "fine-grained" competitive ratio, achieving an O(1) ratio when the request sequence consists of a small number of working sets, gracefully decaying to O(log k) as this number increases. Our result is a generalization of the result by Y. Bartal et al. (1997) that one can achieve an O(r+log n) ratio for the unfair n-state uniform-space Metrical Task System problem. That result was a key component of the polylog(n) competitive randomized algorithm given in that paper for the general Metrical Task System problem. One motivation of this work is that it may be a first step toward achieving a polylog(k) randomized competitive ratio for the much more difficult k-server problem.
[paged storage, Costs, standard setting, unfair n-state uniform-space Metrical Task System problem, finely-competitive paging, online algorithm, competitive randomized algorithm, Independent component analysis, competitive algorithms, randomized competitive ratio, cache storage, k-server problem, randomised algorithms, competitive ratio, working sets, general Metrical Task System problem, graceful decay, request sequence, Marking algorithm, computational complexity, fine-grained competitive ratio]
Reducing network congestion and blocking probability through balanced allocation
40th Annual Symposium on Foundations of Computer Science
None
1999
We compare the performance of a variant of the standard dynamic alternative routing (DAR) technique commonly used in telephone and ATM networks to a path selection algorithm that is based on the balanced allocations principle-the Balanced Dynamic Alternative Routing (BDAR) algorithm. While the standard technique checks alternative routes sequentially until available bandwidth is found, the BDAR algorithm compares and chooses the best among a small number of alternatives. We show that, at the expense of a minor increase in routing overhead, the BDAR gives a substantial improvement in network performance in terms of both network congestion and blocking probabilities.
[Protocols, telecommunication congestion control, asynchronous transfer mode, Electrical capacitance tomography, network congestion reduction, blocking probability reduction, standard dynamic alternative routing technique, Network servers, balanced allocation, Network topology, network performance, sequential alternative route checking, Bandwidth, Telephony, balanced allocations principle, path selection algorithm, Balanced Dynamic Alternative Routing algorithm, probability, Routing, Telecommunications, bandwidth allocation, telecommunication network routing, telephone networks, Load management, Asynchronous transfer mode, ATM networks]
On the complexity of SAT
40th Annual Symposium on Foundations of Computer Science
None
1999
We show that non-deterministic time NTIME(n) is not contained in deterministic time n/sup 2-/spl epsiv// and polylogarithmic space, for any /spl epsiv/>0. This implies that (infinitely often), satisfiability cannot be solved in time O(n/sup 2-/spl epsiv//) and polylogarithmic space. A similar result is presented for uniform circuits; a log-space uniform circuit of polylogarithmic width computing satisfiability requires infinitely often almost quadratic size.
[Circuit simulation, non-deterministic time, polylogarithmic space, computability, NTIME, polylogarithmic width, deterministic time, Computer science, almost quadratic size, Microwave integrated circuits, uniform circuits, Turing machines, SAT complexity, satisfiability, log-space uniform circuit, Polynomials, theorem proving, computational complexity]
Hardness of approximating /spl Sigma//sub 2//sup p/ minimization problems
40th Annual Symposium on Foundations of Computer Science
None
1999
We show that a number of natural optimization problems in the second level of the Polynomial Hierarchy are /spl Sigma//sub 2//sup p/-hard to approximate to within n/sup /spl epsiv// factors, for specific /spl epsiv/>0. The main technical tool is the use of explicit dispersers to achieve strong, direct inapproximability results. The problems we consider include Succinct Set Cover, Minimum Equivalent DNF, and other problems relating to DNF minimization. Under a slightly stronger complexity assumption, our method gives optimal n/sup 1-/spl epsiv// inapproximability results for some of these problems. We also prove inapproximability of a variant of an NP optimization problem, Monotone Minimum Satisfying Assignment, to within an n/sup /spl epsiv// factor using the same technique.
[Greedy algorithms, Succinct Set Cover, /spl Sigma//sub 2//sup p/ minimization problem approximation, explicit dispersers, Monotone Minimum Satisfying Assignment, set theory, Machinery, optimal inapproximability results, Minimum Equivalent DNF, Logic circuits, Polynomials, Bipartite graph, natural optimization problems, Minimization methods, stronger complexity assumption, DNF minimization, Polynomial Hierarchy, Computer science, hardness, direct inapproximability results, NP optimization problem, Approximation algorithms, Circuit synthesis, minimisation, computational complexity]
Private quantum channels
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We investigate how a classical private key can be used by two players, connected by an insecure one-way quantum channel, to perform private communication of quantum information. In particular, we show that in order to transmit n qubits privately, 2n bits of shared private key are necessary and sufficient. This result may be viewed as the quantum analogue of the classical one-time pad encryption scheme.
[Teleportation, insecure one-way quantum channel, cryptography, Ice, Entropy, classical private key, quantum information, one-time pad encryption scheme, Information security, quantum computing, qubits, Cryptography, Marine vehicles, private quantum channels, private communication]
Extracting randomness from samplable distributions
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
The standard notion of a randomness extractor is a procedure which converts any weak source of randomness into an almost uniform distribution. The conversion necessarily uses a small amount of pure randomness, which can be eliminated by complete enumeration in some, but not all, applications. We consider the problem of deterministically converting a weak source of randomness into an almost uniform distribution. Previously, deterministic extraction procedures were known only for sources satisfying strong independence requirements. We look at sources which are samplable, i.e. can be generated by an efficient sampling algorithm. We seek an efficient deterministic procedure that, given a sample from any samplable distribution of sufficiently large min-entropy, gives an almost uniformly distributed output. We explore the conditions under which such deterministic extractors exist. We observe that no deterministic extractor exists if the sampler is allowed to use more computational resources than the extractor. On the other hand, if the extractor is allowed (polynomially) more resources than the sampler, we show that deterministic extraction becomes possible. This is true unconditionally in the nonuniform setting (i.e., when the extractor can be computed by a small circuit), and (necessarily) relies on complexity assumptions in the uniform setting.
[Algorithm design and analysis, deterministic extraction procedures, complexity, samplable distributions, almost uniform distribution, Circuits, random processes, randomness extractor, min-entropy, Complexity theory, Cryptographic protocols, Computer science, sampling algorithm, NP-hard problem, Sampling methods, Polynomials, Error correction codes, Cryptography, computational complexity]
Extracting randomness via repeated condensing
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
On an input probability distribution with some (min-)entropy an extractor outputs a distribution with a (near) maximum entropy rate (namely the uniform distribution). A natural weakening of this concept is a condenser, whose output distribution has a higher entropy rate than the input distribution (without losing much of the initial entropy). We construct efficient explicit condensers. The condenser constructions combine (variants or more efficient versions of) ideas from several works, including the block extraction scheme of Nisan and Zuckerman (1996), the observation made by Srinivasan and Zuckerman (1994) and Nisan and Ta-Schma (1999) that a failure of the block extraction scheme is also useful, the recursive "win-win" case analysis of Impagliazzo et al. (1999, 2000), and the error correction of random sources used by Trevisan (1999). As a natural byproduct, (via repeated iterating of condensers), we obtain new extractor constructions. The new extractors give significant qualitative improvements over previous ones for sources of arbitrary min-entropy; they are nearly optimal simultaneously in the main two parameters-seed length and output length. Specifically, our extractors can make any of these two parameters optimal (up to a constant factor), only at a poly-logarithmic loss in the other. Previous constructions require polynomial loss in both cases for general sources. We also give a simple reduction converting "standard" extractors (which are good for an average seed) to "strong " ones (which are good for mast seeds), with essentially the same parameters.
[Availability, input probability distribution, output distribution, probability, random processes, Entropy, Probability distribution, random sources, maximum entropy rate, block extraction scheme, Computer science, polynomial loss, entropy, USA Councils, randomness extraction, Failure analysis, recursive win-win case analysis, Polynomials, Error correction, repeated condensing, condenser, computational complexity, error correction]
Approximating the single source unsplittable min-cost flow problem
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
In the single source unsplittable min-cost flow problem, commodities must be routed simultaneously from a common source vertex to certain destination vertices in a given graph with edge capacities and costs; the demand of each commodity must be routed along a single path and the total cost must not exceed a given budget. This problem has been introduced by J.M. Kleinberg (1996) and generalizes several NP-complete problems from various areas in combinatorial optimization such as packing, partitioning, scheduling load balancing, and virtual-circuit routing. S.G. Kolliopoulos and C. Stein (2000) and Y.N. Dinitz et al. (1999) developed algorithms improving the first approximation results of Kleinberg for the problem to minimize the violation of edge capacities and for other variants. However, none of the developed techniques is capable of providing solutions without also violating the cost constraint. We give the first approximation results with hard cost constraints. Moreover all our results dominate the best known bicriteria approximations. Finally, we provide results on the hardness of approximation for several variants of the problem.
[edge capacities, load balancing, bicriteria approximations, Routing, Partitioning algorithms, NP-complete problem, packing, common source vertex, processor scheduling, virtual-circuit routing, combinatorial optimization, hardness, optimisation, hard cost constraints, resource allocation, single source unsplittable min-cost flow problem, NP-complete problems, Cost function, Load management, Approximation algorithms, partitioning, computational complexity, destination vertices]
Universality and tolerance
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
For any positive integers r and n, let H(r,n) denote the family of graphs on n vertices with maximum degree r, and let H(r,n,n) denote the family of bipartite graphs H on 2n vertices with n vertices in each vertex class, and with maximum degree r. On one hand, we note that any H(r,n)-universal graph must have /spl Omega/(n/sup 2-2/r/) edges. On the other hand, for any n/spl ges/n/sub 0/(r), we explicitly construct H(r,n)-universal graphs G and /spl Lambda/ on n and 2n vertices, and with O(n/sup 2-/spl Omega//(1/r log r)) and O(n/sup 2-1/r/ log/sup 1/r/ n) edges, respectively, such that we can efficiently find a copy of any H /spl epsiv/ H (r,n) in G deterministically. We also achieve sparse universal graphs using random constructions. Finally, we show that the bipartite random graph G=G(n,n,p), with p=cn/sup -1/2r/ log/sup 1/2r/ n is fault-tolerant; for a large enough constant c, even after deleting any /spl alpha/-fraction of the edges of G, the resulting graph is still H(r,/spl alpha/(/spl alpha/)n,/spl alpha/(/spl alpha/)n)-universal for some /spl alpha/: [0,1)/spl rarr/(0,1].
[universality, Costs, Particle separators, graph theory, vertices, Very large scale integration, fault-tolerant bipartite random graph, Mathematics, random constructions, Computer science, Geometry, sparse universal graphs, Fault tolerance, graphs, bipartite graphs, maximum degree, Bipartite graph, Circuit synthesis, positive integers, tolerance]
Nested graph dissection and approximation algorithms
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
This paper considers approximation algorithms for graph completion problems using the nested dissection paradigm. Given a super-additive function of interest (the smallest planar or chordal extension for example) and a test that relates it to an upper bound of the smallest separator, we provide a framework how to dissect the graph recursively such that no subgraph has more than half the value of its parent, (or is indistinguishable via separator tests) in polynomial time. Interestingly we cannot bound such a function till we have constructed the entire nested dissection. We achieve a partition of the graph with respect to a constant number of such unknown estimator functions simultaneously. Using the framework the paper presents improvements in approximating the chordal completion size (by a factor of log n), operation count (by a factor of log/sup 2/ n and the polynomial term depending on degree) and elimination height. We show that there exists a nested dissection ordering that simultaneously minimizes the elimination height, chordal completion, operation count to within O(log n) factors of the best possible (which may be obtained by three independent orderings) improving the previous existence theorem by factors of log n and d/sup 1/3/ log/sup 3/ n for the latter two. We also show that graphs with small crossing number or fill-in have better approximations of the elimination height, completion and operation count. As a consequence we can approximate the pathwidth, cutwidth, vertex ranking problems better for such graphs. The paper also improves, in some cases, the approximation results of minimum drawing size (number of vertices plus the crossing number) of a planar embedding of a graph, and its layout area on a grid.
[Linear systems, Embedded computing, approximation theory, Particle separators, polynomials, graph theory, Very large scale integration, computational geometry, planar embedding, upper bound, approximation algorithms, Sparse matrices, Upper bound, nested dissection paradigm, vertex ranking, chordal completion size, nested graph dissection, Approximation algorithms, Grid computing, Polynomials, polynomial term, Testing]
Randomizing polynomials: A new representation with applications to round-efficient secure computation
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Motivated by questions about secure multi-party computation, we introduce and study a new natural representation of functions by polynomials, which we term randomizing polynomials. "Standard" low-degree polynomials over a finite field are easy to compute with a small number of communication rounds in virtually any setting for secure computation. However, most Boolean functions cannot be evaluated by a polynomial whose degree is smaller than their input size. We get around this barrier by relaxing the requirement of evaluating f into a weaker requirement of randomizing f: mapping the inputs of f along with independent random inputs into a vector of outputs, whose distribution depends only on the value of f. We show that degree-3 polynomials are sufficient to randomize any function f, relating the efficiency of such a randomization to the branching program size of f. On the other hand, by characterizing the exact class of Boolean functions which can be randomized by degree-2 polynomials, we show that 3 is the minimal randomization degree of most functions. As an application, randomizing polynomials provide a powerful, general, and conceptually simple tool for the design of round-efficient secure protocols. Specifically, the secure evaluation of any function can be reduced to a secure evaluation of degree-3 polynomials. One corollary of this reduction is that two (respectively, three) communication rounds are sufficient for k parties to compute any Boolean function f of their inputs, with perfect information-theoretic [k-1/3]-privacy (resp., [k-1/2]-privacy), and communication complexity which is at most quadratic in the branching program size of f (with a small probability of one-sided error).
[Protocols, Computational modeling, Input variables, polynomials, probability, secure multi-party computation, Complexity theory, communication complexity, Galois fields, natural representation, Computer science, round-efficient secure computation, low-degree polynomials, Boolean functions, Computer applications, round-efficient secure protocols, Computer errors, randomizing polynomials, Polynomials, degree-3 polynomials, protocols, branching program]
Efficient algorithms for universal portfolios
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
A constant rebalanced portfolio is an investment strategy which keeps the same distribution of wealth among a set of stocks from day to day. There has been much work on Cover's Universal algorithm, which is competitive with the best constant rebalanced portfolio determined in hindsight (D. Helmbold et al., 1995; A. Blum and A. Kalai, 1999; T.M. Cover and E. Ordentlich, 1996). While this algorithm has good performance guarantees, all known implementations are exponential in the number of stocks, restricting the number of stocks used in experiments. We present an efficient implementation of the Universal algorithm that is based on non-uniform random walks that are rapidly mixing (D. Applegate and R. Kannanm, 1991). This same implementation also works for non-financial applications of the Universal algorithm, such as data compression (T.M. Cover, 1886) and language modeling (A. Kalai et al., 1999).
[wealth, non-financial applications, data compression, Engineering profession, universal portfolios, investment strategy, Laboratories, constant rebalanced portfolio, Data compression, competitive algorithms, investment, Mathematics, Computer science, non-uniform random walks, Investments, performance guarantees, Sampling methods, language modelin, competitive algorithm, stock markets, Portfolios, best constant rebalanced portfolio, Universal algorithm]
Polynomial time approximation schemes for geometric k-clustering
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We deal with the problem of clustering data points. Given n points in a larger set (for example, R/sup d/) endowed with a distance function (for example, L/sup 2/ distance), we would like to partition the data set into k disjoint clusters, each with a "cluster center\
[Operations research, distance function, k-median problem, Hamming distance, high dimensional geometry, data point clustering, computational geometry, Partitioning algorithms, binary cube, geometric k-clustering, data set partitioning, Construction industry, Uniform resource locators, NP-hard problem, pattern clustering, Euclidean distance, Cities and towns, Polynomials, Contracts, polynomial time approximation schemes, Computational biology, computational complexity]
On computing the determinant and Smith form of an integer matrix
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
A probabilistic algorithm is presented to find the determinant of a nonsingular, integer matrix. For a matrix A/spl isin/Z/sup n/spl times/n/ the algorithm requires O(n/sup 3.5/(log n)/sup 4.5/) bit operations (assuming for now that entries in A have constant size) using standard matrix and integer arithmetic. Using asymptotically fast matrix arithmetic, a variant is described which requires O(n/sup 2+/spl theta//2//spl middot/log/sup 2/nloglogn) bit operations, where n/spl times/n matrices can be multiplied with O(n/sup /spl theta//) operations. The determinant is found by computing the Smith form of the integer matrix an extremely useful canonical form in itself. Our algorithm is probabilistic of the Monte Carlo type. That is, it assumes a source of random bits and on any invocation of the algorithm there is a small probability of error.
[Monte Carlo method, Costs, mathematics computing, probability, Sparse matrices, Smith form, random bits, matrix algebra, matrix determinant computing, Computer science, Computational geometry, matrix multiplication, Monte Carlo methods, integer arithmetic, Councils, probabilistic algorithm, Computer applications, nonsingular integer matrix, asymptotically fast matrix arithmetic, Arithmetic, computational complexity]
Straightening polygonal arcs and convexifying polygonal cycles
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Consider a planar linkage, consisting of disjoint polygonal arcs and cycles of rigid bars joined at incident endpoints (polygonal chains), with the property that no cycle surrounds another arc or cycle. We prove that the linkage can be continuously moved so that the arcs become straight, the cycles become convex, and no bars cross while preserving the bar lengths. Furthermore, our motion is piecewise-differentiable, does not decrease the distance between any pair of vertices, and preserves any symmetry present in the initial configuration. In particular this result settles the well-studied carpenter's rule conjecture.
[graph theory, polygonal cycle convexifying, Fasteners, computational geometry, polygonal arc straightening, convex cycles, Wire, rule conjecture, Couplings, Computational geometry, Physics computing, symmetry, piecewise-differentiable motion, Biology computing, planar linkage, Polymers, polygonal chains, Robots, Bars]
Stochastic models for the Web graph
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
The Web may be viewed as a directed graph each of whose vertices is a static HTML Web page, and each of whose edges corresponds to a hyperlink from one Web page to another. We propose and analyze random graph models inspired by a series of empirical observations on the Web. Our graph models differ from the traditional G/sub n,p/ models in two ways: 1. Independently chosen edges do not result in the statistics (degree distributions, clique multitudes) observed on the Web. Thus, edges in our model are statistically dependent on each other. 2. Our model introduces new vertices in the graph as time evolves. This captures the fact that the Web is changing with time. Our results are two fold: we show that graphs generated using our model exhibit the statistics observed on the Web graph, and additionally, that natural graph models proposed earlier do not exhibit them. This remains true even when these earlier models are generalized to account for the arrival of vertices over time. In particular, the sparse random graphs in our models exhibit properties that do not arise in far denser random graphs generated by Erdos-Renyi models.
[information resources, hyperlink, sparse random graphs, Erdos-Renyi models, vertices, Stochastic processes, random processes, Predictive models, HTML, Statistics, Computer science, Couplings, stochastic models, Web graph, directed graphs, Web pages, directed graph, random graph model, static HTML Web page, stochastic processes, statistics]
Entropy waves, the zig-zag graph product, and new constant-degree expanders and extractors
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
The main contribution is a new type of graph product, which we call the zig-zag product. Taking a product of a large graph with a small graph, the resulting graph inherits (roughly) its size from the large one, its degree from the small one, and its expansion properties from both. Iteration yields simple explicit constructions of constant-degree expanders of every size, starting from one constant-size expander. Crucial to our intuition (and simple analysis) of the properties of this graph product is the view of expanders as functions which act as "entropy wave" propagators-they transform probability distributions in which entropy is concentrated in one area to distributions where that concentration is dissipated. In these terms, the graph product affords the constructive interference of two such waves. A variant of this product can be applied to extractors, giving the first explicit extractors whose seed length depends (poly)logarithmically on only the entropy deficiency of the source (rather than its length) and that extract almost all the entropy of high min-entropy sources. These high min-entropy extractors have several interesting applications, including the first constant-degree explicit expanders which beat the "eigenvalue bound".
[Codes, graph theory, probability, zig-zag graph product, constant-degree extractors, Entropy, Graph theory, Probability distribution, Mathematics, Complexity theory, explicit extractors, eigenvalues and eigenfunctions, Computer science, constructive interference, entropy, constant-degree expanders, probability distributions, high min-entropy sources, entropy waves, Eigenvalues and eigenfunctions, Polynomials, Cryptography, eigenvalue bound]
The randomness recycler: a new technique for perfect sampling
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
For many probability distributions of interest, it is quite difficult to obtain samples efficiently. Often, Markov chains are employed to obtain approximately random samples from these distributions. The primary drawback to traditional Markov chain methods is that the mixing time of the chain is usually unknown, which makes it impossible to determine how close the output samples are to having the target distribution. The authors present a novel protocol, the randomness recycler (RR), that overcomes this difficulty. Unlike classical Markov chain approaches, an RR-based algorithm creates samples drawn exactly from the desired distribution. Other perfect sampling methods such as coupling from the past, use existing Markov chains, but RR does not use the traditional Markov chain at all. While by no means universally useful, RR does apply to a wide variety of problems. In restricted instances of certain problems, it gives the first expected linear time algorithms for generating samples. The authors apply RR to self-organizing lists, the Ising model, random independent sets, random colorings, and the random cluster model.
[Algorithm design and analysis, restricted instances, first expected linear time algorithms, Protocols, perfect sampling methods, RR-based algorithm, graph theory, Markov chains, Probability distribution, Monte Carlo methods, output samples, random colorings, probability distributions, Statistical distributions, Clustering algorithms, Ising model, classical Markov chain approaches, sampling methods, probability, random processes, RR protocol, self-adjusting systems, approximately random samples, Time measurement, perfect sampling, self-organizing lists, random cluster model, Sampling methods, target distribution, sample generation, randomness recycler, random independent sets, mixing time]
On clusterings-good, bad and spectral
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We propose a new measure for assessing the quality of a clustering. A simple heuristic is shown to give worst-case guarantees under the new measure. Then we present two results regarding the quality of the clustering found by a popular spectral algorithm. One proffers worst case guarantees whilst the other shows that if there exists a "good" clustering then the spectral algorithm will find one close to it.
[Algorithm design and analysis, Engineering profession, worst-case guarantees, heuristic, Mathematics, Partitioning algorithms, randomized algorithm, Spectral analysis, randomised algorithms, Computer science, spectral algorithm, heuristic programming, pattern clustering, polynomial time algorithms, Clustering algorithms, Performance analysis, clustering quality assessment measure, spectral clustering, computational complexity]
Fast parallel circuits for the quantum Fourier transform
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We give new bounds on the circuit complexity of the quantum Fourier transform (QFT). We give an upper bound of O(log n+log log(1//spl epsiv/)) on the circuit depth for computing an approximation of the QFT with respect to the modulus 2/sup n/ with error bounded by /spl epsiv/. Thus, even for exponentially small error, our circuits have depth O(log n). The best previous depth bound was O(n), even for approximations with constant error. Moreover, our circuits have size O(n log(n//spl epsiv/)). As an application of this depth bound, we show that P. Shor's (1997) factoring algorithm may be based on quantum circuits with depth only O(log n) and polynomial size, in combination with classical polynomial-time pre- and postprocessing. Next, we prove an /spl Omega/(log n) lower bound on the depth complexity of approximations of the QFT with constant error. This implies that the above upper bound is asymptotically tight (for a reasonable range of values of /spl epsiv/). We also give an upper bound of O(n(log n)/sup 2/ log log n) on the circuit size of the exact QFT modulo 2/sup n/, for which the best previous bound was O(n/sup 2/). Finally, based on our circuits for the QFT with power-of-2 moduli, we show that the QFT with respect to an arbitrary modulus m can be approximated with accuracy /spl epsiv/ with circuits of depth O((log log m)(log log 1//spl epsiv/)) and size polynomial in log m+log(1//spl epsiv/).
[Heart, circuit complexity, Fourier transforms, quantum circuits, constant error, Circuits, depth complexity, Complexity theory, circuit depth, factoring algorithm, polynomial size, arbitrary modulus, Quantum computing, Polynomials, theorem proving, QFT, Discrete Fourier transforms, depth bound, upper bound, lower bound, Computer science, Upper bound, classical polynomial-time processing, Signal processing algorithms, quantum computing, fast parallel circuits, quantum Fourier transform]
Cost-distance: two metric network design
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Presents the cost-distance problem, which consists of finding a Steiner tree which optimizes the sum of edge costs along one metric and the sum of source-sink distances along an unrelated second metric. We give the first known O(log k) randomized approximation scheme for the cost-distance problem, where k is the number of sources. We reduce several common network design problems to cost-distance problems, obtaining (in some cases) the first known logarithmic approximation for them. These problems include a single-sink buy-at-bulk problem with variable pipe types between different sets of nodes, facility location with buy-at-bulk-type costs on edges, constructing single-source multicast trees with good cost and delay properties, and multi-level facility location. Our algorithm is also easier to implement and significantly faster than previously known algorithms for buy-at-bulk design problems.
[Algorithm design and analysis, 2-metric network design, single-sink buy-at-bulk problem, Delay, cost-distance problem, facility location, Network servers, single-source multicast trees, Tree graphs, Network topology, randomized approximation scheme, network synthesis, Cost function, approximation theory, logarithmic approximation, variable pipe types, cost, source number, trees (mathematics), edges, delay properties, randomised algorithms, source-sink distance sum optimization, Computer science, Multicast algorithms, Steiner tree, telecommunication network routing, multi-level facility location, Approximation algorithms, edge cost sum optimization, Joining processes, computational complexity]
Approximability and in-approximability results for no-wait shop scheduling
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We investigate the approximability of no-wait shop scheduling problems under the makespan criterion. In a flow shop, all jobs pass through the machines in the same ordering. In the more general job shop, the routes of the jobs are job-dependent. We present a polynomial time approximation scheme (PTAS) for the no-wait flow shop problem on any fixed number of machines. Unless P=NP, this result cannot be extended to the job shop problem on a fixed number of machines: We show that the no-wait job shop problem is APX-hard on (i) two machines with at most five operations per job, and on (ii) three machines with at most three operations per job.
[Job shop scheduling, makespan criterion, no-wait shop scheduling, Traveling salesman problems, processor scheduling, in-approximability results, Processor scheduling, polynomial approximation, polynomial time approximation scheme, Production, Polynomials, APX-hard, computational complexity, approximability]
Zaps and their applications
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
A zap is a two-round, witness-indistinguishable protocol in which the first round, consisting of a message from the verifier to the prover, can be fixed "once-and-for-all" and applied to any instance, and where the verifier does not use any private coins. We present a zap for every language in NP, based on the existence of non-interactive zero-knowledge proofs in the shared random string model. The zap is in the standard model, and hence requires no common guaranteed random string. We introduce and construct verifiable pseudo-random bit generators (VPRGs), and give a complete existential characterization of both noninteractive zero-knowledge proofs and zaps in terms of approximate VPRGs. We present several applications for zaps; In the timing model of C. Dwork et al. (1998) and using moderately hard functions, we obtain 3-round concurrent zero knowledge and 2-round concurrent deniable authentication (the latter protocol also operates in the resettable model of R. Canetti et al. (2000)). In the standard model we obtain 2-round oblivious transfer using public keys (3-round otherwise). We note that any zap yields resettable 2-round witness-indistinguishability and obtain a 3-round timing-based resettable zero-knowledge argument system for any language in NP.
[Protocols, zap, public keys, cryptography, NP completeness, concurrent zero knowledge, Application software, verifiable pseudo-random bit generators, Computer science, concurrent deniable authentication, Authentication, Public key, verifier, witness-indistinguishable protocol, Timing, Nuclear magnetic resonance, theorem proving, Cryptography, shared random string model, Random sequences, Testing, computational complexity, zero-knowledge proofs]
"Soft-decision" decoding of Chinese remainder codes
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Given n relatively prime integers p/sub 1/<...<p/sub n/ and an integer k<n, the Chinese Remainder Code, CRT/sub p1,...,pnik/, has as its message space M={0,...,/spl Pi//sub i=1//sup k/,pi-1}, and encodes a message m /spl isin/M as the vector <m/sub 1/,...,m/sub n/>, where m/sub i/=m(mod p/sub i/). The soft-decision decoding problem for the Chinese remainder code is given as input a vector of residues r/spl I.oarr/=(r/sub 1/,...,r/sub n/), a vector of weights <w/sub 1/,...,w/sub n/>, and an agreement parameter t. The goal is to find all messages m /spl isin/ M such that the weighted agreement between the encoding of m and r/spl I.oarr/(i.e., /spl Sigma//sub i/ w/sub i/ summed over all i such that r/sub i/=m(mod pi)) is at least t. Here we give a new algorithm for solving the soft-decision problem for the CRT code that works provided the agreement parameter t is sufficiently large. We derive our algorithm by digging deeper into the algebra underlying the error-correcting algorithms and unveiling an "ideal"-theoretic view of decoding. When all weights are equal to 1, we obtain the more commonly studied "list decoding" problem. List decoding algorithms for the Chinese Remainder Code were given recently by O. Goldreich et al. (1999), and improved by D. Boneh. Their algorithms work for t/spl ges//spl radic/(2knlogp/sub n//logp1) and t/spl ges//spl radic/(knlogp/sub n//logp/sub 1/), respectively. We improve upon the algorithms above by using our soft-decision decoding algorithm with a non-trivial choice of weights, solve the list decoding problem provided t/spl ges//spl radic/(k(n+/spl epsi/)), for arbitrarily small /spl epsi//spl ges/0.
[error-correcting algorithms, Codes, Engineering profession, Laboratories, Redundancy, Chinese remainder codes, Encoding, Decoding, encoding, decoding, Computer science, Algebra, agreement parameter, prime integers, message space, list decoding, Cathode ray tubes, US Department of Defense, soft-decision decoding]
The product replacement algorithm is polynomial
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
The product replacement algorithm is a heuristic designed to generate random group elements. The idea is to run a random walk on generating /spl kappa/-tuples of the group, and then output a random component. The algorithm was designed by C.R. Leedham-Green, and further investigated by F. Cellar et al. (1995). It was found to have an outstanding performance, much better than the previously known algorithms (P. Diaconis and L. Saloff-Coste, 1996). The algorithm is now included in two major group algebra packages: GAP (M. Scheonert et al., 1995) and MAGMA (W. Bosma et al., 1997). In spite of the many serious attempts and partial results, the analysis of the algorithm remains difficult at best. For small values of /spl kappa/, even graph connectivity becomes a serious obstacle. The most general results are due to Diaconis and Saloff-Coste, who used a state of the art analytic technique to obtain polynomial bounds in special cases, and (sub)-exponential bounds in the general case. The main result of the paper is a polynomial upper bound for the cost of the algorithm, provided /spl kappa/ is large enough.
[Costs, Heuristic algorithms, Mathematics, generating /spl kappa/-tuples, polynomial bounds, random number generation, random walk, Algebra, heuristic programming, state of the art analytic technique, random group elements, Tail, random component, symbol manipulation, Polynomials, MAGMA, Random number generation, group algebra packages, sub exponential bounds, polynomials, heuristic, Nearest neighbor searches, group theory, graph connectivity, Upper bound, polynomial upper bound, GAP, Packaging, product replacement algorithm]
On the hardness of graph isomorphism
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We show that the graph isomorphism problem is hard under logarithmic space many-one reductions for the complexity classes NL, PL (probabilistic logarithmic space), for every logarithmic space modular class Mod/sub k/L and for the class DET of problems NC/sup 1/ reducible to the determinant. These are the strongest existing hardness results for the graph isomorphism problem, and imply a randomized logarithmic space reduction from the perfect matching problem to graph isomorphism.
[complexity classes, probabilistic logarithmic space, Circuits, graph theory, logarithmic space many-one reductions, Encoding, Orbits, hardness results, NP-complete problem, encoding, Jacobian matrices, hardness, Upper bound, Tree graphs, graph isomorphism, perfect matching, Polynomials, Eigenvalues and eigenfunctions, randomized logarithmic space reduction, computational complexity, determinant]
Fast broadcasting and gossiping in radio networks
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We establish an O(n log/sup 2/n) upper bound on the time for deterministic distributed broadcasting in multi-hop radio networks with unknown topology. This nearly matches the known lower bound of /spl Omega/(n log n). The fastest previously known algorithm for this problem works in time O(n/sup 3/2/). Using our broadcasting algorithm, we develop an O(n/sup 3/2/log/sup 2/n) algorithm for gossiping in the same network model.
[radio broadcasting, radio networks, gossiping, multi-hop radio networks, Radio transmitters, time complexity, upper bound, lower bound, Radio broadcasting, network topology, communication complexity, deterministic algorithms, Computer science, Intelligent networks, Upper bound, Network topology, fast broadcasting algorithm, Spread spectrum communication, Radio network, Radio networks, deterministic distributed broadcasting, Communication networks]
The quantum complexity of set membership
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Studies the quantum complexity of the static set membership problem: given a subset S (|S|/spl les/n) of a universe of size m(/spl Gt/n), store it as a table, T:(0,1)/sup r//spl rarr/(0,1), of bits so that queries of the form 'is x in S?' can be answered. The goal is to use a small table and yet answer queries using a few bit probes. This problem was considered by H. Buhrman et al. (2000), who showed lower and upper bounds for this problem in the classical deterministic and randomised models. In this paper, we formulate this problem in the "quantum bit-probe model". We assume that access to the table T is provided by means of a black-box (oracle) unitary transform O/sub T/ that takes the basis state (y,b) to the basis state |y,b/spl oplus/T(y)>. The query algorithm is allowed to apply O/sub T/ on any superposition of basis states. We show tradeoff results between the space (defined as 2/sup r/) and the number of probes (oracle calls) in this model. Our results show that the lower bounds shown by Buhrman et al. for the classical model also hold (with minor differences) in the quantum bit-probe model. These bounds almost match the classical upper bounds. Our lower bounds are proved using linear algebraic arguments.
[static set membership problem, upper bounds, Mathematics, oracle calls, set theory, quantum complexity, black-box unitary transform, query processing, Quantum computing, basis state superposition, probes, Probes, linear algebra, Data structures, Extraterrestrial measurements, query algorithm, lower bounds, Computer science, query answering, Upper bound, quantum bit-probe model, quantum computing, bit table, space-probe tradeoff, Feeds, computational complexity]
Hierarchical placement and network design problems
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Gives constant approximations for a number of layered network design problems. We begin by modeling hierarchical caching, where the caches are placed in layers and each layer satisfies a fixed percentage of the demand (bounded miss rates). We present a constant approximation to the minimum total cost of placing the caches and to the routing demand through the layers. We extend this model to cover more general layered caching scenarios, giving a constant combinatorial approximation to the well-studied multi-level facility location problem. We consider a facility location variant, the load-balanced facility location problem, in which every demand is served by a unique facility and each open facility must serve at least a certain amount of demand. By combining load-balanced facility location with our results on hierarchical caching, we give a constant approximation for the access network design problem.
[multi-level facility location problem, Spine, cache storage, minimum total cost, facility location, hierarchical caching, combinatorial approximation, open facilities, resource allocation, network synthesis, routing demand, layered caching scenarios, Cost function, subscriber loops, IP networks, approximation theory, access network design problem, Routing, bounded miss rates, Time measurement, hierarchical systems, Computer science, constant approximations, hierarchical placement, load-balanced facility location problem, file organisation, layered network design problems, Internet]
The online median problem
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We introduce a natural variant of the (metric uncapacitated) k-median problem that we call the online median problem. Whereas the k-median problem involves optimizing the simultaneous placement of k facilities, the on-line median problem imposes the following additional constraints: the facilities are placed one at a time; a facility cannot be moved once it is placed, and the total number of facilities to be placed, k, is not known in advance. The objective of an online median algorithm is to minimize the competitive ratio, that is, the worst-case ratio of the cost of an online placement to that of an optimal offline placement. Our main result is a linear-time constant-competitive algorithm for the online median problem. In addition, we present a related, though substantially simpler linear-time constant-factor approximation algorithm for the (metric uncapacitated) facility location problem. The latter algorithm is similar in spirit to the recent primal-dual-based facility location algorithm of Jain and Vazirani, but our approach is more elementary and yields an improved running time.
[approximation theory, k-median problem, primal-dual-based facility location algorithm, worst-case ratio, facility location, linear-time constant-factor approximation algorithm, Computer science, competitive ratio, heuristic programming, Linear approximation, Cities and towns, Approximation algorithms, Cost function, Polynomials, online median problem, linear-time constant-competitive algorithm]
New data structures for orthogonal range searching
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We present new general techniques for static orthogonal range searching problems in two and higher dimensions. For the general range reporting problem in R/sup 3/, we achieve query time O(log n+k) using space O(n log/sup 1+/spl epsiv// n), where n denotes the number of stored points and k the number of points to be reported. For the range reporting problem on an n/spl times/n grid, we achieve query time O(log log n+k) using space O(n log/sup /spl epsiv// n). For the two-dimensional semi-group range sum problem we achieve query time O(log n) using space O(n log n).
[two-dimensional semi-group range sum problem, Computational modeling, Read-write memory, orthogonal range searching, Data structures, Remuneration, Computer science, Upper bound, Databases, data structures, Books, general range reporting problem, Contracts, search problems]
How bad is selfish routing?
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We consider the problem of routing traffic to optimize the performance of a congested network. We are given a network, a rate of traffic between each pair of nodes, and a latency function for each edge specifying the time needed to traverse the edge given its congestion; the objective is to route traffic such that the sum of all travel times-the total latency-is minimized. In many settings, including the Internet and other large-scale communication networks, it may be expensive or impossible to regulate network traffic so as to implement an optimal assignment of routes. In the absence of regulation by some central authority, we assume that each network user routes its traffic on the minimum-latency path available to it, given the network congestion caused by the other users. In general such a "selfishly motivated" assignment of traffic to paths will not minimize the total latency; hence, this lack of regulation carries the cost of decreased network performance. We quantify the degradation in network performance due to unregulated traffic. We prove that if the latency of each edge is a linear function of its congestion, then the total latency of the routes chosen by selfish network users is at most 4/3 times the minimum possible total latency (subject to the condition that all traffic must be routed). We also consider the more general setting in which edge latency functions are assumed only to be continuous and non-decreasing in the edge congestion.
[congested network, Costs, telecommunication congestion control, Telecommunication traffic, Routing, Nash equilibrium, traffic routing, Game theory, selfish routing, latency function, network traffic regulation, Delay, Computer science, Degradation, telecommunication network routing, edge congestion, minimum-latency path, Large-scale systems, Internet, performance optimization, Communication networks, optimal route assignment, telecommunication traffic, large-scale communication networks]
Existential second-order logic over graphs: charting the tractability frontier
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Fagin's (1974) theorem, the first important result of descriptive complexity, asserts that a property of graphs is in NP if and only if it is definable by an existential second-order formula. We study the complexity of evaluating existential second-order formulas that belong to prefix classes of existential second-order logic, where a prefix class is the collection of all existential second-order and the first-order quantifiers obey a certain quantifier pattern. We completely characterize the computation complexity of prefix classes of existential second-order logic in three different contexts: over directed graphs; over undirected graphs with self-loops; and over undirected graphs without self-loops. Our main result is that in each of these three contexts a dichotomy holds, i.e., each prefix class of existential second-order logic either contains sentences that can express NP-complete problems or each of its sentences expresses a polynomial-time solvable problem. Although the boundary of the dichotomy coincides for the first two cases, it changes, as one move to undirected graphs without self-loops.
[graph theory, descriptive complexity, first-order quantifiers, Complexity theory, NP-complete problem, Computational complexity, Machinery, Combinatorial mathematics, tractability, formal logic, graphs, Tree graphs, polynomial-time solvable problem, directed graphs, NP-complete problems, existential second-order formula, Polynomials, Logic, undirected graphs, existential second-order logic, computational complexity, prefix class]
On the boundary complexity of the union of fat triangles
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
A triangle is said to be /spl delta/-fat if its smallest angle is at least /spl delta/>0. A connected component of the complement of the union of a family of triangles is called hole. It is shown that any family of /spl delta/-far triangles in the plane determines at most O (n//spl delta/ log 2//spl delta/) holes. This improves on some earlier bounds of (Efrat et al., 1993; Matousek et al., 1994). Solving a problem of (Agarwal and Bern, 1999) we also give a general upper bound for the number of holes determined by n triangles in the plane with given angles. As a corollary, we obtain improved upper bounds for the boundary complexity of the union of fat polygons in the plane, which, in turn, leads to better upper bounds for the running times of some known algorithms for motion planning, for finding a separator line for a set of segments, etc.
[Heart, separator line, Geographic Information Systems, motion planning, Particle separators, computational geometry, upper bounds, Educational institutions, Computational geometry, Upper bound, fat triangle union, Computer graphics, Cities and towns, boundary complexity, holes, computational complexity]
Testing that distributions are close
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Given two distributions over an n element set, we wish to check whether these distributions are statistically close by only sampling. We give a sublinear algorithm which uses O(n/sup 2/3//spl epsiv//sup -4/ log n) independent samples from each distribution, runs in time linear in the sample size, makes no assumptions about the structure of the distributions, and distinguishes the cases when the distance between the distributions is small (less than max(/spl epsiv//sup 2//32/sup 3//spl radic/n,/spl epsiv//4/spl radic/n=)) or large (more than /spl epsiv/) in L/sub 1/-distance. We also give an /spl Omega/(n/sup 2/3//spl epsiv//sup -2/3/) lower bound. Our algorithm has applications to the problem of checking whether a given Markov process is rapidly mixing. We develop sublinear algorithms for this problem as well.
[Markov process, sampling methods, Engineering profession, Filtering, distribution closeness testing, probability, sampling, lower bound, rapidly mixing process, Computer science, sublinear algorithms, National electric code, Markov processes, Sampling methods, sublinear algorithm, Testing, computational complexity]
Pseudorandom generators in propositional proof complexity
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We call a pseudorandom generator G/sub n/:{0,1}/sup n//spl rarr/{0,1}/sup m/ hard for a propositional proof system P if P can not efficiently prove the (properly encoded) statement G/sub n/(x/sub 1/,...,x/sub n/)/spl ne/b for any string b/spl epsiv/{0,1}/sup m/. We consider a variety of "combinatorial" pseudorandom generators inspired by the Nisan-Wigderson generator on one hand, and by the construction of Tseitin tautologies on the other. We prove that under certain circumstances these generators are hard for such proof systems as resolution, polynomial calculus and polynomial calculus with resolution (PCR).
[polynomial calculus, polynomial calculus with resolution, pseudorandom generators, Circuits, random processes, Calculus, Computational complexity, resolution, Nisan-Wigderson generator, Computer science, Tseitin tautologies, process algebra, propositional proof complexity, Polynomials, theorem proving, combinatorial pseudorandom generators, computational complexity]
Hardness of approximate hypergraph coloring
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We introduce the notion of covering complexity of a probabilistic verifier. The covering complexity of a verifier on a given input is the minimum number of proofs needed to "satisfy" the verifier on every random string, i.e., on every random string, at least one of the given proofs must be accepted by the verifier. The covering complexity of PCP verifiers offers a promising route to getting stronger inapproximability results for some minimization problems, and in particular (hyper)-graph coloring problems. We present a PCP verifier for NP statements that queries only four bits and yet has a covering complexity of one for true statements and a super-constant covering complexity for statements not in the language. Moreover the acceptance predicate of this verifier is a simple Not-all-Equal check on the four bits it reads. This enables us to prove that for any constant c, it is NP-hard to color a 2-colorable 4-uniform hypergraph using just c colors, and also yields a super-constant inapproximability result under a stronger hardness assumption.
[probabilistic verifier, Engineering profession, Laboratories, PCP verifier, computational geometry, approximate hypergraph coloring, graph colouring, Computer science, hardness, hardness assumption, Numerical analysis, covering complexity, 2-colorable 4-uniform hypergraph, minimization problems, minimisation, computational complexity]
Topological persistence and simplification
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We formalize a notion of topological simplification within the framework of a filtration, which is the history of a growing complex. We classify a topological change that happens during growth as either a feature or noise, depending on its life-time or persistence within the filtration. We give fast algorithms for completing persistence and experimental evidence for their speed and utility.
[Filtration, Shape, alpha shapes, fast algorithms, topology, computational geometry, Mathematics, Noise shaping, Topology, topological simplification, History, topological persistence, topological change, Computer science, Computational geometry, filtration, Computer graphics, algorithm theory, computational topology, Density functional theory, homology groups, growing complex]
Lower bounds on the efficiency of generic cryptographic constructions
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We present lower bounds on the efficiency of constructions for Pseudo-Random Generators (PRGs) and Universal One-Way Hash Functions (UOWHFs) based on black-box access to one-way permutations. Our lower bounds are tight as they match the efficiency of known constructions. A PRG (resp. UOWHF) construction based on black-box access is a machine that is given oracle access to a permutation. Whenever the permutation is hard to invert, the construction is hard to break. In this paper we give lower bounds on the number of invocations to the oracle by the construction. If S is the assumed security of the oracle permutation /spl pi/ (i.e. no adversary of size S can invert /spl pi/ on a fraction larger than 1/S of its inputs) then a PRG (resp. UOWHF) construction that stretches (resp. compresses) its input by k bits must query /spl pi/ in q=/spl Omega/(k/log S) points. This matches known constructions. Our results are given in an extension of the Impagliazzo-Rudich model. That is, we prove that a proof of the existence of PRG (resp. UOWHF) black-box constructions that beat our lower bound would imply a proof of the unconditional existence of such construction (which would also imply P/spl ne/NP).
[Circuits, cryptography, Security, random number generation, lower bounds, universal one-way hash functions, black-box access, generic cryptographic constructions, one-way permutations, file organisation, Polynomials, pseudo-random generators, Cryptography, Digital signatures, Impagliazzo-Rudich model]
Super-linear time-space tradeoff lower bounds for randomized computation
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We prove the first time-space lower bound tradeoffs for randomized computation of decision problems. The bounds hold even in the case that the computation is allowed to have arbitrary probability of error on a small fraction of inputs. Our techniques are an extension of those used by M. Ajtai (1999) in his time-space tradeoffs for deterministic RAM algorithms computing element distinctness and for deterministic Boolean branching programs computing an explicit function based on quadratic forms over GF(2). Our results also give a quantitative improvement over those given by Ajtai. Ajtai shows, for certain specific functions, that any branching program using space S=o(n) requires time T that is superlinear. The functional form of the superlinear bound is not given in his paper, but optimizing the parameters in his arguments gives T= /spl Omega/(n log log n/log log log n) for S=0(n/sup 1-/spl epsiv//). For the same functions considered by Ajtai, we prove a time-space tradeoff of the form T=/spl Omega/(n/spl radic/(log(n/S)/log log(n/S))). In particular for space 0(n/sup 1-/spl epsiv//), this improves the lower bound on time to /spl Omega/(n/spl radic/(log n/log log n)).
[deterministic Boolean branching programs, Input variables, randomized computation, probability, Binary decision diagrams, Read-write memory, Mathematics, Complexity theory, lower bound, History, Sorting, randomised algorithms, Computer science, Turing machines, Computer errors, super-linear time-space tradeoff lower bounds, decision problems, deterministic RAM algorithms, time-space tradeoff, computational complexity, branching program]
Sampling adsorbing staircase walks using a new Markov chain decomposition method
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Staircase walks are lattice paths from (0,0) to (2n,0) which take diagonal steps and which never fall below the x-axis. A path hitting the x-axis /spl kappa/ times is assigned a weight of /spl lambda//sup /spl kappa//, where /spl lambda/>0. A simple local Markov chain, which connects the state space and converges to the Gibbs measure (which normalizes these weights) is known to be rapidly mixing when /spl lambda/=1, and can easily be shown to be rapidly mixing when /spl lambda/<1. We give the first proof that this Markov chain is also mixing in the more interesting case of /spl lambda/>1, known in the statistical physics community as adsorbing staircase walks. The main new ingredient is a decomposition technique which allows us to analyze the Markov chain in pieces, applying different arguments to analyze each piece.
[Gibbs measure, state space, Lattices, Length measurement, Mathematics, Markov chain, Tree graphs, Space technology, theorem proving, local Markov chain, lambda calculus, adsorbing staircase walks, Probability, Educational institutions, lattice paths, statistical physics community, decomposition technique, State-space methods, Markov chain decomposition method, /spl lambda//sup /spl kappa//, Physics, diagonal steps, Markov processes, first proof, Sampling methods]
Clustering data streams
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We study clustering under the data stream model of computation where: given a sequence of points, the objective is to maintain a consistently good clustering of the sequence observed so far, using a small amount of memory and time. The data stream model is relevant to new classes of applications involving massive data sets, such as Web click stream analysis and multimedia data analysis. We give constant-factor approximation algorithms for the k-median problem in the data stream model of computation in a single pass. We also show negative results implying that our algorithms cannot be improved in a certain sense.
[Data analysis, k-median problem, data analysis, Web click stream analysis, Computational modeling, Laboratories, constant-factor approximation algorithms, Application software, deterministic algorithms, point sequence, Computer science, data stream clustering, pattern clustering, very large databases, Clustering algorithms, Web pages, Streaming media, Telephony, multimedia data analysis, Approximation algorithms, massive data sets, computational complexity, data stream model]
Optimization problems in congestion control
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
One of the crucial elements in the Internet's success is its ability to adequately control congestion. The paper defines and solves several optimization problems related to Internet congestion control, as a step toward understanding the virtues of the TCP congestion control algorithm currently used and comparing it with alternative algorithms. We focus on regulating the rate of a single unicast flow when the bandwidth available to it is unknown and may change over time. We determine near-optimal policies when the available bandwidth is unchanging, and near-optimal competitive policies when the available bandwidth is changing in a restricted manner under the control of an adversary.
[Algorithm design and analysis, Protocols, telecommunication congestion control, Turning, available bandwidth, Delay, near-optimal policies, Computer science, Degradation, Internet congestion control, optimisation, Unicast, Bandwidth, near-optimal competitive policies, single unicast flow, Internet, TCP congestion control algorithm, Probes, optimization problems]
An improved quantum Fourier transform algorithm and applications
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We give an algorithm for approximating the quantum Fourier transform over an arbitrary Z/sub p/ which requires only O(n log n) steps where n=log p to achieve an approximation to within an arbitrary inverse polynomial in n. This improves the method of A.Y. Kitaev (1995) which requires time quadratic in n. This algorithm also leads to a general and efficient Fourier sampling technique which improves upon the quantum Fourier sampling lemma of L. Hales and S. Hallgren (1997). As an application of this technique, we give a quantum algorithm which finds the period of an arbitrary periodic function, i.e. a function which may be many-to-one within each period. We show that this algorithm is efficient (polylogarithmic in the period of the function) for a large class of periodic functions. Moreover, using standard quantum lower-bound techniques, we show that this characterization is right. That is, this is the maximal class of periodic functions with an efficient quantum period-finding algorithm.
[quantum period-finding algorithm, Fourier transforms, Multidimensional systems, polynomials, standard quantum lower-bound techniques, periodic functions, maximal class, Application software, Machinery, arbitrary inverse polynomial, Fourier sampling technique, quantum Fourier sampling lemma, arbitrary periodic function, Computer science, Quantum computing, quantum computing, Sampling methods, Polynomials, Eigenvalues and eigenfunctions, improved quantum Fourier transform algorithm, Logic, computational complexity]
Fully dynamic transitive closure: breaking through the O(n/sup 2/) barrier
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We introduce a general framework for casting fully dynamic transitive closure into the problem of reevaluating polynomials over matrices. With this technique, we improve the best known bounds for fully dynamic transitive closure, in particular we devise a deterministic algorithm for general directed graphs that achieves O(n/sup 2/) amortized time for updates, while preserving unit worst-case cost for queries. In case of deletions only, our algorithm performs updates faster in O(n) amortized time. Our matrix-based approach yields an algorithm for directed acyclic graphs which breaks through the O(n/sup 2/) barrier on the single-operation complexity of fully dynamic transitive closure. We can answer queries in O(n/sup /spl epsiv//) time and perform updates in O(n/sup /spl omega/(1,/spl epsiv/,1)-/spl epsiv//+n/sup 1+/spl epsiv//) time, for any /spl epsiv//spl isin/[0,1], where /spl omega/(1,/spl epsiv/,1) is the exponent of the multiplication of an n/spl times/n/sup /spl epsiv// matrix by an n/sup /spl epsiv///spl times/n matrix. The current best bounds on /spl omega/(1,/spl epsiv/,1) imply an O(n/sup 0.575/) query time and an O(n/sup 1.575/) update time. Our subquadratic algorithm is randomized, and has one-side error.
[Costs, fully dynamic transitive closure, Heuristic algorithms, polynomials, Data engineering, randomized algorithm, Remuneration, deterministic algorithm, deterministic algorithms, queries, randomised algorithms, Uniform resource locators, matrix multiplication, single-operation complexity, Councils, amortized time, directed graphs, subquadratic algorithm, Polynomials, directed acyclic graphs, Contracts, computational complexity, unit worst-case cost]
Succinct quantum proofs for properties of finite groups
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
The article considers a quantum computational variant of nondeterminism based on the notion of a quantum proof, which is a quantum state that plays a role similar to a certificate in an NP-type proof. Specifically, we consider quantum proofs for properties of black-box groups, which are finite groups whose elements are encoded as strings of a given length and whose group operations are performed by a group oracle. We prove that for an arbitrary group oracle, there exist succinct (polynomial-length) quantum proofs for the Group Non-Membership problem that can be checked with small error in polynomial time on a quantum computer. Classically, this is impossible; it is proved that there exists a group oracle, relative to which this problem does not have succinct proofs that can be checked classically with bounded error in polynomial time (i.e., the problem is not in MA relative to the group oracle constructed). By considering a certain subproblem of the Group Non-Membership problem, we obtain a simple proof that there exists an oracle relative to which BQP is not contained in MA. Finally, we show that quantum proofs for non-membership and classical proofs for various other group properties can be combined to yield succinct quantum proofs for other group properties not having succinct proofs in the classical setting, such as verifying that a number divides the order of a group and verifying that a group is not a simple group.
[finite group properties, certificate, simple proof, succinct quantum proofs, Complexity theory, Quantum computing, Turing machines, strings, Group Non-Membership problem, quantum proof, quantum state, Polynomials, polynomial time, theorem proving, classical proofs, nondeterminism, NP-type proof, Context, quantum computer, bounded error, group oracle, group operations, Computer science, group theory, quantum computational variant, quantum computing, finite groups, Computer errors, group properties, black-box groups, computational complexity, quantum proofs]
Nearly optimal expected-case planar point location
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We consider the planar point location problem from the perspective of expected search time. We are given a planar polygonal subdivision S and for each polygon of the subdivision the probability that a query point lies within this polygon. The goal is to compute a search structure to determine which cell of the subdivision contains a given query point, so as to minimize the expected search time. This is a generalization of the classical problem of computing an optimal binary search tree for one-dimensional keys. In the one-dimensional case it has long been known that the entropy H of the distribution is the dominant term in the lower bound on the expected-case search time, and further there exist search trees achieving expected search times of at most H+2. Prior to this work, there has been no known structure for planar point location with an expected search time better than 2H, and this result required strong assumptions on the nature of the query point distribution. Here we present a data structure whose expected search time is nearly equal to the entropy lower bound, namely H+o(H). The result holds for any polygonal subdivision in which the number of sides of each of the polygonal cells is bounded, and there are no assumptions on the query distribution within each cell. We extend these results to subdivisions with convex cells, assuming a uniform query distribution within each cell.
[nearly optimal expected-case planar point location, probability, trees (mathematics), planar point location, data structure, Binary search trees, computational geometry, Data structures, Educational institutions, Entropy, polygonal cells, convex cells, Computer science, Computational geometry, subdivision, optimal binary search tree, expected search time, search structure, planar polygonal subdivision, search problems, polygonal subdivision]
Combinatorial feature selection problems
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Motivated by frequently recurring themes in information retrieval and related disciplines, we define a genre of problems called combinatorial feature selection problems. Given a set S of multidimensional objects, the goal is to select a subset K of relevant dimensions (or features) such that some desired property /spl Pi/ holds for the set S restricted to K. Depending on /spl Pi/, the goal could be to either maximize or minimize the size of the subset K. Several well-studied feature selection problems can be cast in this form. We study the problems in this class derived from several natural and interesting properties /spl Pi/, including variants of the classical p-center problem as well as problems akin to determining the VC-dimension of a set system. Our main contribution is a theoretical framework for studying combinatorial feature selection, providing (in most cases essentially tight) approximation algorithms and hardness results for several instances of these problems.
[combinatorial mathematics, Data engineering, set theory, approximation algorithms, Data mining, optimisation, Vapnik-Chervonenkis dimension, feature extraction, Bonding, Multidimensional systems, subset size maximization, p-center problem, information retrieval, Data processing, VC-dimension, hardness results, combinatorial feature selection problems, subset size minimization, Computer science, Bridges, Approximation algorithms, US Department of Defense, Risk management, multidimensional objects, computational complexity]
Linear waste of best fit bin packing on skewed distributions
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We prove that best-fit bin packing has linear waste on the discrete distribution U{j,k} (where items are drawn uniformly from the set {1/k, 2/k, ..., j/k}) for sufficiently large k when j=/spl alpha/k and 0.66/spl les//spl alpha/<2/3. Our results extend to continuous skewed distributions, where items are drawn uniformly on [0,a], for 0.66/spl les/a<2/3. This implies that the expected asymptotic performance ratio of best-fit bin packing is strictly greater than 1 for these distributions.
[Algorithm design and analysis, Engineering profession, linear waste, probability, H infinity control, asymptotic performance ratio, bin packing, continuous skewed distributions, discrete distribution, Approximation algorithms, performance index, Performance analysis, Random variables, best-fit bin packing]
Building Steiner trees with incomplete global knowledge
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
A networking problem of present-day interest is that of distributing a single data item to multiple clients while minimizing network usage. Steiner tree algorithms are a natural solution method, but only when the set of clients requesting the data is known. We study what can be done without this global knowledge, when a given vertex knows only the probability that any other client wishes to be connected, and must simply specify a fixed path to the data to be used in case it is requested. Our problem is an example of a class of network design problems with concave cost functions (which arise when the design problem exhibits economies of scale). In order to solve our problem, we introduce a new version of the facility location problem: one in which every open facility is required to have some minimum amount of demand assigned to it. We present a simple bicriterion approximation for this problem, one which is loose in both assignment cost and minimum demand, but within a constant factor of the optimum for both. This suffices for our application. We leave open the question of finding an algorithm that produces a truly feasible approximate solution.
[Algorithm design and analysis, Steiner trees, data item distribution, vertex, Laboratories, uncertainty handling, facility location, bicriterion approximation, Degradation, open facilities, network synthesis, Bandwidth, Cost function, assignment cost, economies of scale, client-server systems, approximation theory, trees (mathematics), incomplete global knowledge, data-requesting clients, combinational switching, connection probability, network design problems, concave cost functions, facility location problem, Computer science, minimum demand, fixed data path specification, Economies of scale, minimisation, Joining processes, network usage minimization]
A polylogarithmic approximation of the minimum bisection
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
A bisection of a graph with n vertices is a partition of its vertices into two sets, each of size n/2. The bisection cost is the number of edges connecting the two sets. Finding the bisection of minimum cost is NP-hard. We present an algorithm that finds a bisection whose cost is within ratio of O(log/sup 2/ n) from the optimal. For graphs excluding any fixed graph as a minor (e.g. planar graphs) we obtain an improved approximation ratio of O(log n). The previously known approximation ratio for bisection was roughly /spl radic/n.
[minimum bisection, approximation ratio, complexity, Minimization methods, Particle separators, bisection cost, graph theory, vertices, computational geometry, edges, Mathematics, vertex partitioning, Partitioning algorithms, graph, Computer science, Cost function, Approximation algorithms, Polynomials, Dynamic programming, Joining processes, computational complexity, polylogarithmic approximation]
Using upper confidence bounds for online learning
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We show how a standard tool from statistics, namely confidence bounds, can be used to elegantly deal with situations which exhibit an exploitation/exploration trade-off. Our technique for designing and analyzing algorithms for such situations is very general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We consider two models with such an exploitation/exploration trade-off. For the adversarial bandit problem our new algorithm suffers only O/spl tilde/(T/sup 1/2/) regret over T trials which improves significantly over the previously best O/spl tilde/(T/sup 2/3/) regret. We also extend our results for the adversarial bandit problem to shifting bandits. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O/spl tilde/(T/sup 3/4/) to O/spl tilde/(T/sup 1/2/).
[Algorithm design and analysis, uncertain information, Uncertainty, Statistical analysis, exploitation decision, adversarial bandit problem, random processes, uncertainty handling, Random processes, linear value functions, Information analysis, Learning, Computer science, random process, exploration decision, Random variables, learning (artificial intelligence), statistical analysis, upper confidence bounds, associative reinforcement learning, online learning, statistics]
The cover time, the blanket time, and the Matthews bound
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We prove upper and lower bounds and give an approximation algorithm for the cover time of the random walk on a graph. We introduce a parameter M motivated by the well-known Matthews bounds (P. Matthews, 1988) on the cover time, C, and prove that M/2<C= O(M(lnlnn)/sup 2/). We give a deterministic-polynomial time algorithm to approximate M within a factor of 2; this then approximates C within a factor of O((lnlnn)/sup 2/), improving the previous bound O(lnn) due to Matthews. The blanket time B was introduced by P. Winkler and D. Zuckerman (1996): it is the expectation of the first time when all vertices are visited within a constant factor of the number of times suggested by the stationary distribution. Obviously C/spl les/B. Winkler and Zuckerman conjectured B=O(C) and proved B=O(Clnn). Our bounds above are also valid for the blanket time, and so it follows that B=O(C(lnlnn)/sup 2/).
[approximation theory, cover time, graph theory, deterministic-polynomial time algorithm, Mathematics, deterministic algorithms, graph, random walk, Upper bound, stationary distribution, Matthews bound, approximation algorithm, Approximation algorithms, Polynomials, theorem proving, blanket time, Matthews bounds, computational complexity]
Concurrent oblivious transfer
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We consider the problem of designing an efficient oblivious transfer (OT) protocol that is provably secure in a concurrent setting, i.e., where many OT sessions may be running concurrently with their messages interleaved arbitrarily. Known OT protocols use zero-knowledge proofs, and no concurrent zero-knowledge proofs are known that use less than a poly-logarithmic number of rounds (at least without requiring a pre-processing phase, a public random string, an auxiliary string, timing constraints, or pre-distributed public keys). We introduce a model for proving security of concurrent OT protocols, and present a protocol that is proven secure in this model based on the decisional Diffie-Hellman problem. The protocol is efficient, requiring only a slightly non-constant number of rounds.
[timing, cryptography, poly-logarithmic number, Ice, public random string, Security, Helium, Cryptographic protocols, concurrent oblivious transfer, protocol, auxiliary string, timing constraints, Public key, Authentication, Public key cryptography, Polynomials, Timing, Internet, decisional Diffie-Hellman problem, protocols, concurrent setting, zero-knowledge proofs]
Randomized rumor spreading
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Investigates the class of epidemic algorithms that are commonly used for the lazy transmission of updates to distributed copies of a database. These algorithms use a simple randomized communication mechanism to ensure robustness. Suppose n players communicate in parallel rounds in each of which every player calls a randomly selected communication partner. In every round, players can generate rumors (updates) that are to be distributed among all players. Whenever communication is established between two players, each one must decide which of the rumors to transmit. The major problem is that players might not know which rumors their partners have already received. For example, a standard algorithm forwarding each rumor form the calling to the called players for /spl Theta/(ln n) rounds needs to transmit the rumor /spl Theta/(n ln n) times in order to ensure that every player finally receives the rumor with high probability. We investigate whether such a large communication overhead is inherent to epidemic algorithms. On the positive side, we show that the communication overhead can be reduced significantly. We give an algorithm using only O(n ln ln n) transmissions and O(ln n) rounds. In addition, we prove the robustness of this algorithm. On the negative side, we show that any address-oblivious algorithm needs to send /spl Omega/(n ln ln n) messages for each rumor, regardless of the number of rounds. Furthermore, we give a general lower bound showing that time and communication optimality cannot be achieved simultaneously using random phone calls, i.e. every algorithm that distributes a rumor in O(ln n) rounds needs /spl omega/(n) transmissions.
[address-oblivious algorithm, randomized rumor spreading, random telephone calls, communication complexity, communication overhead, randomized communication mechanism, message transmissions, Distributed databases, parallel rounds, Robustness, information theory, epidemic algorithms, replicated databases, lazy update transmission, robustness, Computer crashes, lower bound, randomised algorithms, database theory, Computer science, randomly selected communication partner, distributed database copies, communication optimality, time optimality, commmunication complexity]
Fairness measures for resource allocation
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
In many optimization problems, one seeks to allocate a limited set of resources to a set of individuals with demands. Thus, such allocations can naturally be viewed as vectors, with one coordinate representing each individual. Motivated by work in network routing and bandwidth assignment, we consider the problem of producing solutions that simultaneously approximate all feasible allocations in a coordinate-wise sense. This is a very strong type of "global" approximation guarantee, and we explore its consequences in a range of discrete optimization problems, including facility location, scheduling, and bandwidth assignment in networks. A fundamental issue-one not encountered in the traditional design of approximation algorithms-is that good approximations in this global sense need not exist for every problem instance; there is no a priori reason why there should be an allocation that simultaneously approximates all others. As a result, the existential questions concerning such good allocations lead to a new perspective on a number of basic problems in resource allocation, and on the structure of their feasible solutions.
[Algorithm design and analysis, Career development, discrete optimization problems, network routing, Routing, facility location, Computer science, bandwidth allocation, global approximation guarantee, optimisation, vectors, resource allocation, Aggregates, bandwidth assignment, telecommunication network routing, Bandwidth, Channel allocation, scheduling, Resource management, fairness measures, optimization problems]
Opportunistic data structures with applications
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We address the issue of compressing and indexing data. We devise a data structure whose space occupancy is a function of the entropy of the underlying data set. We call the data structure opportunistic since its space occupancy is decreased when the input is compressible and this space reduction is achieved at no significant slowdown in the query performance. More precisely, its space occupancy is optimal in an information-content sense because text T[1,u] is stored using O(H/sub k/(T))+o(1) bits per input symbol in the worst case, where H/sub k/(T) is the kth order empirical entropy of T (the bound holds for any fixed k). Given an arbitrary string P[1,p], the opportunistic data structure allows to search for the occurrences of P in T in O(p+occlog/sup /spl epsiv//u) time (for any fixed /spl epsiv/>0). If data are uncompressible we achieve the best space bound currently known (Grossi and Vitter, 2000); on compressible data our solution improves the succinct suffix array of (Grossi and Vitter, 2000) and the classical suffix tree and suffix array data structures either in space or in query time or both. We also study our opportunistic data structure in a dynamic setting and devise a variant achieving effective search and update time bounds. Finally, we show how to plug our opportunistic data structure into the Glimpse tool (Manber and Wu, 1994). The result is an indexing tool which achieves sublinear space and sublinear query time complexity.
[sublinear query time complexity, Costs, query performance, Data engineering, Entropy, succinct suffix array, Postal services, Fault tolerance, search, database indexing, entropy, suffix tree data structures, Glimpse tool, data structures, Plugs, Tree data structures, data compression, suffix array data structures, data set, Data structures, opportunistic data structures, database theory, Computer science, data indexing, sublinear space complexity, Indexing, computational complexity]
On levels in arrangements of curves
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Analyzing the worst-case complexity of the k-level in a planar arrangement of n curves is a fundamental problem in combinatorial geometry. We give the first subquadratic upper bound (roughly O(nk/sup 1-2/3/*)) for curves that are graphs of polynomial functions of an arbitrary fixed degree s. Previously, nontrivial results were known only for the case s=1 and s=2. We also improve the earlier bound for pseudo-parabolas (curves that pairwise intersect at most twice) to O(nk/sup 7/9/log/sup 2/3/ k). The proofs are simple and rely on a theorem of Tamaki and Tokuyama on cutting pseudo-parabolas into pseudo-segments, as well as a new observation for cutting pseudo-segments into pieces that can be extended to pseudo-lines. We mention applications to parametric and kinetic minimum spanning trees.
[Algorithm design and analysis, pseudo-parabolas, polynomials, computational geometry, pseudo-segments, worst-case complexity, History, Design optimization, Computer science, Computational geometry, Upper bound, combinatorial geometry, subquadratic upper bound, polynomial functions, kinetic minimum spanning trees, Polynomials, Kinetic theory, Books, Motion analysis, planar arrangement, computational complexity]
The common fragment of CTL and LTL
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
It is well-known that CTL (computation tree logic) and LTL (linear time logic) have incomparable expressive power. In this paper, we give an inductive definition of those ACTL (Action-based CTL) formulas that can be expressed in LTL. In addition, we obtain a procedure to decide whether an ACTL formula lies in LTL, and show that this problem is PSPACE-complete. By omitting path quantifiers, we get an inductive definition of the LTL formulas that are expressible in ACTL. We can show that the fragment defined by our logic represents exactly those LTL formulas the negation of which can be represented by a 1-weak Bu/spl uml/chi automaton and that, for this fragment, the representing automaton can be chosen to be of size linear in the size of the formula.
[negation, finite automata, LTL, temporal logic, linear time logic, expressive power, Presses, Boolean functions, decidability, action-based computation tree logic, Logic, inductive definition, ACTL formulas, 1-weak Buchi automaton, automaton size, trees (mathematics), path quantifiers, Data structures, formula size, Automata, Open systems, PSPACE-complete problem, CTL, computational complexity, common fragment]
Stable distributions, pseudorandom generators, embeddings and data stream computation
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: we show how to maintain (using only O(log n//spl epsiv//sup 2/) words of storage) a sketch C(p) of a point p/spl isin/l/sub 1//sup n/ under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate |p-q|/sub 1/ up to a factor of (1+/spl epsiv/) with large probability. We obtain another sketch function C' which maps l/sub 1//sup n/ into a normed space l/sub 1//sup m/ (as opposed to C), such that m=m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l/sub 1/ norm we give an explicit embedding of l/sub 2//sup n/ into l/sub l//sup nO(log n)/ with distortion (1+1/n/sup /spl theta/(1)/) and a non-constructive embedding of l/sub 2//sup n/ into l/sub 1//sup O(n)/ with distortion (1+/spl epsiv/) such that the embedding can be represented using only O(n log/sup 2/ n) bits (as opposed to at least n/sup 2/ used by earlier methods).
[stable distributions, embeddings, Embedded computing, pseudorandom generators, probability, Distributed power generation, bounded space, random number generation, Distributed computing, Statistics, data stream computation, Counting circuits, dimensionality reduction, Tail, Power generation]
Optimal myopic algorithms for random 3-SAT
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Let F/sub 3/(n,m) be a random 3-SAT formula formed by selecting uniformly, independently and with replacement, m clauses among all 8(/sup n/C/sub 3/) possible 3-clauses over n variables. It has been conjectured that there exists a constant r/sub 3/ such that, for any /spl epsiv/>0, F/sub 3/[n,(r/sub 3/-/spl epsiv/)n] is almost surely satisfiable, but F/sub 3/[n,(r/sub 3/+/spl epsiv/)n] is almost surely unsatisfiable. The best lower bounds for the potential value of r/sub 3/ have come form analyzing rather simple extensions of unit-clause propagation. It was shown by D. Achlioptas (2000) that all these extensions can be cast in a common framework and analyzed in a uniform manner by employing differential equations. We determine optimal algorithms that are expressible in that framework, establishing r/sub 3/>3.26. We extend the analysis via differential equations, and make extensive use of a new optimization problem that we call the "max-density multiple-choice knapsack" problem. The structure of optimal knapsack solutions elegantly characterizes the choices made by an optimal algorithm.
[Chaos, optimization problem, unit-clause propagation extensions, expressible algorithms, computability, Mathematics, optimal knapsack solution structure, Physics, lower bounds, differential equations, randomised algorithms, Computer science, Upper bound, optimisation, 3-clauses, satisfiability, max-density multiple-choice knapsack problem, Differential equations, random 3-SAT formula, optimal myopic algorithms]
The relationship between public key encryption and oblivious transfer
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
In this paper we study the relationships among some of the most fundamental primitives and protocols in cryptography: public-key encryption (i.e. trapdoor predicates), oblivious transfer (which is equivalent to general secure multi-party computation), key agreement and trapdoor permutations. Our main results show that public-key encryption and oblivious transfer are incomparable under black-box reductions. These separations are tightly matched by our positive results where a restricted (strong) version of one primitive does imply the other primitive. We also show separations between oblivious transfer and key agreement. Finally, we conclude that neither oblivious transfer nor trapdoor predicates imply trapdoor permutations. Our techniques for showing negative results follow the oracle separations of R. Impagliazzo and S. Rudich (1989).
[key agreement, Technological innovation, secure multi-party computation, trapdoor predicates, public key encryption, cryptography, Computational Intelligence Society, Complexity theory, Security, black-box reductions, Cryptographic protocols, Graphics, primitives, public key cryptography, Public key, trapdoor permutations, oblivious transfer, Public key cryptography, protocols, Digital signatures, public-key encryption]
On the approximability of trade-offs and optimal access of Web sources
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We study problems in multiobjective optimization, in which solutions to a combinatorial optimization problem are evaluated with respect to several cost criteria, and we are interested in the trade-off between these objectives (the so-called Pareto curve). We point out that, under very general conditions, there is a polynomially succinct curve that /spl epsiv/-approximates the Pareto curve, for any /spl epsiv/>0. We give a necessary and sufficient condition under which this approximate Pareto curve can be constructed in time polynomial in the size of the instance and 1//spl epsiv/. In the case of multiple linear objectives, we distinguish between two cases: when the underlying feasible region is convex, then we show that approximating the multi-objective problem is equivalent to approximating the single-objective problem. If however the feasible region is discrete, then we point out that the question reduces to an old and recurrent one: how does the complexity of a combinatorial optimization problem change when its feasible region is intersected with a hyperplane with small coefficients; we report some interesting new findings in this domain. Finally, we apply these concepts and techniques to formulate and solve approximately a cost-time-quality trade-off for optimizing access to the World-Wide Web, in a model first studied by Etzioni et al. (1996) (which was actually the original motivation for this work).
[complexity, Operations research, combinatorial mathematics, combinatorial optimization problem, Pareto optimization, Web sources, Pareto curve, cost criteria, multiobjective optimization, Sufficient conditions, optimisation, Cost function, Polynomials, polynomially succinct curve, hyperplane, multiple linear objectives, cost-time-quality trade-off, information resources, Delay effects, trade-offs, information retrieval, optimal access, Computer science, World-Wide Web, Microeconomics, computational complexity, approximability]
Cache-oblivious B-trees
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
We present dynamic search-tree data structures that perform well in the setting of a hierarchical memory (including various levels of cache, disk, etc.), but do not depend on the number of memory levels, the block sizes and number of blocks at each level, or the relative speeds of memory access. In particular between any pair of levels in the memory hierarchy, where transfers between the levels are done in blocks of size B, our data structures match the optimal search bound of /spl Theta/(log/sub B/ N) memory transfers. This bound is also achieved by the classic B-tree data structure, but only when the block size B is known, which in practice requires careful tuning on each machine platform. One of our data structures supports insertions and deletions in /spl Theta/(log/sub B/ N) amortized memory transfers, which matches the B-tree's worst-case bounds. We augment this structure to support scans optimally in /spl Theta/(N/B) memory transfers. In this second data structure insertions and deletions require /spl Theta/(log/sub B/ N+log/sup 2/N/B) amortized memory transfers. Thus, we match the performance of the B-tree for B=/spl Omega/(log N log log N).
[Algorithm design and analysis, Costs, dynamic search-tree data structures, insertions, Laboratories, deletions, worst-case bounds, Data structures, cache-oblivious B-trees, memory hierarchy, cache storage, amortized memory transfers, Registers, tree searching, Computer science, Network-on-a-chip, Clustering algorithms, hierarchical memory, optimal search bound, Computer networks, Central Processing Unit, tree data structures, computational complexity]
Detecting a network failure
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Measuring the properties of a large, unstructured network can be difficult: one may not have full knowledge of the network topology, and detailed global measurements may be infeasible. A valuable approach to such problems is to take measurements from selected locations within the network and then aggregate them to infer large-scale properties. One sees this notion applied in settings that range from Internet topology discovery tools to remote software agents that estimate the download times of popular Web pages. Some of the most basic questions about this type of approach, however, are largely unresolved at an analytical level. How reliable are the results? How much does the choice of measurement locations affect the aggregate information one infers about the network? We describe algorithms that yield provable guarantees for a particular problem of this type: detecting a network failure. Suppose we want to detect events of the following form: an adversary destroys up to k nodes or edges, after which two subsets of the nodes, each at least an /spl epsi/ fraction of the network, are disconnected from one another. We call such an event an (/spl epsi/,k) partition. One method for detecting such events would be to place "agents" at a set D of nodes, and record a fault whenever two of them become separated from each other. To be a good detection set, D should become disconnected whenever there is an (/spl epsi/,k)-partition; in this way, it "witnesses" all such events. We show that every graph has a detection set of size polynomial in k and /spl epsi//sup -1/, and independent of the size of the graph itself. Moreover, random sampling provides an effective way to construct such a set. Our analysis establishes a connection between graph separators and the notion of VC-dimension, using techniques based on matchings and disjoint paths.
[global measurements, Event detection, Internet topology discovery tools, random sampling, VC-dimension, software agents, Network topology, Aggregates, Fault detection, network failure detection, matchings, Web pages, Sampling methods, Software agents, Polynomials, disjoint paths, Large-scale systems, Internet, computer network reliability, remote software agents]
On the existence of booster types
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
A data type's consensus number measures its power in asynchronous concurrent models of computation. We characterize the circumstances under which types of high consensus number can be constructed from types with lower consensus numbers, a process called boosting. In settings where boosting is impossible, we can reason about the synchronization power of objects in isolation. We give a new and simple topological condition, called /spl kappa/-solo-connectivity sufficient to ensure that one-shot types cannot be boosted to consensus number /spl kappa/. The booster type need not be one-shot; it can be arbitrary. We also show that, for /spl kappa/>2, any type that is not /spl kappa/-solo-connected can be boosted to consensus number /spl kappa/. For types that can be boosted, we establish an upper bound on the amount the consensus number can be increased. For finite types, these properties and bounds are computable. For deterministic one-shot types, the /spl kappa/-solo-connectivity property also exactly characterizes the types that have consensus number less than /spl kappa/.
[Protocols, Computational modeling, deterministic one-shot types, booster types, Boosting, concurrency theory, consensus number, synchronization power, upper bound, type theory, Registers, Delay, Multiprocessing systems, synchronisation, data type, Concurrent computing, Computer science, solo-connectivity, Power measurement, Upper bound, distributed algorithms, topological condition, asynchronous concurrent models of computation]
Using expander graphs to find vertex connectivity
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
The (vertex) connectivity /spl kappa/ of a graph is the smallest number of vertices whose deletion separates the graph or makes it trivial. We present the fastest known algorithm for finding /spl kappa/. For a digraph with n vertices, m edges and connectivity /spl kappa/ the time bound is O((n+min(/spl kappa//sup 5/2/,/spl kappa/n/sup 3/4/))m). This improves the previous best bound of O((n+min(/spl kappa//sup 3/,/spl kappa/n))m). For an undirected graph both of these bounds hold with m replaced /spl kappa/n. Our approach uses expander graphs to exploit nesting properties of certain separation triples.
[nesting properties, complexity, Terminology, Particle separators, graph theory, separation triples, time bound, Graph theory, Computer science, vertex connectivity, digraph, undirected graph, expander graphs, computational complexity]
Testing of clustering
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
A set X of points in /spl Rfr//sup d/ is (k,b)-clusterable if X can be partitioned into k subsets (clusters) so that the diameter (alternatively, the radius) of each cluster is at most b. We present algorithms that by sampling from a set X, distinguish between the case that X is (k,b)-clusterable and the case that X is /spl epsiv/-far from being (k,b')-clusterable for any given 0</spl epsiv//spl les/1 and for b'/spl ges/b. In /spl epsiv/-far from being (k,b')-clusterable we mean that more than /spl epsiv/.|X| points should be removed from X so that it becomes (k,b')-clusterable. We give algorithms for a variety of cost measures that use a sample of size independent of |X|, and polynomial in k and 1//spl epsiv/. Our algorithms can also be used to find approximately good clusterings. Namely, these are clusterings of all but an /spl epsiv/-fraction of the points in X that have optimal (or close to optimal) cost. The benefit of our algorithms is that they construct an implicit representation of such clusterings in time independent of |X|. That is, without actually having to partition all points in X, the implicit representation can be used to answer queries concerning the cluster any given point belongs to.
[Performance evaluation, cost measures, clustering testing, sampling, Educational institutions, Size measurement, Mathematics, Partitioning algorithms, optimal cost, lower bounds, USA Councils, pattern clustering, Clustering algorithms, Sampling methods, Cost function, statistical analysis, Testing, computational complexity]
Testing of function that have small width branching programs
Proceedings 41st Annual Symposium on Foundations of Computer Science
None
2000
Combinatorial property testing, initiated formally by (Goldreich et al., 1996) and inspired by (Rubinfeld and Sudan, 1996), deals with the following relaxation of decision problems: given a fixed property and an input x, one wants to decide whether x has the property or is being far from having the property. The main result here is that if G={g:{0,1}/sup n//spl rarr/{0,1}} is a family of Boolean functions that have read-once branching programs of width w, then for every n and /spl epsiv/>0 there is a randomized algorithm that always accepts every x/spl isin/{0,1}/sup n/ if g(x)=1, and rejects it with height probability if at least /spl epsiv/n bits of x should be modified in order for it to be in g/sup -1/(1). The algorithm queries (2w//spl epsiv/)/sup 0(w)/ many queries. In particular, for constant /spl epsiv/ and w, the query complexity is 0(1). This generalizes the results of (Alon et al., 1999) asserting that regular languages are efficiently (/spl epsiv/,O(1))-testable.
[Hamming distance, regular languages, probability, Binary decision diagrams, query complexity, combinatorial property testing, randomized algorithm, randomised algorithms, Computer science, Boolean functions, directed graphs, small width branching programs, decision problems, read-once branching programs, Testing, computational complexity]
Semi-direct product in groups and zig-zag product in graphs: connections and applications
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We consider the standard semi-direct product A/spl times/B of finite groups A, B. We show that with certain choices of generators for these three groups, the Cayley graph of A/spl times/B is (essentially) the zigzag product of the Cayley graphs of A and B. Thus, using the results of O. Reingold et al. (2000), the new Cayley graph is an expander if and only if its two components are. We develop some general ways of using this construction to obtain large constant-degree expanding Cayley graphs from small ones. A. Lubotzky and B. Weiss (1993) asked whether expansion is a group property; namely, is being an expander for (a Cayley graph of) a group G depend solely on G and not on the choice of generators. We use the above construction to answer the question in the negative, by showing an infinite family of groups A/sub i//spl times/B/sub i/ which are expanders with one choice of a (constant-size) set of generators and are not with another such choice. It is interesting to note that this problem is still open, though for "natural" families of groups like the symmetric groups S/sub n/ or the simple groups PSL(2, p).
[constant-size generator set, graph theory, large constant-degree expanding Cayley graphs, group property, infinite family, Graph theory, Mathematics, set theory, Application software, expansion, Computer science, Geometry, Bridges, group theory, Algebra, semi-direct product, finite groups, symmetric groups, simple groups, standard semi-direct product, Error correction, Labeling, zig-zag product, Power generation]
On the impossibility of basing trapdoor functions on trapdoor predicates
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We prove that, somewhat surprisingly, there is no black-box reduction of (poly-to-one) trapdoor functions to trapdoor predicates (equivalently, to public-key encryption schemes). Our proof follows the methodology that was introduced by R. Impagliazzo and S. Rudich (1989), although we use a new, weaker model of separation.
[poly-to-one trapdoor functions, trapdoor predicates, cryptography, black-box reduction, Computer science, public-key encryption schemes, Public key, Public key cryptography, Polynomials, theorem proving, proof, computational complexity, weaker separation model]
Resettably-sound zero-knowledge and its applications
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Resettably-sound proofs and arguments maintain soundness even when the prover can reset the verifier to use the same random coins in repeated executions of the protocol. We show that resettably-sound zero-knowledge arguments for NP exist if collision-free hash functions exist. In contrast, resettably-sound zero-knowledge proofs are possible only for languages in P/poly. We present two applications of resettably-sound zero-knowledge arguments. First, we construct resettable zero-knowledge arguments of knowledge for NP, using a natural relaxation of the definition of arguments (and proofs) of knowledge. We note that, under the standard definition of proof of knowledge, it is impossible to obtain resettable zero-knowledge arguments of knowledge for languages outside BPP. Second, we construct a constant-round resettable zero-knowledge argument for NP in the public-key model, under the assumption that collision-free hash functions exist. This improves upon the sub-exponential hardness assumption required by previous constructions. We emphasize that our results use non-black-box zero-knowledge simulations. Indeed, we show that some of the results are impossible to achieve using black-box simulations. In particular, only languages in BPP have resettably-sound arguments that are zero-knowledge with respect to black-box simulation.
[Smart cards, NP, BPP, P/poly languages, public-key model, Security, non-black-box zero-knowledge simulations, sub-exponential hardness assumption, Polynomials, constant-round resettable zero-knowledge argument, Cryptography, protocols, random coins, natural relaxation, standard definition, resettably-sound proofs, collision-free hash functions, cryptography, Application software, Cryptographic protocols, Computer science, resettably-sound zero-knowledge arguments, Public key, black-box simulations, computational complexity]
Query efficient PCPs with perfect completeness
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
For every integer k>1, we present a PCP characterization of NP where the verifier uses logarithmic randomness, queries 4k+k/sup 2/ bits in the proof, accepts a correct proof with probability 1 (i.e. it is has perfect completeness) and accepts any supposed proof of a false statement with a certain maximum probability. In particular, the verifier achieves optimal amortized query complexity of 1+/spl delta/ for arbitrarily small constant /spl delta/>0. Such a characterization was already proved by A. Samorodnitsky and L. Trevisan (2000), but their verifier loses perfect completeness and their proof makes an essential use of this feature. By using an adaptive verifier, we can decrease the number of query bits to 2k+k/sup 2/, the same number obtained by Samorodnitsky and Trevisan. Finally, we extend some of the results to larger domains.
[System testing, optimal amortized query complexity, Protocols, NP, query bits, probability, query efficient PCPs, probabilistic checking of proofs, Equations, perfect completeness, correct proof, Computer science, query processing, adaptive verifier, formal verification, logarithmic randomness, verifier, arbitrarily small constant, Polynomials, theorem proving, false statement, Time factors, PCP characterization, computational complexity]
How to go beyond the black-box simulation barrier
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The simulation paradigm is central to cryptography. A simulator is an algorithm that tries to simulate the interaction of the adversary with an honest party, without knowing the private input of this honest party. Almost all known simulators use the adversary's algorithm as a black-box. We present the first constructions of non-black-box simulators. Using these new non-black-box techniques, we obtain several results that were previously proven to be impossible to obtain using black-box simulators. Specifically, assuming the existence of collision resistent hash functions, we construct a new zero-knowledge argument system for NP that satisfies the following properties: 1. This system has a constant number of rounds with negligible soundness error. 2. It remains zero knowledge even when composed concurrently n times, where n is the security parameter. Simultaneously obtaining 1 and 2 has been recently proven to be impossible to achieve using black-box simulators. 3. It is an Arthur-Merlin (public coins) protocol. Simultaneously obtaining 1 and 3 was known to be impossible to achieve with a black-box simulator. 4. It has a simulator that runs in strict polynomial time, rather than in expected polynomial time. All previously known constant-round, negligible-error zero-knowledge arguments utilized expected polynomial-time simulators.
[non-black-box simulators, collision resistent hash functions, NP, Arthur-Merlin protocol, zero-knowledge argument system, digital simulation, security parameter, polynomial-time simulators, Security, black-box simulation barrier, simulation paradigm, private input, black-box simulators, Polynomials, expected polynomial time, Cryptography, protocols, Computational modeling, Computer simulation, Access protocols, cryptography, Knowledge management, negligible soundness error, Computer science, honest party, strict polynomial time, adversary algorithm, public coins protocol, computational complexity, constant-round negligible-error zero-knowledge arguments]
Testing subgraphs in large graphs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Let H be a fixed graph with h vertices, let G be a graph on n vertices and suppose that at least /spl epsi/n/sup 2/ edges have to be deleted from it to make it H-free. It is known that in this case G contains at least f (/spl epsi/, H)n/sup h/ copies of H. We show that the largest possible function f (/spl epsi/, H) is polynomial in /spl epsi/ if and only if H is bipartite. This implies that there is a one-sided error property tester for checking H-freeness, whose query complexity is polynomial in 1//spl epsi/, if and only if H is bipartite.
[one-sided error property tester, H-freeness, graph theory, subgraph testing, probability, testing, query complexity, Mathematics, bipartite, largest possible function, Computer science, polynomial complexity, Information geometry, Chromium, graph isomorphism, Polynomials, Testing, computational complexity]
Deterministic computation of the Frobenius form
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
A deterministic algorithm for computing the Frobenius canonical-form of a matrix over a field is described. A similarity transformation-matrix is recovered in the same time. The algorithm is nearly optimal, requiring about the same number of field operations as required for matrix multiplication. Previously-known reductions to matrix multiplication are probabilistic.
[similarity transformation matrix, Costs, Shape, Frobenius canonical-form, Application software, deterministic algorithm, deterministic algorithms, field operations, Computer science, matrix multiplication, optimisation, Frobenius form, Algorithms, Algebra, deterministic computation, Bismuth, Polynomials, probabilistic reductions, Testing, computational complexity]
A replacement for Voronoi diagrams of near linear size
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
For a set P of n points in R/sup d/, we define a new type of space decomposition. The new diagram provides an /spl epsi/-approximation to the distance function associated with the Voronoi diagram of P, while being of near linear size, for d/spl ges/2. This contrasts with the standard Voronoi diagram that has /spl Omega/ (n/sup [d/2]/) complexity in the worst case.
[complexity, distance function, /spl epsi/-approximation, computational geometry, set theory, Nearest neighbor searches, Computer science, Graphics, Surface reconstruction, Voronoi diagram replacement, Neural networks, Clustering algorithms, Chromium, Polynomials, near linear size Voronoi diagrams, geometric computing, Artificial intelligence, Mesh generation, standard Voronoi diagram, computational complexity, space decomposition]
Game theory and mathematical economics: a theoretical computer scientist's introduction
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
There has been recently increasing interaction between game theory and, more generally, economic theory, with theoretical computer science, mainly in the context of the Internet. The paper is an invitation to this important frontier.
[programming theory, economic theory, game theory, Nash equilibrium, mathematical economics, Game theory, Combinatorial mathematics, bibliographies, Centralized control, Computer science, Degradation, economics, theoretical computer science, Polynomials, Concrete, Internet, Books]
Building low-diameter P2P networks
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
In a peer-to-peer (P2P) network, nodes connect into an existing network and participate in providing and availing of services. There is no dichotomy between a central server and distributed clients. Current P2P networks (e.g., Gnutella) are constructed by participants following their own uncoordinated (and often whimsical) protocols; they consequently suffer from frequent network overload and fragmentation into disconnected pieces separated by choke-points with inadequate bandwidth. The authors propose a simple scheme for participants to build P2P networks in a distributed fashion, and prove that it results in connected networks of constant degree and logarithmic diameter. It does so with no global knowledge of all the nodes in the network. In the most common P2P application to date (search), these properties are important.
[current P2P networks, Protocols, choke-points, connected networks, Vehicle dynamics, low-diameter P2P network design, Network servers, Network topology, heuristic programming, distributed clients, Web and internet services, Bandwidth, Gnutella, central server, theorem proving, IP networks, protocols, stochastic processes, Web server, Assembly, client-server systems, Peer to peer computing, information retrieval, logarithmic diameter, disconnected pieces, P2P application, network overload, Internet, peer-to-peer network]
Online facility location
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We consider the online variant of facility location, in which demand points arrive one at a time and we must maintain a set of facilities to service these points. We provide a randomized online O(1)-competitive algorithm in the case where points arrive in random order. If points are ordered adversarially, we show that no algorithm can be constant-competitive, and provide an O(log n)-competitive algorithm. Our algorithms are randomized and the analysis depends heavily on the concept of expected waiting time. We also combine our techniques with those of M. Charikar and S. Guha (1999) to provide a linear-time constant approximation for the offline facility location problem.
[Algorithm design and analysis, Costs, randomized online competitive algorithm, offline facility location problem, online facility location, expected waiting time, competitive algorithms, random processes, Extraterrestrial measurements, algorithms randomization, Application software, Cables, randomised algorithms, facility location, Computer science, Network servers, demand points, random order, Web pages, Linear approximation, linear-time constant approximation, computational complexity]
Distributions on level-sets with applications to approximation algorithms
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We consider a family of distributions on fixed-weight vectors in {0, 1}/sup t/; these distributions enjoy certain negative correlation properties and also satisfy pre-specified conditions on their marginal distributions. We show the existence of such families, and present a linear-time algorithm to sample from them. This yields improved approximation algorithms for the following problems: (a) low-congestion multi-path routing; (b) maximum coverage versions of set cover; (c) partial vertex cover problems for bounded-degree graphs; and (d) the Group Steiner Tree problem. For (a) and (b), the improvement is in the approximation ratio; for (c), we show how to speedup existing approximation algorithms while preserving the best-known approximation ratio; we also improve the approximation ratio for certain families of instances of unbounded degree. For (d), we derive an approximation algorithm whose approximation guarantee is at least as good as what is known; our algorithm is shown to have a better approximation guarantee for the worst known input families for existing algorithms.
[marginal distributions, approximation theory, maximum coverage versions, trees (mathematics), computational geometry, negative correlation properties, approximation algorithms, level-sets, linear-time algorithm, low-congestion multi-path routing, approximation guarantee, partial vertex cover problems, Approximation algorithms, Concrete, fixed-weight vectors, bounded-degree graphs, group Steiner tree problem]
Lower bounds for matrix product
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We prove lower bounds on the number of product gates in bilinear and quadratic circuits that compute the product of two n /spl times/ n matrices over finite fields. In particular we obtain the following results: 1. We show that the number of product gates in any bilinear (or quadratic) circuit that computes the product of two n /spl times/ n matrices over GF(2) is at least 3n/sup 2/ o(n/sup 2/). 2. We show that the number of product gates in any bilinear circuit that computes the product of two n /spl times/ n matrices over GF(p) is at least (2.5 + 1.5/p/sup 3/-1)n/sup 2/ - o(n/sup 2/). These results improve the former results of N.H. Bshouty (1997) and M. Blaser (1999) who proved lower bounds of 2.5n/sup 2/ o(n/sup 2/).
[circuit theory, matrix product, Computational modeling, Circuits, Galois fields, lower bounds, Computer science, matrix multiplication, finite fields, Upper bound, quadratic circuits, Character generation, Ear, Polynomials, theorem proving, bilinear circuits, product gates, Arithmetic, computational complexity]
Almost tight upper bounds for vertical decompositions in four dimensions
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We show that the complexity of the vertical decomposition of an arrangement of n fixed-degree algebraic surfaces or surface patches in four dimensions is O(n/sup 4+/spl epsi//) for any /spl epsi/ > 0. This improves the best previously known upper bound for this problem by a near-linear factor, and settles a major problem in the theory of arrangements of surfaces, open since 1989. The new bound can be extended to higher dimensions, yielding the bound O (n/sup 2d-4+/spl epsi//), for any /spl epsi/ > 0, on the complexity of vertical decompositions in dimensions d /spl ges/ 4. We also describe the immediate algorithmic applications of these results, which include improved algorithms for point location, range searching, ray shooting, robot motion planning, and some geometric optimization problems.
[computational geometry, ray shooting, immediate algorithmic applications, robot motion planning, geometric optimization problems, Computer science, Robot motion, Motion planning, surface patches, Computational geometry, fixed-degree algebraic surfaces, Upper bound, optimisation, near-linear factor, Polynomials, theorem proving, almost tight upper bounds, point location, vertical decompositions, range searching, computational complexity]
Clustering motion
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Given a set of moving points in R/sup d/, we show that one can cluster them in advance, using a small number of clusters, so that at any point in time this static clustering is competitive with the optimal k-centre clustering of the point-set at this point in time. The advantage of this approach is that it avoids the usage of kinetic data structures and as such it does not need to update the clustering as time passes. To implement this static clustering efficiently, we describe a simple technique for speeding up clustering algorithms, and apply it to achieve faster clustering algorithms for several problems. In particular, we present a linear time algorithm for computing a 2-approximation to the k-centre clustering of a set of n points in R/sup d/. This is a slight improvement over the algorithm of T. Feder and D. Greene (1988), that runs in /spl Theta/(n log k) time (which is optimal in the comparison model).
[clustering motion, kinetic data structures, Spatial databases, moving point clustering, point-set, set theory, Unsupervised learning, Space stations, Computer science, optimal k-center clustering, 2-approximation, faster clustering algorithms, Sections, pattern clustering, clustering algorithms, Clustering algorithms, Ear, static clustering, Polynomials, data structures, Kinetic theory, theorem proving, computational complexity]
Truthful mechanisms for one-parameter agents
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The authors show how to design truthful (dominant strategy) mechanisms for several combinatorial problems where each agent's secret data is naturally expressed by a single positive real number. The goal of the mechanisms we consider is to allocate loads placed on the agents, and an agent's secret data is the cost she incurs per unit load. We give an exact characterization for the algorithms that can be used to design truthful mechanisms for such load balancing problems using appropriate side payments. We use our characterization to design polynomial time truthful mechanisms for several problems in combinatorial optimization to which the celebrated VCG mechanism does not apply. For scheduling related parallel machines (Q/spl par/C/sub max/), we give a 3-approximation mechanism based on randomized rounding of the optimal fractional solution. This problem is NP-complete, and the standard approximation algorithms (greedy load-balancing or the PTAS) cannot be used in truthful mechanisms. We show our mechanism to be frugal, in that the total payment needed is only a logarithmic factor more than the actual costs incurred by the machines, unless one machine dominates the total processing power. We also give truthful mechanisms for maximum flow, Q/spl par//spl Sigma/C/sub j/ (scheduling related machines to minimize the sum of completion times), optimizing an affine function over a fixed set, and special cases of uncapacitated facility location. In addition, for Q/spl par//spl Sigma/w/sub j/C/sub j/ (minimizing the weighted sum of completion times), we prove a lower bound of 2//spl radic/3 for the best approximation ratio achievable by truthful mechanism.
[Algorithm design and analysis, fixed set, Operations research, combinatorial mathematics, dominant strategy mechanisms, greedy load-balancing, load allocation, single positive real number, NP-complete, parallel machines, Cost accounting, processor scheduling, 3-approximation mechanism, Design optimization, truthful mechanisms, VCG mechanism, one-parameter agents, total payment, optimisation, resource allocation, PTAS, load balancing problems, side payments, affine function, Polynomials, approximation ratio, logarithmic factor, total processing power, Power generation economics, game theory, polynomial time truthful mechanisms, lower bound, Game theory, uncapacitated facility location, Computer science, combinatorial problems, combinatorial optimization, standard approximation algorithms, exact characterization, secret data, maximum flow, Load management, Approximation algorithms, optimal fractional solution, randomized, computational complexity]
Sorting and selection with structured costs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The study of the effect of priced information on basic algorithmic problems was initiated by M. Charikar et al. (2000). The authors continue the study of sorting and selection in the priced comparison model, i.e., when each comparison has an associated cost, and answer some of the open problems suggested by Charikar et al. If the comparison costs are allowed to be arbitrary, we show that one cannot get good approximation ratios. A different way to assign costs is based on the idea that one can distill out an intrinsic value for each item being compared, such that the cost of comparing two elements is some "well-behaved" or "structured" function of their values. We feel that most practical applications will have some structured cost property. The authors study the problems of sorting and selection (which includes finding the maximum and the median) in the structured cost model. We get a variety of approximation results for these problems, depending on the restrictions we put on the structured costs. We show that it is possible to get much improved results with the structured cost model than the case when we do not have any assumptions on comparison costs.
[priced comparison model, trees (mathematics), priced information, costing, structured costs, comparison costs, Sorting, Computer science, structured cost model, Databases, structured cost property, structured function, approximation results, sorting, basic algorithmic problems, approximation ratios, practical applications, Cost function, Robustness, theorem proving, intrinsic value, associated cost, computational complexity]
Randomly colouring graphs with lower bounds on girth and maximum degree
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We consider the problem of generating a random q-colouring of a graph G=(V, E). We consider the simple Glauber Dynamics chain. We show that if the maximum degree /spl Delta/>c/sub l/ ln n and the girth g>c/sub 2/ ln ln n (n=|V|), then this chain mixes rapidly provided C/sub 1/, C/sub 2/ are sufficiently large, q/A>/spl beta/, where /spl beta//spl ap/1.763 is the root of /spl beta/=e/sup 1//spl beta//. For this class of graphs, this beats the 11/spl Delta//6 bound of E. Vigoda (1999) for general graphs. We extend the result to random graphs.
[general graphs, random processes, random graphs, graph colouring, lower bounds, simple Glauber Dynamics chain, Computer science, optimisation, random graph colouring, maximum degree, Character generation, Chromium, girth, random q-colouring, computational complexity]
Approximation algorithms for the job interval selection problem and related scheduling problems
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The authors consider the job interval selection problem (JISP), a simple scheduling model with a rich history and numerous applications. Special cases of this problem include the so-called real-time scheduling problem (also known as the throughput maximization problem) in single and multiple machine environments. In these special cases we have to maximize the number of jobs scheduled between their release date and deadline (preemption is not allowed). Even the single machine case is NP-hard. The unrelated machines case, as well as other special cases of JISP, are MAX SNP-hard. A simple greedy algorithm gives a 2-approximation for JISP. Despite many efforts, this was the best approximation guarantee known, even for throughput maximization on a single machine. The authors break this barrier and show an approximation guarantee of less than 1.582 for arbitrary instances of JISP. For some special cases, we show better results. Our methods can be used to give improved bounds for some related resource allocation problems that were considered recently in the literature.
[iterative methods, NP-hard, arbitrary instances, Throughput, Electronic mail, approximation algorithms, optimisation, resource allocation, real-time scheduling problem, scheduling, simple scheduling model, MAX SNP-hard, Contracts, approximation theory, Job shop scheduling, multiple machine environments, scheduling problems, throughput maximization, throughput maximization problem, Application software, Scheduling algorithm, Computer science, 2-approximation, approximation guarantee, Adaptive scheduling, single machine environments, Processor scheduling, simple greedy algorithm, real-time systems, Approximation algorithms, job interval selection problem, release date, unrelated machines case, JISP, preemption, computational complexity]
On the complexity of many faces in arrangements of circles
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We obtain improved bounds on the complexity of m distinct faces in an arrangement of n circles and in an arrangement of n unit circles. The bounds are worst-case tight for unit circles, and, for general circles, they nearly coincide with the best known bounds for the number of incidences between m points and n circles.
[Heart, Solid modeling, combinatorial mathematics, worst-case tight, distinct face complexity, computational geometry, best known bounds, Application software, general circles, Computer science, Computational geometry, Information geometry, Information science, circle arrangements, Computer graphics, Mathematical model, Robots, computational complexity]
Source routing and scheduling in packet networks
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We study routing and scheduling in packet-switched networks. We assume an adversary that controls the injection time, source, and destination for each packet injected. A set of paths for these packets is admissible if no link in the network is overloaded. We present the first on-line routing algorithm that finds a set of admissible paths whenever this is feasible. Our algorithm calculates a path for each packet as soon as it is injected at its source using a simple shortest path computation. The length of a link reflects its current congestion. We also show how our algorithm can be implemented under today's Internet routing paradigms. When the paths are known (either given by the adversary or computed as above) our goal is to schedule the packets along the given paths so that the packets experience small end-to-end delays. The best previous delay bounds for deterministic and distributed scheduling protocols were exponential in the path length. In this paper we present the first deterministic and distributed scheduling protocol that guarantees a polynomial end-to-end delay for every packet. Finally, we discuss the effects of combining routing with scheduling. We first show that some, unstable scheduling protocols remain unstable no matter how the paths are chosen. However, the freedom to choose paths can make a difference. For example, we show that a ring with parallel links is stable for all greedy scheduling protocols if paths are chosen intelligently, whereas this is not the case if the adversary specifies the paths.
[length link, source scheduling, packet switching, destination, on-line routing algorithm, greedy scheduling protocols, Delay, Intelligent networks, shortest path computation, polynomial end-to-end delay, scheduling, Polynomials, Routing protocols, protocols, source routing, parallel links, injection time, packet-switched networks, deterministic algorithms, Scheduling algorithm, Internet routing paradigms, Computer science, unstable scheduling protocols, Processor scheduling, distributed algorithms, admissible paths, telecommunication network routing, deterministic scheduling protocols, Internet, congestion, delay bounds, distributed scheduling protocols]
Designing networks for selfish users is hard
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We consider a directed network in which every edge possesses a latency function specifying the time needed to traverse the edge given its congestion. Selfish, noncooperative agents constitute the network traffic and wish to travel from a source s to a sink t as quickly as possible. Since the route chosen by one network user affects the congestion (and hence the latency) experienced by others, we model the problem as a noncooperative game. Assuming each agent controls only a negligible portion of the overall traffic, Nash equilibria in this noncooperative game correspond to s-t flows in which all flow paths have equal latency. We give optimal inapproximability results and approximation algorithms for several network design problems of this type. For example, we prove that for networks with n nodes and continuous, nondecreasing latency functions, there is no approximation algorithm for this problem with approximation ratio less than n/2 (unless P = NP). We also prove this hardness result to be best possible by exhibiting an n/2-approximation algorithm. For networks in which the latency of each edge is a linear function of the congestion, we prove that there is no (4/3 - /spl epsi/)-approximation algorithm for the problem (for any /spl epsi/ > 0, unless P = NP); the existence of a 4/3-approximation algorithm follows easily from existing work, proving this hardness result sharp.
[Algorithm design and analysis, flow paths, Protocols, multi-agent systems, selfish noncooperative agents, Telecommunication traffic, Switches, Nash equilibrium, approximation algorithms, latency function, Delay, optimal inapproximability results, Nash equilibria, linear function, hardness result, theorem proving, IP networks, approximation ratio, game theory, directed network, Routing, selfish users, network design problems, s-t flows, Computer science, network traffic, nondecreasing latency functions, directed graphs, Approximation algorithms, n/2 -approximation algorithm, noncooperative game, computational complexity]
Designing networks incrementally
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We consider the problem of incrementally designing a network to route demand to a single sink on an underlying metric space. We are given cables whose costs per unit length scale in a concave fashion with capacity. Under certain natural restrictions on the costs (called the Access Network Design constraints), we present a simple and efficient randomized algorithm that is competitive to the minimum cost solution when the demand points arrive online. In particular, if the order of arrival is a random permutation, we can prove a O(1) competitive ratio. For the fully adversarial case, the algorithm is O(K) -competitive, where K is the number of different pipe types. Since the value of K is typically small, this improves the previous O(log n log log n)-competitive algorithm which was based on probabilistically approximating the underlying metric by a tree metric. Our algorithm also improves the best known approximation ratio and running time for the offline version of this problem.
[Algorithm design and analysis, graph theory, pipe types, offline version, natural restrictions, random permutation, competitive ratio, demand points, incremental network design, demand routing, Telephony, Cost function, theorem proving, competitive algorithm, adversarial case, approximation ratio, network routing, single sink, probabilistic approximation, competitive algorithms, Extraterrestrial measurements, randomized algorithm, Application software, Cables, Nearest neighbor searches, randomised algorithms, Computer science, Access Network Design constraints, underlying metric space, Economies of scale, Approximation algorithms, minimum cost solution, order of arrival, computational complexity]
Compact oracles for reachability and approximate distances in planar digraphs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
It is shown that a planar digraph can be preprocessed in near-linear time, producing a near-linear space distance oracle that can answer reachability queries in constant time. The oracle can be distributed as an O(log n) space label for each vertex and then we can determine if one vertex can reach another considering their two labels only. The approach generalizes to approximate distances in weighted planar digraphs where we can then get a (1+/spl epsi/) approximation distance in O(log log /spl Delta/+1//spl epsi/) time where /spl Delta/ is the longest finite distance in the graph and weights are assumed to be non-negative integers. Our scheme can be extended to find and route along the short dipaths. Our technique is based on a novel dipath decomposition of planar digraphs that instead of using the standard separator with O(/spl radic/n) vertices, in effect finds a separator using a constant number of dipaths.
[reachability queries, non-negative integers, Roads, Laboratories, query processing, approximate distances, approximation distance, novel dipath decomposition, reachability analysis, Particle separators, short dipaths, compact oracles, Extraterrestrial measurements, space label, Topology, near-linear space distance oracle, constant time, Application software, longest finite distance, planar digraph, near-linear time, weighted planar digraphs, directed graphs, standard separator, computational complexity]
Glauber dynamics on trees and hyperbolic graphs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We study discrete time Glauber dynamics for random configurations with local constraints (e.g. proper coloring, Ising and Potts models) on finite graphs with n vertices and of bounded degree. We show that the relaxation time (defined as the reciprocal of the spectral gap 1-/spl lambda//sub 2/) for the dynamics on trees and on certain hyperbolic graphs, is polynomial in n. For these hyperbolic graphs, this yields a general polynomial sampling algorithm for random configurations. We then show that if the relaxation time /spl tau//sub 2/ satisfies /spl tau//sub 2/=O(n), then the correlation coefficient, and the mutual information, between any local function (which depends only on the configuration in a fixed window) and the boundary conditions, decays exponentially in the distance between the window and the boundary. For the Ising model on a regular tree, this condition is sharp.
[local constraints, Terminology, regular tree, graph theory, bounded degree graphs, Convergence, discrete time Glauber dynamics, Monte Carlo methods, Tree graphs, Ising model, Polynomials, relaxation time, mutual information, correlation coefficient, boundary conditions, correlation theory, sampling methods, Ising models, Potts models, Boundary conditions, Potts model, Physics, hyperbolic graphs, Computer science, general polynomial sampling algorithm, random configurations, Markov processes, spectral gap, Sampling methods, local function, finite graphs, Mutual information, computational complexity]
An iterative rounding 2-approximation algorithm for the element connectivity problem
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
In the survivable network design problem (SNDP), given an undirected graph and values r/sub ij/ for each pair of vertices i and j, we attempt to find a minimum-cost subgraph such that there are r/sub ij/ disjoint paths between vertices i and j. In the edge connected version of this problem (EC-SNDP), these paths must be edge-disjoint. In the vertex connected version of the problem (VC-SNDP), the paths must be vertex disjoint. K. Jain et al. (1999) propose a version of the problem intermediate in difficulty to these two, called the element connectivity problem (ELC-SNDP, or ELC). These variants of SNDP are all known to be NP-hard. The best known approximation algorithm for the EC-SNDP has performance guarantee of 2 (K. Jain, 2001), and iteratively rounds solutions to a linear programming relaxation of the problem. ELC has a primal-dual O (log k) approximation algorithm, where k=max/sub i,j/ r/sub ij/. VC-SNDP is not known to have a non-trivial approximation algorithm; however, recently L. Fleischer (2001) has shown how to extend the technique of K. Jain ( 2001) to give a 2-approximation algorithm in the case that r/sub ij//spl isin/{0, 1, 2}. She also shows that the same techniques will not work for VC-SNDP for more general values of r/sub ij/. The authors show that these techniques can be extended to a 2-approximation algorithm for ELC. This gives the first constant approximation algorithm for a general survivable network design problem which allows node failures.
[Algorithm design and analysis, iterative methods, SNDP, NP-hard, graph theory, reliability, linear programming, VC-SNDP, ELC-SNDP, approximation algorithm, performance guarantee, linear programming relaxation, Robustness, Polynomials, disjoint paths, undirected graph, primal-dual, approximation theory, fault tolerance, survivable network design problem, iterative rounding 2-approximation algorithm, Linear programming, Computer science, edge connected version, minimum-cost subgraph, EC-SNDP, vertex connected version, 2-approximation algorithm, Approximation algorithms, Iterative algorithms, constant approximation algorithm, element connectivity problem, ELC, node failures, computational complexity]
Extractors from Reed-Muller codes
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Finding explicit extractors is an important derandomization goal that has received a lot of attention in the past decade. Previous research has focused on two approaches, one related to hashing and the other to pseudorandom generators. A third view, regarding extractors as good error correcting codes, was noticed before. Yet, researchers had failed to build extractors directly from a good code without using other tools from pseudorandomness. We succeed in constructing an extractor directly from a Reed-Muller code. To do this, we develop a novel proof technique. Furthermore, our construction is the first to achieve a degree close to linear. In contrast, the best previous constructions brought the log of the degree within a constant of optimal, which gives polynomial degree. This improvement is important for certain applications. For example, it follows that approximating the VC dimension to within a factor of N/sup 1-/spl delta// is AM-hard for any positive /spl delta/.
[Virtual colonoscopy, VC dimension, error correction codes, pseudorandom generators, error correcting codes, Buildings, Circuits, random processes, Reed-Muller code extractors, Reed-Muller codes, AM-hard, Entropy, proof technique, History, explicit extractors, Computer science, derandomization goal, Character generation, polynomial degree, Polynomials, pseudorandomness, Error correction codes, theorem proving, computational complexity]
Universally composable security: a new paradigm for cryptographic protocols
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We propose a novel paradigm for defining security of cryptographic protocols, called universally composable security. The salient property of universally composable definitions of security is that they guarantee security even when a secure protocol is composed of an arbitrary set of protocols, or more generally when the protocol is used as a component of an arbitrary system. This is an essential property for maintaining security of cryptographic protocols in complex and unpredictable environments such as the Internet. In particular, universally composable definitions guarantee security even when an unbounded number of protocol instances are executed concurrently in an adversarially controlled manner, they guarantee non-malleability with respect to arbitrary protocols, and more. We show how to formulate universally composable definitions of security for practically any cryptographic task. Furthermore, we demonstrate that practically any such definition can be realized using known techniques, as long as only a minority of the participants are corrupted. We then proceed to formulate universally composable definitions of a wide array of cryptographic tasks, including authenticated and secure communication, key-exchange, public-key encryption, signature, commitment, oblivious transfer, zero knowledge and more. We also make initial steps towards studying the realizability of the proposed definitions in various settings.
[complex unpredictable environments, universally composable security, key-exchange, computability, arbitrary system, cryptographic task, Reactive power, authenticated communication, oblivious transfer, adversarially controlled execution, Cryptography, Mathematical model, protocols, Computer security, public-key encryption, cryptography, Job design, non-malleability, Application software, concurrent composition, bibliographies, Cryptographic protocols, Radio access networks, cryptographic protocol paradigm, secure communication, Computer science, arbitrary protocols, message authentication, universally composable definitions, secure protocol, Internet, zero knowledge]
Testing random variables for independence and identity
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Given access to independent samples of a distribution A over [n] /spl times/ [m], we show how to test whether the distributions formed by projecting A to each coordinate are independent, i.e., whether A is /spl epsi/-close in the L/sub 1/ norm to the product distribution A/sub 1//spl times/A/sub 2/ for some distributions A/sub 1/ over [n] and A/sub 2/ over [m]. The sample complexity of our test is O/spl tilde/(n/sup 2/3/m/sup 1/3/poly(/spl epsi//sup -1/)), assuming without loss of generality that m/spl les/n. We also give a matching lower bound, up to poly (log n, /spl epsi//sup -1/) factors. Furthermore, given access to samples of a distribution X over [n], we show how to test if X is /spl epsi/-close in L/sub 1/ norm to an explicitly specified distribution Y. Our test uses O/spl tilde/(n/sup 1/2/poly(/spl epsi//sup -1/)) samples, which nearly matches the known tight bounds for the case when Y is uniform.
[independent samples, product distribution, probability, random processes, testing, random variable testing, matching lower bound, Statistics, sample complexity, Computer science, known tight bounds, explicitly specified distribution, National electric code, Cities and towns, Random variables, Testing, computational complexity, L/sub 1/ norm]
The complexity of factors of multivariate polynomials
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The existence of string functions, which are not polynomial time computable, but whose graph is checkable in polynomial time, is a basic assumption in cryptography. We prove that in the framework of algebraic complexity, there are no such families of polynomial functions of p-bounded degree overfields of characteristic zero. The proof relies on a polynomial upper bound on the approximative complexity of a factor g of a polynomial f in terms of the (approximative) complexity of f and the degree of the factor g. This extends a result by E. Kaltofen (1986). The concept of approximative complexity allows us to cope with the case that a factor has an exponential multiplicity, by using a perturbation argument. Our result extends to randomized (two-sided error) decision complexity.
[multivariate polynomial factor complexity, perturbation argument, Mathematics, Decision feedback equalizers, p-bounded degree, Tree graphs, string functions, polynomial functions, Polynomials, algebraic complexity, theorem proving, proof, Cryptography, Testing, approximative complexity, characteristic zero, Computational modeling, polynomials, cryptography, randomized decision complexity, Computer science, Upper bound, polynomial time computable, polynomial upper bound, exponential multiplicity, Arithmetic, computational complexity]
Algorithmic applications of low-distortion geometric embeddings
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The author surveys algorithmic results obtained using low-distortion embeddings of metric spaces into (mostly) normed spaces. He shows that low-distortion embeddings provide a powerful and versatile toolkit for solving algorithmic problems. Their fundamental nature makes them applicable in a variety of diverse settings, while their relation to rich mathematical fields (e.g., functional analysis) ensures availability of tools for their construction.
[algorithmic problems, functional analysis, metric spaces, set theory, bibliographies, mathematical fields, normed spaces, versatile toolkit, algorithm theory, Chromium, geometry, low-distortion embeddings, Artificial intelligence]
Web search via hub synthesis
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We present a model for web search that captures in a unified manner three critical components of the problem: how the link structure of the web is generated, how the content of a web document is generated, and how a human searcher generates a query. The key to this unification lies in capturing the correlations between these components in terms of proximity in a shared latent semantic space. Given such a combined model, the correct answer to a search query is well defined, and thus it becomes possible to evaluate web search algorithms rigorously. We present a new web search algorithm, based on spectral techniques, and prove that it is guaranteed to produce an approximately correct answer in our model. The algorithm assumes no knowledge of the model, and is well-defined regardless of the model's accuracy.
[information resources, Embedded computing, spectral techniques, Humans, Information retrieval, search query, web search, Computer science, Couplings, query processing, web document, hub synthesis, Web pages, Search engines, Internet, Books, Web search, shared latent semantic space]
Approximate shape fitting via linearization
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Shape fitting is a fundamental optimization problem in computer science. The authors present a general and unified technique for solving a certain family of such problems. Given a point set P in R/sup d/, this technique can be used to /spl epsi/-approximate: (i) the min-width annulus and shell that contains P, (ii) minimum width cylindrical shell containing P, (iii) diameter, width, minimum volume bounding box of P, and (iv) all the previous measures for the case the points are moving. The running time of the resulting algorithms is O(n + 1//spl epsi//sup c/), where c is a constant that depends on the problem at hand. Our new general technique enables us to solve those problems without resorting to a careful and painful case by case analysis, as was previously done for those problems. Furthermore, for several of those problems our results are considerably simpler and faster than what was previously known. In particular, for the minimum width cylindrical shell problem, our solution is the first algorithm whose running time is subquadratic in n. (In fact we get running time linear in n.).
[Shape, minimum volume bounding box, computational geometry, set theory, optimisation, running time, Volume measurement, computer science, approximate shape fitting, fundamental optimization problem, Cities and towns, unified technique, cylindrical shell, approximation theory, linearisation techniques, Spatial databases, minimum width cylindrical shell problem, Computer science, Graphics, Object detection, min-width annulus, Chromium, Metrology, Kinetic theory, point set, computational complexity]
The natural work-stealing algorithm is stable
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
In this paper we analyse a very simple dynamic work-stealing algorithm. In the work-generation model, there are n generators which are arbitrarily distributed among a set of n processors. During each time-step, with probability /spl lambda/, each generator generates a unit-time task which it inserts into the queue of its host processor. After the new tasks are generated, each processor removes one task from its queue and services it. Clearly, the work-generation model allows the load to grow more and more imbalanced, so, even when /spl lambda/<1, the system load can be unbounded. The natural work-stealing algorithm that we analyse works as follows. During each time step, each empty processor sends a request to a randomly selected other processor. Any non-empty processor having received at least one such request in turn decides (again randomly) in favour of one of the requests. The number of tasks which are transferred from the non-empty processor to the empty one is determined by the so-called work-stealing function f. We analyse the long-term behaviour of the system as a function of /spl lambda/ and f. We show that the system is stable for any constant generation rate /spl lambda/<1 and for a wide class of functions f. We give a quantitative description of the functions f which lead to stable systems. Furthermore, we give upper bounds on the average system load (as a function of f and n).
[Algorithm design and analysis, Heuristic algorithms, upper bounds, nonempty processor, Distributed computing, numerical stability, natural work stealing algorithm stability, resource allocation, Kernel, Contracts, Load modeling, host processor, average system load, probability, unit-time task, work-generation model, empty processor, constant generation rate, Computer science, Upper bound, Parallel programming, distributed algorithms, Load management, dynamic work stealing algorithm, queue, long-term behaviour]
Vickrey prices and shortest paths: what is an edge worth?
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We solve a shortest path problem that is motivated by recent interest in pricing networks or other computational resources. Informally, how much is an edge in a network worth to a user who wants to send data between two nodes along a shortest path? If the network is a decentralized entity, such as the Internet, in which multiple self-interested agents own different parts of the network, then auction-based pricing seems appropriate. A celebrated result from auction theory shows that the use of Vickrey pricing motivates the owners of the network resources to bid truthfully. In Vickrey's scheme, each agent is compensated in proportion to the marginal utility he brings to the auction. In the context of shortest path routing, an edge's utility is the value by which it lowers the length of the shortest path, i.e., the difference between the shortest path lengths with and without the edge. Our problem is to compute these marginal values for all the edges of the network efficiently. The naive method requires solving the single-source shortest path problem up to n times, for an n-node network. We show that the Vickrey prices for all the edges can be computed in the same asymptotic time complexity as one single-source shortest path problem. This solves an open problem posed by N. Nisan and A. Ronen (1999).
[Protocols, Operations research, multi-agent systems, graph theory, n-node network, costing, edge utility, auction theory, Shortest path problem, optimisation, shortest path routing, shortest path problem, single-source shortest path problem, Pricing, Computer graphics, naive method, Computer networks, IP networks, multiple self-interested agents, auction based pricing, network routing, decentralized entity, asymptotic time complexity, Application software, Computer science, pricing networks, computational resources, operations research, Vickrey prices, marginal values, network resources, network edge worth, Internet, marginal utility, shortest path lengths, computational complexity]
Simple extractors for all min-entropies and a new pseudo-random generator
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We present a simple, self-contained extractor construction that produces good extractors for all min-entropies (min-entropy measures the amount of randomness contained in a weak random source). Our construction is algebraic and builds on a new polynomial-based approach introduced by A. Ta-Shma et al. (2001). Using our improvements, we obtain, for example, an extractor with output length m=k/sup 1-/spl delta// and seed length O(log n). This matches the parameters of L. Trevisan's (1999) breakthrough result and additionally achieves those parameters for small min-entropies k. Our construction gives a much simpler and more direct solution to this problem. Applying similar ideas to the problem of building pseudo-random generators, we obtain a new pseudo-random generator construction that is not based on the NW generator (N. Nisan and A. Widgerson, 1994), and turns worst-case hardness directly into pseudo-randomness. The parameters of this generator are strong enough to obtain a new proof that P=BPP if E requires exponential size circuits. Essentially, the same construction yields a hitting set generator with optimal seed length that outputs s/sup /spl Omega/(1)/ bits when given a function that requires circuits of size s (for any s). This implies a hardness versus randomness trade off for RP and BPP that is optimal (up to polynomial factors), solving an open problem raised by R. Impagliazzo et al. (1999). Our generators can also be used to derandomize AM.
[Protocols, output length, Circuits, exponential size circuits, Complexity theory, random number generation, setf-contained extractor construction, Polynomials, pseudorandomness, theorem proving, worst-case hardness, minimum entropy methods, Codes, min-entropy extractors, randomness measurement, Buildings, pseudo-random generator, optimal seed length, random processes, min-entropy, polynomial-based approach, randomised algorithms, polynomial factors, Approximation algorithms, weak random source, computational complexity, hitting set generator]
Traveling with a Pez dispenser (or, routing issues in MPLS)
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
MultiProtocol Label Switching (MPLS) is a routing model proposed by the IETF for the Internet, and is becoming widely popular. In this paper, we initiate a theoretical study of the routing model, and give routing algorithms and lower bounds in a variety of situations. We first study the routing problems on the line. We then build up our results from paths through trees to more general graphs. The basic technique to go to general graphs is that of finding a tree cover, which is a small set of subtrees of the graph such that for each pair of vertices, one of the trees contains a shortest (or near-shortest) path between them. The concept of tree covers appears to have many interesting applications.
[Pez dispenser, network routing, trees (mathematics), subtrees, Switches, tree covers, Data mining, multiprotocol label switching, lower bounds, Information analysis, Multiprotocol label switching, Computer science, routing algorithms, Tree graphs, shortest path problem, Trademarks, Routing protocols, tree cover, Internet, Performance analysis, protocols, routing model]
Linear-time recognition of circular-arc graphs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
A graph G is a circular-arc graph if it is the intersection graph of a set of arcs on a circle. That is, there is one arc for each vertex of G, and two vertices are adjacent in G if the corresponding arcs intersect. We give a linear time bound for recognizing this class of graphs. When G is a member of the class, the algorithm gives a certificate in the form of a set of arcs that realize it.
[circular-arc graph, graph recognition, vertex, graph theory, certificate, linear-time recognition, matrix algebra, Computer science, DNA, Genetics, linear time bound, Testing, computational complexity, intersection graph, interval graph]
Random evolution in massive graphs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Many massive graphs (such as the WWW graph and Call graphs) share certain universal characteristics which can be described by the so-called "power law." In this paper, we, examine three important aspects of power law graphs, (1) the evolution of power law graphs, (2) the asymmetry of in-degrees and out-degrees, (3) the "scale invariance" of power law graphs. In particular, we give three increasingly general directed graph models and one general undirected graph model for generating power law graphs by adding at most one node and possibly one or more edges at a time. We show that for any given edge density and desired power laws for in-degrees and out-degrees, not necessarily the same, the resulting graph will almost surely have the desired edge density and the power laws for the in-degrees and out-degrees. Our most general directed and undirected models include nearly all known power law evolution models as special cases. Finally, we show that our evolution models generate "scale invariant" graphs. We describe a method for scaling the time in our evolution model such that the power law of the degree sequences remains invariant.
[WWW graph, random evolution, World Wide Web, Routing, Power grids, Call graphs, formal specification, directed graphs, Motion pictures, Robustness, power law graphs, directed graph models, IP networks, massive graphs, Internet telephony, Power generation, evolution models]
Lower bounds for polynomial calculus: non-binomial case
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We generalize recent linear lower bounds for Polynomial Calculus based on binomial ideals. We produce a general hardness criterion (that we call immunity) which is satisfied by a random function and prove linear lower bounds on the degree of PC refutations for a wide class of tautologies based on immune functions. As some applications of our techniques, we introduce mod/sub p/ Tseitin tautologies in the Boolean case (e.g. in the presence of axioms x/sub i//sup 2/=x/sub i/), prove that they are hard for PC over fields with characteristic different from p, and generalize them to Flow tautologies which are based on the MAJORITY function and are proved to be hard over any field. We also show the /spl Omega/(n) lower bound for random k-CNFs over fields of characteristic 2.
[polynomial calculus, Computer aided software engineering, nonbinomial case, Computational modeling, polynomials, general hardness criterion, random function, Calculus, MAJORITY function, Computational complexity, Machinery, lower bounds, Boolean case, Computer science, Microwave integrated circuits, Boolean functions, immune functions, binomial ideals, PC refutations, tautologies, Polynomials, Robustness, computational complexity]
Fully dynamic all pairs shortest paths with real edge weights
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We present the first fully dynamic algorithm for maintaining all pairs shortest paths in directed graphs with real-valued edge weights. Given a dynamic directed graph G such that each edge can assume at most S different real values, we show how to support updates deterministically in O(S/spl middot/n/sup 2.5/log/sup 3/n) amortized time and queries in optimal worst-case time. No previous fully dynamic algorithm was known for this problem. In the special case where edge weights can only be increased, we give a randomized algorithm with one-sided error which supports updates faster in O(S/spl middot/nlog/sup 3/n) amortized time.
[real-valued edge weights, real edge weights, optimal worst-case time, Heuristic algorithms, Data engineering, randomized algorithm, Remuneration, Surges, randomised algorithms, fully dynamic algorithm, optimisation, Councils, amortized time, directed graphs, dynamic directed graph, one-sided error, Error correction, fully dynamic all pairs shortest paths, Contracts, computational complexity]
Expander-based constructions of efficiently decodable codes
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We present several novel constructions of codes which share the common thread of using expander (or expander-like) graphs as a component. The expanders enable the design of efficient decoding algorithms that correct a large number of errors through various forms of "voting" procedures. We consider both the notions of unique and list decoding, and in all cases obtain asymptotically good codes which are decodable up to a "maximum" possible radius and either: (a) achieve a similar rate as the previously best known codes but come with significantly faster algorithms, or (b) achieve a rate better than any prior construction with similar error-correction properties. Among our main results are: i) codes of rate /spl Omega/(/spl epsi//sup 2/) over constant-sized alphabet that can be list decoded in quadratic time from (1-/spl epsi/) errors; ii) codes of rate /spl Omega/(/spl epsi/) over constant-sized alphabet that can be uniquely decoded from (1/2-/spl epsi/) errors in near-linear time (this matches AG-codes with much faster algorithms); iii) linear-time encodable and decodable binary codes of positive rate (in fact, rate /spl Omega/(/spl epsi//sup 2/)) that can correct up to (1/4-/spl epsi/) fraction errors.
[Algorithm design and analysis, decodable binary codes, graph theory, efficiently decodable codes, decoding algorithms, asymptotically good codes, AG-codes, constant-sized alphabet, set theory, Yarn, unique decoding, Binary codes, list decoding, error correction properties, decoding algorithm, binary codes, error correction codes, linear-time encodable binary codes, Encoding, Decoding, decoding, Computer science, near-linear time, common thread, expander-based constructions, Communication channels, Computer errors, voting procedures, quadratic time, Error correction codes, Error correction, computational complexity]
Simple routing strategies for adversarial systems
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
In this paper we consider the problem of delivering dynamically changing input streams in dynamically changing networks where both the topology and the input streams can change in an unpredictable way. In particular, we present two simple distributed balancing algorithms (one for packet injections and one for flow injections) and show that for the case of a single receiver these algorithms will always ensure that the number of packets or flow in the system is bounded at any time step, even for an injection process that completely saturates the capacities of the available edges and even if the network topology changes in a completely unpredictable way. We also show that the maximum number of packets or flow that can be in the system at any time is essentially best possible by providing a lower bound that holds for any online algorithm, whether distributed or not. Interestingly, our balancing algorithms do not behave well in a completely adversarial setting. We show that also in the other extreme of a static network and a static injection pattern the algorithms will converge to a point in which they achieve an average routing time that is close to the best possible average routing time that can be achieved by any strategy. This demonstrates that there are simple algorithms that can be efficient for very different scenarios.
[Algorithm design and analysis, network routing, online algorithm, distributed balancing algorithms, Routing, Mobile ad hoc networks, dynamically changing input streams, Fault tolerance, Wireless sensor networks, Network topology, packet injections, distributed algorithms, Robustness, flow injections, Distributed algorithms, Contracts, Context modeling, dynamically changing networks]
Three theorems regarding testing graph properties
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Property testing is a relaxation of decision problems in which it is required to distinguish YES-instances (i.e., objects having a predetermined property) from instances that are far from any YES-instance. We present three theorems regarding testing graph properties in the adjacency matrix representation. More specifically, these theorems relate to the project of characterizing graph properties according to the complexity of testing them (in the adjacency matrix representation). The first theorem is that there exist monotone graph properties in /spl Nscr//spl Pscr/ for which testing is very hard (i.e., requires one to examine a constant fraction of the entries in the matrix). The second theorem is that every graph property that can be tested making a number of queries that is independent of the size of the graph, can be so tested by uniformly selecting a set of vertices and accepting iff the induced subgraph has some fixed graph property (which is not necessarily the same as the one being tested). The third theorem refers to the framework of graph partition problems, and is a characterization of the subclass of properties that can be tested using a one-sided error tester, making a number of queries that is independent of the size of the graph.
[graph property testing, YES-instances, complexity, NP, fixed graph property, graph theory, monotone graph properties, induced subgraph, probability, testing, one-sided error tester, Computer science, constant fraction, graph partition problems, predetermined property, adjacency matrix representation, decision problems, Testing, computational complexity]
A Ramsey-type theorem for metric spaces and its applications for metrical task systems and related problems
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The paper gives a nearly logarithmic lower bound on the randomized competitive ratio for a Metrical Task Systems model (A. Borodin et al., 1992). This implies a similar lower bound for the extensively studied K-server problem. Our proof is based on proving a Ramsey-type theorem for metric spaces. In particular, we prove that in every metric space there exists a large subspace which is approximately a "hierarchically well-separated tree" (HST) (Y. Bartal, 1996). This theorem may be of independent interest.
[Algorithm design and analysis, trees (mathematics), Switches, K-server problem, HST, Extraterrestrial measurements, randomized competitive ratio, metric spaces, large subspace, metrical task systems, hierarchically well-separated tree, Upper bound, online operation, Ramsey-type theorem, nearly logarithmic lower bound, Cost function, Performance analysis, Space exploration, theorem proving, proof, computational complexity]
Tight approximation results for general covering integer programs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
In this paper we study approximation algorithms for solving a general covering integer program. An n-vector x of nonnegative integers is sought, which minimizes c/sup T//spl middot/x, subject to Ax/spl ges/b, x/spl les/d. The entries of A, b, c are nonnegative. Let m be the number of rows of A. Covering problems have been heavily studied in combinatorial optimization. We focus on the effect of the multiplicity constraints, x/spl les/d, on approximately. Two longstanding open questions remain for this general formulation with upper bounds on the variables. (i) The integrality gap of the standard LP relaxation is arbitrarily large. Existing approximation algorithms that achieve the well-known O(log m)-approximation with respect to the LP value do so at the expense of violating the upper bounds on the variables by the same O(log m) multiplicative factor. What is the smallest possible violation of the upper bounds that still achieves cost within O(log m) of the standard LP optimum? (ii) The best known approximation ratio for the problem has been O(log(max/sub j//spl Sigma//sub i/A/sub ij/)) since 1982. This bound can be as bad as polynomial in the input size. Is an O(log m)-approximation, like the one known for the special case of Set Cover, possible? We settle these two open questions. To answer the first question we give an algorithm based on the relatively simple new idea of randomly rounding variables to smaller-than-integer units. To settle the second question we give a reduction from approximating the problem while respecting multiplicity constraints to approximating the problem with a bounded violation of the multiplicity constraints.
[multiplicity constraints, integer programming, randomly rounding variables, Software algorithms, computational geometry, approximation algorithms, nonnegative integers, combinatorial optimization, Upper bound, NP-hard problem, Chromium, Approximation algorithms, Cost function, Polynomials, general covering integer programs, computational complexity]
Counting axioms do not polynomially simulate counting gates
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We give a family of tautologies whose algebraic translations have constant-degree, polynomial size polynomial calculus refutations over Z/sub 2/, but which require superpolynomial size bounded-depth Frege proofs from Count/sub 2/ axioms. This gives a superpolynomial size separation of bounded-depth Frege plus mod 2 counting axioms from bounded-depth Frege plus parity gates. Combined with another result of the authors, it gives the first size (as opposed to degree) separation between the polynomial calculus and Nullstellensatz systems.
[polynomial calculus, bounded-depth Frege proofs, Computational modeling, Computer simulation, polynomials, algebraic translations, Count/sub 2/ axioms, Circuits, Counting Gates, Calculus, Complexity theory, Approximation methods, bounded-depth Frege, Computer science, formal logic, polynomial size polynomial calculus, Nullstellensatz systems, decision trees, tautologies, Polynomials, theorem proving, Decision trees, Counting Axioms]
Informational complexity and the direct sum problem for simultaneous message complexity
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Given m copies of the same problem, does it take m times the amount of resources to solve these m problems? This is the direct sum problem, a fundamental question that has been studied in many computational models. We study this question in the simultaneous message (SM) model of communication introduced by A.C. Yao (1979). The equality problem for n-bit strings is well known to have SM complexity /spl Theta/(/spl radic/n). We prove that solving m copies of the problem has complexity /spl Omega/(m/spl radic/n); the best lower bound provable using previously known techniques is /spl Omega/(/spl radic/(mn)). We also prove similar lower bounds on certain Boolean combinations of multiple copies of the equality function. These results can be generalized to a broader class of functions. We introduce a new notion of informational complexity which is related to SM complexity and has nice direct sum properties. This notion is used as a tool to prove the above results; it appears to be quite powerful and may be of independent interest.
[n-bit strings, Protocols, equality problem, Computational modeling, Boolean combinations, Complexity theory, communication complexity, simultaneous message complexity, equality function, direct sum properties, Computer science, best lower bound, National electric code, direct sum problem, Samarium, theorem proving, string matching, SM complexity, Books, informational complexity, simultaneous message model]
Spectral partitioning of random graphs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Problems such as bisection, graph coloring, and clique are generally believed hard in the worst case. However, they can be solved if the input data is drawn randomly from a distribution over graphs containing acceptable solutions. In this paper we show that a simple spectral algorithm can solve all three problems above in the average case, as well as a more general problem of partitioning graphs based on edge density. In nearly all cases our approach meets or exceeds previous parameters, while introducing substantial generality. We apply spectral techniques, using foremost the observation that in all of these problems, the expected adjacency matrix is a low rank matrix wherein the structure of the solution is evident.
[Algorithm design and analysis, Temperature, spectral techniques, bisection, computational geometry, random graphs, graph coloring, Partitioning algorithms, graph colouring, Computer science, spectral algorithm, clique, adjacency matrix, Simulated annealing, spectral partitioning]
Resolution is not automatizable unless W[P] is tractable
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We show that neither Resolution nor tree-like Resolution is automatizable unless the class W[P] from the hierarchy of parameterized problems is fixed-parameter tractable by randomized algorithms with one-sided error.
[tractable, parameterized problems, Quadratic programming, randomised algorithms, Computer science, Microwave integrated circuits, tree-like Resolution, interpolation, randomized algorithms, one-sided error, Polynomials, Cryptography, computational complexity]
How powerful is adiabatic quantum computation?
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The authors analyze the computational power and limitations of the recently proposed 'quantum adiabatic evolution algorithm'. Adiabatic quantum computation is a novel paradigm for the design of quantum algorithms; it is truly quantum in the sense that it can be used to speed up searching by a quadratic factor over any classical algorithm. On the question of whether this new paradigm may be used to efficiently solve NP-complete problems on a quantum computer, we show that the usual query complexity arguments cannot be used to rule out a polynomial time solution. On the other hand, we argue that the adiabatic approach may be thought of as a kind of 'quantum local search'. We design a family of minimization problems that is hard for such local search heuristics, and establish an exponential lower bound for the adiabatic algorithm for these problems. This provides insights into the limitations of this approach. It remains an open question whether adiabatic quantum computation can establish an exponential speed-up over traditional computing or if there exists a classical algorithm that can simulate the quantum adiabatic process efficiently.
[Algorithm design and analysis, Laboratories, local search heuristics, quantum local search, Quantum computing, Simulated annealing, NP-complete problems, Polynomials, polynomial time solution, Cryptography, adiabatic quantum computation, quantum adiabatic evolution algorithm, quantum algorithms, search problems, quantum computer, query complexity arguments, Computational modeling, Computer simulation, exponential lower bound, Computer science, Quantum mechanics, quantum computing, computational power, quadratic factor, minimization problems, minimisation, exponential speed-up, computational complexity]
Sequential and parallel algorithms for mixed packing and covering
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We describe sequential and parallel algorithms that approximately solve linear programs with no negative coefficients (aka mixed packing and covering problems). For explicitly given problems, our fastest sequential algorithm returns a solution satisfying all constraints within a 1/spl plusmn//spl epsi/ factor in O(mdlog(m)//spl epsi//sup 2/) time, where m is the number of constraints and d is the maximum number of constraints any variable appears in. Our parallel algorithm runs in time polylogarithmic in the input size times /spl epsi//sup -4/ and uses a total number of operations comparable to the sequential algorithm. The main contribution is that the algorithms solve mixed packing and covering problems (in contrast to pure packing or pure covering problems, which have only "/spl les/" or only "/spl ges/" inequalities, but not both) and run in time independent of the so-called width of the problem.
[parallel algorithms, Partitioning algorithms, Parallel algorithms, constraints, Lagrangian functions, Ellipsoids, Computer science, Constraint optimization, linear program solving, Linear approximation, Differential equations, Iterative algorithms, Polynomials, mixed packing and covering problems, computational complexity, sequential algorithms]
"Planar" tautologies hard for resolution
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We prove exponential lower bounds on the resolution proofs of some tautologies, based on rectangular grid graphs. More specifically, we show a 2/sup /spl Omega/(n)/ lower bound for any resolution proof of the mutilated chessboard problem on a 2n/spl times/2n chessboard as well as for the Tseitin tautology (G. Tseitin, 1968) based on the n/spl times/n rectangular grid graph. The former result answers a 35 year old conjecture by J. McCarthy (1964).
[graph theory, rectangular grid graphs, mutilated chessboard problem, Tseitin tautology, Graph theory, Computer science, exponential lower bounds, resolution proofs, rectangular grid graph, Bipartite graph, theorem proving, computational complexity, planar tautologies]
Lower bounds for quantum communication complexity
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We prove new lower bounds for bounded error quantum communication complexity. Our methods are based on the Fourier transform of the considered functions. First we generalize a method for proving classical communication complexity lower bounds developed by R. Raz (1995) to the quantum case. Applying this method we give an exponential separation between bounded error quantum communication complexity and nondeterministic quantum communication complexity. We develop several other Fourier based lower bound methods, notably showing that /spl radic/(s~(f)/log n) n, for the average sensitivity s~(f) of a function f, yields a lower bound on the bounded error quantum communication complexity of f (x/spl and/y/spl oplus/yz), where x is a Boolean word held by Alice and y, z are Boolean words held by Bob. We then prove the first large lower bounds on the bounded error quantum communication complexity of functions, for which a polynomial quantum speedup is possible. For all the functions we investigate, only the previously applied general lower bound method based on discrepancy yields bounds that are O(log n).
[Protocols, Quantum entanglement, bounded error quantum communication complexity, classical communication complexity lower bounds, polynomial quantum speedup, exponential separation, Fourier transform, Mechanical factors, Complexity theory, Boolean algebra, Application software, communication complexity, average sensitivity, Boolean word, Physics, lower bounds, Computer science, Quantum computing, nondeterministic quantum communication complexity, Quantum mechanics, Polynomials, theorem proving, quantum communication]
Unique sink orientations of cubes
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Suppose we are given (the edge graph of) an n-dimensional hypercube with its edges oriented so that every face has a unique sink. Such an orientation is called a unique sink orientation, and we are interested in finding the unique sink of the whole cube, when the orientation is given implicitly. The basic operation available is the so-called vertex evaluation, where we can access an arbitrary vertex of the cube, for which we obtain the orientations of the incident edges. Unique sink orientations occur when the edges of a deformed geometric n-dimensional cube (i.e., a polytope with the combinatorial structure of a cube) are oriented according to some generic linear function. These orientations are easily seen to be acyclic. The main motivation for studying unique sink orientations are certain linear complementarity problems, which allow this combinatorial abstraction (due to Stickney and Watson, 1978), where orientations with cycles can arise. Similarly, some quadratic optimization problems, like computing the smallest enclosing ball of a finite point set, can be formulated as finding a sink in a unique sink orientation (with cycles possible). For acyclic unique sink orientations, randomized procedures due to Bernd Gartner (1998, 2001) with an expected number of at Most e/sup 2/spl radic/n/ vertex evaluations have been known. For the general case, a simple randomized (3/2)/sup n/ procedure exists (without explicit mention in the literature). We present new algorithms, a deterministic O(1.61/sup n/) procedure and a randomized O((43/20)/sup n/2/)=O(1.47/sup n/) procedure for unique sink orientations. An interesting aspect of these algorithms is that they do not proceed on a path to the sink (in a simplex-like fashion), but they exploit the potential of random access (in the sense of arbitrary access) to any vertex of the cube. We consider this feature the main contribution of the paper. We believe that unique sink orientations have a rich structure, and there is ample space for improvement on the bounds given above.
[combinatorial mathematics, Lattices, computational geometry, Cost accounting, deterministic procedure, quadratic optimization problems, smallest enclosing ball, Hypercubes, finite point set, Labeling, linear complementarity problems, combinatorial abstraction, Combinatorial mathematics, deterministic algorithms, Radio access networks, randomised algorithms, Computer science, n-dimensional hypercube, Computational geometry, randomized procedures, Character generation, deformed geometric n-dimensional cube, random access, unique sink orientations, generic linear function, Artificial intelligence, vertex evaluation, computational complexity]
Planar graphs, negative weight edges, shortest paths, and near linear time
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The authors present an O(n log/sup 3/ n) time algorithm for finding shortest paths in a planar graph with real weights. This can be compared to the best previous strongly polynomial time algorithm developed by R. Lipton et al., (1978 )which ran in O(n/sup 3/2/) time, and the best polynomial algorithm developed by M. Henzinger et al. (1994) which ran in O/spl tilde/(n/sup 4/3/) time. We also present significantly improved algorithms for query and dynamic versions of the shortest path problems.
[Particle separators, Scholarships, graph theory, planar graphs, dynamic versions, Partitioning algorithms, negative weight edges, Radio access networks, Shortest path problem, Computer science, Image segmentation, optimisation, Tree graphs, best previous strongly polynomial time algorithm, shortest path problems, Polynomials, Bipartite graph, query versions, near linear time, real weights, computational complexity]
The confluence of ground term rewrite systems is decidable in polynomial time
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The confluence property of ground (i.e., variable-free) term rewrite systems (GTRS) is well-known to be decidable. This was proved independently by M. Dauchet et al. (1987; 1990) and by M. Oyamaguchi (1987) using tree automata techniques and ground tree transducer techniques (originated from this problem), yielding EXPTIME decision procedures (PSPACE for strings). Since then, it has been a well-known longstanding open question whether this bound is optimal. The authors give a polynomial-time algorithm for deciding the confluence of GTRS, and hence alsofor the particular case of suffix- and prefix string rewrite systems or Thue systems. We show that this bound is optimal for all these problems by proving PTIME-hardness for the string case. This result may have some impact on other areas of formal language theory, and in particular on the theory of tree automata.
[automata theory, Formal languages, confluence property, GTRS, optimisation, Algebra, decidability, EXPTIME decision procedures, prefix string rewrite systems, Polynomials, polynomial time, theorem proving, formal language theory, rewriting systems, Transducers, ground term rewrite systems, trees (mathematics), ground tree transducer techniques, PSPACE, suffix-string rewrite systems, Computer science, Computer languages, Thue systems, tree automata techniques, Automata, PTIME-hardness, computational complexity]
Arc-disjoint paths in expander digraphs
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
Given a digraph D=(V, A) and a set of /spl kappa/ pairs of vertices in V, we are interested in finding for each pair (x/sub i/, y/sub i/), a directed path connecting x/sub i/ to y/sub i/, such that the set of /spl kappa/ paths so found is arc-disjoint. For arbitrary graphs, the problem is /spl Nscr//spl Pscr/-complete, even for /spl kappa/=2. We present a polynomial time randomized algorithm for finding arc-disjoint paths in an r-regular expander digraph D. We show that if D has sufficiently strong expansion properties and r is sufficiently large, then all sets of /spl kappa/=/spl Omega/(n/log n) pairs of vertices can be joined. This is within a constant factor of best possible.
[arbitrary graphs, NP-complete, set theory, strong expansion properties, randomised algorithms, directed path, arc-disjoint paths, directed graphs, r-regular expander digraph, expander digraphs, Polynomials, Joining processes, polynomial time randomized algorithm, computational complexity]
On the average-case hardness of CVP
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
We prove a connection of the worst-case complexity to the average-case complexity based on the Closest Vector Problem (CVP) for lattices. We assume that there is an efficient algorithm which can approximately solve a random instance of CVP, with a non-trivial success probability. For lattices under a certain natural distribution, we show that one can approximately solve several lattice problems (including a version of CVP) efficiently for every lattice with high probability.
[Lattices, probability, average-case hardness, Linear programming, Probability distribution, worst-case complexity, Closest Vector Problem, non-trivial success probability, Distributed computing, Tellurium, Computer science, average-case complexity, random instance, vectors, lattice problems, natural distribution, Councils, Polynomials, CVP, computational complexity]
Approximating directed multicuts
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The seminal paper of F.T. Leighton and S. Rao (1988) and subsequent papers presented approximate min-max theorems relating multicommodity flow values and cut capacities in undirected networks, developed the divide-and-conquer method for designing approximation algorithms, and generated novel tools for utilizing linear programming relaxations. Yet, despite persistent research efforts, these achievements could not be extended to directed networks, excluding a few cases that are "symmetric" and therefore similar to undirected networks. The paper is an attempt to remedy the situation. We consider the problem of finding a minimum multicut in a directed multicommodity flow network, and give the first nontrivial upper bounds on the maxflow-to-min multicut ratio. Our results are algorithmic, demonstrating nontrivial approximation guarantees.
[Algorithm design and analysis, Design methodology, cut capacities, directed multicommodity flow network, linear programming, approximation algorithms, minimax techniques, directed networks, divide-and-conquer method, nontrivial approximation guarantees, linear programming relaxations, Contracts, approximation theory, directed multicut approximation, max flow-to-min multicut ratio, flow graphs, Linear programming, Statistics, Combinatorial mathematics, Computer science, Upper bound, approximate min-max theorems, nontrivial upper bounds, Chromium, Approximation algorithms, undirected networks, multicommodity flow values]
Facility location with nonuniform hard capacities
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
The authors give the first constant factor approximation algorithm for the facility location problem with nonuniform, hard capacities. Facility location problems have received a great deal of attention in recent years. Approximation algorithms have been developed for many variants. Most of these algorithms are based on linear programming, but the LP techniques developed thus far have been unsuccessful in dealing with hard capacities. A local-search based approximation algorithm (M. Korupolu et al., 1998; F.A. Chudak and D.P. Williamson, 1999) is known for the special case of hard but uniform capacities. We present a local-search heuristic that yields an approximation guarantee of 9 + /spl epsi/ for the case of nonuniform hard capacities. To obtain this result, we introduce new operations that are natural in this context. Our proof is based on network flow techniques.
[approximation theory, local-search heuristic, local-search based approximation algorithm, Linear programming, linear programming, LP techniques, uniform capacities, facility location, approximation guarantee, heuristic programming, nonuniform hard capacities, special case, Writing, network flow techniques, Approximation algorithms, Cost function, proof, search problems, Capacity planning, computational complexity, constant factor approximation algorithm]
Coding theory: tutorial &amp; survey
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
false
[Computer science, Tutorial, Codes, Chromium]
Fast Monte-Carlo algorithms for approximate matrix multiplication
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
false
[Character generation]
S/sub 2/ ZPP/sup NP/
Proceedings 2001 IEEE International Conference on Cluster Computing
None
2001
false
[Computer science, Upper bound, Circuits, Sampling methods, Polynomials]
Learning intersections and thresholds of halfspaces
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We give the first polynomial time algorithm to learn any function of a constant number of halfspaces under the uniform distribution to within any constant error parameter. We also give the first quasipolynomial time algorithm for learning any function of a polylog number of polynomial-weight halfspaces under any distribution. As special cases of these results we obtain algorithms for learning intersections and thresholds of halfspaces. Our uniform distribution learning algorithms involve a novel non-geometric approach to learning halfspaces; we use Fourier techniques together with a careful analysis of the noise sensitivity of functions of halfspaces. Our algorithms for learning under any distribution use techniques from real approximation theory to construct low degree polynomial threshold functions.
[Algorithm design and analysis, Machine learning algorithms, Fourier techniques, Fourier analysis, Probability distribution, Mathematics, Approximation methods, polynomial time algorithm, uniform distribution learning algorithms, Computer science, Boolean functions, polylog number, quasipolynomial time algorithm, Machine learning, Approximation algorithms, Polynomials, low degree polynomial threshold functions, learning (artificial intelligence), constant error parameter, noise sensitivity, computational complexity, polynomial-weight halfspaces]
Erratum to "Vickrey pricing and shortest paths: What is an edge worth?"
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
false
[Shortest path problem, Costs, Computer graphics]
On the (non)universality of the one-time pad
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Randomization is vital in cryptography: secret keys should be randomly generated and most cryptographic primitives (e.g., encryption) must be probabilistic. We initiate the quantitative study concerning feasibility of building secure cryptographic primitives using imperfect random sources. Specifically, we concentrate on symmetric-key encryption and message authentication, where the shared secret key comes from an imperfect random source instead of being assumed truly random. In each case, we compare the class of "cryptographic" sources for the task at hand with the classes of "extractable" and "simulatable" sources, where: (1) "cryptographic" refers to sources for which the corresponding symmetric-key primitive can be built; (2) "extractable" refers to a very narrow class of sources from which one can extract nearly perfect randomness; and (3) "simulatable" refers to a very general class of weak random sources which are known to suffice for BPP simulation. For both encryption and authentication, we show that the corresponding cryptographic sources lie strictly in between extractable and simulatable sources, which implies that "cryptographic usage" of randomness is more demanding than the corresponding "algorithmic usage\
[imperfect random source, shared secret key, simulatable sources, simulation, cryptographic sources, secret keys, History, Distributed computing, one-time pad universality, symmetric-key encryption, probabilistic algorithms, encryption, randomization, Atherosclerosis, Cryptography, Message authentication, BPP simulation, Testing, general cryptographic use, Computational modeling, cryptographic primitives, random processes, cryptography, nearly perfect randomness, secure cryptographic primitives, Cryptographic protocols, Computer science, imperfect random sources, message authentication, weak random sources, Approximation algorithms, symmetric-key primitive, extractable sources]
LT codes
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We introduce LT codes, the first rateless erasure codes that are very efficient as the data length grows.
[codes, reliable transport, Costs, universal codes, LT codes, Encoding, Decoding, rateless codes, encoding, randomised algorithms, rateless erasure codes, Computer science, Aggregates, randomized algorithms, erasure codes, Tornadoes, balls and bins]
Dependent rounding in bipartite graphs
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We combine the pipage rounding technique of Ageev & Sviridenko with a recent rounding method developed by Srinivasan (2001), to develop a new randomized rounding approach for fractional vectors defined on the edge-sets of bipartite graphs. We show various ways of combining this technique with other ideas, leading to the following applications: richer random-graph models for graphs with a given degree-sequence; improved approximation algorithms for: (i) throughput-maximization in broadcast scheduling, (ii) delay-minimization in broadcast scheduling, and (iii) capacitated vertex cover; fair scheduling of jobs on unrelated parallel machines. A useful feature of our method is that it lets us prove certain (probabilistic) per-user fairness properties.
[edge-sets, graph theory, approximation algorithms, telecommunication computing, Delay, processor scheduling, bipartite graphs, Broadcasting, scheduling, capacitated vertex cover, Bipartite graph, random-graph models, broadcast scheduling, fair scheduling, randomized rounding approach, Educational institutions, Application software, Scheduling algorithm, Computer science, rounding method, unrelated parallel machines, Character generation, Approximation algorithms, pipage rounding technique, fractional vectors, Random variables, broadcast channels, computational complexity, per-user fairness properties]
Concurrent zero knowledge with logarithmic round-complexity
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We show that every language in NP has a (black-box) concurrent zero-knowledge proof system using O/spl tilde/(log n) rounds of interaction. The number of rounds in our protocol is optimal, in the sense that any language outside BPP requires at least /spl Omega//spl tilde/(log n) rounds of interaction in order to be proved in black-box concurrent zero-knowledge. The zero-knowledge property of our main protocol is proved under the assumption that there exists a collection of claw free functions. Assuming only the existence of one-way functions, we show the existence of O/spl tilde/(log n)-round concurrent zero-knowledge arguments for all languages in NP.
[Computational modeling, Computer simulation, Buildings, Access protocols, black-box concurrent zero-knowledge proof system, Cryptographic protocols, Computer science, Analytical models, main protocol, claw free functions, Interleaved codes, Polynomials, Internet, theorem proving, protocols, one-way functions, computational complexity]
The hardness of 3-uniform hypergraph coloring
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We prove that coloring a 3-uniform 2-colorable hypergraph with any constant number of colors is NP-hard. The best known algorithm (Krivelevich, Nathaniel, and Sudakov, 2001)colors such a graph using O(n/sup 1/5/) colors. Our result immediately implies that for any constants k > 2 and c/sub 2/ > c/sub 1/ > 1, coloring a k-uniform c/sub 1/-colorable hypergraph with c/sub 2/ colors is NP-hard; leaving completely open only the k = 2 graph case. We are the first to obtain a hardness result for approximately-coloring a 3-uniform hypergraph that is colorable with a constant number of colors. For k /spl ges/ 4 such a result has been shown by Guruswami et al. (2000), who also discussed the inherent difference between the k = 3 case and k /spl ges/ 4. Our proof presents a new connection between the Long-Code and the Kneser graph, and relies on the high chromatic numbers of the Kneser graph (Kneser, 1955; Lovasz, 1978) and the Schrijver graph (Schrijver, 1978). We prove a certain maximization variant of the Kneser conjecture, namely that any coloring of the Kneser graph by fewer colors than its chromatic number, has 'many' non-monochromatic edges.
[Long-Code, 3-uniform 2-colorable hypergraph coloring, NP-hard, Law, maximization variant, 3-uniform hypergraph coloring hardness, chromatic numbers, Kneser graph, Combinatorial mathematics, graph colouring, Schrijver graph, Kneser conjecture, Computer science, Upper bound, optimisation, Chromium, Approximation algorithms, Polynomials, k-uniform c/sub 1/-colorable hypergraph, proof, nonmonochromatic edges, Legal factors, computational complexity]
Breaking the O(n/sup 1/(2k-1)/) barrier for information-theoretic Private Information Retrieval
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Private information retrieval (PIR) protocols allow a user to retrieve a data item from a database while hiding the identity of the item being retrieved. Specifically, in information-theoretic, k-server PIR protocols the database is replicated among k servers, and each server learns nothing about the item the user retrieves. The cost of such protocols is measured by the communication complexity of retrieving one out of n bits of data. For any fixed k, the complexity of the best protocols prior to our work was O(n/sup 1/2k-1/). Since then several methods were developed in an attempt to beat this bound, but all these methods yielded the same asymptotic bound. In this paper, this barrier is finally broken and the complexity of information-theoretic k-server PIR is improved to n/sup O(log log k/k log k)/. The new PIR protocols can also be used to construct k-query binary locally decodable codes of length exp(n/sup O(log log k/k log k)/), compared to exp(n/sup 1/k-1/) in previous constructions. The improvements presented in this paper apply even for small values of k: the PIR protocols are more efficient than previous ones for every k/spl ges/3, and the locally decodable codes are shorter for every k/spl ges/4.
[Scholarships, polynomials, Access protocols, information retrieval, Information retrieval, Decoding, Complexity theory, communication complexity, Upper bound, database, Databases, data item, k-server PIR protocols, protocols, information-theoretic private information retrieval]
Proving integrality gaps without knowing the linear program
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Proving integrality gaps for linear relaxations of NP optimization problems is a difficult task and usually undertaken on a case-by-case basis. We initiate a more systematic approach. We prove an integrality gap of 2-o(1) for three families of linear relaxations for vertex cover, and our methods seem relevant to other problems as well.
[NP optimization problems, vertex cover, Computational modeling, graph theory, linear relaxations, Educational institutions, Linear programming, linear program, linear programming, Ellipsoids, Computer science, Upper bound, NP-hard problem, integrality gap proving, Approximation algorithms, Cost function, Polynomials]
Generalized compact knapsacks, cyclic lattices, and efficient one-way functions from worst-case complexity assumptions
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We study a generalization of the compact knapsack problem for arbitrary rings: given m = O(log n) ring elements a/sub 1/, . . . , a/sub m/ /spl isin/ R and a target value b /spl isin/ R, find coefficients x/sub 1/, . . . , x/sub m/ /spl isin/ X (where X is a subset of R of size 2/sup n/) such that /spl Sigma/a/sub i/x/sub i/ = b. The computational complexity of this problem depends on the choice of the ring R and set of coefficients X. This problem is known to be solvable in quasi polynomial time when R is the ring of the integers and X is the set of small integers {0, . . . , 2/sup n/ $1}. We show that if R is an appropriately chosen ring of modular polynomials and X is the subset of polynomials with small coefficients, then the compact knapsack problem is as hard to solve on the average as the worst case instance of approximating the covering radius (or the length of the shortest vector, or various other well known lattice problems) of any cyclic lattice within a polynomial factor. Our proof adapts, to the cyclic lattice setting, techniques initially developed by Ajtai (1996) for the case of general lattices.
[Engineering profession, Lattices, worst-case average-case connection, cryptography, cyclic lattices, efficient one-way functions, Security, Proposals, Computational complexity, knapsack problems, worst-case complexity assumptions, quasi polynomial time, NP-hard problem, Character generation, Public key cryptography, Polynomials, generalized compact knapsacks, modular polynomials, Testing, computational complexity]
Hardness results for coloring 3-colorable 3-uniform hypergraphs
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider the problem of coloring a 3-colorable 3-uniform hypergraph. In the minimization version of this problem, given a 3-colorable 3-uniform hypergraph, one seeks an algorithm to color the hypergraph with as few colors as possible. We show that it is NP-hard to color a 3-colorable 3-uniform hypergraph with constantly many colors. In fact, we show a stronger result that it is NP-hard to distinguish whether a 3-uniform hypergraph with n vertices is 3-colorable or it contains no independent set of size /spl delta/n for an arbitrarily small constant /spl delta/ > 0. In the maximization version of the problem, given a 3-uniform hypergraph, the goal is to color the vertices with 3 colors so as to maximize the number of non-monochromatic edges. We show that it is NP-hard to distinguish whether a 3-uniform hypergraph is 3-colorable or any coloring of the vertices with 3 colors has at most 8/9 + /spl epsi/ fraction of the edges nonmonochromatic where /spl epsi/ > 0 is an arbitrarily small constant. This result is tight since assigning a random color independently to every vertex makes 8/9 fraction of the edges non-monochromatic. These results are obtained via a new construction of a probabilistically checkable proof system (PCP) for NP. We develop a new construction of the PCP Outer Verifier. An important feature of this construction is smoothening of the projection maps. Dinur, Regev and Smyth (2002) independently showed that it is NP-hard to color a 2-colorable 3-uniform hypergraph with constantly many colors. In the "good case\
[Minimization methods, NP-hard, probabilistically checkable proof system, PCP Outer Verifier, hardness results, graph colouring, vertex coloring, Computer science, 3-colorable 3-uniform hypergraph coloring, projection maps, arbitrarily small constant, theorem proving, minimization version, minimisation, nonmonochromatic edges, Testing, computational complexity, random color]
Decoding turbo-like codes via linear programming
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We introduce a novel algorithm for decoding turbo-like codes based on linear programming. We prove that for the case of repeat-accumulate (RA) codes, under the binary symmetric channel with a certain constant threshold bound on the noise, the error probability of our algorithm is bounded by an inverse polynomial in the code length. Our linear program (LP) minimizes the distance between the received bits and binary variables representing the code bits. Our LP is based on a representation of the code where code words are paths through a graph. Consequently, the LP bears a strong resemblance to the min-cost flow LP. The error bounds are based on an analysis of the probability, over the random noise of the channel, that the optimum solution to the LP is the path corresponding to the original transmitted code word.
[Error probability, Error analysis, turbo-like codes decoding, error bounds, Laboratories, repeat-accumulate codes, error probability, Linear programming, linear programming, inverse polynomial, Iterative decoding, decoding, Maximum likelihood decoding, Computer science, turbo codes, Turbo codes, Polynomials, Contracts, error statistics]
Improved dynamic reachability algorithms for directed graphs
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We obtain several new dynamic algorithms for maintaining the transitive closure of a directed graph, and several other algorithms for answering reachability queries without explicitly maintaining a transitive closure matrix. Among our algorithms are: (i) a decremental algorithm for maintaining the transitive closure of a directed graph, through an arbitrary sequence of edge deletions, in O(mn) total expected time, essentially the time needed for computing the transitive closure of the initial graph. Such a result was previously known only for acyclic graphs; (ii) two fully dynamic algorithms for answering reachability queries. The first is deterministic and has an amortized insert/delete time of O(m/spl radic/n), and worst-case query time of O(/spl radic/n). The second is randomized and has an amortized insert/delete time of O(m/sup 0.58/n) and worst-case query time of O(m/sup 0.43/). This significantly improves the query times of algorithms with similar update times; and (iii) a fully dynamic algorithm for maintaining the transitive closure of an acyclic graph. The algorithm is deterministic and has a worst-case insert time of O(m), constant amortized delete time of O(1), and a worst-case query time of O(n/ log n). Our algorithms are obtained by combining several new ideas, one of which is a simple sampling idea used for detecting decompositions of strongly connected components, with techniques of Even and Shiloach (1981), Italiano (1988), Henzinger and King (1995), and Frigioni et al. (2001). We also adapt results of Cohen (1997) on estimating the size of the transitive closure to the dynamic setting.
[strongly connected component decomposition detection, Costs, reachability analysis, Heuristic algorithms, expected time, randomized algorithm, deterministic algorithm, deterministic algorithms, randomised algorithms, Computer science, Monte Carlo methods, edge deletions, reachability query answering, directed graphs, decremental algorithm, Sampling methods, amortized insert/delete time, worst-case query time, transitive closure, dynamic reachability algorithms, computational complexity]
On-line end-to-end congestion control
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Congestion control in the current Internet is accomplished mainly by TCP/IP. To understand the macroscopic network behavior that results from TCP/IP and similar end-to-end protocols, one main analytic technique is to show that the the protocol maximizes some global objective function of the network traffic. We analyze a particular end-to-end MIMD (multiplicative-increase, multiplicative-decrease) protocol. We show that if all users of the network use the protocol, and all connections last for at least logarithmically many rounds, then the total weighted throughput (value of all packets received) is near the maximum possible. Our analysis includes round-trip-times, and (in contrast to most previous analyses) gives explicit convergence rates, allows connections to start and stop, and allows capacities to change.
[macroscopic network behavior, Protocols, telecommunication congestion control, Telecommunication traffic, end-to-end protocols, end-to-end multiplicative increase multiplicative decrease protocol, Throughput, Convergence, explicit convergence rates, TCPIP, TCP/IP, Communication system traffic control, capacities, IP networks, round trip times, connections, total weighted throughput, H infinity control, global objective function, on-line end-to-end congestion control, network traffic, transport protocols, Internet, telecommunication traffic, Propagation delay]
Locally testable codes and PCPs of almost-linear length
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Locally testable codes are error-correcting codes that admit very efficient codeword tests. Specifically, using a constant number of (random) queries, noncodewords are rejected with probability proportional to their distance from the code. Locally testable codes are believed to be the combinatorial core of PCPs. However, the relation is less immediate than commonly believed. Nevertheless, we show that certain PCP systems can be modified to yield locally testable codes. On the other hand, we adapt techniques we develop for the construction of the latter to yield new PCPs. Our main results are locally testable codes and PCPs of almost-linear length. Specifically, we present: 1. Locally testable (linear) codes in which k information bits are encoded by a codeword of length approximately k /spl middot/ exp(/spl radic/(log)). This improves over previous results that either yield codewords of exponential length or obtained almost quadratic length codewords for sufficiently large non-binary alphabet. 2. PCP systems of almost-linear length for SAT. The length of the proof is approximately n /spl middot/ exp(/spl radic/(log n)) and verification in performed by a constant number (i.e., 19) of queries, as opposed to previous results that used proof length n/sup 1+O(1/q)/ for verification by q queries. The novel techniques in use include a random projection of certain codewords and PCP-oracles, an adaptation of PCP constructions to obtain "linear PCP-oracles" for proving conjunctions of linear conditions, and a direct construction of locally testable (linear) codes of sub-exponential length.
[combinatorial core, System testing, error correction codes, combinatorial mathematics, linear codes, proof length, codeword, Laboratories, quadratic length codewords, SAT, locally testable codes, information bits, nonbinary alphabet, Computer science, error-correcting codes, Linearity, almost-linear length PCPs, Polynomials, Error correction codes, Error correction, codeword tests, noncodewords]
On random symmetric travelling salesman problems
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Let the edges of the complete graph K/sub n/ be assigned independent uniform [0,1] random edge weights. Let Z/sub TSP/ and Z/sub 2FAC/ be the weights of the minimum length travelling salesman tour and minimum weight 2-factor respectively. We show that whp/sup 1/ |Z/sub TSP/-Z/sub 2FAC/|=(1). The proof is via the analysis of a polynomial time algorithm that finds a tour only a little longer than Z/sub 2FAC/.
[Computer science, travelling salesman problems, random symmetric travelling salesman problems, polynomials, minimum weight 2-factor, independent uniform random edge weights, probability, complete graph, Artificial intelligence, polynomial time algorithm]
Dynamic planar convex hull
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
In this paper we determine the computational complexity of the dynamic convex hull problem in the planar case. We present a data structure that maintains a finite set of n points in the plane under insertion and deletion of points in amortized O(log n) time per operation. The space usage of the data structure is O(n). The data structure supports extreme point queries in a given direction, tangent queries through a given point, and queries for the neighboring points on the convex hull in O(log n) time. The extreme point queries can be used to decide whether or not a given line intersects the convex hull, and the tangent queries to determine whether a given point is inside the convex hull. We give a lower bound on the amortized asymptotic time complexity that matches the performance of this data structure.
[Tree data structures, data structure, computational geometry, Data structures, Application software, tree searching, Computational complexity, Computer science, Jacobian matrices, planar computational geometry, Computational geometry, amortized asymptotic time complexity, tangent queries, finger searches, Fingers, search trees, data structures, dynamic convex hull problem, Contracts, Clocks, computational complexity]
Correlation clustering
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider the following clustering problem: we have a complete graph on n vertices (items), where each edge (u, /spl upsi/) is labeled either + or - depending on whether a and /spl upsi/ have been deemed to be similar or different. The goal is to produce a partition of the vertices (a clustering) that agrees as much as possible with the edge labels. That is, we want a clustering that maximizes the number of + edges within clusters, plus the number of - edges between clusters (equivalently, minimizes the number of disagreements: the number of - edges inside clusters plus the number of + edges between clusters). This formulation is motivated from a document clustering problem in which one has a pairwise similarity function f learned from past data, and the goal is to partition the current set of documents in a way that correlates with f as much as possible; it can also be viewed as a kind of "agnostic learning" problem. An interesting feature of this clustering formulation is that one does not need to specify the number of clusters k as a separate parameter, as in measures such as k-median or min-sum or min-max clustering. Instead, in our formulation, the optimal number of clusters could be any value between 1 and n, depending on the edge labels. We look at approximation algorithms for both minimizing disagreements and for maximizing agreements. For minimizing disagreements, we give a constant factor approximation. For maximizing agreements we give a PTAS. We also show how to extend some of these results to graphs with edge labels in [-1, +1], and give some results for the case of random noise.
[pairwise similarity function, min-sum clustering, approximation theory, edge labels, probability, approximation algorithms, randomised algorithms, correlation clustering, Computer science, Databases, min-max clustering, Clustering algorithms, Training data, Approximation algorithms, complete graph]
Power from random strings
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We show that sets consisting of strings of high Kolmogorov complexity provide examples of sets that are complete for several complexity classes under probabilistic and non-uniform reductions. These sets are provably not complete under the usual many-one reductions. Let R/sub K/, R/sub Kt/, R/sub KS/, R/sub KT/ be the sets of strings x having complexity at least |x|/2, according to the usual Kolmogorov complexity measure K, Levin's time-bounded Kolmogorov complexity Kt [27], a space-bounded Kolmogorov measure KS, and the time-bounded Kolmogorov complexity measure KT that was introduced in [4], respectively. Our main results are: 1. R/sub KS/ and R/sub Kt/ are complete for PSPACE and EXP, respectively, under P/poly-truth-table reductions. 2. EXP = NP/sup R(Kt)/. 3. PSPACE = ZPP/sup R(KS)/ /spl sube/ P/sup R(K)/. 4. The Discrete Log, Factoring, and several lattice problems are solvable in BPP/sup R(KT)/.
[high Kolmogorov complexity, Engineering profession, complexity classes, time-bounded Kolmogorov complexity measure, Circuits, Lattices, set theory, PSPACE, space-bounded Kolmogorov measure, nonuniform reductions, Polynomials, EXP, Artificial intelligence, probabilistic reductions, computational complexity]
Scheduling over a time-varying user-dependent channel with applications to high speed wireless data
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
In a wireless network, a basestation transmits data to mobiles at time-varying, mobile-dependent rates due to the ever changing nature of the communication channels. In this paper we consider a wireless system in which the channel conditions and data arrival processes are governed by an adversary. We first consider a single server and a set of users. At each time step t the server can only transmit data to one user. If user i is chosen the transmission rate is r/sub i/(t). We say that the system is (/spl omega/, /spl epsiv/)-admissible if in any window of /spl omega/ time steps the adversary can schedule the users so that the total data arriving to each user is at most 1 - /spl epsiv/ times the total service it receives. Our objective is to design on-line scheduling algorithms to ensure stability in an admissible system. We first show, somewhat surprisingly, that the admissibility condition alone does not guarantee the existence of a stable online algorithm, even in a subcritical system (i.e. /spl epsiv/ > 0). For example, if the nonzero rates in an infinite rate set can be arbitrarily small, then a subcritical system can be unstable for any deterministic online algorithm. On a positive note, we present a tracking algorithm that attempts to mimic the behavior of the adversary. This algorithm ensures stability for all (/spl omega/, /spl epsiv/)-admissible systems that are not excluded by our instability results. As a special case, if the rate set is finite, then the tracking algorithm is stable even for a critical system (i.e. /spl epsiv/ = 0). Moreover, the queue sizes are independent of e. For subcritical systems, we also show that a simpler max weight algorithm is stable as long as the user rates are bounded away from zero. The offline version of our problem resembles the problem of scheduling unrelated machines and can be modeled by an integer program. We present a rounding algorithm for its linear relaxation and prove that the rounding technique cannot be substantially improved. We conclude by discussing the extension of our model to the network setting.
[Algorithm design and analysis, Process design, server, on-line scheduling, Stability, Mobile communication, rounding algorithm, Scheduling algorithm, radio access networks, Network servers, Wireless networks, admissible system, Communication channels, scheduling machines, scheduling, packet scheduling, Data communication, wireless network, multiple queues, Web server]
Constant-round coin-tossing with a man in the middle or realizing the shared random string model
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We present the first constant-round non-malleable commitment scheme and the first constant-round non-malleable zero-knowledge argument system, as defined by Dolev, Dwork and Naor (1991). Previous constructions either used a non-constant number of rounds, or were only secure under stronger setup assumptions. An example of such an assumption is the shared random string model where we assume all parties have access to a reference string that was chosen uniformly at random by a trusted dealer. We obtain these results by defining an adequate notion of non-malleable coin-tossing, and presenting a constant-round protocol that satisfies it. This protocol allows us to transform protocols that are non-malleable in (a modified notion of) the shared random string model into protocols that are non-malleable in the plain model (without any trusted dealer or setup assumptions). Observing that known constructions of a non-interactive non-malleable zero-knowledge argument systems in the shared random string model (De Santis et. al., 2001) are in fact non-malleable in the modified model, and combining them with our coin-tossing protocol we obtain the results mentioned above. The techniques we use are different from those used in previous constructions of non-malleable protocols. In particular our protocol uses diagonalization and a non-black-box proof of security (in a sense similar to Barak's zero-knowledge argument (2001)).
[Terminology, Communication system control, probability, Access protocols, Security, Cryptographic protocols, Computer science, constant-round nonmalleable zero-knowledge argument system, security of data, diagonalization, transport protocols, constant-round nonmalleable commitment scheme, setup assumptions, Communication channels, Cryptography, shared random string model]
Small induced-universal graphs and compact implicit graph representations
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We show that there exists a graph G with n /spl middot/ 2/sup O(log* n)/ nodes, where any forest with n nodes is a node-induced subgraph of G. Furthermore, the result implies the existence of a graph with n/sup k/2/sup O(log* n)/ nodes that contains all n-node graphs of fixed arboricity k as node-induced subgraphs. We provide a lower bound of /spl Omega/(n/sup k/) for the size of such a graph. The upper bound is obtained through a simple labeling scheme for parent queries in rooted trees.
[simple labeling scheme, Terminology, graph theory, node-induced subgraph, lower bound, Sparse matrices, bibliographies, small induced-universal graphs, rooted trees, n-node graphs, Upper bound, Tree graphs, parent queries, compact implicit graph representations, Labeling, Artificial intelligence, fixed arboricity, Testing, computational complexity]
Explicit unique-neighbor expanders
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We present a simple, explicit construction of an infinite family F of bounded-degree 'unique-neighbor' expanders /spl Gamma/; i.e., there are strictly positive constants /spl alpha/ and /spl epsi/, such that all /spl Gamma/ = (X, E(/spl Gamma/)) /spl isin/ F satisfy the following property. For each subset S of X with no more than /spl alpha/|X| vertices, there are at least /spl epsi/|S| vertices in X/spl bsol/S that are adjacent in /spl Gamma/ to exactly one vertex in S. The construction of F is simple to specify, and each /spl Gamma/ /spl isin/ F is 6-regular. We then extend the technique and present easy to describe explicit infinite families of 4-regular and 3-regular unique-neighbor expanders, as well as explicit families of bipartite graphs with nonequal color classes and similar properties. This has several applications and settles an open problem considered by various researchers.
[Algorithm design and analysis, bounded degree unique-neighbor expanders, graph theory, vertices, explicit unique neighbor expanders, Routing, Graph theory, Mathematics, Parallel algorithms, explicit infinite families, Geometry, bipartite graphs, nonequal color classes, 4-regular unique neighbor expanders, Bipartite graph, Distributed algorithms, 3-regular unique neighbor expanders]
Randomness extractors and their many guises
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Since its introduction by Nisan and Zuckerman at STOC '93 (1996) nearly a decade ago, the notion of a randomness extractor has proven to be a fundamental and powerful one. Extractors and their variants have found widespread application in a variety of areas, including pseudorandomness and derandomization, combinatorics, cryptography, data structures, and computational complexity. Equally striking has been a sequence of discoveries showing that, under different interpretations, extractors are close relatives of a number of other important objects, such as expander graphs, hash functions, error-correcting codes, pseudorandom generators, and sampling algorithms. Through these connections, extractors have unified the study of these objects and have led to new and improved constructions of each. We give an introduction to the study of extractors. The article is built around the connections between extractors and the other objects mentioned above. Within the context of these connections, we hope to convey an understanding of the definition of extractors, some intuition for how they are constructed, and a glimpse of their use in applications.
[pseudorandom generators, graph theory, hash functions, Data mining, data structures, pseudorandomness, Cryptography, combinatorics, random processes, Data structures, cryptography, Graph theory, derandomization, Combinatorial mathematics, Computational complexity, randomness extractors, randomised algorithms, Computer science, error-correcting codes, Sampling methods, Error correction codes, Random variables, expander graphs, sampling algorithms, computational complexity]
Graph isomorphism is in SPP
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We show that graph isomorphism is in the complexity class SPP and hence it is in /spl oplus/P (in fact, it is in Mod/sub k/P for each k/spl ges/2). We derive this result as a corollary of a more general result: we show that a generic problem FIND-GROUP has an FP SPP algorithm. This general result has other consequences: for example, it follows that the hidden subgroup problem for permutation groups, studied in the context of quantum algorithms, has an FP/sup SPP/ algorithm. Also, some other algorithmic problems over permutation groups known to be at least as hard as graph isomorphism (e.g. coset intersection) are in SPP, and thus in Mod/sub k/P for each k>2.
[graph theory, complexity class SPP, Complexity theory, Machinery, Computer science, Character generation, graph isomorphism, permutation groups, Polynomials, Artificial intelligence, quantum algorithms, Testing, computational complexity]
Authentication of quantum messages
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Authentication is a well-studied area of classical cryptography: a sender A and a receiver B sharing a classical secret key want to exchange a classical message with the guarantee that the message has not been modified or replaced by a dishonest party with control of the communication line. In this paper we study the authentication of messages composed of quantum states. We give a formal definition of authentication in the quantum setting. Assuming A and B have access to an insecure quantum channel and share a secret, classical random key, we provide a non-interactive scheme that enables A to both encrypt and authenticate an m qubit message by encoding it into m+s qubits, where the error probability decreases exponentially in the security parameter s. The scheme requires a secret key of size 2m+O(s). To achieve this, we give a highly efficient protocol for testing the purity of shared EPR pairs. It has long been known that learning information about a general quantum state will necessarily disturb it. We refine this result to show that such a disturbance can be done with few side effects, allowing it to circumvent cryptographic protections. Consequently, any scheme to authenticate quantum messages must also encrypt them. In contrast, no such constraint exists classically. This reasoning has two important consequences: It allows us to give a lower bound of 2m key bits for authenticating m qubits, which makes our protocol asymptotically optimal. Moreover, we use it to show that digitally signing quantum states is impossible.
[disturbance, Communication system control, security parameter, noninteractive scheme, protocol, quantum cryptography, qubits, Cryptography, protocols, Testing, authentication, Paramagnetic resonance, Purification, receiver, error probability, Teleportation, cryptography, encoding, Cryptographic protocols, secret classical random key sharing, Computer science, digital signature, sender, Authentication, Information security, message authentication, quantum states, insecure quantum channel, quantum messages, computational complexity]
On approximating the radii of point sets in high dimensions
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Let P be a set of n points in /spl Ropf//sup d/. For any 1/spl les/k/spl les/d, the outer k-radius of P, denoted by R/sub k/(P), is the minimum, over all (d-k) -dimensional fiats F, of max/sub p/spl isin/P/ d(p, F), where d(p, F) is the Euclidean distance between the point p and fiat F. We consider the scenario when the dimension d is not fixed and can be as large as n. Computing the various radii of point sets is a fundamental problem in computational convexity with many applications. The main result of this paper is a randomized polynomial time algorithm that approximates Rk (P) to within a factor of O/spl radic/(log n/spl middot/log d) for any 1/spl les/k/spl les/d. This algorithm is obtained using techniques from semidefinite programming and dimension reduction. Previously, good approximation algorithms were known only for the case k=1 and for the case when k=d-c for any constant c; there are polynomial time algorithms that approximate Rk(P) to within a factor of (1+/spl epsi/), for any /spl epsi/>0, when d-k is any fixed constant. On the other hand, some results from the mathematical programming community on approximating certain kinds of quadratic programs imply an O/spl radic/(log n) approximation for R/sub 1/ (P), the width of the point set P. We also prove an inapproximability result for computing Rk (P), which easily yields the conclusion that our approximation algorithm performs quite well for a large range of values of k. Our inapproximability result for Rk (P) improves the previous known hardness result of Brieden, and is proved by improving the parameters in Brieden's construction using basic ideas from PCP theory.
[inapproximability, approximation algorithms, Data mining, high dimensions, dimension reduction, semidefinite programming, Engineering management, flat, computational convexity, Polynomials, Mathematical programming, Engine cylinders, randomized polynomial time algorithm, Statistics, quadratic programming, mathematical programming, randomised algorithms, Computer science, Computational geometry, hardness, Euclidean distance, quadratic programs, Approximation algorithms, point set radii approximation, computational complexity]
Nash equilibria in competitive societies, with applications to facility location, traffic routing and auctions
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider the following class of problems. The value of an outcome to a society is measured via a submodular utility function (submodularity has a natural economic interpretation: decreasing marginal utility). Decisions, however, are controlled by non-cooperative agents who seek to maximise their own private utility. We present, under basic assumptions, guarantees on the social performance of Nash equilibria. For submodular utility functions, any Nash equilibrium gives an expected social utility within a factor 2 of optimal, subject to a function-dependent additive term. For non-decreasing, submodular utility functions, any Nash equilibrium gives an expected social utility within a factor 1+/spl delta/ of optimal, where 0/spl les//spl delta//spl les/1 is a number based upon discrete curvature of the function. A condition under which all sets of social and private utility functions induce pure strategy Nash equilibria is presented. The case in which agents themselves make use of approximation algorithms in decision making is discussed and performance guarantees given. Finally we present specific problems that fall into our framework. These include competitive versions of the facility location problem and k-median problem, a maximisation version of the traffic routing problem studied by Roughgarden and Tardos (2000), and multiple-item auctions.
[expected social utility, Costs, competitive societies, function-dependent additive term, Nash equilibrium, approximation algorithms, noncooperative agents, facility location, outcome value, decreasing marginal utility, Measurement standards, discrete curvature function, Nash equilibria, performance guarantees, social performance guarantees, submodular utility function, Gold, maximisation, k-median problem, network routing, Decision making, game theory, competitive algorithms, Routing, multiple-item auctions, Societies, traffic routing, Game theory, private utility functions, social utility functions, decision making, Approximation algorithms, Internet, computational complexity]
Spectral gap and log-Sobolev constant for balanced matroids
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We compute tight lower bounds on the log-Sobolev constant of a class of inductively defined Markov chains, which contains the bases-exchange walks for balanced matroids studied by Feder and Mihail. As a corollary, we obtain improved upper bounds for the mixing time of a variety of Markov chains. An example: the "natural" random walk on spanning trees of a graph G as proposed by Broder - which has been studied by a number of authors - mixes in time O(mn log n), where n is the number of vertices of G and m the number of edges. This beats the best previous upper bound on this walk by a factor n/sup 2/.
[Algorithm design and analysis, Markov chain simulation, tight lower bounds, combinatorial mathematics, edges, Markov chains, Time measurement, lower bounds, Convergence, matrix algebra, log-Sobolev constant, Upper bound, matroids, Tree graphs, Current measurement, balanced matroids, bases-exchange walks, Markov processes, Sampling methods, Robustness, combinatorial structures, Informatics, Contracts, computational complexity]
The asymptotic order of the random k-SAT threshold
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Form a random k-SAT formula on n variables by selecting uniformly and independently m=rn clauses out of all 2/sup k/ (/sub k//sup n/) possible k-clauses. The satisfiability threshold conjecture asserts that for each k there exists a constant r/sub k/ such that, as n tends to infinity, the probability that the formula is satisfiable tends to 1 if r<r/sub k/ and to 0 if r>r/sub k/. It has long been known that 2/sup k//k<r/sub k/<2/sup k/. We prove that r/sub k/>2/sup k-1/ ln 2-d/sub k/, where d/sub k//spl rarr/(1+ln2)/2. Our proof also allows a blurry glimpse of the "geometry" of the set of satisfying truth assignments.
[Chaos, Computer science, Laboratories, probability, H infinity control, computability, satisfiability threshold conjecture, truth assignments, NP-complete problem, computational complexity, asymptotic order, random k-SAT threshold]
Switching lemma for small restrictions and lower bounds for k-DNF resolution
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We prove a new switching lemma that works for restrictions that set only a small fraction of the variables and is applicable to DNFs with small conjunctions. We use this to prove lower bounds for the Res(k) propositional proof system, an extension of resolution which works with k-DNFs instead of clauses. We also obtain an exponential separation between depth d circuits of bottom fan-in k and depth d circuits of bottom fan-in k+1. Our results for Res(k) are: 1. The 2n to n weak pigeonhole principle requires exponential size to refute in Res(k), for k /spl les/ /spl radic/(log n/ log log n). 2. For each constant k, there exists a constant w > k so that random w-CNFs require exponential size to refute in Res(k). 3. For each constant k, there are sets of clauses which have polynomial size Res(k+1) refutations, but which require exponential size Res(k) refutations.
[combinatorial mathematics, Circuits, propositional proof system, switching lemma, exponential separation, Complexity theory, Computer science, weak pigeonhole principle, Web pages, Polynomials, theorem proving, Arithmetic, computational complexity]
A simple algorithmic characterization of uniform solvability
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
The Herlihy-Shavit (HS) conditions characterizing the solvability of asynchronous tasks over n processors have been a milestone in the development of the theory of distributed computing. Yet, they were of no help when researcher sought algorithms that do not depend on n. To help in this pursuit we investigate the uniform solvability of an infinite uniform sequence of tasks T/sub 0/, T/sub 1/, T/sub 2/,..., where T/sub i/ is a task over processors p/sub 0/, p/sub 1/,...,p/sub i/, and T/sub i/ extends T/sub i-1/. We say that such a sequence is uniformly solvable if there exit protocols to solve each T/sub i/ and the protocol for T/sub i/ extends the protocol for T/sub i-1/. This paper establishes that although each T/sub i/ may be solvable, the uniform sequence is not necessarily uniformly solvable. We show this by proposing a novel uniform sequence of solvable tasks and proving that the sequence is not amenable to a uniform solution. We then extend the HS conditions for a task over n processors, to uniform solvability in a natural way. The technique we use to accomplish this is to generalize the alternative algorithmic proof, by Borowsky and Gafni, of the HS conditions, by showing that the infinite uniform sequence of task of Immediate Snapshots is uniformly solvable. A side benefit of the technique is a widely applicable methodology for the development of uniform protocols.
[Protocols, uniform protocols, asynchronous tasks, infinite uniform sequence, Distributed computing, uniform sequence, distributed computing, Computer science, uniform solvability, algorithmic characterization, protocols, Pursuit algorithms, Herlihy-Shavit conditions, computational complexity]
Lower bounds on the bounded coefficient complexity of bilinear maps
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We prove lower bounds of order n log n for both the problem to multiply polynomials of degree n, and to divide polynomials with remainder, in the model of bounded coefficient arithmetic circuits over the complex numbers. These lower bounds are optimal up to order of magnitude. The proof uses a recent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrix multiplication. It reduces the linear problem to multiply a random circulant matrix with a vector to the bilinear problem of cyclic convolution. We treat the arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp. 305-306, 1973] in a unitarily invariant way. This establishes a new lower bound on the bounded coefficient complexity of linear forms in terms of the singular values of the corresponding matrix.
[Computational modeling, polynomials, Circuits, Discrete Fourier transforms, bounded coefficient arithmetic circuits, bilinear systems, Vectors, Mathematics, lower bound, Matrix decomposition, complex numbers, random circulant matrix, Computer science, matrix multiplication, Convolution, bounded coefficient complexity, cyclic convolution, bilinear problem, linear problem, Polynomials, singular values, Arithmetic, computational complexity]
On the decidability of self-assembly of infinite ribbons
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Self-assembly, the process by which objects autonomously come together to form complex structures, is omnipresent in the physical world. A systematic study of self-assembly as a mathematical process has been initiated. The individual components are modelled as square tiles on the infinite two-dimensional plane. Each side of a tile is covered by a specific "glue\
[motif construction, Solid modeling, rectangles, Laboratories, glue, arbitrarily large structures, computational geometry, 2D structures, Amorphous materials, nonself-crossing tile sequence, square tiles, decidability, infinite directed zipper, infinite ribbons, strong plane filling property, Assembly, sticking, DNA computing, mathematical process, unlimited infinite snake problem, squares, Computer science, self-assembly, undecidable tiling problem, Self-assembly, infinite 2D plane, Tiles, Biology computing, aperiodic tilings, directed tiles, geometry, Plastics]
PAC=PAExact and other equivalent models in learning
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
The probably almost exact model (PAExact) can be viewed as the exact model relaxed so that: 1. The counterexamples to equivalence queries are distributionally drawn rather than adversarially chosen. 2. The output hypothesis is equal to the target with negligible error (1//spl omega/(poly) for any poly). This model allows studying (almost) exact learnability of infinite classes and is in some sense analogous to the Exact-learning model for finite classes. It is known that PAExact-learnable/spl rArr/PAC-learnable [BJT02]. In this paper we show that if a class is PAC-learnable (in polynomial time) then it is PAExact-learnable (in polynomial time). Therefore, PAExact-learnable=PAC-learnable. It follows from this result that if a class is PAC-learnable then it is learnable in the probabilistic prediction model from examples with an algorithm that runs in polynomial time for each prediction (polynomial in log(the number of trials)) and that after polynomial number of mistakes achieves a hypothesis that predicts the target with probability 1-1/2/sup poly/. We also show that if a class is PAC-learnable in parallel then it is PAExact-learnable in parallel.
[Computer simulation, Computational modeling, probability, Predictive models, equivalence queries, Boosting, learning, Distributed computing, PAExact-learnable, Computer science, equivalent models, probably almost exact model, probabilistic prediction model, Computer errors, Polynomials, Error correction, learning (artificial intelligence)]
Market equilibrium via a primal-dual-type algorithm
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Although the study of market equilibria has occupied center stage within mathematical economics for over a century, polynomial time algorithms for such questions have so far evaded researchers. We provide the first such algorithm for the linear version of a problem defined by Irving Fisher in 1891. Our algorithm is modeled after Kuhn's (1995) primal-dual algorithm for bipartite matching.
[Heart, primal-dual algorithm, market equilibria, Linear programming, Educational institutions, mathematical economics, bipartite matching, Computer science, polynomial time algorithms, linear problem, Polynomials, economic cybernetics, Bipartite graph, computational complexity]
Graphs with tiny vector chromatic numbers and huge chromatic numbers
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Karger Motwani and Sudan (1998) introduced the notion of a vector coloring of a graph. In particular they show that every k-colorable graph is also vector k-colorable, and that for constant k, graphs that are vector k-colorable can be colored by roughly /spl Delta//sup 1-2/k/ colors. Here /spl Delta/ is the maximum degree in the graph. Their results play a major role in the best approximation algorithms for coloring and for maximal independent set. We show that for every positive integer k there are graphs that are vector k-colorable but do not have independent sets significantly larger than n//spl Delta//sup 1-2/k/ (and hence cannot be colored with significantly less that /spl Delta//sup 1-2/k/ colors). For k = O(log n/log log n) we show vector k-colorable graphs that do not have independent sets of size (log n)/sup c/, for some constant c. This shows that the vector chromatic number does not approximate the chromatic number within factors better than n/polylogn. As part of our proof, we analyze "property testing" algorithms that distinguish between graphs that have an independent set of size n/k, and graphs that are "far" from having such an independent set. Our bounds on the sample size improve previous bounds of Goldreich, Goldwasser and Ron (1998) for this problem.
[Algorithm design and analysis, vector coloring, graph theory, chromatic numbers, Mathematics, k-colorable graph, graph coloring, property testing, Computer science, maximum degree, graphs, NP-hard problem, Approximation algorithms, Polynomials, Testing, computational complexity]
A constant-factor approximation algorithm for the multicommodity rent-or-buy problem
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We present the first constant factor approximation algorithm for network design with multiple commodities and economies of scale. We consider the rent-or-buy problem, a type of multicommodity buy-at-bulk network design in which there are two ways to install capacity on any given edge. Capacity can be rented, with cost incurred on a per-unit of capacity basis, or bought, which allows unlimited use after payment of a large fixed cost. Given a graph and a set of source-sink pairs, we seek a minimum-cost way of installing sufficient capacity on edges so that a prescribed amount of flow can be sent simultaneously from each source to the corresponding sink. Recent work on buy-at-bulk network design has concentrated on the special case where all sinks are identical; existing constant factor approximation algorithms for this special case make crucial use of the assumption that all commodities ship flow to the same sink vertex and do not obviously extend to the multicommodity rent-or-buy problem. Prior to our work, the best heuristics for the multicommodity rent-or-buy problem achieved only logarithmic performance guarantees and relied on the machinery of relaxed metrical task systems or of metric embeddings. By contrast, we solve the network design problem directly via a novel primal-dual algorithm.
[Algorithm design and analysis, Piecewise linear techniques, multicommodity buy-at-bulk network design, Piecewise linear approximation, graph theory, multiple commodities, Machinery, Computer science, source-sink pairs, capacity basis, Economies of scale, Approximation algorithms, Cost function, multicommodity rent-or-buy problem, network design, economies of scale, Marine vehicles, constant-factor approximation algorithm, computational complexity]
Minimizing congestion in general networks
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
A principle task in parallel and distributed systems is to reduce the communication load in the interconnection network, as this is usually the major bottleneck for the performance of distributed applications. We introduce a framework for solving online problems that aim to minimize the congestion (i.e. the maximum load of a network link) in general topology networks. We apply this framework to the problem of online routing of virtual circuits and to a dynamic data management problem. For both scenarios we achieve a competitive ratio of O(log/sup 3/ n) with respect to the congestion of the network links. Our online algorithm for the routing problem has the remarkable property that it is oblivious, i.e., the path chosen for a virtual circuit is independent of the current network load. Oblivious routing strategies can easily be implemented in distributed environments and have therefore been intensively studied for certain network topologies as e.g. meshes, tori and hypercubic networks. This is the first oblivious path selection algorithm that achieves a polylogarithmic competitive ratio in general networks.
[telecommunication congestion control, Multiprocessor interconnection networks, tori, graph theory, multiprocessor interconnection networks, Mathematics, online problems, distributed applications, competitive ratio, Intelligent networks, Circuit topology, routing problem, Network topology, interconnection network, online virtual circuit routing, Bandwidth, distributed systems, Workstations, meshes, polylogarithmic competitive ratio, Routing, general networks, distributed environments, Application software, Computer science, oblivious path selection algorithm, parallel systems, communication load, general topology networks, hypercubic networks, telecommunication network routing, network link, network topologies, dynamic data management problem, minimisation, congestion minimization, maximum load]
Conflict-free colorings of simple geometric regions with applications to frequency assignment in cellular networks
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Motivated by a frequency assignment problem in cellular networks, we introduce and study a new coloring problem called minimum conflict-free coloring (min-CF-coloring). In its general form, the input of the min-CF-coloring problem is a set system (X, S), where each S /spl isin/ S is a subset of X. The output is a coloring X of the sets in S that satisfies the following constraint: for every x /spl isin/ X there exists a color i and a unique set S /spl isin/ S, such that x /spl isin/ S and /spl chi/(S) = i. The goal is to minimize the number of colors used by the coloring X. Min-CF-coloring of general set systems is not easier than the classic graph coloring problem. However, in view of our motivation, we consider set systems induced by simple geometric regions in the plane. In particular, we study disks (both congruent and non-congruent), axis-parallel rectangles (with a constant ratio between the smallest and largest rectangle) regular hexagons (with a constant ratio between the smallest and largest hexagon), and general congruent centrally-symmetric convex regions in the plane. In all cases we have coloring algorithms that use O(log n) colors (where n is the number of regions). For rectangles and hexagons we obtain a constant-ratio approximation algorithm when the ratio between the largest and smallest rectangle (hexagon) is a constant. We also show that, even in the case of unit disks, /spl Theta/(log n) colors may be necessary.
[minimum conflict-free coloring, frequency allocation, set system, disks, Spine, set theory, regular hexagons, graph colouring, Intelligent networks, Network servers, cellular networks, constant ratio approximation algorithm, Base stations, axis-parallel rectangles, Interference, simple geometric regions, Computer science, Land mobile radio cellular systems, Character generation, frequency assignment problem, congruent centrally symmetric convex regions, coloring algorithms, Frequency, Approximation algorithms, computational complexity, cellular radio]
Load balancing with memory
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
A standard load balancing model considers placing n balls into n bins by choosing d possible locations for each ball independently and uniformly at random and sequentially placing each in the least loaded of its chosen bins. It is well known that allowing just a small amount of choice (d = 2) greatly improves performance over random placement (d = 1). In this paper, we show that similar performance gains occur by introducing memory. We focus on the situation where each time a ball is placed, the least loaded of that ball's choices after placement is remembered and used as one of the possible choices for the next ball. For example, we show that when each ball gets just one random choice, but can also choose the best of the last ball's choices, the maximum number of balls in a bin is log log n/2 log /spl phi/ + O(1) with high probability, where /spl phi/ = (1 + /spl radic/5)/2 is the golden ratio. The asymptotic performance is therefore better with one random choice and one choice from memory than with two fresh random choices for each ball; the performance with memory asymptotically matches the asymmetric policy, using two choices introduced by Vocking (1999). More generally, we find that a small amount of memory, like a small amount of choice, can dramatically improve the load balancing performance. We also investigate continuous time variations corresponding to queueing systems, where we find similar results.
[Algorithm design and analysis, Gold, Grounding, asymptotic performance, probability, Switches, standard load balancing model, Performance gain, Routing, Scheduling algorithm, bin packing, random placement, Telephony, Load management, golden ratio, Load modeling]
Optimal system of loops on an orientable surface
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Every compact orientable boundaryless surface /spl Mscr/ can be cut along simple loops with a common point /spl upsi//sub 0/, pairwise disjoint except at /spl upsi//sub 0/, so that the resulting surface is a topological disk; such a set of loops is called a fundamental system of loops for /spl Mscr/. The resulting disk is a polygon in which the edges are pairwise identified on the surface; it is called a polygonal schema Assuming that /spl Mscr/ is triangulated, and that each edge has a given length, we are interested in a shortest (or optimal) system homotopic to a given one, drawn on the vertex-edge graph of /spl Mscr/. We prove that each loop of such an optimal system is a shortest loop among all simple loops in its homotopy class. We give a polynomial (under some reasonable assumptions) algorithm to build such a system. As a byproduct, we get a polynomial algorithm to compute a shortest simple loop homotopic to a given simple loop.
[Algorithm design and analysis, Stability, Piecewise linear techniques, vertex-edge graph, graph theory, homotopy class, polygonal schema, computational geometry, polynomial algorithm, compact orientable boundaryless surface, Stress, Geometry, optimal system, polygon, common point, Polynomials, computational complexity]
On-line confidence machines are well-calibrated
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Transductive Confidence Machine (TCM) and its computationally efficient modification, inductive confidence machine (ICM), are ways of complementing machine-learning algorithms with practically useful measures of confidence. We show that when TCM and ICM are used in the on-line mode, their confidence measures are well-calibrated, in the sense that predictive regions at confidence level 1-/spl delta/ will be wrong with relative frequency at most /spl delta/ (approaching /spl delta/ in the case of randomised TCM and ICM) in the long run. This is not just an asymptotic phenomenon: actually the error probability of randomised TCM and ICM is d at every trial and errors happen independently at different trials.
[Error probability, confidence measures, probabilistic logic, probability, inductive confidence machine, error probability, Reliability theory, machine-learning algorithms, Frequency measurement, online confidence machines, Distributed computing, Computer science, transductive confidence machine, Machine learning, Packaging, Computer errors, Prediction algorithms, learning (artificial intelligence), Testing, error statistics]
Deterministic broadcasting time in radio networks of unknown topology
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
In a seminal paper, Bar-Yehuda et al. (1992) considered broadcasting in radio networks whose nodes know only their own label and labels of their neighbors. They claimed a linear lower bound on the time of deterministic broadcasting in such radio networks, by constructing a class of graphs of diameter 3, with the property that every broadcasting algorithm requires linear time on one of these graphs. Due to a subtle error in the argument, this result is incorrect. We construct an algorithm that broadcasts in logarithmic time on all graphs from the work of Bar-Yehuda et al. Moreover, we show how to broadcast in sublinear time on all n-node graphs of diameter o(log log n). On the other hand, we construct a class of graphs of diameter 4, such that every broadcasting algorithm requires time /spl Omega/(4/spl radic/n) on one of these graphs. In view of the randomized algorithm, running in expected time O(D log n + log/sup 2/ n) on all n-node graphs of diameter D, our lower bound gives the first correct proof of an exponential gap between determinism and randomization in the time of radio broadcasting.
[radio broadcasting, radio networks, graph theory, communication complexity, Distributed computing, logarithmic time, Intelligent networks, broadcasting algorithm, graphs, Network topology, randomization, Radio network, determinism, Radio transmitters, Time measurement, unknown topology, randomized algorithm, lower bound, Radio broadcasting, labels, nodes, Current measurement, sublinear time, deterministic broadcasting time, Radio networks, Clocks]
Abstract combinatorial programs and efficient property testers
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Property testing is a relaxation of classical decision problems which aims at distinguishing between functions having a predetermined property and functions being far from any function having the property. In this paper we present a novel framework for analyzing property testing algorithms with one-sided error. Our framework is based on a connection of property testing and a new class of problems which we call abstract combinatorial programs. We show that if the problem of testing a property can be reduced to an abstract combinatorial program of small dimension, then the property has an efficient tester. We apply our framework to a variety of classical combinatorial problems. Among others, we present efficient property testing algorithms for geometric clustering problems, the reversal distance problem, and graph and hypergraph coloring problems. We also prove that, informally, any hereditary graph property can be efficiently tested if and only if it can be reduced to an abstract combinatorial program of small size. Our framework allows us to analyze all our testers in a unified way and the obtained complexity bounds either match or improve the previously known bounds. We believe that our framework will help to better understand the structure of efficiently testable properties.
[Algorithm design and analysis, Costs, Error probability, hypergraph coloring problems, graph theory, graph coloring problems, hereditary graph property, computational geometry, geometric clustering problems, Mathematics, reversal distance problem, complexity bounds, randomised algorithms, Computer science, Boolean functions, efficient property testers, abstract combinatorial programs, Clustering algorithms, predetermined property, one-sided error, Approximation algorithms, property testing algorithms, Contracts, Testing]
Kolmogorov's structure functions with an application to the foundations of model selection
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Kolmogorov (1974) proposed a non-probabilistic approach to statistics, an individual combinatorial relation between the data and its model. We vindicate, for the first time, the rightness of the original "structure function\
[Maximum likelihood estimation, Technological innovation, Shape, Engineering profession, structure function, probability, model selection, ML estimator, Mathematics, Proposals, Statistics, Kolmogorov structure functions, combinatorial relation, MDL estimator, Region 8, Mathematical model, Logic, minimum randomness, maximal complexity, computational complexity, data-to-model code length]
Limits on the power of quantum statistical zero-knowledge
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
In this paper we propose a definition for (honest verifier) quantum statistical zero-knowledge interactive proof systems and study the resulting complexity class, which we denote QSZK/sub HV/. We prove several facts regarding this class, including: the following problem is a complete promise problem for QSZKHV: given instructions for preparing two mixed quantum states, are the states close together or far apart in the trace norm metric? This problem is a quantum generalization of the complete promise problem of Sahai and Vadhan (1997) for (classical) statistical zero-knowledge; QSZK/sub HV/ is closed under complement; QSZK/sub HV//spl sube/PSPACE. (At present it is not known if arbitrary quantum interactive proof systems can be simulated in PSPACE even for one-round proof systems); any polynomial-round honest verifier quantum statistical zero-knowledge proof system can be simulated by a two-message (i.e., one-round) honest verifier quantum statistical zero-knowledge proof system. Similarly, any polynomial-round honest verifier quantum statistical zero-knowledge proof system can be simulated by a three-message public-coin honest verifier quantum statistical zero-knowledge proof system. These facts establish close connections between classical statistical zero-knowledge and our definition for quantum statistical zero-knowledge, and give some insight regarding the effect of this zero-knowledge restriction on quantum interactive proof systems. The relationship between our definition and possible definitions of general (i.e., not necessarily honest) quantum statistical zero-knowledge are also discussed.
[two message honest verifier quantum statistical zero knowledge proof system, polynomial round honest verifier quantum statistical zero knowledge proof system, Protocols, Computational modeling, complexity class, trace norm metric, classical statistical zero-knowledge, Complexity theory, Computer science, Quantum computing, Upper bound, Physics computing, mixed quantum states, quantum cryptography, complete promise problem, Polynomials, theorem proving, Cryptography, Artificial intelligence, quantum statistical zero knowledge interactive proof systems, computational complexity, three message public coin honest verifier quantum statistical zero knowledge proof system]
Low-dimensional linear programming with violations
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Megiddo (1984) and Dyer (1984) showed that linear programming in 2 and 3 dimensions (and subsequently, any constant number of dimensions) can be solved in linear time. In this paper, we consider linear programming with at most k violations: finding a point inside all but at most k of n given halfspaces. We give a simple algorithm in 2-d that runs in O((n + k/sup 2/) log n) expected time; this is faster than earlier algorithms by Everett, Robert, and van Kreveld (1993) and Matousek (1994) and is probably near-optimal for all k /spl Lt/ n/2. A (theoretical) extension of our algorithm in 3-d runs in near O(n + k/sup 11/4/n/sup 1/4/) expected time. Interestingly; the idea is based on concave-chain decompositions (or covers) of the (/spl les/ k)-level, previously used in proving combinatorial k -level bounds. Applications in the plane include improved algorithms for finding a line that misclassifies the fewest among a set of bichromatic points, and finding the smallest circle enclosing all but k points. We also discuss related problems of finding local minima in levels.
[bichromatic points, Statistical analysis, expected time, computational geometry, Linear programming, linear programming, low-dimensional linear programming, Computer science, Computational geometry, concave-chain decompositions, local minima, Metrology, Robustness, computational complexity]
Privacy and interaction in quantum communication complexity and a theorem about the relative entropy of quantum states
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We prove a fundamental theorem about the relative entropy of quantum states, which roughly states that if the relative entropy, S(/spl rho//spl par//spl sigma/)/spl Delta/=Tr /spl rho/(log /spl rho/-log /spl sigma/), of two quantum states /spl rho/ and /spl sigma/ is at most c, then /spl rho//2/sup O(c)/ 'sits inside' /spl sigma/. Using this 'substate' theorem, we give tight lower bounds for the privacy loss of bounded error quantum communication protocols for the index function problem. We also use the 'substate' theorem to give tight lower bounds for the k-round bounded error quantum communication complexity of the pointer chasing problem, when the wrong player starts, and all the log n bits of the kth pointer are desired.
[bounded error quantum communication protocols, Protocols, tight lower bounds, quantum communication complexity, Entropy, Probability distribution, k-round bounded error quantum communication complexity, Complexity theory, communication complexity, Privacy, relative entropy theorem, pointer chasing problem, entropy, Quantum mechanics, interaction, quantum states, index function problem, Relativistic quantum mechanics, protocols, quantum communication, privacy loss, wrong player, substate theorem]
Random lattices and a conjectured 0 - 1 law about their polynomial time computable properties
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We formulate a conjecture about random n-dimensional lattices with a suitable distribution. The conjecture says that every polynomial time computable property of a random lattice holds with a probability either close to 0 or close to 1. Accepting the conjecture we get a large class of hard lattice problems. We describe an analogy between our conjecture and a set theoretical axiom, which cannot be proved in ZFC. This axiom says that there exists a nontrivial /spl sigma/-additive 0 - 1 measure defined on the set of all subsets of some set S.
[Computational modeling, Lattices, computability, lattice theory, hard lattice problems, random lattice, Turing machines, polynomial time computable, Set theory, Polynomials, random n-dimensional lattices, set theoretical, Cryptography, computational complexity]
The parameterized complexity of counting problems
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We develop a parameterized complexity theory for counting problems. As the basis of this theory, we introduce a hierarchy of parameterized counting complexity classes #W[t], for t/spl ges/1, that corresponds to Downey and Fellows' (1999) W-hierarchy and show that a few central W-completeness results for decision problems translate to #W-completeness results for the corresponding counting problems. Counting complexity gets interesting with problems whose decision version is tractable, but whose counting version is hard. Our main result states that counting cycles and paths of length k in both directed and undirected graphs, parameterized by k, are #W[1]-complete. This makes it highly unlikely that any of these problems is fixed-parameter tractable, even though their decision versions are. More explicitly, our result shows that most likely there is no f(k)/spl middot/n/sup c/-algorithm for counting cycles or paths of length k in a graph of size n for any computable function f:/spl Nopf//spl rarr//spl Nopf/ and constant c, even though there is a 2/sup O(k)//spl middot/n/sup 2.376/ algorithm for finding a cycle or path of length k (2).
[Algorithm design and analysis, graph theory, Complexity theory, W-hierarchy, parameterized complexity theory, counting cycles, Databases, parameterized counting complexity classes, #W-completeness results, Polynomials, Bipartite graph, undirected graphs, Computational biology, algorithm, counting problems, Computational complexity, Computer science, directed graphs, counting paths, Approximation algorithms, computable function, decision problems, Artificial intelligence, W-completeness results, computational complexity]
On the hardness of optimal auctions
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We study a fundamental problem in microeconomics called optimal auction design: a seller wishes to sell an item to a group of self-interested agents. Each agent i has a privately known valuation v/sub i/ for the object. Given a distribution on these valuations, the goal is to construct an optimal auction, i.e. a truth revealing protocol that maximizes the seller's expected revenue. We study this problem from a computational perspective and show several lower bounds. In particular we prove that no deterministic polynomial time ascending auction can achieve an approximation ratio better than 3/4. The probability distribution constructed in our example has sensitive dependencies among the agents. In contrast, we show that if the dependency between the agents' valuations is bounded, the problem can be approximated with a factor close to 1.
[seller, Protocols, deterministic polynomial time ascending auction, Probability distribution, Yarn, Cost accounting, Constraint optimization, optimal auction design, self-interested agents, probability distribution, Polynomials, truth revealing protocol, sensitive dependencies, approximation ratio, microeconomics, probability, Game theory, lower bounds, Computer science, Waste materials, hardness, economic cybernetics, privately known valuation, computational complexity, maximized expected revenue]
Global information from local observation
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We observe a certain random process on a graph "locally\
[Algorithm design and analysis, graph theory, Random processes, random process, orientable surface, random edge excitation, Physics computing, Polynomials, polynomial time, global information, Distributed algorithms, connected subgraph, local observation, random processes, random walk returns, Birds, Information retrieval, random balancing, graph, Computer science, nodes, genus, Sampling methods, faces, computational complexity]
Learning a hidden matching
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider the problem of learning a matching (i.e., a graph in which all vertices have degree 0 or 1) in a model where the only allowed operation is to query whether a set of vertices induces an edge. This is motivated by a problem that arises in molecular biology. In the deterministic nonadaptive setting, we prove a ( 1/2 +o(1))(n/2) upper bound and a nearly matching 0.32(n/2) lower bound for the minimum possible number of queries. In contrast, if we allow randomness then we obtain (by a randomized, nonadaptive algorithm) a much lower O(n log n) upper bound, which is best possible (even for randomized fully adaptive algorithms).
[molecular biology, Sequences, Biological system modeling, Adaptive algorithm, vertices, Genomics, probability, deterministic nonadaptive setting, upper bound, Mathematics, randomness, lower bound, nonadaptive algorithm, deterministic algorithms, randomised algorithms, Upper bound, DNA, learning (artificial intelligence), hidden matching learning, Bioinformatics, randomized fully adaptive algorithms, Assembly, Testing]
A lower bound for testing 3-colorability in bounded-degree graphs
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider the problem of testing 3-colorability in the bounded-degree model. We show that, for small enough /spl epsiv/, every tester for 3-colorability must have query complexity /spl Omega/(n). This is the first linear lower bound for testing a natural graph property in the bounded-degree model. An /spl Omega/(/spl radic/n) lower bound was previously known. For one-sided error testers, we also show an /spl Omega/(n) lower bound for testers that distinguish 3-colorable graphs from graphs that are (1/3 - /spl alpha/)-far from 3-colorable, for arbitrarily small /spl alpha/. In contrast, a polynomial time algorithm by Frieze and Jerrum (1997) distinguishes 3-colorable graphs from graphs that are 1/5-far from 3-colorable. As a by-product of our techniques, we obtain tight unconditional lower bounds on the approximation ratios achievable by sublinear time algorithms for Max E3SAT, Max E3LIN-2 and other problems.
[query complexity, one-sided error testers, sublinear time algorithms, lower bound, Logic testing, polynomial time algorithm, graph colouring, natural graph property testing, Max E3SAT, Computer science, bounded-degree model, Max E3LIN-2, Automatic testing, Automata, approximation ratios, Approximation algorithms, 3-colorability testing, Polynomials, bounded-degree graphs, computational complexity]
Forbidden information
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
There appears to be a gap between usual interpretations of Godel Theorem and what is actually proven. Closing this gap does not seem obvious and involves complexity theory. (This is unrelated to, well studied before, complexity quantifications of the usual Godel effects.) Similar problems and answers apply to other unsolvability results for tasks where required solutions are not unique, such as, e.g., non-recursive tilings.
[Computer science, Godel theorem, complexity theory, probability, computational complexity, forbidden information]
Protocols and impossibility results for gossip-based communication mechanisms
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
In recent years, gossip-based algorithms have gained prominence as a methodology for designing robust and scalable communication schemes in large distributed systems. The premise underlying distributed gossip is very simple: in each time step, each node v in the system selects some other node w as a communication partner, generally by a simple randomized rule, and exchanges information with w; over a period of time, information spreads through the system in an "epidemic fashion". A fundamental issue which is not well understood is the following: how does the underlying low-level gossip mechanism (the means by which communication partners are chosen) affect one's ability to design efficient high-level gossip-based protocols? We establish one of the first concrete results addressing this question, by showing a fundamental limitation on the power of the commonly used uniform gossip mechanism for solving nearest-resource location problems. In contrast, very efficient protocols for this problem can be designed using a non-uniform spatial gossip mechanism, as established in earlier work with Alan Demers. We go on to consider the design of protocols for more complex problems, providing an efficient distributed gossip-based protocol for a set of nodes in Euclidean space to construct an approximate minimum spanning tree. Here too, we establish a contrasting limitation on the power of uniform gossip for solving this problem. Finally, we investigate gossip-based packet routing as a primitive that underpins the communication patterns in many protocols, and as a way to understand the capabilities of different gossip mechanisms at a general level.
[Algorithm design and analysis, Context, low-level gossip mechanism, Career development, high-level gossip-based protocols, message passing, Design methodology, large distributed systems, Computer science, Fault tolerance, Aggregates, transport protocols, gossip-based algorithms, Robustness, Concrete, Routing protocols, Euclidean space, robust scalable communication schemes, impossibility results, gossip-based communication mechanisms]
Bounded-depth Frege lower bounds for weaker pigeonhole principles
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We prove a quasi-polynomial lower bound on the size of bounded-depth Frege proofs of the pigeonhole principle PHP/sub n//sup m/ where m = (1 + 1/polylog n)n. This lower bound qualitatively matches the known quasipolynomial-size bounded-depth Frege proofs for these principles. Our technique, which uses a switching lemma argument like other lower bounds for bounded-depth Frege proofs, is novel in that the tautology to which this switching lemma is applied remains random throughout the argument.
[weaker pigeonhole principles, bounded-depth Frege proofs, combinatorial mathematics, bounded-depth Frege lower bounds, switching lemma argument, quasi-polynomial lower bound, propositional pigeonhole principle, Combinatorial mathematics, Radio access networks, Computer science, quasipolynomial-size bounded-depth Frege proofs, Polynomials, theorem proving, computational complexity]
Linear Diophantine equations over polynomials and soft decoding of Reed-Solomon codes
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We generalize the classical Knuth-Schonhage algorithm computing GCD of two polynomials for solving arbitrary linear Diophantine systems over polynomials in time, quasi-linear in the maximal degree. As an application, we consider the following weighted curve fitting problem: given a set of points in the plain, find an algebraic curve (satisfying certain degree conditions) that goes through each point the prescribed number of times. The main motivation for this problem comes from coding theory, namely it is ultimately related to the list decoding of Reed-Solomon codes. We present a new fast algorithm for the weighted curve fitting problem, based on the explicit construction of Groebner basis. This gives another fast algorithm for soft-decoding of Reed-Solomon codes different from the procedure proposed by Feng (1999), which works in time (w/r)/sup O(1)/ n log/sup 2/ n loglogn, where r is the rate of the code, and w is the maximal weight assigned to a vertical line.
[Linear systems, algebraic curve, Reed-Solomon codes, coding theory, Groebner basis, list decoding, Polynomials, weighted curve fitting problem, vertical line, polynomials, GCD, Decoding, Application software, Equations, decoding, Computer science, Interpolation, maximal weight, Approximation algorithms, curve fitting, fast algorithm, Curve fitting, linear Diophantine equations, soft decoding, classical Knuth-Schonhage algorithm, computational complexity]
Dimension reduction in the /spl lscr//sub 1/ norm
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
The Johnson-Lindenstrauss lemma shows that any set of n points in Euclidean space can be mapped linearly down to O((log n)//spl epsi//sup 2/) dimensions such that all pairwise distances are distorted by at most 1+/spl epsi/. We study the basic question of whether there exists an analogue of the Johnson-Lindenstrauss lemma for the /spl lscr//sub 1/ norm? Note that Johnson-Lindenstrauss lemma gives a linear embedding which is independent of the point set. For the /spl lscr//sub 1/ norm, we show that one cannot hope to use linear embeddings as a dimensionality reduction tool for general point sets, even if the linear embedding is chosen as a function of the given point set. In particular, we construct a set of O(n) points in /spl lscr//sub 1//sup n/ such that any linear embedding into /spl lscr//sub 1//sup d/ must incur a distortion of /spl Omega//spl radic/(n/d). This bound is tight up to a log n factor. We then initiate a systematic study of general classes of /spl lscr//sub 1/ embeddable metrics that admit low dimensional, small distortion embeddings. In particular, we show dimensionality reduction theorems for tree metrics, circular-decomposable metrics, and metrics supported on K/sub 2,3/-free graphs, giving embeddings into /spl lscr//sub 1//sup O(log(2) n)/ with constant distortion. Finally, we also present lower bounds on dimension reduction techniques for other /spl lscr//sub p/ norms. Our work suggests that the notion of a stretch-limited embedding, where no distance is stretched by more than a factor d in any dimension, is important to the study of dimension reduction for /spl lscr//sub 1/. We use such stretch limited embeddings as a tool for proving lower bounds for dimension reduction and also as an algorithmic tool for proving positive results.
[point sets, Embedded computing, stretch-limited embedding, trees (mathematics), Extraterrestrial measurements, Johnson-Lindenstrauss lemma, circular-decomposable metrics, /spl lscr/ /sub 1/ norm, dimension reduction, /spl lscr/ /sub 1/ embeddable metrics, lower bounds, tree metrics, 3/ free graphs, Upper bound, low dimensional small distortion embeddings, linear embedding, pairwise distances, algorithmic tool, Euclidean space, Books, K/sub 2, computational complexity]
Auctions with severely bounded communication
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We study auctions with severe bounds on the communication allowed: each bidder may only transmit t bits of information to the auctioneer. We consider both welfare-maximizing and revenue-maximizing auctions under this communication restriction. For both measures, we determine the optimal auction and show that the loss incurred relative to unconstrained auctions is mild. We prove unsurprising properties of these kinds of auctions, e.g. that discrete prices are informationally efficient, as well as some surprising properties, e.g. that asymmetric auctions are better than symmetric ones.
[bidder, Humans, Routing, optimal auction, Loss measurement, Electronic commerce, communication complexity, Distributed computing, Game theory, severely bounded communication, revenue-maximizing auctions, unconstrained auctions, asymmetric auctions, Computer science, welfare-maximizing auctions, loss, discrete prices, Bandwidth, Internet, Resource management, information transmission, electronic commerce, symmetric auctions]
Rapidly mixing Markov chains for sampling contingency tables with a constant number of rows
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider the problem of sampling almost uniformly from the set of contingency tables with given row and column sums, when the number of rows is a constant. (2002) have recently given a fully polynomial randomized approximation scheme (fpras) for the related counting problem, which only employs Markov chain methods indirectly. But they leave open the question as to whether a natural Markov chain on such tables mixes rapidly. Here we answer this question in the affirmative, and hence provide a very different proof of the main result of Cryan and Dyer. We show that the "2 /spl times/ 2 heat-bath" Markov chain is rapidly mixing. We prove this by considering first a heat-bath chain operating on a larger window. Using techniques developed by Morris and Sinclair (2002) (see also Morris (2002)) for the multidimensional knapsack problem, we show that this chain mixes rapidly. We then apply the comparison method of Diaconis and Saloff-Coste (1993) to show that the 2 /spl times/ 2 chain is rapidly mixing. As part of our analysis, we give the first proof that the 2 /spl times/ 2 chain mixes in time polynomial in the input size when both the number of rows and the number of columns is constant.
[Algorithm design and analysis, contingency tables, Multidimensional systems, sampling methods, Error probability, sampling, randomized approximation, randomised algorithms, Computer science, Markov chain, Markov processes, Sampling methods, Polynomials, Informatics]
An information statistics approach to data stream and communication complexity
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We present a new method for proving strong lower bounds in communication complexity. This method is based on the notion of the conditional information complexity of a function which is the minimum amount of information about the inputs that has to be revealed by a communication protocol for the function. While conditional information complexity is a lower bound on the communication complexity, we show that it also admits a direct sum theorem. Direct sum decomposition reduces our task to that of proving (conditional) information complexity lower bounds for simple problems (such as the AND of two bits). For the latter, we develop novel techniques based on Hellinger distance and its generalizations.
[Algorithm design and analysis, Context, conditional information complexity, Protocols, Computational modeling, probability, communication protocol, Complexity theory, communication complexity, data stream, Statistics, Sun, information complexity, Computer science, Hellinger distance, information statistics approach, Frequency, Approximation algorithms, strong lower bounds, direct sum theorem, protocols]
The partition technique for overlays of envelopes
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We obtain a near-tight bound of O(n/sup 3+/spl epsiv//), for any /spl epsiv/ > 0, on the complexity of the overlay of the minimization diagrams of two collections of surfaces in four dimensions. This settles a long-standing problem in the theory of arrangements, most recently cited by Agarwal and Sharir (2000), and substantially improves and simplifies a result previously published by the authors (2002). Our bound has numerous algorithmic and combinatorial applications, some of which are presented in this paper. Our result is obtained by introducing a new approach to the analysis of combinatorial structures arising in geometric arrangements of surfaces. This approach, which we call the 'partition technique', is based on k-fold divide and conquer, in which a given collection /spl Fscr/ of n surfaces is partitioned into k subcollections /spl Fscr//sub i/ of n/k surfaces each, and the complexity of the relevant combinatorial structure in /spl Fscr/ is recursively related to the complexities of the corresponding structures in each of the /spl Fscr//sub i/'s. We introduce this approach by applying it first to obtain a new simple proof for the known near-quadratic bound on the complexity of an overlay of two minimization diagrams of collections of surfaces in /spl Ropf//sup 3/, thereby simplifying the previously available proof (1996).
[combinatorial mathematics, divide and conquer methods, combinatorial applications, partition technique, computational geometry, near-quadratic bound, minimization diagrams, algorithmic applications, Computer science, Geometry, near-tight bound, Polynomials, combinatorial structures, k-fold divide and conquer, geometric arrangements, computational complexity]
Testing juntas [combinatorial property testing]
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We show that a Boolean function over n Boolean variables can be tested for the property of depending on only k of them, using a number of queries that depends only on k and the approximation parameter /spl epsi/. We present two tests, both non-adaptive, that require a number of queries that is polynomial k and linear in /spl epsi//sup -1/. The first test is stronger in that it has a 1-sided error, while the second test has a more compact analysis. We also present an adaptive version and a 2-sided error version of the first test, that have a somewhat better query complexity than the other algorithms. We then provide a lower bound of /spl Omega//spl tilde/(/spl radic/ k) on the number of queries required for the non-adaptive testing of the above property; a lower bound of /spl Omega/(log(k + 1)) for adaptive algorithms naturally follows from this. In providing this we also prove a result about random walks on the group Z/sub 2//sup q/ that may be interesting in its own right. We show that for some t(q) = O/spl tilde/(q/sup 2/), the distributions of the random walk at times t and t + 2 are close to each other, independently of the step distribution of the walk. We also discuss related questions. In particular, when given in advance a known k junta function h, we show how to test a function f for the property of being identical to h up to a permutation of the variables, in a number of queries that is polynomial in k and /spl epsi/.
[random walks, combinatorial mathematics, Adaptive algorithm, random processes, query complexity, lattice theory, Boolean function, combinatorial property testing, approximation parameter, Computer science, error version, Boolean functions, Boolean variables, Approximation algorithms, Polynomials, Testing, computational complexity]
The 3-XORSAT threshold
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We prove the existence of the 3-XORSAT threshold, establishing its value as a function of the root of a transcendental equation.
[Linear systems, 3-XORSAT threshold, Upper bound, probability, Gaussian processes, H infinity control, computability, Probability distribution, Dentistry, Moment methods, transcendental equation, Equations]
Covering problems with hard capacities
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider the classical vertex cover and set cover problems with the addition of hard capacity constraints. This means that a set (vertex) can only cover a limited number of its elements (adjacent edges) and the number of available copies of each set (vertex) is bounded. This is a natural generalization of the classical problems that also captures resource limitations in practical scenarios. We obtain the following results. For the unweighted vertex cover problem with hard capacities we give a 3-approximation algorithm which is based on randomized rounding with alterations. We prove that the weighted version is at least as hard as the set cover problem. This is an interesting separation between the approximability of weighted and unweighted versions of a "natural" graph problem. A logarithmic approximation factor for both the set cover and the weighted vertex cover problem with hard capacities follows from the work of Wolsey (1982) on submodular set cover. We provide in this paper a simple and intuitive proof for this bound.
[Drugs, Costs, vertex cover, graph theory, Glycomics, hard capacity constraints, Linear programming, randomized rounding, capacitated set cover, facility location, logarithmic approximation factor, Computer science, set cover, NP-hard problem, graph problem, Approximation algorithms, unweighted vertex cover, undirected graph, approximability]
Satisfiability, branch-width and Tseitin tautologies
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
For a CNF /spl tau/, let w/sub b/(/spl tau/) be the branch-width of its underlying hypergraph. In this paper we design an algorithm for solving SAT in time n/sup O(1)/2/sup O(w(b)(/spl tau/))/. This in particular implies a polynomial algorithm for testing satisfiability on instances with tree-width O(log n). Our algorithm is a modification of the width based automated theorem prover (WBATP) which is a popular (at least on the theoretical level) heuristic for finding resolution refutations of unsatisfiable CNFs. We show that instead of the exhaustive enumeration of all provable clauses, one can do a better search based on the Robertson-Seymour algorithm for approximating the branch-width of a graph. We call the resulting procedure Branch-Width Based Automated Theorem Prover (BWBATP). As opposed to WBATP, it always produces regular refutations. Perhaps more importantly, the running time of our algorithm is bounded in terms of a clean combinatorial characteristic that can be efficiently approximated, and that the algorithm also produces, within the same time, a satisfying assignment if /spl tau/ happens to be satisfiable. In the second part of the paper we investigate the behavior of BWBATP on the Well-studied class of Tseitin tautologies. We argue that in this case BWBATP is better than WBATP. Namely, we show that its running time on any Tseitin tautology /spl tau/ is |/spl tau/|/sup O(1)/. 2/sup O(w(/spl tau//spl boxvr/O))/ as opposed to the obvious bound n/sup O(w(/spl tau//spl boxvr/O))/ provided by WBATP. This in particular implies that Resolution is automatizable on those Tseitin tautologies for which we know the relation w(/spl tau//spl boxvr//spl phi/) /spl les/ O(log S(/spl tau/)). We identify one such subclass and prove partial results toward establishing this relation for larger classes of graphs.
[Algorithm design and analysis, width based automated theorem prover, Particle separators, graph theory, Branch-Width Based Automated Theorem Prover, computability, regular refutations, clean combinatorial characteristic, polynomial algorithm, resolution refutations, Robertson-Seymour algorithm, NP-complete problem, Application software, Computer science, Tseitin tautologies, underlying hypergraph, Tree graphs, satisfiability, Approximation algorithms, Polynomials, Bipartite graph, theorem proving, Testing, computational complexity]
Static optimality theorem for external memory string access
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Data warehouses are increasingly storing and managing large scale string data, and dealing with large volume of transactions that update and search string data. Motivated by this context, we initiate the study of self-adjusting data structures for string dictionary operations, that is, data structures that are designed to be efficient on an entire sequence rather than individual string operations. Furthermore, we study this problem in the external memory model where string data is too massive to be stored in internal memory and has to reside in disks; each access to a disk page fetches B items, and the cost of the operations is the number of pages accessed (I/Os).
[static optimality theorem, Dictionaries, Navigation, string data, probability, Data warehouses, Data structures, large scale string data, Transaction databases, external memory string access, Tree graphs, XML, Character generation, string dictionary operations, data structures, Internet, Large-scale systems, self-adjusting data structures, memory model, data warehouses]
A dichotomy theorem for constraints on a three-element set
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
The Constraint Satisfaction Problem (CSP) provides a common framework for many combinatorial problems. The general CSP is known to be NP-complete; however, certain restrictions on the possible form of constraints may affect the complexity, and lead to tractable problem classes. There is, therefore, a fundamental research direction, aiming to separate those subclasses of the CSP which are tractable, from those which remain NP-complete. In 1978 Schaefer gave an exhaustive solution of this problem for the CSP on a 2-element domain. In this paper we generalise this result to a classification of the complexity of CSPs on a 3-element domain. The main result states that every subclass of the CSP defined by a set of allowed constraints is either tractable or NP-complete, and the criterion separating them is that conjectured by Bulatov et al. (2001). We also exhibit a polynomial time algorithm which, for a given set of allowed constraints, determines whether if this set gives rise to a tractable problem class. To obtain the main result and the algorithm we extensively use the algebraic technique for the CSP developed by Jeavons (1998) and Bulatov et al.
[Linear systems, 3-element domain, combinatorial mathematics, Laboratories, set theory, constraints, algebraic technique, Constraint theory, Polynomials, tractable problem class, three-element set, dichotomy theorem, constraint theory, constraint satisfaction problem, Spatial databases, Computational complexity, polynomial time algorithm, Computer science, combinatorial problems, Processor scheduling, Machine vision, operations research, subclass, Artificial intelligence, computational complexity]
A spectral algorithm for learning mixtures of distributions
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We show that a simple spectral algorithm for learning a mixture of k spherical Gaussians in /spl Ropf//sup n/ works remarkably well - it succeeds in identifying the Gaussians assuming essentially the minimum possible separation between their centers that keeps them unique. The sample complexity and running time are polynomial in both n and k. The algorithm also works for the more general problem of learning a mixture of "weakly isotropic" distributions (e.g. a mixture of uniform distributions on cubes).
[complexity, Engineering profession, Probability, Gaussian distribution, Mathematics, Gaussian approximation, Computer science, spectral algorithm, spherical Gaussian mixture learning, running time, Statistical distributions, Gaussian processes, weakly isotropic distribution mixture learning, Polynomials, learning (artificial intelligence), computational complexity]
Packing 2-dimensional bins in harmony
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider 2-Dimensional (Finite) Bin Packing (2BP), which is one of the most important generalizations of the well-known Bin Packing (BP) and calls for orthogonally packing a given set of rectangles (that cannot be rotated) into the minimum number of unit size squares. There are many open questions concerning the approximability of 2BP, whereas the situation for the 2-stage case, in which the items must first be packed into shelves that are then packed into bins, is essentially settled. For this reason, we study the asymptotic worst-case ratio between the optimal solution values of the 2-stage and general 2BP, showing that it is equal to T/sub /spl infin//=1.691..., the well-known worst-case ratio of the Harmonic algorithm for BP. This ratio is achieved by packing the items into shelves by decreasing heights as in the Harmonic algorithm and then optimally packing the resulting shelves into bins. This immediately yields polynomial time approximation algorithms for 2BP whose asymptotic worst-case ratio is arbitrarily close to T/sub /spl infin//, i.e. substantially smaller than 2+/spl epsi/, that was the best ratio achievable so far and constituted the first (recent) improvement over the 2.125 ratio shown in the early 80s. In particular, we manage to push the approximability threshold below 2, which is often a critical value in approximation. The main idea in our analysis is to use the fact that the fractional and integer BP solutions have almost the same value, which is implicit in the approximation schemes for the problem, as a stand-alone structural result. This implies the existence of modified heights for the shelves whose sum yields approximately the number of bins needed to pack them. With this in mind, our proof can easily be adapted to different cases. For instance, we can derive new upper bounds on the worst-case ratio of several shelf heuristics for 2BP, among which a bound of (17/10)(11/9)=2.077... (rather than 2.125) on the 20-years-lasting champion mentioned above. Moreover, we can easily derive the asymptotic worst-case ratio between the 2-stage and general 2BP solution values as a function of the maximum width of the rectangles, showing that this ratio is independent of the maximum height. Finally, under a conjecture that appears to be supported by experimental evidence, we show that our main heuristic has an asymptotic worst-case ratio in the interval (1.490,1.507) when all the rectangles to be packed are squares. This would improve the current best worst-case ratio of 14/9=1.555... for this special case.
[Strips, approximability threshold, 2D finite bin packing, bin packing, Computer science, asymptotic worst-case ratio, Upper bound, optimal solution values, Approximation algorithms, polynomial time approximation algorithms, Polynomials, harmonic algorithm, stand-alone structural result]
Equivalence between priority queues and sorting
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We present a general deterministic linear space reduction from priority queues to sorting implying that if we can sort up to n keys in S(n) time per key, then there is a priority queue supporting delete and insert in S(n)+O(1) time and find-min in constant time. Conversely, a priority queue can trivially be used for sorting: first insert all keys to be sorted, then extract them in sorted order by repeatedly deleting the minimum. Hence, asymptotically this settles the complexity of priority queues in terms of that of sorting. Besides nailing down the complexity of priority queues to that of sorting, and vice versa, we translate known sorting results into new results on priority queues for integers and strings in different computational models.
[Greedy algorithms, computational models, complexity, Costs, queueing theory, Computational modeling, Laboratories, Read-write memory, Data structures, deterministic algorithms, Scheduling algorithm, Sorting, integers, Processor scheduling, Operating systems, strings, sorting, priority queues, deterministic linear space reduction, computational complexity]
Fast approximation algorithms for fractional Steiner forest and related problems
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We give a fully polynomial time approximation scheme (FPTAS) for the optimum fractional solution to the Steiner forest problem. This can easily be generalized to obtain an FPTAS for a hitting set problem on a collection of clutters. We also identify three other problems on collections of clutters and show how these four problems are related when the clutters have the max-flow min-cut (MFMC) property. Two of these problems which are generalizations of maximum multicommodity flow and maximum concurrent flow have been well studied in the past and this paper is the first attempt at designing efficient algorithms for the other two problems. Our algorithms are very simple to describe and have running times better than those of existing algorithms. For clutters that do not satisfy the MFMC property (e.g., k-spanner, multicommodity flows, T-cuts, T-joins etc.), our algorithms are the only ones known (other than the generic algorithms for linear programming) for solving these hitting set problems.
[Algorithm design and analysis, Steiner trees, Costs, hitting set problem, NP-hard, combinatorial mathematics, optimum fractional solution, Linear programming, linear programming, clutters, Concurrent computing, fully polynomial time approximation, maximum concurrent flow, combinatorial optimization, optimisation, NP-hard problem, Steiner forest problem, Character generation, Approximation algorithms, Iterative algorithms, Polynomials, multicommodity flow, computational complexity]
Integer sorting in O(n/spl radic/(log log n)) expected time and linear space
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We present a randomized algorithm sorting n integers in O(n/spl radic/(log log n)) expected time and linear space. This improves the previous O(n log log n) bound by Anderson et al. (1995). As an immediate consequence, if the integers are bounded by U, we can sort them in O(n/spl radic/(log log U)) expected time. This is the first improvement over the O (n log log U) bound obtained with van Emde Boas' data structure. At the heart of our construction, is a technical deterministic lemma of independent interest; namely, that we split n integers into subsets of size at most /spl radic/n in linear time and space. This also implies improved bounds for deterministic string sorting and integer sorting without multiplication.
[Heart, linear space, Laboratories, data structure, Data structures, randomized algorithm, deterministic algorithms, deterministic string sorting, Sorting, randomised algorithms, Computer science, Computer languages, integer sorting, sorting, Cities and towns, Polynomials, data structures, deterministic lemma]
Quantum lower bounds for the collision and the element distinctness problems
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
Given a function f as an oracle, the collision problem is to find two distinct inputs i and j such that f(i)=f(j), under the promise that such inputs exist. In this paper, we prove that any quantum algorithm for finding a collision in an r-to-one function must evaluate the function /spl Omega/ ((n/r)/sup 1/3/) times, where n is the size of the domain and r|n. This lower bound matches, up to a constant factor, the upper bound of Brassard, Hoyer and Tapp (1997), which uses the quantum algorithm of Grover (1996) in a novel way. The previously best quantum lower bound is /spl Omega/ ((n/r)/sup 1/5/) evaluations, due to Aaronson (2002). Our result implies a quantum lower bound of /spl Omega/ (n/sup 2/3/) queries to the inputs for another well studied problem, the element distinctness problem, which is to determine whether or not the given n real numbers are distinct. The previous best lower bound is /spl Omega/ (/spl radic/n) queries in the black-box model; and /spl Omega/ (/spl radic/n log n) comparisons in the comparisons-only model, due to Hoyer Neerbek, and Shi (2001).
[Algorithm design and analysis, oracle, integer factorization, quantum algorithm, cryptography, collision problem, Computer science, quantum cryptanalysis, Quantum computing, Upper bound, quantum lower bound, Quantum mechanics, quantum cryptography, quantum computing, Cryptography, computational complexity]
Implicit B-trees: New results for the dictionary problem
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We reopen the issue of finding an implicit data structure for the dictionary problem. In particular, we examine the problem of maintaining n data values in the first n locations of an array in such a way that we can efficiently perform the operations insert, delete and search. No information other than n and the data is to be retained; and the only operations which we may perform on the data values (other than reads and writes) are comparisons. Our structure supports these operations in O(log/sup 2/ n/log log n) time, marking the first improvement on the problem since the mid 1980's. En route we develop a number of space efficient techniques for handling segments of a large array in a memory hierarchy. We achieve a cost of O(log/sub B/ n) block transfers like in regular B-trees, under the realistic assumption that a block stores B = /spl Omega/(log n) keys, so that reporting r consecutive keys in sorted order has a cost of O(log/sub B/n+r/B) block transfers. Being implicit, our B-tree occupies exactly [n/B] blocks after each update.
[block transfers, Educational programs, Dictionaries, Costs, merging, Memory, Data structures, implicit data structure, Cultural differences, encoding, space efficient techniques, implicit B-trees, Computer science, Moore's Law, Councils, dictionary problem, Computer science education, tree data structures]
Quantum computation and lattice problems
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We present the first explicit connection between quantum computation and lattice problems. Namely, we show a solution to the unique shortest vector problem (SVP) under the assumption that there exists an algorithm that solves the hidden subgroup problem on the dihedral group by coset sampling. Moreover, we solve the hidden subgroup problem on the dihedral group by using an average case subset sum routine. By combining the two results, we get a quantum reduction from /spl Theta//spl tilde/(n/sup 2.5/)-unique-SVP to the average case subset sum problem. This is a better connection than the known classical results.
[Pervasive computing, coset sampling, Computational modeling, dihedral group, Lattices, lattice theory, average case subset sum routine, Vectors, Application software, quantum reduction, group theory, Quantum computing, lattice problems, Physics computing, hidden subgroup problem, quantum computing, unique shortest vector problem, Sampling methods, Polynomials, Cryptography, quantum computation, computational complexity]
An inverse-Ackermann style lower bound for the online minimum spanning tree verification problem
The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.
None
2002
We consider the problem of preprocessing an edge-weighted tree T in order to quickly answer queries of the following type: does a given edge e belong in the minimum spanning tree of T /spl cup/ {e}? Whereas the offline minimum spanning tree verification problem admits a lovely linear time solution, we demonstrate an inherent inverse-Ackermann type tradeoff in the online MST verification problem. In particular, any scheme that answers queries in t comparisons must invest /spl Omega/(n log /spl lambda//sub t/ (n)) time preprocessing the tree, where /spl lambda//sub t/ is the inverse of the t/sup th/ row of Ackermann's function. This implies a query lower bound of /spl Omega/(/spl alpha/(n)) for the case of linear preprocessing time. We also show that our lower bound is tight to within a factor of 2 in the t parameter.
[Tree data structures, edge weighted tree, online minimum spanning tree verification, preprocessing, computational geometry, Data structures, inverse-Ackermann style lower bound, Computer science, linear time solution, query lower bound, tree data structures, Decision trees, computational complexity]
Stability and efficiency of a random local load balancing protocol
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We study the long term (steady state) performance of a simple, randomized, local load balancing technique. We assume a system of n processors connected by an arbitrary network topology. Jobs are placed in the processors by a deterministic or randomized adversary. The adversary knows the current and past load distribution in the network and can use this information to place the new tasks in the processors. The adversary can put a number of new jobs in each processor, in each step, as long as the (expected) total number of new jobs arriving at a given step is bounded by /spl lambda/n. A node can execute one job per step, and also participate in one load balancing operation in which it can move tasks to a direct neighbor in the network. In the protocol we analyze here, a node equalizes its load with a random neighbor in the graph. We first study the stability of a system running our load balancing protocol. Clearly, if /spl lambda/ > 1 the system cannot be stable. We show that for any /spl lambda/ < 1, and any connected network topology, the system is stable. When the system is stable, the next performance parameter of interest is the waiting time of jobs. We develop high probability bounds and bounds on the expectation of the waiting time of jobs in terms of the network topology. In particular, if the network is an expander graph the expected wait of a task is O(log n), and the waiting time of a task that enters the network at an arbitrary time is O(log n) with high probability. We contrast these results with the work stealing load balancing protocol, where we show that, in sparse networks, the load in the system and the waiting time can be exponential in the network size.
[steady state performance, Protocols, past load distribution, random local load balancing protocol efficiency, network load, Heuristic algorithms, work stealing load balancing protocol, Steady-state, processors, processor scheduling, job execution, Runtime, deterministic adversary, Network topology, resource allocation, randomized adversary, protocols, Load modeling, waiting time, Stability, long term performance, current load distribution, probability bounds, Graph theory, random local load balancing protocol stability, randomised algorithms, Computer science, distributed algorithms, Load management, bounded arrival, computational complexity, arbitrary network topology]
Average case and smoothed competitive analysis of the multi-level feedback algorithm
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
In this paper, we introduce the notion of smoothed competitive analysis of online algorithms. Smoothed analysis has been proposed by Spielman and Teng (2001) to explain the behavior of algorithms that work well in practice while performing very poorly from a worst case analysis point of view. We apply this notion to analyze the Multi-Level Feedback (MLF) algorithm to minimize the total flow time on a sequence of jobs released over time when the processing time of a job is only known at time of completion. The initial processing times are integers in the range [1,2/sup K/] We use a partial bit randomization model, where the initial processing times are smoothened by changing the k least significant bits under a quite general class of probability distributions. We show that MLF admits a smoothed competitive ratio of O((2/sup k///spl sigma/)/sup 3/ + (2/sup k///spl sigma/)/sup 2/2/sup K-k/), where /spl sigma/ denotes the standard deviation of the distribution. In particular, we obtain a competitive ratio of O(2/sup K-k/) if /spl sigma/ = /spl Theta/(2/sup k/). We also prove an /spl Omega/(2/sup K-k/) lower bound for any deterministic algorithm that is run on processing times smoothened according to the partial bit randomization model. For various other smoothening models, we give a higher lower bound of /spl Omega/(2/sup K/). A direct consequence of our result is also the first average case analysis of MLF. We show a constant expected ratio of the total flow time of MLF to the optimum under several distributions including the uniform distribution.
[Algorithm design and analysis, Computer aided software engineering, jobs sequence, smoothing methods, multilevel feedback algorithm, Gaussian distribution, partial bit randomization model, Probability distribution, average case analysis, completion time, smoothed competitive ratio, Information analysis, integers, feedback, worst case analysis, Feedback, probability distributions, Computer networks, Performance analysis, Contracts, smoothed analysis, competitive algorithms, smoothed competitive analysis, total flow time minimization, lower bound, deterministic algorithm, deterministic algorithms, online algorithms, algorithm behavior, randomised algorithms, Computer science, processing time, benchmark testing, minimisation, standard deviation]
Symmetric polynomials over /spl Zopf//sub m/ and simultaneous communication protocols
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We study the problem of representing symmetric Boolean functions as symmetric polynomials over /spl Zopf//sub m/. We show an equivalence between such representations and simultaneous communication protocols. Computing a function f on 0 - 1 inputs with a polynomial of degree d modulo pq is equivalent to a two player simultaneous protocol for computing f where one player is given the first [log/sub p/d] digits of the weight in base q. This reduces the problem of proving bounds on the degree of symmetric polynomials to proving bounds on simultaneous communication protocols. We use this equivalence to show lower bounds of /spl Omega/(n) on symmetric polynomials weakly representing classes of Mod/sub r/ and Threshold functions. We show there exist symmetric polynomials over /spl Zopf//sub m/ of degree o(n) strongly representing Threshold c for c constant, using the fact that the number of solutions of certain exponential Diophantine equations are finite. Conversely, the fact that the degree is o(n) implies that some classes of Diophantine equations can have only finitely many solutions. Our results give simplifications of many previously known results and show that polynomial representations are intimately related to certain questions in number theory.
[Protocols, Complexity theory, communication complexity, /spl Zopf//sub m/, bound proving, Boolean functions, polynomial representations, Polynomials, protocols, threshold functions, equivalence, finite solutions, simultaneous communication protocols, polynomials, degree, Educational institutions, exponential Diophantine equations, Application software, symmetric Boolean functions, Equations, lower bounds, Computer science, rings, symmetric polynomials, Character generation, Chromium, modulo, number theory]
The resolution complexity of random constraint satisfaction problems
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We consider random instances of constraint satisfaction problems where each variable has domain size d, and each constraint contains t restrictions on k variables. For each (d, k, t) we determine whether the resolution complexity is a.s. constant, polynomial or exponential in the number of variables. For a particular range of (d, k, t) we determine a sharp threshold for resolution complexity where the resolution complexity drops from a.s. exponential to a.s. polynomial when the clause density passes a specific value.
[domain size, variables, polynomial resolution complexity, constraint theory, resolution complexity, random processes, clause density, computability, restrictions, constant resolution complexity, Computer science, Chromium, random constraint satisfaction problems, Polynomials, exponential resolution complexity, Artificial intelligence, computational complexity]
I/O-efficient strong connectivity and depth-first search for directed planar graphs
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We present the first I/O-efficient algorithms for the following fundamental problems on directed planar graphs: finding the strongly connected components, finding a simple-path 2/3-separator, and computing a depth-first spanning (DFS) tree. Our algorithms for the first two problems perform O(sort(N)) I/Os, where N = V + E and sort(N) = /spl Theta/((N/B)) is the number of I/Os required to sort N elements. The DFS-algorithm performs O(sort(N) log(N/M)) I/Os, where M is the number of elements that fit into main memory.
[Performance evaluation, Geographic Information Systems, Engineering profession, I/O-efficient algorithm, Electronic switching systems, tree searching, Sorting, directed planar graph, Computer science, Runtime, DFS tree, Tree graphs, depth-first search, directed graphs, depth-first spanning, tree data structures, I/O-efficient strong connectivity, Fellows, computational complexity]
A group-theoretic approach to fast matrix multiplication
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We develop a new, group-theoretic approach to bounding the exponent of matrix multiplication. There are two components to this approach: (1) identifying groups G that admit a certain type of embedding of matrix multiplication into the group algebra /spl Copf/[G], and (2) controlling the dimensions of the irreducible representations of such groups. We present machinery and examples to support (1), including a proof that certain families of groups of order n/sup 2+o(1)/ support n /spl times/ n matrix multiplication, a necessary condition for the approach to yield exponent 2. Although we cannot yet completely achieve both (1) and (2), we hope that it may be possible, and we suggest potential routes to that result using the constructions in this paper.
[exponent bounding, group algebra, irreducible group representations, Matrix decomposition, History, Machinery, Equations, Computer science, group theory, matrix multiplication, Fast Fourier transforms, Sections, group identification, Linear algebra, representation theory, Polynomials, group-theoretic approach, Mirrors, fast matrix multiplication, dimension control, computational complexity]
Rank bounds and integrality gaps for cutting planes procedures
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We present a new method for proving rank lower bounds for Cutting Planes (CP) and several procedures based on lifting due to Lovasz and Schrijver (LS), when viewed as proof systems for unsatisfiability. We apply this method to obtain the following new results: first, we prove near-optimal rank bounds for Cutting Planes and Lovasz-Schrijver proofs for several prominent unsatisfiable CNF examples, including random kCNF formulas and the Tseitin graph formulas. It follows from these lower bounds that a linear number of rounds of CP or LS procedures when applied to relaxations of integer linear programs is not sufficient for reducing the integrality gap. Secondly, we give unsatisfiable examples that have constant rank CP and LS proofs but that require linear rank resolution proofs. Thirdly, we give examples where the CP rank is O(log n) but the LS rank is linear. Finally, we address the question of size versus rank: we show that, for both proof systems, rank does not accurately reflect proof size. Specifically, there are examples with polynomial-size CP/LS proofs, but requiring linear rank.
[CP rank, proof systems, integer linear programming, integer programming, unsatisfiability, polynomial-size CP/LS proofs, Optimization methods, cutting planes procedures, integrality gap, computability, near-optimal rank bounds, linear programming, random kCNF formulas, Ellipsoids, size versus rank, boundary integral equations, Integral equations, integrality gaps, Polynomials, theorem proving, Tseitin graph formulas, O(log n), Instruments, Lovasz-Schrijver proofs, Linear programming, rank bound proving, linear rank resolution proofs, Computer science, Councils, integer linear programs, Integer linear programming, unsatisfiable CNF examples, computational complexity]
Breaking a time-and-space barrier in constructing full-text indices
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Suffix trees and suffix arrays are the most prominent full-text indices, and their construction algorithms are well studied. It has been open for a long time whether these indices can be constructed in both O(n log n) time and O(n log n)-bit working space, where n denotes the length of the text. In the literature, the fastest algorithm runs in O(n) time, while it requires O(n log n)-bit working space. On the other hand, the most space-efficient algorithm requires O(n)-bit working space while it runs in O(n log n) time. This paper breaks the long-standing time-and-space barrier under the unit-cost word RAM. We give an algorithm for constructing the suffix array which takes O(n) time and O(n)-bit working space, for texts with constant-size alphabets. Note that both the time and the space bounds are optimal. For constructing the suffix tree, our algorithm requires O(n log/sup /spl epsi//n) time and O(n)-bit working space for any 0 < /spl epsi/ < 1. Apart from that, our algorithm can also be adopted to build other existing full-text indices, such as Compressed Suffix Tree, Compressed Suffix Arrays and FM-index. We also study the general case where the size of the alphabet A is not constant. Our algorithm can construct a suffix array and a suffix tree using optimal O(n log |A|)-bit working space while running in O(n log log |A|) time and O(n log/sup /spl epsi//n) time, respectively. These are the first algorithms that achieve 0(n log n) time with optimal working space, under a reasonable assumption that log |A| = o(log n).
[Biotechnology, text analysis, constant-size alphabets, compressed suffix tree, space-efficient algorithm, Proteins, unit-cost word RAM, polynomial time, tree data structures, FM-index, Sequences, time-and-space barrier, indexing, trees (mathematics), Read-write memory, construction algorithm, Data structures, Information technology, full-text index, Computer science, compressed suffix array, DNA, Chromium, Indexing, computational complexity]
Separating the power of monotone span programs over different fields
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Monotone span programs are a linear-algebraic model of computation. They are equivalent to linear secret sharing schemes and have various applications in cryptography and complexity. A fundamental question is how the choice of the field in which the algebraic operations are performed effects the power of the span program. In this paper we prove that the power of monotone span programs over finite fields of different characteristics is incomparable; we show a super-polynomial separation between any two fields with different characteristics, answering an open problem of Pudlak and Sgall (1998). Using this result we prove a super-polynomial lower bound for monotone span programs for a function in uniform - /spl Nscr/;/spl Cscr/;/sup 2/ (and therefore in /spl Pscr/;), answering an open problem of Babai, Wigderson, and Gal (1999). Finally, we show that quasi-linear schemes, a generalization of linear secret sharing schemes introduced in Beimel and Ishai (2001), are stronger than linear secret sharing schemes. In particular, this proves, without any assumptions, that non-linear secret sharing schemes are more efficient than linear secret sharing schemes.
[linear secret sharing schemes, Circuits, finite fields, algebraic computational model, nonlinear secret sharing schemes, uniform function, Cryptography, quasilinear schemes, linear algebra, super-polynomial separation, Computational modeling, polynomials, cryptography, Vectors, monotone span program power, Galois fields, Computational complexity, Combinatorial mathematics, super-polynomial lower bound, Computer science, Linear algebra, linear computation, linear-algebraic model, Arithmetic, computational complexity]
On worst-case to average-case reductions for NP problems
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We show that if an NP-complete problem has a non-adaptive self-corrector with respect to a distribution that can be sampled then coNP is contained in AM/poly and the polynomial hierarchy collapses to the third level. Feigenbaum and Fortnow show the same conclusion under the stronger assumption that an NP-complete problem has a non-adaptive random self-reduction. Our result shows it is impossible (using non-adaptive reductions) to base the average-case hardness of a problem in NP or the security of a one-way function on the worst-case complexity of an NP-complete problem (unless the polynomial hierarchy collapses).
[average case reductions, polynomial hierarchy, nonadaptive self corrector, worst case complexity, average case hardness, cryptography, NP problems, third level collapse, Security, NP-complete problem, one-way function, Computer science, nonadaptive random self reduction, distributional NP, Public key cryptography, Polynomials, worst case reductions, computational complexity]
An in-place sorting with O(n log n) comparisons and O(n) moves
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We present the first in-place algorithm for sorting an array of size n that performs, in the worst case, at most O(n log n) element comparisons and O(n) element transports. This solves a long-standing open problem, stated explicitly, e.g., in J.I. Munro and V. Raman (1992), of whether there exists a sorting algorithm that matches the asymptotic lower bounds on all computational resources simultaneously.
[in-place sorting, merging, Merging, asymptotic lower bound, O(n) element transports, sorting algorithm, History, O(n log n) element comparisons, Sorting, Computer science, storage management, Upper bound, computational resources, Layout, in-place algorithm, sorting, Internet, Contracts, computational complexity]
Solving sparse, symmetric, diagonally-dominant linear systems in time O(m/sup 1.31/
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We present a linear-system solver that, given an n-by-n symmetric positive semi-definite, diagonally dominant matrix A with m non-zero entries and an n-vector b, produces a vector x/spl tilde/ within relative distance /spl epsi/ of the solution to Ax = b in time O(m/sup 1.31/log(n//spl epsi/)b/sup O(1)/), where b is the log of the ratio of the largest to smallest non-zero entry of A. If the graph of A has genus m/sup 2/spl theta// or does not have a K/sub m/spl theta// minor, then the exponent of m can be improved to the minimum of 1 + 5/spl theta/ and (9/8)(1 + /spl theta/). The key contribution of our work is an extension of Vaidya's techniques for constructing and analyzing combinatorial preconditioners.
[Linear systems, Gradient methods, sparse symmetric diagonally-dominant linear systems, Transmission line matrix methods, combinatorial mathematics, Mathematics, Sparse matrices, combinatorial preconditioners, optimization, average degree construction, Chebyshev approximation, solution time, positive diagonals, elliptic differential equations, Iterative methods, n-by-n symmetric positive semi-definite diagonally dominant matrix, linear-system solver, Symmetric matrices, scientific computing, fast algorithms, nonzero entries, Computer science, Artificial intelligence, sparse matrices, computational complexity]
On /spl epsiv/-biased generators in NC/sup 0/
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
M. Cryan and P.B. Miltersen (2001) recently considered the question of whether there can be a pseudorandom generator in NC/sup 0/, that is, a pseudorandom generator that maps n bits strings to m bits strings and such that every bit of the output depends on a constant number k of bits of the seed. They show that for k = 3, if m /spl ges/ 4n + 1, there is a distinguisher; in fact, they show that in this case it is possible to break the generator with a linear test, that is, there is a subset of bits of the output whose XOR has a noticeable bias. They leave the question open for k /spl ges/ 4. In fact they ask whether every NC/sup 0/ generator can be broken by a statistical test that simply XORs some bits of the input. Equivalently, is it the case that no NC/sup 0/ generator can sample an /spl epsiv/-biased space with negligible /spl epsiv/? We give a generator for k = 5 that maps n bits into cn bits, so that every bit of the output depends on 5 bits of the seed, and the XOR of every subset of the bits of the output has bias 2/sup -/spl Omega/(n/c4)/. For large values of k, we construct generators that map n bits to n/sup /spl Omega/(/spl radic/k)/ bits and such that every XOR of outputs has bias 2/sup -n1/(2/spl radic/k)/. We also present a polynomial-time distinguisher for k = 4, m /spl ges/ 24n having constant distinguishing probability. For large values of k we show that a linear distinguisher with a constant distinguishing probability exists once m /spl ges/ /spl Omega/(2/sup k/n/sup [k/2]/). Finally, we consider a variant of the problem where each of the output bits is a degree k polynomial in the inputs. We show there exists a degree k = 2 pseudorandom generator for which the XOR of every subset of the outputs has bias 2/sup -/spl Omega/(n)/ and which map n bits to /spl Omega/(n/sup 2/) bits.
[linear distinguisher, /spl epsiv/-biased space, pseudorandom generator, Circuits, NC/sup 0/, /spl epsiv/-biased generator, random number generation, Research and development, Computer science, distinguishing probability, Polynomials, XOR, polynomial time, Random variables, National security, linear testing, Contracts, computational complexity]
The Ising model on trees: boundary conditions and mixing time
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We give the first comprehensive analysis of the effect of boundary conditions on the mixing time of the Glauber dynamics for the Ising model. Specifically, we show that the mixing time on an n-vertex regular tree with (+) boundary remains O(n log n) at all temperatures (in contrast to the free boundary case, where the mixing time is not bounded by any fixed polynomial at low temperatures). We also show that this bound continues to hold in the presence of an arbitrary external field. Our results are actually stronger, and provide tight bounds on the log-Sobolev constant and the spectral gap of the dynamics. In addition, our methods yield simpler proofs and stronger results for the mixing time in the regime where it is insensitive to the boundary condition. Our techniques also apply to a much wider class of models, including those with hard constraints like the antiferromagnetic Potts model at zero temperature (colorings) and the hard-core model (independent sets).
[Temperature, Phase measurement, Mathematics, Glauber dynamics, trees, graph colouring, hard-core model, Ising model, Polynomials, antiferromagnetic Potts model, boundary condition, antiferromagnetism, trees (mathematics), Antiferromagnetic materials, colorings, Boundary conditions, Potts model, independent sets, Statistics, log-Sobolev constant, Computer science, zero temperature, Character generation, Chromium, Markov processes, spectral gap, computational complexity, mixing time]
Learning DNF from random walks
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We consider a model of learning Boolean functions from examples generated by a uniform random walk on {0, 1}/sup n/. We give a polynomial time algorithm for learning decision trees and DNF formulas in this model. This is the first efficient algorithm for learning these classes in a natural passive learning model where the learner has no influence over the choice of examples used for learning.
[Algorithm design and analysis, Humans, Knowledge representation, Fourier analysis, Boolean function, Mathematics, passive learning model, Statistics, polynomial time algorithm, learning decision tree, Computer science, Learning systems, random walk, Boolean functions, disjunctive normal form, decision trees, Polynomials, Decision trees, learning (artificial intelligence), DNF, computational complexity]
More on average case vs approximation complexity
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We consider the problem to determine the maximal number of satisfiable equations in a linear system chosen at random. We make several plausible conjectures about the average case hardness of this problem for some natural distributions on the instances, and relate them to several interesting questions in the theory of approximation algorithms and in cryptography. Namely we show that our conjectures imply the following facts: (1) Feige's hypothesis about the hardness of refuting a random 3CNF is true, which in turn implies inapproximability within a constant for several combinatorial problems, for which no NP-hardness of approximation is known. (2) It is hard to approximate the nearest codeword within factor n/sup 1 - /spl epsi//. (3) It is hard to estimate the rigidity of a matrix. More exactly, it is hard to distinguish between matrices of low rigidity and random ones. (4) There exists a secure public-key (probabilistic) cryptosystem, based on the intractability of decoding of random binary codes. Our conjectures are strong in that they assume cryptographic hardness: no polynomial algorithm can solve the problem on any non-negligible fraction of inputs. Nevertheless, to the best of our knowledge no efficient algorithms are currently known that refute any of our hardness conjectures.
[Linear systems, Feiges hypothesis, Computer aided software engineering, probabilistic cryptosystem, public-key cryptosystem, matrix rigidity, approximation complexity, combinatorial problem, computability, polynomial algorithm, hardness conjecture, random 3CNF, public key cryptography, Polynomials, Cryptography, approximation theory, binary codes, codeword, average case hardness, cryptography, Decoding, NP-complete problem, Equations, decoding, Computer science, NP-hardness, Public key, random binary code, Approximation algorithms, cryptographic hardness, computational complexity]
Polynomial degree vs. quantum query complexity
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
The degree of a polynomial representing (or approximating) a function f is a lower bound for the quantum query complexity of f. This observation has been a source of many lower bounds on quantum algorithms. It has been an open problem whether this lower bound is tight. We exhibit a function with polynomial degree M and quantum query complexity (M/sup 1.321.../). This is the first superlinear separation between polynomial degree and quantum query complexity. The lower bound is shown by a new, more general version of quantum adversary method.
[quantum query complexity, quantum adversary method, Search problems, quantum algorithm, Mathematics, Complexity theory, Approximation methods, communication complexity, superlinear separation, Computer science, Quantum computing, Boolean functions, polynomial representation, Councils, polynomial approximation, quantum computing, Computer errors, polynomial degree, Polynomials]
Bounded-concurrent secure two-party computation in a constant number of rounds
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We consider the problem of constructing a general protocol for secure two-party computation in a way that preserves security under concurrent composition. In our treatment, we focus on the case where an a-priori bound on the number of concurrent sessions is specified before the protocol is constructed. (a.k.a. bounded concurrency). We make no setup assumptions. Lindel (STOC 2003) has shown that any protocol for bounded-concurrent secure two-party computation, whose security is established via black-box simulation, must have round complexity that is strictly larger than the bound on the number of concurrent sessions. In this paper, we construct a (non black-box) protocol for realizing bounded-concurrent secure two-party computation in a constant number of rounds. Our constructions rely on the existence of enhanced trapdoor permutations, as well as on the existence of hash functions that are collision-resistant against subexponential sized circuits.
[nonblack-box protocol, Laboratories, constant rounds, hash functions, Concurrent computing, subexponential sized circuits, trapdoor permutations, Polynomials, protocols, Computer security, bounded-concurrent two-party computation, Computational modeling, Circuit simulation, cryptography, concurrency theory, concurrent composition, round complexity, Cryptographic protocols, Computer science, bounded concurrency, general protocol construction, functionality evaluation, black box simulation, Interleaved codes, concurrent sessions, Random variables, secure two-party computation, a-priori bound, computational complexity]
Approximation algorithms for orienteering and discounted-reward TSP
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
In this paper, we give the first constant-factor approximation algorithm for the rooted orienteering problem, as well as a new problem that we call the Discounted-Reward TSP, motivated by robot navigation. In both problems, we are given a graph with lengths on edges and prizes (rewards) on nodes, and a start node s. In the orienteering problem, the goal is to find a path that maximizes the reward collected, subject to a hard limit on the total length of the path. In the Discounted-Reward TSP, instead of a length limit we are given a discount factor /spl gamma/, and the goal is to maximize total discounted reward collected, where reward for a node reached at time t is discounted by /spl gamma//sup t/. This is similar to the objective considered in Markov decision processes (MDPs) except we only receive a reward the first time a node is visited. We also consider tree and multiple-path variants of these problems and provide approximations for those as well. Although the unrooted orienteering problem, where there is no fixed start node s, has been known to be approximable using algorithms for related problems such as k-TSP (in which the amount of reward to be collected is fixed and the total length is approximately minimized), ours is the first to approximate the rooted question, solving an open problem based on B. Awerbuch et al. (1999) and E.M. Arkin (1998).
[discounted-reward TSP, robot navigation, approximation theory, Navigation, Power supplies, Laboratories, multiple-path variant, Traveling salesman problems, Path planning, travelling salesman problem, Batteries, discount factor, Computer science, travelling salesman problems, tree variant, length limit, Markov decision processes, k-TSP, decision trees, Packaging, approximation algorithm, rooted orienteering problem, Approximation algorithms, Robots]
List-decoding using the XOR lemma
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We show that Yao's XOR Lemma, and its essentially equivalent rephrasing as a Direct Product Lemma, can be re-interpreted as a way of obtaining error-correcting codes with good list-decoding algorithms from error-correcting codes having weak unique-decoding algorithms. To get codes with good rate and efficient list decoding algorithms, one needs a proof of the Direct Product Lemma that, respectively, is strongly derandomized, and uses very small advice. We show how to reduce advice in Impagliazzo's proof of the Direct Product Lemma for pairwise independent inputs, which leads to error-correcting codes with O(n/sup 2/) encoding length, 0/sup /spl tilde//(n/sup 2/) encoding time, and probabilistic 0/sup /spl tilde//(n) list-decoding time. (Note that the decoding time is sub-linear in the length of the encoding.) Back to complexity theory, our advice-efficient proof of Impagliazzo's hard-core set results yields a (weak) uniform version of O'Donnell results on amplification of hardness in NP. We show that if there is a problem in NP that cannot be solved by BPP algorithms on more than a 1 - 1/(log n)/sup c/ fraction of inputs, then there is a problem in NP that cannot be solved by BPP algorithms on more than a 3/4 + 1/(log n)/sup c/ fraction of inputs, where c > 0 is an absolute constant.
[unique-decoding algorithm, pairwise independent input, NP hardness, encoding length, Complexity theory, Code standards, Distributed computing, Boolean functions, direct product lemma, probabilistic list-decoding time, theorem proving, list-decoding algorithm, error correction codes, polynomials, Encoding, encoding time, Decoding, decoding, randomised algorithms, Computer science, BPP algorithm, Error correction codes, XOR lemma, error-correcting code, computational complexity]
Machine learning: my favorite results, directions, and open problems
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
false
[Computer science, Machine learning]
The value of knowing a demand curve: bounds on regret for online posted-price auctions
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We consider price-setting algorithms for a simple market in which a seller has an unlimited supply of identical copies of some good, and interacts sequentially with a pool of n buyers, each of whom wants at most one copy of the good. In each transaction, the seller offers a price between 0 and 1, and the buyer decides whether or not to buy, by comparing the offered price to his privately-held valuation for the good. The price offered to a given buyer may be influenced by the outcomes of prior transactions, but each individual buyer participates only once. In this setting, what is the value of knowing the demand curve? In other words, how much revenue can an uninformed seller expect to obtain, relative to a seller with prior information about the buyers' valuations? The answer depends on how the buyers' valuations are modeled. We analyze three cases - identical, random, and worst-case valuations - in each case deriving upper and lower bounds which match within a sublogarithmic factor.
[electronic trading, demand forecasting, cost optimal control, upper bound, Mathematics, Probability distribution, lower bound, price setting algorithm, online transaction, sublogarithmic factor, Cost accounting, Computer science, Image analysis, Upper bound, online posted-price auction, transaction model, Pricing, good valuation, demand curve, pricing]
On the maximum satisfiability of random formulas
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Maximum satisfiability is a canonical NP-complete problem that appears empirically hard for random instances. At the same time, it is rapidly becoming a canonical problem for statistical physics. In both of these realms, evaluating new ideas relies crucially on knowing the maximum number of clauses one can typically satisfy in a random k-CNF formula. In this paper we give asymptotically tight estimates for this quantity. Our result gives very tight bounds for the fraction of satisfiable clauses in a random k-CNF. In particular, for k > 2 it improves upon all previously known such bound.
[random k-CNF formula, Operations research, asymptotically tight estimates, random instances, Glass, computability, Mathematics, Boolean functions, canonical problem, Benchmark testing, satisfiable clauses, Boolean CNF formula, Stationary state, probability, H infinity control, random processes, NP-complete problem, Statistics, Physics, Computer science, statistical physics, tight bounds, maximum satisfiability, random formulas, maximum clauses, computational complexity]
Instability of FIFO at arbitrarily low rates in the adversarial queuing model
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We study the stability of the commonly used packet forwarding protocol, FIFO (First In First Out), in the adversarial queuing model. We prove that FIFO can become unstable, i.e., lead to unbounded buffer-occupancies and queuing delays, at arbitrarily low injection rates. In order to demonstrate instability at rate r, we use a network of size polynomial in 1/r.
[Protocols, queueing theory, unbounded buffer-occupancy, Stability, queuing delay, first in first out, Stochastic processes, packet switching, Telecommunication traffic, adversarial queuing model, FIFO, Delay, Computer science, packet forwarding protocol, telecommunication network routing, Traffic control, Polynomials, Robustness, injection rate, protocols, Queueing analysis]
Simulated annealing in convex bodies and an O*(n/sup 4/) volume algorithm
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We present a new algorithm for computing the volume of a convex body in R/sup n/. The main ingredient of the algorithm is a "morphing" technique that can be viewed as a variant of simulated annealing. Its complexity is O*(n/sup 4/), improving on the previous best algorithm by a factor of n.
[Temperature distribution, complexity, Engineering profession, simulated annealing, Computational modeling, morphing technique, Optimization methods, computational geometry, Mathematics, randomized algorithm, Space stations, randomised algorithms, convex bodies, Search methods, Simulated annealing, Polynomials, polynomial time, volume computation algorithm, Artificial intelligence, computational complexity]
Performance analysis of dynamic processes
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
The article covers various approaches for modeling and analyzing dynamic processes in networks. Modeling the dynamic performance as a stochastic process, we apply tools from discrete and continuous time Markov processes theory, renewal theory and queuing theory to analyze the long term, steady state performance of the processes. Non-stochastic approaches include adversarial queuing theory, and game theory techniques.
[dynamic process, queueing theory, Stochastic processes, game theory, stochastic process, Steady-state, Game theory, Computer science, discrete time Markov process, Stochastic systems, adversarial queueing theory, renewal theory, Markov processes, continuous time Markov process, game theory technique, Computer networks, Routing protocols, Performance analysis, computer network reliability, Queueing analysis, performance analysis]
A non-Markovian coupling for randomly sampling colorings
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We study a simple Markov chain, known as the Glauber dynamics, for randomly sampling (proper) k-colorings of an input graph G on n vertices with maximum degree /spl Delta/ and girth g. We prove the Glauber dynamics is close to the uniform distribution after O(n log n) steps whenever k > (1 + /spl epsiv/)/spl Delta/, for all /spl epsiv/ > 0, assuming g /spl ges/ 9 and /spl Delta/ = /spl Omega/(log n). The best previously known bounds were k > 11/spl Delta//6 for general graphs, and k > 1.489/spl Delta/ for graphs satisfying girth and maximum degree requirements. Our proof relies on the construction and analysis of a non-Markovian coupling. This appears to be the first application of a non-Markovian coupling to substantially improve upon known results.
[girth requirements, sampling methods, Computational modeling, Computer simulation, Antiferromagnetic materials, nonMarkovian coupling, Mathematics, graph coloring, Glauber dynamics, random coloring sampling, uniform distribution, Physics, graph colouring, maximum degree requirements, Computer science, Markov chain, Character generation, Chromium, Markov processes, Sampling methods, Polynomials, computational complexity]
Always Good Turing: asymptotically optimal probability estimation
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
While deciphering the German Enigma code during World War II, I.J. Good and A.M. Turing considered the problem of estimating a probability distribution from a sample of data. They derived a surprising and unintuitive formula that has since been used in a variety of applications and studied by a number of researchers. Borrowing an information-theoretic and machine-learning framework, we define the attenuation of a probability estimator as the largest possible ratio between the per-symbol probability assigned to an arbitrarily-long sequence by any distribution, and the corresponding probability assigned by the estimator. We show that some common estimators have infinite attenuation and that the attenuation of the Good-Turing estimator is low, yet larger than one. We then derive an estimator whose attenuation is one, namely, as the length of any sequence increases, the per-symbol probability assigned by the estimator is at least the highest possible. Interestingly, some of the proofs use celebrated results by Hardy and Ramanujan on the number of partitions of an integer. To better understand the behavior of the estimator, we study the probability it assigns to several simple sequences. We show that some sequences this probability agrees with our intuition, while for others it is rather unexpected.
[estimation theory, probability, information-theoretic, per symbol probability, Computer science, Turing estimator, Turing machines, Good-Turing estimator, asymptotically optimal probability estimation, probability distribution, learning (artificial intelligence), machine-learning framework, number theory]
Switch scheduling via randomized edge coloring
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
The essence of an Internet router is an n /spl times/ n switch which routes packets from input to output ports. Such a switch can be viewed as a bipartite graph with the input and output ports as the two vertex sets. Packets arriving at input port i and destined for output port j can be modeled as an edge from i to j. Current switch scheduling algorithms view the routing of packets at each time step as a selection of a bipartite matching. We take the view that the switch scheduling problem across a sequence of time-steps is an instance of the edge coloring problem for a bipartite multigraph. Implementation considerations lead us to seek edge coloring algorithms for bipartite multigraphs that are fast, decentralized, and online. We present a randomized algorithm which has the desired properties, and uses only a near-optimal /spl Delta/ + o(/spl Delta/) colors on dense bipartite graphs arising in the context of switch scheduling. This algorithm extends to non-bipartite graphs as well. It leads to a novel switch scheduling algorithm which, for stochastic online edge arrivals, is stable, i.e. the queue length at each input port is bounded at all times. We note that this is the first decentralized switch scheduling algorithm that is also guaranteed to be stable.
[randomized edge coloring, Stochastic processes, packet switching, Switches, time step, queue length, communication complexity, Internet router, graph colouring, processor scheduling, scheduling algorithms, decentralized algorithm, switch scheduling, Traffic control, Hardware, Bipartite graph, packet routing, stochastic online edge arrivals, bipartite multigraph, Packet switching, queueing theory, output port, edge coloring problem, online algorithm, Routing, randomized algorithm, bipartite graph, Scheduling algorithm, Communication switching, bipartite matching, randomised algorithms, n /spl times/ n switch, vertex sets, telecommunication network routing, input port, near-optimal colors, Internet]
Hardness of approximating the shortest vector problem in high L/sub p/ norms
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We show that for every /spl epsi/ > 0, there is a constant p(/spl epsi/) such that for all integers p /spl ges/ p(/spl epsi/), it is NP-hard to approximate the shortest vector problem in L/sub p/ norm within factor p/sup 1 - /spl epsi// under randomized reductions. For large values of p, this improves the factor 2/sup 1/p/ - /spl delta/ hardness shown by D. Micciancio (1998).
[shortest vector problem, Lattices, Length measurement, Linear programming, History, randomized reduction, randomised algorithms, Computer science, Geometry, vectors, NP-hard problem, polynomial approximation, L/sub p/ norm, Gaussian processes, Public key cryptography, Polynomials, Books, computational complexity]
A lower bound for the bounded round quantum communication complexity of set disjointness
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We show lower bounds in the multi-party quantum communication complexity model. In this model, there are t parties where the ith party has input X/sub i/ /spl sube/ [n]. These parties communicate with each other by transmitting qubits to determine with high probability the value of some function F of their combined input (X/sub 1/,...,X/sub t/). We consider the class of Boolean valued functions whose value depends only on X/sub 1/ /spl cap/.../spl cap/ X/sub t/; that is, for each F in this class there is an f/sub F/ : 2/sup [n]/ /spl rarr/ {0,1}, such that F(X/sub 1/,...,X/sub t/) = f/sub F/(X/sub 1/ /spl cap/.../spl cap/ X/sub t/). We show that the t-party k-round communication complexity of F is /spl Omega/(s/sub m/(f/sub F/)/(k/sup 2/)), where s/sub m/(f/sub F/) stands for the monotone sensitivity of f/sub F/' and is defined by s/sub m/(f/sub F/) = /sup /spl utri// max/sub S/spl sube//[n] |{i : f/sub F/(S /spl cup/ {i}) /spl ne/ f/sub F/(S)}|. For two-party quantum communication protocols for the set disjointness problem, this implies that the two parties must exchange /spl Omega/(n/k/sup 2/) qubits. An upper bound of O(n/k) can be derived from the O(/spl radic/n) upper bound due to S. Aaronson and A. Ambainis (2003). For k = 1, our lower bound matches the /spl Omega/(n) lower bound observed by H. Buhrman and R. de Wolf (2001) (based on a result of A. Nayak (1999)), and for 2 /spl les/ k /spl Lt/ n/sup 1/4 /, improves the lower bound of /spl Omega/(/spl radic/n) shown by A. Razborov (2002). For protocols with no restrictions on the number of rounds, we can conclude that the two parties must exchange /spl Omega/(n/sup 1/3/) qubits. This, however, falls short of the optimal /spl Omega/ (/spl radic/n) lower bound shown by A. Razborov (2002). Our result is obtained by adapting to the quantum setting the elegant information-theoretic arguments of Z. Bar-Yossef et al. (2002). Using this method we can show similar lower bounds for the L/sub /spl infin// function considered in Z. Bar-Yossef et al. (2002).
[Career development, Protocols, Scholarships, monotone sensitivity, set disjointness, information-theoretic argument, Complexity theory, communication complexity, Distributed computing, Combinatorial mathematics, Boolean valued function, Computer science, Quantum computing, Upper bound, Boolean functions, quantum computing, qubits transmission, Books, quantum communication, quantum communication protocol, round quantum communication complexity]
Zero-knowledge sets
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We show how a polynomial-time prover can commit to an arbitrary finite set S of strings so that, later on, he can, for any string x, reveal with a proof whether x /spl isin/ S or x /spl notin/ S, without revealing any knowledge beyond the verity of these membership assertions. Our method is non interactive. Given a public random string, the prover commits to a set by simply posting a short and easily computable message. After that, each time it wants to prove whether a given element is in the set, it simply posts another short and easily computable proof, whose correctness can be verified by any one against the public random string. Our scheme is very efficient; no reasonable prior way to achieve our desiderata existed. Our new primitive immediately extends to providing zero-knowledge databases.
[Modular construction, Laboratories, trees (mathematics), polynomial-time prover, random string, cryptography, Mathematics, Security, Computer science, zero-knowledge set, zero-knowledge database, Upper bound, arbitrary finite set, National electric code, Polynomials, elementary database, computational complexity]
General composition and universal composability in secure multi-party computation
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Concurrent general composition relates to a setting where a secure protocol is run in a network concurrently with other, arbitrary protocols. Clearly, security in such a setting is what is desired, or even needed, in modern computer networks where many different protocols are executed concurrently. Our main result is a proof that security under concurrent general composition is equivalent to a relaxed variant of universal composability (where the only difference relates to the order of quantifiers in the definition). An important corollary of this theorem is that existing impossibility results for universal composability (or actually its relaxed variant) are inherent in any definition achieving security under concurrent general composition. We stress that the impossibility results obtained are not "black-box\
[nonblack box simulation, Protocols, multiprocessing systems, network security, concurrent general composition, computer networks, secure multiparty computation, cryptography, concurrency theory, Stress, Concurrent computing, Intelligent networks, Privacy, secure protocol, Computer networks, Polynomials, Random variables, Cryptography, protocols, impossibility results, Computer security, universal composability]
Paths, trees, and minimum latency tours
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We give improved approximation algorithms for a variety of latency minimization problems. In particular, we give a 3.59-approximation to the minimum latency problem, improving on previous algorithms by a multiplicative factor of 2. Our techniques also give similar improvements for related problems like k-traveling repairmen and its multiple depot variant. We also observe that standard techniques can be used to speed up the previous and this algorithm by a factor of O/sup /spl tilde//(n).
[approximation theory, Minimization methods, trees (mathematics), minimum latency tours, Educational institutions, Delay, Equations, Computer science, travelling salesman problems, Tree graphs, minimum latency problem, k-traveling repairmen, multiple depot variant, approximation algorithm, Approximation algorithms, Cost function, Space exploration, minimisation, latency minimization]
Bounded geometries, fractals, and low-distortion embeddings
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
The doubling constant of a metric space (X, d) is the smallest value /spl lambda/ such that every ball in X can be covered by /spl lambda/ balls of half the radius. The doubling dimension of X is then defined as dim (X) = log/sub 2//spl lambda/. A metric (or sequence of metrics) is called doubling precisely when its doubling dimension is bounded. This is a robust class of metric spaces which contains many families of metrics that occur in applied settings. We give tight bounds for embedding doubling metrics into (low-dimensional) normed spaces. We consider both general doubling metrics, as well as more restricted families such as those arising from trees, from graphs excluding a fixed minor, and from snowflaked metrics. Our techniques include decomposition theorems for doubling metrics, and an analysis of a fractal in the plane according to T. J. Laakso (2002). Finally, we discuss some applications and point out a central open question regarding dimensionality reduction in L/sub 2/.
[Algorithm design and analysis, decomposition theorem, low-distortion embedding, snowflaked metric, graph theory, computational geometry, Extraterrestrial measurements, doubling metrics, Fractals, Graph theory, Combinatorial mathematics, fixed minor, fractals, Computer science, Geometry, normed spaces, graphs, dimensionality reduction, Tree graphs, finite metric space, Robustness, bounded geometries, Power generation, computational complexity]
Locally testable cyclic codes
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Cyclic linear codes of block length n over a finite field F/sub q/ are the linear subspaces of F/sub q//sup n/ that are invariant under a cyclic shift of their coordinates. A family of codes is good if all the codes in the family have constant rate and constant normalized distance (distance divided by block length). It is a long-standing open problem whether there exists a good family of cyclic linear codes based on F.J. MacWilliams and N.J.A. Sloane (1977). A code C is r-testable if there exist a randomized algorithm which, given a word x /spl isin/ F/sub q//sup n/, adaptively selects r positions, checks the entries of x in the selected positions, and makes a decision (accept or reject x) based on the positions selected and the numbers found, such that (i) if x /spl isin/ C then x is surely accepted; (ii) if dist(x,C) /spl ges/ /spl epsi/n then x is probably rejected (dist refers to Hamming distance). A family of codes is locally testable if all members of the family are r-testable for some constant r. This concept arose from holographic proofs/PCPs. O. Goldreich and M. Sudan (2002) asked whether there exist good, locally testable families of codes. In this paper we address the intersection of the two questions stated.
[normalized distance, error correction codes, linear codes, Hamming distance, block length, linear code, holographic proof, cyclic codes, randomized algorithm, Galois fields, randomised algorithms, Computer science, block codes, testable cyclic code, Testing]
Group strategy proof mechanisms via primal-dual algorithms
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We develop a general method for turning a primal-dual algorithm into a group strategy proof cost-sharing mechanism. We use our method to design approximately budget balanced cost sharing mechanisms for two NP-complete problems: metric facility location, and single source rent-or-buy network design. Both mechanisms are competitive, group strategyproof and recover a constant fraction of the cost. For the facility location game our cost-sharing method recovers a 1/3rd of the total cost, while in the network design game the cost shares pay for a 1/15 fraction of the cost of the solution.
[Costs, Design methodology, primal-dual algorithm, multiprocessor interconnection networks, competitive algorithms, game theory, group strategy proof mechanism, Turning, NP-complete problem, communication complexity, cost-sharing mechanism, facility location, Computer science, single source rent-or-buy network design, metric facility location, NP-complete problems, IP networks, facility location game, network design game]
Gossip-based computation of aggregate information
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Over the last decade, we have seen a revolution in connectivity between computers, and a resulting paradigm shift from centralized to highly distributed systems. With massive scale also comes massive instability, as node and link failures become the norm rather than the exception. For such highly volatile systems, decentralized gossip-based protocols are emerging as an approach to maintaining simplicity and scalability while achieving fault-tolerant information dissemination. In this paper, we study the problem of computing aggregates with gossip-style protocols. Our first contribution is an analysis of simple gossip-based protocols for the computation of sums, averages, random samples, quantiles, and other aggregate functions, and we show that our protocols converge exponentially fast to the true answer when using uniform gossip. Our second contribution is the definition of a precise notion of the speed with which a node's data diffuses through the network. We show that this diffusion speed is at the heart of the approximation guarantees for all of the above problems. We analyze the diffusion speed of uniform gossip in the presence of node and link failures, as well as for flooding-based mechanisms. The latter expose interesting connections to random walks on graphs.
[Protocols, Scalability, gossip-style protocols, decentralized gossip-based protocols, Distributed computing, scalability, Temperature sensors, Fault tolerant systems, volatile systems, gossip-based computation, computer connectivity, distributed systems, aggregate computation, Large-scale systems, computer network reliability, protocols, diffusion speed, aggregate information, random walks, Peer to peer computing, fault-tolerant information dissemination, computer networks, flooding-based mechanisms, data diffusion, Stress, uniform gossip, Computer science, random samples, quantiles, Aggregates, instability, fault tolerant computing, exponential convergence, node failures, link failures, aggregate functions]
Linear upper bounds for random walk on small density random 3-CNFs
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We analyze the efficiency of the random walk algorithm on random 3-CNF instances, and prove linear upper bounds on the running time of this algorithm for small clause density, less than 1.63. Our upper bound matches the observed running time to within a multiplicative factor. This is the first sub-exponential upper bound on the running time of a local improvement algorithm on random instances. Our proof introduces a simple, yet powerful tool for analyzing such algorithms, which may be of further use. This object, called a terminator, is a weighted satisfying assignment. We show that any CNF having a good (small weight) terminator is assured to be solved quickly by the random walk algorithm. This raises the natural question of the terminator threshold which is the maximal clause density for which such assignments exist (with high probability). We use the analysis of the pure literal heuristic presented by Broder, Frieze and Upfal and show that for small clause densities good terminators exist. Thus we show that the pure literal threshold (/spl ap/ 1.63) is a lower bound on the terminator threshold. One nice property of terminators is that they can be found efficiently, via linear programming. This makes tractable the future investigation of the terminator threshold, and also provides an efficiently computable certificate for short running time of the simple random-walk heuristic.
[Algorithm design and analysis, random walk algorithm, maximal clause density, Clouds, Laboratories, random processes, small clause density, computability, Linear programming, Displays, linear programming, linear upper bounds, efficiency analysis, solvability, small density random 3-CNF, randomised algorithms, Computer science, Upper bound, running time, local improvement algorithm, sub-exponential upper bound, weighted satisfying assignment, pure literal heuristic, computational complexity]
The cost of cache-oblivious searching
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Tight bounds on the cost of cache-oblivious searching are proved. It is shown that no cache-oblivious search structure can guarantee that a search performs fewer than lg e log/sub B/N block transfers between any two levels of the memory hierarchy. This lower bound holds even if all of the block sizes are limited to be powers of 2. A modified version of the van Emde Boas layout is proposed, whose expected block transfers between any two levels of the memory hierarchy arbitrarily close to [lg e + O(lg lg B/ lgB)] logB N + O(1). This factor approaches lg e /spl ap/ 1.443 as B increases. The expectation is taken over the random placement of the first element of the structure in memory. As searching in the disk access model (DAM) can be performed in log/sub B/N + 1 block transfers, this result shows a separation between the 2-level DAM and cache-oblivious memory-hierarchy models. By extending the DAM model to k levels, multilevel memory hierarchies can be modeled. It is shown that as k grows, the search costs of the optimal k-level DAM search structure and of the optimal cache-oblivious search structure rapidly converge. This demonstrates that for a multilevel memory hierarchy, a simple cache-oblivious structure almost replicates the performance of an optimal parameterized k-level DAM structure.
[Algorithm design and analysis, block transfer, block size, Computational modeling, Laboratories, Random access memory, multilevel memory hierarchies, Read-write memory, memory hierarchy, cache storage, Helium, tree searching, disk access model, van Emde Boas layout, Computer science, Information science, memory architecture, cache-oblivious searching, Cost function, tree data structures, search structure, search costs, Contracts]
Proving hard-core predicates using list decoding
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We introduce a unifying framework for proving that predicate P is hard-core for a one-way function f, and apply it to a broad family of functions and predicates, reproving old results in an entirely different way as well as showing new hard-core predicates for well known one-way function candidates. Our framework extends the list-coding method of Goldreich and Levin for showing hard-core predicates. Namely, a predicate will correspond to some error correcting code, predicting a predicate will correspond to access to a corrupted codeword, and the task of inverting one-way functions will correspond to the task of list decoding a corrupted codeword. A characteristic of the error correcting codes which emerge and are addressed by our framework is that codewords can be approximated by a small number of heavy coefficients in their Fourier representation. Moreover, as long as corrupted words are close enough to legal codewords, they will share a heavy Fourier coefficient. We list decodes, by devising a learning algorithm applied to corrupted codewords for learning heavy Fourier coefficients. For codes defined over {0, 1}/sup n/ domain, a learning algorithm by Kushilevitz and Mansour already exists. For codes defined over Z/sub N/, which are the codes which emerge for predicates based on number theoretic one-way functions such as the RSA and Exponentiation modulo primes, we develop a new learning algorithm. This latter algorithm may be of independent interest outside the realm of hard-core predicates.
[learning algorithm, error correction codes, RSA prime, cryptography, Decoding, Fourier representation, Fourier coefficient, decoding, one-way function, Computer science, Boolean functions, exponentiation modulo primes, hard-core predicate, list decoding, error correcting code, learning (artificial intelligence), corrupted codeword, coefficient learning, computational complexity, OWF]
Logconcave functions: geometry and efficient sampling algorithms
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
The class of logconcave functions in R/sup n/ is a common generalization of Gaussians and of indicator functions of convex sets. Motivated by the problem of sampling from a logconcave density function, we study their geometry and introduce an analysis technique for "smoothing" them out. This leads to efficient sampling algorithms with no assumptions on the local smoothness of the density function. After appropriate preprocessing, both the ball walk (with a Metropolis filter) and a generalization of hit-and-run produce a point from approximately the right distribution in time O*(n/sup 4/), and in amortized time O*(n/sup 3/) if many sample points are needed (where the asterisk indicates that dependence on the error parameter and factors of log n are not shown). The bounds are optimal in terms of a "roundness" parameter and match the best-known bounds for the special case of the uniform density over a convex set.
[logconcave density function, Lattices, Stochastic processes, Gaussian distribution, computational geometry, Gaussian generalization, Mathematics, Probability distribution, density function smoothness, convex set, Filters, Density functional theory, ball walk, sampling methods, Engineering profession, time complexity, error parameter, Geometry, sampling algorithm, Metropolis filter, uniform density, Gaussian processes, Sampling methods, geometry, computational complexity]
Quantum search of spatial regions
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Can Grover's quantum search algorithm speed up search of a physical region - for example a 2D grid of size /spl radic/n x /spl radic/n? The problem is that /spl radic/n time seems to be needed for each query, just to move amplitude across the grid. Here we show that this problem can be surmounted, refuting a claim to the contrary by Benioff. In particular, we show how to search a d-dimensional hypercube in time 0(/spl radic/n) for d /spl ges/ 3, or 0(/spl radic/n log/sup 3/ n) for d = 2. More generally, we introduce a model of quantum query complexity on graphs, motivated by fundamental physical limits on information storage, particularly the holographic principle from black hole thermodynamics. Our results in this model include almost-tight upper and lower bounds for many search tasks; a generalized algorithm that works for any graph with good expansion properties, not just hypercubes; and relationships among several notions of locality for unitary matrices acting on graphs. As an application of our results, we give an 0(/spl radic/n)-qubit communication protocol for the disjointness problem, which improves an upper bound of Hoyer and de Wolf and matches a lower bound of Razborov.
[Protocols, Laboratories, graph theory, quantum query complexity, unitary matrix, Holography, information storage, disjointness problem, Physics, Orbital robotics, query processing, generalized algorithm, Thermodynamics, spatial region, Upper bound, Databases, quantum computing, 0(/spl radic/n)-qubit communication protocol, Hypercubes, black hole thermodynamics, quantum search algorithm, Robots, search problems, computational complexity]
Approximation algorithms for asymmetric TSP by decomposing directed regular multigraphs
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
A directed multigraph is said to be d-regular if the indegree and outdegree of every vertex is exactly d. By Hall's theorem one can represent such a multigraph as a combination of at most n/sup 2/ cycle covers each taken with an appropriate multiplicity. We prove that if the d-regular multigraph does not contain more than /spl lfloor/d/2/spl rfloor/ copies of any 2-cycle then we can find a similar decomposition into 0(n/sup 2/) pairs of cycle covers where each 2-cycle occurs in at most one component of each pair. Our proof is constructive and gives a polynomial algorithm to find such decomposition. Since our applications only need one such a pair of cycle covers whose weight is at least the average weight of all pairs, we also give a simpler algorithm to extract a single such pair. This combinatorial theorem then comes handy in rounding a fractional solution of an LP relaxation of the maximum and minimum TSP problems. For maximum TSP, we obtain a tour whose weight is at least 2/3 of the weight of the longest tour, improving a previous 5/8 approximation. For minimum TSP we obtain a tour whose weight is at most 0.842log/sub 2/ n times the optimal, improving a previous 0.999log/sub 2/ n approximation. Utilizing a reduction from maximum TSP to the shortest superstring problem we obtain a 2.5-approximation algorithm for the latter problem which is again much simpler than the previous one. Other applications of the rounding procedure are approximation algorithms for maximum 3-cycle cover (factor 2/3, previously 3/5) and maximum asymmetric TSP with triangle inequality (factor 10/13, previously 3/4 ).
[Algorithm design and analysis, LP relaxation, Halls theorem, graph theory, directed regular multigraph, combinatorial theorem, asymmetric TSP, Traveling salesman problems, polynomial algorithm, travelling salesman problem, Application software, maximum TSP problem, Computer science, travelling salesman problems, d-regular multigraph, polynomial approximation, Computer applications, minimum TSP problem, Biology computing, approximation algorithm, Approximation algorithms, Polynomials, theorem proving, Computational biology]
Mixing [Markov chain]
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
In this paper, we introduce the notion of a Markov chain and explore how it can be used to sample from a large set of configurations. Our primary focus is determining how quickly a Markov chain "mixes," or converges to its stationary distribution, as this is the key factor in the running time. We provide an overview of several techniques used to establish good bounds on the mixing time. The applications are mostly chosen from statistical physics, although the methods are much more general.
[Algorithm design and analysis, Pervasive computing, sampling methods, Computational modeling, Stochastic processes, random processes, State-space methods, Nearest neighbor searches, Convergence, Monte Carlo methods, stationary distribution, running time, Physics computing, Markov processes, Sampling methods, Markov chain mixing, mixing time]
Approximation via cost-sharing: a simple approximation algorithm for the multicommodity rent-or-buy problem
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We study the multicommodity rent-or-buy problem, a type of network design problem with economies of scale. In this problem, capacity on an edge can be rented, with cost incurred on a per-unit of capacity basis, or bought, which allows unlimited use after payment of a large fixed cost. Given a graph and a set of source-sink pairs, we seek a minimum-cost way of installing sufficient capacity on edges so that a prescribed amount of flow can be sent simultaneously from each source to the corresponding sink. The first constant-factor approximation algorithm for this problem was recently given by Kumar et al.; however, this algorithm and its analysis are both quite complicated, and its performance guarantee is extremely large. In this paper, we give a conceptually simple 12-approximation algorithm for this problem. Our analysis of this algorithm makes crucial use of cost sharing, the task of allocating the cost of an object to many users of the object in a "fair" manner. While techniques from approximation algorithms have recently yielded new progress on cost sharing problems, our work is the first to show the converse - those ideas from cost sharing can be fruitfully applied in the design and analysis of approximation algorithms.
[Algorithm design and analysis, approximation theory, graph theory, cost allocation, facility location, Computer science, edge capacity rental, minimum cost method, Economies of scale, Chromium, approximation algorithm, Approximation algorithms, Cost function, multicommodity rent-or-buy problem, Performance analysis, network design, economies of scale, cost-sharing, computational complexity]
Logics for reasoning about cryptographic constructions
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We present two logical systems for reasoning about cryptographic constructions which are sound with respect to standard cryptographic definitions of security. Soundness of the first system is proved using techniques from nonstandard models of arithmetic. Soundness of the second system is proved by an interpretation into the first system. We also present examples of how these systems may be used to formally prove the correctness of some elementary cryptographic constructions.
[nonstandard arithmetic models, reasoning, logical systems, soundness proving, cryptography, inference mechanisms, Programming profession, Cryptographic protocols, meta-logic, Computer science, cryptographic security definitions, Acoustical engineering, cryptographic constructions, Authentication, Logic, Cryptography, Computer security, formal deduction systems, Arithmetic]
Proofs of the Parisi and Coppersmith-Sorkin conjectures for the finite random assignment problem
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
Suppose that there are n jobs and n machines and it costs c/sub ij/ to execute job i on machine j. The assignment problem concerns the determination of a one-to-one assignment of jobs onto machines so as to minimize the cost of executing all the jobs. The average case analysis of the classical random assignment problem has received a lot of interest in the recent literature, mainly due to the following pleasing conjecture of Parisi: The average value of the minimum-cost permutation in an n /spl times/ n matrix with i.i.d. exp(1) entries equals /spl Sigma//sub i=1//sup n/ 1/(i/sup 2/). D. Coppersmith and G. Sorkin (1999) have generalized Parisi's conjecture to the average value of the smallest k-assignment when there are n jobs and m machines. We prove both conjectures based on a common set of combinatorial and probabilistic arguments.
[finite random assignment problem, Costs, combinatorial mathematics, combinatorial argument, probability, Linear programming, task analysis, parallel machines, matrix algebra, Computer science, k-assignment, Coppersmith-Sorkin conjecture, Upper bound, Character generation, classical random assignment problem, Chromium, probabilistic argument, job assignment, random functions, Random variables, theorem proving, minimum-cost permutation, Parisi conjecture]
A polynomial algorithm for recognizing perfect graphs
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We present a polynomial algorithm for recognizing whether a graph is perfect, thus settling a long standing open question. The algorithm uses a decomposition theorem of Conforti, Cornuejols and Vuskovic. Another polynomial algorithm for recognizing perfect graphs, which does not use decomposition, was obtained simultaneously by Chudnovsky and Seymour. Both algorithms need a first phase developed jointly by Chudnovsky, Cornuejols, Liu, Seymour and Vuskovic.
[Computer science, decomposition theorem, recognition algorithm, graph theory, perfect graph recognition, algorithm theory, polynomial algorithm, Polynomials, Cleaning, odd hole, Sun, Artificial intelligence]
On the impossibility of dimension reduction in /spl lscr//sub 1/
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
The Johnson-Lindenstrauss Lemma shows that any n points in Euclidean space (with distances measured by the /spl lscr//sub 2/ norm) may be mapped down to O((log n)//spl epsiv//sup 2/) dimensions such that no pairwise distance is distorted by more than a (1+ /spl epsiv/) factor. Determining whether such dimension reduction is possible in /spl lscr//sub 1/ has been an intriguing open question. We show strong lower bounds for general dimension reduction in /spl lscr//sub 1/. We give an explicit family of n points in /spl lscr//sub 1/ such that any embedding with distortion /spl delta/ requires n/sup /spl Omega/(1//spl delta/2)/ dimensions. This proves that there is no analog of the Johnson-Lindenstrauss Lemma for /spl lscr//sub 1/; in fact embedding with any constant distortion requires n/sup /spl Omega/(1)/ dimensions. Further, embedding the points into /spl lscr//sub 1/ with 1 + /spl epsiv/ distortion requires n/sup 1/2 -O(/spl epsiv/log(1//spl epsiv/))/ dimensions. Our proof establishes this lower bound for shortest path metrics of series-parallel graphs. We make extensive use of linear programming and duality in devising our bounds. We expect that the tools and techniques we develop will be useful for future investigations of embeddings into /spl lscr//sub 1/.
[Algorithm design and analysis, US Department of Energy, Embedded computing, Engineering profession, graph theory, pairwise distance, Linear programming, Extraterrestrial measurements, linear programming, series-parallel graph, lower bound, duality, Computer science, point embedding, Upper bound, Johnson-Lindenstrauss Lemma, Clustering algorithms, dimension reduction impossibility, Approximation algorithms, duality (mathematics), Euclidean space, computational complexity]
Deterministic extractors for bit-fixing sources and exposure-resilient cryptography
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We give an efficient deterministic algorithm which extracts /spl Omega/(n/sup 2/spl gamma//) almost-random bits from sources where n/sup 1/2 + /spl gamma// of the n bits are uniformly random and the rest are fixed in advance. This improves on previous constructions which required that at least n/2 of the bits be random. Our construction also gives explicit adaptive exposure-resilient functions and in turn adaptive all-or-nothing transforms. For sources where instead of bits the values are chosen from [d], for d > 2, we give an algorithm which extracts a constant fraction of the randomness. We also give bounds on extracting randomness for sources where the fixed bits can depend on the random bits.
[Computational modeling, random processes, cryptography, adaptive transform, Data mining, deterministic algorithm, deterministic algorithms, bit-fixing source, Computer science, exposure-resilient function, exposure-resilient cryptography, fixed bit, Cryptography, deterministic extractor, random bit, all-or-nothing transform, Protection]
On levels in arrangements of curves. II. A simple inequality and its consequences
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We give a surprisingly short proof that in any planar arrangement of n curves where each pair intersects at most a fixed number (s) of times, the k-level has subquadratic (O(n/sup 2-1/2s/)) complexity. This answers one of the main open problems from the author's previous paper, which provided a weaker bound for a restricted class of curves (graphs of degree-s polynomials) only. When combined with existing tools (cutting curves, sampling, etc.), the new idea generates a slew of improved k-level results for most of the curve families studied earlier, including a near-O(n/sup 3/2/) bound for parabolas.
[Algorithm design and analysis, curve cutting, polynomials, k-level, graph theory, polynomial graphs, subquadratic complexity, sampling, computational geometry, parabola, Computer science, Computational geometry, Upper bound, curve arrangement, Sampling methods, Polynomials, curve fitting, Kinetic theory, Books, planar arrangement, computational complexity]
Broadcasting algorithms in radio networks with unknown topology
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
In this paper we present new randomized and deterministic algorithms for the classical problem of broadcasting in radio networks with unknown topology. We consider directed n-node radio networks with specified eccentricity D (maximum distance from the source node to any other node). Our first main result closes the gap between the lower and upper bound: we describe an optimal randomized broadcasting algorithm whose running time complexity is O(D log(n/D) + log/sup 2/n), with high probability. In particular, we obtain a randomized algorithm that completes broadcasting in any n-node radio network in time O(n), with high probability. The main source of our improvement is a better "selecting sequence" used by the algorithm that brings some stronger property and improves the broadcasting time. Next, we demonstrate how to apply our approach to deterministic broadcasting, and describe a deterministic oblivious algorithm that completes broadcasting in almost optimal time O(n log/sup 2/D). Finally, we show how our randomized broadcasting algorithm can be used to improve the randomized complexity of the gossiping problem.
[radio broadcasting, radio networks, deterministic broadcasting, selecting sequence, gossiping problem, Broadcast technology, optimal randomized broadcasting algorithm, Electronic mail, communication complexity, Intelligent networks, Network topology, Radio network, Informatics, directed n-node radio networks, optimal time, running time complexity, eccentricity, upper bound, deterministic oblivious algorithm, randomized complexity, unknown topology, lower bound, Radio broadcasting, deterministic algorithms, randomised algorithms, broadcasting, Computer science, Upper bound, source node distance, randomized algorithms, broadcasting time, Radio networks]
Tight lower bounds for the distinct elements problem
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We prove strong lower bounds for the space complexity of (/spl epsi/, /spl delta/)-approximating the number of distinct elements F/sub 0/ in a data stream. Let m be the size of the universe from which the stream elements are drawn. We show that any one-pass streaming algorithm for (/spl epsi/, /spl delta/)-approximating F/sub 0/ must use /spl Omega/(1//spl epsi//sup 2/) space when /spl epsi/ = /spl Omega/(m/sup -1/(9 + k)/), for any k > 0, improving upon the known lower bound of /spl Omega/(1//spl epsi/) for this range of /spl epsi/. This lower bound is tight up to a factor of log log m for small /spl epsi/ and log 1//spl epsi/ for large /spl epsi/. Our lower bound is derived from a reduction from the one-way communication complexity of approximating a Boolean function in Euclidean space. The reduction makes use of a low-distortion embedding from an l/sub 2/ to l/sub 1/ norm.
[Algorithm design and analysis, data stream element, one-pass streaming algorithm, one-way communication complexity, Boolean function, Complexity theory, tight lower bound, communication complexity, Computer crime, Boolean functions, Databases, distinct element problem, Approximation algorithms, Euclidean space, IP networks, space complexity]
A lattice problem in quantum NP
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We consider coGapSVP/sub /spl radic/n/, a gap version of the shortest vector in a lattice problem. This problem is known to be in AM /spl cap/ coNP but is not known to be in NP or in MA. We prove that it lies inside QMA, the quantum analogue of NP. This is the first non-trivial upper bound on the quantum complexity of a lattice problem. The proof relies on two novel ideas. First, we give a new characterization of QMA, called QMA+ formulation allows us to circumvent a problem which arises commonly in the context of QMA: the prover might use entanglement between different copies of the same state in order to cheat. The second idea involves using estimations of autocorrelation functions for verification. We make the important observation that autocorrelation functions are positive definite functions and using properties of such functions we severely restrict the prover's possibility to cheat. We hope that these ideas will lead to further developments in the field.
[Algorithm design and analysis, quantum NP, QMA, graph theory, autocorrelation function estimation, Lattices, quantum entanglement, lattice problem, quantum complexity, QMA+ formulation, Computer science, Quantum computing, Upper bound, Quantum mechanics, quantum computing, Polynomials, Autocorrelation, Cryptography, computational complexity]
On the implementation of huge random objects
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We initiate a general study of pseudo-random implementations of huge random objects, and apply it to a few areas in which random objects occur naturally. For example, a random object being considered may be a random connected graph, a random bounded-degree graph, or a random error-correcting code with good distance. A pseudo-random implementation of such type T objects must generate objects of type T that can not be distinguished from random ones, rather than objects that can not be distinguished from type T objects (although they are not type T at all). We will model a type T object as a function and access objects by queries into these functions. We investigate supporting both standard queries that only evaluates the primary function at locations of the user's choice (e.g., edge queries in a graph), and complex queries that may ask for the result of a computation on the primary function, where this computation is infeasible to perform with a polynomial number of standard queries (e.g., providing the next vertex along a Hamiltonian path in the graph).
[Performance evaluation, Laboratories, graph theory, random bounded-degree graph, random processes, complex query, edge query, Mathematics, Code standards, random connected graph, random object, Stress, random error-correcting code, Computer science, Linear code, huge random objects, standard query, Polynomials, computational complexity]
Lower bounds for non-black-box zero knowledge
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We show new lower bounds and impossibility results for general (possibly non-black-box) zero-knowledge proofs and arguments. Our main results are that, under reasonable complexity assumptions: 1. There does not exist a constant-round zero-knowledge strong proof (or argument) of knowledge (as defined by Goldreich, 2001) for a nontrivial language; 2. There does not exist a two-round zero-knowledge proof system with perfect completeness for an NP-complete language; 3. There does not exist a constant-round public-coin proof system for a nontrivial language that is resettable zero knowledge. This result also extends to bounded resettable zero knowledge. In contrast, we show that under reasonable assumptions, there does exist such a (computationally sound) argument system that is bounded-resettable zero knowledge.
[auxiliary-input zero-knowledge proofs, bounded-resettable zero knowledge, nontrivial language, Access protocols, cryptography, lower bounds, Computer science, nonblack-box zero knowledge, computationally sound argument system, public-coin proof system, complexity assumptions, general zero-knowledge, NP-complete language, Cryptography, impossibility results, computational complexity]
On certain connectivity properties of the Internet topology
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We show that random graphs in the preferential connectivity model have constant conductance, and hence have worst-case routing congestion that scales logarithmically with the number of nodes. Another immediate implication is constant spectral gap between the first and second eigenvalues of the random walk matrix associated with these graphs. We also show that the expected frugality (overpayment in the Vickrey-Clarke-Groves mechanism for shortest paths) of a random graph is bounded by a small constant.
[Weight measurement, connectivity property, routing congestion, eigenvalues, random graph, graph theory, Telecommunication traffic, Educational institutions, Routing, Topology, eigenvalues and eigenfunctions, matrix algebra, random walk matrix, shortest path, Computer science, preferential connectivity model, telecommunication network routing, Vickrey-Clarke-Groves mechanism, spectral gap, Brain modeling, Eigenvalues and eigenfunctions, Internet, IP networks, Internet topology]
Clustering with qualitative information
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We consider the problem of clustering a collection of elements based on pairwise judgments of similarity and dissimilarity. N. Bansal et al. (2002) cast the problem thus: given a graph G whose edges are labeled "+" (similar) or "-" (dissimilar), partition the vertices into clusters so that the number of pairs correctly (resp. incorrectly) classified with respect to the input labeling is maximized (resp. minimized). Complete graphs, where the classifier labels every edge, and general graphs, where some edges are not labeled, are both worth studying. We answer several questions left open by N. Bansal et al. (2002) and provide a sound overview of clustering with qualitative information. We give a factor 4 approximation for minimization on complete graphs, and a factor O(log n) approximation for general graphs. For the maximization version, a PTAS for complete graphs is shown by N. Bansal et al. (2002); we give a factor 0.7664 approximation for general graphs, noting that a PTAS is unlikely by proving APX-hardness. We also prove the APX-hardness of minimization on complete graphs.
[US Department of Energy, pattern classification, approximation, Engineering profession, data analysis, graph theory, maximization, element clustering, vertex partitioning, APX-hardness, qualitative information, pattern clustering, pairwise judgment, minimization, Clustering algorithms, polynomial time approximation scheme, Labeling, minimisation, computational complexity, graph edge labelling]
On the (In)security of the Fiat-Shamir paradigm
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
In 1986, Fiat and Shamir proposed a general method for transforming secure 3-round public-coin identification schemes into digital signature schemes. The idea of the transformation was to replace the random message of the verifier in the identification scheme, with the value of some deterministic hash function evaluated on various quantities in the protocol and on the message to be signed. The Fiat-Shamir methodology for producing digital signature schemes quickly gained popularity as it yields efficient and easy to implement digital signature schemes. The most important question however remained open: are the digital signatures produced by the Fiat-Shamir methodology secure? We answer this question negatively. We show that there exist secure 3-round public-coin identification schemes for which the Fiat-Shamir transformation yields insecure digital signature schemes for any hash function used by the transformation. This is in contrast to the work of Pointcheval and Stern which proved that the Fiat-Shamir methodology always produces digital signatures secure against chosen message attack in the "Random Oracle Model" - when the hash function is modeled by a random oracle. Among other things, we make new usage of Barak's technique for taking advantage of nonblack-box access to a program, this time in the context of digital signatures.
[Fiat-Shamir paradigm, Protocols, Law, Design methodology, deterministic hash function, digital signature scheme, Fiat-Shamir transformation, Computer science, public key cryptography, message authentication, Cost function, Forgery, theorem proving, Cryptography, Computer security, Random Oracle Model, Digital signatures, Legal factors, nonblack-box access, 3-round public-coin identification schemes]
The complexity of homomorphism and constraint satisfaction problems seen from the other side
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
We give a complexity theoretic classification of homomorphism problems for graphs and, more generally, relational structures obtained by restricting the left hand side structure in a homomorphism. For every class C of structures, let HOM(C, /spl I.bar/) be the problem of deciding whether a given structure A /spl isin/ C has a homomorphism to a given (arbitrary) structure B. We prove that, under some complexity theoretic assumption from parameterized complexity theory, HOM(C, /spl I.bar/) is in polynomial time if, and only if, the cores of all structures in C have bounded tree-width (as long as the structures in C only contain relations of bounded arity). Due to a well known correspondence between homomorphism problems and constraint satisfaction problems, our classification carries over to the latter.
[graph homomorphism, constraint theory, graph theory, Relational databases, constraint satisfaction problem, Complexity theory, NP-complete problem, bounded tree-width, Computational complexity, relational structure, parameterized complexity theory, Computer science, Tree graphs, Constraint theory, Polynomials, polynomial time, Dynamic programming, homomorphism complexity, Artificial intelligence, computational complexity]
Towards a dichotomy theorem for the counting constraint satisfaction problem
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
The Counting Constraint Satisfaction Problem (#CSP) over a finite domain can be expressed as follows: given a first-order formula consisting of a conjunction of predicates, determine the number of satisfying assignments to the formula. #CSP can be parametrized by the set of allowed constraint predicates. In this paper we start a systematic study of subclasses of #CSP restricted in this way. The ultimate goal of this investigation is to distinguish those restricted subclasses of #CSP which are tractable, i.e. solvable in polynomial time, from those which are not. We show that the complexity of any restricted #CSP class on a finite domain can be deduced from the properties of polymorphisms of the allowed constraints, similar to that for the decision CSP. Then we prove that if a subclass of the #CSP is tractable, then constraints allowed by the class satisfy some very restrictive condition: it has to have a Mal'tsev polymorphism, that is a ternary operation m(x, y, z) such that m(x, y, y) = m(y, y, x) = x. This condition uniformly explains all existing complexity results for particular cases of #CSP, and allows us to obtain new results and to conjecture a criterion distinguishing tractable counting CSPs. We also obtain a dichotomy theorem for the complexity of #CSP with a 3-element domain and give new simpler proofs of the dichotomy results for the problem of counting graph homomorphisms.
[Mal'tsev polymorphism, 3-element domain, tractable subclasses, graph homomorphism, Laboratories, graph theory, Search problems, constraint predicates, Prototypes, Constraint theory, Polynomials, polynomial time, Logic, constraint handling, finite domain, restricted #CSP class complexity, dichotomy theorem, constraint theory, constraint predicate, Graph theory, counting constraint satisfaction problem, Physics, Computer science, predicate conjunction, #CSP parametrization, Artificial intelligence, computational complexity]
Towards a characterization of truthful combinatorial auctions
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
This paper analyzes incentive compatible (truthful) mechanisms over restricted domains of preferences, the leading example being combinatorial auctions. Our work generalizes the characterization of Roberts (1979) who showed that truthful mechanisms over unrestricted domains with at least 3 possible outcomes must be "affine maximizers". We show that truthful mechanisms for combinatorial auctions (and related restricted domains) must be "almost affine maximizers" if they also satisfy an additional requirement of "independence of irrelevant alternatives". This requirement is without loss of generality for unrestricted domains as well as for auctions between two players where all goods must be allocated. This implies unconditional results for these cases, including a new proof of Roberts' theorem. The computational implications of this characterization are severe, as reasonable "almost affine maximizers" are shown to be as computationally hard as exact optimization. This implies the near-helplessness of such truthful polynomial-time auctions in all cases where exact optimization is computationally intractable.
[Algorithm design and analysis, combinatorial mathematics, computational hardness, exact optimization, Distributed computing, Environmental economics, Cost accounting, Design optimization, affine maximizer, Computer science, truthful combinatorial auction characterization, optimisation, Abstracts, Polynomials, polynomial-time auction, IP networks, Communication networks, computational intractability, computational complexity]
Proceedings 44th IEEE Symposium on Foundations of Computer Science - FOCS 2003
44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.
None
2003
The following topics are discussed: computer science; network performance analysis; algorithms and data structures, computational complexity, cryptography, computational geometry, algorithmic graph theory and combinatorics, parallel and distributed computing, machine learning, applications of logic, algorithmic algebra and coding theory, theoretical aspects of databases, information retrieval, networks, computational biology, robotics, and quantum computing; constraint satisfaction problems; and traveling salesman problem.
[computational biology, algorithms, graph theory, logic applications, computational geometry, algorithmic graph theory, robotics, Complexity theory, network performance analysis, distributed computing, travelling salesman problems, coding theory, computer science, algorithm theory, data structures, robots, Cryptography, parallel computing, Robots, combinatorics, traveling salesman problem, information retrieval, Traveling salesman problems, cryptography, Graph theory, machine learning, database theory, Computer science, constraint satisfaction problems, Algorithms, quantum computing, algorithmic algebra, computational complexity]
Foreword
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Presents the welcome message from the conference proceedings.
[]
Conference Organization and Program Committee
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Provides a listing of current committee members.
[]
Quantum weak coin-flipping with bias of 0.192
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We present a family of protocols for flipping a coin over a telephone in a quantum mechanical setting. The family contains protocols with n + 2 messages for all n > 1, and asymptotically achieves a bias of 0.192. The case n = 2 is equivalent to the protocol of Spekkens and Rudolph with bias 0.207, which was the best known protocol. The case n = 3 achieves a bias of 0.199, and n = 8 achieves a bias of 0.193. The analysis of the protocols uses Kitaev's description of coin-flipping as a semidefinite program. We construct an analytical solution to the dual problem which provides an upper bound on the amount that a party can cheat.
[Protocols, quantum mechanical setting, Quantum entanglement, Optimized production technology, quantum weak coin-flipping, Security, Kitaev description, Quantum computing, Upper bound, Quantum mechanics, Collaboration, quantum cryptography, Telephony, semidefinite program, protocols]
Quantum walk algorithm for element distinctness
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We use quantum walks to construct a new quantum algorithm for element distinctness and its generalization. For element distinctness (the problem of finding two equal items among N given items), we get an O(N/sup 2/3/) query quantum algorithm. This improves the previous O(N/sup 3/4/) quantum algorithm of Buhrman et al. and matches the lower bound by Shi. We also give an O(N/sup k/(k+1)/) query quantum algorithm for the generalization of element distinctness in which we have to find k equal items among N items.
[Algorithm design and analysis, Quantum computing, Costs, Laboratories, quantum computing, element distinctness, query quantum algorithm, Mathematics, Sorting, quantum walk algorithm]
Quantum speed-up of Markov chain based algorithms
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We develop a generic method for quantizing classical algorithms based on random walks. We show that under certain conditions, the quantum version gives rise to a quadratic speed-up. This is the case, in particular, when the Markov chain is ergodic and its transition matrix is symmetric. This generalizes the celebrated result of L. K. Grover (1996)and a number of more recent results, including the element distinctness result of Ambainis and the result of Ambainis, Kempe and Rivosh that computes properties of quantum walks on the d-dimensional torus. Among the consequences is a faster search for multiple marked items. We show that the quantum escape time, just like its classical version, depends on the spectral properties of the transition matrix with the marked rows and columns deleted.
[Algorithm design and analysis, quantum walks, random walks, Symmetric matrices, Computational modeling, Stochastic processes, element distinctness, quantum escape time, random processes, d-dimensional torus, quadratic speed-up, State-space methods, Genetic algorithms, Quantum computing, Monte Carlo methods, Markov chain based algorithms, transition matrix, Quantum mechanics, quantum computing, quantum speed-up, Simulated annealing, Markov processes]
Adiabatic quantum computation is equivalent to standard quantum computation
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
The model of adiabatic quantum computation has recently attracted attention in the physics and computer science communities, but its exact computational power has been unknown. We settle this question and describe an efficient adiabatic simulation of any given quantum algorithm. This implies that the adiabatic computation model and the standard quantum circuit model are polynomially equivalent. We also describe an extension of this result with implications to physical implementations of adiabatic computation. We believe that our result highlights the potential importance of the adiabatic computation model in the design of quantum algorithms and in their experimental realization.
[Computational modeling, Stationary state, adiabatic simulation, quantum algorithm, Mathematics, polynomially equivalent, Computer science, Quantum computing, Power engineering computing, Physics computing, Quantum mechanics, quantum computing, standard quantum computation, computational power, Polynomials, Eigenvalues and eigenfunctions, adiabatic quantum computation, standard quantum circuit model, computational complexity]
Maximizing quadratic programs: extending Grothendieck's inequality
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
This paper considers the following type of quadratic programming problem. Given an arbitrary matrix A, whose diagonal elements are zero, find x /spl isin/ {-1, 1}/sup n/ such that x/sup T/Ax is maximized. Our approximation algorithm for this problem uses the canonical semidefinite relaxation and returns a solution whose ratio to the optimum is in /spl Omega/(1/ logn). This quadratic programming problem can be seen as an extension to that of maximizing x/sup T/Ay (where y's components are also /spl plusmn/1). Grothendieck's inequality states that the ratio of the optimum value of the latter problem to the optimum of its canonical semidefinite relaxation is bounded below by a constant. The study of this type of quadratic program arose from a desire to approximate the maximum correlation in correlation clustering. Nothing substantive was known about this problem; we present an /spl Omega/ (1/logn) approximation, based on our quadratic programming algorithm. We can also guarantee that our quadratic programming algorithm returns a solution to the MAXCUT problem that has a significant advantage over a random assignment.
[US Department of Energy, approximation theory, Engineering profession, random assignment, maximum correlation, Linear matrix inequalities, Quadratic programming, quadratic programming, correlation clustering, Computer science, MAXCUT problem, Clustering algorithms, relaxation theory, approximation algorithm, Approximation algorithms, Grothendieck inequality, canonical semidefinite relaxation, computational complexity, correlation methods]
An approximate max-Steiner-tree-packing min-Steiner-cut theorem
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Given an undirected multigraph G and a subset of vertices S /spl sube/ V(G), the Steiner tree packing problem is to find a largest collection of edge-disjoint trees that each connects S. This problem and its generalizations have attracted considerable attention from researchers in different areas because of their wide applicability. This problem was shown to be APX-hard (no polynomial time approximation scheme unless P=NP). In fact, prior to this paper, not even an approximation algorithm with asymptotic ratio o(n) was known despite several attempts. In this work, we close this huge gap by presenting the first polynomial time constant factor approximation algorithm for the Steiner tree packing problem. The main theorem is an approximate min-max relation between the maximum number of edge-disjoint trees that each connects S (i.e. S-trees) and the minimum size of an edge-cut that disconnects some pair of vertices in S (i.e. S-cut). Specifically, we prove that if the minimum S-cut in G has 26k edges, then G has at least k edge-disjoint S-trees; this answers Kriesell's conjecture affirmatively up to a constant multiple. The techniques that we use are purely combinatorial, where matroid theory is the underlying ground work.
[Algorithm design and analysis, approximation theory, matroid theory, combinatorial mathematics, trees (mathematics), polynomial time constant factor, Very large scale integration, Routing, min-Steiner-cut theorem, Application software, minimax techniques, undirected multigraph, Combinatorial mathematics, Computer science, Upper bound, approximation algorithm, Approximation algorithms, Polynomials, edge-disjoint trees, Circuit synthesis, max-Steiner-tree-packing theorem, APX-hard, approximate min-max relation, computational complexity]
Edge-disjoint paths in planar graphs
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We study the maximum edge-disjoint paths problem (MEDP). We are given a graph G = (V, E) and a set T = {s/sub 1/t/sup 1/, s/sub 2/t/sup 2/,..., s/sub k/t/sup k/} of pairs of vertices: the objective is to find the maximum number of pairs in T that can be connected via edge-disjoint paths. Our main result is a poly-logarithmic approximation for MEDP on undirected planar graphs if a congestion of 2 is allowed, that is, we allow up to 2 paths to share an edge. Prior to our work, for any constant congestion, only a polynomial-factor approximation was known for planar graphs although much stronger results are known for some special cases such as grids and grid-like graphs. We note that the natural multi-commodity flow relaxation of the problem has an integrality gap of /spl Omega/(/spl radic/|V|) even on planar graphs when no congestion is allowed. Our starting point is the same relaxation and our result implies that the integrality gap shrinks to a poly-logarithmic factor once 2 paths are allowed per edge. Our result also extends to the unsplittable flow problem and the maximum integer multicommodity flow problem. A set X /spl sube/V is well-linked if for each S /spl sub/ V, |/spl delta/(S)| /spl ges/ min{|S /spl cap/ X |, |(V - S) /spl cap/ X|}. The heart of our approach is to show that in any undirected planar graph, given any matching M on a well-linked set X, we can route /spl Omega/(|M|) pairs in M with a congestion of 2. Moreover, all pairs in M can be routed with constant congestion for a sufficiently large constant. This results also yields a different proof of a theorem of Klein, Plotkin, and Rao that shows an O(1) maxflow-mincut gap for uniform multicommodity flow instances in planar graphs. The framework developed in this paper applies to general graphs as well. If a certain graph theoretic conjecture is true, it yields poly-logarithmic integrality gap for MEDP with constant congestion.
[Heart, maximum edge-disjoint paths, poly-logarithmic approximation, unsplittable flow problem, Engineering profession, Circuits, graph theory, Very large scale integration, maxflow-mincut gap, uniform multicommodity flow, Routing, Computational Intelligence Society, multicommodity flow relaxation, High-speed networks, Tree graphs, grid-like graphs, Approximation algorithms, undirected planar graphs, Polynomials, polynomial-factor approximation, theorem proving, computational complexity]
Machine minimization for scheduling jobs with interval constraints
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
The problem of scheduling jobs with interval constraints is a well-studied classical scheduling problem. The input to the problem is a collection of n jobs where each job has a set of intervals on which it can be scheduled. The goal is to minimize the total number of machines needed to schedule all jobs subject to these interval constraints. In the continuous version, the allowed intervals associated with a job form a continuous time segment, described by a release date and a deadline. In the discrete version of the problem, the set of allowed intervals for a job is given explicitly. So far, only an O(log n/( log log n))-approximation is known for either version of the problem, obtained by a randomized rounding of a natural linear programming relaxation of the problem. In fact, we show here that this analysis is tight for both versions of the problem by providing a matching lower bound on the integrality gap of the linear program. Moreover, even when all jobs can be scheduled on a single machine, the discrete case has recently been shown to be /spl Omega/(log log n)-hard to approximate. In this paper, we provide improved approximation factors for the number of machines needed to schedule all jobs in the continuous version of the problem. Our main result is an O(1)-approximation algorithm when the optimal number of machines needed is bounded by a fixed constant. Thus, our results separate the approximability of the continuous and the discrete cases of the problem. For general instances, we strengthen the natural linear programming relaxation in a recursive manner by forbidding certain configurations which cannot arise in an integral feasible solution. This yields an O(OPT)-approximation, where OPT denotes the number of machines needed by an optimal solution. Combined with earlier results, our work implies an O(/spl radic/log n/(log log n))-approximation for any value of OPT.
[Engineering profession, O(OPT)-approximation, Optimized production technology, Linear programming, linear programming, randomized rounding, Computer science, Information science, interval constraints, relaxation theory, scheduling, linear programming relaxation, approximation algorithm, machine minimization, Single machine scheduling, Contracts, job scheduling, computational complexity]
Random edge can be exponential on abstract cubes
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We prove that random edge, the simplex algorithm that always chooses a random improving edge to proceed on, can take a mildly exponential number of steps in the model of abstract objective functions (introduced by K. W. Hoke (1998) and by G. Kalai (1988) under different names). We define an abstract objective function on the n-dimensional cube for which the algorithm, started at a random vertex, needs at least exp(const /spl middot/ n/sup 1/3/) steps with high probability. The best previous lower bound was quadratic. So in order for random edge to succeed in polynomial time, geometry must help.
[Costs, graph theory, probability, random processes, exponential number, Linear programming, Mathematics, abstract cubes, abstract objective functions, Computer science, random vertex, Information geometry, Upper bound, random edge, Polynomials, polynomial time, geometry, Mathematical model, Arithmetic, n-dimensional cube]
On the integrality ratio for asymmetric TSP
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
The traveling salesman problem comes in two variants. The symmetric version (STSP) assumes that the cost c/sub ij/ of going to city i to city j is equal to c/sub ji/, while the more general asymmetric version (ATSP) does not make this assumption. In both cases, it is usually assumed that we are in the metric case, i.e., the costs satisfy the triangle inequality: c/sub ij/ + c/sub jk/ /spl ges/ c/sub ik/ for all i, j, k. In this assumption, we improve the lower bound on the integrality ratio of the Held-Karp bound for asymmetric TSP (with triangle inequality) from 4/3 to 2.
[Algorithm design and analysis, Ant colony optimization, Engineering profession, asymmetric TSP, Traveling salesman problems, Linear programming, travelling salesman problem, Computer science, travelling salesman problems, Held-Karp bound, integrality ratio, triangle inequality, Cities and towns, Approximation algorithms, Cost function, Polynomials]
The hardness of metric labeling
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
The metric labeling problem is an elegant and powerful mathematical model capturing a wide range of classification problems. The input to the problem consists of a set of labels and a weighted graph. Additionally, a metric distance function on the labels is defined, and for each label and each vertex, an assignment cost is given. The goal is to find a minimum-cost assignment of the vertices to the labels. The cost of the solution consists of two parts: the assignment costs of the vertices and the separation costs of the edges (each edge pays its weight times the distance between the two labels to which its endpoints are assigned). Due to the simple structure and variety of the applications, the problem and its special cases (with various distance functions on the labels) have recently received much attention. Metric labeling has a known logarithmic approximation, and it has been an open question for several years whether a constant approximation exists. We refute this possibility and show that no constant approximation can be obtained for the problem unless P=NP, and we also show that the problem is /spl Omega/(/spl radic/logn)-hard to approximate, unless NP has quasi-polynomial time algorithms.
[metric distance function, Computer vision, approximation theory, weighted graph, quasi-polynomial time algorithm, logarithmic approximation, minimum-cost assignment, graph theory, Linear programming, Computer science, Earth, Character generation, mathematical model, Approximation algorithms, Cost function, classification problems, assignment cost, Labeling, Mathematical model, distance functions, Contracts, computational complexity, metric labeling, separation costs]
Hardness of buy-at-bulk network design
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We consider the buy-at-bulk network design problem in which we wish to design a network for carrying multicommodity demands from a set of source nodes to a set of destination nodes. The key feature of the problem is that the cost of capacity on each edge is concave and hence exhibits economies of scale. If the cost of capacity per unit length can be different on different edges then, we say that the problem is non-uniform. The problem is uniform otherwise. We show that for any constant /spl gamma/, if NP /spl nsube/ ZPTIME(n/sup polylog n/), then there is no O(log/sup 1/2 - /spl gamma//N)-approximation algorithm for non-uniform buy-at-bulk network design and there is no O(log/sup 1/4 - /spl gamma//N)-approximation algorithm for the uniform problem.
[Algorithm design and analysis, approximation theory, Costs, telecommunication network planning, buy-at-bulk network design, Spine, graph theory, Telecommunication traffic, destination nodes, source nodes, multicommodity demands, Aggregates, Character generation, Bandwidth, Economies of scale, approximation algorithm, Approximation algorithms, Communication networks, computational complexity]
Hardness of approximating the shortest vector problem in lattices
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Let p > 1 be any fixed real. We show that assuming NP /spl nsube/ RP, it is hard to approximate the shortest vector problem (SVP) in l/sub p/ norm within an arbitrarily large constant factor. Under the stronger assumption NP /spl nsube/ RTIME(2/sup poly(log n)/), we show that the problem is hard to approximate within factor 2/sup log n1/2 - /spl epsi// where n is the dimension of the lattice and /spl epsi/> 0 is an arbitrarily small constant. This greatly improves all previous results in l/sub p/ norms with 1 < p < /spl infin/. The best results so far gave only a constant factor hardness, namely, 2/sup 1/p/ - /spl epsi/ by Micciancio and p/sup 1 - /spl epsi// in high l/sub p/ norms by Khot. We first give a new (randomized) reduction from closest vector problem (CVP) to SVP that achieves some constant factor hardness. The reduction is based on BCH codes. Its advantage is that the SVP instances produced by the reduction behave well under the augmented tensor product, a new variant of tensor product that we introduce. This enables us to boost the hardness factor to 2/sup log n1/2-/spl epsi//.
[approximation theory, shortest vector problem, hardness factor, Lattices, Linear programming, lattice theory, tensors, Paper technology, History, randomized reduction, randomised algorithms, Geometry, Computer science, Tensile stress, closest vector problem, NP-hard problem, augmented tensor product, Gaussian processes, Polynomials, BCH codes, Books, lattices, computational complexity]
Ruling out PTAS for graph min-bisection, densest subgraph and bipartite clique
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Assuming that NP /spl nsube//spl cap//sub /spl epsi/> 0/ BPTIME(2/sup n/spl epsi//), we show that graph min-bisection, densest subgraph and bipartite clique have no PTAS. We give a reduction from the minimum distance of code problem (MDC). Starting with an instance of MDC, we build a quasi-random PCP that suffices to prove the desired inapproximability results. In a quasi-random PCP, the query pattern of the verifier looks random in some precise sense. Among the several new techniques introduced, we give a way of certifying that a given polynomial belongs to a given subspace of polynomials. As is important for our purpose, the certificate itself happens to be another polynomial and it can be checked by reading a constant number of its values.
[polynomials, graph theory, Vectors, Computer science, quasirandom PCP, Linear code, graph min-bisection, PTAS, bipartite clique, query pattern, polynomial, Binary codes, minimum distance of code problem, Approximation algorithms, densest subgraph, Polynomials, Bipartite graph, Artificial intelligence, computational complexity]
Optimal inapproximability results for MAX-CUT and other 2-variable CSPs?
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
In this paper, we give evidence suggesting that MAX-CUT is NP-hard to approximate to within a factor of /spl alpha//sub cw/+ /spl epsi/, for all /spl epsi/ > 0, where /spl alpha//sub cw/ denotes the approximation ratio achieved by the Goemans-Williamson algorithm (1995). /spl alpha//sub cw/ /spl ap/ .878567. This result is conditional, relying on two conjectures: a) the unique games conjecture of Khot; and, b) a very believable conjecture we call the majority is stablest conjecture. These results indicate that the geometric nature of the Goemans-Williamson algorithm might be intrinsic to the MAX-CUT problem. The same two conjectures also imply that it is NP-hard to (/spl beta/ + /spl epsi/)-approximate MAX-2SAT, where /spl beta/ /spl ap/ .943943 is the minimum of (2 + (2//spl pi/) /spl theta/)/(3 - cos(/spl theta/)) on (/spl pi//2, /spl pi/). Motivated by our proof techniques, we show that if the MAX-2CSP and MAX-2SAT problems are slightly restricted - in a way that seems to retain all their hardness -then they have (/spl alpha//sub GW/-/spl epsi/)- and (/spl beta/ - /spl epsi/)-approximation algorithms, respectively. Though we are unable to prove the majority is stablest conjecture, we give some partial results and indicate possible directions of attack. Our partial results are enough to imply that MAX-CUT is hard to (3/4 + 1/(2/spl pi/) + /spl epsi/)-approximate (/spl ap/ .909155), assuming only the unique games conjecture. We also discuss MAX-2CSP problems over non-Boolean domains and state some related results and conjectures. We show, for example, that the unique games conjecture implies that it is hard to approximate MAX-2LIN(q) to within any constant factor.
[MAX-CUT, NP-hard, MAX-2SAT problem, communicating sequential processes, graph theory, computability, nonBoolean domains, Mathematics, Boolean functions, optimisation, majority is stablest conjecture, approximation algorithm, MAX-2CSP problem, Bipartite graph, Goemans-Williamson algorithm, Labeling, Additive noise, approximation theory, CSP, Stability, game theory, Statistics, Computer science, Approximation algorithms, optimal inapproximability, games conjecture, computational complexity]
Assignment testers: towards a combinatorial proof of the PCP-theorem
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
In this work, we look back into the proof of the PCP theorem, with the goal of finding new proofs that are "more combinatorial" and arguably simpler. For that, we introduce the notion of an assignment tester, which is a strengthening of the standard PCP verifier, in the following sense. Given a statement and an alleged proof for it, while the PCP verifier checks correctness of the statement, the assignment-tester checks correctness of the statement and the proof. This notion enables composition that is truly modular, i.e., one can compose two assignment-testers without any assumptions on how they are constructed. A related notion was independently introduced in (Ben-Sasson et. al. STOC 04). We provide a toolkit of (non-trivial) generic transformations on assignment testers. These transformations may be interesting in their own right, and allow us to present the following two main results: 1. The first is a new proof of the PCP theorem. This proof relies on a rather weak assignment tester given as a "black box". From this, we construct combinatorially the full PCP. An important component of this proof is a new combinatorial aggregation technique (i.e., a new transformation that allows the verifier to read fewer, though possibly longer, "pieces" of the proof). An implementation of the black-box tester can be obtained from the algebraic proof techniques that already appear in L. Babai et al., 1991 and U. Feige et al., 1991. Obtaining a combinatorial implementation of this tester would give a purely combinatorial proof for the PCP theorem, which we view as an interesting open problem. 2. Our second construction is a "standalone" combinatorial construction showing NP /spl sube/ PCP (S. Arora et al., 1998). This implies, for example, that approximating max-SAT is quasi-NP-hard. This construction relies on a transformation that makes an assignment tester "oblivious": so that the proof locations read are independent of the statement that is being proven. This eliminates, in a rather surprising manner, the need for aggregation in a crucial point in the proof.
[Heart, combinatorial proof, Career development, combinatorial aggregation technique, Circuits, graph theory, PCP verifier, black-box tester, Mathematics, algebra, combinatorial construction, max-SAT approximation, assignment testers, Computer science, algebraic proof techniques, National electric code, Polynomials, PCP-theorem, theorem proving, quasi-NP-hard, Testing, computational complexity]
Cryptography in NC/sup 0/
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We study the parallel time-complexity of basic cryptographic primitives such as one-way functions (OWFs) and pseudorandom generators (PRGs). Specifically, we study the possibility of computing instances of these primitives by NC/sup 0/ circuits, in which each output bit depends on a constant number of input bits. Despite previous efforts in this direction, there has been no significant theoretical evidence supporting this possibility, which was posed as an open question in several previous works. We essentially settle this question by providing overwhelming positive evidence for the possibility of cryptography in NC/sup 0/. Our main result is that every "moderately easy" OWF (resp., PRG), say computable in NC/sup 1/, can be compiled into a corresponding OWF (resp., low-stretch PRG) in NC/sub 4//sup 0/, i.e. whose output bits each depend on at most 4 input bits. The existence of OWF and PRG in NC/sup 1/ is a relatively mild assumption, implied by most number-theoretic or algebraic intractability assumptions commonly used in cryptography. Hence, the existence of OWF and PRG in NC/sup 0/ follows from a variety of standard assumptions. A similar compiler can also be obtained for other cryptographic primitives such as one-way permutations, encryption, commitment, and collision-resistant flashing. The above results leave a small gap between the possibility of cryptography in NC/sub 4//sup 0/, and the known impossibility of implementing even OWF in NC/sub 2//sup 0/. We partially close this gap by providing evidence for the existence of OWF in NC/sub 3//sup 0/. Finally, our techniques can also be applied to obtain unconditionally provable constructions of non-cryptographic PRGs. In particular, we obtain e-biased generators in NC/sub 3//sup 0/, resolving an open question posed by Mossel et al. (2003), as well as a PRG for logspace in NC/sup 0/. Our results make use of the machinery of randomizing polynomials which was originally motivated by questions in the domain of information-theoretic secure multiparty computation.
[e-biased generators, pseudorandom generators, Circuits, NC/sup 0/, random number generation, Machinery, Concurrent computing, encryption, one-way permutations, Polynomials, Cryptography, collision-resistant flashing, parallel algorithms, polynomials, cryptographic primitives, parallel time-complexity, cryptography, information-theoretic secure multiparty computation, randomised algorithms, Computer science, algebraic intractability assumptions, Chromium, randomizing polynomials, Concrete, one-way functions, computational complexity]
An unconditional study of computational zero knowledge
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We prove a number of general theorems about CZK, the class of problems possessing computational zero knowledge proofs. Our results are unconditional, in contrast to most previous works on CZK which rely on the assumption that one-way functions exist. We establish several new characterizations of CZK, and use these characterizations to prove results such as: 1) Honest-verifier CZK equals general CZK. 2) Public-coin CZK equals private-coin CZK. 3) CZK is closed under union (and more generally, "monotone formula closure"). 4) CZK with imperfect completeness equals CZK with perfect completeness. 5) Any problem in CZK /spl cap/ NP can be proven in computational zero knowledge by a BPP/sup NP/ prover. 6) CZK with black-box simulators equals CZK with general, non-black-box simulators. The above equalities refer to the resulting class of problems (and do not necessarily preserve other efficiency measures such as round complexity). Our approach is to combine the conditional techniques previously used in the study of CZK with the unconditional techniques developed in the study of SZK, the class of problems possessing statistical zero knowledge proofs. To enable this combination, we prove that every problem in CZK can be decomposed into a problem in SZK together with a set of instances from which a one-way function can be constructed.
[Knowledge engineering, Computational modeling, cryptography, conditional techniques, round complexity, Cryptographic protocols, Computer science, nonblack-box simulators, monotone formula closure, NP problem, black-box simulators, computational zero knowledge, Polynomials, statistical zero knowledge, theorem proving, one-way functions, computational complexity]
Universally composable protocols with relaxed set-up assumptions
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
A desirable goal for cryptographic protocols is to guarantee security when the protocol is composed with other protocol instances. Universally composable (UC) protocols provide this guarantee in a strong sense: A protocol remains secure even when composed concurrently with an unbounded number of instances of arbitrary protocols. However, UC protocols for carrying out general tasks are known to exist only if a majority of the participants are honest, or in the common reference string (CRS) model where all parties are assumed to have access to a common string that is drawn from some pre-defined distribution. Furthermore, carrying out many interesting tasks in a UC manner and without honest majority or set-up assumptions is impossible, even if ideally authenticated communication is provided. A natural question is thus whether there exist more relaxed set-up assumptions than the CRS model that still allow for UC protocols. We answer this question in the affirmative: we propose alternative and relaxed set-up assumptions and show that they suffice for reproducing the general feasibility results for UC protocols in the CRS model. These alternative assumptions have the flavor of a "public-key infrastructure": parties have registered public keys, no single registration authority needs to be fully trusted, and no single piece of information has to be globally trusted and available. In addition, unlike known protocols in the CRS model, the proposed protocols guarantee some basic level of security even if the set-up assumption is violated.
[registered public keys, cryptographic protocols, Access protocols, relaxed set-up assumptions, common reference string model, Cryptographic protocols, Radio access networks, Computer science, arbitrary protocols, public key cryptography, authenticated communication, registration authority, Information security, Public key, authorisation, Isolation technology, universally composable protocols, Cryptography, public-key infrastructure]
On the (im)possibility of cryptography with imperfect randomness
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We investigate the feasibility of a variety of cryptographic tasks with imperfect randomness. The kind of imperfect randomness we consider are entropy sources, such as those considered by Santha and Vazirani, Chor and Goldreich, and Zuckerman. We show the following: (1) certain cryptographic tasks like bit commitment, encryption, secret sharing, zero-knowledge, non-interactive zero-knowledge, and secure two-party computation for any non-trivial junction are impossible to realize if parties have access to entropy sources with slightly less-than-perfect entropy, i.e., sources with imperfect randomness. These results are unconditional and do not rely on any un-proven assumption. (2) On the other hand, based on stronger variants of standard assumptions, secure signature schemes are possible with imperfect entropy sources. As another positive result, we show (without any unproven assumption) that interactive proofs can be made sound with respect to imperfect entropy sources.
[unproven assumption, Engineering profession, Computational modeling, imperfect randomness, cryptography, Entropy, Security, imperfect entropy sources, Cryptographic protocols, secure signature, Computer science, entropy, encryption, noninteractive zero-knowledge, interactive proofs, nontrivial junction, Polynomials, less-than-perfect entropy, Cryptography, secret sharing, secure two-party computation, bit commitment]
Approximating the stochastic knapsack problem: the benefit of adaptivity
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We consider a stochastic variant of the NP-hard 0/1 knapsack problem in which item values are deterministic and item sizes are independent random variables with known, arbitrary distributions. Items are placed in the knapsack sequentially, and the act of placing an item in the knapsack instantiates its size. Our goal is to compute a solution "policy" that maximizes the expected value of items placed in the knapsack, and we consider both non-adaptive policies (that designate a priori a fixed sequence of items to insert) and adaptive policies (that can make dynamic choices based on the instantiated sizes of items placed in the knapsack thus far). We show that adaptivity provides only a constant-factor improvement by demonstrating a greedy non-adaptive algorithm that approximates the optimal adaptive policy within a factor of 7. We also design an adaptive polynomial-time algorithm which approximates the optimal adaptive policy within a factor of 5 + /spl epsiv/, for any constant /spl epsiv/ > 0.
[Algorithm design and analysis, greedy algorithms, Stochastic processes, stochastic knapsack problem, Mathematics, knapsack problems, nonadaptive policies, greedy nonadaptive algorithm, Computer science, NP-hard 0/1 knapsack problem, adaptive polynomial-time algorithm, Processor scheduling, optimal adaptive policy, Approximation algorithms, Polynomials, Random variables, Performance analysis, stochastic processes, Artificial intelligence, independent random variables, computational complexity]
An edge in time saves nine: LP rounding approximation algorithms for stochastic network design
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Real-world networks often need to be designed under uncertainty, with only partial information and predictions of demand available at the outset of the design process. The field of stochastic optimization deals with such problems where the forecasts are specified in terms of probability distributions of future data. In this paper, we broaden the set of models as well as the techniques being considered for approximating stochastic optimization problems. For example, we look at stochastic models where the cost of the elements is correlated to the set of realized demands, and risk-averse models where upper bounds are placed on the amount spent in each of the stages. These generalized models require new techniques, and our solutions are based on a novel combination of the primal-dual method truncated based on optimal LP relaxation values, followed by a tree-rounding stage. We use these to give constant-factor approximation algorithms for the stochastic Steiner tree and single sink network design problems in these generalized models.
[Algorithm design and analysis, Costs, Uncertainty, Stochastic processes, constant-factor approximation algorithms, upper bounds, Probability distribution, linear programming, stochastic optimization, stochastic Steiner tree, Intelligent networks, optimisation, stochastic models, probability distributions, Bandwidth, stochastic processes, tree-rounding stage, approximation theory, risk-averse models, trees (mathematics), computer networks, single sink network design problems, primal-dual method, stochastic network design, LP rounding approximation algorithms, Computer science, optimal LP relaxation values, generalized models, Approximation algorithms, Joining processes]
Stochastic optimization is (almost) as easy as deterministic optimization
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Stochastic optimization problems attempt to model uncertainty in the data by assuming that (part of) the input is specified in terms of a probability distribution. We consider the well-studied paradigm of 2-stage models with recourse: first, given only distributional information about (some of) the data one commits on initial actions, and then once the actual data is realized (according to the distribution), further (recourse) actions can be taken. We give the first approximation algorithms for 2-stage discrete stochastic optimization problems with recourse for which the underlying random data is given by a "black box" and no restrictions are placed on the costs in the two stages, based on an FPRAS for the LP relaxation of the stochastic problem (which has exponentially many variables and constraints). Among the range of applications we consider are stochastic versions of the set cover, vertex cover, facility location, multicut (on trees), and multicommodity flow problems.
[Uncertainty, LP relaxation, Stochastic processes, Transportation, multicommodity flow problem, Probability distribution, 2-stage models, approximation algorithms, statistical distributions, Design optimization, 2-stage discrete stochastic optimization, facility location, Constraint optimization, set cover, multicut, optimisation, uncertainty modeling, probability distribution, Cost function, stochastic processes, distributional information, vertex cover, Instruments, random data, deterministic optimization, well-studied paradigm, Approximation algorithms, FPRAS, Logistics]
O(/spl radic/log n) approximation to SPARSEST CUT in O/spl tilde/(n/sup 2/) time
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We show how to compute O(/spl radic/log n)-approximations to SPARSEST CUT and BALANCED SEPARATOR problems in O/spl tilde/(n/sup 2/) time, thus improving upon the recent algorithm of Arora, Rao and Vazirani (2004). Their algorithm uses semidefinite programming and required O/spl tilde/(n/sup 4.5/) time. Our algorithm relies on efficiently finding expander flows in the graph and does not solve semidefinite programs. The existence of expander flows was also established by Arora, Rao, and Vazirani.
[Algorithm design and analysis, expander flow, SPARSEST CUT, Particle separators, BALANCED SEPARATOR problem, graph theory, Routing, Phase change random access memory, Graph theory, Partitioning algorithms, O(/spl radic/log n) approximation, Computer science, semidefinite programming, NP-hard problem, O(n/sup 2/) time, Clustering algorithms, Approximation algorithms, computational complexity]
Maximum matchings via Gaussian elimination
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We present randomized algorithms for finding maximum matchings in general and bipartite graphs. Both algorithms have running time O(n/sup w/), where w is the exponent of the best known matrix multiplication algorithm. Since w < 2.38, these algorithms break through the O(n/sup 2.5/) barrier for the matching problem. They both have a very simple implementation in time O(n/sup 3/) and the only non-trivial element of the O(n/sup w/) bipartite matching algorithm is the fast matrix multiplication algorithm. Our results resolve a long-standing open question of whether Lovasz's randomized technique of testing graphs for perfect matching in time O(n/sup w/) can be extended to an algorithm that actually constructs a perfect matching.
[graph theory, Partitioning algorithms, randomized algorithm, Matrix decomposition, randomised algorithms, Computer science, bipartite matching algorithm, matrix multiplication, bipartite graphs, Lovasz randomized technique, Gaussian elimination, Gaussian processes, perfect matching, Polynomials, Bipartite graph, maximum matching, Informatics, Testing]
Exponentially many steps for finding a Nash equilibrium in a bimatrix game
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
The Lemke-Howson algorithm is the classical algorithm for the problem NASH of finding one Nash equilibrium of a bimatrix game. It provides a constructive and elementary proof of existence of an equilibrium, by a typical "directed parity argument\
[Algorithm design and analysis, bimatrix game, complexity class PPAD, game theory, Nash equilibrium, Linear programming, Routing, Mathematics, Complexity theory, Electronic commerce, dual cyclic polytopes, Game theory, Lemke-Howson algorithm, matrix algebra, Computer science, directed parity argument, exponential time, IP networks, computational complexity]
Edge pricing of multicommodity networks for heterogeneous selfish users
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We examine how the selfish behavior of heterogeneous users in a network can be regulated through economic disincentives, i.e., through the introduction of appropriate taxation. One wants to impose taxes on the edges so that any traffic equilibrium reached by the selfish users who are conscious of both the travel latencies and the taxes will minimize the social cost, i.e., will minimize the total latency. We generalize previous results of Cole, Dodis and Roughgarden that held for a single origin-destination pair to the multicommodity setting. Our approach, which could be of independent interest, is based on the formulation of traffic equilibria as a nonlinear complementarity problem by Aashtiani and Magnanti (1981), We extend this formulation so that each of its solutions will give us a set of taxes that forces the network users to conform, at equilibrium, to a certain prescribed routing. We use the special nature of the prescribed minimum-latency flow in order to reduce the difficult nonlinear complementarity formulation to a pair of primal-dual linear programs. LP duality is then enough to derive our results.
[economic disincentives, appropriate taxation, LP duality, network routing, graph theory, origin-destination pair, game theory, multicommodity networks, selfish behavior, linear programming, traffic equilibrium, prescribed minimum-latency flow, Computer science, heterogeneous selfish users, edge pricing, Pricing, duality (mathematics), primal-dual linear program]
Tolls for heterogeneous selfish users in multicommodity networks and generalized congestion games
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We prove the existence of tolls to induce multicommodity, heterogeneous network users that independently choose routes minimizing their own linear function of tolls versus latency to collectively form the traffic pattern of a minimum average latency flow. This generalizes both the previous known results of the existence of tolls for multicommodity, homogeneous users (Beckman et al., 1956) and for single commodity, heterogeneous users (Cole et al., 2003). Unlike previous proofs for single commodity users in general graphs, our proof is constructive - it does not rely on a fixed point theorem - and results in a simple polynomial-sized linear program to compute tolls when the number of different types of users is bounded by a polynomial. We show that our proof gives a complete characterization of flows that are enforceable by tolls. In particular, tolls exist to induce any traffic pattern that is the result of minimizing an arbitrary function from R/sup E(G)/ to the reals that is nondecreasing in each of its arguments. Thus, tolls exist to induce flows with minimum average weighted latency, minimum maximum latency, and other natural objectives. We give an exponential bound on tolls that is independent of the number of network users and the number of commodities. We use this to show that multicommodity tolls also exist when users are not from discrete classes, but instead define a general function that trades off latency versus toll preference. Finally, we show that our result extends to very general frameworks. In particular, we show that tolls exist to induce the Nash equilibrium of general nonatomic congestion games to be system optimal. In particular, tolls exist even when 1) latencies depend on user type; 2) latency functions are nonseparable functions of traffic on edges; 3) the latency of a set S is an arbitrary function of the latencies of the resources contained in S. Our exponential bound on size of tolls also holds in this case; and we give an example of a congestion game that shows this is tight; it requires tolls that are exponential in the size of the game.
[general graphs, telecommunication congestion control, graph theory, Transportation, Telecommunication traffic, Nash equilibrium, generalized congestion games, Delay, Intelligent networks, linear function, Cost function, Polynomials, polynomial-sized linear program, minimum maximum latency, game theory, multicommodity networks, multicommodity homogeneous users, minimum average latency flow, minimum average weighted latency, Computer science, single commodity heterogeneous users, heterogeneous selfish users, traffic pattern, telecommunication traffic, general nonatomic congestion games]
A polynomial time algorithm for computing an Arrow-Debreu market equilibrium for linear utilities
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We provide the first polynomial time exact algorithm for computing an Arrow-Debreu market equilibrium for the case of linear utilities. Our algorithm is based on solving a convex program using the ellipsoid algorithm and simultaneous diophantine approximation. As a side result, we prove that the set of assignments at equilibria is convex and the equilibria prices themselves are log-convex. Our convex program is explicit and intuitive, which allows maximizing a concave function over the set of equilibria. On the practical side, Ye developed an interior point algorithm (Ye, 2004) to find an equilibrium based on our convex program. We also derive separate combinatorial characterizations of equilibrium for Arrow-Debreu and Fisher cases. Our convex program can be extended for many non-linear utilities (Codenotti and Varadarajan, 2004; Jain and Ye) and production models (Jain). Our paper also makes a powerful theorem even more powerful in the area of geometric algorithms and combinatorial optimization. The main idea in this generalization is to allow ellipsoids not to contain the whole convex region but a part of it. This theorem is of independent interest.
[geometric algorithm, simultaneous diophantine approximation, game theory, convex programming, polynomial time algorithm, linear utilities, Ellipsoids, Computer science, ellipsoid algorithm, marketing, combinatorial optimization, Feedback, Production, Approximation algorithms, Polynomials, Arrow-Debreu market equilibrium, concave function, convex program, computational complexity]
The price of stability for network design with fair cost allocation
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Network design is a fundamental problem for which it is important to understand the effects of strategic behavior. Given a collection of self-interested agents who want to form a network connecting certain endpoints, the set of stable solutions - the Nash equilibria - may look quite different from the centrally enforced optimum. We study the quality of the best Nash equilibrium, and refer to the ratio of its cost to the optimum network cost as the price of stability. The best Nash equilibrium solution has a natural meaning of stability in this context - it is the optimal solution that can be proposed from which no user will "defect". We consider the price of stability for network design with respect to one of the most widely-studied protocols for network cost allocation, in which the cost of each edge is divided equally between users whose connections make use of it; this fair-division scheme can be derived from the Shapley value, and has a number of basic economic motivations. We show that the price of stability for network design with respect to this fair cost allocation is O(log k), where k is the number of users, and that a good Nash equilibrium can be achieved via best-response dynamics in which users iteratively defect from a starting solution. This establishes that the fair cost allocation protocol is in fact a useful mechanism for inducing strategic behavior to form near-optimal equilibria. We discuss connections to the class of potential games defined by Monderer and Shapley, and extend our results to cases in which users are seeking to balance network design costs with latencies in the constructed network, with stronger results when the network has only delays and no construction costs. We also present bounds on the convergence time of best-response dynamics, and discuss extensions to a weighted game.
[weighted game, Protocols, graph theory, strategic behavior, network cost allocation, fair-division scheme, Nash equilibrium, Delay, Convergence, near-optimal equilibria, self-interested agents, optimum network cost, Cost function, protocols, network design, Stability, game theory, Routing, fair cost allocation, Computer science, Shapley value, stability price, Load management, best-response dynamics, Joining processes]
Holographic algorithms
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We introduce a new notion of efficient reduction among computational problems. Classical reductions involve gadgets that map local solutions of one problem to local solutions of another in one-to-one, or possibly many-to-one or one-to-many, fashion. Our proposed reductions allow for gadgets with many-to-many correspondences. Their objective is to preserve the sum of the local solutions. Such reductions provide a method of translating a combinatorial problem to a family of finite systems of polynomial equations with integer coefficients such that the number of solutions of the combinatorial problem can be counted in polynomial time if some system in the family has a solution over the complex numbers. We can derive polynomial time algorithms in this way for ten problems for which only exponential time algorithms were known before. General questions about complexity classes are also formulated. If the method is applied to a #P-complete problem then we obtain families of polynomial systems such that the solvability of any one member would imply P/sup #P/ = NC2.
[combinatorial mathematics, integer coefficients, complexity class, combinatorial problem, Holography, holographic algorithms, #P-complete problem, computational problem, many-to-many correspondence, Research and development, exponential time algorithm, complex number, many-to-one mapping, Polynomials, National security, Contracts, polynomial system, one-to-many mapping, polynomials, one-to-one mapping, Interference, Graph theory, Ice, polynomial equations, solvability, polynomial time algorithm, Equations, Physics, computational complexity, holographic algorithm]
Hierarchy theorems for probabilistic polynomial time
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We show a hierarchy for probabilistic time with one bit of advice, specifically we show that for all real numbers 1 /spl les/ /spl alpha/ /spl les/ /spl beta/, BPTIME(n/sup /spl alpha//)/l /spl sube/ BPTIME(n/sup /spl beta//)/l. This result builds on and improves an earlier hierarchy of Barak using O(log log n) bits of advice. We also show that for any constant d > 0, there is a language L computable on average in BPP but not on average in BPTIME (n/sup d/). We build on Barak's techniques by using a different translation argument and by a careful application of the fact that there is a PSPACE-complete problem L such that worst-case probabilistic algorithms for L take only slightly more time than average-case algorithms.
[Computational modeling, Computer simulation, probability, hierarchy theorems, Computer science, Barak techniques, translation argument, probabilistic polynomial time, PSPACE-complete problem, Computer errors, Approximation algorithms, Polynomials, probabilistic time, computational complexity, worst-case probabilistic algorithm]
Private codes or succinct random codes that are (almost) perfect
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Coding theory addresses the design and analysis of codes that enable communication over noisy channels. Two types of channels that have been extensively considered are the binary symmetric channel and the adversarial channel. In a binary symmetric channel each bit of the sent message is flipped independently with some probability p, implying that the noise imposed by the channel is random in nature where the amount of noise is determined by p. In an adversarial channel the message is treated as a whole, and the noise may be an arbitrary (and malicious) function of the message being sent, as long as it does not effect more that a certain fraction (say p) of the bits transmitted. Roughly speaking, any code designed for an adversarial channel can be used on a corresponding binary symmetric channel successfully, whereas the contrary is not necessarily true. In this work we present a construction that transforms the best codes for binary symmetric channels into "codes" for corresponding adversarial channels. The "codes" we present assume that the sender and the receiver of the message have a joint secret random string (which is not known to the channel). These codes are referred to as private codes. Intuitively, this private randomness allows a reduction between the random and adversarial channels. Such a reduction is simple once the size of the joint random string is /spl Theta/(n log n) (here the codes are a subset of {0,1 }/sup n/). In this work we present private codes in which the size of the joint random string is O(log n). Moreover, we show that our result is tight. Namely, to design private codes that allow communication over adversarial channels that meet the bounds achievable when communicating over binary symmetric channels, an amount of /spl Omega/(log n) shared random bits are required. To the best of our knowledge, no prior results of this nature have been presented in the past. As part of our proof we establish a connection between list decodable codes and private codes which complements a recent result of Guruswami (CCC '03) on list decoding with side information.
[telecommunication channels, binary symmetric channel, private codes, decoding, Computer science, random codes, coding theory, decodable code, joint secret random string, succinct random codes, noisy channel, adversarial channel, computational complexity]
On the list and bounded distance decodibility of Reed-Solomon codes
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
For an error-correcting code and a distance bound, the list decoding problem is to compute all the codewords within a given distance to a received message. The bounded distance decoding problem is to find one codeword if there is at least one codeword within the given distance, or to output the empty set if there is not. Obviously the bounded distance decoding problem is not as hard as the list decoding problem. For a Reed-Solomon code [n, k]/sup q/, a simple counting argument shows that for any integer 0 < g < n, there exists at least one Hamming ball of radius n - g, which contains at least (/sup n//sub g/)/q/sup g-k/ many codewords. Let g(n, k, q) be the smallest positive integer g such that (/sup n//sub g/)/q/sup g-k/ < 1. One knows that k /spl les/ g(n, k, q) /spl les/ /spl radic/nk /spl les/ n. For the distance bound up to n- /spl radic/nk;, it is well known that both the list and bounded distance decoding can be solved efficiently. For the distance bound between n - /spl radic/nk and n - g(n, k, q), we do not know whether the Reed-Solomon code is list, or bounded distance decodable, nor do we know whether there are polynomially many codewords in all balls of the radius. It is generally believed that the answers to both questions are no. There are public key cryptosystems proposed recently, whose security is based on the assumptions. In this paper, we prove: (1) List decoding can not be done for radius n - g(n, k: q) or larger, otherwise the discrete logarithm over F/sub qg(m, k, q)-k/ is easy. (2) Let h and g be positive integers satisfying q /spl ges/ max(g/sup 2/, (h-l)/sup 2+/spl epsiv//) and g /spl ges/ (4//spl epsiv/ + 2)(h + 1) for a constant /spl epsiv/ > 0. We show that the discrete logarithm problem over F/sub qh/ can be efficiently reduced by a randomized algorithm to the bounded distance decoding problem of the Reed-Solomon code [q, g - h]/sub q/ with radius q - g. These results show that the decoding problems for the Reed-Solomon code are at least as hard as the discrete logarithm problem over finite fields. The main tools to obtain these results are an interesting connection between the problem of list-decoding of Reed-Solomon code and the problem of discrete logarithm over finite fields, and a generalization of Katz's theorem on representations of elements in an extension finite field by products of distinct linear factors.
[Hamming distance, Engineering profession, list decoding problem, Decoding, randomized algorithm, Security, Galois fields, bounded distance decodibility, decoding, randomised algorithms, Reed-Solomon codes, Computer science, bounded distance decoding problem, Katz theorem, public key cryptography, Computer errors, Public key cryptography, Error correction codes, distance bound, discrete logarithm problem, public key cryptosystem, error-correcting code]
Multilinear-NC/sub 1/ /spl ne/ multilinear-NC/sub 2/
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
An arithmetic circuit or formula is multilinear if the polynomial computed at each of its wires is multilinear. We give an explicit example for a polynomial f(x/sub 1/,..., x/sub n/), with coefficients in {0,1}, such that over any field: (1) f can be computed by a polynomial-size multilinear circuit of depth O(log/sup 2/ n). (2) Any multilinear formula for f is of size n/sup /spl Omega/(log n)/. This gives a super-polynomial gap between multilinear circuit and formula size, and separates multilinear NC/sub 1/ circuits from multilinear NC/sub 2/ circuits.
[circuit complexity, polynomial-size multilinear circuit, Input variables, Circuits, Complexity theory, Computer science, Tree graphs, Wires, digital arithmetic, multilinear-NC/sub 2/, multilinear-NC/sub 1/, Polynomials, arithmetic circuit, Arithmetic, super-polynomial gap]
Algebras with polynomial identities and computing the determinant
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Nisan (1991) proved an exponential lower bound on the size of an algebraic branching program (ABP) that computes the determinant of a matrix in the non-commutative "free algebra" setting, in which there are no non-trivial relationships between the matrix entries. By contrast, when the matrix entries commute there are polynomial size ABPs for the determinant. This paper extends Nisan's result to a much wider class of non-commutative algebras, including all non-trivial matrix algebras over any field of characteristic 0, group algebras of all non-abelian finite groups over algebraically closed fields of characteristic 0, the quaternion algebra and the Clifford algebras. As a result, we obtain more compelling evidence for the essential role played by commutativity in the efficient computation of the determinant. The key to our approach is a characterization of non-commutative algebras by means of the polynomial identities that they satisfy. Extending Nisan's lower bound framework, we find that any reduction in complexity compared to the free algebra must arise from the ability of the identities to reduce the rank of certain naturally associated matrices. Using results from the theory of algebras with polynomial identities, we are able to show that none of the identities of the above classes of algebras is able to achieve such a rank reduction.
[polynomial identities, group algebra, polynomials, Binary decision diagrams, algebraic branching program, exponential lower bound, matrix algebra, Computer science, group theory, nonabelian finite group, Algebra, Quaternions, noncommutative algebra, Polynomials, Matrices, quaternion algebra, Clifford algebra, Testing, matrix determinant]
Lattice problems in NP /spl cap/ coNP
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We show that the problems of approximating the shortest and closest vector in a lattice to within a factor of /spl radic/n lie in NP intersect coNP. The result (almost) subsumes the three mutually-incomparable previous results regarding these lattice problems: Banaszczyk (1993), Goldreich and Goldwasser (2000), and Aharonov and Regev (2003). Our technique is based on a simple fact regarding succinct approximation of functions using their Fourier transform over the lattice. This technique might be useful elsewhere - we demonstrate this by giving a simple and efficient algorithm for one other lattice problem (CVPP,) improving on a previous result of Regev (2003). An interesting fact is that our result emerged from a "dequantization" of our previous quantum result in (Aharanov and Regev, 2003). This route to proving purely classical results might be beneficial elsewhere.
[Computer science, approximation theory, Fourier transforms, lattice problems, function approximation, Lattices, Fourier transform, approximation problem, computational complexity]
Worst-case to average-case reductions based on Gaussian measures
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We show that solving modular linear equation on the average is at least as hard as approximating several lattice problems in the worst case within a factor almost linear in the rank of the lattice. The lattice problems we consider are the shortest vector problem, the shortest independent vectors problem and the covering radius problem. The approximation factor we obtain is O(n) for all three problems. This greatly improves on all previous work on the subject starting from Ajtai's seminal paper (STOC, 1996), up to the strongest previously known results by Micciancio (STOC, 2002). Our results also bring us closer to the limit where the problems are no longer known to be in NP /spl cap/ coNP. Our main tools are Gaussian measures on lattices and the high dimensional Fourier transform. We start by defining a new lattice parameter which determines the amount of Gaussian noise that one has to add to a lattice in order to get close to a uniform distribution, in addition to yielding quantitatively much stronger results, the use of this parameter allows us to simplify many of the complications in previous work. Our technical contributions are two-fold. First, we show tight connections between this new parameter and existing lattice parameters. One such important connection is between this parameter and the length of the shortest set of linearly independent vectors. Second, we prove that the distribution that one obtains after adding Gaussian noise to the lattice has the following interesting property: the distribution of the noise vector when conditioning on the final value behaves in many respects like the original Gaussian noise vector. In particular, its moments remain essentially unchanged.
[average-case reduction, approximation theory, high dimensional Fourier transform, Fourier transforms, shortest vector problem, Military computing, Engineering profession, Lattices, lattice theory, Vectors, modular linear equation, lattice problem, uniform distribution, Equations, worst-case reduction, Computer science, approximation factor, shortest independent vectors problem, Gaussian noise, Gaussian measures, covering radius problem, Polynomials, Cryptography, computational complexity]
Extracting randomness using few independent sources
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
In this work we give the first deterministic extractors from a constant number of weak sources whose entropy rate is less than 1/2. Specifically, for every /spl delta/ > 0 we give an explicit construction for extracting randomness from a constant (depending polynomially on 1//spl delta/) number of distributions over {0, l}n, each having min-entropy /spl delta/n. These extractors output n bits, which are 2/sup -n/ close to uniform. This construction uses several results from additive number theory, and in particular a recent one by Bourgain, Katz and Tao (2003) and of Konyagin (2003). We also consider the related problem of constructing randomness dispersers. For any constant output length m, our dispersers use a constant number of identical distributions, each with min-entropy /spl Omega/(log n) and outputs every possible m-bit string with positive probability. The main tool we use is a variant of the "stepping-up lemma" used in establishing lower bound on the Ramsey number for hyper-graphs (Erdos and Hajnal, 1980).
[Algorithm design and analysis, positive probability, Protocols, graph theory, probability, random processes, min-entropy, Entropy, Data mining, Distributed computing, Computer science, entropy, Ramsey number, Measurement standards, randomness extraction, deterministic extractors, Polynomials, stepping-up lemma, Random variables, Cryptography, hyper-graphs, additive number theory, number theory]
Deterministic extractors for bit-fixing sources by obtaining an independent seed
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
An {n, k)-bit-fixing source is a distribution X over {0, 1}/sup n/ such that there is a subset of k variables in X/sub 1/, ..., X/sub n/ which are uniformly distributed and independent of each other, and the remaining n - k variables are fixed. A deterministic bit-fixing source extractor is a function E : {0, l}/sup n/ /spl rarr/ {0, l}/sup m/ which on an arbitrary (n, k)-bit-fixing source outputs m bits that are statistically-close to uniform. Recently, Kamp and Zuckerman (2003) gave a construction of deterministic bit-fixing source extractor that extracts /spl Omega/(k/sup 2//n) bits, and requires k > /spl radic/n. In this paper we give constructions of deterministic bit-fixing source extractors that extract (1 -o(1))k bits whenever k > (log n)/sup c/ for some universal constant c > 0. Thus, our constructions extract almost all the randomness from bit-fixing sources and work even when k is small. For k /spl Gt/ /spl radic/n the extracted bits have statistical distance 2/sup -n/spl Omega/(1)/ from uniform, and for k /spl les/ /spl radic/n the extracted bits have statistical distance k/sup -/spl Omega/(1)/ from uniform. Our technique gives a general method to transform deterministic bit-fixing source extractors that extract few bits into extractors which extract almost all the bits.
[Computer science, Scholarships, statistical distance, Circuits, random functions, universal constant, statistical distributions, deterministic extractor, independent seed, Distributed computing, Radio access networks, deterministic bit-fixing source extractor]
Constructing expander graphs by 2-lifts and discrepancy vs. spectral gap
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We present a new explicit construction for expander graphs with nearly optimal spectral gap. The construction is based on a series of 2-lift operations. Let G be a graph on n vertices. A 2-lift of G is a graph H on 2n vertices, with a covering map /spl pi/ : H /spl rarr/ G. It is not hard to see that all eigenvalues of G are also eigenvalues of H. In addition, H has n "new" eigenvalues. We conjecture that every d-regular graph has a 2-lift such that all new eigenvalues are in the range [-2/spl radic/d-1, /spl radic/d-1] (If true, this is tight , e.g. by the Alon-Boppana bound). Here we show that every graph of maximal degree d has a 2-lift such that all "new" eigenvalues are in the range [-c/spl radic/d log/sup 3/d, c/spl radic/d log/sup 3/ d] for some constant c. This leads to a polynomial time algorithm for constructing arbitrarily large d-regular graphs, with second eigenvalue O(/spl radic/d log/sup 3/ d). The proof uses the following lemma (Lemma 3.6): Let A be a real symmetric matrix with zeros on the diagonal. Let d be such that the l/sub 1/ norm of each row in A is at most d. Suppose that (|xAy|)/(/spl par/x/spl par//spl par/y/spl par/) /spl les/ /spl alpha/ for every x,y /spl isin/ {0, l}/sup n/ with (x,y)= 0. Then the spectral radius of A is O(/spl alpha/(log(d//spl alpha/) + 1)). An interesting consequence of this lemma is a converse to the expander mixing lemma.
[eigenvalues, Discrepancy, expander mixing lemma, graph theory, Expander Graphs, Lifts, d-regular graph, Mathematics, eigenvalues and eigenfunctions, Eigenvalues and eigenfunctions, Polynomials, expander graph, nearly optimal spectral gap, Symmetric matrices, H infinity control, Graph theory, Signed Graphs, Application software, polynomial time algorithm, matrix algebra, Computer science, Lifts of Graphs, Chromium, 2-lifts discrepancy, computational complexity, l/sub 1/ norm]
Testing polynomials over general fields
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
In this work we fill in the knowledge gap concerning testing polynomials over finite fields. As previous works show, when the cardinality of the field, q, is sufficiently larger than the degree bound, d, then the number of queries sufficient for testing is polynomial or even linear in d. On the other hand, when q = 2 then the number of queries, both sufficient and necessary, grows exponentially with d. Here we study the intermediate case where 2 < q /spl les/ O(d) and show a smooth transition between the two extremes. Specifically, let p be the characteristic of the field (so that p is prime and q = p/sup s/ for some integer s > 1). Then the number of queries performed by the test grows like /spl lscr/ /spl middot/ q/sup 2/spl lscr/+1/, where /spl lscr/ = /spl lceil/(d+1)/((q-q)/p)/spl rceil/. Furthermore, q/sup /spl Omega/(/spl lscr/)/ queries are necessary when q /spl les/ O(d). The test itself provides a unifying view of the two extremes: it considers random affine subspaces of dimension /spl lscr/ and verifies that the function restricted to the selected subspaces is a degree d polynomial. Viewed in the context of coding theory, our result shows that Reed-Muller codes over general fields (usually referred to as generalized Reed-Muller (GRM) codes) are locally testable. In the course of our analysis we provide a characterization of small-weight words that span the code. Such a characterization was previously known only when the field size is a prime or is sufficiently large, in which case the minimum weight words span the code.
[Knowledge engineering, Performance evaluation, System testing, Codes, polynomials, Reed-Muller codes, Galois fields, small-weight words, Computer science, Bridges, polynomial testing, finite fields, Chromium, Polynomials, general fields, computational complexity, random affine subspaces]
Testing low-degree polynomials over prime fields
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We present an efficient randomized algorithm to test if a given function f : F/sub p/ /sup n/ /spl rarr/ F/sub p/ (where p is a prime) is a low-degree polynomial. This gives a local test for generalized Reed-Muller codes over prime fields. For a given integer t and a given real /spl epsiv/ > 0, the algorithm queries f at 1//spl epsiv/ + t/spl middot/p/sup 2r/p-1+O(1)/ points to determine whether f can be described by a polynomial of degree at most t. If f is indeed a polynomial of degree at most t, our algorithm always accepts, and if f has a relative distance at least e from every degree t polynomial, then our algorithm rejects f with probability at least 1/2. Our result is almost optimal since any such algorithm must query f on at least /spl Omega/(1//spl epsiv/ + p/sup r+1/p-1/) points.
[Computer science, Codes, Automatic testing, polynomials, low-degree polynomial testing, generalized Reed-Muller codes, Reed-Muller codes, Polynomials, randomized algorithm, prime fields, randomised algorithms]
Measured descent: a new embedding method for finite metrics
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We devise a new embedding technique, which we call measured descent, based on decomposing a metric space locally, at varying speeds, according to the density of some probability measure. This provides a refined and unified framework for the two primary methods of constructing Frechet embeddings for finite metrics, due to J. Bourgain and S. Rao. We prove that any n-point metric space (X, d) embeds in Hilbert space with distortion O(/spl radic//spl alpha//sub X//spl middot/log n), where /spl alpha//sub X/ is a geometric estimate on the decomposability of X. An an immediate corollary, we obtain an O(/spl radic/log /spl lambda//sub X//spl middot/log n) distortion embedding, where /spl lambda//sub X/ is the doubling constant of X. Since /spl lambda//sub X/ /spl les/ n, this result recovers Bourgain 5 theorem, but when the metric X is, in a sense, "low-dimensional\
[Density measurement, weighted n-point planar graph, graph theory, distortion embedding, computational geometry, probability measure, Hilbert space, Velocity measurement, Distortion measurement, measured descent, Frechet embeddings, probability, geometric estimate, Extraterrestrial measurements, Hilbert spaces, Application software, Yield estimation, Computer science, volume-respecting embeddings, finite metrics, Upper bound, Coordinate measuring machines, metric space decomposition, embedding method, computational complexity]
Triangulation and embedding using small sets of beacons
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Concurrent with recent theoretical interest in the problem of metric embedding, a growing body of research in the networking community has studied the distance matrix defined by node-to-node latencies in the Internet, resulting in a number of recent approaches that approximately embed this distance matrix into low-dimensional Euclidean space. Here we give algorithms with provable performance guarantees for beacon-based triangulation and embedding. We show that in addition to multiplicative error in the distances, performance guarantees for beacon-based algorithms typically must include a notion of slack - a certain fraction of all distances may be arbitrarily distorted. For metrics of bounded doubling dimension (which have been proposed as a reasonable abstraction of Internet latencies), we show that triangulation-based reconstruction with a constant number of beacons can achieve multiplicative error 1 + /spl delta/ on a 1 - /spl epsiv/ fraction of distances, for arbitrarily small constants /spl delta/ and /spl epsiv/. For this same class of metrics, we give a beacon-based embedding algorithm that achieves constant distortion on a 1 - /spl epsiv/ fraction of distances; this provides some theoretical justification for the success of the recent global network positioning algorithm of Ng and Zhang, and it forms an interesting contrast with lower bounds showing that it is not possible to embed all distances in a doubling metric with constant distortion. We also give results for other classes of metrics, as well as distributed algorithms that require only a sparse set of distances but do not place too much measurement load on any one node.
[Algorithm design and analysis, low-dimensional Euclidean space, Embedded computing, multiplicative error, global network positioning algorithm, Internet measurement algorithm, Extraterrestrial measurements, metric embedding, Linear matrix inequalities, beacon-based embedding, distance matrix, Delay, matrix algebra, Computer science, bounded doubling dimension, Constraint theory, triangulation-based reconstruction, Internet, IP networks, Distortion measurement, node-to-node latency, beacon-based triangulation]
A simple linear time (1 + /spl epsiv/)-approximation algorithm for k-means clustering in any dimensions
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We present the first linear time (1 + /spl epsiv/)-approximation algorithm for the k-means problem for fixed k and /spl epsiv/. Our algorithm runs in O(nd) time, which is linear in the size of the input. Another feature of our algorithm is its simplicity - the only technique involved is random sampling.
[approximation theory, sampling methods, Image processing, Image retrieval, random processes, Image sampling, Information retrieval, linear time approximation algorithm, random sampling, Partitioning algorithms, Application software, Data mining, Computer science, k-means clustering, pattern clustering, Clustering algorithms, Web search, computational complexity]
On the power of discrete and of lexicographic Helly-type theorems
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Helly's theorem says that if every d + 1 elements of a given finite set of convex objects in /spl Ropf//sup d/ have a common point, then there is a point common to all of the objects in the set. We define three types of Helly theorems: discrete Helly theorems - where the common point should belong to an a-priori given set, lexicographic Helly theorems - where the common point should not be lexicographically greater than a given point, and lexicographic-discrete Helly theorems. We show the relations between these Helly theorems and their corresponding (standard) Helly theorems. We obtain several discrete and lexicographic Helly numbers. Using these types of Helly theorems we get linear time solutions for various optimization problems. For this, we define a framework, DLP-type (discrete linear programming type), and provide algorithms that solve in randomized linear time fixed-dimensional DLP-type problems. We show that the complexity of the DLP-type class stands somewhere between linear programming (LP) and integer programming (IP). Finally, we use our results in order to solve in randomized linear time problems such as the discrete p-center on the real line, the discrete weighted 1-center problem in /spl Ropf//sup d/ with l/sub /spl infin// norm, the standard (continuous) problem of finding a line transversal for a totally separable set of planar convex objects, a discrete version of the problem of finding a line transversal for a set of axis-parallel planar rectangles, and the (planar) lexicographic rectilinear p-center problem for p = 1,2,3. These are the first known linear time algorithms for these problems.
[convex objects, randomized linear time problems, integer programming, axis-parallel planar rectangles, discrete Helly theorems, lexicographic rectilinear p-center problem, Linear programming, linear programming, linear time optimization, set theory, lexicographic Helly theorems, lexicographic-discrete Helly theorems, randomised algorithms, Geometry, Computer science, discrete linear programming, line transversal, Artificial intelligence, discrete p-center on the real line, Assembly, computational complexity, discrete weighted 1-center problem]
An optimal randomised cell probe lower bound for approximate nearest neighbour searching
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We consider the approximate nearest neighbour search problem on the Hamming cube {0, 1 }/sup d/. We show that a randomised cell probe algorithm that uses polynomial storage and word size d/sup O(1)/ requires a worst case query time of /spl Omega/ (log log d/ log log log d). The approximation factor may be as loose as 2/sup log 1 - /spl eta//d for any fixed /spl eta/ > 0. This generalises an earlier result (Chakrabarti et al., 1999) on the deterministic complexity of the same problem and, more importantly, fills a major gap in the study of this problem since all earlier lower bounds either did not allow randomisation according to Chakrabarti et al. (1999) and Liu (2003) or did not allow approximation according to Borodin et al. (1999), Barkol and Rabani (2000), and Jayram et al. (2003). We also give a cell probe algorithm which proves that our lower bound is optimal. Our proof uses a lower bound on the round complexity of the related communication problem. We show, additionally, that considerations of bit complexity alone cannot prove any nontrivial cell probe lower bound for the problem. This shows that the richness technique (Miltersen et al., 1995) used in a lot of research around this problem would not have helped here. Our proof is based on information theoretic techniques for communication complexity, a theme that has been prominent in research by Chakrabarti et al. (2001), Bar-Yossef et al. (2002), Sen (2003) and Jain et al. (2003). In particular, we make heavy use of the round elimination and message compression ideas in the work of Sen (2003) and Jain et al. (2003), and also introduce a technique which we call message switching.
[round elimination, word size, message compression, bit complexity, Search problems, Complexity theory, communication complexity, approximation factor, Polynomials, information theory, theorem proving, Probes, message switching, richness technique, search problems, worst case query time, approximation theory, data compression, polynomial storage, Extraterrestrial phenomena, optimal randomised cell probe lower bound, Hamming cube, Educational institutions, Information retrieval, Spatial databases, approximate nearest neighbour searching, Application software, round complexity, randomised algorithms, Computer science, deterministic complexity]
Dynamic optimality - almost [competitive online binary search tree]
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We present an O(lg lg n)-competitive online binary search tree, improving upon the best previous (trivial) competitive ratio of O(lg n). This is the first major progress on Sleator and Tarjan's dynamic optimality conjecture of 1985 that O(1)-competitive binary search trees exist.
[Tree data structures, Laboratories, trees (mathematics), Binary search trees, Read-write memory, Data structures, competitive online binary search tree, Computer science, Information science, optimisation, dynamic optimality, Artificial intelligence, computational complexity]
No sorting? Better searching! [optimal array organization]
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Sorting is commonly meant as the task of arranging keys in increasing or decreasing order (or small variations of this order). Given n keys underlying a total order, the best organization in an array is maintaining them in sorted order. Searching requires /spl Theta/ (log n) comparisons in the worst case, which is optimal. We demonstrate that this basic fact in data structures does not hold for the general case of multidimensional keys, whose comparison cost is proportional to their length. In two papers by Andersson et al. (1994) and Andersson et al. (1995) and the full version in 2001, Andersson et al. study the complexity of searching a sorted array of n keys, each of length k, arranged in lexicographic (or alphabetic) order for an arbitrary, possibly unbounded, ordered alphabet. They give sophisticated arguments for proving a tight bound in the worst case for this basic data organization, up to a constant factor, obtaining /spl Theta/(((k log log n)/(log log (4 + ((k log log n)/log n)))) + k log n) character comparisons (or probes). Note that the bound is /spl Theta/ (log n) when k = 1, which is the case that is well known in algorithmics. We describe a permutation of the n keys that is different from the sorted order, and sorting is just the starting point for describing our preprocessing. When keys are stored according to this "unsorted" order in the array, the complexity of searching drops to /spl Theta/ (k + log n) character comparisons (or probes) in the worst case, which is optimal among all possible permutations of the n keys in the array, up to a constant factor. Again, the bound is /spl Theta/ (log n) when k = 1. Jointly with the aforementioned result of Anders son et al., our finding provably shows that keeping k-dimensional keys sorted in an array is not the best data organization for searching. This fact was not observable before by just considering k = O(1) as sorting is an optimal organization in this case. More implications of our result are commented in the introduction.
[Algorithm design and analysis, Costs, Dictionaries, key sorting, unsorted array ordering, array organization, multidimensional keys, Data structures, array searching complexity, Time measurement, character comparisons, Sorting, Computer science, data organization, ordered alphabet, data structures, arrays, Books, algorithmics, Probes, search problems, computational complexity]
Dynamic approximate all-pairs shortest paths in undirected graphs
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We obtain three dynamic algorithms for the approximate all-pairs shortest paths problem in unweighted undirected graphs: 1) For any fixed /spl epsiv/ > 0, a decremental algorithm with an expected total running time of O(mn), where m is the number of edges and n is the number of vertices in the initial graph. Each distance query is answered in O(1) worst-case time, and the stretch of the returned distances is at most 1 + /spl epsiv/. The algorithm uses O(n/sup 2/) space; 2) For any fixed integer k /spl ges/ 1, a decremental algorithm with an expected total running time of O(mn). Each query is answered in O(1) worst-case time, and the stretch of the returned distances is at most 2k - 1. This algorithm uses, however, only O(m + n/sup 1+1/k/) space. It is obtained by dynamizing techniques of Thorup and Zwick. In addition to being more space efficient, this algorithm is also one of the building blocks used to obtain the first algorithm; 3) For any fixed /spl epsiv/, /spl delta/ > 0 and every t /spl les/ m/sup 1/2-/spl delta//, a fully dynamic algorithm with an expected amortized update time of O(mn/t) and worst-case query time of O(t). The stretch of the returned distances is at most 1+/spl epsiv/. All algorithms can also be made to work on undirected graphs with small integer edge weights. If the largest edge weight is b, then all bounds on the running times are multiplied by b.
[Computer science, Shortest path problem, query processing, dynamic approximate all-pairs shortest paths, distance query, Heuristic algorithms, graph theory, amortized update time, dynamic algorithms, decremental algorithm, unweighted undirected graphs, computational complexity]
Dynamic transitive closure via dynamic matrix inverse: extended abstract
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We consider dynamic evaluation of algebraic functions such as computing determinant, matrix adjoint, matrix inverse and solving linear system of equations. We show that in the dynamic setup the above problems can be solved faster than evaluating everything from scratch. In the case when rows and columns of the matrix can change we show an algorithm that achieves O(n/sup 2/) arithmetic operations per update and O(1) arithmetic operations per query. Next, we describe two algorithms, with different tradeoffs, for updating the inverse and determinant when single entries of the matrix are changed. The fastest update for the first tradeoff is O(n/sup 1.575/) arithmetic operations per update and O(n/sup 0.575/) arithmetic operations per query. The second tradeoff gives O(n/sup 1.495/) arithmetic operations per update and O(n/sup 1.495/) arithmetic operations per query. We also consider the case when some number of columns or rows can change. We use dynamic determinant computations to solve the following problems in the dynamic setup: computing the number of spanning trees in a graph and testing if an edge in a graph is contained in some perfect matching. These are the first dynamic algorithms for these problems. Next, with the use of dynamic matrix inverse, we solve fully dynamic transitive closure in general directed graphs. The bounds on arithmetic operations for dynamic matrix inverse translate directly to time bounds for dynamic transitive closure. Thus we obtain the first known algorithm with O(n/sup 2/) worst-case update time and constant query time and two algorithms for transitive closure in general digraphs with subquadratic update and query times. Our algorithms for transitive closure are randomized with one-sided error. We also consider for the first time the case when the edges incident with a part of vertices of the graph can be changed.
[Linear systems, determinants, Heuristic algorithms, Data preprocessing, dynamic matrix inverse, trees (mathematics), matrix adjoint, dynamic algebraic function evaluation, matrix inversion, Vectors, spanning trees, Equations, query processing, Tree graphs, directed graphs, Tin, general directed graphs, Informatics, dynamic transitive closure, Arithmetic, Testing, computational complexity, matrix determinant]
Dynamic speed scaling to manage energy and temperature
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We first consider online speed scaling algorithms to minimize the energy used subject to the constraint that every job finishes by its deadline. We assume that the power required to run at speed s is P(s) = s/sup /spl alpha//. We provide a tight /spl alpha//sup /spl alpha// bound on the competitive ratio of the previously proposed optimal available algorithm. This improves the best known competitive ratio by a factor of 2/sup /spl alpha//. We then introduce an online algorithm, and show that this algorithm's competitive ratio is at most 2(/spl alpha//(/spl alpha/ - 1))/sup /spl alpha//e/sup /spl alpha//. This competitive ratio is significantly better and is approximately 2e/sup /spl alpha/+1/ for large /spl alpha/. Our result is essentially tight for large /spl alpha/. In particular, as /spl alpha/ approaches infinity, we show that any algorithm must have competitive ratio e/sup /spl alpha// (up to lower order terms). We then turn to the problem of dynamic speed scaling to minimize the maximum temperature that the device ever reaches, again subject to the constraint that all jobs finish by their deadlines. We assume that the device cools according to Fourier's law. We show how to solve this problem in polynomial time, within any error bound, using the ellipsoid algorithm.
[Kirk field collapse effect, online speed scaling, Temperature, Cooling, optimal available algorithm, competitive algorithms, dynamic speed scaling, Batteries, temperature management, Computer science, competitive ratio, maximum temperature minimization, ellipsoid algorithm, energy management systems, Microprocessors, Switching loss, Voltage, energy conservation, cooling, Frequency, energy management, Energy management, Fourier law, computational complexity]
Optimal power-down strategies
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We consider the problem of selecting threshold times to transition a device to low-power sleep states during an idle period. The two-state case in which there is a single active and a single sleep state is a continuous version of the ski-rental problem. We consider a generalized version in which there is more than one sleep state, each with its own power consumption rate and transition costs. We give an algorithm that, given a system, produces a deterministic strategy whose competitive ratio is arbitrarily close to optimal. We also give an algorithm to produce the optimal online strategy given a system and a probability distribution that generates the length of the idle period. We also give a simple algorithm that achieves a competitive ratio of 3 + 2/spl radic/2 /spl ap/ 5.828 for any system.
[Energy consumption, Costs, optimal power-down strategies, probability, competitive algorithms, Probability distribution, Application software, Yarn, power consumption, Computer science, Sleep, Computer applications, energy conservation, probability distribution, IP networks, low-power sleep states, Clocks, computational complexity, power consumption rate]
On the streaming model augmented with a sorting primitive
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
The need to deal with massive data sets in many practical applications has led to a growing interest in computational models appropriate for large inputs. The most important quality of a realistic model is that it can be efficiently implemented across a wide range of platforms and operating systems. In this paper, we study the computational model that results if the streaming model is augmented with a sorting primitive. We argue that this model is highly practical, and that a wide range of important problems can be efficiently solved in this (relatively weak) model. Examples are undirected connectivity, minimum spanning trees, and red-blue line segment intersection, among others. This suggests that using more powerful, harder to implement models may not always be justified. Our main technical contribution is to show a hardness result for the "streaming and sorting" model, which demonstrates that the main limitation of this model is that it can only access one data stream at a time. Since our model is strong enough to solve "pointer chasing" problems, the communication complexity based techniques commonly used in showing lower bounds for the streaming model cannot be adapted to our model. We therefore have to employ techniques to obtain these results. Finally, we compare our model to a popular restriction of external memory algorithms that access their data mostly sequentially.
[sorting primitive, Costs, Computational modeling, streaming model, Pipelines, pointer chasing, Complexity theory, Power system modeling, Sorting, external memory algorithms, Degradation, Operating systems, sorting, Hardware, Computational efficiency, computational modeling]
Approximating edit distance efficiently
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Edit distance has been extensively studied for the past several years. Nevertheless, no linear-time algorithm is known to compute the edit distance between two strings, or even to approximate it to within a modest factor. Furthermore, for various natural algorithmic problems such as low-distortion embeddings into normed spaces, approximate nearest-neighbor schemes, and sketching algorithms, known results for the edit distance are rather weak. We develop algorithms that solve gap versions of the edit distance problem: given two strings of length n with the promise that their edit distance is either at most k or greater than /spl lscr/, decide which of the two holds. We present two sketching algorithms for gap versions of edit distance. Our first algorithm solves the k vs. (kn)/sup 2/3/ gap problem, using a constant size sketch. A more involved algorithm solves the stronger k vs. /spl lscr/ gap problem, where /spl lscr/ can be as small as O(k/sup 2/) - still with a constant sketch - but works only for strings that are mildly "nonrepetitive". Finally, we develop an n/sup 3/7/-approximation quasilinear time algorithm for edit distance, improving the previous best factor of n/sup 3/4/ (Cole and Hariharan, 2002); if the input strings are assumed to be nonrepetitive, then the approximation factor can be strengthened to n/sup 1/3/.
[Algorithm design and analysis, approximation theory, approximate nearest-neighbor schemes, Heuristic algorithms, natural algorithmic problems, Genomics, edit distance approximation, nonrepetitive strings, Text processing, approximation factor, Approximation algorithms, sketching algorithms, gap versions, Dynamic programming, string matching, approximation quasilinear time algorithm, Books, low-distortion embeddings, Bioinformatics, Web search, Computational biology, computational complexity]
Strong spatial mixing for lattice graphs with fewer colours
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
Recursively-constructed couplings have been used in the past for mixing on trees. We show for the first time how to extend this technique to nontree-like graphs such as the integer lattice. Using this method, we obtain the following general result. Suppose that G is a triangle-free graph and that for some /spl Delta/ /spl ges/ 3, the maximum degree of G is at most /spl Delta/. We show that the spin system consisting of q-colourings of G has strong spatial mixing, provided q > /spl alpha//spl Delta/, where /spl alpha/ /spl ap/ 1.76322 is the solution to /spl alpha//sup /spl alpha// = e. Note that we have no additional lower bound on q or /spl Delta/. This is important for us because our main objective is to have results which are applicable to the lattices studied in statistical physics such as the integer lattice /spl Zopf//sup d/ and the triangular lattice. For these graphs (in fact, for any graph in which the distance-k neighbourhood of a vertex grows sub-exponentially in k), strong spatial mixing implies that there is a unique infinite-volume Gibbs measure. That is, there is one macroscopic equilibrium rather than many. We extend our general result, obtaining, for example, the first "hand proof" of strong spatial mixing for 7-colourings of triangle-free 4-regular graphs. (Computer-assisted proofs of this result were provided by Salas and Sokal (1997) for the rectangular lattice and by Bubley et al. (1999)). The extension also gives the first hand proof of strong spatial mixing for 5-colourings of triangle-free 3-regular graphs. (A computer-assisted proof for the special case of the hexagonal lattice was provided by Salas and Sokal). Towards the end of the paper we show how to improve our general technique by considering the geometry of the lattice. The idea is to construct the recursive coupling from a system of recurrences rather than from a single recurrence. We use the geometry of the lattice to derive the system of recurrences. This gives us an analysis with a horizon of more than one level of induction, which leads to improved results. We illustrate this idea by proving strong spatial mixing for q = 10 on the lattice /spl Zopf//sup 3/. Finally, we apply the idea to the triangular lattice, adding computational assistance. This gives us the first (machine-assisted) proof of strong spatial mixing for 10-colourings of the triangular lattice. (Such a proof for 11 colours was given by Salas and Sokal.).
[Lattices, infinite-volume Gibbs measure, computational geometry, integer lattice, lattice graphs, graph colouring, Tree graphs, nontree-like graphs, q-colourings, spin system, theorem proving, Antiferromagnetic materials, recursively-constructed couplings, macroscopic equilibrium, triangle-free graph, hexagonal lattice, Physics, Computer science, physics computing, Computational geometry, statistical physics, free energy, strong spatial mixing, triangular lattice, spin systems, statistical mechanics]
Shuffling by semi-random transpositions
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
In the cyclic-to-random shuffle, we are given n cards arranged in a circle. At step k, we exchange the kth card along the circle with a uniformly chosen random card. The problem of determining the mixing time of the cyclic-to-random shuffle was raised by Aldous and Diaconis in 1986. Mironov used this shuffle as a model for the cryptographic system known as RC4, and proved an upper bound of O(n log n) for the mixing time. We prove a matching lower bound, thus establishing that the mixing time is indeed of order /spl Theta/(n log n). We also prove an upper bound of O(n log n) for the mixing time of any "semirandom transposition shuffle\
[Algorithm design and analysis, cryptographic system, uniform random permutations, random processes, Fasteners, cryptography, Mathematics, State-space methods, Statistics, cyclic-to-random shuffle, complex analysis tools, semirandom transposition shuffle, complex-valued test function, Computer science, Upper bound, nonreversible Markov chains, Markov processes, Sampling methods, theorem proving, Cryptography, Testing, computational complexity, mixing time]
Randomly coloring constant degree graphs
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We study a simple Markov chain, known as the Glauber dynamics, for generating a random k-coloring of a n-vertex graph with maximum degree /spl Delta/. We prove that the dynamics converges to a random coloring after O(n log n) steps assuming k /spl ges/ k/sub 0/ for some absolute constant k/sub 0/, and either: (i) k//spl Delta/ > /spl alpha/* /spl ap/ 1.763 and the girth g /spl ges/ 5, or (ii) k//spl Delta/ > /spl beta/* /spl ap/ 1.489 and the girth g /spl ges/ 6. Previous results on this problem applied when k = /spl Omega/(log n), or when k > 11 /spl Delta//6 for general graphs.
[Computer science, Markov chain, Character generation, constant degree graphs, Markov processes, Glauber dynamics, random coloring, graph colouring, computational complexity]
The exact satisfiability threshold for a potentially intractable random constraint satisfaction problem
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We determine the exact threshold of satisfiability for random instances of a particular NP-hard constraint satisfaction problem. The problem appears to share many of the threshold characteristics of random k-SAT for k /spl ges/ 3; for example, we prove the problem almost surely has high resolution complexity. We also prove the analogue of the (2+p)-SAT conjecture for a class of problems that includes this problem and XOR-SAT.
[Heart, Temperature, NP-hard constraint satisfaction problem, constraint theory, resolution complexity, computability, exact satisfiability threshold, random k-SAT, SAT conjecture, Physics, Computer science, potentially intractable random constraint satisfaction problem, Chromium, XOR-SAT, Artificial intelligence, computational complexity]
Spectral analysis of random graphs with skewed degree distributions
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We extend spectral methods to random graphs with skewed degree distributions through a degree based normalization closely connected to the normalized Laplacian. The normalization is based on intuition drawn from perturbation theory of random matrices, and has the effect of boosting the expectation of the random adjacency matrix without increasing the variances of its entries, leading to better perturbation bounds. The primary implication of this result lies in the realm of spectral analysis of random graphs with skewed degree distributions, such as the ubiquitous "power law graphs". Mihail and Papadimitriou (2002) argued that for randomly generated graphs satisfying a power law degree distribution, spectral analysis of the adjacency matrix simply produces the neighborhoods of the high degree nodes as its eigenvectors, and thus miss any embedded structure. We present a generalization of their model, incorporating latent structure, and prove that after applying our transformation, spectral analysis succeeds in recovering the latent structure with high probability.
[graph theory, spectral analysis, random graphs, Distributed power generation, degree based normalization, Information systems, eigenvalues and eigenfunctions, perturbation theory, Intelligent systems, Power generation, latent structure, normalized Laplacian, Laplace equations, Social network services, skewed degree distributions, random processes, Boosting, Spectral analysis, random adjacency matrix, matrix algebra, Computer science, power law degree distribution, eigenvectors, Frequency, ubiquitous power law graphs]
Learning with errors in answers to membership queries
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We study the learning models defined by Angluin et al. (1997): learning with equivalence and limited membership queries and learning with equivalence and malicious membership queries. We show that if a class of concepts that is closed under projection is learnable in polynomial time using equivalence and (standard) membership queries then it is learnable in polynomial time in the above models. This closes the open problems by Angluin et al. (1997). Our algorithm can also handle errors in the equivalence queries.
[limited membership queries, Doped fiber amplifiers, equivalence queries, learning, Computer science, query processing, Boolean functions, Computer errors, Polynomials, Decision trees, learning (artificial intelligence), error handling, malicious membership queries, computational complexity]
Learnability and automatizability
45th Annual IEEE Symposium on Foundations of Computer Science
None
2004
We consider the complexity of properly learning concept classes, i.e. when the learner must output a hypothesis of the same form as the unknown concept. We present the following upper and lower bounds on well-known concept classes: 1) We show that unless NP = RP, there is no polynomial-time PAC learning algorithm for DNF formulae where the hypothesis is an OR-of-thresholds. Note that as special cases, we show that neither DNF nor OR-of-thresholds are properly learnable unless NP = RP. Previous hardness results have required strong restrictions on the size of the output DNF formula. We also prove that it is NP-hard to learn the intersection of /spl lscr/ /spl ges/ 2 halfspaces by the intersection of k halfspaces for any constant k > 0. Previous work held for the case when k = /spl lscr/; 2) Assuming that NP /spl nsube/ DTIME(2/sup n/spl epsi//) for a certain constant /spl epsiv/ < 1 we show that it is not possible to learn size s decision trees by size s/sup k/ decision trees for any k /spl ges/ 0. Previous hardness results for learning decision trees held for k /spl les/ 2; 3) We present the first nontrivial upper bounds on properly learning DNF formulae and decision trees. In particular we show how to learn size s DNF by DNF in time 2/sup O~/(/spl radic/(n log s)), and how to learn size s decision trees by decision trees in time n/sup O(log s)/. The hardness results for DNF formulae and intersections of halfspaces are obtained via specialized graph products for amplifying the hardness of approximating the chromatic number as well as applying work on the hardness of approximate hypergraph coloring. The hardness results for decision trees, as well as the upper bounds, are obtained by developing a connection between automatizability in proof complexity and learnability, which may have other applications.
[Scholarships, proof complexity, Circuits, automatizability, approximate hypergraph coloring, learnability, graph colouring, DNF formulae, Computer science, Upper bound, NP-hard problem, halfspaces, properly learning concept classes complexity, decision trees, nontrivial upper bounds, OR-of-thresholds, Polynomials, theorem proving, Decision trees, Cryptography, learning (artificial intelligence), computational complexity, polynomial-time PAC learning]
Foreword
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Presents the welcome message from the conference proceedings.
[]
Committees
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Provides a listing of current committee members.
[]
Knuth Prize
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
false
[Awards]
Algorithmic Techniques and Tools from Computational Geometry
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Algorithmic Techniques and Tools from Computational Geometry
[Computer science, Computational geometry]
Agnostically learning halfspaces
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We give the first algorithm that (under distributional assumptions) efficiently learns halfspaces in the notoriously difficult agnostic framework of Kearns, Schapire, & Sellie, where a learner is given access to labeled examples drawn from a distribution, without restriction on the labels (e.g. adversarial noise). The algorithm constructs a hypothesis whose error rate on future examples is within an additive /spl epsi/ of the optimal halfspace, in time poly(n) for any constant /spl epsi/ > 0, under the uniform distribution over {-1, 1}/sup n/ or the unit sphere in /spl Ropf//sup n/ , as well as under any log-concave distribution over /spl Ropf/ /sup n/. It also agnostically learns Boolean disjunctions in time 2/sup O~(/spl radic/n)/ with respect to any distribution. The new algorithm, essentially L/sub 1/ polynomial regression, is a noise-tolerant arbitrary distribution generalization of the "low degree" Fourier algorithm of Linial, Mansour, & Nisan. We also give a new algorithm for PAC learning halfspaces under the uniform distribution on the unit sphere with the current best bounds on tolerable rate of "malicious noise".
[Machine learning algorithms, Error analysis, Engineering profession, optimal halfspace, polynomials, Optimized production technology, Artificial neural networks, regression analysis, agnostic learning halfspace, Boolean disjunction, uniform distribution, Learning systems, Support vector machines, Boolean functions, Machine learning, Fourier algorithm, Polynomials, noise-tolerant arbitrary distribution generalization, learning (artificial intelligence), computational complexity, polynomial regression]
Noise stability of functions with low influences: Invariance and optimality
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
In this paper, we study functions with low influences on product probability spaces. The analysis of Boolean functions f {-1, 1}/sup n/ /spl rarr/ {-1, 1} with low influences has become a central problem in discrete Fourier analysis. It is motivated by fundamental questions arising from the construction of probabilistically checkable proofs in theoretical computer science and from problems in the theory of social choice in economics. We prove an invariance principle for multilinear polynomials with low influences and bounded degree; it shows that under mild conditions the distribution of such polynomials is essentially invariant for all product spaces. Ours is one of the very few known non-linear invariance principles. It has the advantage that its proof is simple and that the error bounds are explicit. We also show that the assumption of bounded degree can be eliminated if the polynomials are slightly "smoothed"; this extension is essential for our applications to "noise stability "-type problems. In particular; as applications of the invariance principle we prove two conjectures: the "Majority Is Stablest" conjecture [29] from theoretical computer science, which was the original motivation for this work, and the "It Ain't Over Till It's Over" conjecture [27] from social choice theory. The "Majority Is Stablest" conjecture and its generalizations proven here, in conjunction with the "Unique Games Conjecture" and its variants, imply a number of (optimal) inapproximability results for graph problems.
[noise stability function, Stability, discrete Fourier transforms, polynomials, Probability, discrete Fourier analysis, nonlinear invariance principle, Harmonic analysis, Extraterrestrial measurements, Boolean function, Application software, Combinatorial mathematics, probabilistically checkable proof, Computer science, Computational geometry, unique games conjecture, Boolean functions, theoretical computer science, graph problem, social choice theory, Polynomials, product probability space, inapproximability result, multilinear polynomial]
Every decision tree has an influential variable
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We prove that for any decision tree calculating a Boolean function f : {-1,1}/sup n/ /spl rarr/ {-1, 1}, Var[f] /spl les/ /spl Sigma/ /sub i=1/ /sup n/ /spl delta//sup i/Inf/sub i/(f), i = 1 where /spl delta//sup i/ is the probability that the ith input variable is read and Inf/sub i/(f) is the influence of the ith variable on f. The variance, influence and probability are taken with respect to an arbitrary product measure on {-1, 1}/sup n/n. It follows that the minimum depth of a decision tree calculating a given balanced function is at least the reciprocal of the largest influence of any input variable. Likewise, any balanced Boolean function with a decision tree of depth d has a variable with influence at least 1/d. The only previous nontrivial lower bound known was /spl Omega/(d2/sup -d/). Our inequality has many generalizations, allowing us to prove influence lower bounds for randomized decision trees, decision trees on arbitrary product probability spaces, and decision trees with nonBoolean outputs. As an application of our results we give a very easy proof that the randomized query complexity of nontrivial monotone graph properties is at least/spl Omega/(v/sup 4/3//p/sup 1/3/), where v is the number of vertices and p /spl les/ 1/2 is the critical threshold probability. This supersedes the milestone /spl Omega/(v/sup 4/3//p/sup 1/3/) bound of Hajnal (1991) and is sometimes superior to the best known lower bounds of Chakrabarti-Khot (2001) and Friedgut-Kahn-Wigderson (2002).
[vertex number, Engineering profession, Input variables, arbitrary product measure, nontrivial lower bound, Boolean function, Probability distribution, randomized decision tree, nontrivial monotone graph property, Computer science, Boolean functions, influential variable, decision trees, arbitrary product probability space, threshold probability, Cost function, minimum depth, Decision trees, randomized query complexity]
Lower bounds for the noisy broadcast problem
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We prove the first nontrivial (superlinear) lower bound in the noisy broadcast model of distributed computation. In this model, there are n + 1 processors P/sub 0/, P/sub 1/, ..., P/sub n/. Each P/sub i/, for i /spl ges/ 1, initially has a private bit x/sub i/ and the goal is for P/sub 0/ to learn f (x/sub l/, ..., x/sub n/) for some specified function f. At each time step, a designated processor broadcasts some function of its private bit and the bits it has heard so far. Each broadcast is received by the other processors but each reception may be corrupted by noise. In this model, Gallager (1988) gave a noise-resistant protocol that allows P/sub 0/ to learn the entire input in O(n log log n) broadcasts. We prove that Gallager's protocol is optimal up to a constant factor. Our lower bound follows from a lower bound in a new model, the generalized noisy decision tree model, which may be of independent interest.
[Protocols, Gallager protocol, Computational modeling, multiprocessor interconnection networks, processors designated processor broadcast, distributed computation, noisy broadcast problem, communication complexity, Distributed computing, noise-resistant protocol, Quantum computing, noisy broadcast model, generalized noisy decision tree model, constant factor, Broadcasting, Noise cancellation, Computer networks, Circuit noise, nontrivial superlinear lower bound, Decision trees, protocols, private bit, Context modeling]
The unique games conjecture, integrality gap for cut problems and embeddability of negative type metrics into l/sub 1/
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
In this paper, we disprove the following conjecture due to Goemans (1997) and Linial (2002): "Every negative type metric embeds into with constant distortion." We show that for every /spl delta/ > 0, and for large enough n, there is an n-point negative type metric which requires distortion at-least (log log n) /sup 1/6-/spl delta// to embed into l/sub 1/. Surprisingly, our construction is inspired by the Unique Games Conjecture (UGC) of Khot (2002), establishing a previously unsuspected connection between PCPs and the theory of metric embeddings. We first prove that the UGC implies super-constant hardness results for (non-uniform) sparsest cut and minimum uncut problems. It is already known that the UGC also implies an optimal hardness result for maximum cut (2004). Though these hardness results depend on the UGC, the integrality gap instances rely "only" on the PCP reductions for the respective problems. Towards this, we first construct an integrality gap instance for a natural SDP relaxation of unique games. Then, we "simulate" the PCP reduction and "translate"the integrality gap instance of unique games to integrality gap instances for the respective cut problems! This enables us to prove a (log log n) /sup 1/6-/spl delta// integrality gap for (nonuniform) sparsest cut and minimum uncut, and an optimal integrality gap for maximum cut. All our SDP solutions satisfy the so-called "triangle inequality" constraints. This also shows, for the first time, that the triangle inequality constraints do not add any power to the Goemans-Williamson's SDP relaxation of maximum cut. The integrality gap for sparsest cut immediately implies a lower bound for embedding negative type metrics into l/sub i/. It also disproves the non-uniform version of Arora, Rao and Vazirani's Conjecture (2004), asserting that the integrality gap of the sparsest cut SDP, with the triangle inequality constraints, is bounded from above by a constant.
[Algorithm design and analysis, Embedded computing, optimal hardness, User-generated content, metric embedding theory, game theory, integrality gap, triangle inequality constraint, Educational institutions, Extraterrestrial measurements, super-constant hardness, embedding negative type metric, Partitioning algorithms, Game theory, unique games conjecture, NP-hard problem, cut problem, Approximation algorithms, Iterative algorithms, constant distortion, unsuspected connection, computational complexity]
The closest substring problem with small distances
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
In the closest substring problem k strings s/sub 1/, ..., s/sub k/ are given, and the task is to find a string s of length L such that each string s/sub i/, has a consecutive substring of length L whose distance is at most d from s. The problem is motivated by applications in computational biology. We present two algorithms that can be efficient for small fixed values of d and k: for some functions f and g, the algorithms have running time f(d) /spl middot/ n(O(log d)) and g(d,k) /spl middot/ n(O(log log k)), respectively. The second algorithm is based on connections with the extremal combinatorics of hypergraphs. The closest substring problem is also investigated from the parameterized complexity point of view. Answering an open question from (Evans et al., 2003; Fellows et al.; Gramm et al., 2003), we show that the problem is W[1] hard even if both d and k are parameters. It follows as a consequence of this hardness result that our algorithms are optimal in the sense that the exponent of n in the running time cannot be improved to o(log d) or to o(log log k) (modulo some complexity-theoretic assumptions). Another consequence is that the running time n/sup O(1//spl epsiv/4)/ of the approximation scheme for closest substring presented in (Li et al., 2002) cannot be improved to f(/spl epsiv/) /spl middot/ n/sup c/, i.e. the /spl epsiv/ has to appear in the exponent of n.
[computational biology, complexity theory, Sequences, RNA, graph theory, paramererized complexity point, Combinatorial mathematics, Proteins, hypergraphs extremal combinatorics, NP-hard problem, string searching, DNA, closest substring problem, Approximation algorithms, Polynomials, approximation scheme, Pattern matching, Computational biology, computational complexity]
Fitting tree metrics: Hierarchical clustering and phylogeny
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Given dissimilarity data on pairs of objects in a set, we study the problem of fitting a tree metric to this data so as to minimize additive error (i.e. some measure of the difference between the tree metric and the given data). This problem arises in constructing an M-level hierarchical clustering of objects (or an ultrametric on objects) so as to match the given dissimilarity data - a basic problem in statistics. Viewed in this way, the problem is a generalization of the correlation clustering problem (which corresponds to M = 1). We give a very simple randomized combinatorial algorithm for the M-level hierarchical clustering problem that achieves an approximation ratio of M+2. This is a generalization of a previous factor 3 algorithm for correlation clustering on complete graphs. The problem of fitting tree metrics also arises in phylogeny where the objective is to learn the evolution tree by fitting a tree to dissimilarity data on taxa. The quality of the fit is measured by taking the l/sub p/ norm of the difference between the tree metric constructed and the given data. Previous results obtained a factor 3 approximation for finding the closest tree tree metric under the l/spl infin/ norm. No nontrivial approximation for general l/sub p/ norms was known before. We present a novel LP formulation for this problem and obtain an O((log n log log n)/sup 1/p/) approximation using this. Enroute, we obtain an O((log n log log n)/sup 1/p/) approximation for the closest ultrametric under the l/sub p/ norm. Our techniques are based on representing and viewing an ultrametric as a hierarchy of clusterings, and may be useful in other contexts.
[ultrametric clustering hierarchy, US Department of Energy, Additives, Taxonomy, nontrivial approximation, Phylogeny, tree fitting, Tree graphs, ultrametric object, Clustering algorithms, approximation theory, Engineering profession, trees (mathematics), fitting tree metric, Statistics, randomised algorithms, taxa dissimilarity data, randomized combinatorial algorithm, correlation clustering problem, Approximation algorithms, hierarchical clustering, evolution tree, complete graph, Joining processes, computational complexity]
Metric embeddings with relaxed guarantees
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We consider the problem of embedding finite metrics with slack: we seek to produce embeddings with small dimension and distortion while allowing a (small) constant fraction of all distances to be arbitrarily distorted. This definition is motivated by recent research in the networking community, which achieved striking empirical success at embedding Internet latencies with low distortion into low-dimensional Euclidean space, provided that some small slack is allowed. Answering an open question of Kleinberg, Slivkins, and Wexler (2004), we show that provable guarantees of this type can in fact be achieved in general: any finite metric can be embedded, with constant slack and constant distortion, into constant-dimensional Euclidean space. We then show that there exist stronger embeddings into /spl lscr//sub 1/ which exhibit gracefully degrading distortion: these is a single embedding into /spl lscr//sub 1/ that achieves distortion at most O(log 1//spl epsi/) on all but at most an /spl epsi/ fraction of distances, simultaneously for all /spl epsi/ > 0. We extend this with distortion O(log 1//spl epsi/)/sup 1/p/ to maps into general /spl lscr//sub p/, p /spl ges/ 1 for several classes of metrics, including those with bounded doubling dimension and those arising from the shortest-path metric of a graph with an excluded minor. Finally, we show that many of our constructions are tight, and give a general technique to obtain lower bounds for /spl epsi/-slack embeddings from lower bounds for low-distortion embeddings.
[Algorithm design and analysis, low-dimensional Euclidean space, Engineering profession, relaxed guaranty, graph theory, constant slack, shortest-path metric, Extraterrestrial measurements, metric embedding, Delay, Computer science, Degradation, finite metrics, Approximation algorithms, Cost function, constant distortion, IP networks, Distortion measurement, computational complexity]
Nonembeddability theorems via Fourier analysis
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Various new nonembeddability results (mainly into L/sub 1/) are proved via Fourier analysis. In particular, it is shown that the edit distance on {0, 1}/sup d/ has L/sub 1/ distortion (log d)/sup 1/2 - o(1)/. We also give new lower bounds on the L/sub 1/ distortion of quotients of the discrete hypercube under group actions, and the transportation cost (Earthmover) metric.
[edit distance, Costs, Transportation, Extraterrestrial measurements, Harmonic analysis, Educational institutions, Fourier analysis, transportation cost metric, Surges, Earthmover metric, Computer science, Computational geometry, nonembeddability theorem, Hypercubes, Approximation algorithms, computational complexity]
On the complexity of two-player win-lose games
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
The efficient computation of Nash equilibria is one of the most formidable challenges in computational complexity today. The problem remains open for two-player games. We show that the complexity of two-player Nash equilibria is unchanged when all outcomes are restricted to be 0 or 1. That is, win-or-lose games are as complex as the general case for two-player games.
[Laboratories, game theory, Nash equilibrium, Search problems, Linear programming, Probability distribution, win-or-lose games, Game theory, Computational complexity, Computer science, Nash equilibria, two-player game, two-player win-lose games, two-player games, Artificial intelligence, computational complexity]
Nash equilibria in random games
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We consider Nash equilibria in 2-player random games and analyze a simple Las Vegas algorithm for finding an equilibrium. The algorithm is combinatorial and always finds a Nash equilibrium; on m /spl times/ n payoff matrices, it runs in time O(m/sup 2/n log log n + n/sup 2/m log log m) with high probability. Our main tool is a polytope formulation of equilibria.
[Algorithm design and analysis, Symmetric matrices, combinatorial mathematics, polytope formulation, game theory, Gaussian distribution, Nash equilibrium, Educational institutions, Linear programming, Las Vegas algorithm, Probability distribution, Mathematics, Game theory, random games, combinatorial algorithm, Nash equilibria, Polynomials, computational complexity]
Query incentive networks
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
The concurrent growth of on-line communities exhibiting large-scale social structure, and of large decentralized peer-to-peer file-sharing systems, has stimulated new interest in understanding networks of interacting agents as economic systems. Here we formulate a model for query incentive networks, motivated by such systems: users seeking information or services can pose queries, together with incentives for answering them, that are propagated along paths in a network. This type of information-seeking process can be formulated as a game among the nodes in the network, and this game has a natural Nash equilibrium. In such systems, it is a fundamental question to understand how much incentive is needed in order for a node to achieve a reasonable probability of obtaining an answer to a query from the network. We study the size of query incentives as a function both of the rarity of the answer and the structure of the underlying network. This leads to natural questions related to strategic behavior in branching processes. Whereas the classically studied criticality of branching processes is centered around the region where the branching parameter is 1, we show in contrast that strategic interaction in incentive propagation exhibits critical behavior when the branching parameter is 2.
[peer-to-peer computing, Peer to peer computing, Social network services, information-seeking process, strategic behavior, game theory, Nash equilibrium, Routing, Power system modeling, Information systems, Computer science, branching process, nodes game, decentralized peer-to-peer file-sharing, incentive propagation, Large-scale systems, Internet, branching parameter, Joining processes, query incentive network]
Sink equilibria and convergence
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We introduce the concept of a sink equilibrium. A sink equilibrium is a strongly connected component with no outgoing arcs in the strategy profile graph associated with a game. The strategy profile graph has a vertex set induced by the set of pure strategy profiles; its arc set corresponds to transitions between strategy profiles that occur with nonzero probability. (Here our focus will just be on the special case in which the strategy profile graph is actually a best response graph; that is, its arc set corresponds exactly to best response moves that result from myopic or greedy behaviour). We argue that there is a natural convergence process to sink equilibria in games where agents use pure strategies. This leads to an alternative measure of the social cost of a lack of coordination, the price of sinking, which measures the worst case ratio between the value of a sink equilibrium and the value of the socially optimal solution. We define the value of a sink equilibrium to be the expected social value of the steady state distribution induced by a random walk on that sink. We illustrate the value of this measure in three ways. Firstly, we show that it may more accurately reflects the inefficiency of uncoordinated solutions in competitive games when the use of pure strategies is the norm. In particular, we give an example (a valid-utility game) in which the game converges to solutions which are a factor n worse than socially optimal. The price of sinking is indeed n, but the price of anarchy is close to 1. Secondly, sink equilibria always exist. Thus, even in games in which pure strategy Nash equilibria (PSNE) do not exist, we can still calculate the price of sinking. Thirdly, we show that bounding the price of sinking can have important implications for the speed of convergence to socially good solutions in games where the agents make best response moves in a random order. We present two examples to illustrate our ideas. (i) Unsplittable selfish routing (and weighted congestion games):we prove that the price of sinking for the weighted unsplittable flow version of the selfish routing problem (for bounded-degree polynomial latency functions) is at most O(2/sup 2d/ d/sup 2d + 3/). In comparison, we give instances of these games without any PSNE. Moreover, our proof technique implies fast convergence to socially good (approximate) solutions. This is in contrast to the negative result of Fabrikant, Papadimitriou, and Talwar (2004) showing the existence of exponentially long best-response paths. (ii) Valid-utility games: we show that for valid-utility games the price of sinking is at most n+1; thus the worst case price of sinking in a valid-utility game is between it and n+1. We use our proof to show fast convergence to constant factor approximate solutions in basic-utility games. In addition, we present a hardness result which shows that, in general, there might be states that are exponentially far from any sink equilibrium in valid-utility games. We prove this by showing that the problem of finding a sink equilibrium (or a PSNE) in valid-utility games is PLS-complete.
[social cost, sink equilibria, graph theory, game theory, Routing, Control systems, Nash equilibrium, Steady-state, socially optimal solution, utility game, selfish routing, valid-utility game, Delay, Convergence, Computer science, myopic behaviour, Nash equilibria, strategy profile graph, greedy behaviour, weighted congestion, Cost function, Polynomials, Performance analysis, sink equilibrium]
On the complexity of real functions
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We establish a new connection between the two most common traditions in the theory of real computation, the Blum-Shub-Smale model and the computable analysis approach. We then use the connection to develop a notion of computability and complexity of functions over the reals that can be viewed as an extension of both models. We argue that this notion is very natural when one tries to determine just how difficult a certain function is for a very rich class of functions.
[Scientific computing, Computational modeling, Scholarships, Predictive models, Read-write memory, computability, Computational complexity, Computer science, Blum-Shub-Smale model, Turing machines, Physics computing, real computation, real functions complexity, functions computability, Logic, computable analysis, computational complexity]
Linear lower bounds on real-world implementations of concurrent objects
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
This paper proves /spl Omega/(n) lower bounds on the time to perform a single instance of an operation in any implementation of a large class of data structures shared by n processes. For standard data structures such as counters, stacks, and queues, the bound is tight. The implementations considered may apply any deterministic primitives to a base object. No bounds are assumed on either the number of base objects or their size. Time is measured as the number of steps a process performs on base objects and the number of stalls it incurs as a result of contention with other processes.
[Performance evaluation, deterministic primitives, Scalability, concurrent objects, Laboratories, Area measurement, Data structures, Time measurement, Sun, linear lower bounds, Counting circuits, Computer science, Upper bound, concurrency control, process contention, data structures, computational complexity]
Towards a final analysis of pairing heaps
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Fredman, Sedgewick, Sleator and Tarjan proposed the pairing heap as a self-adjusting, streamlined version of the Fibonacci heap. It provably supports all priority queue operations in logarithmic time and is known to be extremely efficient in practice. However despite its simplicity and empirical superiority, the pairing heap is one of the few popular data structures whose basic complexity remains open. In this paper we prove that pairing heaps support the deletemin operation in optimal logarithmic time and all other operations (insert, meld, and decreasekey) in time O(2/sup 2 /spl radic/(log log n)/). This result gives the first sub-logarithmic time bound for decreasekey and comes close to the lower bound of /spl Omega/(log log n) established by Fredman. Pairing heaps have a well known but poorly understood relationship to splay trees and, to date, the transfer of ideas has flowed in one direction: from splaying to pairing. One contribution of this paper is a new analysis that reasons explicitly with information-theoretic measures. Whether these ideas could contribute to the analysis of splay trees is an open question.
[Costs, priority queue, trees (mathematics), Data structures, logarithmic time bound, Fibonacci heap, Information analysis, splay trees, Computer science, Tree graphs, pairing heaps, Libraries, data structures, information theory, deletemin operation, Queueing analysis, computational complexity]
Structuring labeled trees for optimal succinctness, and beyond
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Consider an ordered, static tree /spl Tscr/ on t nodes where each node has a label from alphabet set /spl Sigma/. Tree /spl Tscr/ may be of arbitrary degree and of arbitrary shape. Say, we wish to support basic navigational operations such as find the parent of node u, the ith child of u, and any child of it with label /spl alpha/. In a seminal work over fifteen years ago, Jacobson (1989) observed that pointer-based tree representations are wasteful in space and introduced the notion of succinct data structures. He studied the special case of unlabeled trees and presented a succinct data structure of 2t + o(t) bits supporting navigational operations in O(1) time. The space used is asymptotically optimal with the information-theoretic lower bound averaged over all trees. This led to a slew of results on succinct data structures for arrays, trees, strings and multisets. Still, for the fundamental problem of structuring labeled trees succinctly, few results, if any, exist even though labeled trees arise frequently in practice, e.g. in the data as in markup text (XML) or in augmented data structures. We present a novel approach to the problem of succinct manipulation of labeled trees by designing what we call the xbw transform of the tree, in the spirit of the well-known Burrows-Wheeler transform for strings. The xbw transform uses path-sorting and grouping to linearize the labeled tree /spl Tscr/ into two coordinated arrays, one capturing the structure and the other the labels. Using the properties of the xbw transform, we (i) derive the first-known (near-)optimal results for succinct representation of labeled trees with O(1) time for navigation operations, (ii) optimally support the powerful subpath search operation for the first time, and (iii) introduce a notion of tree entropy and present linear time algorithms for compressing a given labeled tree up to its entropy beyond the information-theoretic lower bound averaged over all tree inputs. Our xbw transform is simple and likely to spur new results in the theory of tree compression and indexing, and may have some practical impact in XML data processing.
[Shape, Entropy, Jacobian matrices, linear time algorithm, entropy, tree data structures, Tree data structures, Navigation, tree indexing, string transform, path sorting, Burrows-Wheeler transform, Data processing, tree compression, XML data processing, tree searching, succinct data structures, xbw transform, XML, Binary trees, tree entropy, information theoretic lower bound, labeled trees, Indexing, computational complexity]
Approximation algorithms for unique games
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We present a polynomial time algorithm based on semidefinite programming that, given a unique game of value 1 - O(1/logn), satisfies a constant fraction of constraints, where n is the number of variables. For sufficiently large alphabets, it improves an algorithm of Khot (STOC'02) that satisfies a constant fraction of constraints in unique games of value 1 -O(1/(k/sup 10/(log k)/sup 5/)), where k is the size of the alphabet. We also present a simpler algorithm for the special case of unique games with linear constraints. Finally, we present a simple approximation algorithm for 2-to-1 games.
[approximation theory, large alphabets, game theory, 2-to-1 games, Khot algorithm, polynomial time algorithm, unique games, Computer science, semidefinite programming, Tree graphs, constraints fraction, approximation algorithm, linear constraints game, Approximation algorithms, Polynomials, computational complexity]
On non-approximability for quadratic programs
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
This paper studies the computational complexity of the following type of quadratic programs: given an arbitrary matrix whose diagonal elements are zero, find x /spl isin/ {-1, 1}/sup n/ that maximizes x/sup T/Mx. This problem recently attracted attention due to its application in various clustering settings, as well as an intriguing connection to the famous Grothendieck inequality. It is approximable to within a factor of O(log n), and known to be NP-hard to approximate within any factor better than 13/11 - /spl epsi/ for all /spl epsi/ > 0. We show that it is quasi-NP-hard to approximate to a factor better than O(log/sup /spl gamma// n)for some /spl gamma/ > 0. The integrality gap of the natural semidefinite relaxation for this problem is known as the Grothendieck constant of the complete graph, and known to be /spl Theta/(log n). The proof of this fact was nonconstructive, and did not yield an explicit problem instance where this integrality gap is achieved. Our techniques yield an explicit instance for which the integrality gap is /spl Omega/ (log n/log log n), essentially answering one of the open problems of Alon et al. [AMMN].
[Grothendieck constant, Glass, clustering settings, nonapproximability, Linear matrix inequalities, Quadratic programming, zero diagonal elements, Computational complexity, quadratic programming, Physics, Computer science, semidefinite relaxation, Clustering algorithms, quadratic programs, Approximation algorithms, Polynomials, Grothendieck inequality, complete graph, Context modeling, computational complexity]
Hardness of approximating the closest vector problem with pre-processing
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We show that, unless NP/spl sube/DTIME(2/sup poly log(n)/) the closest vector problem with pre-processing, for /spl lscr//sub p/ norm for any p /spl ges/ 1, is hard to approximate within a factor of (log n)/sup 1/p - /spl epsi//' /P for any /spl epsi/ > 0. This improves the previous best factor of 3/sup 1/p/ - /spl epsi/ due to Regev (2004). Our results also imply that under the same complexity assumption, the nearest codeword problem with pre-processing is hard to approximate within a factor of (log n)/sup 1 - /spl epsi//' for any /spl epsi/ > 0.
[Lattices, nearest codeword problem, Educational institutions, Mathematics, Application software, Equations, approximation hardness, Computer science, closest vector problem, complexity assumption, Gaussian processes, Computer applications, Polynomials, Cryptography, computational complexity]
Hardness of the undirected edge-disjoint paths problem with congestion
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
In the edge-disjoint paths problem with congestion (EDPwC), we are given a graph with n nodes, a set of terminal pairs and an integer c. The objective is to route as many terminal pairs as possible, subject to the constraint that at most c demands can be routed through any edge in the graph. When c = 1, the problem is simply referred to as the edge-disjoint paths (EDP) problem. In this paper, we study the hardness of EDPwC in undirected graphs. We obtain an improved hardness result for EDP, and also show the first polylogarithmic integrality gaps and hardness of approximation results for EDPwC. Specifically, we prove that EDP is (log/sup 1/2 - /spl epsiv// n)-hard to approximate for any constant /spl epsiv/ > 0, unless NP /spl sube/ ZPTIME(n/sup polylog n/). We also show that for any congestion c = o(log log n/log log log n), there is no (log/sup (1-/spl epsiv/)/(c+1)/ n) approximation algorithm for EDPwC, unless NP /spl sube/ ZPTIME(n/sup polylog n/). For larger congestion, where c /spl les/ /spl eta/ log log n/log log log n for some constant /spl eta/, we obtain superconstant inapproximability ratios. All of our hardness results can be converted into integrality gaps for the multicommodity flow relaxation. We also present a separate elementary direct proof of this integrality gap result. Finally, we note that similar results can be obtained for the all-or-nothing flow (ANF) problem, a relaxation of EDP, in which the flow unit routed between the source-sink pairs does not have follow a single path, so the resulting flow is not necessarily integral. Using standard transformations, our results also extend to the node-disjoint versions of these problems as well as to the directed setting.
[path problem, Engineering profession, undirected edge-disjoint, polylogarithmic integrality, graph theory, Very large scale integration, Routing, Graph theory, Computational Intelligence Society, superconstant inapproximability ratio, terminal pairs, edge-disjoint path, multicommodity flow relaxation, Tree graphs, all-or-nothing flow, Approximation algorithms, Polynomials, Resource management, undirected graph, computational complexity]
A recursive greedy algorithm for walks in directed graphs
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Given an arc-weighted directed graph G = (V, A, /spl lscr/) and a pair of nodes s, t, we seek to find an s-t walk of length at most B that maximizes some given function f of the set of nodes visited by the walk. The simplest case is when we seek to maximize the number of nodes visited: this is called the orienteering problem. Our main result is a quasi-polynomial time algorithm that yields an O(log OPT) approximation for this problem when f is a given submodular set function. We then extend it to the case when a node v is counted as visited only if the walk reaches v in its time window [R(v), D(v)]. We apply the algorithm to obtain several new results. First, we obtain an O(log OPT) approximation for a generalization of the orienteering problem in which the profit for visiting each node may vary arbitrarily with time. This captures the time window problem considered earlier for which, even in undirected graphs, the best approximation ratio known [Bansal, N et al. (2004)] is O(log/sup 2/ OPT). The second application is an O(log/sup 2/ k) approximation for the k-TSP problem in directed graphs (satisfying asymmetric triangle inequality). This is the first non-trivial approximation algorithm for this problem. The third application is an O(log/sup 2/ k) approximation (in quasi-poly time) for the group Steiner problem in undirected graphs where k is the number of groups. This improves earlier ratios (Garg, N et al.) by a logarithmic factor and almost matches the inapproximability threshold on trees (Halperin and Krauthgamer, 2003). This connection to group Steiner trees also enables us to prove that the problem we consider is hard to approximate to a ratio better than /spl Omega/(log/sup 1-/spl epsi// OPT), even in undirected graphs. Even though our algorithm runs in quasi-poly time, we believe that the implications for the approximability of several basic optimization problems are interesting.
[Greedy algorithms, Steiner trees, quasi-poly time, k-traveling salesman problem, greedy algorithms, graph theory, Optimized production technology, Traveling salesman problems, Routing, Delay, Steiner problem, Vehicles, travelling salesman problems, orienteering problem, time window problem, Tree graphs, quasipolynomial time algorithm, Approximation algorithms, directed graphs walk, greedy algorithm, Floors, computational complexity]
Approximation algorithms for scheduling on multiple machines
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We develop a single rounding algorithm for scheduling on unrelated parallel machines; this algorithm works well with the known linear programming, quadratic programming, and convex programming-relaxations for scheduling to minimize completion time, makespan, and other well-studied objective functions. We obtain the following applications for the general setting of unrelated parallel machines: (i) a bicriteria algorithm for a schedule whose weighted completion-time and makespan simultaneously exhibit the current-best individual approximations for these criteria (3/2 and 2, respectively); (ii) better-than-two approximation guarantees for scheduling under the L/sub p/ norm for all 1 < p < /spl infin/, improving on the 2-approximation algorithms of Azar & Epstein; and (iii) the first constant-factor multicriteria approximation algorithms that handle the weighted completion-time and any given collection of integer L/sub p/ norms. Our algorithm yields a common generalization of rounding theorems due to Karp et al and Shmoys & Tardos; among other applications, this yields an improved approximation for scheduling with resource-dependent processing times studied by Grigoriev et al.
[approximation theory, Laboratories, Parallel machines, better-than-two approximation, Educational institutions, Linear programming, multiple machine scheduling, linear programming, Application software, parallel machines, quadratic programming, Scheduling algorithm, processor scheduling, Computer science, single rounding algorithm, Processor scheduling, convex programming-relaxations, bicriteria algorithm, 2-approximation algorithm, Approximation algorithms, Cost function, constant-factor multicriteria approximation algorithm, resource-dependent processing times, computational complexity]
AdWords and generalized on-line matching
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
How does a search engine company decide what ads to display with each query so as to maximize its revenue? This turns out to be a generalization of the online bipartite matching problem. We introduce the notion of a tradeoff revealing LP and use it to derive two optimal algorithms achieving competitive ratios of 1-1/e for this problem.
[Algorithm design and analysis, Greedy algorithms, search engines, advertising data processing, search engine, Companies, Displays, optimal algorithms, Probability distribution, Floods, information services, deterministic algorithms, randomised algorithms, Computer science, AdWords, online bipartite matching problem, Search engines, Internet, string matching, Advertising, on-line matching]
The parking permit problem
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We consider online problems where purchases have time durations which expire regardless of whether the purchase is used or not. The parking permit problem is the natural analog of the well-studied ski rental problem in this model, and we provide matching upper and lower bounds on the competitive ratio for this problem. By extending the techniques thus developed, we give an online-competitive algorithm for the problem of renting steiner forest edges with time durations.
[steiner forest edges, Costs, purchasing, trees (mathematics), Telecommunication traffic, competitive algorithms, Drives, ski rental problem, rental, Network servers, marketing, Web and internet services, Pricing, Traffic control, Frequency, parking permit problem, Web server, Assembly, online-competitive algorithm]
Correcting errors beyond the Guruswami-Sudan radius in polynomial time
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We introduce a new family of error-correcting codes that have a polynomial-time encoder and a polynomial-time list-decoder, correcting a fraction of adversarial errors up to /spl tau//sub M/ = 1 - /sup M+1//spl radic/(M/sup M/R/sup M/) where R is the rate of the code and M /spl ges/ 1 is an arbitrary integer parameter. This makes it possible to decode beyond the Guruswami-Sudan radius of 1 /spl radic/R for all rates less than 1/16. Stated another way, for any /spl epsiv/ > 0, we can list-decode in polynomial time a fraction of errors up to 1 - /spl epsiv/ with a code of length n and rate /spl Omega/(/spl epsiv//log(1//spl epsiv/)), defined over an alphabet of size n/sup M/ = n/sup O(log(1//spl epsiv/))/. Notably, this error-correction is achieved in the worst-case against adversarial errors: a probabilistic model for the error distribution is neither needed nor assumed. The best results so far for polynomial-time list-decoding of adversarial errors required a rate of O(/spl epsiv//sup 2/) to achieve the correction radius of 1 - /spl epsiv/. Our codes and list-decoders are based on two key ideas. The first is the transition from bivariate polynomial interpolation, pioneered by Sudan and Guruswami-Sudan [1999], to multivariate interpolation decoding. The second idea is to part ways with Reed-Solomon codes, for which numerous prior attempts at breaking the O(/spl epsiv//sup 2/) rate barrier in the worst-case were unsuccessful. Rather than devising a better list-decoder for Reed-Solomon codes, we devise better codes. Standard Reed-Solomon encoders view a message as a polynomial f(X) over a field F/sub q/, and produce the corresponding codeword by evaluating f(X) at n distinct elements of F/sub q/. Herein, given f(X), we first compute one or more related polynomials g/sub 1/(X), g/sub 2/(X), ..., g/sub M-1/(X) and produce the corresponding codeword by evaluating all these polynomials. Correlation between f(X) and g/sub i/(X), carefully designed into our encoder, then provides the additional information we need to recover the encoded message from the output of the multivariate interpolation process.
[Reed-Solomon encoders, Probability distribution, Code standards, probabilistic model, Reed-Solomon codes, Polynomials, polynomial time, polynomial-time encoder, error correction, integer parameter, multivariate interpolation decoding, polynomial-time list-decoder, Hamming distance, error correction codes, probability, Decoding, error distribution, Galois fields, decoding, Interpolation, error-correcting codes, bivariate polynomial interpolation, Guruswami-Sudan radius, Error correction, Error correction codes, computational complexity]
Error-correcting codes for automatic control
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
In many control-theory applications one can classify all possible states of the device by an infinite state graph with polynomially-growing expansion. In order for a controller to control or estimate the state of such a device, it must receive reliable communications from its sensors; if there is channel noise, the encoding task is subject to a stringent real-time constraint. We show a constructive on-line error correcting code that works for this class of applications. Our code is computationally efficient and enables on-line estimation and control in the presence of channel noise. It establishes a constructive (and optimal-within-constants) analog, for control applications, of the Shannon coding theorem.
[Base stations, control applications, error correction codes, automatic control, Communication system control, Mathematics, online estimation, polynomially-growing expansion, Application software, infinite state graph, Engines, Computer science, online control, Shannon coding theorem, channel noise, control-theory applications, Automatic control, Polynomials, Error correction codes, Space exploration, constructive error correcting code, error correction, online error correcting code]
Almost orthogonal linear codes are locally testable
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
A code is said to be locally testable if an algorithm can distinguish between a codeword and a vector being essentially far from the code using a number of queries that is independent of the code's length. The question of characterizing codes that are locally testable is highly complex. In this work we provide a sufficient condition for linear codes to be locally testable. Our condition is based on the weight distribution (spectrum) of the code and of its dual. Codes of (large) length n and minimum distance n/2 - /spl Theta/(/spl radic/n) have size which is at most polynomial in n. We call such codes almost-orthogonal. We use our condition to show that almost-orthogonal codes are locally testable, and, moreover, their dual codes can be spanned by words of constant weights (weight of a codeword refers to the number of its non-zero coordinates). Dual-BCH(n, t) codes are generalizations of the well studied Hadamard codes (t = 1 is Hadamard). Alon et al. (2003) raised the question whether Dual-BCH(n, t) codes are locally testable for constant t. As these codes are known to be almost-orthogonal, we solve this question. We further show that BCH(n, t) code is spanned by its almost shortest words, that is by codewords of weight at most 2t + 2, while the minimum weight is 2t + 1. Our results can be straightforwardly extended to Goppa codes and trace subcodes of algebraic-geometric codes.
[Performance evaluation, linear codes, Goppa codes, dual-BCH(n, Vectors, Galois fields, almost-orthogonal codes, weight distribution spectrum, Computer science, Linear code, Sufficient conditions, orthogonal linear codes, t) codes, geometric codes, Error correction, Error correction codes, BCH codes, Probes, Hadamard codes, Testing, algebraic codes, algebraic-geometric codes]
On Delsarte's linear programming bounds for binary codes
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We prove two results about the value of Delsarte 's linear program for binary codes. Our main result is a new lower bound on the value of the program, which, in particular, is nearly tight for low rate codes. We also give an easy proof of a (known) upper bound, which coincides with the best known bound for a wide range of parameters.
[binary codes, Linear programming, upper bound, Vectors, linear programming, Application software, Linear code, Upper bound, Binary codes, Communication channels, Error correction codes, Random variables, Binary sequences]
Fast algorithms for approximate semidefinite programming using the multiplicative weights update method
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Semidefinite programming (SDP) relaxations appear in many recent approximation algorithms but the only general technique for solving such SDP relaxations is via interior point methods. We use a Lagrangian-relaxation based technique (modified from the papers of Plotkin, Shmoys, and Tardos (PST), and Klein and Lu) to derive faster algorithms for approximately solving several families of SDP relaxations. The algorithms are based upon some improvements to the PST ideas - which lead to new results even for their framework - as well as improvements in approximate eigenvalue computations by using random sampling.
[Algorithm design and analysis, multiplicative weights update, fast algorithms, interior point method, random sampling, Frequency estimation, approximation algorithms, Lagrangian functions, Ellipsoids, eigenvalues and eigenfunctions, Computer science, NP-hard problem, Approximation algorithms, Sampling methods, Polynomials, Eigenvalues and eigenfunctions, Lagrangian-relaxation based technique, eigenvalue computations, approximate semidefinite programming, computational complexity]
Improved smoothed analysis of the shadow vertex simplex method
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Spielman and Teng (JACM '04), proved that the smoothed complexity of a two-phase shadow-vertex method for linear programming is polynomial in the number of constraints n, the number of variables d, and the parameter of perturbation 1//spl sigma/. The key geometric result in their proof was an upper bound of O(nd/sup 3//min (/spl sigma/, (9d ln n)/sup 1/2 /)/sup 6/) on the expected size of the shadow of the polytope defined by the perturbed linear program. In this paper, we give a much simpler proof of a better bound: O(n/sup 2/ d ln n/min (/spl sigma/, (4d ln n)/sup 1/2 /)/sup 2/). When evaluated at /spl sigma/ = (9d ln n)/sup 1/2 /, this improves the size estimate from O(nd/sup 6/ ln/sup 3/ n) to O(n/sup 2/d/sup 2/ ln n). The improvement only becomes better as /spl sigma/ decreases. The bound on the running time of the two-phase shadow vertex proved by Spielman and Teng is dominated by the exponent of /spl sigma/ in the shadow-size bound. By reducing this exponent from 6 to 2, we decrease the exponent in the smoothed complexity of the two-phase shadow vertex method by a multiplicative factor of 3.
[Algorithm design and analysis, perturbed linear program, Gallium arsenide, smoothed analysis, two-phase shadow-vertex method, Linear programming, Mathematics, Vectors, linear programming, smoothed complexity, Computer science, perturbation parameter, Upper bound, Polynomials, shadow-size bound, shadow vertex simplex method, computational complexity]
Sampling-based approximation algorithms for multi-stage stochastic optimization
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Stochastic optimization problems provide a means to model uncertainty in the input data where the uncertainty is modeled by a probability distribution over the possible realizations of the actual data. We consider a broad class of these problems in which the realized input is revealed through a series of stages, and hence are called multi-stage stochastic programming problems. Our main result is to give the first fully polynomial approximation scheme for a broad class of multi-stage stochastic linear programming problems with any constant number of stages. The algorithm analyzed, known as the sample average approximation (SAA) method, is quite simple, and is the one most commonly used in practice. The algorithm accesses the input by means of a "black box" that can generate, given a series of outcomes for the initial stages, a sample of the input according to the conditional probability distribution (given those outcomes). We use this to obtain the first polynomial-time approximation algorithms for a variety of k-stage generalizations of basic combinatorial optimization problems.
[sampling-based approximation algorithm, Uncertainty, Costs, combinatorial mathematics, multistage stochastic programming, conditional probability distribution, Stochastic processes, stochastic programming, combinatorial optimization problem, multistage stochastic linear programming, Linear programming, Probability distribution, linear programming, polynomial-time approximation, Investments, Approximation algorithms, Reservoirs, Polynomials, multistage stochastic optimization, sample average approximation, Water resources, k-stage generalizations, computational complexity, polynomial approximation scheme]
How to pay, come what may: approximation algorithms for demand-robust covering problems
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Robust optimization has traditionally focused on uncertainty in data and costs in optimization problems to formulate models whose solutions will be optimal in the worst-case among the various uncertain scenarios in the model. While these approaches may be thought of defining data- or cost-robust problems, we formulate a new "demand-robust" model motivated by recent work on two-stage stochastic optimization problems. We propose this in the framework of general covering problems and prove a general structural lemma about special types of first-stage solutions for such problems: there exists a first-stage solution that is a minimal feasible solution for the union of the demands for some subset of the scenarios and its objective function value is no more than twice the optimal. We then provide approximation algorithms for a variety of standard discrete covering problems in this setting, including minimum cut, minimum multi-cut, shortest paths, Steiner trees, vertex cover and un-capacitated facility location. While many of our results draw from rounding approaches recently developed for stochastic programming problems, we also show new applications of old metric rounding techniques for cut problems in this demand-robust setting.
[Steiner trees, Uncertainty, data-robust problem, Stochastic processes, stochastic programming, robust optimization, objective function value, metric rounding technique, general structural lemma, approximation algorithm, Cost function, Robustness, cost-robust problem, demand-robust covering problems, Mathematical programming, minimum multicut, vertex cover, demand-robust model, Minimax techniques, Application software, shortest path, uncapacitated facility location, Computer science, minimum cut, Decision theory, standard discrete covering problem, Approximation algorithms, two-stage stochastic optimization problem, stochastic programming problem, computational complexity]
Group-theoretic algorithms for matrix multiplication
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We further develop the group-theoretic approach to fast matrix multiplication introduced by Cohn and Umans, and for the first time use it to derive algorithms asymptotically faster than the standard algorithm. We describe several families of wreath product groups that achieve matrix multiplication exponent less than 3, the asymptotically fastest of which achieves exponent 2.41. We present two conjectures regarding specific improvements, one combinatorial and the other algebraic. Either one would imply that the exponent of matrix multiplication is 2.
[Fourier transforms, combinatorial mathematics, group-theoretic algorithm, Mathematics, algebra, Organizing, Computer science, group theory, matrix multiplication, Upper bound, Linear algebra, Matrices, Standards development]
Answering distance queries in directed graphs using fast matrix multiplication
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Let G = (V, E, w) be a weighted directed graph, where w : E /spl rarr/ {-M, ..., 0, ..., M}. We show that G can be preprocessed in O/spl tilde/(Mn/sup /spl omega//) time, where /spl omega/ < 2.376 is the exponent of fast matrix multiplication, such that subsequently, each distance /spl delta/(u, v) in the graph, where u, v /spl epsi/ V, can be computed exactly in O(n) time. We also present a tradeoff between the processing time and the query answering time. As a very special case, we obtain an O/spl tilde/(Mn/sup /spl omega//) time algorithm for the single source shortest paths (SSSP) problem for directed graphs with integer weights of absolute value at most M. For sufficiently dense graphs, with small enough edge weights, this improves upon the O(m/spl radic/n log M) time algorithm of Goldberg. We note that even the case M = 1, in which all the edge weights are in {-1, 0, +1}, is an interesting case for which no improvement over Goldberg's O(m/spl radic/n) algorithm was known. Our new O/spl tilde/(Mn/sup /spl omega//) algorithm is faster whenever m > n/sup /spl omega/- 1/2 / /spl sime/ n/sup 1.876/.
[Shortest path problem, matrix multiplication, directed graphs, weighted directed graph, O/spl tilde/(Mn/sup /spl omega//) time algorithm, Data structures, query answering time, fast matrix multiplication, single source shortest paths, distance queries, computational complexity]
A randomness-efficient sampler for matrix-valued functions and applications
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
In this paper we give a randomness-efficient sampler for matrix-valued functions. Specifically, we show that a random walk on an expander approximates the recent Chernoff-like bound for matrix-valued functions of Ahlswede and Winter [2002], in a manner which depends optimally on the spectral gap. The proof uses perturbation theory, and is a generalization of Gillman's and Lezaud's analyses of the Ajtai-Komlos-Szemeredi sampler for real-valued functions [Gillman, 1993]. Derandomizing our sampler gives a few applications, yielding deterministic polynomial time algorithms for problems in which derandomizing independent sampling gives only quasi-polynomial time deterministic algorithms. The first (which was our original motivation) is to a polynomial-time derandomization of the Alon-Roichman theorem [Alon and Roichman, 1994]: given a group of size n, find O(log n) elements which generate it as an expander. This implies a second application - efficiently constructing a randomness-optimal homo-morphism tester, significantly improving the previous result of Shpilka and Wigderson [2004]. A third application, which derandomizes a generalization of the set cover problem, is deferred to the full version of this paper.
[quasipolynomial time deterministic algorithm, Error probability, Alon-Roichman theorem, matrix-valued functions, random walk, perturbation theory, polynomial-time derandomization, Polynomials, Cryptography, State estimation, Testing, Symmetric matrices, Chernoff-like bound, real-valued functions, randomness-efficient sampler, deterministic polynomial time algorithm, Graph theory, Application software, deterministic algorithms, matrix algebra, randomised algorithms, Computer science, Sampling methods, set cover problem, randomness-optimal homo-morphism tester, computational complexity]
Deterministic extractors for affine sources over large fields
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
An (n, k)-affine source over a finite field F is a random variable X = (X/sub 1/, ..., X/sub n/) /spl epsi/ F/sub n/, which is uniformly distributed over an (unknown) k-dimensional affine subspace of F/sub n/. We show how to (deterministically) extract practically all the randomness from affine sources, for any field of size larger than n/sup c/ (where c is a large enough constant). Our main results are as follows: 1. (For arbitrary k): For any n, k and any F of size larger than n/sub 20/, we give an explicit construction for a function D : F/sub n/ /spl rarr/ F/sub k-1/, such that for any (n, k)-affine source X over F, the distribution of D(X) is /spl epsiv/-close to uniform, where /spl epsiv/ is polynomially small in |F|. 2. (For k = 1): For any n and any F of size larger than n/sup c/, we give an explicit construction for a function D : F/sup n/ /spl rarr/ {0,1}/sup (1-/spl sigma/)log//sub 2/|F|, such that for any (n, 1)-affine source X over F, the distribution of D(X) is /spl epsiv/-close to uniform, where /spl epsiv/ is polynomially small in |F|. Here, /spl delta/ > 0 is an arbitrary small constant, and c is a constant depending on /spl delta/.
[Computer science, polynomials, (n, random processes, k)-affine source, deterministic extractors, random variable, k-dimensional affine subspace, Polynomials, Random variables, Galois fields, Radio access networks]
Additive approximation for edge-deletion problems
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
A graph property is monotone if it is closed under removal of vertices and edges. In this paper we consider the following edge-deletion problem; given a monotone property P and a graph G, compute the smallest number of edge deletions that are needed in order to turn G into a graph satisfying P. We denote this quantity by E/sub P/'(G). The first result of this paper states that the edge-deletion problem can be efficiently approximated for any monotone property. 1) For any /spl epsiv/ > 0 and any monotone property P, there is a deterministic algorithm, which given a graph G of size n, approximates E/sub P/'(G) in time O(n/sup 2/) to within an additive error of /spl epsiv/n/sup 2/. Given the above, a natural question is for which monotone properties one can obtain better additive approximations of E/sub P/'. Our second main result essentially resolves this problem by giving a precise characterization of the monotone graph properties for which such approximations exist; 1. If there is a bipartite graph that does not satisfy P, then there is a /spl delta/ > 0 for which it is possible to approximate E/sub P/' to within an additive error of n/sup 2-/spl delta// in polynomial time. 2) On the other hand, if all bipartite graphs satisfy P, then for any /spl delta/ > 0 it is NP-hard to approximate E/sub P/' to within an additive error of n/sup 2-/spl delta//. While the proof of (1) is simple, the proof of (2) requires several new ideas and involves tools from extremal graph theory together with spectral techniques. This approach may be useful for obtaining other hardness of approximation results. Interestingly, prior to this work it was not even known that computing E/sub P/' precisely for the properties in (2) is NP-hard. We thus answer (in a strong form) a question of Yannakakis [1981], who asked in 1981 if it is possible to find a large and natural family of graph properties for which computing E/sub P/' is NP-hard.
[Additives, spectral techniques, graph property, additive approximation, monotone property, graph theory, Graph theory, deterministic algorithm, bipartite graph, deterministic algorithms, NP-hardness, Computational geometry, optimisation, edge-deletion problems, Polynomials, Bipartite graph, extremal graph theory, computational complexity]
A characterization of the (natural) graph properties testable with one-sided error
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
The problem of characterizing all the testable graph properties is considered by many to be the most important open problem in the area of property-testing. Our main result in this paper is a solution of an important special case of this general problem; Call a property tester oblivious if its decisions are independent of the size of the input graph. We show that a graph property P has an oblivious one-sided error tester, if and only if P is (semi) hereditary. We stress that any "natural" property that can be tested (either with one-sided or with two-sided error) can be tested by an oblivious tester In particular, all the testers studied thus far in the literature were oblivious. Our main result can thus be considered as a precise characterization of the "natural" graph properties, which are testable with one-sided error. One of the main technical contributions of this paper is in showing that any hereditary graph property can be tested with one-sided error. This general result contains as a special case all the previous results about testing graph properties with one-sided error. These include the results of Goldreich et al., [1998] about testing k-colorability, the characterization of Goldreich and Trevisan [2001] of the graph-partition problems that are testable with 1-sided error, the induced vertex colorability properties of Alon et al., [2000], the induced edge colorability properties of Fischer [2001], a transformation from 2-sided to 1-sided error testing [Goldreich and Trevisan, 2001], as well as a recent result about testing monotone graph properties [Alon and Shapira, 2005]. More importantly, as a special case of our main result, we infer that some of the most well studied graph properties, both in graph theory and computer science, are testable with one-sided error. Some of these properties are the well known graph properties of being perfect, chordal, interval, comparability and more. None of these properties was previously known to be testable.
[Algorithm design and analysis, monotone graph property, graph theory, one-sided error tester, Graph theory, testable graph property, graph-partition problem, vertex colorability property, Computer science, Geometry, natural graph property, Linearity, Computer errors, edge colorability property, k-colorability, Testing, two-sided error]
An algorithmic version of the hypergraph regularity method
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Extending the Szemeredi Regularity Lemma for graphs, P. Frank and Rodl [2002] stablished a 3-graph Regularity Lemma guaranteeing that all large triple systems admit partitions of their edge sets into constantly many classes where most classes consist of regularly distributed edges. Many applications of this lemma require a companion Counting Lemma [Nagle and Rodl, 2003] allowing one to estimate the number of copies of K/sub k//sup 3/ in a "dense and regular" environment created by the 3-graph Regularity Lemma. Combined applications of these lemmas are known as the 3-graph Regularity Method. In this paper, we provide an algorithmic version of the 3-graph Regularity Lemma which, as we show, is compatible with a Counting Lemma. We also discuss some applications. For general k-uniform hypergraphs, Regularity and Counting Lemmas were recently established by Gowers [2005] and by Nagle et al., [2005]. We believe the arguments here provide a basis toward a general algorithmic hypergraph regularity method.
[k-uniform hypergraphs, algorithmic hypergraph regularity method, graph theory, Szemeredi regularity lemma, Graph theory, Application software, Combinatorial mathematics, Statistics, 3-graph regularity lemma, regularly distributed edge, Computer science, counting lemma, Artificial intelligence]
Cryptography in the bounded quantum-storage model
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We initiate the study of two-party cryptographic primitives with unconditional security, assuming that the adversary's quantum memory is of bounded size. We show that oblivious transfer and bit commitment can be implemented in this model using protocols where honest parties need no quantum memory, whereas an adversarial player needs quantum memory of size at least n/2 in order to break the protocol, where n is the number of qubits transmitted. This is in sharp contrast to the classical bounded-memory model, where we can only tolerate adversaries with memory of size quadratic in honest players' memory size. Our protocols are efficient, non-interactive and can be implemented using today's technology. On the technical side, a new entropic uncertainty relation involving min-entropy is established.
[Uncertainty, bounded quantum-storage model, Cryptographic protocols, Computer science, bounded-memory model, quantum memory, Councils, entropic uncertainty relation, Information security, Quantum mechanics, quantum cryptography, Computer errors, qubits, memory size, two-party cryptography, Cryptography, protocols, National security, Computer security]
Quantum information and the PCP theorem
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Our main result is that the membership x /spl epsi/ SAT (for x of length n) can be proved by a logarithmic-size quantum state |/spl Psi/>, together with a polynomial-size classical proof consisting of blocks of length polylog(n) bits each, such that after measuring the state |/spl Psi/> the verifier only needs to read one block of the classical proof. This shows that if a short quantum witness is available then a (classical) PCP with only one query is possible. Our second result is that the class QIP/qpoly contains all languages. That is, for any language L (even non-recursive), the membership x /spl epsi/ L (for x of length n) can be proved by a polynomial-size quantum interactive proof, where the verifier is a polynomial-size quantum circuit with working space initiated with some quantum state |/spl Psi//sub L,n/> (depending only on L and n). Moreover, the interactive proof that we give is of only one round, and the messages communicated are classical. The advice |/spl Psi//sub L,n/> given to the verifier can also be replaced by a classical probabilistic advice, as long as this advice is kept as a secret from the proven Our result can hence be interpreted as: the class IP/rpoly contains all languages. For the proof of the second result, we introduce the quantum low-degree-extension of a string of bits. The main result requires an additional machinery of quantum low-degree-test.
[quantum witness, program verification, Circuits, Length measurement, computability, Time measurement, classical probabilistic advice, Machinery, quantum information, Radio access networks, Q measurement, Quantum computing, polynomial-size classical proof, Quantum mechanics, quantum computing, logarithmic-size quantum state, polynomial-size quantum interactive proof, polynomial-size quantum circuit, quantum low-degree-test, Polynomials, PCP theorem, computational complexity, quantum low-degree-extension]
From optimal measurement to efficient quantum algorithms for the hidden subgroup problem over semidirect product groups
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We approach the hidden subgroup problem by performing the so-called pretty good measurement on hidden subgroup states. For various groups that can be expressed as the semidirect product of an abelian group and a cyclic group, we show that the pretty good measurement is optimal and that its probability of success and unitary implementation are closely related to an average-case algebraic problem. By solving this problem, we find efficient quantum algorithms for a number of nonabelian hidden subgroup problems, including some for which no efficient algorithm was previously known: certain metacyclic groups as well as all groups of the form /spl Zopf//sub p/ /sup r/ /spl times/ /spl Zopf//sub p/ fixed r (including the Heisenberg group, r = 2). In particular our results show that entangled measurements across multiple copies of hidden subgroup states can be useful for efficiently solving the nonabelian HSP.
[Performance evaluation, Heisenberg group, Fourier transforms, Lattices, probability, pretty good measurement, metacyclic groups, abelian group, Physics, Computer science, group theory, semidirect product groups, Quantum computing, entangled measurements, Quantum mechanics, quantum computing, nonabelian hidden subgroup problems, Particle measurements, Polynomials, State estimation, quantum algorithms]
The symmetric group defies strong Fourier sampling
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We resolve the question of whether Fourier sampling can efficiently solve the hidden subgroup problem in general groups. Specifically, we show that the hidden subgroup problem in the symmetric group cannot be efficiently solved by strong Fourier sampling. Indeed we prove the stronger statement that no measurement of a single coset state can reveal more than an exponentially small amount of information about the identity of the hidden subgroup, in the special case relevant to the graph isomorphism problem.
[Fourier transforms, sampling methods, strong Fourier sampling, Lattices, Fourier analysis, group theory, Quantum computing, graph isomorphism problem, Measurement standards, hidden subgroup problem, Tin, Sampling methods, Polynomials, symmetric group, Cryptography, Information theory]
On learning mixtures of heavy-tailed distributions
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We consider the problem of learning mixtures of arbitrary symmetric distributions. We formulate sufficient separation conditions and present a learning algorithm with provable guarantees for mixtures of distributions that satisfy these separation conditions. Our bounds are independent of the variances of the distributions; to the best of our knowledge, there were no previous algorithms known with provable learning guarantees for distributions having infinite variance and/or expectation. For Gaussians and log-concave distributions, our results match the best known sufficient separation conditions by D. Achlioptas and F. McSherry (2005) and S. Vempala and G. Wang (2004). Our algorithm requires a sample of size O/spl tilde/(dk), where d is the number of dimensions and k is the number of distributions in the mixture. We also show that for isotropic power-laws, exponential, and Gaussian distributions, our separation condition is optimal up to a constant factor.
[heavy-tailed distributions, Parameter estimation, Statistical analysis, log normal distribution, arbitrary symmetric distributions, Gaussian distribution, normal distribution, log-concave distributions, Probability distribution, Data mining, learning mixture, Computer science, exponential distribution, isotropic power-laws, Statistical distributions, Machine learning, Gaussians distributions, Polynomials, Iterative methods, learning (artificial intelligence)]
Learning mixtures of product distributions over discrete domains
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We consider the problem of learning mixtures of product distributions over discrete domains in the distribution learning framework introduced by Kearns et al. (1994). We give a poly(n//spl epsi/) time algorithm for learning a mixture of k arbitrary product distributions over the n-dimensional Boolean cube {0, 1}/sup n/ to accuracy /spl epsi/, for any constant k. Previous poly(n)-time algorithms could only achieve this for k = 2 product distributions; our result answers an open question stated independently in M. Cryan (1999) and Y. Freund and Y. Mansour (1999). We further give evidence that no polynomial time algorithm can succeed when k is superconstant, by reduction from a notorious open problem in PAC learning. Finally, we generalize our poly(n//spl epsi/) time algorithm to learn any mixture of k = O(1) product distributions over {0, 1,... , b }/sup n/,for any b = O(1).
[Engineering profession, PAC learning, Geology, probability, distribution learning framework, Probability distribution, Entropy, poly(n//spl epsi/) time algorithm, n-dimensional Boolean cube, Computer science, Learning, Boolean functions, product distributions, learning mixtures, Polynomials, learning (artificial intelligence), Artificial intelligence, computational complexity]
A general lower bound for mixing of single-site dynamics on graphs
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We prove that any Markov chain that performs local, reversible updates on randomly chosen vertices of a bounded-degree graph necessarily has mixing time at least /spl Omega/(n log n), where it is the number of vertices. Our bound applies to the so-called "Glauber dynamics" that has been used extensively in algorithms for the Ising model, independent sets, graph colorings and other structures in computer science and statistical physics, and demonstrates that many of these algorithms are optimal up to constant factors within their class. Previously no super-linear lower bound for this class of algorithms was known. Though widely conjectured, such a bound had been proved previously only in very restricted circumstances, such as for the empty graph and the path. We also show that the assumption of bounded degree is necessary by giving a family of dynamics on graphs of unbounded degree with mixing time O(n).
[Algorithm design and analysis, Terminology, super-linear lower bound, Probability distribution, graph coloring, State-space methods, bounded-degree graph, Physics, Markov random fields, Convergence, Computer science, Markov chain, Monte Carlo methods, directed graphs, Markov processes, Ising model, Glauber dynamic, Random variables, graph dynamics, empty graph, computational complexity]
Analysis and prediction of the long-run behavior of probabilistic sequential programs with recursion
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We introduce a family of long-run average properties of Markov chains that are useful for purposes of performance and reliability analysis, and show that these properties can effectively be checked for a subclass of infinite-state Markov chains generated by probabilistic programs with recursive procedures. We also show how to predict these properties by analyzing finite prefixes of runs, and present an efficient prediction algorithm for the mentioned subclass of Markov chains.
[Algorithm design and analysis, program verification, probabilistic programs, software reliability, recursive procedure, recursive functions, infinite-state Markov chains, probabilistic sequential programs, Computer science, reliability analysis, prediction algorithm, Automata, Failure analysis, Computer errors, Markov processes, Prediction algorithms, long-run behavior, Performance analysis, Logic, Personal digital assistants, Informatics, software performance evaluation, performance analysis]
Safraless decision procedures
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
The automata-theoretic approach is one of the most fundamental approaches to developing decision procedures in mathematical logics. To decide whether a formula in a logic with the tree-model property is satisfiable, one constructs an automaton that accepts all (or enough) tree models of the formula and then checks that the language of this automaton is nonempty. The standard approach translates formulas into alternating parity tree automata, which are then translated, via Safra's determinization construction, into nondeterministic parity automata. This approach is not amenable to implementation because of the difficulty of implementing Safra's construction and the nonemptiness test for nondeterministic parity tree automata. In this paper, we offer an alternative to the standard automata-theoretic approach. The crux of our approach is avoiding the use of Safra's construction and of nondeterministic parity tree automata. Our approach goes instead via universal co-Buchi tree automata and nondeterministic Buchi tree automata. Our translations are significantly simpler than the standard approach, less difficult to implement, and have practical advantages like being amenable to optimizations and a symbolic implementation. We also show that our approach yields better complexity bounds.
[nonemptiness test, automata theory, trees (mathematics), nondeterministic Buchi tree automata, Computer science, Upper bound, determinization construction, Automatic testing, Automata, tree-model property, nondeterministic parity automata, complexity bound, Logic, Artificial intelligence, alternating parity tree automata, nondeterministic parity tree automata, Formal verification, computational complexity]
How to play almost any mental game over the net - concurrent composition via super-polynomial simulation
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We construct a secure protocol for any multiparty functionality that remains secure (under a relaxed definition of security introduced by Prabhakaran and Sahai (STOC '04)) when executed concurrently with multiple copies of itself and other protocols, without any assumptions on existence of trusted parties, common reference string, honest majority or synchronicity of the network. The relaxation of security is obtained by allowing the ideal-model simulator to run in quasipolynomial (as opposed to polynomial) time. Quasipolynomial simulation suffices to ensure security for most applications of multiparty computation. Furthermore, Lindell (FOCS '03, TCC' 04) recently showed that such a protocol is impossible to obtain under the more standard definition of polynomial-time simulation by an ideal adversary. Our construction is the first such protocol under reasonably standard cryptographic assumptions (i.e., existence of a hash function collection that is collision resistent with respect to circuits of subexponential size, and existence of trapdoor permutations which are secure with respect to circuits of quasi-polynomial size). We introduce a new technique: "protocol condensing". That is, taking a protocol that has strong security properties but requires super-polynomial communication and computation, and then transforming it into a protocol with polynomial communication and computation, that still inherits the strong security properties of the original protocol. Our result is obtained by combining this technique with previous techniques of Canetti, Lindell, Ostrovsky, and Sahai (STOC '02) and Pass (STOC '04).
[polynomial-time simulation, protocol condensing, Computational modeling, Computer simulation, Circuits, superpolynomial computation, Nominations and elections, cryptography, multiparty computation, Security, Cryptographic protocols, Computer science, super-polynomial simulation, superpolynomial communication, Voting, multiparty functionality, quasipolynomial simulation, secure protocol, ideal-model simulator, Polynomials, Cryptography, protocols, quasipolynomial time, computational complexity]
On the impossibility of obfuscation with auxiliary input
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Barak et al. formalized the notion of obfuscation, and showed that there exist (contrived) classes of functions that cannot be obfuscated. In contrast, Canetti and Wee showed how to obfuscate point functions, under various complexity assumptions. Thus, it would seem possible that most programs of interest can be obfuscated even though in principle general purpose obfuscators do not exist. We show that this is unlikely to be the case. In particular; we consider the notion of obfuscation w.r.t. auxiliary input, which corresponds to the setting where the adversary, which is given the obfuscated circuit, may have some additional a priori information. This is essentially the case of interest in any usage of obfuscation we can imagine. We prove that there exist many natural classes of functions that cannot be obfuscated w.r.t. auxiliary input, both when the auxiliary input is dependent of the function being obfuscated and even when the auxiliary input is independent of the function being obfuscated. We also give a positive result. In particular; we show that any obfuscator for the class of point functions is also an obfuscator with independent auxiliary input.
[obfuscate point function, additional a priori information, Protocols, programming theory, Circuits, contrived function class, Security, History, principle general purpose obfuscator, obfuscation impossibility, Boolean functions, obfuscation notion, complexity assumption, independent auxiliary input, point function class, Polynomials, obfuscated circuit, Context modeling]
Concurrent non-malleable commitments
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We present a non-malleable commitment scheme that retains its security properties even when concurrently executed a polynomial number of times. That is, a man-in-the-middle adversary who is simultaneously participating in multiple concurrent commitment phases of our scheme, both as a sender and as a receiver cannot make the values he commits to depend on the values he receives commitments to. Our result is achieved without assuming an a-priori bound on the number of executions and without relying on any set-up assumptions. Our construction relies on the existence of standard collision resistant hash functions and only requires a constant number of communication rounds.
[Circuits, multiple concurrent commitment, cryptography, security property, Communication standards, Cryptographic protocols, Computer science, collision resistant hash functions, Polynomials, protocols, concurrent nonmalleable commitment, Usability, Computer security, Contracts]
The complexity of online memory checking
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We consider the problem of storing a large file on a remote and unreliable server. To verify that the file has not been corrupted, a user could store a small private (randomized) "fingerprint" on his own computer. This is the setting for the well-studied authentication problem in cryptography, and the required fingerprint size is well understood. We study the problem of sub-linear authentication: suppose the user would like to encode and store the file in a way that allows him to verify that it has not been corrupted, but without reading the entire file. If the user only wants to read t bits of the file, how large does the size s of the private fingerprint need to be? We define this problem formally, and show a tight lower bound on the relationship between s and t when the adversary is not computationally bounded, namely: s /spl times/ t = /spl Omega/(n), where n is the file size. This is an easier case of the online memory checking problem, introduced by Blum et al. in 1991, and hence the same (tight) lower bound applies also to that problem. It was previously shown that when the adversary is computationally bounded, under the assumption that one-way functions exist, it is possible to construct much better online memory checkers and sub-linear authentication schemes. We show that the existence of one-way functions is also a necessary condition: even slightly breaking the s /spl times/ t = /spl Omega/(n) lower bound in a computational setting implies the existence of one-way functions.
[storage allocation, program verification, Fingerprint recognition, private fingerprint, File servers, Size measurement, cryptography, Encoding, Decoding, sublinear authentication, Authentication, Information security, message authentication, Polynomials, Error correction, Cryptography, one-way functions, online memory checking complexity]
Rational secure computation and ideal mechanism design
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Secure computation essentially guarantees that whatever computation n players can do with the help of a trusted party, they can also do by themselves. Fundamentally, however, this notion depends on the honesty of at least some players. We put forward and implement a stronger notion, rational secure computation, that does not depend on player honesty, but solely on player rationality. The key to our implementation is showing that the ballot-box - the venerable device used throughout the world to tally secret votes securely - can actually be used to securely compute any function. Our work bridges the fields of game theory and cryptography, and has broad implications for mechanism design.
[Protocols, venerable device, Buildings, game theory, ideal mechanism design, ballot box, cryptography, Security, Game theory, Bridges, Computer science, Privacy, Voting, Communication channels, rational secure computation, Cryptography]
Truthful and near-optimal mechanism design via linear programming
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We give a general technique to obtain approximation mechanisms that are truthful in expectation. We show that for packing domains, any /spl alpha/-approximation algorithm that also bounds the integrality gap of the IF relaxation of the problem by a can be used to construct an /spl alpha/-approximation mechanism that is truthful in expectation. This immediately yields a variety of new and significantly improved results for various problem domains and furthermore, yields truthful (in expectation) mechanisms with guarantees that match the best known approximation guarantees when truthfulness is not required. In particular, we obtain the first truthful mechanisms with approximation guarantees for a variety of multi-parameter domains. We obtain truthful (in expectation) mechanisms achieving approximation guarantees of O(/spl radic/m) for combinatorial auctions (CAs), (1 + /spl epsi/ ) for multiunit CAs with B = /spl Omega/(log m) copies of each item, and 2 for multiparameter knapsack problems (multiunit auctions). Our construction is based on considering an LP relaxation of the problem and using the classic VCG mechanism by W. Vickrey (1961), E. Clarke (1971) and T. Groves (1973) to obtain a truthful mechanism in this fractional domain. We argue that the (fractional) optimal solution scaled down by a, where a is the integrality gap of the problem, can be represented as a convex combination of integer solutions, and by viewing this convex combination as specifying a probability distribution over integer solutions, we get a randomized, truthful in expectation mechanism. Our construction can be seen as a way of exploiting VCG in a computational tractable way even when the underlying social-welfare maximization problem is NP-hard.
[Algorithm design and analysis, IF relaxation, expectation mechanism, combinatorial mathematics, LP relaxation, social-welfare maximization problem, combinatorial auctions, Probability distribution, Mathematics, linear programming, commerce, knapsack problems, /spl alpha/-approximation algorithm, Pricing, probability distribution, truthful mechanism design, near-optimal mechanism design, probability, Linear programming, Mechanical factors, Computer science, NP-hardness, Content addressable storage, Approximation algorithms, multiparameter knapsack problems, computational complexity]
Mechanism design via machine learning
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We use techniques from sample-complexity in machine learning to reduce problems of incentive-compatible mechanism design to standard algorithmic questions, for a wide variety of revenue-maximizing pricing problems. Our reductions imply that for these problems, given an optimal (or /spl beta/-approximation) algorithm for the standard algorithmic problem, we can convert it into a (1 + /spl epsi/)-approximation (or /spl beta/(1 +/spl epsi/)-approximation) for the incentive-compatible mechanism design problem, so long as the number of bidders is sufficiently large as a function of an appropriate measure of complexity of the comparison class of solutions. We apply these results to the problem of auctioning a digital good, the attribute auction problem, and to the problem of item-pricing in unlimited-supply combinatorial auctions. From a learning perspective, these settings present several challenges: in particular the loss function is discontinuous and asymmetric, and the range of bidders' valuations may be large.
[Algorithm design and analysis, item pricing, Machine learning algorithms, attribute auction problem, combinatorial mathematics, unlimited-supply combinatorial auction, (1 + /spl epsi/)-approximation, Automobiles, commerce, machine learning, Cost accounting, incentive-compatible mechanism design, sample complexity, Computer science, Measurement standards, Machine learning, Pricing, revenue-maximizing pricing problem, Writing, Marketing and sales, learning (artificial intelligence), pricing, optimal algorithm, computational complexity]
Beyond VCG: frugality of truthful mechanisms
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
We study truthful mechanisms for auctions in which the auctioneer is trying to hire a team of agents to perform a complex task, and paying them for their work. As common in the field of mechanism design, we assume that the agents are selfish and will act in such a way as to maximize their profit, which in particular may include misrepresenting their true incurred cost. Our first contribution is a new and natural definition of the frugality ratio of a mechanism, measuring the amount by which a mechanism "overpays \
[Costs, Protocols, Mechanical factors, r-out-of-k sets auctions, Electronic commerce, History, monopoly-free set systems, Game theory, Computer science, truthful mechanisms, VCG mechanism, Tree graphs, security of data, shortest path auctions, Performance analysis, Resource management, electronic commerce]
An approximation algorithm for the disjoint paths problem in even-degree planar graphs
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
In joint work with Eva Tardos in 1995, we asked whether it was possible to obtain a polynomial-time, polylogarithmic approximation algorithm for the disjoint paths problem in the class of all even-degree planar graphs. This paper answers the question in the affirmative, by providing such an algorithm. The algorithm builds on recent work of C. Chekuri et al. (2004, 2005), who considered muting problems in planar graphs where each edge can carry up to two paths.
[graph theory, Optimized production technology, Very large scale integration, Routing, Graph theory, Design optimization, muting problems, Upper bound, Tree graphs, polynomial-time polylogarithmic approximation algorithm, even-degree planar graphs, Approximation algorithms, Polynomials, disjoint paths, Joining processes, computational complexity]
Algorithmic graph minor theory: Decomposition, approximation, and coloring
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
At the core of the seminal graph minor theory of Robertson and Seymour is a powerful structural theorem capturing the structure of graphs excluding a fixed minor. This result is used throughout graph theory and graph algorithms, but is existential. We develop a polynomial-time algorithm using topological graph theory to decompose a graph into the structure guaranteed by the theorem: a clique-sum of pieces almost-embeddable into bounded-genus surfaces. This result has many applications. In particular we show applications to developing many approximation algorithms, including a 2-approximation to graph coloring, constant-factor approximations to treewidth and the largest grid minor, combinatorial polylogarithmic approximation to half-integral multicommodity flow, subexponential fixed-parameter algorithms, and PTASs for many minimization and maximization problems, on graphs excluding a fixed minor.
[Heart, graph colouring, Tree graphs, half-integral multicommodity flow, topological graph theory, approximation algorithm, Polynomials, polynomial-time algorithm, constant-factor approximation, combinatorial polylogarithmic approximation, treewidth, Testing, Tree data structures, Minimization methods, Graph theory, graph coloring, Application software, largest grid minor, graph algorithm, maximization problem, Computer science, algorithmic graph minor theory, subexponential fixed-parameter algorithm, minimization problem, Approximation algorithms, computational complexity]
A linear-time approximation scheme for planar weighted TSP
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
In view of the fact that an /spl epsi/-optimal tour can be found in the Euclidean case in time that it is polynomial with a fixed degree, independent of /spl epsi/, it seems natural to ask whether the same holds true for the planar case. We give an algorithm requiring O(c/sup 1/c2/ n) time to find an /spl epsi/-optimal traveling salesman tour in the metric defined by a planar graph with nonnegative edge-lengths.
[Buildings, graph theory, Optimization methods, Traveling salesman problems, Turning, planar graph, travelling salesman problems, Tree graphs, Linear approximation, Euclidean distance, planar weighted TSP, /spl epsi/-optimal traveling salesman tour, Approximation algorithms, Polynomials, nonnegative edge-lengths, linear-time approximation, Testing, computational complexity]
A tale of two dimensional bin packing
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
The 2-dimensional bin packing problem (2BP) is a generalization of the classical Bin Packing problem and is defined as follows: Given a collection of rectangles specified by their width and height, pack these into the minimum number of square bins of unit size. We study the case of 'orthogonal packing without rotations', where rectangles cannot be rotated and must be packed parallel to the edges of a bin. Often in practical cases of 2BP problems there are additional constraints on how complicated the packing patterns in a bin can be. A well-studied and frequently used constraint is that every rectangle in the packing must be obtainable by recursively applying a sequence of edge-to-edge cuts parallel to the edges of the bin. Such cuts are known as guillotine cuts. Our main result is that the guillotine 2BP problem admits an asymptotic polynomial time approximation scheme. This is in sharp contrast with the fact that the general 2BP problem is APX-Hard. En route to our main result, we show a structural theorem about approximating general guillotine packings by simpler packings, which could be of independent interest.
[Computer science, 2D bin packing, edge-to-edge cuts, Chromium, APX-Hard, guillotine 2BP problem, asymptotic polynomial time approximation, bin packing, orthogonal packing, computational complexity]
Error correction via linear programming
46th Annual IEEE Symposium on Foundations of Computer Science
None
2005
Suppose we wish to transmit a vector f &#x003F5; Rn reliably. A frequently discussed approach consists in encoding f with an m by n coding matrix A. Assume now that a fraction of the entries of Af are corrupted in a completely arbitrary fashion by an error e. We do not know which entries are affected nor do we know how they are affected. Is it possible to recover f exactly from the corrupted m-dimensional vector y = Af + e?
[Linear code, Linear programming, Particle measurements, Vectors, Mathematics, Encoding, Functional analysis, Error correction, Error correction codes, Decoding]
Foreword
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Presents the welcome message from the conference proceedings.
[]
Organizing Committees
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Provides a listing of current committee members.
[]
Statistical Zero-Knowledge Arguments for NP from Any One-Way Function
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We show that every language in NP has a statistical zero-knowledge argument system under the (minimal) complexity assumption that one-way functions exist. In such protocols, even a computationally unbounded verifier cannot learn anything other than the fact that the assertion being proven is true, whereas a polynomial-time prover cannot convince the verifier to accept a false assertion except with negligible probability. This resolves an open question posed by Naor et al. (1998). Departing from previous works on this problem, we do not construct standard statistically hiding commitments from any one-way function. Instead, we construct a relaxed variant of commitment schemes called "1-out-of-2-binding commitments," recently introduced by Nguyen et al. (2006)
[Computational modeling, Probability, polynomial-time prover, cryptography, commitment schemes, Complexity theory, one-way function, Cryptographic protocols, 1-out-of-2-binding commitments, minimal complexity assumption, Information security, Authentication, DH-HEMTs, Polynomials, statistical zero-knowledge argument, Cryptography, statistical analysis, computational complexity]
Fault-Tolerant Distributed Computing in Full-Information Networks
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
In this paper, we use random-selection protocols in the full-information model to solve classical problems in distributed computing. Our main results are the following: An O(log n)-round randomized Byzantine agreement (BA) protocol in a synchronous full-information network tolerating t &lt; n/(3+epsi) faulty players (for any constant epsi &gt; 0). As such, our protocol is asymptotically optimal in terms of fault-tolerance. An O(1)-round randomized BA protocol in a synchronous full-information network tolerating t = O(n/((log n)1.58)) faulty players. A compiler that converts any randomized protocol Pi<sub>in</sub> designed to tolerate t fail-stop faults, where the source of randomness of Pi<sub>in</sub> is an SV-source, into a protocol Pi<sub>out</sub> that tolerates min(t, n/3) Byzantine faults. If the round-complexity of Pi<sub>in</sub> is r, that of Pi<sub>out</sub> is O(r log* n). Central to our results is the development of a new tool, "audited protocols". Informally "auditing" is a transformation that converts any protocol that assumes built-in broadcast channels into one that achieves a slightly weaker guarantee, without assuming broadcast channels. We regard this as a tool of independent interest, which could potentially find applications in the design of simple and modular randomized distributed algorithms
[Algorithm design and analysis, audited protocols, Protocols, fail-stop fault, Distributed computing, Fault tolerance, Broadcasting, Computer networks, information theory, Cryptography, protocols, fault-tolerant distributed computing, Distributed algorithms, built-in broadcast channel, faulty players, Computational modeling, random-selection protocols, randomised algorithms, full-information networks, randomized distributed algorithms, distributed algorithms, Communication channels, fault tolerant computing, computational complexity, full-information model, Byzantine fault]
Explicit Exclusive Set Systems with Applications to Broadcast Encryption
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
A family of subsets C of [n] =def {1,...,n} is (r, t)-exclusive if for every S sub [n] of size at least n - r, there exist S<sub>1</sub>,...,S<sub>t</sub> isin C with S = S<sub>1</sub>cupS<sub>2</sub>cup...cupS<sub>t</sub>. These families, also known as complement-cover families, have cryptographic applications, and form the basis of information-theoretic broadcast encryption and multi-certificate revocation. We give the first explicit construction of such families with size poly(r, t)nrt/, essentially matching a basic lower bound. Our techniques are algebraic in nature. When r = O(t), as is natural for many applications, we can improve our bound to poly(r, t)(n <sub>r</sub>)1t/. Further, when r, t are small, our construction is tight up to a factor of r. We also provide a poly(r, t, log n) algorithm for finding S<sub>1 </sub>,...,S<sub>t</sub>, which is crucial for efficient use in applications. Previous constructions either had much larger size, were randomized and took super-polynomial time to find S<sub>1</sub>,...,S <sub>t</sub>, or did not work for arbitrary n, r, and t. Finally, we improve the known lower bound on the number of sets containing each i isin [n]. Our bound shows that our derived broadcast encryption schemes have essentially optimal total number of keys and keys per user for n users, transmission size t, and revoked set size r
[cryptography, cryptographic application, set theory, complement-cover families, multicertificate revocation, Algebra, information-theoretic broadcast encryption, Broadcasting, exclusive set systems, Polynomials, Cryptography, Information theory, computational complexity, superpolynomial time]
A simple condition implying rapid mixing of single-site dynamics on spin systems
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Spin systems are a general way to describe local interactions between nodes in a graph. In statistical mechanics, spin systems are often used as a model for physical systems. In computer science, they comprise an important class of families of combinatorial objects, for which approximate counting and sampling algorithms remain an elusive goal. The Dobrushin condition states that every row sum of the "influence matrix" for a spin system is less than 1 - epsiv, where epsiv &gt; 0. This criterion implies rapid convergence (O(n log n) mixing time) of the single-site (Glauber) dynamics for a spin system, as well as uniqueness of the Gibbs measure. The dual criterion that every column sum of the influence matrix is less than 1 - epsiv has also been shown to imply the same conclusions. We examine a common generalization of these conditions, namely that the maximum eigenvalue of the influence matrix is less than 1 epsiv. Our main result is that this criterion implies O(n log n) mixing time for the Glauber dynamics. As applications, we consider the Ising model, hard-core lattice gas model, and graph colorings, relating the mixing time of the Glauber dynamics to the maximum eigenvalue for the adjacency matrix of the graph. For the special case of planar graphs, this leads to improved bounds on mixing time with quite simple proofs
[Gibbs measure, Transmission line matrix methods, Temperature, graph theory, planar graphs, Glauber dynamics, Convergence, influence matrix, Dobrushin condition states, single-site dynamics, rapid mixing, Ising model, Eigenvalues and eigenfunctions, combinatorial objects, H infinity control, local interactions, Time measurement, Helium, hard-core lattice gas model, matrix algebra, Computer science, Upper bound, quantum computing, Sampling methods, spin systems, computational complexity, statistical mechanics]
Fast Algorithms for Logconcave Functions: Sampling, Rounding, Integration and Optimization
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We prove that the hit-and-run random walk is rapidly mixing for an arbitrary logconcave distribution starting from any point in the support. This extends the work of Lovasz and Vempala (2004), where this was shown for an important special case, and settles the main conjecture formulated there. From this result, we derive asymptotically faster algorithms in the general oracle model for sampling, rounding, integration and maximization of logconcave functions, improving or generalizing the main results of Lovasz and Vempala (2003), Applegate and Kannan (1990) and Kalai and Vempala respectively. The algorithms for integration and optimization both use sampling and are surprisingly similar
[Algorithm design and analysis, general oracle model, sampling methods, logconcave distribution, logconcave function integration, H infinity control, random processes, logconcave function sampling, logconcave function rounding, hit-and-run random walk, maximization, statistical distributions, Ellipsoids, optimisation, Gaussian processes, logconcave function optimization, integration, Sampling methods, Polynomials]
A Local Switch Markov Chain on Given Degree Graphs with Application in Connectivity of Peer-to-Peer Networks
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We study a switch Markov chain on regular graphs, where switches are allowed only between links that are at distance 2; we call this the flip. The motivation for studying the flip Markov chain arises in the context of unstructured peer-to-peer networks, which constantly perform such flips in an effort to randomize. We show that the flip Markov chain on regular graphs is rapidly mixing, thus justifying this widely used peer-to-peer networking practice. Our mixing argument uses the Markov chain comparison technique. In particular, we extend this technique to embedding arguments where the compared Markov chains are defined on different state spaces. We give several conditions which generalize our results beyond regular graphs
[peer-to-peer computing, Peer to peer computing, Computational modeling, graph theory, Switches, regular graphs, State-space methods, degree graphs, flip Markov chain, local switch Markov chain, Computer science, Analytical models, Network topology, Simulated annealing, Markov processes, Sampling methods, Bipartite graph, peer-to-peer network connectivity]
Strategic Network Formation through Peering and Service Agreements
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We introduce a game theoretic model of network formation in an effort to understand the complex system of business relationships between various Internet entities (e.g., autonomous systems, enterprise networks, residential customers). This system is at the heart of Internet connectivity. In our model we are given a network topology of nodes and links where the nodes (modeling the various Internet entities) act as the players of the game, and links represent potential contracts. Nodes wish to satisfy their demands, which earn potential revenues, but nodes may have to pay (or be paid by) their neighbors for links incident to them. By incorporating some of the qualities of Internet business relationships, we hope that our model has predictive value. Specifically, we assume that contracts are either customer-provider or peering contracts. As often occurs in practice, we also include a mechanism that penalizes nodes if they drop traffic emanating from one of their customers. For a natural objective function, we prove that the price of stability is at most 2. With respect to social welfare, however, the prices of anarchy and stability can both be unbounded, leading us to consider how much we must perturb the system to obtain good stable solutions. We thus focus on the quality of Nash equilibria achievable through centralized incentives; solutions created by an "altruistic entity" (e.g., the government) able to increase individual payouts for successfully routing a particular demand. We show that if every payout is increased by a factor of 2, then there is a Nash equilibrium as good as the original centrally defined social optimum. We also show how to find equilibria efficiently in multicast trees. Finally, we give a characterization of Nash equilibria as flows of utility with certain constraints, which helps to visualize the structure of stable solutions and provides us with useful proof techniques
[Heart, strategic network formation, service agreements, Stability, Peer to peer computing, trees (mathematics), game theory, peering agreements, Predictive models, telecommunication network topology, network topology, Game theory, altruistic entity, Network topology, multicast trees, Nash equilibria, multicast communication, game theoretic model, Internet business relationships, Traffic control, Internet, IP networks, Contracts, stability]
Towards Secure and Scalable Computation in Peer-to-Peer Networks
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We consider the problems of Byzantine agreement and leader election, where a constant fraction b &lt; 1/3 of processors are controlled by a malicious adversary. The first problem requires that all uncorrupted processors come to an agreement on a bit initially held by one of the uncorrupted processors; the second requires that the uncorrupted processors choose a leader who is uncorrupted. Motivated by the need for robust and scalable computation in peer-to-peer networks, we design the first scalable protocols for these problems for a network whose degree is polylogarithmic in its size. By scalable, we mean that each uncorrupted processor sends and processes a number of bits that is only polylogarithmic in n. (We assume no limit on the number of messages sent by corrupted processors.) With high probability, our Byzantine agreement protocol results in agreement among a 1 - O(1/ln n) fraction of the uncorrupted processors. With constant probability, our leader election protocol elects an uncorrupted leader and ensures that a 1 - O(1/ln n) fraction of the uncorrupt processors know this leader. We assume a full information model. Thus, the adversary is assumed to have unlimited computational power and has access to all communications, but does not have access to processors' private random bits
[Algorithm design and analysis, Protocols, leader election, peer-to-peer computing, Byzantine agreement, Peer to peer computing, peer-to-peer networks, Nominations and elections, Fingerprint recognition, polylogarithmic network, secure computation, Computer science, scalable protocols, Admission control, scalable computation, Computer networks, Robustness, Distributed algorithms, computational complexity]
Lp metrics on the Heisenberg group and the Goemans-Linial conjecture
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We prove that the function d : Ropf3 times Ropf 3 rarr [0,infin] given by d((x,y,z),(t,u,v)) = ([((t-x) 2+(u-y)2)2 + (v-z+2xu-2yt)2] frac12 + (t-x)2 + (u-y)2)frac12 is a metric on Ropf3 such that (Ropf3,radicd) is isometric to a subset of Hilbert space, yet (Ropf3, d) does not admit a bi-Lipschitz embedding into L<sub>1</sub>. This yields a new simple counter example to the Goemans-Linial conjecture on the integrality gap of the semidefinite relaxation of the sparsest cut problem. The metric above is doubling, and hence has a padded stochastic decomposition at every scale. We also study the Lp version of this problem, and obtain a counter example to a natural generalization of a classical theorem of Bretagnolle et al. (1996) (of which the Goemans-Linial conjecture is a particular case). Our methods involve Fourier analytic techniques, and a breakthrough of Cheeger and Kleiner (2006), together with classical results of Pansu (1989) on the differentiability of Lipschitz functions on the Heisenberg group
[Heisenberg group, Stochastic processes, L<sub>p</sub> metrics, integrality gap, Extraterrestrial measurements, Fourier analysis, Hilbert spaces, Complexity theory, sparsest cut problem, Counting circuits, Computer science, Geometry, group theory, semidefinite relaxation, NP-hard problem, padded stochastic decomposition, Lipschitz functions, Approximation algorithms, Hilbert space, Polynomials, stochastic processes, Goemans-Linial conjecture]
Ramsey partitions and proximity data structures
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
This paper addresses the non-linear isomorphic Dvoretzky theorem and the design of good approximate distance oracles for large distortion. We introduce and construct optimal Ramsey partitions, and use them to show that for every epsiv isin (0,1), any n-point metric space has a subset of size n1-epsiv which embeds into Hilbert space with distortion O(1/epsiv). This result is best possible and improves part of the metric Ramsey theorem of Bartal et al. (2005), in addition to considerably simplifying its proof. We use our new Ramsey partitions to design approximate distance oracles with a universal constant query time, closing a gap left open by Thorup and Zwick (2005). Namely, we show that for any n point metric space X, and k ges 1, there exists an O(k)-approximate distance oracle whose storage requirement is O(n1+1k/), and whose query time is a universal constant. We also discuss applications to various other geometric data structures, and the relation to well separated pair decompositions
[proximity data structures, approximate distance oracles, Ramsey partitions, Data structures, Extraterrestrial measurements, Hilbert spaces, Mathematics, nonlinear isomorphic Dvoretzky theorem, Application software, History, Computer science, Nonlinear distortion, Hilbert space, data structures, computational complexity]
Algorithms on negatively curved spaces
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We initiate the study of approximate algorithms on negatively curved spaces. These spaces have become of interest in various domains of computer science including networking and vision. The classical example of such a space is the real-hyperbolic space Hd for d ges 2, but our approach applies to a more general family of spaces characterized by Gromov's (combinatorial) hyperbolic condition. We give efficient algorithms and data structures for problems like approximate nearest-neighbor search and compact, low-stretch routing on subsets of negatively curved spaces of fixed dimension (including Hd as a special case). In a different direction, we show that there is a PTAS for the traveling salesman problem when the set of cities lie, for example, in Hd. This generalizes Arora's results for Ropf d. Most of our algorithms use the intrinsic distance geometry of the data set, and only need the existence of an embedding into some negatively curved space in order to function properly. In other words, our algorithms regard the inter-point distance function as a black box, and are independent of the representation of the input points
[Algorithm design and analysis, traveling salesman problem, interpoint distance function, Traveling salesman problems, Data structures, Extraterrestrial measurements, Routing, Gromov combinatorial hyperbolic condition, negatively curved spaces, Nearest neighbor searches, Computer science, travelling salesman problems, approximate algorithms, Computational geometry, algorithm theory, intrinsic distance geometry, Cities and towns, data structures, geometry, Space exploration]
Beyond Hirsch Conjecture: Walks on Random Polytopes and Smoothed Complexity of the Simplex Method
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Spielman and Teng proved that the shadow-vertex simplex method had polynomial smoothed complexity. On a slight random perturbation of arbitrary linear program, the simplex method finds the solution after a walk on the feasible polytope(s) with expected length polynomial in the number of constraints n, the number of variables d and the inverse standard deviation of the perturbation 1/sigma. We show that the length of walk is actually polylogarithmic in the number of constraints n. We thus improve Spielman-Teng's bound on the walk O*(n86d 55sigma-30) to O(max(d5log2n, d9 log4d, d 3sigma-4)). This in particular shows that the tight Hirsch conjecture n - d on the diameter of polytopes is not a limitation for the smoothed linear programming. Random perturbations create short paths between vertices. We propose a randomized phase-I for solving arbitrary linear programs. Instead of finding a vertex of a feasible set, we add a vertex at random to the feasible set. This does not affect the solution of the linear program with constant probability. So, in expectation it takes a constant number of independent trials until a correct solution is found. This overcomes one of the major difficulties of smoothed analysis of the simplex method - one can now statistically decouple the walk from the smoothed linear program. This yields a much better reduction of the smoothed complexity to a geometric quantity - the size of planar sections of random polytopes. We also improve upon the known estimates for that size
[Algorithm design and analysis, Transmission line matrix methods, Gallium arsenide, Optimization methods, random processes, Probability, Linear programming, Vectors, linear programming, smoothed complexity, random perturbations, Computer science, random polytope walks, Constraint theory, Hirsch conjecture, Polynomials, simplex method, smoothed linear programming, computational complexity]
Improved Approximation Algorithms for Large Matrices via Random Projections
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Several results appeared that show significant reduction in time for matrix multiplication, singular value decomposition as well as linear (lscr<sub>2</sub>) regression, all based on data dependent random sampling. Our key idea is that low dimensional embeddings can be used to eliminate data dependence and provide more versatile, linear time pass efficient matrix computation. Our main contribution is summarized as follows. 1) Independent of the results of Har-Peled and of Deshpande and Vempala, one of the first - and to the best of our knowledge the most efficient - relative error (1 + epsi) parA $A<sub>k</sub>par<sub>F</sub> approximation algorithms for the singular value decomposition of an m times n matrix A with M non-zero entries that requires 2 passes over the data and runs in time O((M(k/epsi+k log k) + (n+m)(k/epsi+k log k)2)log (1/sigma)). 2) The first o(nd2) time (1 + epsi) relative error approximation algorithm for n times d linear (lscr<sub>2</sub>) regression. 3) A matrix multiplication and norm approximation algorithm that easily applies to implicitly given matrices and can be used as a black box probability boosting tool
[Algorithm design and analysis, black box probability boosting tool, Embedded computing, approximation theory, Automation, sampling methods, relative error approximation, random projections, random processes, regression analysis, linear regression, Boosting, Matrix decomposition, Sparse matrices, data dependent random sampling, matrix multiplication, norm approximation, Linear algebra, Approximation algorithms, Sampling methods, large matrices, singular value decomposition, Singular value decomposition, computational complexity]
Worst-case and Smoothed Analysis of the ICP Algorithm, with an Application to the k-means Method
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We show a worst-case lower bound and a smoothed upper bound on the number of iterations performed by the iterative closest point (ICP) algorithm. First proposed by Besl and McKay, the algorithm is widely used in computational geometry where it is known for its simplicity and its observed speed. The theoretical study of ICP was initiated by Ezra, Sharir and Efrat, who bounded its worst-case running time between Omega(n log n) and O(n2d)d. We substantially tighten this gap by improving the lower bound to Omega(n/d)d+1 . To help reconcile this bound with the algorithm's observed speed, we also show the smoothed complexity of ICP is polynomial, independent of the dimensionality of the data. Using similar methods, we improve the best known smoothed upper bound for the popular k-means method to nO(k) once again independent of the dimension
[Algorithm design and analysis, smoothed upper bound, smoothed analysis, ICP Algorithm, iterative closest point algorithm, worst-case analysis, Niobium, Nearest neighbor searches, polynomial complexity, Computational geometry, Upper bound, Iterative closest point algorithm, worst-case running time, worst-case lower bound, Iterative algorithms, Polynomials, Performance analysis, Iterative methods, k-means method, computational complexity]
The Effectiveness of Lloyd-Type Methods for the k-Means Problem
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We investigate variants of Lloyd's heuristic for clustering high dimensional data in an attempt to explain its popularity (a half century after its introduction) among practitioners, and in order to suggest improvements in its application. We propose and justify a clusterability criterion for data sets. We present variants of Lloyd's heuristic that quickly lead to provably near-optimal clustering solutions when applied to well-clusterable instances. This is the first performance guarantee for a variant of Lloyd's heuristic. The provision of a guarantee on output quality does not come at the expense of speed: some of our algorithms are candidates for being faster in practice than currently used variants of Lloyd's method. In addition, our other algorithms are faster on well-clusterable instances than recently proposed approximation algorithms, while maintaining similar guarantees on clustering quality. Our main algorithmic contribution is a novel probabilistic seeding process for the starting configuration of a Lloyd-type iteration
[Technological innovation, near-optimal clustering solutions, probabilistic seeding process, probability, Mathematics, Computer science, k-means problem, high dimensional data clustering, pattern clustering, Lloyd-type methods, Clustering algorithms, Approximation algorithms, Cost function, Sampling methods, clusterability criterion, Polynomials, Performance analysis, Lloyd-type iteration]
Better lossless condensers through derandomized curve samplers
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Lossless condensers are unbalanced expander graphs, with expansion close to optimal. Equivalently, they may be viewed as functions that use a short random seed to map a source on n bits to a source on many fewer bits while preserving all of the min-entropy. It is known how to build lossless condensers when the graphs are slightly unbalanced in the work of M. Capalbo et al. (2002). The highly unbalanced case is also important but the only known construction does not condense the source well. We give explicit constructions of lossless condensers with condensing close to optimal, and using near-optimal seed length. Our main technical contribution is a randomness-efficient method for sampling FD (where F is a field) with low-degree curves. This problem was addressed before in the works of E. Ben-Sasson et al. (2003) and D. Moshkovitz and R. Raz (2006) but the solutions apply only to degree one curves, i.e., lines. Our technique is new and elegant. We use sub-sampling and obtain our curve samplers by composing a sequence of low-degree manifolds, starting with high-dimension, low-degree manifolds and proceeding through lower and lower dimension manifolds with (moderately) growing degrees, until we finish with dimension-one, low-degree manifolds, i.e., curves. The technique may be of independent interest
[graph theory, min-entropy, Routing, Graph theory, short random seed, lossless condensers, Application software, Combinatorial mathematics, Computer science, Fault tolerance, Simultaneous localization and mapping, unbalanced expander graphs, near-optimal seed length, Sampling methods, derandomized curve samplers, Bipartite graph, Error correction codes, low-degree manifolds, subsampling method]
Approximately List-Decoding Direct Product Codes and Uniform Hardness Amplification
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We consider the problem of approximately locally list-decoding direct product codes. For a parameter k, the k-wise direct product encoding of an N-bit message msg is an Nk-length string over the alphabet {0, l}k indexed by k-tuples (i<sub>1</sub>, .. ., i<sub>k</sub>) isin {1,..., N}k so that the symbol at position (i<sub>1</sub>, .. ., i<sub>k</sub>) of the codeword is msg(i <sub>1</sub>)...msg(i<sub>k</sub>). Such codes arise naturally in the context of hardness amplification of Boolean functions via the direct product lemma (and the closely related Yao 's XOR Lemma), where typically k Lt N (e.g., k = poly log N). We describe an efficient randomized algorithm for approximate local list-decoding of direct product codes. Given access to a word which agrees with the k-wise direct product encoding of some message msg in at least an epsiv fraction of positions, our algorithm outputs a list of poly(l/epsiv) Boolean circuits computing N-bit strings (viewed as truth tables of log N-variable Boolean functions) such that at least one of them agrees with msg in at least 1 - delta fraction of positions, for delta = O(k-0.1), provided that epsiv = Omega(poly(l/k); the running time of the algorithm is polynomial in log N and 1/epsiv. When epsiv &gt; epsivkalpha for a certain constant alpha &gt; 0, we get a randomized approximate list-decoding algorithm that runs in time quasi-polynomial in 1/epsiv (i.e., (1/epsiv)poly log 1epsiv/)By concatenating the k-wise direct product codes with Hadamard codes, we obtain locally list-decodable codes over the binary alphabet, which can be efficiently approximately list-decoded from fewer than frac12 - epsiv fraction of corruptions as long as epsiv = Omega(poly(l/k)). As an immediate application, we get uniform hardness amplification for PNP <sub>par</sub>, the class of languages reducible to NP through one round of parallel oracle queries: If there is a language in PNP <sub>par</sub> that cannot be decided by any BPP algorithm on more that 1 $1/nOmega(1) fraction of inputs, then there is another language in PNP <sub>par</sub> that cannot be decided by any BPP algorithm on more than frac12 + 1/nomega(1) fraction of inputs
[Product codes, Circuits, Encoding, Decoding, randomized algorithm, Code standards, Computational complexity, product codes, randomised algorithms, uniform hardness amplification, Boolean functions, direct product lemma, binary alphabet, Polynomials, Error correction codes, Error correction, list-decoding direct product codes, Boolean circuits, Hadamard codes, computational complexity, randomized approximate list-decoding algorithm]
Index Coding with Side Information
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Motivated by a problem of transmitting data over broadcast channels (BirkandKol, INFOCOM1998), we study the following coding problem: a sender communicates with n receivers R<sub>l</sub>,.., R<sub>n</sub>. He holds an input x isin {0, 1}<sub>n</sub> and wishes to broadcast a single message so that each receiver R<sub>i</sub> can recover the bit x<sub>i</sub>. Each R<sub>i</sub> has prior side information about x, induced by a directed graph G on n nodes; R<sub>i </sub> knows the bits of x in the positions {j | (i, j) is anedge of G}. We call encoding schemes that achieve this goal INDEX codes for {0, 1} n with side information graph G. In this paper we identify a measure on graphs, the minrank, which we conjecture to exactly characterize the minimum length of INDEX codes. We resolve the conjecture for certain natural classes of graphs. For arbitrary graphs, we show that the minrank bound is tight for both linear codes and certain classes of non-linear codes. For the general problem, we obtain a (weaker) lower bound that the length of an INDEX code for any graph G is at least the size of the maximum acyclic induced subgraph of G
[Coaxial cables, minrank bound, linear codes, maximum acyclic induced subgraph, Source coding, Satellite broadcasting, side information, Length measurement, Entropy, Linear code, Filters, directed graphs, directed graph, Position measurement, index coding, broadcast channels, nonlinear codes, Information theory]
Subspace Polynomials and List Decoding of Reed-Solomon Codes
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We show combinatorial limitations on efficient list decoding of Reed-Solomon codes beyond the Johnson and Guruswami-Sudan bounds in the works of S.M. Johnson (1962, 1963) and V. Guruswami and M. Sudan (1999). In particular, we show that for arbitrarily large fields F<sub>N</sub>, |F<sub>N</sub>| - N, for any delta isin (0,1), and K = Ndelta; : middot Existence: there exists a received word w<sub>N</sub> : F<sub>N</sub> rarr F<sub>N</sub> that agrees with a super-polynomial number of distinct degree K polynomials on ap Nradicdelta  points each; middot Explicit: there exists a polynomial time constructible received word w'<sub>N</sub> : F<sub>N</sub> rarr F<sub>N</sub> that agrees with a super-polynomial number of distinct degree K polynomials, on ap 2radic(log N) K points each. In both cases, our results improve upon the previous state of the art, which was ap Ndelta/delta for the existence case in the work J. Justesen and T. Hoboldt (2001), and ap 2Ndelta for the explicit one in the work of V. Guruswami and M. Sudan (2005). Furthermore, for delta close to 1 our bound approaches the Guruswami-Sudan bound (which is radicNK) and implies limitations on extending their efficient RS list decoding algorithm to larger decoding radius. Our proof method is surprisingly simple. We work with polynomials that vanish on subspaces of an extension field viewed as a vector space over the base field. These sub-space polynomials are a subclass of linearized polynomials that were first studied by O. Ore (1933, 1934) in the 1930s, and later by coding theorists. For us their main attraction is their sparsity and abundance of roots, virtues that recently won them pivotal roles in probabilistically checkable proofs of proximity in the works of E. Ben-Sasson et al. (2004) and E. Ben-Sasson and M. Sudan (2005) and sub-linear proof verification in the work of E. Ben-Sasson et al. (2005)
[Career development, polynomials, Decoding, linearized polynomials, decoding, Reed-Solomon codes, Computer science, subspace polynomials, polynomial time constructible received word, Councils, list decoding, Computer errors, Polynomials]
SDP gaps and UGC-hardness for MAXCUTGAIN
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Given a graph with maximum cut of (fractional) size c, the Goemans-Williamson semidefinite programming (SDP) algorithm by M. Goemans and D. Williamson (1995) is guaranteed to find a cut of size .878 middot c. However this guarantee becomes trivial when c is near frac12, since a random cut has expected size frac12. Recently, M. Charikar and K. Worth (2004) (analyzing an algorithm of U. Feige and G. Langberg (2001)) showed that given a graph with maximum cut frac12 + epsiv, one can find a cut of size frac12 + Omega(epsiv/ log(1/epsiv)). The main contribution of our paper is twofold: 1. We give a natural frac12 + epsiv vs. frac12 + O(epsiv/ log(1/epsiv)) SDP gap for MAXCUT in Gaussian space. This shows that the SDP-rounding algorithm of Charikar-Worth is essentially best possible. Further, the "s-linear rounding functions" used in the works of M. Charikar and K. Worth (2004) and U. Freige and M. Langberg (2001) arise as optimizers in our analysis, somewhat confirming a suggestion of U. Freige and M. Langberg (2001). 2. We show how this SDP gap can be translated into a long code test with the same parameters. This implies that beating the Charikar-Worth guarantee with any efficient algorithm is NP-hard, assuming the unique games conjecture (UGC) by S. Khot (2002). We view this result as essentially settling the approximability of MAXCUT, assuming UGC. Building on (1) we show how "randomness reduction" on related SDP gaps for the QUADRATICPROGRAMMING programming problem lets us make the Omega(log(1/epsiv)) gap as large as Omega(log n) for n-vertex graphs. In addition to optimally answering an open question of N. Alen et al. (2006), this technique may prove useful for other SDP gap problems. Finally, illustrating the generality of our technique in (2), we also show how to translate Reeds's SDP gap by J. Reeds (1993) for the Grothendieck Inequality into a UGC-hardness result for computing the par middot par<sub>infin rarr 1</sub> norm of a matrix
[Algorithm design and analysis, long code test, UGC-hardness, User-generated content, graph theory, semidefinite programming algorithm, random cut, Gaussian space, Partitioning algorithms, Linear matrix inequalities, Computational complexity, quadratic programming, Computer science, MAXCUTGAIN, NP-hardness, unique games conjecture, QUADRATICPROGRAMMING programming problem, Approximation algorithms, SDP-rounding algorithm, SDP gaps, Testing, computational complexity]
Correlated Algebraic-Geometric Codes: Improved List Decoding over Bounded Alphabets
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We define a new family of error-correcting codes based on algebraic curves over finite fields, and develop efficient list decoding algorithms for them. Our codes extend the class of algebraic-geometric (AG) codes via a (non-obvious) generalization of the approach in the recent breakthrough work of F. Parvaresh and A. Vardy (2005). Our work shows that the PV framework applies to fairly general settings by elucidating the key algebraic concepts underlying it. Also, more importantly, AG codes of arbitrary block length exist over fixed alphabets Sigma, thus enabling us to establish new trade-offs between the list decoding radius and rate over a bounded alphabet size. Similar to algorithms for AG codes from V. Guruswami and M. Sudan (1999, 2001), our encoding/decoding algorithms run in polynomial time assuming a natural polynomial-size representation of the code. For codes based on a specific "optimal" algebraic curve, we also present an expected polynomial time algorithm to construct the requisite representation. This in turn fills an important void in the literature by presenting an efficient construction of the representation often assumed in the list decoding algorithms for AG codes
[error correction codes, Redundancy, decoding algorithms, Length measurement, Decoding, Galois fields, polynomial time algorithm, decoding, Computer science, Reed-Solomon codes, error-correcting codes, algebraic curves, finite fields, correlated algebraic-geometric codes, algebraic geometric codes, encoding algorithms, Computer errors, bounded alphabets, Error correction, Error correction codes, list decoding radius, polynomial-size code representation, Information theory, computational complexity]
Cryptography from Anonymity
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
There is a vast body of work on implementing anonymous communication. In this paper, we study the possibility of using anonymous communication as a building block, and show that one can leverage on anonymity in a variety of cryptographic contexts. Our results go in two directions. middot Feasibility. We show that anonymous communication over insecure channels can be used to implement unconditionally secure point-to-point channels, broadcast, and general multi-party protocols that remain unconditionally secure as long as less than half of the players are maliciously corrupted. middot Efficiency. We show that anonymous channels can yield substantial efficiency improvements for several natural secure computation tasks. In particular, we present the first solution to the problem of private information retrieval (PIR) which can handle multiple users while being close to optimal with respect to both communication and computation
[Context, Costs, private information retrieval, Information retrieval, cryptography, anonymous communication, Distributed computing, secure computation, Cryptographic protocols, Computer science, Privacy, Information security, Broadcasting, Cryptography, insecure channels]
Secure Multiparty Quantum Computation with (Only) a Strict Honest Majority
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Secret sharing and multiparty computation (also called "secure function evaluation") are fundamental primitives in modern cryptography, allowing a group of mutually distrustful players to perform correct, distributed computations under the sole assumption that some number of them will follow the protocol honestly. This paper investigates how much trust is necessary $that is, how many players must remain honest - in order for distributed quantum computations to be possible. We present a verifiable quantum secret sharing (VQSS) protocol, and a general secure multiparty quantum computation (MPQC) protocol, which can tolerate any cheaters among n players. Previous protocols for these tasks tolerated lfloor (n - 1)/4 rfloor and lfloor (n - 1)/6 rfloor cheaters, respectively. The threshold we achieve is tight - even in the classical case, "fair" multiparty computation is not possible if any set of n/2 players can cheat. Our protocols rely on approximate quantum error-correcting codes, which can tolerate a larger fraction of errors than traditional, exact codes. We introduce new families of authentication schemes and approximate codes tailored to the needs of our protocols, as well as new state purification techniques along the lines of those used in fault-tolerant quantum circuits
[Performance evaluation, approximate quantum error-correcting codes, Purification, error correction codes, authentication schemes, Circuits, cryptography, verifiable quantum secret sharing protocol, Distributed computing, fault-tolerant quantum circuits, Cryptographic protocols, distributed quantum computations, Fault tolerance, Quantum computing, Authentication, quantum computing, quantum cryptography, mutually distrustful players, strict honest majority, secure multiparty quantum computation protocol, Error correction codes, Cryptography, protocols]
Settling the Complexity of Two-Player Nash Equilibrium
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Even though many people thought the problem of finding Nash equilibria is hard in general, and it has been proven so for games among three or more players recently, it's not clear whether the two-player case can be shown in the same class of PPAD-complete problems. We prove that the problem of finding a Nash equilibrium in a two-player game is PPAD-complete
[decision theory, Optimization methods, game theory, two-player Nash equilibrium complexity, Nash equilibrium, Linear programming, Application software, Game theory, Ellipsoids, Computer science, Sampling methods, two-player games, Polynomials, PPAD-complete problems, Mathematical model, computational complexity]
Minimum Bounded Degree Spanning Trees
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We consider the minimum cost spanning tree problem under the restriction that all degrees must be at most a given value k. We show that we can efficiently find a spanning tree of maximum degree at most k+2 whose cost is at most the cost of the optimum spanning tree of maximum degree at most k. This is almost best possible. The approach uses a sequence of simple algebraic, polyhedral and combinatorial arguments. It illustrates many techniques and ideas in combinatorial optimization as it involves polyhedral characterizations, uncrossing, matroid intersection, and graph orientations (or packing of spanning trees). The result generalizes to the setting where every vertex has both upper and lower bounds and gives then a spanning tree which violates the bounds by at most two units and whose cost is at most the cost of the optimum tree. It also gives a better understanding of the subtour relaxation for both the symmetric and asymmetric traveling salesman problems. The generalization to l-edge-connected subgraphs is briefly discussed
[Greedy algorithms, minimum cost spanning tree, trees (mathematics), matroid intersection, Traveling salesman problems, minimum bounded degree spanning trees, algebra, polyhedral arguments, Graphics, combinatorial optimization, optimisation, Tree graphs, polyhedral characterizations, edge connected subgraphs, graph orientations, Prototypes, algebraic arguments, Cost function, Polynomials]
Approximate Min-Max Theorems of Steiner Rooted-Orientations of Hypergraphs
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Given an undirected hypergraph and a subset of vertices S sube V with a specified root vertex r isin S, the Steiner rooted-orientation problem is to find an orientation of all the hyperedges so that in the resulting directed hypergraph the "connectivity" from the root r to the vertices in S is maximized. This is motivated by a multicasting problem in undirected networks as well as a generalization of some classical problems in graph theory. The main results of this paper are the following approximate min-max relations: middot Given an undirected hypergraph H, if S is 2k-hyperedge-connected in H, then H has a Steiner rooted k-hyperarc-connected orientation. middot Given an undirected graph G, if S is 2k-element-connected in G, then G has a Steiner rooted k-element-connected orientation. Both results are tight in terms of the connectivity bounds. These also give polynomial time constant factor approximation algorithms for both problems. The proofs are based on submodular techniques, and a graph decomposition technique used in the Steiner tree packing problem. Some complementary hardness results are presented at the end
[Steiner trees, graph theory, graph decomposition, Graph theory, minimax techniques, Steiner hypergraph rooted-orientations, Computer science, directed hypergraph, Tree graphs, approximate min-max theorems, Steiner tree packing problem, Tail, polynomial time constant factor approximation, Approximation algorithms, Polynomials, computational complexity, undirected hypergraph]
Improved bounds for online routing and packing via a primal-dual approach
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
In this work we study a wide range of online and offline routing and packing problems with various objectives. We provide a unified approach, based on a clean primal-dual method, for the design of online algorithms for these problems, as well as improved bounds on the competitive factor. In particular, our analysis uses weak duality rather than a tailor made (i.e., problem specific) potential function. We demonstrate our ideas and results in the context of routing problems. Using our primal-dual approach, we develop a new generic online routing algorithm that outperforms previous algorithms suggested earlier by Y. Azar et al. (1993, 1997). We then show the applicability of our generic algorithm to various models and provide improved algorithms for achieving coordinate-wise competitiveness, maximizing throughput, and minimizing maximum load. In particular, we improve the results obtained by A. Goel et al. (2001) by an O(log n) factor for the problem of achieving coordinate-wise competitiveness, and by an O(log log n) factor for the problem of maximizing the throughput. For some of the settings we also prove improved lower bounds. We believe our results further our understanding of the applicability of the primal-dual method to online algorithms, and we are confident that the method will prove useful to other online scenarios. Finally, we revisit the notions of coordinate-wise and prefix competitiveness in an offline setting. We design the first polynomial time algorithm that computes an almost optimal coordinate-wise routing for several routing models. We also revisit previously studied routing models by A. Kumar and J.M. Kleinberg (2000) and A. Goel and A. Meyerson (2005) and prove tight lower and upper bounds of Theta(log n) on prefix competitiveness for these models
[Algorithm design and analysis, offline packing, Design methodology, competitive algorithms, Routing, Throughput, offline routing, polynomial time algorithm, primal-dual approach, Computer science, Upper bound, online packing, Bandwidth, online routing, Polynomials, Joining processes, computational complexity]
Improved Dynamic Planar Point Location
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We develop the first linear-space data structures for dynamic planar point location in general subdivisions that achieve logarithmic query time and poly-logarithmic update time
[Tree data structures, Computational modeling, Scholarships, logarithmic query time, Data structures, Spatial databases, Electronic mail, linear-space data structures, Computer science, Graphics, Computational geometry, polylogarithmic update time, improved dynamic planar point location, data structures, Slabs, computational complexity]
Coresets forWeighted Facilities and Their Applications
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We develop efficient (1 + epsiv)-approximation algorithms for generalized facility location problems. Such facilities are not restricted to being points in Ropf, and can represent more complex structures such as linear facilities (lines in Ropfd, j-dimensional flats), etc. We introduce coresets for weighted (point) facilities. These prove to be useful for such generalized facility location problems, and provide efficient algorithms for their construction. Applications include: k-mean and k-median generalizations, i.e., find k lines that minimize the sum (or sum of squares) of the distances from each input point to its nearest line. Other applications are generalizations of linear regression problems to multiple regression lines, new SVD/PCA generalizations, and many more. The results significantly improve on previous work, which deals efficiently only with special cases. Open source code for the algorithms in this paper is also available
[weighted facilities, SVD, Image processing, coresets, Data compression, regression analysis, linear regression, set theory, facility location, Impedance matching, Computer graphics, approximation algorithm, singular value decomposition, Computer vision, approximation theory, Linear regression, k-mean generalization, Application software, k-median generalization, facility location problem, PCA, Computer science, Approximation algorithms, principal component analysis, Principal component analysis]
Planar Point Location in Sublogarithmic Time
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We consider the static planar point location problem in an arbitrary polygonal subdivision given by n segments. We assume points come from the [u]2 grid, and consider algorithms for the RAM with words of O(lg u) bits. We give the first solution to the problem which can surpass the traditional query time of O(lgn). Specifically, we can obtain a query time of O(radic(lg u)). Though computational geometry on a grid has been investigated for a long time (including for this problem), it is generally not known how to make good use of a bounded universe in problems of such nonorthogonal flavor. Our result shows this limitation can be surpassed, at least for planar point location. A result by Timothy Chan, appearing independently in FOCS'06, also achieves sublogarithmic query times. Combining the two results, we obtain the following bound. For any S ges 2, the exists a data structure using space O(n middot S) which supports queries in time: O(min {((lg n)/(lg lg n)), (radic((lg u)/(lg lg u))), ((lg u)/(lg S))})
[Buildings, arbitrary polygonal subdivision, planar point location, data structure, computational geometry, Data structures, Search problems, sublogarithmic time, Sorting, Computer science, Computational geometry, Polynomials, data structures]
Point Location in o(log n) Time, Voronoi Diagrams in o(n log n) Time, and Other Transdichotomous Results in Computational Geometry
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Given n points in the plane with integer coordinates bounded by U les 2w, we show that the Voronoi diagram can be constructed in O(min {n log n/ log log n, n(radic(log U)}) expected time by a randomized algorithm on the unit-cost RAM with word size w. Similar results are also obtained for many other fundamental problems in computational geometry, such as constructing the convex hull of a 3-dimensional point set, computing the Euclidean minimum spanning tree of a planar point set, triangulating a polygon with holes, and finding intersections among a set of line segments. These are the first results to beat the Omega(n log n) algebraic-decision-tree lower bounds known for these problems. The results are all derived from a new two-dimensional version of fusion trees that can answer point location queries in O(min { log n / log log n, radic(log U)}) time with linear space. Higher-dimensional extensions and applications are also mentioned in the paper
[Costs, Voronoi diagrams, Computational modeling, Read-write memory, computational geometry, Data structures, algebra, set theory, randomized algorithm, Sorting, randomised algorithms, Computer science, Computational geometry, Tree graphs, Euclidean distance, decision trees, point location query, Polynomials, Euclidean minimum spanning tree, planar point set, algebraic decision tree]
Concurrent Non-Malleable Zero Knowledge
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We provide the first construction of a concurrent and non-malleable zero knowledge argument for every language in NP. We stress that our construction is in the plain model with no common random string, trusted parties, or super-polynomial simulation. That is, we construct a zero knowledge protocol Pi such that for every polynomial-time adversary that can adaptively and concurrently schedule polynomially many executions of Pi, and corrupt some of the verifiers and some of the provers in these sessions, there is a polynomial-time simulator that can simulate a transcript of the entire execution, along with the witnesses for all statements proven by a corrupt prover to an honest verifier Our security model is the traditional model for concurrent zero knowledge, where the statements to be proven by the honest provers are fixed in advance and do not depend on the previous history (but can be correlated with each other); corrupted provers, of course, can chose the statements adaptively. We also prove that there exists some functionality F (a combination of zero knowledge and oblivious transfer) such that it is impossible to obtain a concurrent non-malleable protocol for F in this model. Previous impossibility results for composable protocols ruled out existence of protocols for a wider class of functionalities {including zero knowledge!) but only if these protocols were required to remain secure when executed concurrently with arbitrarily chosen different protocols (Lindell, FOCS 2003) or if these protocols were required to remain secure when the honest parties' inputs in each execution are chosen adaptively based on the results of previous executions (Lindell, TCC2004). We obtain an Otilde(n) -round protocol under the assumption that one-to-one one-way functions exist. This can be improved to Otilde(k log n) rounds under the assumption that there exist k-round statistically hiding commitment schemes. Our protocol is a black-box zero knowledge protocol
[statistically hiding commitment, polynomial-time simulation, Computational modeling, Computer simulation, concurrent non-malleable zero knowledge, cryptography, Security, History, Stress, Cryptographic protocols, Computer science, security model, NP problem, concurrent nonmalleable protocol, Polynomials, black-box zero knowledge protocol, statistical analysis, computational complexity]
Succinct Non-Interactive Zero-Knowledge Proofs with Preprocessing for LOGSNP
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Let Lambda : {0, 1}n times {0,1}m rarr {0,1} be a Boolean formula of size d, or more generally, an arithmetic circuit of degree d, known to both Alice and Bob, and let y isin {0,1} m be an input known only to Alice. Assume that Alice and Bob interacted in the past in a preamble phase (that is, applied a preamble protocol that depends only on the parameters, and not on Lambday). We show that Alice can (non-interactively) commit to y, by a message of size poly(m, log d), and later on prove to Bob any N statements of the form Lambda (x<sub>1</sub>, y) = z<sub>1</sub>,..., Lambda(x<sub>N </sub>,y) = z<sub>N</sub> by a (computationally sound) non-interactive zero-knowledge proof of size poly(d, log N). (Note the logarithmic dependence on N). We give many applications and motivations for this result. In particular, assuming that Alice and Bob applied in the past the (poly-logarithmic size) preamble protocol: 1. given a CNF formula Psi(w<sub>1</sub>,..., w<sub>m</sub>) of size N, Alice can prove the satisfiability of Psi by a (computationally sound) non-interactive zero-knowledge proof of size poly(m). That is, the size of the proof depends only on the size of the witness and not on the size of the formula. 2. Given a language L in the class LOGSNP and an input x isin {0, 1}n, Alice can prove the membership x isin L by a (computationally sound) non-interactive zero-knowledge proof of size polylog n. 3. Alice can commit to a Boolean formula y of size m, by a message of size poly(m), and later on prove to Bob any N statements of the form y(x<sub>1</sub>) = z<sub>1</sub>,..., y(x<sub>N</sub>) = z<sub>N</sub> by a (computationally sound) non-interactive zero-knowledge proof of size poly(m, log N). Our cryptographic assumptions include the existence of a poly-logarithmic symmetric-private-information-retrieval (SPIR) scheme, as defined in (C. Cachin et. al, 1999), and the existence of commitment schemes, secure against circuits of size exponential in the security parameter
[Protocols, Circuits, Data preprocessing, information retrieval, computability, cryptography, Security, Radio access networks, Computer science, LOGSNP preprocessing, Boolean functions, preamble protocol, satisfiability, Boolean formula, CNF formula, noninteractive zero-knowledge proofs, Polynomials, Cryptography, Arithmetic, poly-logarithmic symmetric-private-information-retrieval]
Input-Indistinguishable Computation
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We put forward a first definition of general secure computation that, without any trusted set-up, handles an arbitrary number of concurrent executions; and is implementable based on standard complexity assumptions. In contrast to previous definitions of secure computation, ours is not simulation-based
[input-indistinguishable computation, Computational modeling, Access protocols, concurrent execution, concurrency theory, Security, secure computation, Concurrent computing, Computer science, Privacy, Processor scheduling, security of data, Veins, complexity assumption, Public key, Internet, computational complexity]
Generalization of Binary Search: Searching in Trees and Forest-Like Partial Orders
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We extend the binary search technique to searching in trees. We consider two models of queries: questions about vertices and questions about edges. We present a general approach to this sort of problem, and apply it to both cases, achieving algorithms constructing optimal decision trees. In the edge query model the problem is identical to the problem of searching in a special class of tree-like posets stated by Ben-Asher et al. (1999). Our upper bound on computation time, O(n3 ), improves the previous best known O(n4 log3n). In the vertex query model we show how to compute an optimal strategy much faster, in O(n) steps. We also present an almost optimal approximation algorithm for another class of tree-like (and forest-like) partial orders
[binary search, questions about edges, edge query model, approximation theory, File servers, tree searching, Computer science, query processing, Upper bound, questions about vertices, File systems, forest-like partial orders, computation time, decision trees, Approximation algorithms, optimal decision trees, vertex query model, Decision trees, Books, optimal approximation algorithm, Testing, computational complexity]
Lower Bounds for Additive Spanners, Emulators, and More
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
An additive spanner of an unweighted undirected graph G with distortion d is a subgraph H such that for any two vertices u,v isin G, we have delta<sub>H</sub>(u, v) les delta<sub>G</sub>(u, v) + d. For every k = O((ln n)/( ln ln n)), we construct a graph G on vertices for which any additive spanner of G with distortion 2k - 1 has Omega((1/k)n1 + 1k/) edges. This matches the lower bound previously known only to hold under a 1963 conjecture of Erdos. We generalize our lower bound in a number of ways. First, we consider graph emulators introduced by Dor, Halperin, and Zwick (FOCS, 1996), where an emulator of an unweighted undirected graph G with distortion d is like an additive spanner except H may be an arbitrary weighted graph such that delta<sub>G</sub>(u, v) ges delta<sub>G</sub>(u, v) ges delta<sub>G</sub>(u, v) + d. We show a lower bound of Omega((1/k2)n1 + 1k/) edges for distortion- (2k - 1) emulators. These are the first non-trivial bounds for k &gt; 3. Second, we parameterize our bounds in terms of the minimum degree of the graph. Namely, for minimum degree n1k + c/ for any c ges 0, we prove a bound of Omega((1/k)n1 + 1k - c(1 + 2/(k - 1))/) for additive spanners and Omega((1/k2)n1 + 1k - c(1 + 2/(k - 1))/) for emulators. For k = 2 these can be improved to OmegaQ(n(3/2) - c). This partially answers a question of Baswana et al. (SODA, 2005) for additive spanners. Finally, we continue the study of pair-wise and source-wise distance preservers defined by Coppersmith and Elkin (SODA, 2005) by considering their approximate variants and their relaxation to emulators. We prove the first lower bounds for such graphs
[source-wise distance preserver, pair-wise distance preserver, Upper bound, graph theory, Routing protocols, IP networks, graph emulator, Distributed algorithms, additive spanners, unweighted undirected graph, lower bounds, computational complexity]
Solving Evacuation Problems Efficiently--Earliest Arrival Flows with Multiple Sources
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Earliest arrival flows capture the essence of evacuation planning. Given a network with capacities and transit times on the arcs, a subset of source nodes with supplies and a sink node, the task is to send the given supplies from the sources to the sink "as quickly as possible". The latter requirement is made more precise by the earliest arrival property which requires that the total amount of flow that has arrived at the sink is maximal for all points in time simultaneously. It is a classical result from the 1970s that, for the special case of a single source node, earliest arrival flows do exist and can be computed by essentially applying the successive shortest path algorithm for min-cost flow computations. While it has previously been observed that an earliest arrival flow still exists for multiple sources, the problem of computing one efficiently has been open for many years. We present an exact algorithm for this problem whose running time is strongly polynomial in the input plus output size of the problem
[shortest path algorithm, Costs, earliest arrival flows, graph theory, evacuation problem, Floods, evacuation planning, network flow algorithm, Fires, polynomial, Tail, Resists, Complex networks, Polynomials, min-cost flow computation, Joining processes, computational complexity]
New Limits on Fault-Tolerant Quantum Computation
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We show that quantum circuits cannot be made fault-tolerant against a depolarizing noise level of thetas = (6 - 2radic2)/7 ap 45%, thereby improving on a previous bound of 50% (due to Razborov, 2004). More precisely, the circuit model for which we prove this bound contains perfect gates from the Clifford group (CNOT, Hadamard, S, X, Y, Z) and arbitrary additional one-qubit gates that are subject to depolarizing noise thetas. We prove that this set of gates cannot be universal for arbitrary (even classical) computation, from which the upper bound on the noise threshold for fault-tolerant quantum computation follows
[quantum circuits, Computational modeling, quantum gates, Circuit faults, Clifford group, Noise level, Fault tolerance, Quantum computing, Upper bound, fault-tolerant quantum computation, Physics computing, integrated circuit modelling, Computer errors, circuit model, Error correction codes, Circuit noise, one-qubit gates]
Postselection threshold against biased noise
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
The highest current estimates for the amount of noise a quantum computer can tolerate are based on fault-tolerance schemes relying heavily on postselecting on no detected errors. However, there has been no proof that these schemes give even a positive tolerable noise threshold. A technique to prove a positive threshold, for probabilistic noise models, is presented. The main idea is to maintain strong control over the distribution of errors in the quantum state at all times. This distribution has correlations which conceivably could grow out of control with postselection. But in fact, the error distribution can be written as a mixture of nearby distributions each satisfying strong independence properties, so there are no correlations for postselection to amplify
[quantum computer, current estimate, Error analysis, fault tolerance, Bit error rate, Probability distribution, error distribution, postselection threshold, Distributed computing, probabilistic noise, Computer science, Fault tolerance, biased noise, Quantum computing, quantum computing, Computer errors, quantum noise, Error correction codes, Error correction]
On the Quantum Query Complexity of Local Search in Two and Three Dimensions
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
The quantum query complexity of searching for local optima has been a subject of much interest in the recent literature. For the d-dimensional grid graphs, the complexity has been determined asymptotically for all fixed d ges 5, but the lower dimensional cases present special difficulties, and considerable gaps exist in our knowledge. In the present paper we present near-optimal lower bounds, showing that the quantum query complexity for the 2-dimensional grid [n] 2 is Omega(nfrac12 - delta), and that for the 3-dimensional grid [n]3 is Omega(n1 - delta), for any fixed delta &gt; 0. A general lower bound approach for this problem, initiated by Aaronson (2004) (based on Ambainis' adversary method (2003) for quantum lower bounds), uses random walks with low collision probabilities. This approach encounters obstacles in deriving tight lower bounds in low dimensions due to the lack of degrees of freedom in such spaces. We solve this problem by the novel construction and analysis of random walks with non-uniform step lengths. The proof employs in a nontrivial way sophisticated results of Sarkozy and Szemeridi (1965), Bose and Chowla (1962-63), and Halasz (1977) from combinatorial number theory, as well as less familiar probability tools like Esseen's inequality
[near-optimal lower bound, graph theory, quantum query complexity, d-dimensional grid graph, probability, Sun, local search, Computer science, Quantum computing, quantum computing, probability tool, Computer errors, Hypercubes, Polynomials, Esseen inequality, Decision trees, search problems, Testing, combinatorial number theory]
On the time complexity of 2-tag systems and small universal Turing machines
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We show that 2-tag systems efficiently simulate Turing machines. As a corollary we find that the small universal Turing machines of Rogozhin, Minsky and others simulate Turing machines in polynomial time. This is an exponential improvement on the previously known simulation time overhead and improves a forty year old result in the area of small universal Turing machines
[Computational modeling, Computer simulation, 2-tag system, time complexity, Educational institutions, Mathematics, Computational complexity, Computer science, Turing machines, Councils, universal Turing machine, Polynomials, polynomial time, simulation time overhead, Informatics, computational complexity]
On the Optimality of the Dimensionality Reduction Method
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We investigate the optimality of (1+epsi)-approximation algorithms obtained via the dimensionality reduction method. We show that: any data structure for the (1 + epsi)-approximate nearest neighbor problem in Hamming space, which uses constant number of probes to answer each query, must use nOmega(1/epsi2) space; any algorithm for the (1 + epsi)-approximate closest substring problem must run in time exponential in 1/epsi2 - gamma for any gamma &gt; 0 (unless 3SAT can be solved in sub-exponential time). Both lower bounds are (essentially) tight
[Algorithm design and analysis, time exponential, approximation theory, Design methodology, data structure, Hamming space, Data structures, Nearest neighbor searches, dimensionality reduction, nearest neighbor, Clustering algorithms, approximation algorithm, Frequency, Polynomials, Concrete, data structures, substring problem, Probes, Pattern analysis]
Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice
[Lattices, Data compression, Data structures, Spatial databases, Decoding, Data mining, bounded-distance decoder, Nearest neighbor searches, Image databases, near-optimal hashing algorithms, d-dimensional Euclidean space, Leech lattice, c-approximate nearest neighbor, Approximation algorithms, file organisation, Probes, computational complexity]
Points on Computable Curves
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
The "analyst's traveling salesman theorem" of geometric measure theory characterizes those subsets of Euclidean space that are contained in curves of finite length. This result, proven for the plane by Jones (1990) and extended to higher-dimensional Euclidean spaces by Okikiolu (1992), says that a bounded set K is contained in some curve of finite length if and only if a certain "square beta sum\
[computable curves, Terminology, geometric measure theory, Jones constriction computability, Traveling salesman problems, Length measurement, computability, computational geometry, traveling salesman theorem, Orbital robotics, Computer science, Robot motion, travelling salesman problems, Quantum computing, Tiles, Euclidean space, Space exploration, Lifting equipment]
Local Graph Partitioning using PageRank Vectors
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
A local graph partitioning algorithm finds a cut near a specified starting vertex, with a running time that depends largely on the size of the small side of the cut, rather than the size of the input graph. In this paper, we present a local partitioning algorithm using a variation of PageRank with a specified starting distribution. We derive a mixing result for PageRank vectors similar to that for random walks, and show that the ordering of the vertices produced by a PageRank vector reveals a cut with small conductance. In particular, we show that for any set C with conductance Phi and volume k, a PageRank vector with a certain starting distribution can be used to produce a set with conductance (O(radic(Phi log k)). We present an improved algorithm for computing approximate PageRank vectors, which allows us to find such a set in time proportional to its size. In particular, we can find a cut with conductance at most oslash, whose small side has volume at least 2b in time O(2 log m/(2b log2 m/oslash2) where m is the number of edges in the graph. By combining small sets found by this local partitioning algorithm, we obtain a cut with conductance oslash and approximately optimal balance in time O(m log4 m/oslash)
[Algorithm design and analysis, Linear systems, local graph partitioning, approximation theory, random walks, Computational modeling, graph theory, random processes, Educational institutions, Vectors, Mathematics, Probability distribution, Partitioning algorithms, PageRank vectors approximation, Analytical models, Clustering algorithms]
Norm of the inverse of a random matrix
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Let A be an n times n matrix, whose entries are independent copies of a centered random variable satisfying the subGaussian tail estimate. We prove that the operator norm of' A-1 does not exceed Cn3/2 with probability close to 1. In a geometric language, this bounds the probability that the affine span of n random vectors in Ropfn with i.i.d. subGaussian coordinates comes close to the origin
[Algorithm design and analysis, estimation theory, subGaussian tail estimation, probability, geometric language, random processes, random matrix inverse, Mathematics, Functional analysis, matrix algebra, Tail, Gaussian processes, Polynomials, geometry, Random variables]
Witnesses for non-satisfiability of dense random 3CNF formulas
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We consider random 3CNF formulas with n variables and m clauses. It is well known that when m &gt; cn (for a sufficiently large constant c), most formulas are not satisfiable. However, it is not known whether such formulas are likely to have polynomial size witnesses that certify that they are not satisfiable. A value of m sime n3/2 was the forefront of our knowledge in this respect. When m &gt; cn3/2 , such witnesses are known to exist, based on spectral techniques. When m &lt; n3/2-epsi, it is known that resolution (which is a common approach for refutation) cannot produce witnesses of size smaller than 2nepsiv. Likewise, it is known that certain variants of the spectral techniques do not work in this range. In the current paper we show that when m &gt; cn7/5, almost all 3CNF formulas have polynomial size witnesses for non-satisfiability. We also show that such a witness can be found in time 2(O(n0.2 log n)), whenever it exists. Our approach is based on an extension of the known spectral techniques, and involves analyzing a certain fractional packing problem for random 3-uniform hypergraphs
[Algorithm design and analysis, polynomial size witnesses, nonsatisfiability, dense random 3CNF formulas, spectral techniques, graph theory, random processes, computability, NP-complete problem, Computer science, random 3-uniform hypergraphs, fractional packing problem, Polynomials, Eigenvalues and eigenfunctions, computational complexity]
Accidental Algorthims
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We provide evidence for the proposition that the computational complexity of individual problems, and of whole complexity classes, hinge on the existence of certain solvable polynomial systems that are unlikely to be encountered other than by systematic explorations for them. We consider a minimalist version of Cook's 3CNF problem, namely that of monotone planar 3 CNF formulae where each variable occurs twice. We show that counting the number of solutions of these modulo 2 is oplusP-complete (hence NP-hard) but counting them modulo 7 is polynomial time computable (sic). We also show a similar dichotomy for a vertex cover problem. To derive completeness results we use a new holographic technique for proving completeness results in oplusP for problems that are in P. For example, we can show in this way that oplus2CNF, the parity problem for 2CNF, is oplusP-complete. To derive efficient algorithms we use computer algebra systems to find appropriate holographic gates. In order to explore the limits of holographic techniques we define the notion of an elementary matchgrid algorithm to capture a natural but restricted use of them. We show that for the NP-complete general 3 CNF problem no such elementary matchgrid algorithm can exist. We observe, however, that it remains open for many natural #P-complete problems whether such elementary matchgrid algorithms exist, and for the general CNF problem whether non-elementary matchgrid algorithms exist
[solvable polynomial systems, NP-hard, parity problem, elementary matchgrid algorithm, Holography, Fasteners, computability, #P-complete problem, Mathematics, NP-complete, accidental algorithms, Algebra, vertex cover problem, CNF problem, Polynomials, Bipartite graph, Encoding, Computational complexity, Equations, Computer science, polynomial time computable, process algebra, holographic technique, computer algebra systems, computational complexity]
The Kesten-Stigum Reconstruction Bound Is Tight for Roughly Symmetric Binary Channels
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We establish the exact threshold for the reconstruction problem for a binary asymmetric channel on the b-ary tree, provided that the asymmetry is sufficiently small. This is the first exact reconstruction threshold obtained in roughly a decade. We discuss the implications of our result for Glauber dynamics, phylogenetic reconstruction, noisy communication and the so-called "replica symmetry breaking" in spin glasses and random satisfiability problems
[replica symmetry breaking, random satisfiability problem, Engineering profession, Stochastic processes, trees (mathematics), Glass, noisy communication, computability, Phylogeny, Glauber dynamics, Statistics, phylogenetic reconstruction, binary asymmetric channel, spin glasses, Noise level, Kesten-Stigum reconstruction, b-ary tree, reconstruction threshold, Computer applications, Markov processes, Eigenvalues and eigenfunctions, information theory, Communication networks]
Algebraic Structures and Algorithms for Matching and Matroid Problems
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We present new algebraic approaches for several well-known combinatorial problems, including non-bipartite matching, matroid intersection, and some of their generalizations. Our work yields new randomized algorithms that are the most efficient known. For non-bipartite matching, we obtain a simple, purely algebraic algorithm with running time O(nw) where n is the number of vertices and w is the matrix multiplication exponent. This resolves the central open problem of Mucha and Sankowski (2004). For matroid intersection, our algorithm has running time O(nrw - 1)for matroids with n elements and rank r that satisfy some natural conditions. This algorithm is based on new algebraic results characterizing the size of a maximum intersection in contracted matroids. Furthermore, the running time of this algorithm is essentially optimal
[Symmetric matrices, pattern matching, combinatorial mathematics, nonbipartite matching, algebraic algorithm, matroid intersection, Data structures, Graph theory, Partitioning algorithms, randomized algorithm, Combinatorial mathematics, randomised algorithms, Computer science, Concurrent computing, combinatorial problems, matrix multiplication, Approximation algorithms, Polynomials, Bipartite graph]
Hardness of Learning Halfspaces with Noise
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Learning an unknown halfspace (also called a perceptron) from, labeled examples is one of the classic problems in machine learning. In the noise-free case, when a half-space consistent with all the training examples exists, the problem can be solved in polynomial time using linear programming. However, under the promise that a halfspace consistent with a fraction (1 - epsiv) of the examples exists (for some small constant epsiv &gt; 0), it was not known how to efficiently find a halfspace that is correct on even 51% of the examples. Nor was a hardness result that ruled out getting agreement on more than 99.9% of the examples known. In this work, we close this gap in our understanding, and prove that even a tiny amount of worst-case noise makes the problem of learning halfspaces intractable in a strong sense. Specifically, for arbitrary epsiv,delta &gt; 0, we prove that given a set of examples-label pairs from the hypercube a fraction (1 - epsiv) of which can be explained by a halfspace, it is NP-hard to find a halfspace that correctly labels a fraction (frac12 + delta) of the examples. The hardness result is tight since it is trivial to get agreement on frac12 the examples. In learning theory parlance, we prove that weak proper agnostic learning of halfspaces is hard. This settles a question that was raised by Blum et. al in their work on learning halfspaces in the presence of random classification noise (A. Blum et. al, 1996), and in some more recent works as well. Along the way, we also obtain a strong hardness for another basic computational problem: solving a linear system over the rationals
[Linear systems, Occupational stress, halfspace learning, perceptrons, perceptron, Linear programming, linear programming, linear systems, machine learning, agnostic learning, Equations, Computer science, NP-hardness, linear system, Veins, Machine learning, Hypercubes, Polynomials, polynomial time, learning (artificial intelligence), Testing, computational complexity]
Cryptographic Hardness for Learning Intersections of Halfspaces
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We give the first representation-independent hardness results for PAC learning intersections of halfspaces, a central concept class in computational learning theory. Our hardness results are derived from two public-key cryptosystems due to Regev, which are based on the worst-case hardness of well-studied lattice problems. Specifically, we prove that a polynomial-time algorithm for PAC learning intersections of nepsi  halfspaces (for a constant epsi &gt; 0) in n dimensions would yield a polynomial-time solution to Otilde(n1.5)-uSVP (unique shortest vector problem). We also prove that PAC learning intersections of nepsi low-weight half-spaces would yield a polynomial-time quantum solution to Otilde(n1.5)-SVP and Otilde(n1.5)-SIVP (shortest vector problem and shortest independent vector problem, respectively). By making stronger assumptions about the hardness of uSVP, SVP, and SIVP, we show that there is no polynomial-time algorithm for learning intersections of log c n halfspaces in n dimensions, for c &gt; 0 sufficiently large. Our approach also yields the first representation-independent hardness results for learning polynomial-size depth-2 neural networks and polynomial-size depth-3 arithmetic circuits
[shortest independent vector problem, public-key cryptosystem, halfspace, Circuits, Lattices, lattice problem, Computer science, Learning, Boolean functions, vectors, public key cryptography, Neural networks, Quantum mechanics, PAC learning intersection, unique shortest vector problem, Public key cryptography, cryptographic hardness, computational learning, Polynomials, learning (artificial intelligence), polynomial-time, Arithmetic, computational complexity]
New Results for Learning Noisy Parities and Halfspaces
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We address well-studied problems concerning the learn-ability of parities and halfspaces in the presence of classification noise. Learning of parities under the uniform distribution with random classification noise, also called the noisy parity problem is a famous open problem in computational learning. We reduce a number of basic problems regarding learning under the uniform distribution to learning of noisy parities. We show that under the uniform distribution, learning parities with adversarial classification noise reduces to learning parities with random classification noise. Together with the parity learning algorithm of Blum et al. (2003), this gives the first nontrivial algorithm for learning parities with adversarial noise. We show that learning of DNF expressions reduces to learning noisy parities of just logarithmic number of variables. We show that learning of k-juntas reduces to learning noisy parities of k variables. These reductions work even in the presence of random classification noise in the original DNF or junta. We then consider the problem of learning halfspaces over Qopfn with adversarial noise or finding a halfspace that maximizes the agreement rate with a given set of examples. We prove an essentially optimal hardness factor of 2 - epsi, improving the factor of (85/84) - epsi due to Bshouty and Burroughs (2002). Finally, we show that majorities of halfspaces are hard to PAC-learn using any representation, based on the cryptographic assumption underlying the Ajtai-Dwork cryptosystem
[pattern classification, Noise reduction, cryptography, Decoding, Noise generators, Ajtai-Dwork cryptosystem, Distributed computing, uniform distribution, classification noise, Computer science, cryptographic assumption, Linear code, Boolean functions, halfspaces, Gaussian noise, noisy parities learning, computational learning, k-juntas, Error correction, Error correction codes, Cryptography, learning (artificial intelligence), computational complexity]
Inclusion--Exclusion Algorithms for Counting Set Partitions
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Given a set U with n elements and a family of subsets S sube 2 U we show how to count the number of k-partitions S<sub>1 </sub> cup ... cup S<sub>k</sub> = U into subsets S<sub>i</sub> isin S in time 2nnO(1). The only assumption on S is that it can be enumerated in time 2nnO(1). In effect we get exact algorithms in time 2nnO(1) for several well-studied partition problems including domatic number, chromatic number, bounded component spanning forest, partition into Hamiltonian subgraphs, and bin packing. If only polynomial space is available, our algorithms run in time 3nnO(1) if membership in S can be decided in polynomial time. For chromatic number, we present a version that runs in time O(2.2461n) and polynomial space. For domatic number, we present a version that runs in time O(2.8718n). Finally, we present a family of polynomial space approximation algorithms that find a number between chi(G) and [(1 + epsi)chi(G)] in time O(1.2209n + 2.2461e-epsin)
[approximation theory, polynomial space approximation, inclusion-exclusion algorithm, chromatic number, Partitioning algorithms, set theory, graph colouring, bin packing, Computer science, domatic number, bounded component spanning forest, Hamiltonian subgraph, Approximation algorithms, Polynomials, counting set partition, Dynamic programming, computational complexity]
An O*(2^n ) Algorithm for Graph Coloring and Other Partitioning Problems via Inclusion--Exclusion
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We use the principle of inclusion and exclusion, combined with polynomial time segmentation and fast Mobius transform, to solve the generic problem of summing or optimizing over the partitions of n elements into a given number of weighted subsets. This problem subsumes various classical graph partitioning problems, such as graph coloring, domatic partitioning, and max k-cut, as well as machine learning problems like decision graph learning and model-based data clustering. Our algorithm runs in O*(2n) time, thus substantially improving on the usual O*(3n)-time dynamic programming algorithm; the notation O* suppresses factors polynomial in n. This result improves, e.g., Byskov's recent record for graph coloring from O*(2.4023n) to O*(2n). We note that twenty five years ago, R. M. Karp used inclusion-exclusion in a similar fashion to reduce the space requirement of the usual dynamic programming algorithms from exponential to polynomial
[Machine learning algorithms, Heuristic algorithms, dynamic programming, transforms, graph coloring, Partitioning algorithms, machine learning, graph colouring, Computer science, Upper bound, fast Mobius transform, inclusion-exclusion, dynamic programming algorithm, polynomial time segmentation, Clustering algorithms, Machine learning, Polynomials, Dynamic programming, graph partitioning, computational complexity]
Faster Algorithms for Approximate Distance Oracles and All-Pairs Small Stretch Paths
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Let G = (V, E) be a weighted undirected graph with |V| = n and |E| = m. An estimate deltacirc(u,v) of the distance delta(u,v) in G between u,v isin V is said to be of stretch t iff delta(u,v) les deltacirc(u,v) les t middot delta(u,v). The most efficient algorithms known for computing small stretch distances in G are the approximate distance oracles of (M. Thorup and U. Zwick, 2005) and the three algorithms in (E. Cohen and U. Zwick, 2001) to compute all-pairs stretch t distances for t = 2, 7/3, and 3. We present faster algorithms for these problems. For any integer k ges 1, Thorup and Zwick (2005) gave an O(kmn1k/) algorithm to construct a data structure of size O(kn1 + 1k/) which, given a query (u,v) isin V times V, returns in O(k) time, a 2k - 1 stretch estimate of delta(u, v). But for small values of k, the time to construct the oracle is rather high. Here we present an O(n2 log n) algorithm to construct such a data structure of size O(kn1+1k/) for all integers k ges 2. Our query answering time is O(k) for k &gt; 2 and Theta (log n) for k = 2. We use a new generic scheme for all-pairs approximate shortest paths for these results. This scheme also enables us to design faster algorithms for all-pairs t-stretch distances for t = 2 and 7/3, and compute all-pairs almost stretch 2 distances in O(n2 log n) time
[Algorithm design and analysis, Upper bound, graph theory, all-pairs small stretch path, Data structures, query answering time, all-pairs approximate shortest path, weighted undirected graph, approximate distance oracle, computational complexity]
Computing Nash Equilibria: Approximation and Smoothed Complexity
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
The authors advance significantly beyond the recent progress on the algorithmic complexity of Nash equilibria by solving two major open problems in the approximation of Nash equilibria and in the smoothed analysis of algorithms. (1) The authors show that no algorithm with complexity poly(n, 1/epsi) can compute an epsi-approximate Nash equilibrium in a two-player game, in which each player has n pure strategies, unless PPAD sube P. In other words, the problem of computing a Nash equilibrium in a two-player game does not have a fully polynomial-time approximation scheme unless PPAD sube P. (2) The authors prove that no algorithm for computing a Nash equilibrium in a two-player game can have smoothed complexity poly(n, 1/sigma) under input perturbation of magnitude sigma, unless PPAD sube RP. In particular, the smoothed complexity of the classic Lemke-Howson algorithm is not polynomial unless PPAD sube RP. Instrumental to our proof, we introduce a new discrete fixed-point problem on a high-dimensional hypergrid with constant side-length, and show that it can host the embedding of the proof structure of any PPAD problem. We prove a key geometric lemma for finding a discrete fixed-point, a new concept defined on n + 1 vertices of a unit hypercube. This lemma enables us to overcome the curse of dimensionality in reasoning about fixed-points in high dimensions
[Algorithm design and analysis, Embedded computing, approximation theory, Instruments, Nash equilibria approximation, game theory, Nash equilibrium, Linear programming, Search problems, smoothed complexity, Lemke-Howson algorithm, Computer science, algorithmic complexity, proof structure, Approximation algorithms, Hypercubes, Polynomials, geometric lemma, computational complexity]
On the Impact of Combinatorial Structure on Congestion Games
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We study the impact of combinatorial structure in congestion games on the complexity of computing pure Nash equilibria and the convergence time of best response sequences. In particular, we investigate which properties of the strategy spaces of individual players ensure a polynomial convergence time. We show, if the strategy space of each player consists of the bases of a matroid over the set of resources, then the lengths of all best response sequences are polynomially bounded in the number of players and resources. We can also prove that this result is tight, that is, the matroid property is a necessary and sufficient condition on the players' strategy spaces for guaranteeing polynomial time convergence to a Nash equilibrium. In addition, we present an approach that enables us to devise hardness proofs for various kinds of combinatorial games, including first results about the hardness of market sharing games and congestion games for overlay network design. Our approach also yields a short proof for the PLS-completeness of network congestion games. In particular, we can show that network congestion games are PLS-complete for directed and undirected networks even in case of linear latency functions
[Costs, combinatorial mathematics, convergence, game theory, linear latency function, Nash equilibrium, Search problems, overlay network design, combinatorial structure, Delay, Convergence, polynomial convergence time, Computer science, Sufficient conditions, resource allocation, Nash equilibria, network congestion games, Polynomials, Resource management, Time factors, matroid]
Balanced Allocations of Cake
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We give a randomized algorithm for the well known caking cutting problem that achieves approximate fairness, and has complexity O(n), when all players are honest. The heart of this result involves extending the standard offline multiple-choice balls and bins analysis to the case where the underlying resources/bins/machines have different utilities to different players/balls/jobs
[Kirk field collapse effect, Heart, Algorithm design and analysis, approximation theory, Protocols, caking cutting problem, game theory, Mathematics, randomized algorithm, bin packing, fair approximation, randomised algorithms, Computer science, bins analysis, offline multiple-choice balls analysis, Books, computational complexity]
On a Geometric Generalization of the Upper Bound Theorem
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Up to the factor of 2, the result generalizes McMullen's upper bound theorem for convex polytopes (the case lscr = 0) and extends a theorem of Linhart for the case d les 4. Moreover, the bound sharpens asymptotic estimates obtained by Clarkson and Shor. The proof is based on the h-matrix of the arrangement (a generalization, introduced by Mulmuley, of the h-vector of a convex polytope). We show that bounding appropriate sums of entries of this matrix reduces to a lemma about quadrupels of sets with certain intersection properties, and we prove this lemma, up to a factor of 2, using tools from multilinear algebra. This extends an approach of Alon and Kalai, who used linear algebra methods for an alternative proof of the classical upper bound theorem. The bounds for the entries of the h-matrix also imply bounds for the number of i-dimensional faces, i &gt; 0, at level at most lscr. Furthermore, we discuss a connection with crossing numbers of graphs that was one of the main motivations for investigating exact bounds that are valid for arbitrary dimensions
[estimation theory, convex polytopes, graph theory, geometric generalization, computational geometry, Mathematics, multilinear algebra, upper bound theorem, Linhart theorem, Stress, matrix algebra, Computational geometry, Upper bound, graphs, Linear algebra, asymptotic estimation]
Higher Lower Bounds for Near-Neighbor and Further Rich Problems
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We convert cell-probe lower bounds for polynomial space into stronger lower bound for near-linear space. Our technique applies to any lower bound proved through the richness method. For example, it applies to partial match, and to near-neighbor problems, either for randomized exact search, or for deterministic approximate search (which are thought to exhibit the curse of dimensionality). These problems are motivated by search in large data bases, so near-linear space is the most relevant regime. Typically, richness has been used to imply Omega(d/lg n) lower bounds for polynomial-space data structures, where d is the number of bits of a query. This is the highest lower bound provable through the classic reduction to communication complexity. However, for space n lg O(1)n, we now obtain bounds of Omega(d/ lg d). This is a significant improvement for natural values of d, such as lgO(1) n. In the most important case of d = Theta (lg n), we have the first superconstant lower bound. From a complexity-theoretic perspective, our lower bounds are the highest known for any static data-structure problem, significantly improving on previous records
[Protocols, polynomials, polynomial-space data structures, Data structures, further rich problems, Complexity theory, near-neighbor problem, communication complexity, cell-probe lower bounds, Nearest neighbor searches, Computer science, Computational geometry, Databases, near-linear space, Polynomials, data structures, Probes]
Planar Earthmover is not in L_1
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We show that any L<sub>1</sub> embedding of the transportation cost (a.k.a. Earthmover) metric on probability measures supported on the grid {0,1,..., n}2 sube Ropf2 incurs distortion Omega(radic;(log n)). We also use Fourier analytic techniques to construct a simple L<sub>1</sub> embedding of this space which has distortion O(log n)
[Computer vision, Transportation, probability, computational geometry, Extraterrestrial measurements, Fourier analysis, Probability distribution, transportation cost metric, Nearest neighbor searches, Computer science, Fourier analytic technique, Computer graphics, Cost function, Distortion measurement, Labeling, planar Earthmover, computational complexity]
Approximation algorithms for allocation problems: Improving the factor of 1 - 1/e
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Combinatorial allocation problems require allocating items to players in a way that maximizes the total utility. Two such problems received attention recently, and were addressed using the same linear programming (LP) relaxation. In the maximum submodular welfare (SMW) problem, utility functions of players are submodular, and for this case Dobzinski and Schapira [SODA 2006] showed an approximation ratio of 1 - 1/e. In the generalized assignment problem (GAP) utility functions are linear but players also have capacity constraints. GAP admits a (1 - 1/e)-approximation as well, as shown by Fleischer, Goemans, Mirrokni and Sviridenko [SODA 2006]. In both cases, the approximation ratio was in fact shown for a more general version of the problem, for which improving 1 - 1/e is NP-hard. In this paper, we show how to improve the 1 - 1/e approximation ratio, both for SMW and for GAP. A common theme in both improvements is the use of a new and optimal fair contention resolution technique. However, each of the improvements involves a different rounding procedure for the above mentioned LP. In addition, we prove APX-hardness results for SMW (such results were known for GAP). An important feature of our hardness results is that they apply even in very restricted settings, e.g. when every player has nonzero utility only for a constant number of items
[approximation theory, NP-hard, combinatorial mathematics, game theory, Linear programming, linear programming, maximum submodular welfare, fair contention resolution, utility functions, APX-hardness, combinatorial allocation problem, generalized assignment problem, Computer science, Linearity, Tin, approximation algorithm, Approximation algorithms, Polynomials, computational complexity, utility theory]
Approximation Algorithms for Non-Uniform Buy-at-Bulk Network Design
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We consider approximation algorithms for non-uniform buy-at-bulk network design problems. The first non-trivial approximation algorithm for this problem is due to Charikar and Karagiozova (STOC 05); for an instance on h pairs their algorithm has an approximation guarantee of exp(O(radic(log h log log h)))for the uniform-demand case, and log D middot exp(O(radic(log h log log h))) for the general demand case, where D is the total demand. We improve upon this result, by presenting the first poly-logarithmic approximation for this problem. The ratio we obtain is O(log3 h middot min{log D, gamma(h2)}) where his the number of pairs and gamma(n) is the worst case distortion in embedding the metric induced by a n vertex graph into a distribution over its spanning trees. Using the best known upper bound on gamma(n) we obtain an O(min{log3 h middot log D, log5 h log log h}) ratio approximation. We also give poly-logarithmic approximations for some variants of the single-source problem that we need for the multicommodity problem
[Algorithm design and analysis, approximation theory, poly-logarithmic approximation, buy-at-bulk network design, vertex graph, Hydrogen, trees (mathematics), network theory (graphs), spanning trees, Cables, Computer science, Upper bound, Network topology, Tree graphs, Bandwidth, Approximation algorithms, Cost function, multicommodity problem]
How to Play Unique Games Using Embeddings
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
In this paper we present a new approximation algorithm for unique games. For a unique game with n vertices and k states (labels), if a (1 - epsiv) fraction of all constraints is satisfiable, the algorithm finds an assignment satisfying a 1 - O(epsiv radic(log n log k)) fraction of all constraints. To this end, we introduce new embedding techniques for rounding semidefinite relaxations of problems with large domain size
[Computer science, semidefinite relaxation, approximation theory, Engineering profession, relaxation theory, game theory, approximation algorithm, embedding technique, Approximation algorithms, Linear programming, Equations, unique games]
Improved approximation algorithms for multidimensional bin packing problems
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
In this paper we introduce a new general framework for set covering problems, based on the combination of randomized rounding of the (near-)optimal solution of the linear programming (LP) relaxation, leading to a partial integer solution, and the application of a well-behaved approximation algorithm to complete this solution. If the value of the solution returned by the latter can be bounded in a suitable way, as is the case for the most relevant generalizations of bin packing, the method leads to improved approximation guarantees, along with a proof of tighter integrality gaps for the LP relaxation. Applying our general framework we obtain a polynomial-time randomized algorithm for d-dimensional vector packing with approximation guarantee arbitrarily close to ln d + 1. For d = 2, this value is 1.693 ..., i.e., we break the natural 2 "barrier" for this case. Moreover, for small values of d this is a notable improvement over the previously-known O(ln d) guarantee by Chekuri and Khanna (2004). For 2-dimensional bin packing with and without rotations, we construct algorithms with performance guarantee arbitrarily close to 1.525..., improving upon previous algorithms with performance guarantee of 2 + epsiv by Jansen and Zhang (2004) for the problem with rotations and1.691... by Caprara (2002) for the problem without rotations. The previously-unknown key property used in our proofs follows from a retrospective analysis of the implications of the landmark bin packing approximation scheme by Fernandez de la Vega and Lueker (1981). We prove that their approximation scheme is "subset oblivious\
[Strips, Operations research, geometric 2-dimensional knapsack, linear programming, set theory, bin packing, Design optimization, vector packing, relaxation theory, approximation algorithm, linear programming relaxation, Polynomials, Metals industry, bin packing problem, approximation theory, Multidimensional systems, set covering problem, Linear programming, Steel, randomised algorithms, Approximation algorithms, Clothing industry, geometry, polynomial-time randomized algorithm, computational complexity]
Lower bounds for circuits with MOD_m gates
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
Let CC<sub>o(n)</sub>[m] be the class of circuits that have size o(n) and in which all gates are MOD[m] gates. We show that CC [m] circuits cannot compute MOD<sub>q</sub> in sub-linear size when m, q &gt; 1 are co-prime integers. No non-trivial lower bounds were known before on the size of CC [m] circuits of constant depth for computing MOD<sub>q</sub>. On the other hand, our results show circuits of type MAJ o CC<sub>o(n)</sub>[m] need exponential size to compute MOD<sub>q </sub>. Using Bourgain's recent breakthrough result on estimates of exponential sums, we extend our bound to the case where small fan-in AND gates are allowed at the bottom of such circuits i.e. circuits of type MAJ o CC[m] o AND <sub>epsiv log n</sub>, where epsiv &gt; 0 is a sufficiently small constant. CC [m] circuits of constant depth need superlinear number of wires to compute both the AND and MOD<sub>q</sub> functions. To prove this, we show that any circuit computing such functions has a certain connectivity property that is similar to that of superconcentration. We show a superlinear lower bound on the number of edges of such graphs extending results on superconcentrators
[estimation theory, Circuit simulation, MOD gates, graph theory, AND gates, lower bounds, graph, Computer science, Boolean functions, Wires, Polynomials, exponential sum estimation, Impedance, CC circuits, computational complexity]
On the Compressibility of NP Instances and Cryptographic Applications
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
We initiate the study of compression that preserves the solution to an instance of a problem rather than preserving the instance itself. Our focus is on the compressibility of NP decision problems. We consider NP problems that have long instances but relatively short witnesses. The question is, can one efficiently compress an instance and store a shorter representation that maintains the information of whether the original input is in the language or not. We want the length of the compressed instance to be polynomial in the length of the witness rather than the length of original input. Such compression enables to succinctly store instances until a future setting will allow solving them, either via a technological or algorithmic breakthrough or simply until enough time has elapsed. We give a new classification of NP with respect to compression. This classification forms a stratification of NP that we call the VC hierarchy. The hierarchy is based on a new type of reduction called W-reduction and there are compression-complete problems for each class. Our motivation for studying this issue stems from the vast cryptographic implications compressibility has. For example, we say that SAT is compressible if there exists a polynomial p(middot, middot) so that given a formula consisting of m clauses over n variables it is possible to come up with an equivalent (w.r.t satisfiability) formula of size at most p(n, log m). Then given a compression algorithm for SAT we provide a construction of collision resistant hash functions from any one-way function. This task was shown to be impossible via black-box reductions (D. Simon, 1998), and indeed the construction presented is inherently non-black-box. Another application of SAT compressibility is a cryptanalytic result concerning the limitation of everlasting security in the bounded storage model when mixed with (time) complexity based cryptography. In addition, we study an approach to constructing an oblivious transfer protocol from any one-way function. This approach is based on compression for SAT that also has a property that we call witness retrievability. However, we mange to prove severe limitations on the ability to achieve witness retrievable compression of SAT
[data compression, pattern classification, Virtual colonoscopy, decision theory, transfer protocol, SAT, computability, time complexity, cryptography, NP classification, Application software, Security, compression-complete problem, Compression algorithms, Secure storage, Cryptographic protocols, Computer science, hash function, non-black-box reductions, NP decision problems, Approximation algorithms, Polynomials, Cryptography, computational complexity]
Dispersion of Mass and the Complexity of Randomized Geometric Algorithms
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
How much can randomness help computation? Motivated by this general question and by volume computation, one of the few instances where randomness provably helps, we analyze a notion of dispersion and connect it to asymptotic convex geometry. We obtain a nearly quadratic lower bound on the complexity of randomized volume algorithms for convex bodies in Ropfn (the current best algorithm has complexity roughly n4, conjectured to be n3). Our main tools, dispersion of random determinants and dispersion of the length of a random point from a convex body, are of independent interest and applicable more generally; in particular, the latter is closely related to the variance hypothesis from convex geometry. This geometric dispersion also leads to lower bounds for matrix problems and property testing
[Algorithm design and analysis, randomized geometric algorithm complexity, Additives, computational geometry, Complexity theory, randomized volume algorithm, Computational complexity, Convergence, randomised algorithms, Computer science, Computational geometry, Sampling methods, Polynomials, mass dispersion, asymptotic convex geometry, Testing, computational complexity]
An \\Omega(n^1/3 ) Lower Bound for Bilinear Group Based Private Information Retrieval
2006 47th Annual IEEE Symposium on Foundations of Computer Science
None
2006
A two server private information retrieval (PIR) scheme allows a user U to retrieve the i-th bit of an n-bit string x replicated between two servers while each server individually learns no information about i. The main parameter of interest in a PIR scheme is its communication complexity, namely the number of bits exchanged by the user and the servers. A large amount of effort has been invested by researchers over the last decade in search for efficient PIR schemes. A number of different schemes ((B. Chor. O. Goldreich. E. Kushilevitz. and M. Sudan, 1998), (A. Beimel and Y. Ishai, 2001) ,(D. Woodruff and S. Yekhanin, 2005)) have been proposed, however all of them ended up with the same communication complexity of O(n1/3). The best known lower bound to date is 5 log n by (S. Wehner and R. de Wolf, 2005) . The tremendous gap between upper and lower bounds is the focus of our paper. We show an Omega(n1/3) lower bound in a restricted model that nevertheless captures all known upper bound techniques. Our lower bound applies to bilinear group based PIR schemes. A bilinear PIR scheme is a one round PIR scheme, where user computes the dot product of servers' responses to obtain the desired value of the i-th bit. Every linear scheme can be turned into a bilinear one with an asymptotically negligible communication overhead. A group based PIR scheme is a PIR scheme that involves servers representing database by a function on a certain finite group G, and allows user to retrieve the value of this function at any group element using the natural secret sharing scheme based on G. Our proof relies on representation theory of finite groups
[finite group, Protocols, private information retrieval, information retrieval, Information retrieval, Complexity theory, Decoding, Indexes, communication complexity, bilinear group, group theory, Privacy, Upper bound, Databases, representation theory, Robustness, natural secret sharing, Cryptography]
Forward
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Presents the introductory welcome message from the conference proceedings.
[]
Conference organization
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Provides a listing of current committee members and society officers.
[]
Structure and Randomness in Combinatorics
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Combinatorics, like computer science, often has to deal with large objects of unspecified (or unusable) structure. One powerful way to deal with such an arbitrary object is to decompose it into more usable components. In particular, it has proven profitable to decompose such objects into a structured component, a pseudo-random component, and a small component (i.e. an error term): in many cases it is the structured component which then dominates. We illustrate this philosophy in a number of model cases.
[Computer science, combinatorial mathematics, pseudo-random component, Hilbert space, Entropy, Polynomials, error term, Bipartite graph, Combinatorial mathematics, Statistics, combinatorics]
Spectral Graph Theory and its Applications
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Spectral graph theory is the study of the eigenvalues and eigenvectors of matrices associated with graphs. In this tutorial, we will try to provide some intuition as to why these eigenvectors and eigenvalues have combinatorial significance, and will sitn'ey some of their applications.
[Laplace equations, eigenvalues, graph theory, Graph theory, Application software, Combinatorial mathematics, eigenvalues and eigenfunctions, matrix algebra, spectral graph theory, Computer science, Image segmentation, eigenvectors, Eigenvalues and eigenfunctions, Physics education, Books, Testing]
Pseudorandom Bits for Polynomials
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We present a new approach to constructing pseudorandom generators that fool low-degree polynomials over finite fields, based on the Gowers norm. Using this approach, we obtain the following main constructions of explicitly computable generators G : FsrarrFn that fool polynomials over a prime field F: (1) a generator that fools degree-2 (i.e., quadratic) polynomials to within error 1/n, with seed length s = O(log n); (2) a generator that fools degree-3 (i.e., cubic) polynomials to within error epsiv, with seed length s = O(Iog<sub>|F|</sub> n) + f(epsiv, F) where f depends only on epsiv and F (not on n), (3) assuming the "Gowers inverse conjecture," for every d a generator that fools degree-d polynomials to within error epsiv, with seed length, s = O(dldrIog<sub>|F|</sub> n) + f(d, epsiv, F) where f depends only on d, epsiv, and F (not on n). We stress that the results in (1) and (2) are unconditional, i.e. do not rely on any unproven assumption. Moreover, the results in (3) rely on a special case of the conjecture which may be easier to prove. Our generator for degree-d polynomials is the component-wise sum of d generators for degree-l polynomials (on independent seeds). Prior to our work, generators with logarithmic seed length were only known for degree-1 (i.e., linear) polynomials (Naor and Naor; SIAM J. Comput., 1993). In fact, over small fields such as F<sub>2</sub> = {0,1}, our results constitute the first progress on these problems since the long-standing generator by Luby, Velickovic and Wigderson (ISTCS1993), whose seed length is much bigger: s = exp (Omega(radiclogn)), even for the case of degree-2 polynomials over F<sub>2</sub>.
[Algorithm design and analysis, Gowers norm, polynomials, Complexity theory, Gowers inverse conjecture, Application software, random number generation, Galois fields, Stress, Computer science, low-degree polynomial, finite field, Polynomials, Cryptography, Books, pseudorandom bit generator, Testing, computational complexity]
Extractors and Rank Extractors for Polynomial Sources
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
In this paper we construct explicit deterministic extractors from polynomial sources, namely from distributions sampled by low degree multivariate polynomials over finite fields. This naturally generalizes previous work on extraction from affine sources. A direct consequence is a deterministic extractor for distributions sampled by polynomial size arithmetic circuits over exponentially large fields. The first step towards extraction is a construction o/rank extractors, which are polynomial mappings that "extract" the algebraic rank from any system of low degree polynomials. More precisely, for any n polynomials, k of which are algebraically independent, a rank extractor outputs k algebraically independent polynomials of slightly higher degree. A result of Wooley allows us to relate algebraic rank and min-entropy and to show that a rank extractor is also a high quality condenser for polynomial sources over polynomially large fields. Finally, to turn this condenser into an extractor, we employ a theorem of Bombieri, giving a character sum estimate for polynomials defined over curves. It allows extracting all the randomness (up to a multiplicative constant) from polynomial sources over exponentially large fields.
[polynomials, Circuits, random processes, min-entropy, algebraic rank, Entropy, Galois fields, Distributed computing, rank extractors, Computer science, polynomial size arithmetic circuits, random process, low degree multivariate polynomial, polynomial sources, Polynomials, Random variables, Error correction, deterministic extractor, Arithmetic, minimum entropy methods]
Polylogarithmic Independence Can Fool DNF Formulas
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We show that any k-wise independent probability measure on {0, 1}n can O(m2ldr2ldr2-radick/10)-fool any boolean function computable by an rn-clauses DNF (or CNF) formula on n variables. Thus, for each constant c &gt; 0. there is a constant e &gt; 0 such that any boolean function computable by an m-clauses DNF (or CNF) formula can be in m-e-fooled by any clog in-wise probability measure. This resolves, asymptotically and up to a logm factor, the depth-2 circuits case of a conjecture due to Linial and Nisan (1990). The result is equivalent to a new characterization of DNF (or CNF) formulas by low degree polynomials. It implies a similar statement for probability measures with the small bias property. Using known explicit constructions of small probability spaces having the limited independence property or the small bias property, we. directly obtain a large class of explicit PRG's ofO(log2 m log n)-seed length for m-clauses DNF (or CNF) formulas on n variables, improving previously known seed lengths.
[polylogarithmic independence, polynomials, boolean function, probability, Probability, Harmonic analysis, Size measurement, k-wise independent probability, Machinery, Counting circuits, Computer science, Boolean functions, disjunctive normal form, polynomial, DNF formula, Polynomials, Random variables]
Derandomization of Sparse Cyclotomic Integer Zero Testing
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
The zero testing and sign determination problems of real algebraic numbers of high extension degree are important in computational complexity and numerical analysis. In this paper we concentrate an sparse cyclotomic integers. Given an integer n and a sparse polynomial f(x) = C<sub>k</sub>xe(k) + c<sub>k-1</sub>xe(k-1) + ... + c<sub>1</sub>xe(1)over Z, we present a deterministic polynomial time algorithm to decide whether f(w<sub>n</sub>) is zero or not, where f(w<sub>n</sub>) denotes the n-th primitive root of unity e2piradic(-1/n). All previously known algorithms are either randomized, or do not run in polynomial time. As a side result, we prove that if n is free of prime factors less than k + 1, there exist k field automorphisms sigma<sub>1</sub>, sigma<sub>2</sub>, ... , sigma<sub>k</sub> in the Galois group Gal (Q(w<sub>n</sub>)/Q) such that for any nonzero integers c<sub>1</sub>, c<sub>2</sub> ... , c<sub>k</sub> and for any integers 0 les e<sub>1</sub> &lt; e<sub>2</sub> &lt; ... &lt; e<sub>k</sub> &lt; n, there exists i so that |sigma<sub>i</sub>(c<sub>k</sub>w<sub>n</sub> ek + c<sub>k-1</sub>w<sub>n</sub> e(k-1) + ... + c<sub>1</sub>w<sub>n</sub> e(1)) | ges 1/2(k(2)log n+klogk).
[Engineering profession, computational geometry, sparse cyclotomic integer zero testing derandomization, Taylor series, real algebraic number, Computational complexity, Computer science, group theory, Computational geometry, Numerical analysis, USA Councils, numerical analysis, deterministic sparse polynomial time algorithm, Polynomials, sign determination problem, Galois group, Testing, computational complexity]
Computing Equilibria in Anonymous Games
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We present efficient approximation algorithms for finding Nash equilibria in anonymous games, that is, games in which the players utilities, though different, do not differentiate between other players. Our results pertain to such games with many players but few strategies. We show that any such game has an approximate pure Nash equilibrium, computable in polynomial time, with approximation O(s2lambda), where s is the number of strategies and lambda is the Lipschitz constant of the utilities. Finally, we show that there is a PTAS for finding an isin-approximate Nash equilibrium when the number of strategies is two.
[Computer science, anonymous game, Nash equilibria, polynomial approximation, game theory, Nash equilibrium, Approximation algorithms, Polynomials, Internet, Game theory, Sprites (computer), computational complexity]
Mechanism Design via Differential Privacy
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We study the role that privacy-preserving algorithms, which prevent the leakage of specific information about participants, can play in the design of mechanisms for strategic agents, which must encourage players to honestly report information. Specifically, we show that the recent notion of differential privacv, in addition to its own intrinsic virtue, can ensure that participants have limited effect on the outcome of the mechanism, and as a consequence have limited incentive to lie. More precisely, mechanisms with differential privacy are approximate dominant strategy under arbitrary player utility functions, are automatically resilient to coalitions, and easily allow repeatability. We study several special cases of the unlimited supply auction problem, providing new results for digital goods auctions, attribute auctions, and auctions with arbitrary structural constraints on the prices. As an important prelude to developing a privacy-preserving auction mechanism, we introduce and study a generalization of previous privacy work that accommodates the high sensitivity of the auction setting, where a single participant may dramatically alter the optimal fixed price, and a slight change in the offered price may take the revenue from optimal to zero.
[Algorithm design and analysis, Data privacy, attribute auctions, arbitrary structural constraints, Data analysis, unlimited supply auction problem, digital goods auctions, approximate dominant strategy, mechanism design, Computer science, privacy-preserving algorithms, Utility theory, arbitrary player utility functions, Pricing, Silicon, Concrete, Robustness, data privacy, optimal fixed price, Protection, differential privacy, strategic agents]
Balloon Popping With Applications to Ascending Auctions
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We study the power of ascending auctions in a scenario in which a seller is selling a collection of identical items to anonymous unit'demand bidders. We show that even with full knowledge of the set of bidders' private valuations for the items, if the bidders are ex-ante identical, no ascending auction can extract more than a constant. times the revenue of the best fixed-price scheme. This problem is equivalent to the problem of coming up with an optimal strategy for blowing up indistinguishable balloons with known capacities in order to maximize the amount of contained, air. We show that the algorithm which simply inflates all balloons to a fixed volume is close to optimal in this setting.
[Heart, balloon popping, H infinity control, identical items selling, Application software, ascending auctions, Cost accounting, anonymous unit demand bidders, Computer science, USA Councils, best fixed-price scheme, Sampling methods, pricing, Capacity planning, electronic commerce]
On the Complexity of Nash Equilibria and Other Fixed Points (Extended Abstract)
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We reexamine, what it means to compute Nash equilibria and, more, generally, what it means to compute a fixed point of a given Brouwer function, and we investigate the complexity of the associated problems. Specifically, we study the complexity of the following problem: given a finite game, Gamma, with 3 or more players, and given epsiv &gt; 0, compute a vector x' (a mixed strategy profile) that is within distance e (say in t^) of some (exact) Nash equilibrium. We show that approximation of an (actual) Nash equilibrium for games with 3 players, even to within any non-trivial constant additive factor epsiv &lt; 1/2 in just one desired coordinate, is at least as hard as the long standing square-root sum problem, as well as more general arithmetic circuit decision problems, and thus that even placing the approximation problem in NP would-resolve a major open problem in the complexity of numerical computation. Furthermore, we show that the (exact or approximate) computation of Nash equilibria for 3 or more players is complete for the class of search problems, which we call FIXP, that can be cast as fixed point computation problems for functions represented by algebraic circuits (straight line programs) over basis {+, <sub>*</sub>, -, /, max, min}, with rational constants. We show that the linear fragment of FIXP equals PPAD. Many problems in game theory, economics, and probability theory, can be cast as fixed point problems for such algebraic functions. We discuss several important such problems: computing the value of Shapley's stochastic games, and the simpler games of Condon, extinction probabilities of branching processes, termination probabilities of stochastic context-free grammars, and of Recursive Markov Chains. We show that for some of them, the approximation, or even exact computation, problem can be placed-in PPAD, while for others, they are at least as hard as the square-root sum and arithmetic circuit decision problems.
[fixed point computation, decision theory, square root sum problem, Circuits, Stochastic processes, termination probabilities, Nash equilibrium, Search problems, History, Shapley stochastic games, Nash equilibria complexity, NP approximation problem, context-free grammars, recursive Markov chains, branching process extinction probabilities, Informatics, search problems, finite game, arithmetic circuit decision problems, probability, game theory, straight line programs, Game theory, Equations, Computer science, algebraic circuits, Condon games, Markov processes, stochastic games, stochastic context-free grammars, Fixed-point arithmetic, computational complexity]
Paths Beyond Local Search: A Tight Bound for Randomized Fixed-Point Computation
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
In 1983, Akhus proved that randomization can speedup local search. For example, it reduces the query complexity of local search over grid [1 : n]d from ominus(nd-1) to 0(d1/2nd/2). It remains open whether randomisation helps fixed-point computation. Inspired by the recent advances on the complexity of equilibrium computation, we solve this open problem by giving an asymptotically tight bound of (Omega(n))d-1 on the randomized query complexity for computing a fixed point of a discrete Brouwer function over grid [1 : n]d. Our result can be extended to the black-box query model for Sperner's I&amp;mma in any dimension. It also yields a tight bound for the computation of d-dimensional approximate Brouwer fixed points as defined by Scarf and by Hirsch, Papadimitriou, and Vavasis. Since the randomized query complexity of global optimization over [1 : n]d is ominus(nd), the randomized query model over [ 1 : n]d strictly separates these three important search problems: Global optimization is harder than fixed-point computation, and fixed-point computation is harder than local search. Our result indeed demonstrates that randomization does not help much in fixed-point computation in the black-box query model. Our randomized lower bound matches the deterministic complexity of this problem, which is ominus(nd-1).
[Flexible printed circuits, discrete Brouwer function, Search problems, Nash equilibrium, Linear programming, local search method, black-box query model, deterministic algorithms, randomised algorithms, Computer science, query processing, global optimization problem, Upper bound, optimisation, deterministic complexity, Grid computing, Iterative algorithms, Polynomials, Iterative methods, randomized query complexity, search problems, randomized fixed-point computation, computational complexity]
Exponential Time/Space Speedups for Resolution and the PSPACE-completeness of Black-White Pebbling
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
The complexity of the Black-White Pebbling Game has remained open for 30 years. It was devised to capture the power of non-deterministic space bounded computation. Since then it has been applied to problems in diverse areas of computer science including VLSI design and more recently propositional proof complexity. In this paper we show that the Black-While Pebbling Game is PSPACE-complete. We then use similar ideas in a more complicated reduction to prove the PSPACE-completeness of Resolution space. The reduction also yields a surprising exponential time/space speedup for Resolution in which an increase of 3 units of space results in an exponential decrease in proof-size.
[black-white pebbling game, Computational modeling, Circuits, Buildings, PSPACE-completeness, game theory, Very large scale integration, nondeterministic space bounded computation, VLSI design, exponential time-space speedups, Computer science, Turing machines, computer science, computational complexity]
Parameterized Proof Complexity
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We propose a proof-theoretic approach for gaining evidence that certain parameterized problems are not fixed-parameter tractable. We consider proofs that witness that a given propositional CNF formula cannot be satisfied by a truth assignment that sets at most k variables to true, considering k as the parameter (we call such a formula a parameterized contradiction). One could separate the parameterized complexity classes FPT and W(M. Cesati, 2006) by showing that there is no fpt-bounded parameterized proof system, i.e., that there is no proof system that admits proofs of size f(k)nO(1) where f is a computable function and n represents the size of the propositional formula. By way of a first step, we introduce the system of parameterized tree-like resolution, and show that this system is not fpt-bounded. Indeed we give a general result on the size of shortest tree-like resolution proofs of parameterized contradictions that uniformly encode first-order principles over a universe of size n. We establish a dichotomy theorem that splits the exponential case of Riis's complexity-gap Theorem into two sub-cases, one that admits proofs of size f(k)nO(1) and one that does not. We also discuss how the set of parameterized contradictions may be embedded into the set of (ordinary) contradictions by the addition of new axioms. When embedded into general (DAG-like) resolution, we demonstrate that the pigeonhole principle has a proof of size 2kn2. This contrasts with the case of tree-like resolution where the embedded pigeonhole principle falls into the "non-FPT" category of our dichotomy.
[Algorithm design and analysis, first-order principles encoding, propositional CNF formula, dichotomy theorem, trees (mathematics), parameterized contradiction, shortest tree-like resolution proofs, complexity-gap theorem, Computer science, Design engineering, parameterized proof complexity, Turing machines, NP-hard problem, Councils, parameterized tree-like resolution, Polynomials, pigeonhole principle, computational complexity]
Non-Linear Index Coding Outperforming the Linear Optimum
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
The following source coding problem was introduced by Birk and Kol: a sender holds a word x epsi {0,1}n, and wishes to broadcast a codeword to n receivers, R<sub>1</sub>,..., R<sub>n</sub>middot. The receiver R<sub>i</sub> is interested in x;, and has prior side information comprising some subset of the n bits. This corresponds to a directed graph G on n vertices, where ij is an edge iff R<sub>i</sub> knows the bit x<sub>j</sub> . An index code for G is an encoding scheme which enables each R<sub>i</sub> to always reconstruct Xj, given his side information. The minimal word length of an index code was studied by Bar-Yossef Birk, Jay ram and Kol. Thev introduced a graph parameter, minrk<sub>2</sub>(G), which completely characterizes the length of an optimal linear index code for G. The authors of (Z. Bar-Yossef, 2006) showed that in various cases linear codes attain the optimal word length, and conjectured that linear index coding is in fact always optimal. In this work, we disprove the main conjecture of (Z. Bar-Yossef, 2006) in the following strong sense: for any epsiv &gt; 0 and sufficiently large n, there is an n-vertex graph G so that evety linear index code for G requires codewords of length at least n1-epsiv and yet a non-linear index code for G has a word length of nepsiv. This is achieved by an explicit construction, which extends Alon's variant of the celebrated Ramsey construction of Frankl and Wilson.
[nonlinear index coding problem, linear codes, source coding, Source coding, Mathematics, Encoding, set theory, Computer science, Linear code, optimal linear index code, directed graphs, Binary codes, directed graph, Broadcasting, source coding problem, nonlinear codes]
Can you beat treewidth?
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
It is well-known that constraint satisfaction problems (CSP) can be solved in time nO(k) if the treewidth of the primal graph of the instance is at most k and n is the size of the input. We show that no algorithm can be significantly better than this treewidth-based algorithm, even if we restrict the problem to some special class of primal graphs. Formally, let g be an arbitrary class of graphs and assume that there is an algorithm A solving binary CSP for instances whose primal graph is in g. We prove that if the running lime of A is f(G)nO(k/logk), where k is the treewidth of the primal graph G and f is an arbitrary function, then the Exponential Time Hypothesis fails. We prove the result also in the more general framework of the homomorphism problem for bounded-arity relational structures. For this problem, the treewidth of the core of the left-hand side structure plays the same role as the. treewidth of the primal graph above.
[constraint theory, graph theory, bounded-arity relational structure, trees (mathematics), homomorphism problem, Relational databases, constraint satisfaction problem, Computer science, arbitrary function, Tree graphs, Polynomials, treewidth-based algorithm, computational complexity]
Adaptive Simulated Annealing: A Near-optimal Connection between Sampling and Counting
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We present a near-optimal reduction from approximately counting the cardinality of a discrete set to approximately sampling elements of the set. An important application of our work is to approximating the partition function Z of a discrete system, such as the Ising model, matchings or colorings of a graph. The standard approach to estimating the partition function Z(beta*) at some desired inverse temperature beta* is to define a sequence, which we call a cooling schedule, beta<sub>0</sub> = 0 &lt; beta<sub>1</sub> &lt; ldrldrldr &lt; beta<sub>l</sub> = beta* where Z(0) is trivial to compute and the ratios Z(beta<sub>i+1</sub>)/Z(beta<sub>i</sub>) are easy to estimate by sampling from the distribution corresponding to Z(beta<sub>i</sub>). Previous approaches required a cooling schedule of length O*(ln A) where A = Z(0), thereby ensuring that each ratio Z(beta<sub>i+1</sub>)/Z(beta<sub>i</sub>) is bounded. We present a cooling schedule of length l = O*(radicln A). For well-studied problems such as estimating the partition function of the Ising model, or approximating the number of colorings or matchings of a graph, our cooling schedule is of length O* (radicln) and the total number of samples required is O*(n). This implies an overall savings of a factor of roughly n in the running time of the approximate counting algorithm compared to the previous best approach. A similar improvement in the length of the cooling schedule was recently obtained by Lovtisz and Vempala in the context of estimating the volume of convex bodies. While our reduction is inspired by theirs, the discrete analogue of their result turns out to be significantly more difficult. Whereas a fixed schedule suffices in their setting, we prove that in the discrete setting we need an adaptive schedule, i. e., the schedule depends on Z. More precisely, we prove any non-adaptive cooling schedule has length at least O*(ln A), and we present an algorithm to find an adaptive schedule of length O* (radicln A) and a nearly matching lower bound.
[cooling schedule, pattern matching, 0/1 matrix, graph colouring, Simulated annealing, near-optimal connection, partition function, Ising model, sampling methods, Cooling, simulated annealing, Computational modeling, Computer simulation, adaptive simulated annealing, graph matchings, approximate counting algorithm, counting method, Educational institutions, Physics, matrix algebra, Computer science, Adaptive scheduling, Processor scheduling, approximate sampling algorithm, Sampling methods, discrete system, graph colorings, computational complexity, sampling method]
Reconstruction for Models on Random Graphs
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Consider a collection of random variables attached to the vertices of a graph. The reconstruction problem requires to estimate one of them given far away' observations. Several theoretical results (and simple algorithms) are available when (heir joint probal)ility distribution is Markov with respect to a tree. In this paper we consider the case of sequences of random graphs that converge locally to trees. In particular, we develop a sufficient condition for the tree and graph reconstruction problem to coincide. We apply such condition to colorings of random graphs. Further, we characterize the behavior of I'sing models on such graphs, both with attractive and random interactions (respectively, ferromagnetic' and 'spin glass').
[Markov process, TV, random graph, trees (mathematics), Glass, random processes, Probability distribution, graph coloring, graph colouring, Computer science, Sufficient conditions, Graphical models, Tree graphs, USA Councils, tree reconstruction problem, Statistical distributions, Random variables]
Mixing Time Power Laws at Criticality
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We study the mixing time of some Markov chains converging to critical physical models. These models are indexed by a parameter beta and there exists some critical value beta<sub>c</sub> where the model undergoes a phase transition. According to physics lore, the mixing time of such Markov chains is often of logarithmic order outside the critical regime, when beta ne beta<sub>c</sub>, and satisfies-some power law at criticality, when beta = beta<sub>c</sub>. We prove this in the two following settings: 1. Lazy random walk on the critical percolation cluster of "mean-field" graphs, which include the complete graph and random d-regular graphs. The critical mixing time here is of order Theta(n). This answers a question of Benjamini, Kozma and Wormald. 2. Swendsen-Wang dynamics on the complete, graph. The critical mixing time, here is of order Theta(n1/4). This improves results of Cooper, Dyer, Frieze and Rue. In both settings, the main tool is understanding the Markov chain dynamics via properties of critical percolation on the underlying graph.
[Temperature distribution, lazy random walk, critical physical models, graph theory, Lattices, Size measurement, Markov chains, critical percolation cluster, mean-field graphs, critical mixing time, Mathematics, Time measurement, Physics, Computer science, Tree graphs, random d-regular graphs, Clustering algorithms, Markov processes, time power laws, Sampling methods, phase transition]
Near Optimal Bounds for Collision in Pollard Rho for Discrete Log
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We analyze-a fairly standard idealization of Pollard's rho algorithm for finding the discrete logarithm in acyclic group G. It is found that, with high probability, a collision occurs in O(radic( |G|log|G|log log|G|)) steps, not far from the widely conjectured value of Theta(radic|G|). Tins improves upon a recent result of Miller-Venkalesan which showed an upper bound of O(radic|G|log3|G|). Our proof is based on analyzing an appropriate nonreversible, non-lazy random walk on a discrete cycle of (odd) length |G|, and showing that the mixing time of the corresponding walk is O(log|G|log log|G|).
[Algorithm design and analysis, probability, random processes, Educational institutions, Mathematics, History, Pollard Rho algorithm, acyclic group, Computer science, group theory, random walk, Upper bound, optimal bound, Elliptic curve cryptography, Approximation algorithms, discrete logarithm, computational complexity]
Intrusion-Resilient Secret Sharing
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We introduce a new primitive called intrusion-resilient secret sharing (IRSS), whose security proof exploits the fact that there exist functions which can be efficiently computed interactively using low communication complexity in k, but not in k-1 rounds. IRSS is a means of sharing a secret message amongst a set of players which comes with a very strong security guarantee. The shares in an IRSS are made artificially large so that it is hard to retrieve them completely, and the reconstruction procedure is interactive requiring the players to exchange k short messages. The adversaries considered can attack the scheme in rounds, where in each round the adversary chooses some player to corrupt and some function, and retrieves the output of that function applied to the share of the corrupted player. This model captures for example computers connected to a network which can occasionally he infected by malicious software like viruses, which can compute any function on the infected machine, but cannot sent out a huge amount of data. Using methods from the bounded-retrieval model, we construct an IRSS scheme which is secure against any computationally unbounded adversary as long as the total amount of information retrieved by the adversary is somewhat less than the length of the shares, and the adversary makes at most k-1 corruption rounds (as described above, where k rounds are necessary for reconstruction). We extend our basic scheme in several ways in order to allow the shares sent by the dealer to be short (the players then blow them up locally) and to handle even stronger adversaries who can learn some of the shares completely. As mentioned, there is an obvious connection between IRSS schemes and the fact that there exist functions with an exponential gap in their communication complexity for k and k-1 rounds. Our scheme implies such a separation which is in several aspects stronger than the previously known ones.
[invasive software, intrusion-resilient secret sharing, Art, Computer viruses, reconstruction procedure, game theory, information retrieval, bounded-retrieval model, Information retrieval, Complexity theory, communication complexity, Cryptographic protocols, Computer science, malicious software, security of data, Information security, interactive systems, Computer networks, Cryptography, Computer security, computational complexity]
Covert Multi-Party Computation
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
In STOC'05, Aim, Hopper and Longford introduced the notion of covert computation. A covert computation protocol is one in which parties am run a protocol without knowing if other parties ore also participating in the protocol or not. At the end of the protocol, if all parties participated in the protocol and if the function output is favorable to all parties, then the output is revealed. Ahn et al. constructed a protocol for covert two-partv computation in the random oracle model In this paper, we offer a construction for covert multiparty computation. Our construction is in the standard model and does not require random oracles. In order to achieve this goal, we introduce a number of new techniques. Central to our work is the development of "zero-knowledge proofs to garbled circuits," which we believe could be of independent interest. Along the way, we also develop a definition of covert computation as per the Ideal/Real model simulation paradigm.
[Computer science, Technological innovation, Protocols, Computational modeling, Circuit simulation, random oracle, garbled circuits, covert computation protocol, computation theory, covert multiparty computation, protocols, zero-knowledge proofs]
Cryptography from Sunspots: How to Use an Imperfect Reference String
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
The common reference string (CRS) model equips all protocol participants with a common string that is sampled from a pre-specified distribution, say the uniform distribution. This model enables otherwise-impossible cryptographic goals such as removing interaction from protocols and guaranteeing composable security. However, knowing the precise distribution of the reference string seems crucial for all known protocols in this model, in the sense that current security analyses fail when the actual distribution of the reference string is allowed to differ from the specified one even by a small amount. This fact rules out many potential implementations of the CRS model, such as measurements of physical phenomena (like sunspots), or alternatively using random sources that might be adversarially influenced. We study the possibility of obtaining universally composable (UC) security in a relaxed variant of the CRS model, where the reference string it taken from an adversarially specified distribution that's unknown to the protocol. On the positive side, we demonstrate that UC general secure computation is obtainable even when the reference string is taken from an arbitrary, adversarially chosen distribution, as long as (a) this distribution has some minimal min-entropy, (b) it has not too long a description, (c) it is efficiently samplable, and (d) the sampling algorithm is known to the adversary (and simulator). On the negative side, we show that if any one of these four conditions is removed then genera! UC secure computation becomes essentially impossible.
[cryptographic protocols, universally composable security, common imperfect reference string model, cryptographic protocol, Entropy, Security, statistical distributions, Distributed computing, uniform distribution, UC Security, Failure analysis, Cryptography, minimum entropy methods, sampling methods, Common Reference String, Computational modeling, Access protocols, Non black-box constructions, Cryptographic protocols, Radio access networks, Computer science, sampling algorithm, Setup Models, minimal min-entropy, Public key]
Planning for Fast Connectivity Updates
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Understanding how a single edge deletion can affect the connectivity of a graph amounts to finding the graph bridges. But when faced with d. &gt; l deletions, can we establish as easily how the connectivity changes? When planning for an emergency, we want to understand the structure of our network ahead of time, and respond swiftly when an emergency actually happens. We describe a linear-space representation of graphs which enables us to determine how a batch of edge updates can impact the graph. Given a set of d edge updates, in time O(d polylg n) we can obtain the number of connected components, the size of each component, and a fast oracle for answering connectivity queries in the updated graph. The initial representation is polynomial-time constructible.
[Algorithm design and analysis, graph linear-space representation, Heuristic algorithms, Roads, fast connectivity update, Spine, graph theory, edge deletion, Statistics, updated graph, Bridges, Computer science, connectivity query, graph connectivity, graphs, Tree graphs, polynomial-time constructible, Approximation algorithms, Polynomials, edge update, search problems]
Strongly History-Independent Hashing with Applications
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We present a strongly history independent (SHI) hash table that supports search in O(l) worst-case time, and insert and delete in O(l) expected time using O(n) data space. This matches the bounds for dynamic perfect hashing, and improves on the best previous results by Naor and league on history independent hashing, which were either weakly history independent, or only supported insertion and search (no delete) each in O(l) expected time. The results can be used to construct many other SHI data structures. We show straightforward constructions for SHI ordered dictionaries: for n keys from {l,..., nk} searches take O(log log n) worst-case time and updates (insertions and deletions) O(log log n) expected time, and for keys in the comparison model searches take O(log n) worst-case time and updates O(log n) expected time. We also describe a SHI data structure for the order-maintenance problem. It supports comparisons in O(l) worst-case time, and updates in 0(1) expected time. All structures use O(n) data space.
[Dictionaries, hash table, Data security, Government, Nominations and elections, Data structures, worst-case time, history, History, Application software, Computer science, Publishing, Information security, strongly history-independent hashing, data structures, computational complexity]
Smooth Histograms for Sliding Windows
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
In the streaming model elements arrive sequentially and can be observed only once. Maintaining statistics and aggregates is an important and non-trivial task in the model. This becomes even more challenging in the sliding windows model, where statistics must be maintained only over the most recent n elements. In their pioneering paper, Datar, Gionis, Indyk and Motwani [15] presented exponential histograms, an effective method for estimating statistics on sliding windows. In this paper we present a new smooth histograms method that improves the approximation error rate obtained via exponential histograms. Furthermore, our smooth histograms method not only captures and improves multiple previous results on sliding windows bur also extends the class functions that can be approximated on sliding windows. In particular, we provide the first approximation algorithms for the following functions: L<sub>p</sub> norms for p notin [1,2], frequency moments, length of increasing subsequence and geometric mean.
[Solid modeling, data analysis, Mathematics, Statistics, sliding window model statistics, geometric mean, Computer science, Histograms, smooth exponential histogram, Aggregates, function approximation error rate, function approximation, Approximation algorithms, Approximation error, Frequency, frequency moment, Books, data streaming model, computational complexity, statistics]
Lower Bounds on Streaming Algorithms for Approximating the Length of the Longest Increasing Subsequence
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We show that any deterministic data-stream algorithm that, makes a constant number of passes over the input and gives a constant, factor approximation of the length of the longest increasing subsequence in a sequence of length n must use space Omega(radicn). This proves a conjecture made by Gopalan, Jayram, Krauthgamer and Kumar |10| who proved a matching upper bound. Our results yield asymptotically tight tower bounds for all approximation factors, thus resolving the main open problem, from their paper. Our proof is based on analyzing a related communication problem and proving a direct sum type property for it.
[Algorithm design and analysis, Protocols, data analysis, Biological system modeling, Computational modeling, constant number, deterministic data-stream algorithm, Sorting, Computer science, Upper bound, factor approximation, Biology computing, Approximation algorithms, Computer networks]
Towards Sharp Inapproximability For Any 2-CSP
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We continue the recent line of work on the connection between semidefinite programming-based approximation algorithms and the Unique Games Conjecture. Given any-boolean 2-CSP (or more generally, any nonnegative objective function on two boolean variables), we show how to reduce the search for a good inapproximability result to a certain numeric minimization problem. The key objects in our analysis are the vector triples arising when doing clause-by-clause analysis of algorithms based on semidefinite programming. Given a weighted set of such triples of a certain restricted type, which are "hard" to round in a certain sense, we obtain a Unique Games-based inapproximability matching this "hardness" of rounding the set of vector triples. Conversely, any instance together with an SDP solution can be viewed as a set of vector triples, and we show that we can always find an assignment to the instance which is at least as good as the "hardness" of rounding the corresponding set of vector triples. We conjecture that the restricted type required for the hardness result is in fact no restriction, which would imply that these upper and lower bounds match exactly. This conjecture is supported by all existing results for specific 2-CSPs. As an application, we show that Max 2-AND is hard to approximate within 0.87435. This improves upon the best previous hardness of alpha<sub>GW</sub> + epsi ap 0.87856, and comes very close to matching the approximation ratio of the best algorithm known, 0.87401. It also establishes that balanced instances of Max 2-AND, i.e., instances in which each variable occurs positively and negatively equally often, are not the hardest to approximate, as these can be approximated within a factor alpha<sub>GW</sub>.
[Algorithm design and analysis, Neodymium, constraint theory, game theory, sharp inapproximability, Minimization, Boolean 2-CSP, numeric minimization problem, clause-by-clause analysis, mathematical programming, Computer science, Boolean functions, vectors, unique games-based inapproximability matching, vector triples, Approximation algorithms, semidefinite programming-based approximation algorithms, minimisation, SDP solution, computational complexity]
Linear Equations Modulo 2 and the L1 Diameter of Convex Bodies
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We design a randomized polynomial time algorithm which, given a 3-tensor of real numbers A={a<sub>ijk</sub>}<sub>ij,k=1</sub> n such that for all i,j,kisin{1,...,n} we have a<sub>ijk</sub>=a<sub>ikj</sub>=a<sub>kji</sub>=a<sub>jik</sub>=a<sub>kij</sub>=a<sub>kji</sub> and a<sub>iik</sub>=a<sub>ijj</sub>=a<sub>iji</sub>=0, computes a number Alg(A) which satisfies with probability at least 1/2, Omega(radic(logn/n))ldrmax<sub>xisin{-1,1}</sub> n Sigma<sub>i,j,k=1</sub> na<sub>ijk</sub>x<sub>i</sub>x<sub>j</sub>x<sub>k</sub>lesAlg(A)lesmax<sub>xisin{-1,1}</sub> n Sigma<sub>i,j,k=1</sub> na<sub>ijk</sub>x<sub>i</sub>x<sub>j</sub>x<sub>k</sub>. On the other hand, we show via a simple reduction from a result of Hastad and Venkatesh that under the assumption NPnsubeDTIME(n(logn) O(1)),for every epsiv&gt;0 there is no algorithm that approximates max<sub>xisin{-1,1}</sub> n Sigma<sub>i,j,k=1</sub> na<sub>ijk</sub>x<sub>i</sub>x<sub>j</sub>x<sub>k</sub> within a factor of 2(logn)t-epsiv in time 2(logn) O(1). Our algorithm is based on a reduction to the problem of computing the diameter of a convex body in Rn with respect to the L<sub>1</sub> norm. We show that it is possible to do so up to a multiplicative error of O(radic(n/logn)), while no randomized polynomial time algorithm can achieve accuracy O(radic(n/logn)). This resolves a question posed by Brieden, Gritzmann, Kantian, Klee, Lovasz and Simonos. We apply our new algorithm improve the algorithm of Hastad and Venkatesh or the Max-E3-Lin-2 problem. Given an over-determined system epsiv of N linear equations modulo 2 in nlesN Boolean variables, such that in each equation appear only three distinct variables, the goal is to approximate in polynomial time the maximum number of satisfiable equations in epsiv minus N/2 (i.e. we subtract the expected number of satisfied equations in a random assignment). Hastad and Venkatesh obtained an algorithm which approximates this value up to a factor of O(radicN). We obtain a O(radic(n/logn)) approximation algorithm. By relating this problem to the refutation problem for random 3-CNF formulas we give evidence that obtaining a significant improvement over this approximation factor is likely to be difficult.
[Algorithm design and analysis, polynomial time approximation, Max-E3-Lin-2 problem, Fourier transforms, probability, randomized polynomial time algorithm, real number tensor, tensors, random 3-CNF formulas, Partitioning algorithms, Boolean algebra, linear equations modulo 2, Equations, randomised algorithms, Computer science, Geometry, convex bodies, polynomial approximation, Boolean variables, Approximation algorithms, Polynomials, computational complexity]
Inapproximability Results for Sparsest Cut, Optimal Linear Arrangement, and Precedence Constrained Scheduling
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We consider (uniform) sparsest cut, optimal linear arrangement and the precedence constrained scheduling problem 1 |prec| Sigmaw<sub>j</sub>C<sub>j</sub>-So far, these three notorious NP-hard problems have resisted all attempts to prove inapproximability results. We show that they have no polynomial time approximation scheme (PTAS), unless NP-complete pmblems can be solved in randomized subexponential time. Furthermore, we prove that the scheduling problem is as-hard to approximate as vertex cover when the so-called fixed cost, that is present in all feasible solutions, is subtracted from the objective function.
[vertex cover, graph theory, optimal linear arrangement, NP-complete problem, Computer science, NP-hard pmblems, optimisation, Processor scheduling, fixed cost, polynomial approximation, randomized subexponential time, sparsest cut, polynomial time approximation scheme, Cost function, Approximation algorithms, Polynomials, precedence constrained scheduling, Single machine scheduling]
On the Optimality of Planar and Geometric Approximation Schemes
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We show for several planar and geometric problems that the best known approximation schemes are essentially optimal with respect to the dependence on epsi. For example, we show that the 2O(1/epsi)ldrn time approximation schemes for planar maximum independent set and for TSP on a metric defined bv a planar graph are essentially optimal: if there is a delta&gt;0 such that any of these problems admits a 2O((1/epsi) 1-delta )nO(1) time PTAS, then the exponential tune hypothesis (ETH) fails. It is known that maximum independent set on unit disk graphs and the planar logic problems MPSAT. TMIN, TMAX admit nO(1/epsi) time approximation schemes. We show that they are optimal in the sense that if there is a delta&gt;0 such that any of these problems admits a 2(1/epsi) O(1) nO((1/epsi) 1-delta ) time PTAS, then ETH fails.
[approximation theory, unit disk graph, graph theory, time approximation, maximum independent set, planar graph, History, Computer science, TSP, geometric approximation scheme, exponential tune hypothesis, planar logic problem, ETH, Polynomials, Logic, planar approximation scheme]
Hardness of Reconstructing Multivariate Polynomials over Finite Fields
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We study the polynomial reconstruction problem, for low-degree multivariate polynomials over F[2]. In this problem, we are given a set of points x epsi {0, 1}n and target values f(x) epsi {0, 1} for each of these points, with the promise that there is a polynomial over F[2] of degree at most d that agrees with f at 1 - epsiv fraction of the points. Our goal is to find agree d polynomial that has good-agreement with f. We show that it is NP-hard to find a polynomial that agrees with f on more than 1 - 2-d + delta fraction of the points for any epsiv, delta &gt; 0. This holds even with the stronger promise that the polynomial that fits the data is in fact linear, wherejis the algorithm is allowed to find a polynomial of degree d. Previously the only known, hardness of approximation (or even NP-completeness) was for the case when d = I, which follows from a celebrated result of Has tad. In the setting of computational learning, our result shows the hardness of (non-proper) agnostic learning of parities, where the learner is allowed, a low-degree polynomial over F[2] as a hypothesis. This is the first non-proper hardness result for this central problem in computational learning. Our results extend-to multivariate polynomial reconstruction over any finite field.
[Engineering profession, multivariate polynomial reconstruction problem, Decoding, set theory, Application software, Galois fields, Computational complexity, approximation hardness, Reed-Solomon codes, Computer science, NP-hard problem, computational learning, Polynomials, Error correction, Error correction codes, learning (artificial intelligence), computational complexity]
Any AND-OR Formula of Size N can be Evaluated in time N^{1/2 + o(1)} on a Quantum Computer
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
For any AND-OR formula of size N, there exists a bounded-error N1/2+o(1)-time quantum algorithm, based on a discrete-time quantum walk, that evaluates this formula on a black-box input. Balanced, or "approximately balanced," formulas can be evaluated in O(radicN) queries, which is optimal. It follows that the (2-o(1))th power of the quantum query complexity is a lower bound on the formula size, almost solving in the positive an open problem posed by Laplante, Lee and Szegedy.
[quantum computer, discrete-time quantum walk, quantum query complexity, AND-OR formula, bounded-error time quantum algorithm, History, Combinatorial mathematics, Spectral analysis, Computer science, formal logic, Quantum computing, Databases, Algorithms, quantum computing, computational complexity]
The Power of Quantum Systems on a Line
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We study the computational strength of quantum particles (each of finite dimensionality) arranged on a line. First, we prove that it is possible to perform universal adiabatic quantum computation using a one-dimensional quantum system (with 9 states per particle). Building on the same construction, but with some additional technical effort and 12 states per particle, we show that the problem of approximating the ground state energy of a system composed of a line of quantum' particles is QMA-complete; QMA is a quantum analogue of NP. This is in striking contrast to the analogous classical problem, one dimensional MAX-2-SAT with nearest neighbor constraints, which is in P.The proof of the QMA-completeness result requires an additional idea beyond the usual techniques in the area: Some illegal configurations cannot be ruled out by local checks, and are instead ruled out because they would, in the future, evolve into a state which can be seen locally to be illegal. Assuming BQP ne QMA, our construction gives a one-dimensional system which takes an exponential time to relax to its ground state at any temperature. This makes it a candidate for a one-dimensional spin glass.
[Temperature, Computational modeling, Circuit simulation, Stationary state, finite dimensionality, ground state energy, Nearest neighbor searches, Computer science, Quantum computing, one-dimensional quantum system, USA Councils, quantum computing, universal adiabatic quantum computation, Polynomials, Power engineering and energy, quantum particle, one-dimensional spin glass]
Simulating Quantum Correlations with Finite Communication
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Assume Alice and Bob share some bipartite d-dimensional quantum state. As is well known, by performing two-outcome measurements, Alice and Bob can produce correlations that cannot be obtained classically. We show that by using only two bits of communication, Alice and Bob can classically simulate any such correlations. All previous protocols for exact simulation required the communication to grow to infinity with the dimension d. Our protocol and analysis are based on a power series method, resembling Krivine's bound on Grothendieck's constant, and on the computation of volumes of spherical tetrahedra.
[Performance evaluation, correlation theory, Protocols, Quantum entanglement, finite communication, Computational modeling, Computer simulation, H infinity control, quantum correlations, Krivine bound, Complexity theory, Computer science, Quantum computing, power series method, Grothendieck's constant, quantum state, Eigenvalues and eigenfunctions, quantum communication]
Quantum Algorithms for Hidden Nonlinear Structures
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Attempts to find new quantum algorithms that outperform classical computation have focused primarily on the nonAbelian hidden subgroup problem, which generalizes the central problem solved by Shor's factoring algorithm. We suggest an alternative generalization, namely to problems of finding hidden nonlinear structures over finite fields. We give examples of two such problems that can be solved efficiently by a quantum computer, but not by a classical computer. We also give some positive results on the quantum query complexity of finding hidden nonlinear structures.
[quantum computer, Fourier transforms, Level set, quantum query complexity, Interference, quantum algorithm, Vectors, Galois fields, Computer science, Quantum computing, USA Councils, Quantum mechanics, quantum computing, hidden nonlinear structure, Polynomials, computational complexity]
Refuting Smoothed 3CNF Formulas
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We introduce the following model for generating .semi-random 3CNF formulas. First, an adversary is allowed to pick an arbitrary formula with n varialdes and in clauses. Then, the formula is slightly perturbed at random. Namely, the smoothing operation leaves the variables of the formula unchanged, but flips the polarity of every variable occurrence in the formula independently with probability a. If the density m/n of a 3CNF formula exceeds a certain threshold value (say, 5epsiv-3) then the smoothing operation almost surely results in a non-satisfiable formula. We present a randomized polynomial time refutation algorithm that for every sufficiently dense 3CNF formula manages to refute most of its smoothed instantiations. The density requirement for our refutation algorithm is roughly epsiv-2 radic(n log log n), which almost matches the density Omega( radicn) required bv known algorithms for refuting 3CNF formulas that are completely random.
[Smoothing methods, formal languages, probability, random processes, Probability distribution, Mathematics, Noise generators, polynomial time algorithm, Noise level, Computer science, random process, Upper bound, Polynomials, Mathematical model, refuted smoothed 3CNF formula, Context modeling, computational complexity]
Hardness Amplification for Errorless Heuristics
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
An errorless heuristic is an algorithm that on all inputs returns either the correct answer or the special symbol perp, which means "I don't know," A central question in average-case complexity is whether every distributional decision problem in N P has an errorless heuristic scheme: This is an algorithm that, for every delta &gt; 0, runs in time polynomial in the instance size and | / delta and answers perp only on a delta fraction of instances. We study the question from the standpoint of hardness amplification and show that If every problem in (NP,U) has errorless heuristic circuits that output the correct answer on n -2/9+omicron(1)-fraction of inputs, then (NP,U) has non-uniform errorless heuristic schemes. If every problem in (NP,U) has randomized errorless heuristic algorithms that output the correct answer on (log n)-1/10+omicron(1)-fraction of inputs, then (NP.W) has randomized errorless heuristic schemes. In both cases, the low-end amplification is achieved by analyzing a new sensitivity property of monotone boolean Junctions in NP. In the non-uniform setting we use a " holographic Junction" introduced by Benjamini, Schramm, and Wilson (STOC 2005). For the uniform setting we introduce a new Junction that can be viewed as an efficient version of Talagrand's "random DNF".
[distributional decision problem, holographic junction, Heuristic algorithms, Circuits, Holography, hardness amplification, Stress, Computer science, average-case complexity, Boolean functions, optimisation, errorless heuristics, monotone boolean junctions, NP problem, Computer errors, Polynomials, Error correction, computational complexity]
One-Way Multi-Party Communication Lower Bound for Pointer Jumping with Applications
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
In this paper we study the one-way multi-party communication model, in which even party speaks exactly once in its turn. For every fixed k, we prove a tight lower hound of Omega (n1/(k-1)) on the probabilistic communication complexity of pointer jumping in a k-layered tree, where the pointers of the i-lh layer reside on the forehead of the i-th party to speak. The lower bound remains nontrivial even for k = (log n)1/2-Omega(1) parties. Previous to our work a lower bound was known only for k = 3 , and in very restricted models for k &gt; 3. Our results have the following consequences to other models and problems, extending previous work in several directions. The one-way model is strong enough to capture general (non one-wav) multi-party protocols of bounded rounds. Thus we generalize to this multi-party model results on two directions studied in the classical 2-party model. The first is a mund hierarchy: We give an exponential separation between the power of r and 2r rounds in general probabilistic k-party protocols, for any fixed k and r. The second is the relative power of determinism and nondeterminism: We prove an exponential separation between nondeterministic and deterministic communication complexity for general k-party protocols with r rounds, for anvfixed k, r. The pointer jumping function is weak enough to be a special case of the well-studied disjointness function. Thus we obtain a lower bound of Omega (n1/(k-1)) on the probabilistic complexity of k-set disjointness in the oneway model, which was known only for k = 3 parties. Our result also extends a similar lower bound for the weaker simultaneous model, in which parties simultaneously send one message to a referee. Finally, we infer an exponential separation between the power of different orders in which parties send messages in the one-way model, for every fixed k. Previous to our work such a separation was only known for k = 3. Our lower bound technique, which handles functions of high discrepancy, may be of independent interest. It provides a "party-elimination " induction, based on a restricted form of a direct-product result, specific to the pointer jumping function.
[Protocols, trees (mathematics), one-way multi-party communication lower bound, Mathematics, disjointness function, Complexity theory, Application software, communication complexity, Distributed computing, Communication standards, Computer science, Forehead, Turing machines, pointer jumping, k-layered tree, probabilistic k-party protocols, probabilistic communication complexity, Mathematical model, protocols, multi-party protocols]
A Lower Bound for the Size of Syntactically Multilinear Arithmetic Circuits
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We construct an explicit polynomial f(x<sub>1</sub>,..., x<sub>n</sub>), with coefficients in {0, 1}, such that the size of any syntactically multilinear arithmetic circuit computing f is at least Omega{n4/3 log2 n} The lower bound holds over any field.
[Computer science, rational functions, polynomials, Circuits, polynomial, Binary trees, syntactically multilinear arithmetic circuits, Digital arithmetic, Polynomials, Mathematics, logic circuits, Radio access networks]
Discrepancy and the Power of Bottom Fan-in in Depth-three Circuits
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We develop a new technique of proving lower bounds for the randomized communication complexity of boolean functions in the multiparty 'number on the forehead' model. Our method is based on the notion of voting polynomial degree of functions and extends the degree-discrepancy lemma in the recent work of Sherstov (2007). Using this we prove that depth three circuits consisting of a MAJORITY gate at the output, gates computing arbitrary symmetric function at the second layer and arbitrary gates of bounded fan-in at the base layer i.e. circuits of type MAJ o SYMM o ANY<sub>O(1)</sub> cannot simulate the circuit class AC0 in sub-exponential size. Further, even if the fan-in of the bottom ANY gates are increased to o(log log n), such circuits cannot simulate AC0 in quasi-polynomial size. This is in contrast to the classical result of Yao and Beigel-Tarui that shows that such circuits, having only MAJORITY gales, can simulate the class ACC0 in quasi-polynomial size when the bottom fan-in is increased to poly-logarithmic size. In the second part, we simplify the arguments in the breakthrough work of Bourgain (2005) for obtaining exponentially small upper bounds on the correlation between the boolean function MOD<sub>q</sub> and functions represented bv polynomials of small degree over Z<sub>m</sub>, when m,q ges 2 are co-prime integers. Our calculation also shows similarity with techniques used to estimate discrepancy of functions in the multiparty communication setting. This results in a slight improvement of the estimates of Bourgain et al. (2005). It is known that such estimates imply that circuits of type MAJ o MOD<sub>m</sub> o AND<sub>isin</sub> <sub>log</sub> <sub>n</sub> cannot compute the MOD<sub>q</sub> function in sub-exponential size. It remains a major open question to determine if such circuits can simulate ACC0 in polynomial size when the bottom fan-in is increased to poly-logarithmic size.
[Circuit simulation, Computational modeling, Scholarships, polynomials, Complexity theory, randomized communication complexity, communication complexity, boolean functions, randomised algorithms, MAJORITY gates, Computer science, Forehead, Boolean functions, Upper bound, Voting, multiparty communication setting, Polynomials, polynomial degree of functions, degree-discrepancy lemma, majority logic]
Maximizing Non-Monotone Submodular Functions
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Submodular maximization generalizes many important problems including Max Cut in directed/undirected graphs and hypergraphs, certain constraint satisfaction problems and maximum facility location problems. Unlike the problem of minimizing submodular functions, the problem of maximizing submodular functions is NP-hard.
[Greedy algorithms, Algorithm design and analysis, constraint satisfaction problem, Mathematics, nonmonotone submodular function, Computer science, optimisation, submodular maximization, NP-hard problem, directed graphs, function approximation, Approximation algorithms, Polynomials, undirected graph, search problems, computational complexity, maximum facility location problem]
On the Hardness and Smoothed Complexity of Quasi-Concave Minimization
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
In this paper, we resolve, the smoothed and approximative complexity of low-rank quasi-concave minimization, providing both upper and lower bounds. As an upper bound, we provide the first smoothed analysis of quasi-concave, minimization. The analysis is based on a smoothed bound for the number of extreme points of the projection of the feasible polytope onto a k-dimensional subspace. where k is the rank (informally, the dimension of nonconvexity)ofthe quasi-concave function. Our smoothed bound is polynomial in the original dimension of the problem n and the perturbation size p. and it is exponential in the rank of the function k. From this, we obtain the first randomized fully polynomial-time approximation scheme for low-rank quasi-concave minimization under broad conditions. In contrast with this, we prove log n-hardness of approximation for general quasi-concave minimization. This shows that our smoothed bound is essentially tight, in that no polynomial smoothed bound is possible for quasi-concave functions of general rank k. The tools that we introduce for the smoothed analysis may be of independent interest. All previous smoothed analyses of polytopes analyzed projections onto two-dimensional subspaces and studied them using trigonometry to examine the angles between vectors and 2-planes in Ropf". In this paper, we provide what is, to our knowledge, the first smoothed analysis of the projection of polytopes onto higher-dimensional subspaces. To do this, we replace the trigonometry with tools from random matrix theory and differential geometry on the Grassmannian. Our hardness reduction is based on entirely different proofs that may also be of independent interest; we show that the stochastic 2-stage minimum spanning tree problem has a supermodular objective and that supermodular minimization is hard to approximate.
[polynomial-time approximation scheme, Stochastic processes, smoothing methods, Mathematics, Design optimization, polynomial approximation, supermodular objective minimization, differential geometry, Hypercubes, Polynomials, stochastic processes, polytope projection, higher-dimensional subspace, smoothed analysis, trees (mathematics), log n-hardness, matrix algebra, Geometry, Computer science, stochastic 2-stage minimum spanning tree problem, Upper bound, random matrix theory, Approximation algorithms, Resource management, minimisation, computational complexity, low-rank quasi concave minimization]
Approximation Algorithms for Partial-Information Based Stochastic Control with Markovian Rewards
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We consider a variant of the classic multi-armed bandit problem (MAB), which we call feedback MAB, where the reward obtained by playing each of n independent arms varies according to an underlying on/off Markov process with known parameters. The evolution of the Markov chain happens irrespective of whether the arm is played, and furthermore, the exact state of the Markov chain is only revealed to the player when the arm is played and the reward observed. At most one arm (or in general, M arms) can be played any time step. The goal is to design a policy for playing the arms in order to maximize the infinite horizon time average expected reward. This problem is an instance of a partially observable Markov decision process (POMDP), and a special case of the notoriously intractable "restless bandit" problem. Unlike the stochastic MAB problem, the feedback MAB problem does not admit to greedy index-based optimal policies. Vie state of the system at any time step encodes the beliefs about the states of different arms, and the policy decisions change these beliefs - this aspect complicates the design and analysis of simple algorithms. We design a constant factor approximation to the feedback MAB problem by solving and rounding a natural LP relaxation to this problem. As far as we are aware, this is the first approximation algorithm for a POMDP problem.
[Algorithm design and analysis, LP relaxation, partial-information based stochastic control, Stochastic processes, Markovian rewards, approximation algorithms, constant factor approximation, multiarmed bandit problem, feedback, restless bandit problem, Transmitters, Feedback, stochastic systems, feedback MAB, partially observable Markov decision process, Infinite horizon, infinite horizon time average expected reward, approximation theory, control system synthesis, Markov chain evolution, Computer science, decision making, Markov processes, Approximation algorithms, Arm, computational complexity]
Beating Simplex for Fractional Packing and Covering Linear Programs
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We give an approximation algorithm for packing and covering linear programs (linear programs with non-negative coefficients). Given a constraint matrix with n non-zeros, r rows, and c columns, the algorithm (with high probability) computes feasible primal and dual solutions whose costs are within a factor of I +epsiv of OPT l+ epsiv of OPT (the optimal cost) in time O(n + (r +c) log(n) / epsiv2). For dense problems (with r,c = O(-radicn)) the time is Omega (n log(n) / epsiv2)-linear even as epsiv rarr 0. In comparison, previous Lagrangian-relaxation algorithms generally take at least Omega(n log(n)/epsiv2) time, while (for small epsiv) the Simplex algorithm typically takes at least Omega(n min(r, c)) time.
[linear programs, Lagrangian-relaxation algorithms, approximation theory, constraint matrix, Optimized production technology, Linear programming, Data structures, linear programming, dense problems, Lagrangian functions, bin packing, Computer science, Constraint optimization, relaxation theory, approximation algorithm, Cost function, Approximation algorithms, fractional packing]
A primal-dual randomized algorithm for weighted paging
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
In the weighted paging problem there is a weight (cost) for fetching each page into the cache. We design a randomized O(log k) -competitive online algorithm for the weighted paging problem, where k is the cache size. This is the first randomized o(k)-competitive algorithm and its competitiveness matches the known lower bound on the problem. More generally, we design an O(log(k/(k - h + I)))-competitive online algorithm for the version of the. problem where, the online algorithm has-cache size k and the online algorithm has cache size h les k. Weighted paging is a special case (weighted star metric) of the well known k-server problem for which it is a major open question whether randomization can be useful in obtaining sub-linear competitive algorithms. Therefore, abstracting and extending the insights from paging is a key step in the resolution of the k-server problem. Our solution for the weighted paging problem is based on a two-step approach. In the first step we obtain an O(log k)-competitive fractional algorithm which is based on a novel online primal-dual approach. In the second step we. obtain a randomized algorithm by rounding online the fractional solution to an actual distribution on integral cache, solutions. We conclude with a randomized O(log N)-competitive algorithm for the well studied Metrical Task System problem (MTS) on a metric defined by a weighted star on N leaves, improving upon a previous O(log2 N)-competitive algorithm of Blum et al. [9].
[Algorithm design and analysis, paged storage, Costs, primal-dual randomized algorithm, Extraterrestrial measurements, History, k-server problem, sub-linear competitive algorithm, randomised algorithms, Computer science, weighted paging problem, metrical task system problem, Space exploration, randomized O(log k) -competitive online algorithm, computational complexity]
Finding Disjoint Paths in Expanders Deterministically and Online
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We describe a deterministic, polynomial time algorithm for finding edge-disjoint paths connecting given pairs of vertices in an expander. Specifically, the input of the algorithm is a sufficiently strong d-regular expander G on n vertices, and a sequence of pairs s<sub>i</sub>, t<sub>i</sub> (1lesilesr) of vertices, where, r=Theta(nd log d/log n), and no vertex appears more than d/3 times in the list of all endpoints s1, t1,... ,s<sub>r</sub>,t<sub>r</sub>. The algorithm outputs edge-disjoint paths Q<sub>1</sub>,...,Q<sub>r</sub>, where Q<sub>i</sub> connects s<sub>i</sub> and t<sub>i</sub>. The paths are constructed online, that is, the algorithm produces Q<sub>i</sub> as soon as it gets s<sub>i,</sub> t<sub>i</sub> and before the next requests in the sequence are revealed. This improves in several respects a long list of previous algorithms for the above problem, whose study is motivated by the investigation of communication networks. An analogous result is established for vertex disjoint paths in blowups of strong expanders.
[Context, polynomials, communication networks, edge-disjoint paths, deterministic polynomial time algorithm, Mathematics, Graph theory, Computer science, vertex disjoint paths, USA Councils, strong expanders, Computer architecture, Polynomials, Communication networks, Joining processes]
Almost Tight Bound for the Union of Fat Tetrahedra in Three Dimensions
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We show that the combinatorial complexity of the. union of n "fat" tetrahedra in 3-space (i.e., tetrahedra all of whose solid angles are at least .some fixed constant) of arbitrary sizes, is O(n2+epsiv),for any epsiv &gt; 0: the bound is almost tight in the worst case, thus almost settling a conjecture of Pach el al. [24]. Our result extends, in a significant way, the result of Pach et al. [24] for the restricted case of nearly congruent cubes. The analysis uses cuttings, combined with the Dobkin-K'irkpatrick hierarchical decomposition of convex polytopes, in order to partition space into subcells, so that, on average, the overwhelming majority of the tetrahedra intersecting a subcell Delta behave as fat dihedral wedges in Delta. As an immediate corollary, we obtain that the combinatorial complexity of the union of n cubes in R3 having arbitrary side lengths, is O(n2+epsiv), for any epsiv &gt; 0 again, significantly extending the result of [24]. Our analysis can easily he extended to yield a nearly-quadratic bound on the complexity of the union of arbitrarily oriented fat triangular prisms (whose cross-sections have, arbitrary sizes) in R3. Finally, we show that a simple variant of our analysis implies a nearly-linear bound on the complexity of the union of fat triangles in the plane.
[computational geometry, Dobkin-Kirkpatrick hierarchical decomposition, fat dihedral wedge, combinatorial complexity, Computer science, Motion planning, Upper bound, USA Councils, fat tetrahedra, Solids, Meeting planning, three dimension, Robots, computational complexity]
Inferring Local Homology from Sampled Stratified Spaces
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We study the reconstruction of a stratified space from a possibly noisy point sample. Specifically, we use the vineyard of the distance function restricted to a 1-parameter family of neighborhoods of a point to assess the local homology of the stratified space at that point. We prove the correctness of this assessment under the assumption of a sufficiently dense sample. We also give an algorithm that constructs the vineyard and makes the local assessment in time at most cubic in the size of the Delaunay triangulation of the point sample.
[Clouds, stratified space reconstruction, local homology inferring, topological data analysis, computational geometry, power diagrams, Mathematics, Fractals, vineyard, local homology, Power measurement, Bioinformatics, Topological data analysis, Computational biology, algorithms., Data analysis, distance function, Extraterrestrial phenomena, data analysis, topology, Extraterrestrial measurements, mesh generation, Delaunay triangulation, simplicial complexes, Computer science, stratified spaces, Delaunay triangulations, persistence, Voronoi decomposition]
Testing for Concise Representations
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We describe a general method for testing whether a function on n input variables has a concise representation. The approach combines ideas from the junta test of Fischer et al. 16 with ideas from learning theory, and yields property testers that make po!y(s/epsiv) queries (independent of n) for Boolean function classes such as s-term DNF formulas (answering a question posed by Parnas et al. [12]), sizes. decision trees, sizes Boolean formulas, and sizes Boolean circuits. The method can be applied to non-Boolean valued function classes as well. This is achieved via a generalization of the notion of van at ion/row Fischer et al. to non-Boolean functions. Using this generalization we extend the original junta test of Fischer et al. to work for non-Boolean functions, and give poly(s/e)-query testing algorithms for non-Boolean valued function classes such as sizes algebraic circuits and s-sparse polynomials over finite fields. We also prove an Omega(radic(s)) query lower bound for nonadaptively testing s-sparse polynomials over finite fields of constant size. This shows that in some instances, our general method yields a property tester with query complexity that is optimal (for nonadaptive algorithms) up to a polynomial factor.
[nonBoolean valued function, sizes algebraic circuits, Input variables, polynomials, size-s decision trees, Binary decision diagrams, s-sparse polynomials, query complexity, yields property testers, concise representations testing, Circuit testing, Galois fields, size-s Boolean circuits, Computer science, Boolean functions, size-s Boolean formulas, Neural networks, Polynomials, Decision trees, learning theory, sparse matrices]
Strong Lower Bounds for Approximating Distribution Support Size and the Distinct Elements Problem
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We consider the problem of approximating the support size of a distribution from a small number of samples, when each element in the distribution appears with probability at least 1/n. This problem is closely related to the problem of approximating the number of distinct elements in a sequence of length n. For both problems, we prove a nearly linear in n lower bound on the query complexity, applicable even for approximation with additive error. At the heart of the lower bound is a construction of two positive integer random variables. X<sub>1</sub> and X<sub>2</sub>, with very different expectations and the following condition on the first k moments: E[X<sub>1</sub>]/E[X<sub>2</sub>] = E[X<sub>1</sub> 2]/E[X<sub>2</sub> 2] = ... = E[X<sub>1</sub> k]/E[X<sub>2</sub> k]. Our lower bound method is also applicable to other problems. In particular, it gives new lower bounds for the sample complexity of (1) approximating the entropy of a distribution and (2) approximating how well a given string is compressed by the Lempel-Ziv scheme.
[Decision support systems, Heart, approximation theory, data compression, boundary-elements methods, probability, distinct elements problem, query complexity, Entropy, Data mining, statistical distributions, Design optimization, Computer science, strong lower bound, distribution support size, Databases, Lempel-Ziv scheme, Statistical distributions, Approximation algorithms, Random variables]
Testing Expansion in Bounded-Degree Graphs
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We consider the problem of testing expansion in bounded degree graphs. We focus on the notion of vertex-expansion: an alpha-expander is a graph G = (V, E) in which even-subset U sube V of at most |V|/2 vertices has a neighborhood of size at least alphaldr|U|. Our main result is that one can distinguish good expanders from graphs that are far from being weak expanders in time O tilde(radicn). We prove that the property testing algorithm proposed by Goldreich and Ron (2000) with appropriately set parameters accepts every alpha-expander with probability at least 2/3 and rejects every graph that is epsiv-far from an alpha*-expander with probability at least 2/3, where alpha*=Theta(alpha2/(d2log (n/epsiv))) and d is the maximum degree of the graphs. The algorithm assumes the bounded-degree graphs model with adjacency list graph representation and its running time is O(d2(radicn log (n/epsiv))/alpha2epsiv3).
[Algorithm design and analysis, decision problem, Transmission line matrix methods, decision theory, graph theory, Formal languages, probability, Graph theory, Mathematics, vertex-testing expansion, set theory, bounded-degree graph model, Sparse matrices, property testing algorithm, Computer science, probability method, Sampling methods, graph representation, statistical testing, Testing, computational complexity]
Approximate Hypergraph Partitioning and Applications
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We show that any partition-problem of hypergraphs has an O(n) time approximate partitioning algorithm and an efficient property tester. This extends the results of Goldreich, Goldwasser and Ron who obtained similar algorithms for the special case of graph partition problems in their seminal paper (1998). The partitioning algorithm is used to obtain the following results: ldr We derive a surprisingly simple O(n) time algorithmic version of Szemeredi's regularity lemma. Unlike all the previous approaches for this problem which only guaranteed to find partitions of tower-size, our algorithm will find a small regular partition in the case that one exists; ldr For any r ges 3, we give an O(n) time randomized algorithm for constructing regular partitions of r-uniform hypergraphs, thus improving the previous O(n2r-1) time (deterministic) algorithms. The property testing algorithm is used to unify several previous results, and to obtain the partition densities for the above problems (rather than the partitions themselves) using only poly(1/isin) queries and constant running time.
[Algorithm design and analysis, regularity lemma, property tester, Density measurement, time approximate partitioning algorithm, graph theory, Graph theory, Time measurement, Partitioning algorithms, Application software, Computer science, Upper bound, graph partition problems, approximate hypergraph partitioning, time randomized algofor, Testing, computational complexity]
Sparse Random Linear Codes are Locally Decodable and Testable
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We show that random sparse binary linear codes are locally testable and locally decodable (under any linear encoding) with constant queries (with probability tending to one). By sparse, we mean that the code should have only polynomially many codewords. Our results are the first to show that local decodability and testability can be found in random, unstructured, codes. Previously known locally decodable or testable codes were either classical algebraic codes, or new ones constructed very carefully. We obtain our results by extending the techniques of Kaufman and Litsyn [11] who used the MacWilliams Identities to show that "almost-orthogonal" binary codes are locally testable. Their definition of almost orthogonality expected codewords to disagree in n/2 plusmn O(radicn) coordinates in codes of block length n. The only families of codes known to have this property were the dual-BCH codes. We extend their techniques, and simplify them in the process, to include codes of distance at least n/2 - O(n1-gamma) for any gamma &gt; 0, provided the number of codewords is O(nt) for some constant t. Thus our results derive the local testability of linear codes from the classical coding theory parameters, namely the rale and the distance of the codes. More significantly, we show that this technique can also be used to prove the "self-correctability" of sparse codes of sufficiently large distance. This allows us to show that random linear codes under linear encoding functions are locally decodable. This ought to be surprising in that the definition of a code doesn't specify the encoding function used! Our results effectively say that any linear function of the bits of the codeword can be locally decoded in this case.
[binary codes, linear codes, sparse random linear codes, locally testable, Encoding, Decoding, Computer science, random codes, Linear code, dual-BCH codes, classical coding theory, USA Councils, locally decodable, Binary codes, Error correction codes, BCH codes, Probes, Testing, codewords, algebraic codes]
Minimizing Average Flow-time : Upper and Lower Bounds
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We consider the problem of minimizing average flow time on multiple machines when each job can be assigned only to a specified subset of the machines. This is a special case of scheduling on unrelated machines and we show that no online algorithm can have a bounded competitive ratio. We provide an O(log P)-approximation algorithm by modifying the single-source unsplittable flow algorithm of Dinitz, et.al. Here P is the ratio of the maximum to the minimum processing times. We establish an Omega(log P)-integrality gap for our LP-relaxation and use this to show an Omega(log P/log log P) lower bound on the approximability of the problem. We then extend the hardness results to the problem of minimizing flow time on parallel machines and establish the first non-trivial lower bounds on the approximability; we show that the problem cannot be approximated to within Omega(radiclog P/log log P).
[O(log P)-approximation algorithm, multiple machines, unrelated machine scheduling, Parallel machines, Time measurement, Security, parallel machines, Scheduling algorithm, processor scheduling, Computer science, LP-relaxation, Fluid flow measurement, average flow-time minimization, Approximation algorithms, Omega(log P)-integrality gap, Polynomials, single-source unsplittable flow algorithm, minimisation, Web server, Single machine scheduling, computational complexity]
Non-Preemptive Min-Sum Scheduling with Resource Augmentation
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We give the first O(l)-speed O(l) approximation polynomial-time algorithms for several nonpreemptive min-sum scheduling problems where jobs arrive over time and must be processed on one machine. More precisely, we give the first O(l)-speed O(l)-approximations for the non-preemptive scheduling problems; l|r<sub>j</sub>| Sigmaw<sub>j</sub>F<sub>j</sub> (weighted flow time), l |r<sub>j</sub>| SigmaT<sub>j</sub> (total tardiness), the broadcast version of 1 |r<sub>j</sub>| Sigmaw<sub>j</sub>F<sub>j</sub> , an O(I)-speed, 1-approximation for l |r<sub>j</sub>| Sigma U macr<sub>j</sub> (throughput maximization), and an O(l)-machine, O(l)-speed O(1)-approximation for l |r<sub>j</sub>| Sigmaw<sub>j</sub>T<sub>j</sub> (weighted tardiness). Our main contribution is an integer programming formulation whose relaxation is sufficiently close to the integer optimum, and which can be transformed to a schedule on a faster machine.
[Algorithm design and analysis, approximation theory, integer programming, Optimized production technology, Optimal scheduling, nonpreemptive min-sum scheduling, Linear programming, resource augmentation, Scheduling algorithm, Computer science, Processor scheduling, polynomial approximation, integer programming formulation, Broadcasting, scheduling, approximation algorithm, Approximation algorithms, Polynomials, polynomial-time algorithm]
On the Advantage over Random for Maximum Acyclic Subgraph
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
In this paper we present a new approximation algorithm for the Max Acyclic Subgraph problem. Given an instance where the maximum acyclic subgraph contains 1/2 + delta fraction of all edges, our algorithm finds an acyclic subgraph with 1/2 + Omega(delta/ log n) fraction of all edges.
[Computer science, Engineering profession, graph theory, random processes, approximation algorithm, Approximation algorithms, maximum acyclic subgraph, Data mining, Mathematical programming]
Buy-at-Bulk Network Design with Protection
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We consider approximation algorithms for buy-at-bulk network design, with the additional constraint that demand pairs be protected against edge or node failures in the network. In practice, the most popular model used in high speed telecommunication networks for protection against failures, is the so-called 1+1 model. In this model, two edge or node-disjoint paths are provisioned for each demand pair. We obtain the first non-trivial approximation algorithms for buy-at-bulk network design in the 1+1 model for both edge and node-disjoint protection requirements. Our results are for the single-cable cost model, which is prevalent in optical networks. More specifically, we present a constant-factor approximation for the single-sink case, and an O(log3 n) approximation for the multi-commodity case. These results are of interest for practical applications and also suggest several new challenging theoretical problems.
[Algorithm design and analysis, Costs, buy-at-bulk network design, node failure protection, Optical fiber cables, Telecommunication traffic, Optical fiber networks, telecommunication network topology, high speed telecommunication network, Computer science, Optical design, Bandwidth, optical network, optical fibre networks, approximation algorithm, Approximation algorithms, Protection, computational complexity]
Space-Efficient Identity Based EncryptionWithout Pairings
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Identity Based Encryption (IBE) systems are often constructed using bilinear maps (a.k.a. pairings) on elliptic curves. One exception is an elegant system due to Cocks which builds an IBE based on the quadratic residuosity problem modulo an RSA composite N. The Cocks system, however, produces long ciphertexts. Since the introduction of the Cocks system in 2001 it has been an open problem to construct a space efficient IBE system without pairings. In this paper we present an IBE system in which ciphertext size is short: an encryption of an f.-bit message consists of a single element in Z/NZ plus lscr + 1 additional bits. Security, as in the Cocks system, relies on the quadratic residuosity problem. The system is based on the theory of ternary quadratic forms and as a result, encryption and decryption are slower than in the Cocks system.
[Identity-based encryption, ternary quadratic form theory, Cocks system, random processes, bilinear map, Application software, space-efficient identity based encryption system, Computer science, cipher text, Elliptic curves, quadratic residuosity problem modulo, public key cryptography, Public key, Public key cryptography, Elliptic curve cryptography, random oracle model, Computer security, RSA cryptography, elliptic curve cryptography]
Round Complexity of Authenticated Broadcast with a Dishonest Majority
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Broadcast among n parties in the presence of t ges n/3 malicious parties is possible only with some additional setup. The most common setup considered is the existence of a PKI and secure, digital signatures, where so-called authenticated broadcast is achievable for any t &lt; n. It is known that t + 1 rounds are necessary and sufficient for deterministic protocols achieving authenticated broadcast. Recently, however, randomized protocols running in expected constant rounds have been shown for the case of t &lt; n/2. It has remained open whether randomization can improve the round complexity when an honest majority is not present. We address this question and show upper/lower bounds on how much randomization can help: ldr For t les n/2 + k, we. show a randomized broadcast protocol that runs in expected O(k2) rounds. In particular, we obtain expected constant-round pivtocols for t = n/2 + O(1). ldr On the negative side, we show that even randomized protocols require Omega(2n/(n-t)) rounds. This in particular rules out expected constant-round protocols when the fraction of honest parties is sub-constant.
[authenticated broadcast round complexity, cryptographic protocols, Computational modeling, dishonest majority, Mathematics, Distributed computing, deterministic protocols, randomized protocols, Resilience, Cryptographic protocols, broadcasting, PKI, Computer science, public key cryptography, Broadcasting, Polynomials, digital signatures, Cryptography, Digital signatures, computational complexity]
Finding Collisions in Interactive Protocols - A Tight Lower Bound on the Round Complexity of Statistically-Hiding Commitments
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We study the round complexity of various cryptographic protocols. Our main result is a tight lower bound on the round complexity of any fully-black-box construction of a statistically-hiding commitment scheme from oneway permutations, and even front trapdoor permutations. This lower bound matches the round complexity of the statistically-hiding commitment scheme due to Naor, Ostrovsky, Venkatesan and Yung (CRYPTO '92). As a corollary, we derive similar tight lower bounds for several other ctyptographicprotocols, such as single-server private information retrieval, interactive hashing, and oblivious transfer that guarantees statistical security for one of the parties. Our techniques extend the collision-finding oracle due to Simon (EUROCRYPT '98) to the setting of interactive protocols (our extension also implies an alternative proof for the main property of the original oracle). In addition, we substantially extend the reconstruction paradigm of Gennaro and Trevisan (FOCS '00). In both cases, our extensions are quite delicate and may be found useful in proving additional black-box separation results.
[fully-black-box construction, cryptographic protocols, Information retrieval, cryptographic protocol, interactive hashing, Mathematics, round complexity, Gas insulated transmission lines, Cryptographic protocols, Computer science, collision-finding oracle, Upper bound, Algorithms, statistically-hiding commitment scheme, Information security, Polynomials, Cryptography, statistical analysis, single-server private information retrieval, interactive protocol, computational complexity]
Lower Bounds on Signatures From Symmetric Primitives
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We show that every black-box construction of one-time signature schemes from a random oracle achieves security at most poly(q)2q. where q is the total number of queries to the oracle by the generation, signing, and verification algorithms. That is, any such scheme can be broken with probability close to 1 by a (computationally unbounded) adversary making poly(q)2q queries to the oracle. This is tight up to a constant factor in the number of queries, since a simple modification of Lamport's scheme achieves 2(0.812-o(1))q security using q queries. Our results extend (with a loss of a constant factor in the number of queries) also to the random permutation and ideal-cipher oracles, and so can be taken as evidence of an inherent efficiency gap between signature schemes and symmetric primitives such as block ciphers, hash functions, and message authentication codes.
[codes, Lattices, black-box construction, ideal-cipher oracles, hash functions, block ciphers, query processing, random permutation, message authentication codes, Cost function, symmetric primitives, Computer security, Lamport scheme, Message authentication, Digital signatures, Random number generation, probability, cryptography, lower bounds, Computer science, random oracle, oracle queries, Public key, Public key cryptography, digital signatures, one-time signature schemes, computational complexity]
Approximation Algorithms Using Hierarchies of Semidefinite Programming Relaxations
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We. introduce, a framework for studying semidefiniie programming (SOP) relaxations based on the Lasserre hierarchy in the context of approximation algorithms for combinatorial problems. As an application of our approach, we give, improved approximation algorithms for two problems. We show that for some fixed constant epsiv &gt; 0, given a 3-uniform hypergraph containing an independent set of size (1/2 - epsiv)v, we can find an independent set of size Omega(nepsiv). This improves upon the result of Krivelevich, Nathaniel and Sitdakov, who gave an algorithm finding an independent set of size Omega(n6gamma-3) for hypergraphs with an independent set of size gamman (but no guarantee for gamma les 1/2). We also give an algorithm which finds an O(n0.2072)-coloring given a 3-colorable graph, improving upon the work of Aurora, Clamtac and Charikar. Our approach stands in contrast to a long series of inapproximability results in the Lovasz Schrijver linear programming (LP) and SDP hierarchies for other problems.
[Algorithm design and analysis, approximation theory, Law, combinatorial problem, Linear programming, linear programming, Electronic mail, set theory, Application software, 3-colorable hypergraph, graph colouring, semidefinite programming relaxation, Design optimization, Computer science, relaxation theory, Lovasz Schrijver linear programming, approximation algorithm, Approximation algorithms, Lasserre hierarchy, Polynomials, Legal factors]
Integrality gaps of 2 - o(1) for Vertex Cover SDPs in the Lov&#x0E9;sz-Schrijver Hierarchy
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Linear and semidefinite programming are highly successful approaches for obtaining good approximations for NP-hard optimization problems. For example, breakthrough approximation algorithms for Max Cut and Sparsest Cut use semidefinite programming. Perhaps the most prominent NP-hard problem whose exact approximation factor is still unresolved is Vertex Cover. PCP-based techniques of Dinur and Safra [7] show that it is not possible to achieve a factor better than 1.36; on the other hand no known algorithm does better than the factor of 2 achieved by the simple greedy algorithm. Furthermore, there is a widespread belief that SDP technicptes are the most promising methods available for improving upon this factor of 2. Following a line of study initiated by Arora et al. [3], our aim is to show that a large family of LP and SDP based algorithms fail to produce an approximation for Vertex Cover better than 2. Lovasz and Schrijver [21] introduced the systems LS and LS<sub>+</sub>for systematically tightening LP and SDP relaxations, respectively, over many rounds. These systems naturally capture large classes of LP and SDP relaxations; indeed, LS<sub>+</sub> captures the celebrated SDP-based algorithms for Max Cur and Sparsest Cur mentioned above. We rule out polynomial-time 2 - Omega(lfloor) approximations for Vertex Cover using LS<sub>+</sub>. In particular, we prove an integrality gap of 2 - o(lfloor)for Vertex Cover SDPs obtained by tightening the standard LP relaxation with Omega(radiclog n/ log log n) rounds of LS<sub>+</sub>. While tight integrality gaps were known for Vertex Cover in the weaker LS system [23 ], previous results did not rule out a2 - Omega(1) approximation after even two rounds of LS<sub>+</sub>.
[Greedy algorithms, Algorithm design and analysis, polynomial time approximation, vertex cover, Sparsest Cut, graph theory, Educational institutions, Linear programming, linear programming, Design optimization, Lovasz-Schrijver hierarchy, Computer science, semidefinite programming, NP-hard problem, Max Cut, integrality gaps, NP-hard optimization problem, Approximation algorithms, Polynomials, Erbium, computational complexity]
Local Global Tradeoffs in Metric Embeddings
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
Suppose that every k points in a metric space X are D-distortion embeddable into <sub>lscr</sub> <sub>1</sub>. We give upper and lower bounds on the distortion required to embed the entire space X into <sub>lscr</sub> <sub>1</sub>. This is a natural mathematical question and is also motivated by the study of relaxations obtained by lift-and-project methods for graph partitioning problems. In this setting, we show that X can be embedded into <sub>lscr</sub> <sub>1</sub> with distortion O(D times log(|X|/k)). Moreover, we give a lower bound showing that this result is tight if D is bounded away from I. For D = 1 + delta we give a lower bound of Omega(log(|X|/k/ log( 1/delta)); and for D = 1, we give a lower bound of Omega( log |X|/(log k +log log | X|)). Our bounds significantly improve on the results of Arora, Jjovdsz, Newman, Rabani, Rabinovich and Vempala, who initiated a study of these questions.
[Embedded computing, Engineering profession, local global tradeoffs, D-distortion, graph theory, Extraterrestrial measurements, Partitioning algorithms, mathematical analysis, Application software, X distortion, mathematical question, Computer science, metric space, lift-and-project methods, Approximation algorithms, graph partitioning pmblems, Mathematical programming, distortion, metric embeddings]
The Computational Hardness of Estimating Edit Distance [Extended Abstract]
48th Annual IEEE Symposium on Foundations of Computer Science
None
2007
We prove the first non-trivial communication complexity lower bound for the problem of estimating the edit distance (aka Levenshtein distance) between two strings. A major feature of our result is that it provides the first setting in which the complexity of computing the edit distance is provably larger than that of Hamming distance. Our lower bound exhibits a trade-off between approximation and communication, asserting, for example, thai protocols with O(1) bits of communication can only obtain approximation a ges Omega(log d/log log d), where d is the length of the input strings. This case of O(1) communication is of particular importance, since it captures constant-size sketches as well as embaddings into spaces like L<sub>1</sub> and squared-L<sub>2</sub>. two prevailing algorithmic approaches for dealing with edit distance. Furthermore, the bound holds not only for strings over alphabet Sigma= {0, 1}, but also for strings that are permu-tations (called the Ulam metric). Besides being applicable to a much richer class of algorithms than all previous results, our bounds are near-tight in at. least one case, namely of embedding permutations into L<sub>1</sub>. The proof uses a new technique, that relies on Fourier analysis in a rather elementary way.
[Algorithm design and analysis, Hamming distance, Computational modeling, Complexity theory, Levenshtein distance, Nearest neighbor searches, Computer science, string edit distance estimation, nontrivial communication complexity lower bound, Biology computing, Approximation algorithms, Polynomials, string matching, Computational biology, computational complexity]
Foreword
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Presents the introductory welcome message from the conference proceedings.
[Computer science, Publishing, Feedback, Meetings, Abstracts, Production, Conference management, Computer Society, Radio access networks]
Committees
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Provides a listing of current committee members.
[]
The Polynomial Method in Quantum and Classical Computing
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
In 1889, A. A. Markov proved a powerful result about low-degree real polynomials: roughly speaking, that such polynomials cannot have a sharp jump followed by a long, relatively flat part. A century later, this result - as well as other results from the field of approximation theory - came to play a surprising role in classical and quantum complexity theory. In this article, the author tries to tell this story in an elementary way, beginning with classic results in approximation theory and ending with some recent applications.
[approximation theory, quantum, Complexity theory, Approximation methods, classical complexity theory, Computational complexity, polynomial method, Computer science, Quantum computing, Boolean functions, Numerical analysis, polynomial approximation, Quantum mechanics, quantum computing, classical computing, Polynomials, quantum complexity theory, computational complexity]
Theory of Sponsored Search Auctions
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Web search engines are becoming an increasingly important advertising medium. When a user poses a query in addition to search results, the search engine also returns a few advertisements. On most major search engines, the choice and assignment of ads to positions is determined by an auction among all advertisers who placed a bid on some keyword that matches the query. The user might click on one or more of the ads, in which case (in the pay-per-click model) the advertiser receiving the click pays the search engine a price determined by the auction.
[search engines, advertising, Computer science, Degradation, query processing, Web search engines, sponsored search auctions, Investments, Pricing, Search engines, Marketing and sales, Internet, Advertising, Portfolios, Web search, electronic commerce]
Average-case Complexity
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We review the many open questions and the few things that are known about the average-case complexity of computational problems. We shall follow the presentations of Impagliazzo, of Goldreich, and of Bogdanov and the author, and focus on the following subjects. (i). Average-case tractability. What does it mean for a problem to have an "efficient on average'' algorithm with respect to a distribution of instances? There is more than one ``correct'' answer to this question, and a numberof subtleties arise, which are interesting to discuss. (ii) Worst case versus average-case. Is the existence of hard-on-averageproblems in a complexity class equivalent to the existence of worst-case-hardproblems? This is the case for complexity classes like PSPACE and EXP, but it is openfor NP, with partial evidence pointing to a negative answer. (To be sure, we believethat hard-on-average, and also worst-case hard problems, exist in NP, and if so theirexistence is ``equivalent'' in the way two true statements are logically equivalent. There is, however, partial evidence that such an equivalence cannot be establishedvia reductions. It is also known that such an equivalence cannot be established viaany relativizing technique.) (iii) Amplification of average-case hardness. A weak sense in which aproblem may be hard-on-average is that every efficient algorithm fails on a noticeable(at least inverse polynomial) fraction of inputs; a strong sense is that noalgorithm can do much better than guess the answer at random. In many settings,the existence of problems of weak average-case complexity implies the existenceof problems, in the same complexity class, of strong average-case complexity.It remains open to prove such equivalence in the setting of uniform algorithmsfor problems in NP. (Some partial results are known even in this setting.) (iv) Reductions and Completeness. Levin initiated a theoryof completeness for distributional problems under reductions that preserveaverage-case tractability. Even establishing the existence of an NP-completeproblem in this theory is a non-trivial (and interesting) result.
[average-case tractability, reductions, average-case hardness, NP problems, Complexity theory, completeness, NP-complete problem, Computational complexity, Computer science, average-case complexity, Writing, Polynomials, worst-case hardness, computational complexity]
Truthful Approximation Schemes for Single-Parameter Agents
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We present the first monotone randomized polynomial-time approximation scheme (PTAS) for minimizing the makespan of parallel related machines (Q||C<sub>max</sub>), the paradigmatic problem in single-parameter algorithmic mechanism design. This result immediately gives a polynomial-time, truthful (in expectation) mechanism whose approximation guarantee attains the best-possible one for all polynomial-time algorithms (assuming P not equal to NP). Our algorithmic techniques are flexible and also yield, among other results, a monotone deterministic quasi-PTAS for Q||C<sub>max</sub> and a monotone randomized PTAS for max-min scheduling on related machines.
[Algorithm design and analysis, algorithmic techniques, Costs, Algorithmic Mechanism Design, Monotone Algorithms, minimax techniques, randomized polynomial-time approximation scheme, parallel machines, Single Parameter Agents, scheduling, polynomial-time algorithms, Polynomials, truthful approximation schemes, Engineering profession, monotone randomized PTAS, Peer to peer computing, single-parameter algorithmic mechanism design, parallel related machines, Scheduling, Approximation Algorithms, Application software, deterministic algorithms, Scheduling algorithm, randomised algorithms, Computer science, Approximation algorithms, max-min scheduling, Resource management, monotone deterministic quasi-PTAS, single-parameter agents, computational complexity]
Discretized Multinomial Distributions and Nash Equilibria in Anonymous Games
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show that there is a polynomial-time approximation scheme for computing Nash equilibria in anonymous games with any fixed number of strategies (a very broad and important class of games), extending the two-strategy result of Daskalakis and Papadimitriou 2007. The approximation guarantee follows from a probabilistic result of more general interest: The distribution of the sum of n independent unit vectors with values ranging over {e<sub>1</sub>,...,ek}, where e<sub>i</sub> is the unit vector along dimension i of the k-dimensional Euclidean space, can be approximated by the distribution of the sum of another set of independent unit vectors whose probabilities of obtaining each value are multiples of 1/z for some integer z, and so that the variational distance of the two distributions is at most eps, where eps is bounded by an inverse polynomial in z and a function of k, but with no dependence on n. Our probabilistic result specifies the construction of a surprisingly sparse epsi-cover- under the total variation distance - of the set of distributions of sums of independent unit vectors, which is of interest on its own right.
[Algorithm design and analysis, approximation theory, polynomial-time approximation scheme, probability, anonymous games, game theory, Nash equilibrium, Game theory, Distributed computing, Computer science, PTAS, Stein's Method, Voting, discretized multinomial distributions, Nash equilibria, Anonymous games, Multinomial Approximations, Approximation algorithms, k-dimensional Euclidean space, Polynomials, Internet, computational complexity]
Approximation Algorithms for Single-minded Envy-free Profit-maximization Problems with Limited Supply
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We present the first polynomial-time approximation algorithms for single-minded envy-free profit-maximization problems (Guruswami et al., 2005) with limited supply. Our algorithms return a pricing scheme and a subset of customers that are designated the winners, which satisfy the envy-freeness constraint, whereas in our analyses, we compare the profit of our solution against the optimal value of the corresponding social-welfare-maximization (SWM) problem of finding a winner-set with maximum total value. Our algorithms take any LP-based alpha-approximation algorithm for the corresponding SWM problem as input and return a solution that achieves profit at least OPT/O (alpha ldr log u<sub>max</sub>), where OPT is the optimal value of the SWM problem, and u<sub>max</sub> is the maximum supply of an item. This immediately yields approximation guarantees of O(radicmlog u<sub>max</sub>) for the general single-minded envy-free problem; and O(log u<sub>max</sub>) for the tollbooth and highway problems (Guruswami et al., 2005), and the graph-vertex pricing problem (Balcan and Blum, 2006) (alpha = O(1) for all the corresponding SWM problems). Since OPT is an upper bound on the maximum profit achievable by any solution (i.e., irrespective of whether the solution satisfies the envy-freeness constraint), our results directly carry over to the non-envy-free versions of these problems too. Our result also thus (constructively) establishes an upper bound of O(alpha ldr log u<sub>max</sub>) on the ratio of (i) the optimum value of the profit-maximization problem and OPT; and (ii) the optimum profit achievable with and without the constraint of envy-freeness.
[Algorithm design and analysis, profitability, graph theory, pricing scheme, Displays, social-welfare-maximization, Road transportation, envy-freeness constraint, optimisation, maximum supply, graph-vertex pricing problem, Pricing, Polynomials, LP-based approximation algorithm, Pricing problems, tollbooth problem, Optimized production technology, highway problem, Envy-free profit-maximization, Linear programming, Combinatorial mathematics, single-minded envy-free profit-maximization, Computer science, Upper bound, Algorithms, optimal value, Approximation algorithms, polynomial-time approximation algorithm, Algorithmic game theory, pricing, computational complexity]
Market Equilibria in Polynomial Time for Fixed Number of Goods or Agents
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We consider markets in the classical Arrow-Debreu model. There are n agents and m goods. Each buyer has a concave utility function (of the bundle of goods he/she buys) and an initial bundle. At an ldquoequilibriumrdquo set of prices for goods, if each individual buyer separately ex-changes the initial bundle for an optimal bundle at the set prices, the market clears, i.e., all goods are exactly consumed. Classical theorems guarantee the existence of equilibria, but computing them has been the subject of much recent research. In the related area of Multi-Agent Games,much attention has been paid to the complexity as well as algorithms. While most general problems are hard, polynomial time algorithms have been developed for restricted classes of games, when one assumes the number of strategies is constant.For the Market Equilibrium problem, several important special cases of utility functions have been tackled. Here we begin a program for this problem similar to that for multi-agent games, where general utilities are considered. We begin by showing that if the utilities are separable piece-wise linear concave (PLC) functions, and the number of goods(or alternatively the number of buyers) is constant, then we can compute an exact equilibrium in polynomial time.Our technique for the constant number of goods is to de-compose the space of price vectors into cells using certain hyperplanes, so that in each cell, each buyerpsilas threshold marginal utility is known. Still, one needs to solve a linear optimization problem in each cell. We then show the main result - that for general (non-separable) PLC utilities, an exact equilibrium can be found in polynomial time provided the number of goods is constant. The starting point of the algorithm is a ldquocell-decompositionrdquo of the space of price vectors using polynomial surfaces (instead of hyperplanes).We use results from computational algebraic geometry to bound the number of such cells. For solving the problem inside each cell, we introduce and use a novel LP-duality based method. We note that if the number of buyers and agents both can vary, the problem is PPAD hard even for the very special case of PLC utilities - namely Leontief utilities.
[computational algebraic geometry, market equilibria, Arrow-Debreu model, computational geometry, utility programs, goods, LP-duality, optimisation, optimization, Polynomials, polynomial time, Mathematical model, algorithm, multiagent games, Piecewise linear techniques, polynomials, piecewise linear techniques, Vectors, Computational complexity, Game theory, agents, Computer science, market, Programmable control, Computational geometry, economics, piecewise linear concave functions, industrial economics, exact, concave utility function, Leontief utilities, equilibrium, pricing]
Arithmetic Circuits: A Chasm at Depth Four
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show that proving exponential lower bounds on depth four arithmetic circuits imply exponential lower bounds for unrestricted depth arithmetic circuits. In other words, for exponential sized circuits additional depth beyond four does not help. We then show that a complete black-box derandomization of identity testing problem for depth four circuits with multiplication gates of small fanin implies a nearly complete derandomization of general identity testing.
[circuit complexity, multiplying circuits, Costs, arithmetic circuits, Depth Reduction, Size measurement, logic testing, Circuit testing, Galois fields, complete black-box derandomization, multiplication gates, Information systems, Computer science, exponential lower bounds, Arithmetic Circuits, Identity Testing, digital arithmetic, circuit testing, identity testing problem, Digital arithmetic, Polynomials, Lower Bounds, Computational Complexity, Circuit Complexity]
Dense Subsets of Pseudorandom Sets
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
A theorem of Green, Tao, and Ziegler can be stated (roughly) as follows: ifR is a pseudorandom set, and D is a dense subset of R, then D may be modeled by a set M that is dense in the entire domain such that D and M are indistinguishable. (The precise statement refers to"measures" or distributions rather than sets.) The proof of this theorem is very general, and it applies to notions of pseudo-randomness and indistinguishability defined in terms of any family of distinguishers with some mild closure properties. The proof proceeds via iterative partitioning and an energy increment argument, in the spirit of the proof of the weak Szemeredi regularity lemma. The "reduction" involved in the proof has exponential complexity in the distinguishing probability. We present a new proof inspired by Nisan's proof of Impagliazzo's hardcore set theorem. The reduction in our proof has polynomial complexity in the distinguishing probability and provides a new characterization of the notion of "pseudoentropy" of a distribution. A proof similar to ours has also been independently discovered by Gowers [2]. We also follow the connection between the two theorems and obtain a new proof of Impagliazzo's hardcore set theorem via iterative partitioning and energy increment. While our reduction has exponential complexity in some parameters, it has the advantage that the hardcore set is efficiently recognizable.
[pseudorandom sets, iterative methods, Additives, Complexity theory, set theory, dense subsets, Research and development, set theorem, distinguishing probability, pseudoentropy, Polynomials, pseudorandomness, Cryptography, additive combinatorics, polynomials, probability, Combinatorial mathematics, exponential complexity, Computer science, iterative partitioning, polynomial complexity, regularity lemmas, indistinguishability, energy increment argument, Arithmetic, computational complexity]
Almost-Natural Proofs
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Razborov and Rudich have shown that so-called "natural proofs" are not useful for separating P from NP unless hard pseudorandom number generators do not exist. This famous result is widely regarded as a serious barrier to proving strong lower bounds in circuit complexity theory. By definition, a natural combinatorial property satisfies two conditions, constructivity and largeness. Our main result is that if the largeness condition is weakened slightly, then not only does the Razborov-Rudich proof break down, but such "almost-natural" (and useful) properties provably exist. Specifically, under the same pseudorandomness assumption that Razborov and Rudich make, a simple, explicit property that we call "discrimination" suffices to separate P/poly from NP; discrimination is nearly linear-time computable and almost large, having density 2-q(n) where q grows slightly faster than a quasi-polynomial function. For those who hope to separate P from NP using random function properties in some sense, discrimination is interesting, because it is constructive, yet may be thought of as a minor alteration of a property of a random function. The proof relies heavily on the self-defeating character of natural proofs. Our proof technique also yields an unconditional result, namely that there exist almost-large and useful properties that are constructive, if we are allowed to call non-uniform low-complexity classes "constructive." We note, though, that this unconditional result can also be proved by a more conventional counting argument.
[circuit complexity, combinatorial mathematics, Circuits, Drives, random function, Complexity theory, hard pseudorandom number generators, Computer science, circuit complexity theory, Boolean functions, combinatorial property, random functions, self-defeating character, naturalization barrier, theorem proving, almost-natural proofs, Razborov-Rudich proof, natural proofs, pseudorandom number generators]
Dynamic Connectivity: Connecting to Networks and Geometry
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Dynamic connectivity is a well-studied problem, but so far the most compelling progress has been confined to the edge-update model: maintain an understanding of connectivity in an undirected graph, subject to edge insertions and deletions. In this paper, we study two more challenging, yet equally fundamental problems: subgraph connectivity asks to maintain an understanding of connectivity under vertex updates: updates can turn vertices on and off, and queries refer to the subgraph induced by "on" vertices. (For instance, this is closer to applications in networks of routers, where node faults may occur.)We describe a data structure supporting vertex updates in O~(m^{2/3}) amortized time, where m denotes the number of edges in the graph. This greatly improves over the previous result [Chan, STOC'02], which required fast matrix multiplication and had an update time of O(m^{0.94}). The new data structure is also simpler. Geometric connectivity asks to maintain a dynamic set of n geometric objects, and query connectivity in their intersection graph. (For instance, the intersection graph of balls describes connectivity in a network of sensors with bounded transmission radius.) Previously, nontrivial fully dynamic results were known only for special cases like axis-parallel line segments and rectangles. We provide similarly improved update times, O~(n^{2/3}), for these special cases. Moreover, we show how to obtain sublinear update bounds for virtually all families of geometric objects which allow sublinear-time range queries. In particular, we obtain the first sublinear update time for arbitrary 2D line segments: O*(n^{9/10}); for d-dimensional simplices: O*(n^{1-1/d(2d+1)}); and for d-dimensional balls: O*(n^{1-1/(d+1)(2d+3)}).
[Solid modeling, intersection graphs, Heuristic algorithms, graph theory, Sensor phenomena and characterization, range queries, Data structures, Turning, Computer science, Computational geometry, Tree graphs, subgraph connectivity, dynamic connectivity, geometric connectivity, Computer networks, edge-update model, undirected graph, Joining processes, computational complexity, edge insertions, intersection graph]
Algorithms for Single-Source Vertex Connectivity
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
In the survivable network design problem (SNDP) the goal is to find a minimum cost subset of edges that satisfies a given set of pairwise connectivity requirements among the vertices. This general network design framework has been studied extensively and is tied to the development of major algorithmic techniques. For the edge-connectivity version of the problem, a 2-approximation algorithm is known for arbitrary pairwise connectivity requirements. However, no non-trivial algorithms are known for its vertex connectivity counterpart. In fact, even highly restricted special cases of the vertex connectivity version remain poorly understood.We study the single-source k-vertex connectivity version of SNDP. We are given a graph G(V,E) with a subset T of terminals and a source vertex s, and the goal is to find a minimum cost subset of edges ensuring that every terminal is k-vertex connected to s. Our main result is an O(k log n)-approximation algorithm for this problem; this improves upon the recent 2O(k 2 )log4 n-approximation. Our algorithm is based on an intuitive rerouting scheme. The analysis relies on a structural result that may be of independent interest: we show that any solution can be decomposed into a disjoint collection of multiple-legged spiders, which are then used to re-route flow from terminals to the source via other terminals.We also obtain the first non-trivial approximation algorithm for the vertex-cost version of the same problem, achieving an O(k7log2 n)-approximation.
[Algorithm design and analysis, Costs, graph theory, network theory (graphs), edge-connectivity version, set theory, multiple-legged spiders, Tree graphs, vertex-cost version, Polynomials, O(k log n)-approximation algorithm, intuitive rerouting scheme, approximation theory, survivable network design problem, Network Design, Vertex Connectivity, graph, minimum cost subset, Computer science, nontrivial approximation algorithm, 2-approximation algorithm, Approximation algorithms, Iterative algorithms, single-source vertex connectivity, Joining processes, computational complexity]
A Polynomial-Time Approximation Scheme for Euclidean Steiner Forest
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We give a randomized O(n2 log n)-time approximation scheme for the Steiner forest problem in the Euclidean plane. For every fixed epsi &gt; 0 and given any n pairs of terminals in the plane, our scheme finds a (1 + epsi)- approximation to the minimum-length forest that connects every pair of terminals.
[Steiner trees, Steiner forest, Costs, polynomial-time approximation scheme, Euclidean plane, Combinatorial mathematics, Computer science, polynomial approximation, approximation algorithm, Approximation algorithms, Euclidean Steiner forest, Polynomials, Dynamic programming, approximation scheme, minimum-length forest, Portals, computational complexity]
Degree Bounded Network Design with Metric Costs
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Given a complete undirected graph, a cost function on edges and a degree bound B, the degree bounded network design problem is to find a minimum cost simple subgraph with maximum degree B satisfying given connectivity requirements. Even for simple connectivity requirement such as finding a spanning tree, computing a feasible solution for the degree bounded network design problem is already NP-hard, and thus there is no polynomial factor approximation algorithm for this problem. In this paper, we show that when the cost function satisfies triangle inequalities, there are constant factor approximation algorithms for various degree bounded network design problems.Global edge-connectivity: There is a (2+1/k)-approximation algorithm for the minimum bounded degree k-edge-connected subgraph problem. Local edge-connectivity: There is a 6-approximation algorithm for the minimum bounded degree Steiner network problem. Global vertex-connectivity: there is a (2+(k-1)/n+1/k)-approximation algorithm for the minimum bounded degree k-vertex-connected subgraph problem. Spanning tree: there is an (1+1/(d-1))-approximation algorithm for the minimum bounded degree spanning tree problem. These approximation algorithms return solutions with smallest possible maximum degree, and the cost guarantee is obtained by comparing to the optimal cost when there are no degree constraints. This demonstrates that degree constraints can be incorporated into network design problems with metric costs.Our algorithms can be seen as a generalization of Christofides' algorithm for metric TSP. The main technical tool is a simplicity-preserving edge splitting-off operation, which is used to "short-cut" vertices with high degree while maintaining connectivity requirements and preserving simplicity of the solutions.
[Algorithm design and analysis, metric cost function, edge splitting-off, network theory (graphs), degree bounded network design, Design engineering, Tree graphs, approximation algorithm, Cost function, Polynomials, Computer networks, undirected graph, network design, minimum bounded degree spanning tree problem, trees (mathematics), Traveling salesman problems, global edge connectivity, minimum bounded degree Steiner network problem, Computer science, graph connectivity, Upper bound, global vertex-connectivity, NP-hard problem, Approximation algorithms, minimisation, degree bounded, computational complexity, constant factor approximation algorithm]
Matrix Sparsification for Rank and Determinant Computations via Nested Dissection
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
The nested dissection method developed by Lipton, Rose, and Tarjan is a seminal method for quickly performing Gaussian elimination of symmetric real positive definite matrices whose support structure satisfies good separation properties (e.g. planar). One can use the resulting LU factorization to deduce various parameters of the matrix. The main results of this paper show that we can remove the three restrictions of being "symmetric\
[arbitrary square matrix, Transmission line matrix methods, Symmetric matrices, Particle separators, Sparse matrices, matrix, rank computations, Computer science, Monte Carlo methods, Tree graphs, NP-hard problem, determinant computations, Gaussian elimination, nested-dissection, Gaussian processes, rank, matrix sparsification, sparse matrices, nested dissection, Arithmetic, computational complexity, determinant]
Fast Modular Composition in any Characteristic
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We give an algorithm for modular composition of degree n univariate polynomials over a finite field F<sub>q</sub> requiring n 1 + o(1) log1 + o(1) q bit operations; this had earlier been achieved in characteristic no(1) by Umans (2008). As an application, we obtain a randomized algorithm for factoring degree n polynomials over F<sub>q</sub> requiring (n1.5 + o(1) + n 1 + o(1) log q) log1 + o(1) q bit operations, improving upon the methods of von zur Gathen &amp; Shoup (1992) and Kaltofen &amp; Shoup (1998). Our results also imply algorithms for irreducibility testing and computing minimal polynomials whose running times are best-possible, up to lower order terms.As in Umans (2008), we reduce modular composition to certain instances of multipoint evaluation of multivariate polynomials. We then give an algorithm that solves this problem optimally (up to lower order terms), in arbitrary characteristic. The main idea is to lift to characteristic 0, apply a small number of rounds of multimodular reduction, and finish with a small number of multidimensional FFTs. The final evaluations are then reconstructed using the Chinese Remainder Theorem. As a bonus, we obtain a very efficient data structure supporting polynomial evaluation queries, which is of independent interest. Our algorithm uses techniques which are commonly employed in practice, so it may be competitive for real problem sizes. This contrasts with previous asymptotically fast methods relying on fast matrix multiplication.
[fast Fourier transforms, Multidimensional systems, Flexible printed circuits, Engineering profession, polynomials, Chinese Remainder Theorem, data structure, Data structures, Mathematics, polynomial factorization, randomized algorithm, irreducibility testing, multivariate polynomials, Application software, modular composition, Galois fields, Computer science, matrix multiplication, univariate polynomials, Polynomials, multidimensional FFT, multipoint evaluation, multimodular reduction, Testing]
Gaussian Bounds for Noise Correlation of Functions and Tight Analysis of Long Codes
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We derive tight bounds on the expected value of products of low influence functions defined on correlated probability spaces. The proofs are based on extending Fourier theory to an arbitrary number of correlated probability spaces, on a generalization of an invariance principle recently obtained with O'Donnell and Oleszkiewicz for multilinear polynomials with low influences and bounded degree and on properties of multi-dimensional Gaussian distributions. Let (X<sub>i</sub> j : 1 les i les k,1 les j les n) be a matrix of random variables whose columns X1,..., Xn are independent and identically distributed and such that any two rows X<sub>i</sub>, X<sub>j</sub> for 1 les inej les k are independent. Assume further that the values that row X<sub>i</sub> takes with non-zero probability are the same no matter how one conditions on the remaining rows X<sub>1</sub>,..., X<sub>i-1</sub>X<sub>i+1</sub>,..., X<sub>k</sub>. Our results show that given k functions f<sub>1</sub>,... , f<sub>k</sub> taking values in [0,1] it holds that |E[Pi<sub>i=1</sub> k f<sub>i</sub> (X<sub>i</sub>)] - Pi<sub>i=1</sub> k E[ fi (X<sub>i</sub>)]| &lt; epsi if all influences of the functions f<sub>i</sub> are smaller than tau(epsi, k) which is independent of n. In words: low influence functions of pairwise independent rows behave like independent random variables. The general statement of our result applies when the rows are not pairwise independent and when (some) of the variables do not have low influences for (some) functions. The results obtained here allow analyzing hyper-graph long-code tests. A number of applications in hardness of approximation assuming the Unique Games Conjecture were obtained using the results derived here in subsequent work by Raghavendra and jointly by Austrin and the author. Our results imply new results on voting schemes in social choice and in additive number theory. In particular we show that among all low influence functions, Majority is asymptotically the most predictable and is (almost) optimal in the context of Condorcet voting.
[|invariance, multidimensional Gaussian distributions, Gaussian distribution, unique games, long codes, Gaussian noise, Voting, Condorcet voting, correlated probability spaces, Economic forecasting, Polynomials, majority, multilinear polynomials, Testing, noise functions correlation, correlation theory, polynomials, long codes tight analysis, Extraterrestrial measurements, Fourier analysis, Gaussian bounds, Application software, predictability, Computer science, Fourier theory, Random variables]
Worst Case to Average Case Reductions for Polynomials
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
A degree-d polynomial p in n variables over a field F is equidistributed if it takes on each of its |F| values close to equally often, and biased otherwise. We say that p has low rank if it can be expressed as a function of a small number of lower degree polynomials. Green and Tao [GT07] have shown that over large fields (i.e when d &lt;|F|) a biased polynomial must have low rank. They have also conjectured that bias implies low rank over general fields, but their proof technique fails to show that. In this work we affirmatively answer their conjecture. Using this result we obtain a general worst case to average case reductions for polynomials. That is, we show that a polynomial that can be approximated by a few polynomials of bounded degree (i.e. a polynomial with non negligible correlation with a function of few bounded degree polynomials), can be computed by a few polynomials of bounded degree. We derive some relations between our results to the construction of pseudorandom generators. Our work provides another evidence to the structure vs. randomness dichotomy.
[average case reductions, Low degree polynomials, Approximation, polynomials, pseudorandom generators, dichotomy, Galois fields, Least squares approximation, Computer science, Finite fields, Polynomials, worst case reductions, Average case to Worst case reductions]
On the Union of Cylinders in Three Dimensions
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show that the combinatorial complexity of the union of n infinite cylinders in R3, having arbitrary radii, is O(n2+epsiv), for any epsiv &gt;0; the bound is almost tight in the worst case, thus settling a conjecture of Agarwal and Sharir, who established a nearly-quadratic bound for the restricted case of nearly congruent cylinders. Our result extends, in a significant way, the result of Agarwal and Sharir, in particular, a simple specialization of our analysis to the case of nearly congruent cylinders yields a nearly-quadratic bound on the complexity of the union in that case, thus significantly simplifying the analysis in. Finally, we extend our technique to the case of "cigars'' of arbitrary radii (that is, Minkowski sums of line-segments and balls), and show that the combinatorial complexity of the union in this case is nearly-quadratic as well. This problem has been studied in for the restricted case where all cigars are (nearly) equal-radii. Based on our new approach, the proof follows almost verbatim from the analysis for infinite cylinders, and is significantly simpler than the proof presented in [3].
[combinatorial mathematics, computational geometry, lower envelope of algebraic surfaces., set theory, 3D infinite cylinder union, combinatorial complexity, Computer science, Motion planning, 1/r-cuttings, arbitrary radii, Upper bound, Geometric arrangements, onion of simply-shaped bodies, congruent cylinder, Robots, computational complexity]
Spherical Cubes and Rounding in High Dimensions
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
What is the least surface area of a shape that tiles Ropfd under translations by Zopfd? Any such shape must have volume 1 and hence surface area at least that of the volume-1 ball, namely Omega(radicd). Our main result is a construction with surface area O(radicd), matching the lower bound up to a constant factor of 2radic2pi/eap3. The best previous tile known was only slightly better than the cube, having surface area on the order of d. We generalize this to give a construction that tiles Ropfd by translations of any full rank discrete lattice Lambda with surface area 2piparV-1par<sub>fb</sub>, where V is the matrix of basis vectors of Lambda, and par.par<sub>fb</sub> denotes the Frobenius norm. We show that our bounds are optimal within constant factors for rectangular lattices. Our proof is via a random tessellation process, following recent ideas of Raz in the discrete setting. Our construction gives an almost optimal noise-resistant rounding scheme to round points in Ropfd to rectangular lattice points.
[least surface area, Shape, Lattices, Frobenius norm, computational geometry, full rank discrete lattice, lattice theory, rounding, rectangular lattice, lower bound, Rounding, Computer science, Foams, Upper bound, Parallel Repetition, Tiles, spherical cubes, Sampling methods, Concrete, random tessellation process, Tiling, Immune system, shape surface area]
Near-Optimal Sparse Recovery in the L1 Norm
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We consider the approximate sparse recovery problem, where the goal is to (approximately) recover a high-dimensional vector xisinRopfn from its lower-dimensional sketch AxisinRopfm. Specifically, we focus on the sparse recovery problem in the L<sub>1</sub> norm: for a parameter k, given the sketch Ax, compute an approximation xcirc of x such that the L<sub>1</sub> approximation error parx-xcircpar<sub>1</sub> is close to min<sub>x'</sub> parx-x'par<sub>1</sub>, where x' ranges over all vectors with at most k terms. The sparse recovery problem has been subject to extensive research over the last few years. Many solutions to this problem have been discovered, achieving different trade-offs between various attributes, such as the sketch length, encoding and recovery times. In this paper we provide a sparse recovery scheme which achieves close to optimal performance on virtually all attributes (see Figure 1). In particular, this is the first recovery scheme that guarantees O(k log(n/k)) sketch length, and near-linear O(n log (n/k)) recovery time simultaneously. It also features low encoding and update times, and is noise-resilient.
[approximation theory, l1 norm, compressed sensing, Data acquisition, expanders, near-optimal sparse recovery, Analog computers, sparse recovery, Encoding, Vectors, high- dimensional vector, streaming algorithms, lower-dimensional sketch, Computer science, Linearity, Computer errors, Approximation error, Hardware, Compressed sensing]
On Basing Lower-Bounds for Learning on Worst-Case Assumptions
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We consider the question of whether P ne NP implies that there exists some concept class that is efficientlyrepresentable but is still hard to learn in the PAC model of Valiant (CACM '84), where the learner is allowed to output any efficient hypothesis approximating the concept, including an "improper" hypothesis that is not itself in the concept class. We show that unless the polynomial hierarchy collapses, such a statement cannot be proven via a large class of reductions including Karp reductions, truth-table reductions, and a restricted form of non-adaptive Turing reductions. Also, a proof that uses a Turing reduction of constant levels of adaptivity would imply an important consequence in cryptography as it yields a transformation from any average-case hard problem in NP to a one-way function. Our results hold even in the stronger model of agnostic learning. These results are obtained by showing that lower bounds for improper learning are intimately related to the complexity of zero-knowledge arguments and to the existence of weak cryptographic primitives. In particular, we prove that if alanguage L reduces to the task of improper learning of circuits, then, depending on the type of the reduction in use, either (1) L has a statistical zero-knowledge argument system, or (2) the worst-case hardness of L implies the existence of a weak variant of one-way functions defined by Ostrovsky-Wigderson (ISTCS '93). Interestingly, we observe that the converse implication also holds. Namely, if (1) or (2) hold then the intractability of L implies that improper learning is hard.
[Karp reduction, complexity, Circuits, polynomial hierarchy, cryptography, lower-bounds, weak cryptography, agnostic learning, Machinery, nonadaptive Turing reduction, Computer science, Turing machines, statistical zero-knowledge argument system, NP problem, truth-table reduction, Polynomials, Cryptography, learning (artificial intelligence), computational complexity]
The Bayesian Learner is Optimal for Noisy Binary Search&#x0A0;&#x0A0;(and Pretty Good for Quantum as Well)
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We use a Bayesian approach to optimally solve problems in noisy binary search. We deal with two variants:1. Each comparison is erroneous with independent probability 1-p. 2. At each stage k comparisons can be performed in parallel and a noisy answer is returned. We present a (classical) algorithm which solves both variants optimally (with respect to p and k), up to an additive term of O(loglog n), and prove matching information-theoretic lower bounds. We use the algorithm to improve the results of Farhi et al., presenting an exact quantum search algorithm in an ordered list of expected complexity less than (log<sub>2</sub> n)/3.
[binary search, quantum search, algorithms, Error probability, probability, Entropy, noisy binary search, Computer science, Quantum computing, Bayesian approach, independent probability, search, Bayesian methods, quantum computing, information-theoretic lower bound matching, noise, Bayesian learner, Error correction, Bayes methods, quantum search algorithm, search problems, Information theory, computational complexity]
Hardness of Minimizing and Learning DNF Expressions
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We study the problem of finding the minimum size DNF formula for a function f : {0, 1}d rarr {0,1} given its truth table. We show that unless NP sube DTIME(npoly(log n)), there is no polynomial time algorithm that approximates this problem to within factor d1-epsiv where epsiv &gt; 0 is an arbitrarily small constant. Our result essentially matches the known O(d) approximation for the problem. We also study weak learnability of small size DNF formulas. We show that assuming NP sube RP, for arbitrarily small constant epsiv &gt; 0 and any fixed positive integer t, a two term DNF cannot be PAC-learnt in polynomial time by a t term DNF to within 1/2 + epsiv accuracy. Under the same complexity assumption, we show that for arbitrarily small constants mu, epsiv &gt; 0 and any fixed positive integer t, an AND function (i.e. a single term DNF) cannot be PAC-learnt in polynomial time under adversarial mu-noise by a t-CNF to within 1/2 + epsiv accuracy.
[Engineering profession, positive integer, Approximation, polynomials, AND function, Logic design, Helium, Hardness, DNF expressions, polynomial time algorithm, Computer science, Learning, Logic circuits, disjunctive normal form, Polynomials, Circuit synthesis, Software tools, DNF]
Elections Can be Manipulated Often
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
The Gibbard-Satterthwaite theorem states that every non-trivial voting method among at least 3 alternatives can be strategically manipulated. We prove a quantitative version of the Gibbard-Satterthwaite theorem: a random manipulation by a single random voter will succeed with non-negligible probability for every neutral voting method among 3 alternatives that is far from being a dictatorship.
[Computational game theory, Nominations and elections, random processes, random manipulation, Gas insulated transmission lines, elections, nontrivial voting method, Computer science, social choice, Voting, Aggregates, Gibbard-Satterthwaite theorem, neutral voting method, government data processing]
On the Hardness of Being Truthful
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
The central problem in computational mechanism design is the tension between incentive compatibility and computational efficiency. We establish the first significant approximability gap between algorithms that are both truthful and computationally-efficient, and algorithms that only achieve one of these two desiderata. This is shown in the context of a novel mechanism design problem which we call the combinatorial public project problem (cppp). cpppis an abstraction of many common mechanism design situations, ranging from elections of kibbutz committees to network design.Our result is actually made up of two complementary results -- one in the communication-complexity model and one in the computational-complexity model. Both these hardness results heavily rely on a combinatorial characterization of truthful algorithms for our problem. Our computational-complexity result is one of the first impossibility results connecting mechanism design to complexity theory; its novel proof technique involves an application of the Sauer-Shelah Lemma and may be of wider applicability, both within and without mechanism design.
[approximation theory, Computational modeling, incentive compatibility, Nominations and elections, Routing, computational efficiency, Complexity theory, communication complexity, Cost accounting, Computer science, computational mechanism design, Design engineering, approximability gap, combinatorial public project problem, USA Councils, computational-complexity model, Computer networks, Sauer-Shelah Lemma, Joining processes]
Multi-unit Auctions with Budget Limits
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We study multi-unit auctions where the bidders have a budget constraint, a situation very common in practice that has received very little attention in the auction theory literature. Our main result is an impossibility: there are no incentive-compatible auctions that always produce a Pareto-optimal allocation. We also obtain some surprising positive results for certain special cases.
[Pareto optimisation, budget limits, multi-unit auctions, budget constraint, commerce, auction theory, Cost accounting, Computer science, Upper bound, Pareto-optimal allocation, Constraint theory, Concrete, Pollution measurement, incentive-compatible auctions]
Multilinear Formulas, Maximal-Partition Discrepancy and Mixed-Sources Extractors
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We study a new method for proving lower bounds for subclasses of arithmetic circuits. Roughly speaking, the lower bound is proved by bounding the correlation between the coefficients' vector of a polynomial and the coefficients' vector of any product of two polynomials with disjoint sets of variables. We prove lower bounds for several old and new subclasses of circuits.
[circuit complexity, mixed-sources extractor, Discrepancy, Complexity theory, maximal-partition discrepancy, multilinear formula, Radio access networks, Computer science, Extractors, digital arithmetic, lower bound proving, Noise cancellation, Digital arithmetic, Polynomials, polynomial coefficient vector, Circuit noise, arithmetic circuit, Lower Bounds, computational complexity]
On the Impossibility of Basing Identity Based Encryption on Trapdoor Permutations
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We ask whether an identity based encryption (IBE) system can be built from simpler public-key primitives. We show that there is no black-box construction of IBE from trapdoor permutations (TDP) or even from chosen ciphertext secure public key encryption (CCA-PKE). These black-box separation results are based on an essential property of IBE, namely that an IBE system is able to compress exponentially many public-keys into a short public parameters string.
[Identity-based encryption, Terrorism, Lattices, chosen ciphertext secure public key encryption, Power system security, Power system modeling, Computer science, identity-based encryption system, public key cryptography, black-box separation, Public key, Public key cryptography, Polynomials, National security, trapdoor permutation, public-key primitive]
Leakage-Resilient Cryptography
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We construct a stream-cipher S whose implementation is secure even if a bounded amount of arbitrary (adversarially chosen) information on the internal state ofS is leaked during computation. This captures all possible side-channel attacks on S where the amount of information leaked in a given period is bounded, but overall can be arbitrary large. The only other assumption we make on the implementation of S is that only data that is accessed during computation leaks information. The stream-cipher S generates its output in chunks K<sub>1</sub>, K<sub>2</sub>, . . . and arbitrary but bounded information leakage is modeled by allowing the adversary to adaptively chose a function f<sub>l</sub> : {0,1}* rarr {0, 1}lambda before K<sub>l</sub> is computed, she then gets f<sub>l</sub>(tau<sub>l</sub>) where tau<sub>l</sub> is the internal state ofS that is accessed during the computation of Kg. One notion of security we prove for S is that Kg is indistinguishable from random when given K<sub>1</sub>,..., K<sub>1-1</sub>,f<sub>1</sub>(tau<sub>1</sub> ),..., f<sub>l</sub>-1(tau<sub>l-1</sub>) and also the complete internal state of S after Kg has been computed (i.e. S is forward-secure). The construction is based on alternating extraction (used in the intrusion-resilient secret-sharing scheme from FOCS'07). We move this concept to the computational setting by proving a lemma that states that the output of any PRG has high HILLpseudoentropy (i.e. is indistinguishable from some distribution with high min-entropy) even if arbitrary information about the seed is leaked. The amount of leakage lambda that we can tolerate in each step depends on the strength of the underlying PRG, it is at least logarithmic, but can be as large as a constant fraction of the internal state of S if the PRG is exponentially hard.
[Energy consumption, leakage-resilient cryptography, stream-cipher, Electromagnetic radiation, cryptography, Data mining, Distributed computing, leakage-resilient, Computer science, Fault detection, Information security, side-channel attack, Cryptography, Mathematical model, Protection]
Succincter
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We can represent an array of n values from {0,1,2} using ceil(n log<sub>2</sub> 3) bits (arithmetic coding), but then we cannot retrieve a single element efficiently. Instead, we can encode every block of t elements using ceil(t log<sub>2</sub> 3) bits, and bound the retrieval time by t. This gives a linear trade-off between the redundancy of the representation and the query time.In fact, this type of linear trade-off is ubiquitous in known succinct data structures, and in data compression. The folk wisdom is that if we want to waste one bit per block, the encoding is so constrained that it cannot help the query in any way. Thus, the only thing a query can do is to read the entire block and unpack it.We break this limitation and show how to use recursion to improve redundancy. It turns out that if a block is encoded with two (!) bits of redundancy, we can decode a single element, and answer many other interesting queries, in time logarithmic in the block size.Our technique allows us to revisit classic problems in succinct data structures, and give surprising new upper bounds. We also construct a locally-decodable version of arithmetic coding.
[succinct data structure, Redundancy, Data compression, Data structures, Encoding, Entropy, Decoding, encoding, arithmetic codes, succincter, Computer science, succinct data structures, Upper bound, locally decodable, arithmetic coding, Digital arithmetic, Huffman coding]
Two Query PCP with Sub-Constant Error
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show that the NP-Complete language 3Sat has a PCPverifier that makes two queries to a proof of almost-linear size and achieves sub-constant probability of error o(1). The verifier performs only projection tests, meaning that the answer to the first query determines at most one accepting answer to the second query.Previously, by the parallel repetition theorem, there were PCP Theorems with two-query projection tests, but only (arbitrarily small) constant error and polynomial size.There were also PCP Theorems with sub-constant error andalmost-linear size, but a constant number of queries that is larger than 2.As a corollary, we obtain a host of new results. In particular, our theorem improves many of the hardness of approximation results that are proved using the parallel repetition theorem. A partial list includes the following:(1) 3Sat cannot be efficiently approximated to withina factor of 7/8+o(1), unless P = NP. This holds even under almost-linear reductions. Previously, the best knownNP-hardness factor was 7/8+epsilon for any constant epsilonGt0, under polynomial reductions.(2) 3Lin cannot be efficiently approximated to withina factor of 1/2+o(1), unless P = NP. This holdseven under almost-linear reductions. Previously, the best known NP-hardness factor was 1/2+epsilon for any constant epsilonGt0, under polynomial reductions.(3) A PCP Theorem with amortized query complexity 1 + o(1)and amortized free bit complexity o(1). Previously, the best known amortized query complexity and free bit complexity were 1+epsilon and epsilon, respectively, for any constant epsilon Gt 0.One of the new ideas that we use is a new technique for doing the composition step in the (classical) proof of the PCP Theorem, without increasing the number of queries to the proof. We formalize this as a composition of new objects that we call Locally Decode/Reject Codes (LDRC). The notion of LDRC was implicit in several previous works, and we make it explicit in this work. We believe that the formulation of LDRCs and their construction are of independent interest.
[Performance evaluation, parallel repetition theorem, program verification, Probabilistically Checkable Proofs (PCP), Mathematics, almost-linear reductions, subconstant error, polynomial reductions, Label-Cover, query PCP, Polynomials, theorem proving, Testing, amortized query complexity, polynomials, hardness of approximation, Decoding, 3SAT, Radio access networks, Computer science, two-query projection tests, Upper bound, locally decode/reject codes, NP-hardness factor, Computer errors, NP-complete language, computational complexity]
Constant-Time Approximation Algorithms via Local Improvements
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We present a technique for transforming classical approximation algorithms into constant-time algorithms that approximate the size of the optimal solution. Our technique is applicable to a certain subclass of algorithms that compute a solution in a constant number of phases. The technique is based on greedily considering local improvements in random order.The problems amenable to our technique include vertex cover, maximum matching, maximum weight matching, set cover, and minimum dominating set. For example, for maximum matching, we give the first constant-time algorithm that for the class of graphs of degree bounded by d, computes the maximum matching size to within epsivn, for any epsivn &gt; 0, where n is the number of nodes in the graph. The running time of the algorithm is independent of n, and only depends on d and epsiv.
[local improvement, vertex cover, Computational modeling, greedy algorithms, graph theory, set theory, maximum weight matching, Computer science, set cover, constant-time approximation algorithm, random order, minimum dominating set, Approximation algorithms, greedy algorithm, Distributed algorithms, Random number generation, computational complexity]
Some Results on Greedy Embeddings in Metric Spaces
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Geographic routing is a family of routing algorithms that uses geographic point locations as addresses for the purposes of routing. Such routing algorithms have proven to be both simple to implement and heuristically effective when applied to wireless sensor networks. Greedy routing is a natural abstraction of this model in which nodes are assigned virtual coordinates in a metric space, and these coordinates are used to perform point-to-point routing. Here we resolve a conjecture of Papadimitriou and Ratajczak that every 3-connected planar graph admits a greedy embedding into the Euclidean plane. This immediately implies that all 3-connected graphs that exclude K<sub>3.3</sub> as a minor admit a greedy embedding into the Euclidean plane. Additionally, we provide the first non-trivial examples of graphs that admit no such embedding. These structural results provide efficiently verifiable certificates that a graph admits a greedy embedding or that a graph admits no greedy embedding into the Euclidean plane.
[Solid modeling, wireless sensor networks, graph theory, geographic routing, wireless sensor network, History, point-to-point routing, Tree graphs, Space technology, 3-connected planar graph, Routing protocols, circuit graph, greedy algorithms, excluded minor, Euclidean plane, Extraterrestrial measurements, Ad hoc networks, greedy embeddings, greedy routing, Computer science, metric space, Wireless sensor networks, geographic point location, routing algorithm, telecommunication network routing, Euclidean distance]
Set Covering with our Eyes Closed
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Given a universe U of n elements and a weighted collection l of m subsets of U, the universal set cover problem is to a-priori map each element u epsi U to a set S(u) epsi l containing u, so that X sube U is covered by S(X)=U<sub>uepsiX</sub>S(u). The aim is finding a mapping such that the cost of S(X) is as close as possible to the optimal set-cover cost for X. (Such problems are also called oblivious or a-priori optimization problems.) Unfortunately, for every universal mapping, the cost of S(X) can be Omega(radicn) times larger than optimal if the set X is adversarially chosen. In this paper we study the performance on average, when X is a set of randomly chosen elements from the universe: we show how to efficiently find a universal map whose expected cost is O(log mn) times the expected optimal cost. In fact, we give a slightly improved analysis and show that this is the best possible. We generalize these ideas to weighted set cover and show similar guarantees to (non-metric) facility location, where we have to balance the facility opening cost with the cost of connecting clients to the facilities. We show applications of our results to universal multi-cut and disc-covering problems, and show how all these universal mappings give us stochastic online algorithms with the same competitive factors.
[stochastic online algorithm, stochastic algorithms, Eyes, Circuits, graph theory, Stochastic processes, a-priori approiximation, set theory, online algorithms, Computer science, optimisation, High performance computing, a-priori optimization, universal approximatiom, Cost function, Approximation algorithms, universal multicut problem, Large-scale systems, stochastic processes, Joining processes, Contracts, universal set cover problem, disc-covering problem, computational complexity]
Minimizing Movement in Mobile Facility Location Problems
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
In the mobile facility location problem, which is a variant of the classical uncapacitated facility location and k-median problems, each facility and client is assigned to a start location in a metric graph and our goal is to find a destination node for each client and facility such that every client is sent to a node which is the destination of some facility. The quality of a solution can be measured either by the total distance clients and facilities travel or by the maximum distance traveled by any client or facility. As we show in this paper (by an approximation preserving reduction), the problem of minimizing the total movement of facilities and clients generalizes the classical k-median problem. The class of movement problems was introduced by Demaine et al. in SODA 2007, where it was observed a simple 2-approximation for the minimum maximum movement mobile facility location while an approximation for the minimum total movement variant and hardness results for both were left as open problems. Our main result here is an 8-approximation algorithm for the minimum total movement mobile facility location problem. Our algorithm is obtained by rounding an LP relaxation in five phases. We also show that this problem generalizes the classical k-median problem using an approximation preserving reduction. For the minimum maximum movement mobile facility location problem, we show that we cannot have a better than a 2-approximation for the problem, unless P = NP; so the simple algorithm observed in is essentially best possible.
[metric graph, facility travel, approximation theory, Costs, k-median problem, LP relaxation, 8-approximation algorithm, graph theory, Petroleum, Delay, minimum total movement, facility location, Computer science, approximation preserving reduction, 2-approximation algorithm, Manufacturing, total distance client, Marine vehicles, Mobile computing, mobile facility location problem, computational complexity]
A Counterexample to Strong Parallel Repetition
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
The parallel repetition theorem states that for any two-prover game, with value 1 - isin (for, say, isin les 1/2), the value of the game repeated in parallel n times is at most (1 - isinc)Omega(n/s), where s is the answers' length (of the original game) and c is a universal constant. Several researchers asked wether this bound could be improved to (1 - isin)Omega(n/s); this question is usually referred to as the strong parallel repetition problem. We show that the answer for this question is negative. More precisely, we consider the odd cycle game of size m; a two-prover game with value 1 - 1/2 m. We show that the value of the odd cycle game repeated in parallel n times is at least 1 - (1/m) ldr O(radicn). This implies that for large enough n (say, n ges Omega(m2)), the value of the odd cycle game repeated in parallel n times is at least (1 - 1/4 m2)O(n). Thus: 1. For parallel repetition of general games: the bounds of (1 - isinc)Omega(n/s) given in are of the right form, up to determining the exact value of the constant c ges 2. 2. For parallel repetition of XOR games, unique games and projection games: the bounds of (1 - isin2)Omega(n) given in (for XOR games) and in (for unique and projection games) are tight. 3. For parallel repetition of the odd cycle game: the bound of 1 - (1/m) ldr Omegatilde(radicn) given in is almost tight. A major motivation for the recent interest in the strong parallel repetition problem is that a strong parallel repetition theorem would have implied that the unique game conjecture is equivalent to the NP hardness of distinguishing between instances of Max-Cut that are at least 1 - isin2 satisfiable from instances that are at most 1 - (2/pi) ldr isin satisfiable. Our results suggest that this cannot be proved just by improving the known bounds on parallel repetition.
[parallel repetition theorem, parallel algorithms, Protocols, two-prover game, NP hardness, general games, projection games, game theory, strong parallel repetition, odd cycle game, XOR games, Game theory, unique games, Radio access networks, Computer science, Max-Cut problem, satisfiability, computational complexity]
Rounding Parallel Repetitions of Unique Games
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show a connection between the semidefinite relaxation of unique games and their behavior under parallel repetition. Specifically,denoting by val(G) the value of a two-prover unique game G, andby sdpval(G) the value of a natural semidefinite program to approximate val(G), we prove that for every l epsi N, if sdpval(G) ges 1-delta, then val(Gl) ges 1-radicsldelta. Here, Gl denotes the l-fold parallel repetition of G, and s=O(log(k/delta)), where k denotes the alphabet size of the game. For the special case where G is an XOR game (i.e., k=2), we obtain the same bound but with s as an absolute constant. Our bounds on s are optimal up to a factor of O(log(1/delta)). For games with a significant gap between the quantities val(G) and sdpval(G), our result implies that val(Gl) may be much larger than val(G)l, giving a counterexample to the strong parallel repetition conjecture. In a recent breakthrough, Raz (FOCS'08) has shown such an example using the max-cut game on oddcycles. Our results are based on a generalization of his techniques.
[XOR game, game theory, Mathematics, max-cut game, mathematical programming, unique games, natural semidefinite program, Computer science, semidefinite relaxation, semidefinite programming, Hellinger distance, two-prover unique game, Councils, Quantum mechanics, correlated sampling, Contracts, parallel repetition, computational complexity]
The Unbounded-Error Communication Complexity of Symmetric Functions
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We prove an essentially tight lower bound on the unbounded-error communication complexity of every symmetric function, i.e.,f(x,y)=D(|x Lambda y|), where D:{0,1,...,n}-rarr{0,1} is a given predicate and x,y range over {0,1}n. Specifically, we show that the communication complexity of f is between Theta(k/log5 n) and Theta(k log n), where k is the number of value changes of D in {0,1,...,n}. The unbounded-error model is the most powerful of the basic models of communication (both classical and quantum), and proving lower bounds in it is a considerable challenge. The only previous nontrivial lower bounds for explicit functions in this model appear in the ground breaking work of Forster (2001) and its extensions. Our proof is built around two novel ideas. First, we show that a given predicate D gives rise to a rapidly mixing random walk on Z<sub>2</sub> n, which allows us to reduce the problem to communication lower bounds for typical predicates. Second, we use Paturi's approximation lower bounds (1992), suitably generalized here to clusters of real nodes in [0,n] and interpreted in their dualform, to prove that a typical predicate behaves analogous to PARITY with respect to a smooth distribution on the inputs.
[approximation theory, Protocols, Costs, Quantum entanglement, cryptographic protocols, symmetric functions, random processes, cryptographic protocol, Complexity theory, Paturi's approximation lower bound, communication complexity, Machinery, Zinc, lower bounds, Computer science, random walk, Boolean functions, symmetric predicate, Communication channels, symmetric Boolean function, Random variables, unbounded-error communication complexity, Mathematical model]
Lower Bounds for Noisy Wireless Networks using Sampling Algorithms
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show a tight lower bound of Omega(N\\log\\log N) on the number of transmissions required to compute several functions (including the parity function and the majority function) in a network of N randomly placed sensors, communicating using local transmissions, and operating with power near the connectivity threshold. This result considerably simplifies and strengthens an earlier result of Dutta, Kanoria Manjunath and Radhakrishnan (SODA 08) that such networks cannot compute the parity function reliably with significantly fewer than N\\log \\log N transmissions, thereby showing that the protocol with O(N\\log \\log N) transmissions due to Ying, Srikant and Dullerud (WiOpt 06) is optimal. We also observe that all the lower bounds shown by Evans and Pippenger (SIAM J. on Computing, 1999) on the average noisy decision tree complexity for several functions can be derived using our technique simply and in a unified way.
[radio networks, Wireless application protocol, majority function, average noisy decision tree complexity, Wireless networks, average case lower bound, Computer networks, Decision trees, protocols, broadcast protocols, Computational modeling, wireless networks, lower bounds, Computer science, Wireless sensor networks, decision trees, local transmissions, noisy decision tree, Sampling methods, noisy wireless networks, Telecommunication network reliability, sampling algorithms, connectivity threshold, Context modeling, computational complexity]
Inapproximability for Metric Embeddings into R^d
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We consider the problem of computing the smallest possible distortion for embedding of a given n-point metric space into R.d, where d is fixed (and small). For d = 1, it was known that approximating the minimum distortion with a factor better than roughly n1/12 is NP-hard. From this result we derive inapproximability with factor roughly n1/(22d-10) for every fixed d ges 2, by a conceptually very simple reduction. However, the proof of correctness involves a nontrivial result in geometric topology (whose current proof is based on ideas due to Jussi Vaisala). For d ges 3,we obtain a stronger inapproximability result by a different reduction: assuming PneNP, no polynomial- time algorithm can distinguish between spaces embeddable in R.d with constant distortion from spaces requiring distortion at least nc/d, for a constant c &gt; 0. The exponent c/d has the correct order of magnitude, since every n-point metric space can be embedded in Rd with distortion O(n2/d log3/2 n) and such an embedding can be constructed in polynomial time by random projection. For d = 2, we give an example of a metric space that requires a large distortion for embedding in R2, while all not too large subspaces of it embed almost isometrically.
[embeddings, Embedded computing, NP-hard, Laboratories, topology, geometric topology, random processes, Extraterrestrial measurements, Mathematics, inapproximability, minimum distortion, Computer science, hardness, Space technology, USA Councils, Approximation algorithms, Polynomials, polynomial-time algorithm, Artificial intelligence, random projection, computational complexity, metric embeddings]
A Geometric Approach to Lower Bounds for Approximate Near-Neighbor Search and Partial Match
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
This work investigates a geometric approach to proving cell probe lower bounds for data structure problems.We consider the {\\em approximate nearest neighbor search problem} on the Boolean hypercube $(\\bool^d,\\onenorm{\\cdot})$ with $d=\\Theta(\\log n)$. We show that any (randomized) data structure for the problem that answers $c$-approximate nearest neighbor search queries using $t$ probes must use space at least $n^{1+\\Omega(1/ct)}$. In particular, our bound implies that any data structure that uses space $\\tilde{O}(n)$ with polylogarithmic word size, and with constant probability gives a constant approximation to nearest neighbor search queries must be probed $\\Omega(\\log n/ \\log\\log n)$ times. This improves on the lower bound of $\\Omega(\\log\\log d/\\log\\log\\log d)$ probes shown by Chakrabarti and Regev~\\cite{ChakrabartiR04} for any polynomial space data structure, and the $\\Omega(\\log\\log d)$ lower bound in \\Patrascu and Thorup~\\cite{PatrascuT07} for linear space data structures.Our lower bound holds for the {\\em near neighbor problem}, where the algorithm knows in advance a good approximation to the distance to the nearest neighbor.Additionally, it is an {\\em average case} lower bound for the natural distribution for the problem. Our approach also gives the same bound for $(2-\\frac{1}{c})$-approximation to the farthest neighbor problem.For the case of non-adaptive algorithms we can improve the bound slightly and show a $\\Omega(\\log n)$ lower bound on the time complexity of data structures with $O(n)$ space and logarithmic word size.We also show similar lower bounds for the partial match problem: any randomized $t$-probe data structure that solves the partial match problem on $\\{0,1,\\star\\}^d$ for $d=\\Theta(\\log n)$ must use space $n^{1+\\Omega(1/t)}$. This implies an $\\Omega(\\log n/\\log\\log n)$ lower bound for time complexity of near linear space data structures, slightly improving the $\\Omega(\\log n /(\\log \\log n)^2)$ lower bound from~\\cite{PatrascuT06a},\\cite{JayramKKR03} for this range of $d$. Recently and independently \\Patrascu achieved similar bounds \\cite{patrascu08}. Our results also generalize to approximate partial match, improving on the bounds of \\cite{BarkolR02,PatrascuT06a}.
[Machine learning algorithms, nearest neighbor search problem, data structure, Partial Match, Hypercubes, Polynomials, data structures, Probes, search problems, cell probe lower bounds, Cell Probe Lower Bounds, Near Neighbor Search, Computational biology, geometric approach, time complexity, Data structures, Information retrieval, nonadaptive algorithm, Nearest neighbor searches, Computer science, Geometry, partial match problem, Approximation algorithms, computational complexity, Boolean hypercube]
Hardness of Nearest Neighbor under L-infinity
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Recent years have seen a significant increase in our understanding of high-dimensional nearest neighbor search (NNS) for distances like the lscr<sub>1</sub> and lscr<sub>2</sub> norms. By contrast, our understanding of the lscr<sub>infin</sub> norm is now where it was (exactly) 10 years ago. In FOCSpsila98, Indyk proved the following unorthodox result: there is a data structure (in fact, a decision tree) of size O(nrho), for any rho &gt; 1, which achieves approximation O(log<sub>rho</sub> log d) for NNS in the d-dimensional lscr<sub>1</sub> metric. In this paper, we provide results that indicate that Indykpsilas unconventional bound might in fact be optimal. Specifically, we show a lower bound for the asymmetric communication complexity of NNS under lscrinfin, which proves that this space/approximation trade-off is optimal for decision trees and for data structures with constant cell-probe complexity.
[Tree data structures, data structure, Extraterrestrial measurements, high-dimensional nearest neighbor search, constant cell-probe complexity, Complexity theory, Proposals, communication complexity, L-infinity, Nearest neighbor searches, Computer science, decision tree, Image databases, asymmetric communication complexity, decision trees, Approximation algorithms, Polynomials, data structures, Decision trees, search problems]
(Data) STRUCTURES
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show that a large fraction of the data-structure lower bounds known today in fact follow by reduction from the communication complexity of lopsided (asymmetric) set disjointness! This includes lower bounds for: (a) high-dimensional problems, where the goal is to show large space lower bounds; (b) constant-dimensional geometric problems, where the goal is to bound the query time for space O(n polylg n); (c) dynamic problems, where we are looking for a trade-off between query and update time. (In this case, our bounds are slightly weaker than the originals, losing a lglg n factor.) Our reductions also imply the following new results: (a) an Omega(lg n / lg lg n) bound for 4-dimensional range reporting, given space O(n ldr poly lg n). This is very timely, since a recent result [Nekrich, SoCG'07] solved 3D reporting in near-constant time, raising the prospect that higher dimensions could also be easy; (b) a tight space lower bound for the partial match problem, for constant query time.(c) the first lower bound for reachability oracles.
[lopsided set disjointness, data-structure lower bounds, 4-dimensional range reporting, constant query time, Predictive models, range queries, Data structures, Complexity theory, Visual databases, communication complexity, constant-dimensional geometric problems, lower bounds, high-dimensional problems, Computer science, Degradation, cell probe, Bibliographies, Data visualization, Writing, data structures, Probes, reachability oracles]
Entangled Games are Hard to Approximate
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We establish the first hardness results for the problem of computing the value of one-round games played by a referee and a team of players who can share quantum entanglement. In particular, we show that it is NP-hard to approximate within an inverse polynomial the value of a one-round game with (i) quantum referee and two entangled players or (ii) classical referee and three entangled players. Previously it was not even known if computing the value exactly is NP-hard. We also describe a mathematical conjecture, which, if true, would imply hardness of approximation to within a constant.We start our proof by describing two ways to modify classical multi-player games to make them resistant to entangled players. We then show that a strategy for the modified game that uses entanglement can be "rounded'' to one that does not. The results then follow from classical inapproximability bounds. Our work implies that, unless P = NP, the values of entangled-player games cannot be computed by semidefinite programs that are polynomial in the size of the referee's system, a method that has been successful for more restricted quantum games.
[Protocols, NP-hard, quantum referee, multiplayer games, inapproximability bounds, quantum entanglement, inverse polynomial, Distributed computing, Quantum computing, semidefinite programs, quantum games, Polynomials, Informatics, Quantum entanglement, game theory, entanglement, quantum interactive proofs, Game theory, Computer science, entangled player games, hardness, mathematical conjecture, Councils, Quantum mechanics, quantum computing, one-round games]
Unique Games with Entangled Provers are Easy
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We consider one-round games between a classical verifier and two provers who share entanglement. We show that when the constraints enforced by the verifier are `unique' constraints (i.e., permutations), the value of the game can be well approximated by a semidefinite program. Essentially the only algorithm known previously was for the special case of binary answers, as follows from the work of Tsirelson in 1980. Among other things, our result implies that the variant of the unique games conjecture where we allow the provers to share entanglement is false. Our proof is based on a novel `quantum rounding technique', showing how to take a solution to an SDP and transform it to a strategy for entangled provers. Using our approximation by a semidefinite program we also show a parallel repetition theorem for unique entangled games.
[parallel repetition theorem, Quantum entanglement, unique constraint, game theory, quantum rounding technique, unique entangled game, Reflection, Application software, Game theory, Computer science, semidefinite programming, unique games conjecture, Councils, theorem proving, Contracts]
Quantum Multi Prover Interactive Proofs with Communicating Provers
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We introduce another variant of quantum MIP, where the provers do not share entanglement, the communication between the verifier and the provers is quantum, but the provers are unlimited in the classical communication between them. At first, this model may seem very weak, as provers who exchange information seem to be equivalent in power to a simple prover. This in fact is not the case-we show that any language in NEXP can be recognized in this model efficiently, with just two provers and two rounds of communication, with a constant completeness-soundness gap. Similar ideas and techniques may help help with other models of quantum MIP, including the dual question, of non communicating provers with unlimited entanglement.
[Protocols, Quantum entanglement, Indium tin oxide, quantum multiprover interactive proofs, quantum entanglement, quantum, NEXP, Computer science, Quantum computing, unlimited entanglement, Quantum mechanics, quantum computing, interactive proofs, Polynomials, theorem proving, communicating provers, multi prover, computational complexity]
A Hypercontractive Inequality for Matrix-Valued Functions with Applications to Quantum Computing and LDCs
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
The Bonami-Beckner hypercontractive inequality is a powerful tool in Fourier analysis of real-valued functions on the Boolean cube. In this paper we present a version of this inequality for matrix-valued functions on the Boolean cube. Its proof is based on a powerful inequality by Ball, Carlen, and Lieb. We also present a number of applications. First, we analyze maps that encode n classical bits into m qubits, in such a way that each set of k bits can be recovered with some probability by an appropriate measurement on the quantum encoding; we show that if m &lt; 0.7 n, then the success probability is exponentially small in k. This result may be viewed as a direct product version of Nayak's quantum random access code bound. It in turn implies strong direct product theorems for the one-way quantum communication complexity of Disjointness and other problems. Second, we prove that error-correcting codes that are locally decodable with 2 queries require length exponential in the length of the encoded string. This gives what is arguably the first "non-quantum" proof of a result originally derived by Kerenidis and de Wolf using quantum information theory.
[hypercontractive inequality, Bonami-Beckner hypercontractive inequality, LDC, Linear matrix inequalities, Complexity theory, History, matrix-valued functions, communication complexity, Quantum computing, qubits, quantum communication, Nayak quantum random access code, quantum information theory, locally decodable codes, error correction codes, Fourier analysis, Encoding, Boolean cube, Application software, quantum encoding, Computer science, random codes, error-correcting codes, Quantum mechanics, quantum computing, Computer applications, Error correction codes]
Sketching and Streaming Entropy via Approximation Theory
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We give near-optimal sketching and streaming algorithms for estimating Shannon entropy in the most general streaming model, with arbitrary insertions and deletions. This improves on prior results that obtain suboptimal space bounds in the general model, and near-optimal bounds in the insertion-only model without sketching. Our high-level approach is simple: we give algorithms to estimate Tsallis entropy, and use them to extrapolate an estimate of Shannon entropy. The accuracy of our estimates is proven using approximation theory arguments and extremal properties of Chebyshev polynomials. Our work also yields the best-known and near-optimal additive approximations for entropy, and hence also for conditional entropy and mutual information.
[approximation theory, streaming entropy, Additives, Tsallis entropy, insertion-only model, Entropy, Frequency estimation, near-optimal sketching algorithm, Approximation methods, Computer science, sketching, streaming, entropy, Chebyshev approximation, Shannon entropy, Approximation algorithms, suboptimal space bounds, Polynomials, Internet, Mutual information, Chebyshev polynomials]
On the Value of Multiple Read/Write Streams for Approximating Frequency Moments
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We consider the read/write streams model, an extension of the standard data stream model in which an algorithm can create and manipulate multiple read/write streams in addition to its input data stream. We show that any randomized read/write stream algorithm with a fixed number of streams and a sublogarithmic number of passes that produces a constant factor approximation of the k-th frequency moment F<sub>k</sub> of an input sequence of length of at most N from {1, ..., N} requires space Omega(N1-4/k-delta) for any delta &gt; 0. For comparison, it is known that with a single read-only data stream there is a randomized constant- factor approximation for F<sub>k</sub> using O(N1-2/k) space and that there is a deterministic algorithm computing F<sub>k</sub> exactly using 3 read/write streams, O(log N) passes, and O(log N) space. Therefore, although the ability to manipulate multiple read/write streams can add substantial power to the data stream model, with a sub-logarithmic number of passes this does not significantly improve the ability to approximate higher frequency moments efficiently. Our lower bounds also apply to (1 + epsi)-approximations of F<sub>k</sub> for epsi ges 1/N.
[approximation theory, Costs, Data Streams, randomized read/write stream algorithm, deterministic algorithm computing, Data engineering, constant factor approximation, Communication Complexity, Statistics, Sorting, Computer science, Databases, frequency moments, Frequency Moments, Traffic control, Frequency, Approximation algorithms, Monitoring, Lower Bounds, computational complexity, multiple read/write stream model]
Clock Synchronization with Bounded Global and Local Skew
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We present a distributed clock synchronization algorithm that guarantees an exponentially improved bound of O(log D) on the clock skew between neighboring nodes in any graph G of diameter D. In light of the lower bound of Omega(log D/ log log D), this result is almost tight. Moreover, the global clock skew between any two nodes, particularly nodes that are not directly connected, is bounded by O(D), which is optimal up to a constant factor. Our algorithm further ensures that the clock values are always within a linear envelope of real time. A better bound on the accuracy with respect to real time cannot be achieved in the absence of an external timer. These results all hold in a general model where both the clock drifts and the message delays may vary arbitrarily within pre-specified bounds.
[clock drifts, message delays, local skew, clock synchronization, Laboratories, bounded global skew, upper bound, gradient property, Synchronization, Distributed computing, Delay, synchronisation, Computer science, clocks, distributed clock synchronization algorithm, Bidirectional control, Computer networks, Hardware, clock skew, Clocks, computational complexity]
Shallow-Low-Light Trees, and Tight Lower Bounds for Euclidean Spanners
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show that for every n-point metric space M and positive integer k, there exists a spanning tree T with unweighted diameter O(k) and weight w(T) = O(k ldr n1/k) ldr w(MST(M)), and a spanning tree T' with weight w(T') = O(k) ldr w(MST(M)) and unweighted diameter O(k ldr n1/k). Moreover, there is a designated point rt such that for every other point v, both dist<sub>T</sub>(rt, v) and dist<sub>T</sub>(rt, v) are at most (1 + epsiv) ldr dist<sub>M</sub>(rt,v), for an arbitrarily small constant epsiv &gt; 0. We prove that the above tradeoffs are tight up to constant factors in the entire range of parameters. Furthermore, our lower bounds apply to a basic one-dimensional Euclidean space. Finally, our lower bounds for the particular case of unweighted diameter O(log n) settle a long-standing open problem in Computational Geometry.
[Algorithm design and analysis, Spanners, Low-Distortion Embeddings, trees (mathematics), computational geometry, Extraterrestrial measurements, Routing, unweighted diameter, Computational Geometry, Distributed computing, shallow-low-light trees, Computer science, Computational geometry, Tree graphs, Euclidean spanners, Euclidean Spanners, spanning tree, Approximation algorithms, computational complexity]
What Can We Learn Privately?
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask: what concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in the contexts where aggregate information is released about a database containing sensitive information about individuals. We present several basic results that demonstrate general feasibility of private learning and relate several models previously studied separately in the contexts of privacy and standard learning.
[Data privacy, private learning problem, data privacy problem, PAC Learning, Cardiac arrest, History, database management systems, Database Privacy, Information analysis, Computer science, database, large real-life data set, Databases, Aggregates, Learning Theory, Blood pressure, Polynomials, data privacy, learning (artificial intelligence), differential privacy, Context modeling]
Learning Geometric Concepts via Gaussian Surface Area
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We study the learnability of sets in Ropfn under the Gaussian distribution, taking Gaussian surface area as the "complexity measure" of the sets being learned. Let C<sub>S</sub> denote the class of all (measurable) sets with surface area at most S. We first show that the class C<sub>S</sub> is learnable to any constant accuracy in time nO(S 2 ), even in the arbitrary noise ("agnostic'') model. Complementing this, we also show that any learning algorithm for C<sub>S</sub> information-theoretically requires 2Omega(S 2 ) examples for learning to constant accuracy. These results together show that Gaussian surface area essentially characterizes the computational complexity of learning under the Gaussian distribution. Our approach yields several new learning results, including the following (all bounds are for learning to any constant accuracy): The class of all convex sets can be agnostically learned in time 2O ~ (radicn) (and we prove a 2Omega(radicn) lower bound for noise-free learning). This is the first subexponential time algorithm for learning general convex sets even in the noise-free (PAC) model. Intersections of k halfspaces can be agnostically learned in time nO(log k) (cf. Vempala's nO(k) time algorithm for learning in the noise-free model).Cones (with apex centered at the origin), and spheres witharbitrary radius and center, can be agnostically learned in time poly(n).
[Convex Sets, Engineering profession, Gaussian Surface Area, Area measurement, Gaussian distribution, computational geometry, complexity measure, Time measurement, Probability distribution, set theory, Noise measurement, Gaussian surface area, Computational complexity, Surface Area, Computer science, Learning, Gaussians, Upper bound, Boolean functions, geometric concept learning, Agnostic Learning, learning (artificial intelligence), general convex sets, computational complexity]
Isotropic PCA and Affine-Invariant Clustering
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We present an extension of principal component analysis (PCA) and a new algorithm for clustering points in \\Rn based on it. The key property of the algorithm is that it is affine-invariant. When the input is a sample from a mixture of two arbitrary Gaussians, the algorithm correctly classifies the sample assuming only that the two components are separable by a hyperplane, i.e., there exists a halfspace that contains most of one Gaussian and almost none of the other in probability mass. This is nearly the best possible, improving known results substantially. For k&gt;2 components, the algorithm requires only that there be some (k-1)-dimensional subspace in which the ``overlap'' in every direction is small. Our main tools are isotropic transformation, spectral projection and a simple reweighting technique. We call this combination isotropic PCA.
[probability, Gaussian distribution, isotropic transformation, Pattern recognition, Covariance matrix, arbitrary Gaussians, isotropic PCA, probability mass, principal components analysis, affine transforms, Computer science, reweighting technique, pattern clustering, Clustering algorithms, Gaussian processes, spectral projection, Polynomials, clustering, Labeling, affine-invariant clustering, mixture models, principal component analysis, Principal component analysis]
Approximate Kernel Clustering
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
In the kernel clustering problem we are given a large ntimesn positive semi-definite matrix A=(a<sub>ij</sub>) with Sigma<sub>i,j</sub> n=1 a<sub>ij</sub>=0 and a small ktimesk positivesemi-definite matrix B=b<sub>ij</sub>. The goal is to find a partition S<sub>1</sub>,..S<sub>k</sub> of {1,...n} which maximizes the quantity Sigma<sub>i,j=1</sub> k(Sigma<sub>(i,j)isinS</sub> <sub>i</sub> <sub>timesS</sub> <sub>j</sub>). We study the computational complexity of this generic clustering problem which originates in the theory of machine learning. We design a constant factor polynomial time approximation algorithm forthis problem, answering a question posed by Song, Smola, Gretton and Borgwardt. In some cases we manage to compute the sharp approximation threshold for this problem assuming the unique games conjecture (UGC). In particular, when B is the 3times3 identity matrix the UGC hardness threshold of this problem is exactly 16pi/27. We present and study a geometricconjecture of independent interest which we show would imply thatthe UGC threshold when B is the ktimesk identity matrix is 8pi/9(1-1/k) for every kges3.
[Algorithm design and analysis, semi-definite matrix, approximation theory, Machine learning algorithms, User-generated content, game theory, inapproximability, Approximation algorithm, Computational complexity, matrix algebra, Computer science, unique games conjecture, pattern clustering, Clustering algorithms, Machine learning, Approximation algorithms, Polynomials, approximate kernel clustering, clustering, Kernel, polynomial time approximation algorithm, computational complexity]
Beating the Random Ordering is Hard: Inapproximability of Maximum Acyclic Subgraph
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We prove that approximating the max. acyclic subgraph problem within a factor better than 1/2 is unique games hard. Specifically, for every constant epsiv &gt; 0 the following holds: given a directed graph G that has an acyclic subgraph consisting of a fraction (1-epsiv) of its edges, if one can efficiently find an acyclic subgraph of G with more than (1/2 + epsiv) of its edges, then the UGC is false. Note that it is trivial to find an acyclic subgraph with 1/2 the edges, by taking either the forward or backward edges in an arbitrary ordering of the vertices of G. The existence of a rho-approximation algorithmfor rho &gt; 1/2 has been a basic open problem for a while. Our result is the first tight inapproximability result for an ordering problem. The starting point of our reduction isa directed acyclic subgraph (DAG) in which every cut isnearly-balanced in the sense that the number of forward and backward edges crossing the cut are nearly equal; such DAGs were constructed by Charikar et al. Using this, we are able to study max. acyclic subgraph, which is a constraint satisfaction problem (CSP) over an unbounded domain, by relating it to a proxy CSP over a bounded domain. The latter is then amenable to powerful techniques based on the invariance principle. Our results also give a super-constant factor inapproximability result for the feedback arc set problem. Using our reductions, we also obtain SDP integrality gapsfor both the problems.
[approximation theory, User-generated content, constraint theory, hardness of approximation, constraint satisfaction problem, Mathematics, inapproximability, unique games, random ordering, feedback arc set, Computer science, feedback, unique games conjecture, directed graphs, integrality gaps, Approximation algorithms, directed acyclic subgraph, maximum acyclic subgraph]
(Acyclic) Job Shops are Hard to Approximate
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
For every euro &gt; 0, we show that the (acyclic) job shop problem cannot be approximated within ratio O(log1+euro lb), unless NP has quasi-polynomial Las-Vegas algorithms, and where lb denotes a trivial lower bound on the optimal value. This almost matches the best known results for acyclic job shops, since an O(log1+euro lb)-approximate solution can be obtained in polynomial time for every euro &gt; 0. Recently, a PTAS was given for the job shop problem, where the number of machines and the number of operations per job are assumed to be constant. Under P ne NP, and when the number mu of operations per job is a constant, we provide an inapproximability result whose value grows with mu to infinity. Moreover, we show that the problem with two machines and the preemptive variant with three machines have no PTAS, unless NP has quasi-polynomial algorithms. These results show that the restrictions on the number of machines and operations per job are necessary to obtain a PTAS.In summary, the presented results close many gaps in our understanding of the hardness of the job shop problem and resolve (negatively) several open problems in the literature.
[Job shop scheduling, Approximation, H infinity control, Scheduling, acyclic job shop problem, Hardness, Computer science, Approximation algorithms, quasi-polynomial Las-Vegas algorithms, Polynomials, polynomial time, job shop scheduling, computational complexity]
Linear Level Lasserre Lower Bounds for Certain k-CSPs
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We show that for kges3 even the Omega(n) level of the Lasserre hierarchy cannot disprove a random k-CSP instance over any predicate type implied by k-XOR constraints, for example k-SAT or k-XOR. (One constant is said to imply another if the latter is true whenever the former is. For example k-XOR constraints imply k-CNF constraints.) As a result the Omega(n) level Lasserre relaxation fails to approximate such CSPs betterthan the trivial, random algorithm. As corollaries, we obtain Omega(n) level integrality gaps for the Lasserre hierarchy of 7/6-epsiv for VERTEXCOVER, 2-epsiv for k-UNIFORMHYPERGRAPHVERTEXCOVER, and any constant for k-UNIFORMHYPERGRAPHINDEPENDENTSET. This is the first construction of a Lasserre integrality gap.Our construction is notable for its simplicity. It simplifies, strengthens, and helps to explain several previous results.
[Lasserre, Computational modeling, communicating sequential processes, semidefinite program hierarchies, random algorithm, set theory, certain k-CSP, linear level Lasserre lower bounds, Computer science, Constraint optimization, NP-hard problems, Lasserre integrality gap, Approximation algorithms, Polynomials, Lasserre relaxation, computational complexity]
The Power of Reordering for Online Minimum Makespan Scheduling
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
In the classic minimum makespan scheduling problem, we are given an input sequence of jobs with processing times. A scheduling algorithm has to assign the jobs to m parallel machines. The objective is to minimize the makespan, which is the time it takes until all jobs are processed. In this paper, we consider online scheduling algorithms without preemption. However, we do not require that each arriving job has to be assigned immediately to one of the machines. A reordering buffer with limited storage capacity can be used to reorder the input sequence in a restricted fashion so as to schedule the jobs with a smaller makespan. This is a natural extension of lookahead. We present an extensive study of the power and limits of online reordering for minimum makespan scheduling. As main result, we give, for m identical machines, tight and, in comparison to the problem without reordering, much improved bounds on the competitive ratio for minimum makespan scheduling with reordering buffers. Depending on m, the achieved competitive ratio lies between 4/3 and 1.4659. This optimal ratio is achieved with a buffer of size Theta(m). We show that larger buffer sizes do not result in an additional advantage and that a buffer of size Omega(m) is necessary to achieve this competitive ratio. Further, we present several algorithms for different buffer sizes. Among others, we introduce, for every buffer size k isin [1, (m+ 1)/2], a (2 middot 1/(m middot k+ 1))-competitive algorithm, which nicely generalizes the well-known result of Graham. For m uniformly related machines, we give a scheduling algorithm that achieves a competitive ratio of 2 with a reordering buffer of size m. Considering that the best knowncompetitive ratio for uniformly related machines without reordering is 5.828, this result emphasizes the power of online reordering further more.
[Algorithm design and analysis, online minimum makespan scheduling, Buffer storage, Parallel machines, Floods, parallel machines, online algorithms, Scheduling algorithm, makespan minimisation, Computer science, Upper bound, minimum makespan scheduling, Processor scheduling, scheduling, job assignment, reordering buffers, minimisation, competitive analysis]
Locally Testing Direct Product in the Low Error Range
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Given a function f : X rarr Sigma, its lscr-wise direct product is the function F = flscr : Xlscr rarr Sigmalscr defined by: F(x<sub>1</sub>,...,x<sub>lscr</sub>) = (f(x<sub>1</sub>),...,f(x<sub>lscr</sub>)). We are interested in the local testability of the direct product encoding (mapping f rarr flscr). Namely, given an arbitrary function F : Xlscr rarr Sigmalscr, we wish to determine how close it is to flscr for some f : X rarr Sigma, by making two random queries into F. In this work we analyze the case of low acceptance probability of the test. We show that even if the test passes with small probability, epsiv&gt;0, already F must have a non-trivial structure and in particular must agree with some flscr on nearly epsiv of the domain. Moreover, we give a structural characterization of all functions F on which the test passes with probability epsiv. Our results can be viewed as a combinatorial analog of the low error dasialow degree testpsila, that is used in PCP constructions.
[direct product encoding, probability, random queries, Encoding, combinatorial analog, property testing, direct product, encoding, Computer science, local direct product testing, list decoding, Computer errors, PCP constructions, low error, Testing]
Kakeya Sets, New Mergers and Old Extractors
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
A merger is a probabilistic procedure which extracts the randomness out of any (arbitrarily correlated) set of random variables, as long as one of them is uniform. Our main result is an efficient, simple, optimal (to constant factors) merger, which, for k random vairables on n bits each, uses a O(log(nk)) seed, and whose error is 1/nk. Our merger can be viewed as a derandomized version of the merger of Lu, Reingold, Vadhan and Wigderson (2003). Its analysis generalizes the recent resolutionof the Kakeya problem in finite fields of Dvir (2008). Following the plan set forth by Ta-Shma (1996), who defined mergers as part of this plan, our merger provides the last "missing link" to a simple and modular construction of extractors for all entropies, which is optimal to constant factorsin all parameters. This complements the elegant construction of optimal extractor by Guruswami, Vadhan and Umans (2007). We also give simple extensions of our merger in two directions. First, we generalize it to handle the case where no source is uniform - in that case the merger will extract the entropy present in the most random of the given sources. Second, we observe that the merger works just as well in the computational setting, when the sources are efficiently samplable, and computational notions of entropy replace the information theoretic ones.
[Modular construction, Corporate acquisitions, extractors, probability, random processes, kakeya, random variable, Harmonic analysis, probabilistic procedure, Entropy, Mathematics, set theory, Data mining, Galois fields, mergers, Computer science, entropy, randomness extraction, finite field, Needles, Random variables, Kakeya set, merger, computational complexity]
A Dichotomy Theorem for the Resolution Complexity of Random Constraint Satisfaction Problems
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We consider random instances of constraint satisfaction problems where each variable has domain size O(1), each constraint is on O(1) variables and the constraints are chosen from a specified distribution. The number of constraints is cn where c is a constant. We prove that for every possible distribution, either the resolution complexity is almost surely polylogarithmic for sufficiently large c, or it is almost surely exponential for every c &gt; 0. We characterize the distributions of each type. To do so, we introduce a closure operation on a set of constraints which yields the set of all constraints that, in some sense, appear implicitly in the random CSP.
[Computer science, random walks, dichotomy theorem, constraint theory, operations research, resolution complexity, H infinity control, random constraint satisfaction problems, Constraint theory, Davis-Putnam algorithms, Polynomials, computational complexity]
Holographic Algorithms by Fibonacci Gates and Holographic Reductions for Hardness
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We propose a new method to prove complexity dichotomy theorems. First we introduce Fibonacci gates which provide a new class of polynomial time holographic algorithms. Then we develop holographic reductions. We show that holographic reductions followed by interpolations provide a uniform strategy to prove #P-hardness.
[Computer science, Interpolation, interpolation, Fibonacci gates, polynomial time holographic algorithms, Fibonacci sequences, Holography, complexity dichotomy theorems, Polynomials, Bipartite graph, holographic reductions, Computational complexity]
Network Extractor Protocols
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We design efficient protocols for processors to extract private randomness over a network with Byzantine faults, when each processor has access to an independent weakly-random n-bit source of sufficient min-entropy.We give several such network extractor protocols in both the information theoretic and computational settings.For a computationally unbounded adversary, we construct protocols in both the synchronous and asynchronous settings.These network extractors imply efficient protocols for leader election (synchronous setting only) and Byzantine agreement which tolerate a linear fraction of faults,even when the min-entropy is only 2(log n) Omega(1).For larger min-entropy,in the synchronous setting the fraction of tolerable faults approaches the bounds in the perfect-randomness case.Our network extractors for a computationally bounded adversary work in the synchronous setting even when 99% of the parties are faulty, assuming trapdoor permutations exist. Further, assuming a strong variant of the Decisional Diffie-Hellman Assumption, we construct a network extractor in which all parties receive private randomness. This yields an efficient protocol for secure multi-party computation with imperfect randomness, when the number of parties is at least polylog (n) and where the parties only have access to an independent source with min-entropy nOmega(1).
[leader election, Byzantine agreement, Computational modeling, Nominations and elections, Access protocols, network extractor protocols, Entropy, Byzantine faults, multiparty computation, Data mining, Distributed computing, Cryptographic protocols, randomised algorithms, Computer science, entropy, distributed algorithms, Network, Computer networks, Extractor, Cryptography, protocols]
Isomorhism of Hypergraphs of Low Rank in Moderately Exponential Time
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We give an algorithm to decide isomorphism of hypergraphs of rank k in time exp (Otilde(k2radicn)), where n is the number of vertices. (The rank is the maximum size of edges; the tilde refers to a polylogarithmic factor.) The case of bounded k answers a 24-year-old question and removes an obstacle to improving the worst case-bound for Graph Isomorphism testing. The best previously known bound, even for k = 3, was Cn (Luks 1999).
[graph theory, isomorphism, Computer science, hypergraph, exponential time, hypergraphs, graph isomorphism testing, graph isomorphism, permutation groups, Polynomials, Testing, computational complexity, algorithm, moderately exponential]
Computing the Tutte Polynomial in Vertex-Exponential Time
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
The deletion-contraction algorithm is perhaps the most popular method for computing a host of fundamental graph invariants such as the chromatic, flow, and reliability polynomials in graph theory, the Jones polynomial of an alternating link in knot theory, and the partition functions of the models of Ising, Potts, and Fortuin-Kasteleyn in statistical physics. Prior to this work, deletion-contraction was also the fastest known general-purpose algorithm for these invariants, running in time roughly proportional to the number of spanning trees in the input graph.Here, we give a substantially faster algorithm that computes the Tutte polynomial-and hence, all the aforementioned invariants and more-of an arbitrary graph in time within a polynomial factor of the number of connected vertex sets. The algorithm actually evaluates a multivariate generalization of the Tutte polynomial by making use of an identity due to Fortuin and Kasteleyn. We also provide a polynomial-space variant of the algorithm and give an analogous result for Chung and Graham's cover polynomial.
[fundamental graph invariants, knot theory, Tutte polynomial, reliability polynomials, partition functions, set theory, spanning trees, exponential-time algorithms, cover polynomial, Quantum computing, vertex-exponential time, Tree graphs, Physics computing, connected vertex sets, Polynomials, Exact algorithms, polynomials, trees (mathematics), Reliability theory, Graph theory, Partitioning algorithms, Potts model, Information technology, Jones polynomial, Computer science, statistical physics, multivariate generalization, Approximation algorithms, computational complexity, deletion-contraction algorithm]
On the Approximability of Budgeted Allocations and Improved Lower Bounds for Submodular Welfare Maximization and GAP
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
In this paper we consider the following maximum budgeted allocation (MBA) problem: Given a set of m indivisible items and n agents; each agent i willing to pay b<sub>ij</sub> on item j and with a maximum budget of B<sub>i</sub>, the goal is to allocate items to agents to maximize revenue. The problem naturally arises as auctioneer revenue maximization in budget-constrained auctions and as winner determination problem in combinatorial auctions when utilities of agents are budgeted-additive.We give a 3/4-approximation algorithm for MBA improving upon the previous best of sime0.632[2, 10]. Our techniques are based on a natural LP relaxation of MBA and our factor is optimal in the sense that it matches the integrality gap of the LP.We prove it is NP-hard to approximate MBA to any factor better than 15/16, previously only NP-hardness was known [21, 17]. Our result also implies NP- hardness of approximating maximum submodular welfare with demand oracle to a factor better than 15/16, improving upon the best known hardness of 275/276[10].Our hardness techniques can be modified to prove that it is NP-hard to approximate the Generalized Assignment Problem (GAP) to any factor better than 10/11. This improves upon the 422/423 hardness of [7, 9].We use iterative rounding on a natural LP relaxation of MBA to obtain the 3/4-approximation. We also give a (3/4 - epsiv) -factor algorithm based on the primal-dual schema which runs in O(nm) time, for any constant epsiv &gt; 0.
[TV, winner determination problem, combinatorial mathematics, demand oracle, commerce, generalized assignment problem, budget-constrained auction, 3/4-approximation algorithm, resource allocation, Allocation, Bismuth, Search engines, Europe, combinatorial auction, maximum submodular welfare, submodular welfare maximization, lower bound, Approximation Algorithms, Computer science, NP-hardness, budgeted allocation, Privatization, auctioneer revenue maximization, Approximation algorithms, Iterative algorithms, Internet, Resource management, computational complexity]
Submodular Approximation: Sampling-based Algorithms and Lower Bounds
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We introduce several generalizations of classical computer science problems obtained by replacing simpler objective functions with general submodular functions.The new problems include submodular load balancing, which generalizes load balancing or minimum-makespan scheduling, submodular sparsest cut and submodular balanced cut, which generalize their respective graph cut problems, as well as submodular function minimization with a cardinality lower bound. We establish upper and lower bounds for the approximability of these problems with a polynomial number of queries to a function-value oracle.The approximation guarantees for most of our algorithms are of the order of radic(n/ln n). We show that this is the inherent difficulty of the problems by proving matching lower bounds.We also give an improved lower bound for the problem of approximately learning a monotone submodular function. In addition, we present an algorithm for approximately learning submodular functions with special structure, whose guarantee is close to the lower bound. Although quite restrictive, the class of functions with this structure includes the ones that are used for lower bounds both by us and in previous work. This demonstrates that if there are significantly stronger lower bounds for this problem, they rely on more general submodular functions.
[sampling-based algorithm, sampling methods, graph theory, monotone submodular function, submodular approximation, Educational institutions, Application software, minimum-makespan scheduling, Computer science, graph cut problem, Processor scheduling, submodular load balancing, submodular balanced cut, function approximation, submodular function minimization, Approximation algorithms, Load management, Polynomials, submodular sparsest cut, minimisation, computational complexity]
Short Proofs May Be Spacious: An Optimal Separation of Space and Length in Resolution
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
A number of works have looked at the relationship between length and space of resolution proofs. A notorious question has been whether the existence of a short proof implies the existence of a proof that can be verified using limited space.In this paper we resolve the question by answering it negatively in the strongest possible way. We show that there are families of 6-CNF formulas of size n, for arbitrarily large n, that have resolution proofs of length O(n) but for which any proof requires space Omega(n/log n). This is the strongest asymptotic separation possible since any proof of length O(n) can always be transformed into a proof in space O(n / log n).Our result follows by reducing the space complexity of so called pebbling formulas over a directed acyclic graph to the black-white pebbling price of the graph.The proof is somewhat simpler than previous results (in particular, those reported in [Nordstrom 2006, Nordstrom and Hastad 2008]) as it uses a slightly different flavor of pebbling formulas which allows for a rather straightforward reduction of proof space to standard black-white pebbling price.
[pebbling games, 6-CNF formulas, short proofs, Length measurement, directed acyclic graph, Extraterrestrial measurements, Size measurement, asymptotic separation, resolution, Computer science, resolution proofs, pebbling formulas, Space technology, black-white pebbling price, directed graphs, Proof complexity, Polynomials, optimal separation, space complexity, computational complexity]
Noise Tolerance of Expanders and Sublinear Expander Reconstruction
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We consider the problem of online sublinear expander reconstruction and its relation to random walks in ``noisy" expanders. Given access to an adjacency list representation of a bounded-degree graph G, we want to convert this graph into a bounded-degree expander G' changing G as little aspossible. The graph G' will be output by a distributed filter: this is sublinear time procedure that given a query vertex, outputs all its neighbors in G', and can do so even in a distributed manner, ensuring consistency in all the answers.One of the main tools in our analysis is a result on the behavior of random walks in graph that are almost expanders: graphs that are formed by arbitrarily connecting a small unknown graph (the noise) to a large expander. We show that a random walk from almost any vertex in the expander part will have fast mixing properties, in the general setting of irreducible finite Markov chains. We alsodesign sublinear time procedures to distinguish vertices of the expander part from those in the noise part, and use this procedure in the reconstruction algorithm.
[Algorithm design and analysis, Pervasive computing, random walks, graph theory, distributed filter, Reconstruction algorithms, Graph theory, bounded-degree graph, Application software, Sublinear algorithms, online sublinear expander reconstruction, Computer science, query processing, Filters, Tree graphs, very large databases, bounded-degree expander, large data sets, Markov processes, Random Walks, expander noise tolerance, Large-scale systems, Expander reconstruction, Joining processes]
Sequence Length Requirement of Distance-Based Phylogeny Reconstruction: Breaking the Polynomial Barrier
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We introduce a new distance-based phylogeny reconstruction technique which provably achieves, at sufficiently short branch lengths, a sequence length requirement growing slower than any polynomial. The technique is based on a new averaging procedure that implicitly reconstructs ancestral sequences.In the same token, we extend previous results on phase transitions in phylogeny reconstruction to general time-reversible models. More precisely, we show that in the so-called Kesten-Stigum zone---roughly, a region of the parameter space where ancestral sequences are well approximated by ``linear combinations'' of observed sequences---sequences of length eradiclog n suffice for reconstruction. Here n is the number of extant species. We improve this result to poly(log n) the ultrametric case. Surprisingly, this last result suggests that a UPGMA-type algorithm may in some sense be "optimal'' under a molecular clock. Our results challenge---to some extent---the conventional wisdom that estimates of evolutionary distances alone carry significantly less information about phylogenies than full sequence datasets.
[Maximum likelihood estimation, Sequences, ancestral sequences, polynomials, Kesten-Stigum zone, Probability, polynomial barrier, sequence length requirement, evolutionary distances, Phylogeny, distance-based phylogeny reconstruction, Physics, short branch lengths, Convergence, Computer science, biology computing, UPGMA-type algorithm, DNA, Linear approximation, Polynomials]
Size Bounds and Query Plans for Relational Joins
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Relational joins are at the core of relational algebra, which in turn is the core of the standard database query language SQL. As their evaluation is expensive and very often dominated by the output size, it is an important task for database query optimisers to compute estimates on the size of joins and to find good execution plans for sequences of joins. We study these problems from a theoretical perspective, both in the worst-case model, and in an average-case model where the database is chosen according to a known probability distribution. In the former case, our first key observation is that the worst-case size of a query is characterised by the fractional edge cover number of its underlying hypergraph, a combinatorial parameter previously known to provide an upper bound. We complete the picture by proving a matching lower bound, and by showing that there exist queries for which the join-project plan suggested by the fractional edge cover approach may be substantially better than any join plan that does not use intermediate projections.
[fractional edge cover approach, query plan, combinatorial parameter, relational algebra, graph theory, Relational databases, query languages, Probability distribution, Database languages, Engines, hypergraph, Algebra, average-case model, probability distribution, Cost function, Database systems, probability, worst-case model, SQL, Computer science, relational joins, Upper bound, Query processing, size bound, database query language]
Eigenvalue Bounds, Spectral Partitioning, and Metrical Deformations via Flows
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We present a new method for upper bounding the second eigenvalue of theLaplacian of graphs. Our approach uses multi-commodity flows to deform the geometry of the graph; we embed the resulting metric into Euclidean space to recover a bound on the Rayleigh quotient. Using this, we show that every n-vertex graph of genus g and maximum degree d satisfies lambda<sub>2</sub>(G) = O((g+1)3d/n).This recovers the O(d/n) bound of Spielman and Teng for planar graphs, and compares to Kelner's bound of O((g+1)poly(d)/n), but our proof does not make use of conformal mappings or circle packings. We are thus able to extend this to resolve positively a conjecture of Spielman and Teng, by proving that lambda<sub>2</sub>(G) = O(dh6log h/n) whenever G is K<sub>h</sub>-minor free. This shows, in particular, that spectral partitioning can be used to recover O(radicn)-sized separators in bounded degree graphs that exclude a fixed minor. We extend this further by obtaining nearly optimal bounds on lambda<sub>2</sub> for graphs which exclude small-depth minors in the sense of Plotkin, Rao, and Smith. Consequently, we show that spectral algorithms find small separators in a general class of geometric graphs. Moreover, while the standard "sweep'' algorithm applied to the second eigenvector may fail to find good quotient cuts in graphs of unbounded degree, our approach produces a vector that works for arbitrary graphs. This yields an alternate proof of the result of Alon, Seymour, and Thomas that every excluded-minor family of graphs has O(radicn)-node balanced separators.
[Laplace equations, Rayleigh quotient, Particle separators, graph theory, sweep algorithm, planar graphs, Parallel machines, Extraterrestrial measurements, Partitioning algorithms, Finite element methods, metrical deformations, eigenvalues and eigenfunctions, Computer science, Geometry, Conformal mapping, eigenvalue bounds, Eigenvalues and eigenfunctions, multi-commodity flows, computational complexity, spectral partitioning]
Embeddings of Topological Graphs: Lossy Invariants, Linearization, and 2-Sums
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We study the properties of embeddings, multicommodity flows, and sparse cuts in minor-closed families of graphs which are also closed under 2-sums; this includes planar graphs, graphs of bounded treewidth, and constructions based on recursive edge replacement.
[sparse graph cut, topological graph embedding, trees (mathematics), linearization method, minor free graph, Mathematics, Graph theory, Topology, planar graph, lossy invariant, Computer science, Upper bound, Tree graphs, bounded treewidth graph, recursive edge replacement, multicommodity flow, Books, computational complexity]
A Simpler Linear Time Algorithm for Embedding Graphs into an Arbitrary Surface and the Genus of Graphs of Bounded Tree-Width
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
For every fixed surface S, orientable or non-orientable, and a given graph G, Mohar (STOC'96 and Siam J. Discrete Math. (1999)) described a linear time algorithm which yields either an embedding of G in S or a minor of G which is not embeddable in S and is minimal with this property. That algorithm, however, needs a lot of lemmas which spanned six additional papers. In this paper, we give a new linear time algorithm for the same problem. The advantages of our algorithm are the following: 1. The proof is considerably simpler: it needs only about 10 pages, and some results (with rather accessible proofs) from graph minors theory, while Mohar's original algorithm and its proof occupy more than 100 pages in total. 2. The hidden constant (depending on the genus g of the surface S) is much smaller. It is singly exponential in g, while it is doubly exponential in Mohar's algorithm. As a spinoff of our main result, we give another linear time algorithm, which is of independent interest. This algorithm computes the genus and constructs minimum genus embeddings of graphs of bounded tree-width. This resolves a conjecture by Neil Robertson and solves one of the most annoying long standing open question about complexity of algorithms on graphs of bounded tree-width.
[Embedded computing, bounded tree-width graph, embedding graphs, graph theory, Very large scale integration, Embedding, Genus of a graph, Graph theory, Mathematics, Surface, Computer science, Cyclic redundancy check, Linear time algorithm, Tree-width, Tree graphs, arbitrary surface, Polynomials, algorithm complexity, Informatics, simpler linear time algorithm, Testing]
Nearly Tight Low Stretch Spanning Trees
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We prove that any graph G with n points has a distribution T over spanning trees such that for any edge (u, v) the expected stretch E<sub>T~T</sub>[d<sub>T</sub>(u, nu)/d<sub>G</sub>(u, nu)] is bounded by Otilde(log n). Our result is obtained via a new approach of building "highways" between portals and a new strong diameter probabilistic decomposition theorem.
[Linear systems, trees (mathematics), portals, Mathematics, spanning tree low stretch, graph, Computer science, Road transportation, Upper bound, Tree graphs, Clustering algorithms, nearly tight low stretch spanning trees, Portals, computational complexity, probabilistic decomposition theorem]
Algorithmic Barriers from Phase Transitions
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
For many random constraint satisfaction problems, by now there exist asymptotically tight estimates of the largest constraint density for which solutions exist. At the same time, for many of these problems, all known polynomial-time algorithms stop finding solutions at much smaller densities. For example, it is well-known that it is easy to color a random graph using twice as many colors as its chromatic number. Indeed, some of the simplest possible coloring algorithms achieve this goal. Given the simplicity of those algorithms, one would expect room for improvement. Yet, to date, no algorithm is known that uses (2 - epsiv)chi colors, in spite of efforts by numerous researchers over the years. In view of the remarkable resilience of this factor of 2 against every algorithm hurled at it, we find it natural to inquire into its origin. We do so by analyzing the evolution of the set of k-colorings of a random graph, viewed as a subset of {1,...,k}n, as edges are added. We prove that the factor of 2 corresponds in a precise mathematical sense to a phase transition in the geometry of this set. Roughly speaking, we prove that the set of k-colorings looks like a giant ball for k ges 2chi, but like an error-correcting code for k les (2 - epsiv)chi. We also prove that an analogous phase transition occurs both in random k-SAT and in random hypergraph 2-coloring. And that for each of these three problems, the location of the transition corresponds to the point where all known polynomial-time algorithms fail. To prove our results we develop a general technique that allows us to establish rigorously much of the celebrated 1-step replica-symmetry-breaking hypothesis of statistical physics for random CSPs.
[estimation theory, Phase Transitions, computability, analogous phase transition, random k-SAT, Random Constraint Satisfaction Problems, graph colouring, asymptotically tight estimation, polynomial-time algorithms, Polynomials, Injuries, celebrated 1-step replica-symmetry-breaking hypothesis, error correction codes, constraint theory, random processes, random hypergraph 2-coloring, Physics, Resilience, Computer science, Geometry, Upper bound, statistical physics, Algorithms, Phase estimation, random constraint satisfaction problems, Error correction codes, Moment methods, error-correcting code, computational complexity]
Mixing Time of Exponential Random Graphs
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
A variety of random graph models have been developed in recent years to study a range of problems on networks, driven by the wide availability of data from many social, telecommunication, biochemical and other networks. A key model, extensively used in the sociology literature, is the exponential random graph model. This model seeks to incorporate in random graphs the notion of reciprocity, that is, the larger than expected number of triangles and other small subgraphs. Sampling from these distributions is crucial for parameter estimation hypothesis testing, and more generally for understanding basic features of the network model itself. In practice sampling is typically carried out using Markov chain Monte Carlo, in particular either the Glauber dynamics or the Metropolis-Hasting procedure.In this paper we characterize the high and low temperature regimes of the exponential random graph model. We establish that in the high temperature regime the mixing time of the Glauber dynamics is Theta(n2 log n), where n is the number of vertices in the graph; in contrast, we show that in the low temperature regime the mixing is exponentially slow for any local Markov chain. Our results, moreover, give a rigorous basis for criticisms made of such models. In the high temperature regime, where sampling with MCMC is possible, we show that any finite collection of edges are asymptotically independent; thus, the model does not possess the desired reciprocity property, and is not appreciably different from the Erdos-Renyi random graph.
[Parameter estimation, graph theory, Glauber dynamics, path coupling, Metropolis-Hasting procedure, Monte Carlo methods, Sociology, Statistical distributions, Markov chain Monte Carlo, pseudo-random graphs, parameter estimation, Large-scale systems, Testing, Temperature distribution, Maximum likelihood estimation, Social network services, sociology literature, Probability, mixing times, parameter estimation hypothesis testing, Markov processes, exponential random graphs, Sampling methods, key model, Erdos-Renyi random graph, mixing time]
k-Wise Independent Random Graphs
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
We study the k-wise independent relaxation of the usual model G(N,p) of random graphs where, as in this model, N labeled vertices are fixed and each edge is drawn with probability p, however, it is only required that the distribution of any subset of k edges is independent.This relaxation can be relevant in modeling phenomena where only k-wise independence is assumed to hold, and is also useful when the relevant graphs are so huge that handling G(N,p) graphs becomes infeasible, and cheaper random-looking distributions (such as k-wise independent ones) must be used instead. Unfortunately, many well-known properties of random graphs in G(N,p) are global, and it is thus not clear if they are guaranteed to hold in the k-wise independent case. We explore the properties of k-wise independent graphs by providing upper-bounds and lower-bounds on the amount of independence, k, required for maintaining the main properties of G(N,p) graphs: connectivity, Hamiltonicity, the connectivity-number, clique-number and chromatic-number and the appearance of fixed subgraphs. Most of these properties are shown to be captured by either constant k or by some k=poly(log(N)) for a wide range of values of p, implying that random looking graphs on N vertices can be generated by a seed of size poly(log(N)). The proofs combine combinatorial, probabilistic and spectral techniques.
[graph theory, random processes, Mathematics, random graphs, Computer science, clique-number, connectivity-number, USA Councils, Emulation, Hamiltonicity, chromatic-number, Sampling methods, Polynomials, k-wise independent random graphs, k-wise independence, Testing, computational complexity, k-wise independent relaxation]
Broadcasting with Side Information
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
A sender holds a word x consisting of n blocks x<sub>i</sub>, each of t bits, and wishes to broadcast a codeword to m receivers, R<sub>1</sub>,...,R<sub>m</sub>. Each receiver R<sub>i</sub> is interested in one block, and has prior side information consisting of some subset of the other blocks. Let beta<sub>t</sub> be the minimum number of bits that has to be transmitted when each block is of length t, and let beta be the limit beta=lim<sub>trarrinfin</sub>beta<sub>t</sub>/t. Informally, beta is the average communication cost per bit in each block (for long blocks). Finding the coding rate beta, for such an informed broadcast setting, generalizes several coding theoretic parameters related to Informed Source Coding on Demand, Index Coding and Network Coding. In this work we show that usage of large data blocks may strictly improve upon the trivial encoding which treats each bit in the block independently. To this end, we provide general bounds on beta<sub>t</sub>, and prove that for any constant C there is an explicit broadcast setting in which beta = 2 but beta<sub>1</sub>&gt; C. One of these examples answers a question of . In addition, we provide examples with the following counterintuitive direct-sum phenomena. Consider a union of several mutually independent broadcast settings. The optimal code for the combined setting may yield a significant saving in communication over concatenating optimal encodings for the individual settings. This result also provides new non-linear coding schemes which improve upon the largest known gap between linear and non-linear Network Coding, thus improving the results of. The proofs are based on a relation between this problem and results in the study of Witsenhausen's rate, OR graph products, colorings of Cayley graphs, and the chromatic numbers of Kneser graphs.
[codeword broadcasting, OR graph product, Cayley graph coloring, Costs, Video on demand, Satellite broadcasting, optimal encoding concatenation, nonlinear network coding, side information, Mathematics, set theory, Multimedia communication, graph colouring, USA Councils, block codes, Binary codes, nonlinear codes, source coding, Source coding, Witsenhausen rate, chromatic number, Kneser graph, broadcasting, concatenated codes, Computer science, Index coding, large data block, Network coding, Source coding on demand, trivial encoding, subset]
[Roster]
2008 49th Annual IEEE Symposium on Foundations of Computer Science
None
2008
Provides a listing of current committee members and society officers.
[]
Message from Program Chair
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Provides a listing of current committee members.
[]
Approximation Algorithms for Multicommodity-Type Problems with Guarantees Independent of the Graph Size
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Linial, London and Rabinovich [16] and Aumann and Rabani [3] proved that the min-cut max-flow ratio for general maximum concurrent flow problems (when there are k commodities) is O(logfe). Here we attempt to derive a more general theory of Steiner cut and flow problems, and we prove bounds that are poly-logarithmic in k for a much broader class of multicommodity flow and cut problems. Our structural results are motivated by the meta question: Suppose we are given a poly(log n) approximation algorithm for a flow or cut problem when can we give a poly(log k) approximation algorithm for a generalization of this problem to a Steiner cut or flow problem? Thus we require that these approximation guarantees be independent of the size of the graph, and only depend on the number of commodities (or the number of terminal nodes in a Steiner cut problem). For many natural applications (when k = no(1)) this yields much stronger guarantees. We construct vertex-sparsifiers that approximately preserve the value of all terminal min-cuts. We prove such sparsifiers exist through zero-sum games and metric geometry, and we construct such sparsifiers through oblivious routing guarantees. These results let us reduce a broad class of multicommodity-type problems to a uniform case (on k nodes) at the cost of a loss of a poly (log k) in the approximation guarantee. We then give poly(log k) approximation algorithms for a number of problems for which such results were previously unknown, such as requirement cut, 1-multicut, oblivious 0-extension, and natural Steiner generalizations of oblivious routing, min-cut linear arrangement and minimum linear arrangement.
[Costs, approximation guarantees, Routing, approximation algorithms, Geometry, Computer science, metric geometry, zero-sum games, polynomial approximation, vertex-sparsifiers, min-cut max-flow ratio, Approximation algorithms, multicommodity flow, multicommodity-type problems, computational complexity]
Faster Generation of Random Spanning Trees
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
In this paper, we set forth a new algorithm for generating approximately uniformly random spanning trees in undirected graphs. We show how to sample from a distribution that is within a multiplicative (1+ ¿) of uniform in expected time O¿(m¿n log 1/¿). This improves the sparse graph case of the best previously known worst-case bound of O(min {mn, n2.376}), which has stood for twenty years. To achieve this goal, we exploit the connection between random walks on graphs and electrical networks, and we use this to introduce a new approach to the problem that integrates discrete random walk-based techniques with continuous linear algebraic methods. We believe that our use of electrical networks and sparse linear system solvers in conjunction with random walks and combinatorial partitioning techniques is a useful paradigm that will find further applications in algorithmic graph theory.
[Linear systems, random walks on graphs, random spanning tree, trees (mathematics), discrete random walk-based technique, electrical network, sparse linear system solver, algorithmic graph theory, Graph theory, Partitioning algorithms, Random processes, Sparse matrices, spanning trees, sparse graph, Computer science, Tree graphs, electrical flows, undirected graph, continuous linear algebraic method, linear algebra, computational complexity, combinatorial partitioning technique]
Oblivious Routing for the Lp-norm
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Gupta et al. [13] introduced a very general multicommodity flow problem in which the cost of a given flow solution on a graph G = (V, E) is calculated by first computing the link loads via a load-function ¿, that describes the load of a link as a function of the flow traversing the link, and then aggregating the individual link loads into a single number via an aggregation function agg:R|E| ¿ R. In this paper we show the existence of an oblivious routing scheme with competitive ratio O(log n) and a lower bound of ¿(log n/log log n) for this model when the aggregation function agg is an L<sub>p</sub>-norm. Our results can also be viewed as a generalization of the work on approximating metrics by a distribution over dominating tree metrics (see e.g. [4], [5], [8]) and the work on minimum congestion oblivious routing [20], [14], [21]. We provide a convex combination of trees such that routing according to the tree distribution approximately minimizes the L<sub>p</sub>-norm of the link loads. The embedding techniques of Bartal [4], [5] and Fakcharoenphol et al. [8] can be viewed as solving this problem in the L<sub>1</sub>-norm while the result of Racke [21] solves it for L<sub>¿</sub>. We give a single proof that shows the existence of a good tree-based oblivious routing for any L<sub>p</sub>-norm. For the Euclidean norm, we also show that it is possible to compute a tree-based oblivious routing scheme in polynomial time.
[Algorithm design and analysis, tree distribution, Lp-norm, load-function, trees (mathematics), Telecommunication traffic, Quality of service, Routing, multicommodity flow problem, link loads, oblivious routing, tree metrics, norm, Computer science, competitive ratio, embedding techniques, aggregation function, tree-based oblivious routing, Cost function, Polynomials, polynomial time, computational complexity, metric embeddings]
Linear Systems over Composite Moduli
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We study solution sets to systems of 'generalized' linear equations of the form: ¿<sub>i</sub> (x<sub>1</sub>, x<sub>2</sub>, ···, x<sub>n</sub>) in ¿ A<sub>i</sub> (mod m) where ¿<sub>1</sub>,..., ¿<sub>t</sub> are linear forms in n Boolean variables, each A<sub>i</sub> is an arbitrary subset of Z<sub>m</sub>, and m is a composite integer that is a product of two distinct primes, like 6. Our main technical result is that such solution sets have exponentially small correlation, i.e. with the boolean function MOD<sub>q</sub>, when m and q are relatively prime. This bound is independent of the number t of equations. This yields progress on limiting the power of constant-depth circuits with modular gates. We derive the first exponential lower bound on the size of depth-three circuits of type MAJ o AND o MODA <sub>m</sub> (i.e having a MAJORITY gate at the top, AND/OR gates at the middle layer and generalized MOD<sub>m</sub> gates at the base) computing the function MOD<sub>q</sub>. This settles an open problem of Beigel and Maciel (Complexity'97) for the case of such modulus m. Our technique makes use of the work of Bourgain on estimating exponential sums involving a low-degree polynomial and ideas involving matrix rigidity from the work of Grigoriev and Razborov on arithmetic circuits over finite fields.
[Linear systems, OR gate, matrix rigidity, Circuits, Mathematics, Complexity theory, set theory, low degree polynomial, modular gates, Boolean functions, USA Councils, composite integer, Boolean variables, Boolean circuit complexity, Polynomials, generalized linear equations, AND gate, depth three circuits, arithmetic circuits, polynomials, generalized MOD gate, Boolean function, linear systems, Equations, matrix algebra, Computer science, constant-depth circuits, Upper bound, logic gates, composite moduli, exponential sums, exponential sum estimation, logic circuits, MAJORITY gate]
Multiparty Communication Complexity and Threshold Circuit Size of AC^0
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We prove an n¿(-1)/4k lower bound on the randomized k-party communication complexity of depth 4 AC0 functions in the number-on-forehead (NOF) model for up to ¿(log n) players. These are the first non-trivial lower bounds for general NOF multiparty communication complexity for any AC0 function for ¿ (log log n) players. For non-constant k the bounds are larger than all previous lower bounds for any AC0 function even for simultaneous communication complexity. Our lower bounds imply the first superpolynomial lower bounds for the simulation of AC0 by MAJ o SYMM o AND circuits, showing that the well-known quasipolynomial simulations of AC0 by such circuits are qualitatively optimal, even for formulas of small constant depth. We also exhibit a depth 5 formula in NP<sub>k</sub> cc - BPP<sub>k</sub> cc for k up to ¿(log n) and derive an ¿(2¿(log n/ ¿(k))) lower bound on the randomized k-party NOF communication complexity of set disjointness for up to ¿(log1/3 n) players which is significantly larger than the O (log log n) players allowed in the best previous lower bounds for multiparty set disjointness. We prove other strong results for depth 3 and 4 AC0 functions.
[circuit complexity, Protocols, Educational products, set disjointness function, Complexity theory, set theory, number-on-forehead model, communication complexity, Concurrent computing, Polynomials, randomized k-party communication complexity, Circuit simulation, Computational modeling, NOF multiparty communication complexity, SYMM circuits, MAJ circuits, superpolynomial lower bounds, lower bounds, AND circuit, Computer science, constant-depth circuits, Upper bound, AC0 functions, quasipolynomial simulations, logic circuits, threshold circuit size]
The Communication Complexity of Set-Disjointness with Small Sets and 0-1 Intersection
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
In this paper, we analyze the following communication complexity problem. It is a variant of the set-disjointness problem, denoted PDISJ<sub>log N</sub>, where each of Alice and Bob gets as an input a subset of [N] of size at most log N, with the promise that the intersection of the two subsets is of size at most 1. We provide an almost tight lower bound of ¿¿(log2 N) on the deterministic communication complexity of the problem. The main motivation for studying this problem comes from the so-called "clique vs. independent-set" problem, introduced by Yannakakis (1988). Proving an ¿(log2 N) lower bound on the communication complexity of the clique vs. independent-set problem for all graphs is a long standing open problem with various implications. Proving such a lower bound for random graphs is also open. In such a graph, both the cliques and the independent sets are of size O(log N) (and obviously their intersection is of size at most 1). Hence, our ¿¿(log2 N) lower bound for PDISJ<sub>log N</sub> can be viewed as a first step in this direction. Interestingly, we note that standard lower bound techniques cannot yield the desired lower bound. Hence, we develop a novel adversary argument that may find other applications.
[Context, clique-set problem, random graph, graph theory, 0-1 intersection, Linear programming, Complexity theory, set theory, Application software, communication complexity, Computational complexity, deterministic algorithms, deterministic communication complexity, Cryptographic protocols, Computer science, small sets, independent-set problem, Concrete, Cryptography, set-disjointness]
Polynomial Hierarchy, Betti Numbers and a Real Analogue of Toda's Theorem
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Toda proved in 1989 that the (discrete) polynomial time hierarchy, PH, is contained in the class P#P, namely the class of languages that can be decided by a Turing machine in polynomial time given access to an oracle with the power to compute a function in the counting complexity class #P. This result which illustrates the power of counting is considered to be a seminal result in computational complexity theory. An analogous result in the complexity theory over the reals (in the sense of BlumShub-Smale real Turing machines) has been missing so far. In this paper we formulate and prove a real analogue of Toda's theorem. Unlike Toda's proof in the discrete case, which relied on sophisticated combinatorial arguments, our proof is topological in nature. As a consequence of our techniques we are also able to relate the computational hardness of two extremely well-studied problems in algorithmic semi-algebraic geometry namely the problem of deciding sentences in the first order theory of the reals with a constant number of quantifier alternations, and that of computing Betti numbers of semi-algebraic sets. We obtain a polynomial time reduction of the compact version of the first problem to the second. This latter result might be of independent interest to researchers in algorithmic semi-algebraic geometry.
[Algorithm design and analysis, discrete polynomial time hierarchy, Semi-algebraic sets, Turing machine, computational geometry, real analogue, Mathematics, Complexity theory, History, programming languages, betti numbers, computational complexity theory, Turing machines, Polynomial hierarchy, Polynomials, oracle, Toda's theorem, polynomial hierarchy, Analog computers, Computational complexity, toda theorem, algorithmic semialgebraic geometry, Betti numbers, Computer science, Computational geometry, computational complexity]
Randomized Self-Assembly for Exact Shapes
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Working in Winfree's abstract tile assembly model, we show that a constant-size tile assembly system can be programmed through relative tile concentrations to build an n × n square with high probability, for any sufficiently large n. This answers an open question of Kao and Schweller (Randomized Self-Assembly for Approximate Shapes, ICALP 2008), who showed how to build an approximately n×n square using tile concentration programming, and asked whether the approximation could be made exact with high probability.
[Assembly systems, Temperature, Shape, Computational modeling, constant size tile assembly system, relative tile concentration, randomized self assembly, Winfree abstract tile assembly model, geometric programming, randomized algorithm, cellular automata, randomised algorithms, Computer science, molecular computation, self-assembly, Self-assembly, Tiles, USA Councils, DNA, tile concentration programming, Mathematical model, exact shape]
The Quantum and Classical Complexity of Translationally Invariant Tiling and Hamiltonian Problems
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We study the complexity of a class of problems involving satisfying constraints which remain the same under translations in one or more spatial directions. In this paper, we show hardness of a classical tiling problem on an (N x N) 2-dimensional grid and a quantum problem involving finding the ground state energy of a 1-dimensional quantum system of N particles. In both cases, the only input is N, provided in binary. We show that the classical problem is NEXP-complete and the quantum problem is QMAEXP-complete. Thus, an algorithm for these problems that runs in time polynomial in N (exponential in the input size) would imply EXP = NEXP or BQEXP = QMAEXP, respectively. Although tiling in general is already known to be NEXP-complete, to our knowledge, all previous reductions require that either the set of tiles and their constraints or some varying boundary conditions be given as part of the input. In the problem considered here, these are fixed, constant-sized parameters of the problem. Instead, the problem instance is encoded solely in the size of the system.
[translationally invariant tiling, Stationary state, classical complexity, Quantum Complexity, Boundary conditions, QMAEXP-complete, quantum complexity, Physics, Computer science, Quantum computing, Hamiltonian problem, Tiles, USA Councils, Tiling Complexity, Quantum mechanics, quantum computing, Constraint theory, Polynomials, time polynomial, Translational Invariance, NEXP-complete, computational complexity]
On Allocating Goods to Maximize Fairness
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We consider the Max-Min Allocation problem: given a set A of m agents and a set I of n items, where agent A ¿ A has utility u<sub>A</sub>,i for item i ¿ I, our goal is to allocate items to agents so as to maximize fairness. Specifically, the utility of an agent is the sum of its utilities for the items it receives, and we seek to maximize the minimum utility of any agent. While this problem has received much attention recently, its approximability has not been well-understood thus far: the best known approximation algorithm achieves an O¿(¿m)-approximation, and in contrast, the best known hardness of approximation stands at 2. Our main result is an algorithm that achieves an O¿(n¿)-approximation for any ¿ = ¿((log log n)/(log n)) in time nO(1/¿). In particular, we obtain poly-logarithmic approximation in quasipolynomial time, and for every constant ¿ &gt; 0, we obtain an O¿(n¿)-approximation in polynomial time. An interesting technical aspect of our algorithm is that we use as a building block a linear program whose integrality gap is ¿(¿m). We bypass this obstacle by iteratively using the solutions produced by the LP to construct new instances with significantly smaller integrality gaps, eventually obtaining the desired approximation. As a corollary of our main result, we also show that for any constant ¿ &gt; 0, an O(m¿)-approximation can be achieved in quasi-polynomial time. We also investigate the special case of the problem, where every item has non-zero utility for at most two agents. This problem is hard to approximate up to any factor better than 2. We give a factor 2-approximation algorithm.
[approximation theory, poly-logarithmic approximation, Approximation Algorithms, minimax techniques, Computer science, Information science, max-min allocation, Upper bound, resource allocation, Allocation Problems, goods allocation, Approximation algorithms, Polynomials, Iterative algorithms, Resource management, quasipolynomial time, computational complexity]
Online Stochastic Matching: Beating 1-1/e
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We study the online stochastic bipartite matching problem, in a form motivated by display ad allocation on the Internet. In the online, but adversarial case, the celebrated result of Karp, Vazirani and Vazirani gives an approximation ratio of 1- 1/e ¿ 0.632, a very familiar bound that holds for many online problems; further, the bound is tight in this case. In the online, stochastic case when nodes are drawn repeatedly from a known distribution, the greedy algorithm matches this approximation ratio, but still, no algorithm is known that beats the 1 - 1/e bound. Our main result is a 0.67-approximation online algorithm for stochastic bipartite matching, breaking this 1 - ¿ barrier. Furthermore, we show that no online algorithm can produce a 1 - ¿ approximation for an arbitrarily small e for this problem. Our algorithms are based on computing an optimal offline solution to the expected instance, and using this solution as a guideline in the process of online allocation. We employ a novel application of the idea of the power of two choices from load balancing: we compute two disjoint solutions to the expected instance, and use both of them in the online algorithm in a prescribed preference order. To identify these two disjoint solutions, we solve a max flow problem in a boosted flow graph, and then carefully decompose this maximum flow to two edge-disjoint (near-)matchings. In addition to guiding the online decision making, these two offline solutions are used to characterize an upper bound for the optimum in any scenario. This is done by identifying a cut whose value we can bound under the arrival distribution. At the end, we discuss extensions of our results to more general bipartite allocations that are important in a display ad application.
[Greedy algorithms, edge disjoint matchings, load balancing, cut, graph theory, Stochastic processes, online decision making, Displays, online problems, Guidelines, matching, resource allocation, online allocation, optimization, advertisement, stochastic, max flow problem, stochastic processes, greedy algorithm, bipartite matching problem, flow, display ad allocation, greedy algorithms, Decision making, Flow graphs, online stochastic matching, Upper bound, decision making, Approximation algorithms, Load management, online, Internet]
Instance-Optimal Geometric Algorithms
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We prove the existence of an algorithm A for computing 2-d or 3-dconvex hulls that is optimal for every point set in the following sense: for every set S of n points and for every algorithm A' in a certain class A, the running time of A on the worst permutation of S for A is at most a constant factor times the running time of A' on the worst permutation of S for A'. In fact, we can establish a stronger property: for every S and A', the running time of A on S is at most a constant factor times the average running time of A' over all permutations of S. We call algorithms satisfying these properties instance-optimal in the order-oblivious and random-order setting. Such instance-optimal algorithms simultaneously subsume output-sensitive algorithms and distribution-dependent average-case algorithms, and all algorithms that do not take advantage of the order of the input or that assume the input is given in a random order. The class A under consideration consists of all algorithms in a decision tree model where the tests involve only multilinear functions with a constant number of arguments. To establish an instance-specific lower bound, we deviate from traditional Ben-Or-style proofs and adopt an interesting adversary argument. For 2-d convex hulls, we prove that a version of the well known algorithm by Kirkpatrick and Seidel (1986) or Chan, Snoeyink, and Yap(1995) already attains this lower bound. For 3-d convex hulls, we propose a new algorithm. We further obtain instance-optimal results for a few other standard problems in computational geometry, such as maxima in 2-d and 3-d, orthogonal line segment intersection in 2-d, offline orthogonal range searching in 2-d, off-line halfspace range reporting in 2-d and 3-d, and off-line point location in 2-d. The theory we develop also neatly reveals connections to entropy-dependent data structures, and yields as a byproduct new expected case results, e.g., for on-line orthogonal range counting in 2-d.
[3d convex hulls, Costs, Adaptive algorithm, entropy-dependent data structures, computational geometry, decision tree model, orthogonal line segment intersection, maxima, instance-optimal geometric algorithms, 2d convex hulls, adaptive algorithms, point location, Decision trees, off-line orthogonal range searching, instance optimality, Testing, Tree data structures, orthogonal segment intersection, multilinear functions, Computational modeling, Data structures, Size measurement, convex programming, geometric programming, off-line point location, off-line halfspace range reporting, convex hull, lower bounds, distribution-dependent average-case algorithms, Computer science, output-sensitive algorithms, Computational geometry, entropy-sensitive data structures, on-line orthogonal range, decision trees, random-order setting]
Delaunay Triangulations in O(sort(n)) Time and More
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We present several results about Delaunay triangulations (DTs) and convex hulls in transdichotomous and hereditary settings: (i) the DT of a planar point set can be computed in expected time O(sort(n)) on a word RAM, where sort(n) is the time to sort n numbers. We assume that the word RAM supports the shuffle-operation in constant time; (ii) if we know the ordering of a planar point set in x- and in y-direction, its DT can be found by a randomized algebraic computation tree of expected linear depth; (iii) given a universe U of points in the plane, we construct a data structure D for Delaunay queries: for any P ¿ U, D can find the DT of P in time O(|P|log log|U|); (iv) given a universe U of points in 3-space in general convex position, there is a data structure D for convex hull queries: for any P ¿ U, D can find the convex hull of P in time O(|P|(log log|U|)2); (v) given a convex polytope in 3-space with n vertices which are colored with ¿ &gt; 2 colors, we can split it into the convex hulls of the individual color classes in time O(n(log log n)2). The results (i)-(iii) generalize to higher dimensions. We need a wide range of techniques. Most prominently, we describe a reduction from DTs to nearest-neighbor graphs that relies on a new variant of randomized incremental constructions using dependent sampling.
[Fuses, data structure, shuffle-operation, nearest-neighbor graphs, convex hulls, Mathematics, transdichotomous algorithm, USA Councils, Parallel processing, hereditary algorithm, Tree data structures, dependent sampling, Read-write memory, randomized algebraic computation tree, mesh generation, Delaunay triangulation, Sorting, Computer science, word RAM, Computational geometry, transdichotomous, Delaunay queries, Delaunay triangulations, Sampling methods, computational complexity, RAM]
Orthogonal Range Reporting in Three and Higher Dimensions
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
In orthogonal range reporting we are to preprocess N points in d-dimensional space so that the points inside a d-dimensional axis-aligned query box can be reported efficiently. This is a fundamental problem in various fields, including spatial databases and computational geometry. In this paper we provide a number of improvements for three and higher dimensional orthogonal range reporting: In the pointer machine model, we improve all the best previous results, some of which have not seen any improvements in almost two decades. In the I/O-model, we improve the previously known three-dimensional structures and provide the first (non-trivial) structures for four and higher dimensions.
[Solid modeling, d-dimensional axis-aligned query box, Computational modeling, Random access memory, Read-write memory, Binary search trees, computational geometry, visual databases, orthogonal range searching, Data structures, Spatial databases, spatial databases, external memory, Computer science, query processing, Computational geometry, Councils, dimensional orthogonal range reporting, pointer machine model, data structures, computational complexity]
Decomposing Coverings and the Planar Sensor Cover Problem
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We show that a k-fold covering using translates of an arbitrary convex polygon can be decomposed into Omega(k) covers (using an efficient algorithm). We generalize this result to obtain a constant factor approximation to the sensor cover problem where the ranges of the sensors are translates of a given convex polygon. The crucial ingredient in this generalization is a constant factor approximation algorithm for a one-dimensional version of the sensor cover problem, called the Restricted Strip Cover (RSC) problem, where sensors are intervals of possibly different lengths. Our algorithm for RSC improves on the previous O(log log log n) approximation.
[Strips, coverings decomposition, factor approximation algorithm, k-fold covering, Restricted Strip Cover, computational geometry, Batteries, Approximation Algorithms, Computer science, Processor scheduling, sensors, USA Councils, Sensor Cover, restricted strip cover, Cities and towns, Approximation algorithms, planar sensor cover problem, Polynomials, Decomposing Multiple Coverings, arbitrary convex polygon]
Bounded Independence Fools Halfspaces
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We show that any distribution on {-1,+1}n that is k-wise independent fools any halfspace (a.k.a. threshold) h : {-1,+1}n ¿ {-1,+1}, i.e., any function of the form h(x) = sign(¿<sub>i=1</sub> n w<sub>i</sub>X<sub>i</sub> - ¿) where the w<sub>1</sub>,..., w<sub>n</sub>, ¿ are arbitrary real numbers, with error ¿ for k = O(¿-2 log2(1/¿)). Our result is tight up to log(1/¿) factors. Using standard constructions of k-wise independent distributions, we obtain the first explicit pseudorandom generators G : {-1,+1}s ¿ {-1,+1}n that fool halfspaces. Specifically, we fool halfspaces with error e and seed length s = k · log n = O(log n · ¿-2 log2(1/¿)). Our approach combines classical tools from real approximation theory with structural results on halfspaces by Servedio (Comput. Complexity 2007).
[Bounded Independence Fools Halfspaces, approximation theory, log normal distribution, Circuits, Educational institutions, Boosting, Approximation methods, random number generation, Game theory, Computer science, Support vector machines, Information science, Boolean functions, halfspaces, Voting, k-wise independent fools, k-wise independent distributions, Silicon, pseudorandomness, explicit pseudorandom generators, computational complexity, distribution function]
Extensions to the Method of Multiplicities, with Applications to Kakeya Sets and Mergers
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We extend the "method of multiplicities" to get the following results, of interest in combinatorics and randomness extraction. 1) We show that every Kakeya set (a set of points that contains a line in every direction) in F<sub>q</sub> n must be of size at least qn/2n. This bound is tight to within a 2 + o(1) factor for every n as q ? ?, compared to previous bounds that were off by exponential factors in n. 2) We give an improved construction of "randomness mergers". Mergers are seeded functions that take as input ? (possibly correlated) random variables in {0,1}N and a short random seed, and output a single random variable in {0,1}N that is statistically close to having entropy (1 - ?) ? N when one of the ? input variables is distributed uniformly. The seed we require is only (1/?) ? log ?-bits long, which significantly improves upon previous construction of mergers. 3) We show how to construct randomness extractors that use logarithmic length seeds while extracting 1 - o(1) fraction of the min-entropy of the source. Previous results could extract only a constant fraction of the entropy while maintaining logarithmic seed length. The "method of multiplicities\
[randomness mergers, Schwartz-Zippel lemma, combinatorial mathematics, Input variables, Entropy, set theory, interpolating polynomials, Polynomial method, Kakeya sets, multiplicities method, entropy, randomness extraction, Polynomials, combinatorics, Corporate acquisitions, polynomials, random processes, min-entropy, Functional analysis, Application software, Combinatorial mathematics, Galois fields, randomness extractors, logarithmic seed length, Computer science, Extractors, Randomness, Random variables]
Constructing Small-Bias Sets from Algebraic-Geometric Codes
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We give an explicit construction of an ¿-biased set over k bits of size O(k/¿2 log(1/¿))5/4This improves upon previous explicit constructions when e is roughly (ignoring logarithmic factors) in the range [k-1.5,k-0.5]. The construction builds on an algebraic-geometric code. However, unlike previous constructions we use low-degree divisors whose degree is significantly smaller than the genus. Studying the limits of our technique, we arrive at a hypothesis that if true implies the existence of e-biased sets with parameters nearly matching the lower bound, and in particular giving binary error correcting codes beating the Gilbert-Varshamov bound.
[error correction codes, small-bias sets, Graph theory, Galois fields, Computer science, Gilbert-Varshamov bound, algebraic geometric codes, binary error correcting codes, Binary codes, Error correction codes, Random variables, Error correction, Contracts, small bias sets, algebraic-geometric codes]
Blackbox Polynomial Identity Testing for Depth 3 Circuits
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We study ¿¿¿(k) circuits, i.e., depth three arithmetic circuits with top fanin k. We give the first deterministic polynomial time blackbox identity test for ¿¿¿(k) circuits over the field Q of rational numbers, thus resolving a question posed by Klivans and Spielman (STOC 2001). Our main technical result is a structural theorem for ¿¿¿(k) circuits that compute the zero polynomial. In particular we show that if a ¿¿¿(k) circuit C = ¿<sub>i¿[k]</sub> A<sub>i</sub> = ¿<sub>i¿[k]</sub> ¿<sub>j¿[d]</sub> ¿<sub>ij</sub> computing the zero polynomial, where each A<sub>i</sub> is a product of linear forms with coefficients in ¿, is simple (gcd{A<sub>i</sub> | i ¿ [k]} = 1) and minimal (for all proper nonempty subsets S ¿ [k], ¿<sub>i¿S</sub> A<sub>i</sub> ¿ 0), then the rank (dimension of the span of the linear forms {¿<sub>ij</sub> | i ¿ [k],j ¿ [d]}) of C can be upper bounded by a function only of k. This proves a weak form of a conjecture of Dvir and Shpilka (STOC 2005) on the structure of identically zero depth three arithmetic circuits. Our blackbox identity test follows from this structural theorem by combining it with a construction of Karnin and Shpilka (CCC 2008). Our proof of the structure theorem exploits the geometry of finite point sets in ¿n. We identify the linear forms appearing in the circuit C with points in ¿n. We then show how to apply high dimensional versions of the Sylvester-Gallai Theorem, a theorem from incidence-geometry, to identify a special linear form appearing in C, such that on the subspace where the linear form vanishes, C restricts to a simpler circuit computing the zero polynomial. This allows us to build an inductive argument bounding the rank of our circuit. While the utility of such theorems from incidence geometry for identity testing has been hinted at before, our proof is the first to develop the connection fully and utilize it effectively.
[Modular construction, Sylvester&#150;Gallai Theorem, depth three arithmetic circuits, incidence-geometry theorem, Circuit testing, Information geometry, structure theorem proving, digital arithmetic, Polynomials, theorem proving, Derandomization, blackbox polynomial identity testing, polynomials, zero polynomial, structural theorem, inductive argument, depth 3 circuits, Galois fields, Computer science, Interpolation, Upper bound, Arithmetic circuits, circuit testing, Digital arithmetic, Sylvester-Gallai theorem]
A New Probability Inequality Using Typical Moments and Concentration Results
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We present two probability inequalities. The simpler first inequality weakens both hypotheses in Hoffding-Azumaine quality. Using it, we generalize concentration results previously known for the uniform density for the TSP, MWST and Random Projections to long-tailed inhomogeneous distributions. The second more complicated inequality further weakens the moment requirements and using it, we prove the best possible concentration for the long-studied bin packing problem as well as some others.
[Algorithm design and analysis, Data analysis, probability inequality, Stochastic processes, probability, Gaussian distribution, Probability distribution, Long-tailed distributions, Concentration, bin packing, Probability Inequality, Computer science, Tree graphs, concentration results, Tail, typical moments, Random variables, long-tailed inhomogeneous distributions, bin packing problem, Hoffding-Azuma inequality]
A Probabilistic Inequality with Applications to Threshold Direct-Product Theorems
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We prove a simple concentration inequality, which is an extension of the Chernoff bound and Hoeffding's inequality for binary random variables. Instead of assuming independence of the variables we use a slightly weaker condition, namely bounds on the co-moments. This inequality allows us to simplify and strengthen several known direct-product theorems and establish new threshold direct-product theorems. Threshold direct-product theorems are statements of the following form: If one instance of a problem can be solved with probability at most p, then solving significantly more than a p-fraction among multiple instances has negligible probability. Results of this kind are crucial when distinguishing whether a process succeeds with probability s or c, for 0 &lt; s &lt; c &lt; 1. Here standard direct-product theorems are of no help since even a process which can solve one instance with probability c will only be able to solve all k instances with exponentially small probability. Using our concentration inequality we show how to obtain threshold (and standard) direct-product theorems from known XOR Lemmas. We give examples of this approach and obtain (threshold) direct-product theorems for quantum XOR games, quantum random access codes, 2-party and multi-party communication complexity and circuits. Similar results can be obtained for other models of computation, e.g. polynomials over GF(2). It is well-known that direct-product theorems and XOR Lemmas are "essentially" equivalent. We show that one direction is often even tight: going from XOR Lemmas to (threshold) direct-product theorems is possible in an information-theoretically optimal way. We believe that our inequality has applications in other contexts as well.
[Direct-product Theorem, Computational modeling, direct product theorems, Circuits, probability, probabilistic inequality, Chernoff bound, XOR Lemmas, Complexity theory, multiparty communication complexity, Application software, Game theory, Computer science, simple concentration inequality, USA Councils, Quantum mechanics, binary random variables, XOR Lemma, threshold direct product theorems, Polynomials, Concentration inequality, Random variables, Hoeffding inequality, multiparty communication circuits]
Choice-Memory Tradeoff in Allocations
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
In the classical balls-and-bins paradigm, where n balls are placed independently and uniformly in n bins, typically the number of bins with at least two balls in them is ¿(n) and the maximum number of balls in a bin is ¿((log n)/(log log n)). It is well known that when each round offers k independent uniform options for bins, it is possible to typically achieve a constant maximal load if and only if k = ¿(log n). Moreover, it is possible whp to avoid any collisions between n/2 balls if k &gt; log<sub>2</sub> n. In this work, we extend this into the setting where only m bits of memory are available. We establish a tradeoff between the number of choices k and the memory m, dictated by the quantity km/n. Roughly put, we show that for km ¿ n one can achieve a constant maximal load, while for km ¿n no substantial improvement can be gained over the case k = 1 (i.e., a random allocation). For any k = ¿(log n) and m = ¿(log2 n), one can typically achieve a constant load if km = ¿(n), yet the load is unbounded if km = o(n). Similarly, if km &gt; Cn then n/2 balls can be allocated without any collisions whp, whereas for km &lt; ¿n there are typically ¿(n) collisions. Furthermore, we show that the load is whp at least (log(n/m))/(log k+log log(n/m)). In particular, whenever k ¿ polylog(n), if m=n1-¿ the optimal maximal load is ¿((log n)/(log log n)) (the same as in the case k = 1), while m = 2n suffices to ensure a constant load. Finally, we analyze non-adaptive allocation algorithms and give tight upper and lower bounds for their performance.
[Algorithm design and analysis, Lower bounds on memory, choice-memory tradeoff, optimal maximal load, Online perfect matching, Application software, Balls and bins paradigm, Research and development, Computer science, Geometry, nonadaptive allocation algorithms, USA Councils, Memory management, balls-and-bins paradigm, Space / performance tradeoffs, Balanced allocations, Load management, Performance analysis, Context modeling, computational complexity]
A Parallel Repetition Theorem for Any Interactive Argument
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
The question of whether or not parallel repetition reduces the soundness error is a fundamental question in the theory of protocols. While parallel repetition reduces (at an exponential rate) the error in interactive proofs and (at a weak exponential rate) in special cases of interactive arguments (e.g., 3-message protocols-Bellare, Impagliazzo and Naor [FOCS '97], and public-coin protocols-Haastad, Pass, Pietrzak and Wikstrom [Manuscript '08]), Bellare et. al gave an example of interactive arguments for which parallel repetition does not reduce the soundness error at all. We show that by slightly modifying any interactive argument, in a way that preserves its completeness and only slightly deteriorates its soundness, we get a protocol for which parallel repetition does reduce the error at a weak exponential rate. In this modified version, the verifier flips at the beginning of each round an (1 - 1/4 m), 1/4 m) biased coin (i.e., 1 is tossed with probability 1/4 m), where m is the round complexity of the (original) protocol. If the coin is one, the verifier halts the interaction and accepts, otherwise it sends the same message that the original verifier would. At the end of the protocol (if reached), the verifier accepts if and only if the original verifier would.
[parallel repetition theorem, Protocols, protocol theory, cryptographic protocols, Computational modeling, soundness error, probability, round complexity, hardness amplification, Computer science, Concurrent computing, weak exponential rate, computationally sound proofs, interactive proof, Computer errors, interactive arguments, Polynomials, interactive argument, parallel repetition, computational complexity]
Resolving the Simultaneous Resettability Conjecture and a New Non-Black-Box Simulation Strategy
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Canetti, Goldreich, Goldwasser, and Micali (STOC 2000) introduced the notion of resettable zero-knowledge proofs, where the protocol must be zero-knowledge even if a cheating verifier can reset the prover and have several interactions in which the prover uses the same random tape. Soon afterwards, Barak, Goldreich, Goldwasser, and Lindell (FOCS 2001) studied the closely related notion of resettable soundness, where the soundness condition of the protocol must hold even if the cheating prover can reset the verifier to have multiple interactions with the same verifier's random tape. The main problem left open by this work was whether it is possible to have a single protocol that is simultaneously resettable zero knowledge and resettably sound. We resolve this question by constructing such a protocol. At the heart of our construction is a new non-black-box simulation strategy, which we believe to be of independent interest. This new strategy allows for simulators which "marry'' recursive rewinding techniques (common in the context of concurrent simulation) with non-black-box simulation. Previous non-black-box strategies led to exponential blowups in computational complexity in such circumstances, which our new strategy is able to avoid.
[Heart, Context-aware services, Zero-knowledge, cryptographic protocols, Computational modeling, Computer simulation, simultaneous resettability conjecture, zero-knowledge proof, Educational institutions, Cryptographic protocols, Computer science, Protocol Composition, Content addressable storage, Randomness, Reset Attacks, non-black-box simulation, Cryptography, Context modeling, computational complexity]
Extracting Correlations
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Motivated by applications in cryptography, we consider a generalization of randomness extraction and the related notion of privacy amplification to the case of two correlated sources. We introduce the notion of correlation extractors, which extract nearly perfect independent instances of a given joint distribution from imperfect, or "leaky," instances of the same distribution. More concretely, suppose that Alice holds a and Bob holds b, where (a, b) are obtained by taking n independent samples from a joint distribution (X, Y) and letting a include all X instances and b include all Y instances. An adversary Eve obtains partial information about (a, b) by choosing a function L with output length t and learning L(a, b). The goal is to design a protocol between Alice and Bob which may use additional fresh randomness, such that for every L as above the following holds. In the end of the interaction, Alice outputs a' and Bob outputs b' such that (a', b') are statistically indistinguishable from m independent instances of (X, Y) even when conditioned on Eve's view, and even when conditioned on the joint view of Eve together with either Alice or Bob. The standard questions of privacy amplification and randomness extraction correspond to the case where X and Y are identical random bits. In this work we address this question for other types of correlations. A central special case is that of OT extractors, which are correlation extractors for the correlation (X, Y) corresponding to the cryptographic primitive of oblivious transfer. Our main result is that for any finite joint distribution (X, Y) there is an explicit correlation extractor which extracts m = ?(n) instances using O(n) bits of communication, even when t = ?(n) bits of information can be leaked to Eve. We present several applications which motivate the concept of correlation extractors and our main result. These include: ? Protecting certain cryptographic protocols against sidechannel attacks. ? A protocol which realizes m instances of oblivious transfer by communicating only O(m) bits. The security of the protocol relies on a number-theoretic intractability assumption. ? A constant-rate unconditionally secure construction of oblivious transfer (for semi-honest parties) from any nontrivial channel. This establishes constant-rate equivalence of any two nontrivial finite channels.
[cryptographic protocols, number-theoretic intractability assumption, leakage-resilient cryptography, privacy amplification, side channel attacks, Mathematics, Data mining, secure computation, Privacy, randomness extraction, cryptographic protocols security, oblivious transfer, Cryptography, noisy channels, Protection, nontrivial finite channels, Technological innovation, correlations extractors, cryptography, Application software, randomness extractors, Cryptographic protocols, Computer science, Information security, finite joint distribution, random functions, data privacy]
Settling the Complexity of Arrow-Debreu Equilibria in Markets with Additively Separable Utilities
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We prove that the problem of computing an Arrow-Debreu market equilibrium is PPAD-complete even when all traders use additively separable, piecewise-linear and concave utility functions. In fact, our proof shows that this market-equilibrium problem does not have a fully polynomial-time approximation scheme, unless every problem in PPAD is solvable in polynomial time.
[Piecewise linear techniques, Piecewise linear approximation, Arrow-Debreu markets, Nash equilibrium, Computational complexity, additively separable utilities, Computer science, Programmable control, marketing, USA Councils, Arrow-Debreu equilibria, PPAD-complete, Pricing, PPAD-completeness, market-equilibrium problem, Polynomials, polynomial time, computational complexity]
Reducibility among Fractional Stability Problems
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
In a landmark paper, Papadimitriou introduced a number of syntactic subclasses of TFNP based on proof styles that (unlike TFNP) admit complete problems. A recent series of results has shown that finding Nash equilibria is complete for PPAD, a particularly notable subclass of TFNP. A major goal of this work is to expand the universe of known PPAD-complete problems. We resolve the computational complexity of a number of outstanding open problems with practical applications. Here is the list of problems we show to be PPAD-complete, along with the domains of practical significance: Fractional Stable Paths Problem (FSPP) - Internet routing; Core of Balanced Games - Economics and Game theory; Scarf's Lemma - Combinatorics; Hypergraph Matching - Social Choice and Preference Systems; Fractional Bounded Budget Connection Games (FBBC) - Social networks; and Strong Fractional Kernel - Graph Theory. In fact, we show that no fully polynomial-time approximation schemes exist (unless PPAD is in FP). This paper is entirely a series of reductions that build in nontrivial ways on the framework established in previous work. In the course of deriving these reductions, we created two new concepts - preference games and personalized equilibria. The entire set of new reductions can be presented as a lattice with the above problems sandwiched between preference games (at the "easy" end) and personalized equilibria (at the "hard" end). Our completeness results extend to natural approximate versions of most of these problems. On a technical note, we wish to highlight our novel "continuous-to-discrete" reduction from exact personalized equilibria to approximate personalized equilibria using a linear program augmented with an exponential number of "min" constraints of a specific form. In addition to enhancing our repertoire of PPAD-complete problems, we expect the concepts and techniques in this paper to find future use in algorithmic game theory.
[personalized equilibria approximation, graph theory, min constraints, linear programming, Complexity theory, balanced games, social choice system, proof styles, TFNP, preference games, Nash equilibria, preference System, polynomial-time approximation schemes, continuous-to-discrete reduction, Polynomials, theorem proving, PPAD-complete problems, IP networks, Scarf's lemma, Kernel, combinatorics, fractional bounded budget connection games, approximation theory, Stability, Social network services, social networks, game theory, Routing, strong fractional kernel, linear program, algorithmic game theory, Graph theory, Game theory, Computational complexity, Combinatorial mathematics, Internet routing, fractional stability problem, economics, lattice, fractional stable paths problem, syntactic subclass, hypergraph matching, computational complexity]
Convergence of Local Dynamics to Balanced Outcomes in Exchange Networks
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Bargaining games on exchange networks have been studied by both economists and sociologists. A Balanced Outcome for such a game is an equilibrium concept that combines notions of stability and fairness. In a recent paper, Kleinberg and Tardos introduced balanced outcomes to the computer science community and provided a polynomial-time algorithm to compute the set of such outcomes. Their work left open a pertinent question: are there natural, local dynamics that converge quickly to a balanced outcome? In this paper, we provide a partial answer to this question by showing that simple edge-balancing dynamics converge to a balanced outcome whenever one exists.
[Stability, convergence, game theory, balanced outcome, network theory (graphs), exchange networks, equilibrium concept, Game theory, Convergence, edge-balancing dynamics convergence, Computer science, Network topology, Sociology, Polynomials, bargaining games, polynomial-time algorithm, computational complexity]
Convergence to Equilibrium in Local Interaction Games
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We study a simple game theoretic model for the spread of an innovation in a network. The diffusion of the innovation is modeled as the dynamics of a coordination game in which the adoption of a common strategy between players has a higher payoff. Classical results in game theory provide a simple condition for an innovation to become widespread in the network. The present paper characterizes the rate of convergence as a function of graph structure. In particular, we derive a dichotomy between well-connected (e.g. random) graphs that show slow convergence and poorly connected, low dimensional graphs that show fast convergence.
[Economics, Technological innovation, Social network services, convergence, local interaction games, Theory, Diffusion processes, dichotomy, game theory, network theory (graphs), simple game theoretic model, Game theory, Convergence, Computer science, coordination game dynamics, Algorithms, Space technology, convergence rate, graph structure, Clocks]
Exact and Approximate Pattern Matching in the Streaming Model
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We present a fully online randomized algorithm for the classical pattern matching problem that uses merely O(log m) space, breaking the O(m) barrier that held for this problem for a long time. Our method can be used as a tool in many practical applications, including monitoring Internet traffic and firewall applications. In our online model we first receive the pattern P of size m and preprocess it. After the preprocessing phase, the characters of the text T of size n arrive one at a time in an online fashion. For each index of the text input we indicate whether the pattern matches the text at that location index or not. Clearly, for index i, an indication can only be given once all characters from index i till index i+m-1 have arrived. Our goal is to provide such answers while using minimal space, and while spending as little time as possible on each character (time and space which are in O(poly(log n)) ).We present an algorithm whereby both false positive and false negative answers are allowed with probability of at most 1/n3. Thus, overall, the correct answer for all positions is returned with a probability of 1/n2. The time which our algorithm spends on each input character is bounded by O(log m), and the space complexity is O(log m) words. We also present a solution in the same model for the pattern matching with k mismatches problem. In this problem, a match means allowing up to k symbol mismatches between the pattern and the subtext beginning at index i. We provide an algorithm in which the time spent on each character is bounded by O(k2poly(log m)), and the space complexity is O(k3poly(log m)) words.
[Algorithm design and analysis, pattern matching, Hamming distance, streaming model, probability, Fingerprint recognition, online randomized algorithm, Internet traffic, Computer science, randomize algorithm, approximate pattern matching, streaming, combinatorial algorithm, Traffic control, Internet, Large-scale systems, Pattern matching, Monitoring, space complexity, Computational biology, computational complexity]
Efficient Sketches for Earth-Mover Distance, with Applications
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We provide the first sub-linear sketching algorithm for estimating the planar Earth-Mover Distance with a constant approximation. For sets living in the two-dimensional grid [¿]2, we achieve space ¿¿ for approximation O(1/¿), for any desired 0 &lt; ¿ &lt; 1. Our sketch has immediate applications to the streaming and nearest neighbor search problems.
[Algorithm design and analysis, efficient sketches, Image recognition, constant approximation, computational geometry, Application software, Nearest neighbor searches, Design optimization, Computer science, two dimensional grid, sketching, streaming, Earth-Mover Distance, sublinear sketching algorithm, Streaming media, Approximation algorithms, Cost function, embedding, earth mover distance, Kernel]
Models for the Compressible Web
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Graphs resulting from human behavior (the web graph, friendship graphs, etc.) have hitherto been viewed as a monolithic class of graphs with similar characteristics; for instance, their degree distributions are markedly heavy-tailed. In this paper we take our understanding of behavioral graphs a step further by showing that an intriguing empirical property of web graphs-their compressibility-cannot be exhibited by well-known graph models for the web and for social networks. We then develop amore nuanced model for web graphs and show that it does exhibit compressibility, in addition to previously modeled web graph properties.
[compressible Web, Statistical analysis, Social network services, Humans, Stochastic processes, Probability, Computer science, USA Councils, Web graphs, social networking (online), Computer networks, Internet, Power generation, Web search]
The Intersection of Two Halfspaces Has High Threshold Degree
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
The threshold degree of a Boolean function f: {0, 1}n ¿ {-1, +1} is the least degree of a real polynomial p such f(x) ¿ sgn p(x). We construct two halfspaces on {0,1}n whose intersection has threshold degree ¿(¿(n)), an exponential improvement on previous lower bounds. This solves an open problem due to Klivans (2002) and rules out the use of perceptronbased techniques for PAC learning the intersection of two halfspaces, a central unresolved challenge in computational learning. We also prove that the intersection of two majority functions has threshold degree ¿(log n), which is tight and settles a conjecture of O'Donnell and Servedio (2003). Our proof consists of two parts. First, we show that for any Boolean functions f and g, the intersection f(x) ¿ g(y) has threshold degree O(d) if and only if ¿f - F||<sub>¿</sub> + ||g - G||<sub>¿</sub> &lt; 1 for some rational functions F, G of degree O(d). Second, we settle the least degree required for approximating a halfspace and a majority function to any given accuracy by rational functions. Our technique further allows us to make progress on Aaronson's challenge (2008) and contribute strong direct product theorems for the threshold degree of composed Boolean functions of the form F(f<sub>1</sub>, ..., f<sub>n</sub>). Essentially the only previous technique for analyzing the threshold degree was symmetrization (1969).
[Circuits, Complexity theory, real polynomial, Boolean functions, Quantum computing, rational functions, intersections of halfspaces, polynomial representations of Boolean functions, Hypercubes, product theorems, rational approximation, Polynomials, learning (artificial intelligence), threshold degree, perceptron-based techniques, PAC learning, Computational modeling, perceptrons, polynomials, direct product theorems, Boolean function, Application software, Computer science, Quantum mechanics, computational learning]
Breaking the Multicommodity Flow Barrier for O(&#x0221A;log n)-Approximations to Sparsest Cut
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
This paper ties the line of work on algorithms that find an O(¿(log n))-approximation to the SPARSEST CUT together with the line of work on algorithms that run in subquadratic time by using only single-commodity flows. We present an algorithm that simultaneously achieves both goals, finding an O(¿(log (n)/¿))-approximation using O(n¿ logO(1) n) max-flows. The core of the algorithm is a stronger, algorithmic version of Arora et al.'s structure theorem, where we show that matching-chaining argument at the heart of their proof can be viewed as an algorithm that finds good augmenting paths in certain geometric multicommodity flow networks. By using that specialized algorithm in place of a black-box solver, we are able to solve those instances much more efficiently. We also show the cut-matching game framework can not achieve an approximation any better than ¿(log(n)/log log(n)) without re-routing flow.
[Algorithm design and analysis, Heart, approximation theory, Laplace equations, O(¿(log n))-approximation, Particle separators, game theory, Partitioning algorithms, multicommodity flow barrier breaking, Computer science, single-commodity flows, Upper bound, USA Councils, sparsest cut, Approximation algorithms, Concrete, sparse matrices, computational complexity]
A Complete Characterization of Statistical Query Learning with Applications to Evolvability
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Statistical query (SQ) learning model of Kearns is a natural restriction of the PAC learning model in which a learning algorithm is allowed to obtain estimates of statistical properties of the examples but cannot see the examples themselves [18]. We describe a new and simple characterization of the query complexity of learning in the SQ learning model. Unlike the previously known bounds on SQ learning [7], [9], [32], [3], [28] our characterization preserves the accuracy and the efficiency of learning. The preservation of accuracy implies that that our characterization gives the first characterization of the statistical query complexity in the agnostic learning framework of Haussler and Kearns, Schapire and Sellie [15], [20]. The preservation of efficiency allows us to derive a new technique for the design of evolutionary algorithms in Valiant's model of evolvability [31]. We use this technique to demonstrate the existence of a large class of monotone evolutionary learning algorithms based on square loss fitness estimation. These results differ significantly from the few known evolutionary algorithms and give evidence that evolvability in Valiant's model is a more versatile phenomenon than there had been previous reason to suspect.
[Algorithm design and analysis, statistical query complexity, Evolutionary computation, SQ learning model, Acoustical engineering, Agnostic Learning, USA Councils, Valiant model, Polynomials, Complexity of Learning, Noise robustness, PAC learning model, monotone evolutionary learning algorithm, learning (artificial intelligence), statistical query learning, Evolvability, statistical property, Statistical Query, Application software, agnostic learning, Computer science, evolutionary computation, evolvability, Impedance, statistical analysis, Information theory, computational complexity, square loss fitness estimation]
Agnostic Learning of Monomials by Halfspaces Is Hard
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We prove the following strong hardness result for learning: Given a distribution on labeled examples from the hypercube such that there exists a monomial (or conjunction) consistent with (1-¿)-fraction of the examples, it is NP-hard to find a halfspace that is correct on ( 1/2 + ¿)-fraction of the examples, for arbitrary constant ¿ &gt; 0. In learning theory terms, weak agnostic learning of monomials by halfspaces is NP-hard. This hardness result bridges between and subsumes two previous results which showed similar hardness results for the proper learning of monomials and halfspaces. As immediate corollaries of our result, we give the first optimal hardness results for weak agnostic learning of decision lists and majorities. Our techniques are quite different from previous hardness proofs for learning. We use an invariance principle and sparse approximation of halfspaces from recent work on fooling halfspaces to give a new natural list decoding of a halfspace in the context of dictatorship tests/label cover reductions. In addition, unlike previous invariance principle based proofs which are only known to give Unique Games hardness, we give a reduction from a smooth version of Label Cover that is known to be NP-hard.
[strong hardness result, Convergence, Learning systems, monomials agnostic learning, Agnostic Learning, NP hard, Hardness of Learning, Hypercubes, invariance, Dictatorship Tests, learning (artificial intelligence), Testing, hypercube, unique games hardness, invariance principle, Decoding, Computer science, Bridges, Support vector machines, label cover, halfspaces, PCPs, Support vector machine classification, sparse approximation, learning theory, computational complexity]
Learning and Smoothed Analysis
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We give a new model of learning motivated by smoothed analysis (Spielman and Teng, 2001). In this model, we analyze two new algorithms, for PAC-learning DNFs and agnostically learning decision trees, from random examples drawn from a constant-bounded product distributions. These two problems had previously been solved using membership queries (Jackson, 1995; Gopalan et al, 2005). Our analysis demonstrates that the "heavy" Fourier coefficients of a DNF suffice to recover the DNF. We also show that a structural property of the Fourier spectrum of any boolean function over "typical" product distributions. In a second model, we consider a simple new distribution over the boolean hypercube, one which is symmetric but is not the uniform distribution, from which we can learn O(log n)-depth decision trees in polynomial time.
[Algorithm design and analysis, Cats, PAC learning, smoothed analysis, boolean function, Optimized production technology, smoothing methods, Smoothed Analysis, constant bounded product distributions, Computer science, Boolean functions, Computational Learning Theory, Animals, agnostically learning decision trees, Machine learning, decision trees, Fourier spectrum, Hypercubes, Polynomials, boolean hypercube, Decision trees, learning (artificial intelligence)]
k-Means Has Polynomial Smoothed Complexity
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
The k-means method is one of the most widely used clustering algorithms, drawing its popularity from its speed in practice. Recently, however, it was shown to have exponential worst-case running time. In order to close the gap between practical performance and theoretical analysis, the k-means method has been studied in the model of smoothed analysis. But even the smoothed analyses so far are unsatisfactory as the bounds are still super-polynomial in the number n of data points. In this paper, we settle the smoothed running time of the k-means method. We show that the smoothed number of iterations is bounded by a polynomial in n and 1/¿, where sigma is the standard deviation of the Gaussian perturbations. This means that if an arbitrary input data set is randomly perturbed, then the k-means method will run in expected polynomial time on that input set.
[smoothed analysis, Data compression, Information retrieval, Mathematics, Biology, Application software, smoothed running time, Computer science, Upper bound, pattern clustering, polynomial smoothed complexity, Clustering algorithms, Gaussian processes, k-means, Polynomials, clustering, Performance analysis, k-means method, computational complexity, standard deviation, Gaussian perturbations]
Approximating Minimum Cost Connectivity Problems via Uncrossable Bifamilies and Spider-Cover Decompositions
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We give approximation algorithms for the Generalized Steiner Network (GSN) problem. The input consists of a graph G = (V, E) with edge/node costs, a node subset S ¿ V, and connectivity requirements {r(s, t) : s,t ¿ T ¿ V}. The goal is to find a minimum cost subgraph H that for all s, t ¿ T contains r(s, t) pairwise edge-disjoint si-paths so that no two of them have a node in S - {s, t} in common. Three extensively studied particular cases are: Edge-GSN (S = 0), Node-GSN (S = V), and Element-GSN (r(s,t) = 0 whenever s ¿ S or t ¿ S). Let k = max<sub>s,t¿T</sub> r(s, t). In Rooted GSN there is s ¿ T so that r(u, t) = 0 for all u¿s, and in the Subset k-Connected Subgraph problem r(s, t) = k for all s, t ¿ T. For edge costs, our ratios are: O(k2) for Rooted GSN and O(k2 log k) for Subset k-Connected Subgraph. This improves the previous ratio O(k2 log n) and settles the approximability of these problems to a constant for bounded k. For node-cost, our ratios are: (1) O(k log |T|) for Element-GSN, matching the best known ratio for Edge-GSN. (2) O(k2 log |T|) for Rooted GSN and O(k3 log |T|) for Subset k-Connected Subgraph, improving the ratio O(ks log2 |T|). (3) O(k4 log2 |T|) for GSN; this is the first non-trivial approximation algorithm for the problem.
[approximation theory, Costs, minimum cost connectivity problems, rooted GSN, graph theory, spider-cover decompositions, subset k-connected subgraph problem, approximation algorithms, Computer science, connectivity requirements, Approximation algorithms, generalized Steiner network problem, uncrossable bifamily, Generalized Steiner Network, computational complexity, edge-GSN]
Improved Approximation Algorithms for PRIZE-COLLECTING STEINER TREE and TSP
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We study the prize-collecting versions of the Steiner tree, traveling salesman, and stroll (a.k.a. Path-TSP) problems (PCST, PCTSP, and PCS, respectively): given a graph (V, E) with costs on each edge and a penalty (a.k.a. prize) on each node, the goal is to find a tree (for PCST), cycle (for PCTSP), or stroll (for PCS) that minimizes the sum of the edge costs in the tree/cycle/stroll and the penalties of the nodes not spanned by it. In addition to being a useful theoretical tool for helping to solve other optimization problems, PCST has been applied fruitfully by AT&amp;T to the optimization of real-world telecommunications networks. The most recent improvements for the first two problems, giving a 2-approximation algorithm for each, appeared first in 1992. (A 2-approximation for PCS appeared in 2003.) The natural linear programming (LP) relaxation of PCST has an integrality gap of 2, which has been a barrier to further improvements for this problem. We present (2 · ¿)-approximation algorithms for all three problems, connected by a unified technique for improving prize-collecting algorithms that allows us to circumvent the integrality gap barrier.
[Steiner trees, Costs, path-TSP, linear programming, stroll, telecommunications network, travelling salesman problems, (2 · ¿)-approximation algorithms, Tree graphs, relaxation theory, linear programming relaxation, approximation algorithm, approximation theory, traveling salesman problem, path-traveling salesman problem, Buildings, trees (mathematics), Traveling salesman problems, Linear programming, graph, Computer science, Waste materials, Steiner tree, prize-collecting steiner tree, 2-approximation algorithm, Approximation algorithms, Personal communication networks, stroll probelm, prize-collecting, optimization problems]
An O(k^3 log n)-Approximation Algorithm for Vertex-Connectivity Survivable Network Design
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
In the Survivable Network Design problem (SNDP), we are given an undirected graph G(V, E) with costs on edges, along with a connectivity requirement r(u, v) for each pair u, v of vertices. The goal is to find a minimum-cost subset E* of edges, that satisfies the given set of pairwise connectivity requirements. In the edge-connectivity version we need to ensure that there are r(u, v) edge-disjoint paths for every pair u, v of vertices, while in the vertex-connectivity version the paths are required to be vertex-disjoint. The edge-connectivity version of SNDP is known to have a 2-approximation. However, no non-trivial approximation algorithm has been known so far for the vertex version of SNDP, except for special cases of the problem. We present an extremely simple algorithm to achieve an O(k3 log |T|)-approximation for this problem, where k denotes the maximum connectivity requirement, and T is the set of vertices that participate in one or more pairs with non-zero connectivity requirements. We also give a simple proof of the recently discovered O(k3 log |T|)-approximation algorithm for the single-source version of vertex-connectivity SNDP. Our results establish a natural connection between vertex-connectivity and a well-understood generalization of edge-connectivity, namely, element-connectivity, in that, any instance of vertex-connectivity can be expressed by a small number of instances of the element-connectivity problem.
[Algorithm design and analysis, approximation theory, Costs, Engineering profession, vertex-connectivity, pairwise connectivity, Computer science, vertex-connectivity survivable network design, survivable network design, edge-connectivity, directed graphs, approximation algorithm, Approximation algorithms, Polynomials, undirected graph, computational complexity]
An Oblivious O(1)-Approximation for Single Source Buy-at-Bulk
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We consider the single-source (or single-sink) buy-at-bulk problem with an unknown concave cost function. We want to route a set of demands along a graph to or from a designated root node, and the cost of routing x units of flow along an edge is proportional to some concave, non-decreasing function f such that f(0) = 0. We present a polynomial time algorithm that finds a distribution over trees such that the expected cost of a tree for any f is within an O(1)-factor of the optimum cost for that f. The previous best simultaneous approximation for this problem, even ignoring computation time, was O(log |D|), where D is the multi-set of demand nodes. We design a simple algorithmic framework using the ellipsoid method that finds an O(1)-approximation if one exists, and then construct a separation oracle using a novel adaptation of the Guha, Meyerson, and Munagala algorithm for the single-sink buy-at-bulk problem that proves an O(1) approximation is possible for all f. The number of trees in the support of the distribution constructed by our algorithm is at most 1+log |D|.
[Algorithm design and analysis, approximation theory, single-source buy-at-bulk, trees (mathematics), separation oracle, Network Design, Routing, O(1)-approximation, Approximation Algorithms, polynomial time algorithm, trees, Ellipsoids, Computer science, single-sink buy-at-bulk, Tree graphs, concave cost function, ellipsoid method, USA Councils, Engineering management, Cost function, Approximation algorithms, Polynomials, algorithmic framework, computational complexity]
Optimal Long Code Test with One Free Bit
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
For arbitrarily small constants epsilon, delta ¿.¿ &gt; 0, we present a long code test with one free bit, completeness 1-epsilon and soundness delta. Using the test, we prove the following two inapproximability results:1. Assuming the Unique Games Conjecture of Khot, given an n-vertex graph that has two disjoint independent sets of size (1/2-¿)n each, it is NP-hard to find an independent set of size delta n.2. Assuming a (new) stronger version of the Unique Games Conjecture, the scheduling problem of minimizing weighted completion time with precedence constraints is inapproximable within factor 2-¿.
[Acoustic testing, NP-hard, graph theory, 1 Free bit, Precedence constrained scheduling, scheduling problem, NP-complete problem, optimal long code test, graph, Computer science, unique games conjecture, optimisation, Unique Games, one free bit, Polynomials, Vertex Cover, Time factors, Single machine scheduling]
Combinatorial PCPs with Efficient Verifiers
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
The PCP theorem asserts the existence of proofs that can be verified by a verifier that reads only a very small part of the proof. The theorem was originally proved by Arora and Safra (J. ACM 45(1)) and Arora et al. (J. ACM 45(3)) using sophisticated algebraic tools. More than a decade later, Dinur (J. ACM 54(3)) gave a simpler and arguably more intuitive proof using alternative combinatorial techniques. One disadvantage of Dinur's proof compared to the previous algebraic proof is that it yields less efficient verifiers. In this work, we provide a combinatorial construction of PCPs with verifiers that are as efficient as the ones obtained by the algebraic methods. The result is the first combinatorial proof of the PCP theorem for (originally proved by Babai et al., STOC 1991), and a combinatorial construction of super-fast PCPs of Proximity for (first constructed by Ben-Sasson et al., CCC 2005).
[combinatorial proof, super-fast, combinatorial mathematics, probabilistic checkable proof, probabilistic logic, Mathematics, Complexity theory, Galois fields, Computer science, Algebra, algebraic proof, PCPP, combinatorial PCP, Approximation algorithms, Polynomials, theorem proving, Dinur proof, PCP of Proximity, PCP, computational complexity]
Composition of Low-Error 2-Query PCPs Using Decodable PCPs
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
The main result of this paper is a generic composition theorem for low error two-query probabilistically checkable proofs (PCPs). Prior to this work, composition of PCPs was well-understood only in the constant error regime. Existing composition methods in the low error regime were non-modular (i.e., very much tailored to the specific PCPs that were being composed), resulting in complicated constructions of PCPs. Furthermore, until recently, composition in the low error regime suffered from incurring an extra 'consistency' query, resulting in PCPs that are not 'two-query' and hence, much less useful for hardness-of-approximation reductions. In a recent breakthrough, Moshkovitz and Raz [In Proc. 49th IEEE Symp. on Foundations of Comp. Science (FOCS), 2008] constructed almost linear-sized low-error 2-query PCPs for every language in NP. Indeed, the main technical component of their construction is a novel composition of certain specific PCPs. We give a modular and simpler proof of their result by repeatedly applying the new composition theorem to known PCP components. To facilitate the new modular composition, we introduce a new variant of PCP, which we call a "decodable PCP (dPCP)". A dPCP is an encoding of an NP witness that is both locally checkable and locally decodable. The dPCP verifier in addition to verifying the validity of the given proof like a standard PCP verifier, also locally decodes the original NP witness. Our composition is generic in the sense that it works regardless of the way the component PCPs are constructed.
[combinatorial mathematics, Mathematics, low error 2 query PCP, Reed-Solomon codes, composition, Polynomials, constant error regime, low error regime, low soundness error, probabilistically checkable proofs, probability, decodable PCP, Encoding, generic composition theorem, NP witness, Decoding, modular composition, randomised algorithms, Computer science, complicated constructions, extra consistency query, hardness-of-approximation reductions, locally decodable, Computer errors, PCP, computational complexity]
The Complexity of Rationalizing Network Formation
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We study the complexity of rationalizing network formation. In this problem we fix an underlying model describing how selfish parties (the vertices) produce a graph by making individual decisions to form or not form incident edges. The model is equipped with a notion of stability (or equilibrium), and we observe a set of "snapshots" of graphs that are assumed to be stable. From this we would like to infer some unobserved data about the system: edge prices, or how much each vertex values short paths to each other vertex. We study two rationalization problems arising from the network formation model of Jackson and Wolinsky [14]. When the goal is to infer edge prices, we observe that the rationalization problem is easy. The problem remains easy even when rationalizing prices do not exist and we instead wish to find prices that maximize the stability of the system. In contrast, when the edge prices are given and the goal is instead to infer valuations of each vertex by each other vertex, we prove that the rationalization problem becomes NP-hard. Our proof exposes a close connection between rationalization problems and the Inequality-SAT (I-SAT) problem. Finally and most significantly, we prove that an approximation version of this NP-complete rationalization problem is NP-hard to approximate to within better than a 1/2 ratio. This shows that the trivial algorithm of setting everyone's valuations to infinity (which rationalizes all the edges present in the input graphs) or to zero (which rationalizes all the non-edges present in the input graphs) is the best possible assuming P ? NP To do this we prove a tight (1/2 + ?) -approximation hardness for a variant of I-SAT in which all coefficients are non-negative. This in turn follows from a tight hardness result for MAX-LlN<sub>R</sub> <sub>+</sub> (linear equations over the reals, with non-negative coefficients), which we prove by a (non-trivial) modification of the recent result of Guruswami and Raghavendra [10] which achieved tight hardness for this problem without the non-negativity constraint. Our technical contributions regarding the hardness of I-SAT and MAX-LIN<sub>R</sub> <sub>+</sub> may be of independent interest, given the generality of these problems.
[approximation theory, Stability, decision theory, network formation games, H infinity control, computability, network theory (graphs), hardness of approximation, Linear programming, Inequality-SAT, NP-complete rationalization problem, Cost accounting, Equations, approximation hardness, Computer science, NP-hard problem, Inequality SAT problem, network formation rationalization complexity, Jackson-Wolinsky model, computational complexity]
Dynamic and Non-uniform Pricing Strategies for Revenue Maximization
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We study the ITEM PRICING problem for revenue maximization in the limited supply setting, where a single seller with n distinct items caters to m buyers with unknown subadditive valuation functions who arrive in a sequence. The seller sets the prices on individual items. Each buyer buys a subset of yet unsold items that maximizes her utility. Our goal is to design pricing strategies that guarantee an expected revenue that is within a small multiplicative factor of the optimal social welfare an upper bound on the maximum revenue that can be generated by any pricing mechanism. Most earlier work has focused on the unlimited supply setting, where selling an item to a buyer does not affect the availability of the item to the future buyers. Recently, Balcan et. al. studied the limited supply setting, giving a randomized pricing strategy that achieves a 2O(?(log n log log n)-approximation; their strategy assigns a single price to all items (uniform pricing), and never changes it (static pricing). They also showed that no pricing strategy that is both static and uniform can give better than 2??(log1/4 n)-approximation. Our first result is a strengthening of the lower bound on approximation achievable by static uniform pricing to 2??(log n). We then design dynamic uniform pricing strategies (all items are identically priced but item prices can change over time), that achieves O(log2 n)-approximation, and also show a lower bound of ? ((log n/ log log n)2) for this class of strategies. Our strategies are simple to implement, and in particular, one strategy is to smoothly decrease the price over time. We also design a static nonuniform pricing strategy (different items can have different prices but prices do not change over time), that give poly-logarithmic approximation in a more restricted setting with few buyers. Thus in the limited supply setting, our results highlight a strong separation between the power of dynamic and non-uniform pricing strategies versus static uniform pricing strategy. To our knowledge, this is the first non-trivial analysis of dynamic and non-uniform pricing schemes for revenue maximization in a setting with multiple distinct items.
[item pricing, poly-logarithmic approximation, item pricing problem, pricing strategies, Cost accounting, Computer science, Information science, Upper bound, USA Councils, Bayesian methods, Pricing, revenue maximization, pricing, limited supply setting]
On the Power of Randomization in Algorithmic Mechanism Design
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
In many settings the power of truthful mechanisms is severely bounded. In this paper we use randomization to overcome this problem. In particular, we construct an FPTAS for multi-unit auctions that is truthful in expectation, whereas there is evidence that no polynomial-time truthful deterministic mechanism provides an approximation ratio better than 2. We also show for the first time that truthful in expectation polynomial-time mechanisms are provably stronger than polynomial-time universally truthful mechanisms. Specifically, we show that there is a setting in which: (1) there is a non-polynomial time truthful mechanism that always outputs the optimal solution, and that (2) no universally truthful randomized mechanism can provide an approximation ratio better than 2 in polynomial time, but (3) an FPTAS that is truthful in expectation exists.
[Algorithm design and analysis, polynomial time truthful deterministic mechanism, multiunit auctions, game theory, Probability distribution, Complexity theory, Truthful Approximation Algorithms, randomised algorithms, Computer science, Design engineering, algorithmic mechanism design, polynomial approximation, randomization, Approximation algorithms, Polynomials, FPTAS, Mechanism Design, Power engineering and energy]
Universal Blind Quantum Computation
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We present a protocol which allows a client to have a server carry out a quantum computation for her such that the client's inputs, outputs and computation remain perfectly private, and where she does not require any quantum computational power or memory. The client only needs to be able to prepare single qubits randomly chosen from a finite set and send them to the server, who has the balance of the required quantum computational resources. Our protocol is interactive: after the initial preparation of quantum states, the client and server use two-way classical communication which enables the client to drive the computation, giving single-qubit measurement instructions to the server, depending on previous measurement outcomes. Our protocol works for inputs and outputs that are either classical or quantum. We give an authentication protocol that allows the client to detect an interfering server; our scheme can also be made fault-tolerant. We also generalize our result to the setting of a purely classical client who communicates classically with two non-communicating entangled servers, in order to perform a blind quantum computation. By incorporating the authentication protocol, we show that any problem in BQP has an entangled two-prover interactive proof with a purely classical verifier. Our protocol is the first universal scheme which detects a cheating server, as well as the first protocol which does not require any quantum computation whatsoever on the client's side. The novelty of our approach is in using the unique features of measurement-based quantum computing which allows us to clearly distinguish between the quantum and classical aspects of a quantum computation.
[purely classical client setting, Protocols, Quantum entanglement, cryptographic protocols, noncommunicating entangled servers, Computer science, quantum prover interactive proofs, Computer aided instruction, Fault tolerance, Privacy, Quantum computing, single-qubit measurement, Fault detection, two-prover interactive proof, Authentication, quantum cryptography, theorem proving, two-way classical communication, authentication protocol, cheating server detection, Informatics, blind quantum computation, measurement-based quantum computing]
Optimal Quantum Strong Coin Flipping
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Coin flipping is a fundamental cryptographic primitive that enables two distrustful and far apart parties to create a uniformly random bit. Quantum information allows for protocols in the information theoretic setting where no dishonest party can perfectly cheat. The previously best-known quantum protocol by Ambain is achieved a cheating probability of at most 3/4. On the other hand, Kitaev showed that no quantum protocol can have cheating probability less than 1/¿2. Closing this gap has been one of the important open questions in quantum cryptography. In this paper, we resolve this question by presenting a quantum strong coin flipping protocol with cheating probability arbitrarily close to 1/¿2. More precisely, we show how to use any weak coin flipping protocol with cheating probability 1/2 + ¿ in order to achieve a strong coin flipping protocol with cheating probability 1/¿2 + O(¿). The optimal quantum strong coin flipping protocol follows from our construction and the optimal quantum weak coin flipping protocol described by Mochon.
[cryptographic protocols, optimal quantum weak coin flipping protocol, probability, strong coin-flipping protocol, optimal quantum strong coin flipping, Ambain, quantum information, Cryptographic protocols, Computer science, information theoretic setting, Quantum computing, cryptographic primitive, quantum strong coin flipping protocol, Quantum mechanics, Information security, quantum cryptography, cheating probability, uniformly random bit, Cryptography, Contracts, quantum protocol]
Two-Message Quantum Interactive Proofs Are in PSPACE
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We prove that QIP(2), the class of problems having two-message quantum interactive proof systems, is a subset of PSPACE. This relationship is obtained by means of an efficient parallel algorithm, based on the matrix multiplicative weights update method, for approximately solving a certain class of semidefinite programs.
[approximation theory, parallel algorithms, Quantum entanglement, QIP(2), parallel algorithm, Quantum interactive proof systems, PSPACE, Parallel algorithms, Computational complexity, Power system modeling, quantum complexity, mathematical programming, matrix multiplicative weights update method, Computer science, two-message quantum interactive proofs, approximate solution, Quantum computing, Upper bound, quantum computing, semidefinite program, Approximation algorithms, Polynomials, theorem proving, computational complexity]
Span Programs and Quantum Query Complexity: The General Adversary Bound Is Nearly Tight for Every Boolean Function
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
The general adversary bound is a semi-definite program (SDP) that lower-bounds the quantum query complexity of a function. We turn this lower bound into an upper bound, by giving a quantum walk algorithm based on the dual SDP that has query complexity at most the general adversary bound, up to a logarithmic factor. In more detail, the proof has two steps, each based on "span programs," a certain linear-algebraic model of computation. First, we give an SDP that outputs for any boolean function a span program computing it that has optimal "witness size." The optimal witness size is shown to coincide with the general adversary lower bound. Second, we give a quantum algorithm for evaluating span programs with only a logarithmic query overhead on the witness size. The first result is motivated by a quantum algorithm for evaluating composed span programs. The algorithm is known to be optimal for evaluating a large class of formulas. The allowed gates include all constant-size functions for which there is an optimal span program. So far, good span programs have been found in an ad hoc manner, and the SDP automates this procedure. Surprisingly, the SDP's value equals the general adversary bound. A corollary is an optimal quantum algorithm for evaluating "balanced" formulas over any finite boolean gate set. The second result extends span programs' applicability beyond the formula-evaluation problem. We extend the analysis of the quantum algorithm for evaluating span programs. The previous analysis shows that a corresponding bipartite graph has a large spectral gap, but only works when applied to the composition of constant-size span programs. We show generally that properties of eigenvalue-zero eigenvectors in fact imply an "effective" spectral gap around zero. A strong universality result for span programs follows. A good quantum query algorithm for a problem implies a good span program, and vice versa. Although nearly tight, this equivalence is nontrivial. Span programs are a promising model for developing more quantum algorithms.
[Algorithm design and analysis, semi-definite program, adversary bound, logarithmic query overhead, graph theory, quantum walk, constant-size functions, dual SDP, eigenvalues and eigenfunctions, finite Boolean gate set, optimal witness size, Boolean functions, Quantum computing, Bipartite graph, quantum walk algorithm, Computational modeling, quantum query complexity, logarithmic factor, span program, formula evaluation, Boolean function, bipartite graph, Computer science, Upper bound, quantum computing, semidefinite program, spectral gap, linear-algebraic model, computational complexity, eigenvalue-zero eigenvector, quantum query algorithm]
A $(\\log n)^{\\Omega(1)}$ Integrality Gap for the Sparsest Cut SDP
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We show that the Goemans-Linial semidefinite relaxation of the Sparsest Cut problem with general demands has integrality gap (log n)&#x003A9;(1). This is achieved by exhibiting n-point metric spaces of negative type whose L<sub>1</sub> distortion is (log n)&#x003A9;(1). Our result is based on quantitative bounds on the rate of degeneration of Lipschitz maps from the Heisenberg group to L<sub>1</sub> when restricted to cosets of the center.
[Algorithm design and analysis, Lipschitz map, Heisenberg group, graph theory, integrality gap, Extraterrestrial measurements, Linear programming, mathematical programming, sparsest cut problem, Computer science, semidefinite programming, Upper bound, NP-hard problem, USA Councils, Approximation algorithms, Goemans-Linial semidefinite relaxation, Sparsest Cut problem, Polynomials, Hilbert space, SDP, computational complexity, metric embeddings]
SDP Integrality Gaps with Local ell_1-Embeddability
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We construct integrality gap instances for SDP relaxation of the MAXIMUM CUT and the SPARSEST CUT problems. If the triangle inequality constraints are added to the SDP, then the SDP vectors naturally define an n-point negative type metric where n is the number of vertices in the problem instance. Our gap-instances satisfy a stronger constraint that every sub-metric on t = O((log log log n)1/6) points is isometrically embeddable into l<sub>1</sub>. The local l<sub>1</sub>-embeddability constraints are implied when the basic SDP relaxation is augmented with t rounds of the Sherali-Adams LP-relaxation. For the MAXIMUM CUT problem, we obtain an optimal gap of &#x003B1;<sub>GW</sub> -1 - &#x003F5;, where &#x003B1;<sub>GW</sub> is the Goemans-Williamson constant [11] and &#x003F5; &#x0226B; 0 is an arbitrarily small constant. For the SPARSEST CUT problem, we obtain a gap of &#x003A9;((log log log n)1/13). The latter result can be rephrased as a construction of an npoint negative type metric such that every t-point sub-metric is isometrically l<sub>1</sub>-embeddable, but embedding the whole metric into l<sub>1</sub> incurs distortion &#x003A9;((log log log n)1/13).
[approximation theory, Engineering profession, maximum cut problems, SDP integrality gaps, Goemans-Williamson constant, Educational institutions, integrality, Programming profession, Computer science, triangle inequality constraints, USA Councils, metric, Approximation algorithms, Sherali-Adams LP-relaxation, Polynomials, embedding, embeddability constraints, sparsest cut problems, SDP, computational complexity]
Integrality Gaps for Strong SDP Relaxations of UNIQUE GAMES
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
With the work of Khot and Vishnoi as a starting point, we obtain integrality gaps for certain strong SDP relaxations of Unique Games. Specifically, we exhibit a Unique Games gap instance for the basic semidefinite program strengthened by all valid linear inequalities on the inner products of up to exp(¿(log log n)1/4) vectors. For a stronger relaxation obtained from the basic semidefinite program by R rounds of Sherali-Adams liftand-project, we prove a Unique Games integrality gap for R = ¿(log log n)1/4. By composing these SDP gaps with UGC-hardness reductions, the above results imply corresponding integrality gaps for every problem for which a UGC-based hardness is known. Consequently, this work implies that including any valid constraints on up to exp(¿(log log n)1/4) vectors to natural semidefinite program, does not improve the approximation ratio for any problem in the following classes: constraint satisfaction problems, ordering constraint satisfaction problems and metric labeling problems over constant-size metrics. We obtain similar SDP integrality gaps for Balanced Separator, building on. We also exhibit, for explicit constants ¿, ¿ &gt; 0, an n-point negative-type metric which requires distortion ¿(log log n)¿ to embed into ¿<sub>1</sub>, although all its subsets of size exp(¿(log log n)¿) embed isometrically into ¿<sub>1</sub>.
[UGC-hardness reduction, User-generated content, graph theory, metric labeling problem, integrality gap, approximation algorithms, semidefinite programming, unique games conjecture, SDP relaxation, ordering constraint satisfaction problem, Labeling, unique games gap instance, Particle separators, game theory, hardness of approximation, Vectors, mathematical programming, Computer science, Sherali--Adams hierarchy, Sherali-Adams hierarchy, integrality gap construction, SDP hierarchies, Approximation algorithms, constant-size metrics, computational complexity]
How to Round Any CSP
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
A large number of interesting combinatorial optimization problems like MAX CUT, MAX k-SAT, and UNIQUE GAMES fall under the class of constraint satisfaction problems (CSPs). Recent work by one of the authors (STOC 2008) identifies a semidefinite programming (SDP) relaxation that yields the optimal approximation ratio for every CSP, under the Unique Games Conjecture (UGC). Very recently (FOCS 2009), the authors also showed unconditionally that the integrality gap of this basic SDP relaxation cannot be reduced by adding large classes of valid inequalities (e.g., in the fashion of Sherali-Adams LP hierarchies). In this work, we present an efficient rounding scheme that achieves the integrality gap of this basic SDP relaxation for every CSP (and it also achieves the gap of much stronger SDP relaxations). The SDP relaxation we consider is stronger or equivalent to any relaxation used in literature to approximate CSPs. Thus, irrespective of the truth of the UGC, our work yields an efficient generic algorithm that for every CSP, achieves an approximation at least as good as the best known algorithm in literature. The rounding algorithm in this paper can be summarized succinctly as follows: Reduce the dimension of SDP solution by random projection, discretize the projected vectors, and solve the resulting CSP instance by brute force! Even the proof is simple in that it avoids the use of the machinery from unique games reductions such as dictatorship tests, Fourier analysis or the invariance principle. A common theme of this paper and the subsequent paper in the same conference is a robustness lemma for SDP relaxations which asserts that approximately feasible solutions can be made feasible by "smoothing'' without changing the objective value significantly.
[unique games reduction, combinatorial mathematics, rounding scheme, User-generated content, sensitivity analysis, combinatorial optimization problem, integrality gap, optimal approximation ratio, Machinery, dimension reduction, Constraint optimization, semidefinite programming, unique games conjecture, generic algorithm, relaxation theory, dictatorship tests, approximation algorithm, Robustness, max k-SAT, Testing, approximation theory, constraint theory, game theory, invariance principle, projected vectors, Linear programming, Fourier analysis, Vectors, mathematical programming, semidefinite programming relaxation, Computer science, brute force, vectors, constraint satisfaction problems, Approximation algorithms, Iterative algorithms, max cut, random projection]
Constraint Satisfaction Problems of Bounded Width
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We provide a full characterization of applicability of The Local Consistency Checking algorithm to solving the non-uniform Constraint Satisfaction Problems. This settles the conjecture of Larose and Zadori.
[Electrooculography, local consistency checking algorithm, combinatorial mathematics, constraint theory, constraint satisfaction problem, Computational complexity, Equations, Computer science, Algebra, Constraint theory, Polynomials, nonuniform constraint satisfaction problem, Artificial intelligence, bounded width, local consistency, computational complexity]
Bit Encryption Is Complete
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Under CPA and CCA1 attacks, a secure bit encryption scheme can be applied bit-by-bit to construct a secure many-bit encryption scheme. The same construction fails, however, under a CCA2 attack. In fact, since the notion of CCA2 security was introduced by Rackoff and Simon [21], it has been an open question to determine whether single bit CCA2 secure encryption implies the existence of many-bit CCA2 security. We positively resolve this long-standing question and establish that bit encryption is complete for CPA, CCA1, and CCA2 notions. Our construction is black-box, and thus requires novel techniques to avoid known impossibility results concerning trapdoor predicates [10]. To the best of our knowledge, our work is also the first example of a non-shielding reduction (introduced in [9]) in the standard (i.e., not random-oracle) model.
[secure bit encryption scheme, many-bit encryption scheme, many-bit CCA2 security, Chosen-ciphertext secure public key encryption, bit encryption, Inspection, Decoding, CCA1 attack, Security, CCA2 secure encryption, Computer science, public key cryptography, Public key, Public key cryptography, CPA attack, Informatics]
2-Source Extractors under Computational Assumptions and Cryptography with Defective Randomness
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We show how to efficiently extract truly random bits from two independent sources of linear min-entropy, under a computational assumption. The assumption we rely on is the existence of an efficiently computable permutation f1, such that for any source X ¿ {0, 1}n with linear min-entropy, any circuit of size poly(n) cannot invert f(X) with non-negligible probability. Under the stronger assumption that f(X) cannot be inverted even by circuits of size poly(n log n) with nonnegligible probability, we design a lossless computational network extractor protocol. Namely, we design a protocol for a set of players, each with access to an independent source of linear min-entropy, with the guarantee that at the end of the protocol, each honest player is left with bits that are computationally indistinguishable from being uniform and private. Our protocol succeeds as long as there are at least two honest players. Our results imply that if such one-way permutations exist, and enhanced trapdoor permutations exist, then secure multiparty computation with imperfect randomness is possible for any number of players, as long as at least two of them are honest. We also construct a network extractor protocol for the case where each source has only polynomially-small min-entropy (n¿ for some constant ¿ &gt; 0). For this we need at least a constant u(¿) (which depends on ¿) number of honest players, and we need that the one-way permutation is hard to invert even on polynomially small min-entropy sources.
[Algorithm design and analysis, lossless computational network extractor protocol, cryptographic protocols, Circuits, extractor, Entropy, Distributed computing, network, linear min-entropy, entropy, computable permutation, Polynomials, Computer networks, Cryptography, probability, Access protocols, imperfect randomness, cryptography, Cryptographic protocols, Computer science, 2-source extractors, defective randomness, nonnegligible probability, weak random source, computational assumptions]
(Meta) Kernelization
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Polynomial time preprocessing to reduce instance size is one of the most commonly deployed heuristics to tackle computationally hard problems. In a parameterized problem, every instance I comes with a positive integer k. The problem is said to admit a polynomial kernel if, in polynomial time, we can reduce the size of the instance I to a polynomial in k, while preserving the answer. In this paper, we show that all problems expressible in Counting Monadic Second Order Logic and satisfying a compactness property admit a polynomial kernel on graphs of bounded genus. Our second result is that all problems that have finite integer index and satisfy a weaker compactness condition admit a linear kernel on graphs of bounded genus. The study of kernels on planar graphs was initiated by a seminal paper of Alber, Fellows, and Niedermeier [J. ACM, 2004 ] who showed that Planar Dominating Set admits a linear kernel. Following this result, a multitude of problems have been shown to admit linear kernels on planar graphs by combining the ideas of Alber et al. with problem specific reduction rules. Our theorems unify and extend all previously known kernelization results for planar graph problems. Combining our theorems with the Erdos-Posa property we obtain various new results on linear kernels for a number of packing and covering problems.
[polynomial time preprocessing, graph theory, planar graphs, Mathematics, History, formal logic, polynomial kernel, bounded genus, Graphs of Bounded Genus, Mathematical analysis, Polynonial Time Preprocessing, Polynomials, Planar Graphs, Logic, Kernel, Informatics, operating system kernels, Finite State, kernelization, Kernelization, Computer science, counting monadic second order logic, NP-hard problem, Councils, Parameterized Algorithms, Finite Integer Index, Counting Monadic Second Order Logic, computational complexity]
Planarity Allowing Few Error Vertices in Linear Time
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We show that for every fixed k, there is a linear time algorithm that decides whether or not a given graph has a vertex set X of order at most k such that G-X is planar (we call this class of graphs k-apex), and if this is the case, computes a drawing of the graph in the plane after deleting at most k vertices. In fact, in this case, we shall determine the minimum value l ? k such that after deleting some l vertices, the resulting graph is planar. If this is not the case, then the algorithm gives rise to a minor which is not k-apex and is minimal with this property. This answers the question posed by Cabello and Mohar in 2005, and by Kawarabayashi and Reed (STOC'07), respectively. Note that the case k = 0 is the planarity case. Thus our algorithm can be viewed as a generalization of the seminal result by Hopcroft and Tarjan (J. ACM 1974), which determines if a given graph is planar in linear time. Our algorithm can be also compared to the algorithms by Mohar (STOC'96 and Siam J. Discrete Math 2001) for testing the embeddability of an input graph in a fixed surface in linear time, by Kawarabayashi and Mohar (STOC'08) for testing polyhedral embeddability of an input graph in a fixed surface in linear time, and by Kawarabayashi and Reed (STOC'07) for testing the fixed crossing number in linear time. Note that deciding the genus of k-apex graphs is NP-complete, even for k = 1, as shown by Mohar. Thus k-apex graphs are very different from bounded genus graphs in a sense. In addition, for any fixed c, k, we apply our algorithm to obtain a linear time approximation scheme for weighted TSP, and for minimum weighted c-edge-connected submultigraph, respectively, for k-apex graphs. (In this case, an embedding of a k-apex graph is not given in the input). The first result generalizes the recent planar result by Klein (FOCS'05), while the second result generalizes Czumaj et al. (SODA'04). We also extend several optimization results for planar graphs by Baker (J. ACM. 1994) and others to k-apex graphs.
[graph theory, bounded genus graphs, planar graphs, set theory, error vertices, polyhedral embeddability, Few errors, travelling salesman problems, vertex set, graphs, linear time algorithm, Tree graphs, k-apex graphs, Feedback, linear time approximation, Polynomials, Informatics, Testing, approximation theory, Planarity, Approximation Algorithms, NP-complete problem, Computer science, TSP, generalization, Linear approximation, Computer errors, Approximation algorithms, planarity, computational complexity, linear time]
Symmetry and Approximability of Submodular Maximization Problems
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
A number of recent results on optimization problems involving submodular functions have made use of the "multilinear relaxation" of the problem. We present a general approach to deriving inapproximability results in the value oracle model, based on the notion of "symmetry gap". Our main result is that for any fixed instance that exhibits a certain "symmetry gap" in its multilinear relaxation, there is a naturally related class of instances for which a better approximation factor than the symmetry gap would require exponentially many oracle queries. This unifies several known hardness results for submodular maximization, e.g. the optimality of (1 - 1/e)-approximation for monotone submodular maximization under a cardinality constraint and the impossibility of (1/2 + ?)-approximation for unconstrained (non-monotone) submodular maximization. It follows from our result that (1/2 + ?)-approximation is also impossible for non-monotone submodular maximization subject to a (non-trivial) matroid constraint. On the algorithmic side, we present a 0.309approximation for this problem, improving the previously known factor of 1/4 - o(1). As another application, we consider the problem of maximizing a non-monotone submodular function over the bases of a matroid. A (1/6 - o(1))-approximation has been developed for this problem, assuming that the matroid contains two disjoint bases. We show that the best approximation one can achieve is indeed related to packings of bases in the matroid. Specifically, for any k ? 2, there is a class of matroids of fractional base packing number v = k/k-1, such that any algorithm achieving a better than (1 - 1/v)-approximation for this class would require exponentially many value queries. On the positive side, we present a 1/2(1 - 1/v - o(1))approximation algorithm for the same problem. Our hardness results hold in fact for very special symmetric instances. For such symmetric instances, we show that the approximation factors of 1/2 (for submodular maximization subject to a matroid constraint) and 1 - 1/v (for a matroid base constraint) can be achieved algorithmically and hence are optimal.
[Greedy algorithms, Merging, multilinear extension, probability, submodular maximization problems, symmetry gap, approximation algorithms, value oracle model, nonmonotone submodular function, Cost accounting, multilinear relaxation, cardinality constraint, Computer science, approximation factor, fixed instance, optimisation, submodular functions. matroids, oracle queries, matroid constraint, Approximation algorithms, Polynomials, symmetric instances, fractional base packing number, optimization problems]
Submodular Function Minimization under Covering Constraints
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
This paper addresses the problems of minimizing nonnegative submodular functions under covering constraints, which generalize the vertex cover, edge cover, and set cover problems. We give approximation algorithms for these problems exploiting the discrete convexity of submodular functions. We first present a rounding 2-approximation algorithm for the submodular vertex cover problem based on the half-integrality of the continuous relaxation problem, and show that the rounding algorithm can be performed by one application of submodular function minimization on a ring family. We also show that a rounding algorithm and a primal-dual algorithm for the submodular cost set cover problem are both constant factor approximation algorithms if the maximum frequency is fixed. In addition, we give an essentially tight lower bound on the approximability of the submodular edge cover problem.
[approximation theory, vertex cover, primal-dual algorithm, submodular function, set theory, rounding algorithm, Computer science, set cover, discrete convexity, edge cover, NP-hard problem, submodular function minimization, rounding 2-approximation algorithm, approximation algorithm, minimisation, constant factor approximation algorithms, computational complexity]
Smoothed Analysis of Multiobjective Optimization
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We prove that the number of Pareto-optimal solutions in any multiobjective binary optimization problem with a finite number of linear objective functions is polynomial in the model of smoothed analysis. This resolves a conjecture of Rene Beier. Moreover, we give polynomial bounds on all finite moments of the number of Pareto-optimal solutions, which yields the first non-trivial concentration bound for this quantity. Using our new technique, we give a complete characterization of polynomial smoothed complexity for binary optimization problems, which strengthens an earlier result due to Beier and Vo¿cking.
[Algorithm design and analysis, Pareto optimisation, linear objective functions, smoothed analysis, smoothing methods, Pareto-optimal solutions, Computer science, Constraint optimization, multiobjective optimization, Pathology, Filters, Tree graphs, Search methods, polynomial smoothed complexity, polynomial approximation, Clustering algorithms, operations research, multiobjective binary optimization problem, polynomial functions, Polynomials, Pareto analysis]
Fully Dynamic (2 + &#x003B5;) Approximate All-Pairs Shortest Paths with Fast Query and Close to Linear Update Time
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
For any fixed 1 &gt; ¿ &gt; 0 we present a fully dynamic algorithm for maintaining (2 + ¿)-approximate all-pairs shortest paths in undirected graphs with positive edge weights. We use a randomized (Las Vegas) update algorithm (but a deterministic query procedure), so the time given is the expected amortized update time. Our query time O(log log log n). The update time is O¿(mnO(1/¿(log n)) log (nR)), where R is the ratio between the heaviest and the lightest edge weight in the graph (so R = 1 in unweighted graphs). Unfortunately, the update time does have the drawback of a super-polynomial dependence on e. it grows as (3/¿)(¿(log n/log(3/¿))) = n(¿(log(3/¿)/log n)). Our algorithm has a significantly faster update time than any other algorithm with sub-polynomial query time. For exact distances, the state of the art algorithm has an update time of O¿(n2). For approximate distances, the best previous algorithm has a O(kmn1/k) update time and returns (2 k - 1) stretch paths. Thus, it needs an update time of O(m¿(n)) to get close to our approximation, and it has to return O(¿(log n)) approximate distances to match our update time.
[approximation theory, all-pairs shortest path approximation, positive edge weights, Heuristic algorithms, graph theory, deterministic query procedure, query time, approximation algorithms, linear update time, super-polynomial dependence, Computer science, shortest paths, dynamic algorithms, Approximation algorithms, randomize dupdate algorithm, undirected graphs, update time, graph algorithms]
Distance Oracles for Sparse Graphs
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Thorup and Zwick, in their seminal work, introduced the approximate distance oracle, which is a data structure that answers distance queries in a graph. For any integer k, they showed an efficient algorithm to construct an approximate distance oracle using space O(kn1+1/k) that can answer queries in time O(k) with a distance estimate that is at most ¿ = 2k-1 times larger than the actual shortest distance (this ratio is called the stretch).They proved that, under a combinatorial conjecture, their data structure is optimal in terms of space: if a stretch of at most 2k-1 is desired, then the space complexity is at least n1+1/k. Their proof holds even if infinite query time is allowed: it is essentially an "incompressibility" result. Also, the proof only holds for dense graphs, and the best bound it can prove only implies that the size of the data structure is lower bounded by the number of edges of the graph. Naturally, the following question arises: what happens for sparse graphs? In this paper we give a new lower bound for approximate distance oracles in the cell-probe model. This lower bound holds even for sparse (polylog(n)-degree) graphs, and it is not an "incompressibility" bound: we prove a three-way tradeoff between space, stretch, and query time. We show that when the query time is t and the stretch is ¿, then the space S must be S ¿ n1+¿(1/t¿)/lg n. This lower bound follows by a reduction from lopsided set disjointness to distance oracles, based on and motivated by recent work of Patrascu. Our results in fact show that for any high-girth regular graph, an approximate distance oracle that supports efficient queries for all subgraphs of G must obey this tradeoff. We also prove some lemmas that count sets of paths in high-girth regular graphs and high-girth regular expanders, which might be of independent interest.
[lopsided set disjointness, high-girth regular expander, graph theory, Data structures, Graph theory, approximate distance oracle, lower bounds, sparse graph, Computer science, high-girth regular graph, Tree graphs, distance oracle, data structures, space complexity, computational complexity, cell-probe model]
Space-Efficient Framework for Top-k String Retrieval Problems
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Given a set D={d<sub>1</sub>, d<sub>2</sub>,..., d<sub>D</sub>} of D strings of total length n, our task is to report the "most relevant"strings for a given query pattern P. This involves somewhat more advanced query functionality than the usual pattern matching, as some notion of "most relevant" is involved. In information retrieval literature, this task is best achieved by using inverted indexes. However, inverted indexes work only for some predefined set of patterns. In the pattern matching community, the most popular pattern-matching data structures are suffix trees and suffix arrays. However, a typical suffix tree search involves going through all the occurrences of the pattern over the entire string collection, which might be a lot more than the required relevant documents. The first formal framework to study such kind of retrieval problems was given by Muthukrishnan. He considered two metrics for relevance: frequency and proximity. He took a threshold-based approach on these metrics and gave data structures taking O(n log n) words of space. We study this problem in a slightly different framework of reporting the top k most relevant documents (in sorted order) under similar and more general relevance metrics. Our framework gives linear space data structure with optimal query times for arbitrary score functions. As a corollary, it improves the space utilization for the problems in while maintaining optimal query performance. We also develop compressed variants of these data structures for several specific relevance metrics.
[pattern matching, top-$k$ queries, pattern-matching data structures, space-efficient framework, proximity metric, arbitrary score function, threshold-based approach, query processing, text indexing, Databases, USA Councils, document retrieval, data structures, Tree data structures, suffix tree search, frequency metric, indexing, trees (mathematics), inverted indexes, information retrieval, linear space data structure, Data structures, Information retrieval, Extraterrestrial measurements, Computer science, succinct data structures, top-k string retrieval problem, relevance feedback, Frequency, query functionality, relevance metrics, Pattern matching, Indexing, suffix arrays]
KKL, Kruskal-Katona, and Monotone Nets
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We generalize the Kahn-Kalai-Linial (KKL) Theorem to random walks on Cayley and Schreier graphs, making progress on an open problem of Hoory, Linial, and Wigderson. In our generalization, the underlying group need not be abelian so long as the generating set is a union of conjugacy classes. An example corollary is that for every f : (<sub>k</sub> [n]) ¿ {0,1} with E[f] and k/n bounded away from 0 and 1, there is a pair 1 ¿ i &lt; j ¿ n such that Z<sub>ij</sub>(f) ¿ ¿(log n/n). Here l<sub>ij</sub>(f) denotes the "influence" on / of swapping the ith and jth coordinates. Using this corollary we obtain a "robust" version of the Kruskal-Katona Theorem: Given a constant-density subset A of a middle slice of the Hamming n-cube, the density of ¿A is greater by at least ¿(log n/n), unless A is noticeably correlated with a single coordinate. As an application of these results, we show that the set of functions {0,1, x<sub>1</sub>,..., x<sub>¿</sub>, Maj} is a (1/2-¿)-net for the set of all n-bit monotone boolean functions, where ¿ = ¿(log n//¿(n)). This distance is optimal for polynomial-size nets and gives an optimal weak-learning algorithm for monotone functions under the uniform distribution, solving a problem of Blum, Burch and Langford.
[Kruskal-Katona, random walks, Hamming n-cube, graph theory, Schreier graphs, learning, Complexity theory, Distributed computing, KKL, monotone nets, boolean functions, Computer science, monotone functions, Boolean functions, USA Councils, random functions, Robustness, Polynomials, Random variables, Cayley graphs, Kahn-Kalai-Linial theorem, Kahn-Kalai-Linial]
Higher Eigenvalues of Graphs
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We present a general method for proving upper bounds on the eigenvalues of the graph Laplacian. In particular, we show that for any positive integer k, the kth smallest eigenvalue of the Laplacian on a bounded-degree planar graph is O(k/n). This bound is asymptotically tight for every k, as it is easily seen to be achieved for planar grids. We also extend this spectral result to graphs with bounded genus, graphs which forbid fixed minors, and other natural families. Previously, such spectral upper bounds were only known for k = 2, i.e. for the Fiedler value of these graphs. In addition, our result yields a new, combinatorial proof of the celebrated result of Korevaar in differential geometry.
[Laplace equations, Transmission line matrix methods, eigenvalues, graph theory, Optimization methods, bounded degree planar graph, Very large scale integration, Korevaar, Partitioning algorithms, eigenvalues and eigenfunctions, Computer science, Geometry, Image segmentation, Upper bound, Fiedler value, differential geometry, Eigenvalues and eigenfunctions, graph Laplacian]
Regularity Lemmas and Combinatorial Algorithms
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We present new combinatorial algorithms for Boolean matrix multiplication (BMM) and preprocessing a graph to answer independent set queries. We give the first asymptotic improvements on combinatorial algorithms for dense BMM in many years, improving on the "Four Russians'' O(n3/(w log n)) bound for machine models with word size w. (For a pointer machine, we can set w = log n.) The algorithms utilize notions from Regularity Lemmas for graphs in a novel way. 1) We give two randomized combinatorial algorithms for BMM. The first algorithm is essentially a reduction from BMM to the Triangle Removal Lemma}. The best known bounds for the Triangle Removal Lemma only imply an O((n3 log ?)/(?w log n)\\right) time algorithm for BMM where ? = (log*n)? for some ? &gt; 0, but improvements on the Triangle Removal Lemma would yield corresponding runtime improvements. The second algorithm applies the Weak Regularity Lemma of Frieze and Kannan along with several information compression ideas, running in O(n3 (log log n)2/(log n)9/4) time with probability exponentially close to 1. When w ? log n, it can be implemented in O(n3 (log log n)2/(w log n)7/6)) time. Our results immediately imply improved combinatorial methods for CFG parsing, detecting triangle-freeness, and transitive closure. 2)Using Weak Regularity, we also give an algorithm for answering queries of the form is S ? V an independent set? in a graph. Improving on prior work, we show how to randomly preprocess a graph in O(n2+?}) time (for all ? &gt; 0) so that with high probability, all subsequent batches of log n independent set queries can be answered deterministically in O(n2 (log log n)2/((log n)5/4)) time. When w ? log n, w queries can be answered in O(n2 (log log n)2/((log n)7/6))\\right) time. In addition to its nice applications, this problem is interesting in that it is not known how to do better than O(n2) using "algebraic'' methods.
[set queries, combinatorial algorithms, machine models, Moon, graph theory, Switches, Weak Regularity, Independent Set Query, Table lookup, set theory, weak regularity lemma, Runtime, triangle-freeness detection, Parallel processing, transitive closure, boolean matrix multiplication, Combinatorial Algorithms, O time algorithm, Application software, Computer science, Algorithms, four Russians bound, Boolean Matrix Multiplication, computational complexity, triangle removal lemma]
Approximability of Combinatorial Problems with Multi-agent Submodular Cost Functions
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
Applications in complex systems such as the Internet have spawned recent interest in studying situations involving multiple agents with their individual cost or utility functions. In this paper, we introduce an algorithmic framework for studying combinatorial problems in the presence of multiple agents with submodular cost functions. We study several fundamental covering problems (Vertex Cover, Shortest Path, Perfect Matching, and Spanning Tree) in this setting and establish tight upper and lower bounds for the approximability of these problems.
[Multiagent systems, multi-agent systems, combinatorial mathematics, multiagent submodular cost functions, multiple agents, Educational institutions, Computer science, combinatorial problems, Tree graphs, USA Councils, Web and internet services, Economies of scale, Cost function, Approximation algorithms, Internet, algorithmic framework, IP networks, approximability]
The Data Stream Space Complexity of Cascaded Norms
2009 50th Annual IEEE Symposium on Foundations of Computer Science
None
2009
We consider the problem of estimating cascaded aggregates over a matrix presented as a sequence of updates in a data stream. A cascaded aggregate P · Q is defined by evaluating aggregate Q repeatedly over each row of the matrix, and then evaluating aggregate P over the resulting vector of values. This problem was introduced by Cormode and Muthukrishnan, PODS, 2005 [CM]. We analyze the space complexity of estimating cascaded norms on an n × d matrix to within a small relative error. Let L<sub>p</sub> denote the p-th norm, where p is a non-negative integer. We abbreviate the cascaded norm L<sub>k</sub> · L<sub>p</sub> by L<sub>k,p</sub>. (1) For any constant k ¿ p ¿ 2, we obtain a 1-pass O¿(n1-2/kd1-2/p)-space algorithm for estimating L<sub>k,p</sub>. This is optimal up to polylogarithmic factors and resolves an open question of [CM] regarding the space complexity of L<sub>4,2</sub>. We also obtain 1-pass space-optimal algorithms for estimating L<sub>¿,k</sub> and L<sub>k,¿</sub>. (2) We prove a space lower bound of ¿(n1-1/k) on estimating L<sub>k,0</sub> and L<sub>k,1</sub>, resolving an open question due to Indyk, IITK Data Streams Workshop (Problem 8), 2006. We also resolve two more questions of [CM] concerning L<sub>k,2</sub> estimation and block heavy hitter problems. Ganguly, Bansal and Dube (FAW, 2008) claimed an O(1)-space algorithm for estimating L<sub>k,p</sub> for any k,p ¿ [0,2]. Our lower bounds show this claim is incorrect.
[cascaded aggregate, polylogarithmic factor, Area measurement, Medical services, Explosions, Data mining, Statistics, Information technology, Computer science, Aggregates, nonnegative integer, Frequency, Internet, cascaded norms, data stream space complexity, computational complexity, 1-pass space-optimal algorithm]
Foreword
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Provides a listing of current committee members.
[]
Program Committee
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Provides a listing of current committee members.
[]
Constructive Algorithms for Discrepancy Minimization
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Given a set system (V, S), V = {1,..., n} and S = {S<sub>1</sub>,...,S<sub>m</sub>}, the minimum discrepancy problem is to find a 2-coloring X : V &#x2192; {-1,+1}, such that each set is colored as evenly as possible, i.e. find X to minimize max<sub>j&#x2208;|m]</sub> &#x03A3;<sub>i&#x2208;sj</sub> X(i)|&#x00B7; In this paper we give the first polynomial time algorithms for discrepancy minimization that achieve bounds similar to those known existentially using the so-called Entropy Method. We also give a first approximation-like result for discrepancy. Specifically we give efficient randomized algorithms to: 1) Construct an O(n1/2) discrepancy coloring for general sets systems when m = O(n), matching the celebrated result of Spencer [17] up to O(1) factors. More generally, for m &#x2265; n, we obtain a discrepancy of O(n1/2 log(2m/n)). 2) Construct a coloring with discrepancy O(t1/2 log n), if each element lies in at most t sets. This matches the (nonconstructive) result of Srinivasan [19]. 3) Construct a coloring with discrepancy O(&#x03BB;log(ram)), where &#x03BB; is the hereditary discrepancy of the set system. The main idea in our algorithms is to produce a coloring over time by letting the color of the elements perform a random walk (with tiny increments) starting from 0 until they reach &#x00B1;1. At each step the random hops for various elements are correlated by a solution to a semidefinite program, where this program is determined by the current state and the entropy method.
[discrepancy coloring, set system, Constructive Algorithms, Color, Discrepancy Theory, Probabilistic logic, Entropy, constructive algorithm, set theory, Approximation methods, polynomial time algorithm, graph colouring, mathematical programming, entropy, minimum discrepancy problem, entropy method, semidefinite program, Approximation algorithms, Polynomials, minimisation, Manganese, computational complexity]
Bounded Independence Fools Degree-2 Threshold Functions
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
For an n-variate degree-2 real polynomial p, we prove that E<sub>x~D</sub>[sig(p(x))] Is determined up to an additive &#x03B5; as long as D is a k-wise Independent distribution over {-1, 1}n for k = poly(1/&#x03B5;). This gives a broad class of explicit pseudorandom generators against degree-2 boolean threshold functions, and answers an open question of Diakonikolas et al. (FOCS 2009).
[Fourier transforms, Smoothing methods, pseudorandom generator, threshold function, Neodymium, polynomials, $k$-wise independence, derandomization, bounded independence fools degree-2, Approximation methods, real polynomial, polynomial threshold functions, Boolean functions, Convolution, Polynomials, Eigenvalues and eigenfunctions, k-wise Independent distribution]
From Sylvester-Gallai Configurations to Rank Bounds: Improved Black-Box Identity Test for Depth-3 Circuits
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We study the problem of identity testing for depth-3 circuits of top fanin k and degree d. We give a new structure theorem for such identities. A direct application of our theorem improves the known deterministic d -time black-box identity test over rationals (Kayal &amp; Saraf, FOCS 2009) to one that takes d(O(k2))-time. Our structure theorem essentially says that the number of independent variables in a real depth-3 identity is very small. This theorem affirmatively settles the strong rank conjecture posed by Dvir &amp; Shpilka (STOC 2005). We devise a powerful algebraic framework and develop tools to study depth-3 identities. We use these tools to show that any depth-3 identity contains a much smaller nucleus identity that contains most of the "complexity" of the main identity. The special properties of this nucleus allow us to get almost optimal rank bounds for depth-3 identities.
[circuit complexity, polynomials, black-box identity test, Vectors, Complexity theory, Sylvester-Gallai configuration, ideal theory, History, incidence configuration, Chinese remaindering, identities, Sylvester-Gallai, depth-3 circuits, Logic gates, rank bound, Polynomials, depth-3 identities, structure theorem, depth-3 circuit, Testing]
The Coin Problem and Pseudorandomness for Branching Programs
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The Coin Problem is the following problem: a coin is given, which lands on head with probability either 1/2 + &#x03B2; or 1/2 - &#x03B2;. We are given the outcome of n independent tosses of this coin, and the goal is to guess which way the coin is biased, and to answer correctly with probability &#x2265; 2/3. When our computational model is unrestricted, the majority function is optimal, and succeeds when &#x03B2; &#x2265; c/&#x221A;n for a large enough constant c. The coin problem is open and interesting in models that cannot compute the majority function. In this paper we study the coin problem in the model of read-once width-w branching programs. We prove that in order to succeed in this model, &#x03B2; must be at least 1/(log n)&#x0398;(&#x03C9;). For constant w this is tight by considering the recursive tribes function, and for other values of w this is nearly tight by considering other read-once AND-OR trees. We generalize this to a Dice Problem, where instead of independent tosses of a coin we are given independent tosses of one of two m-sided dice. We prove that if the distributions are too close and the mass of each side of the dice is not too small, then the dice cannot be distinguished by small-width read-once branching programs. We suggest one application for this kind of theorems: we prove that Nisan's Generator fools width-w read-once regular branching programs, using seed length O (&#x03C9;4 log n log log n + log n log(1/&#x03B5;)). For &#x03C9; = &#x03B5; = &#x0398;(1), this seedlength is O (log n log log n). The coin theorem and its relatives might have other connections to PRGs. This application is related to the independent, but chronologically-earlier, work of Braverman, Rao, Raz and Yehudayoff.
[read once width-w branching program, AND-OR tree, branching program pseudorandomness, Computational modeling, coin problem, game theory, computability, Generators, Magnetic heads, probability function, Approximation methods, tree searching, lower bounds, Upper bound, random sequences, m-sided dice problem, Nisan generator, branching programs, Automata, computational model, pseudorandomness, Random variables, independent toss, computational complexity]
Pseudorandom Generators for Regular Branching Programs
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We give new pseudorandom generators for regular read-once branching programs of small width. A branching program is regular if the in-degree of every vertex in it is either 0 or 2. For every width d and length n, our pseudorandom generator uses a seed of length O((log d + log log n + log(1/&#x03F5;)) log n) to produce n bits that cannot be distinguished from a uniformly random string by any regular width d length n read-once branching program, except with probability &#x03F5;. We also give a result for general read-once branching programs, in the case that there are no vertices that are reached with small probability. We show that if a (possibly non-regular) branching program of length n and width d has the property that every vertex in the program is traversed with probability at least &#x03B3; on a uniformly random input, then the error of the generator above is at most 2&#x03F5;/&#x03B3;2.
[Computational modeling, pseudorandom generators, vertex, explicit constructions, Generators, Electronic mail, random number generation, uniformly random string, Equations, binary decision diagrams, branching programs, Games, Random variables, regular read once branching programs, Labeling, Pseudorandomness]
Boosting and Differential Privacy
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Boosting is a general method for improving the accuracy of learning algorithms. We use boosting to construct improved privacy-pre serving synopses of an input database. These are data structures that yield, for a given set Q of queries over an input database, reasonably accurate estimates of the responses to every query in Q, even when the number of queries is much larger than the number of rows in the database. Given a base synopsis generator that takes a distribution on Q and produces a "weak" synopsis that yields "good" answers for a majority of the weight in Q, our Boosting for Queries algorithm obtains a synopsis that is good for all of Q. We ensure privacy for the rows of the database, but the boosting is performed on the queries. We also provide the first synopsis generators for arbitrary sets of arbitrary low-sensitivity queries, i.e., queries whose answers do not vary much under the addition or deletion of a single row. In the execution of our algorithm certain tasks, each incurring some privacy loss, are performed many times. To analyze the cumulative privacy loss, we obtain an O(&#x03B5;2) bound on the expected privacy loss from a single e-differentially private mechanism. Combining this with evolution of confidence arguments from the literature, we get stronger bounds on the expected cumulative privacy loss due to multiple mechanisms, each of which provides e-differential privacy or one of its relaxations, and each of which operates on (potentially) different, adaptively chosen, databases.
[Data privacy, learning algorithm, boosting, data structure, Boosting, Data structures, Generators, cumulative privacy loss, query algorithm, synopsis generator, query processing, Privacy, Accuracy, expected privacy loss, database, Databases, algorithm theory, data structures, learning (artificial intelligence), differential privacy]
A Multiplicative Weights Mechanism for Privacy-Preserving Data Analysis
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We consider statistical data analysis in the interactive setting. In this setting a trusted curator maintains a database of sensitive information about individual participants, and releases privacy-preserving answers to queries as they arrive. Our primary contribution is a new differentially private multiplicative weights mechanism for answering a large number of interactive counting (or linear) queries that arrive online and may be adaptively chosen. This is the first mechanism with worst-case accuracy guarantees that can answer large numbers of interactive queries and is efficient (in terms of the runtime's dependence on the data universe size). The error is asymptotically optimal in its dependence on the number of participants, and depends only logarithmically on the number of queries being answered. The running time is nearly linear in the size of the data universe. As a further contribution, when we relax the utility requirement and require accuracy only for databases drawn from a rich class of databases, we obtain exponential improvements in running time. Even in this relaxed setting we continue to guarantee privacy for any input database. Only the utility requirement is relaxed. Specifically, we show that when the input database is drawn from a smooth distribution - a distribution that does not place too much weight on any single data item - accuracy remains as above, and the running time becomes poly-logarithmic in the data universe size. The main technical contributions are the application of multiplicative weights techniques to the differential privacy setting, a new privacy analysis for the interactive setting, and a technique for reducing data dimensionality for databases drawn from smooth distributions.
[Data privacy, differentially private multiplicative weights mechanism, data analysis, Noise, data universe, Noise measurement, privacy-preserving data analysis, data dimensionality reduction, query processing, Privacy, Histograms, multiplicative weights mechanism, Accuracy, Databases, data privacy, statistical data analysis, question answering (information retrieval), statistical analysis]
Impossibility of Differentially Private Universally Optimal Mechanisms
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The notion of universally utility-maximizing privacy mechanism was recently introduced by Ghosh, Rough garden, and Sundararajan [STOC 2009]. These are mechanisms that guarantee optimal utility to a large class of information consumers, simultaneously, while preserving Differential Privacy [Dwork, McSherry, Nissim, and Smith, TCC 2006]. Ghosh, Rough garden and Sundararajan have demonstrated, quite surprisingly, a case where such a universally-optimal differentially-private mechanisms exists, when the information consumers are Bayesian. This result was recently extended by Gupte and Sundararajan [PODS 2010] to risk-averse consumers. Both positive results deal with mechanisms (approximately) computing a single count query (i.e., the number of individuals satisfying a specific property in a given population), and the starting point of our work is a trial at extending these results to similar settings, such as sum queries with non-binary individual values, histograms, and two (or more) count queries. We show, however, that universally-optimal mechanisms do not exist for all these queries, both for Bayesian and risk-averse consumers. For the Bayesian case, we go further, and give a characterization of those functions that admit universally-optimal mechanisms, showing that a universally-optimal mechanism exists, essentially, only for a (single) count query. At the heart of our proof is a representation of a query function f by its privacy constraint graph G<sub>f</sub> whose edges correspond to values resulting by applying f to neighboring databases.
[universally optimal mechanisms, Data privacy, Bayesian consumer, Noise, graph theory, privacy constraint graph, utility, Degradation, Privacy, Histograms, geometric mechanism, Databases, risk-averse consumer, Bayesian methods, universally utility-maximizing privacy mechanism, differentially private universally optimal mechanism, data privacy, Bayes methods, differential privacy]
Settling the Polynomial Learnability of Mixtures of Gaussians
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Given data drawn from a mixture of multivariate Gaussians, a basic problem is to accurately estimate the mixture parameters. We give an algorithm for this problem that has running time and data requirements polynomial in the dimension and the inverse of the desired accuracy, with provably minimal assumptions on the Gaussians. As a simple consequence of our learning algorithm, we we give the first polynomial time algorithm for proper density estimation for mixtures of k Gaussians that needs no assumptions on the mixture. It was open whether proper density estimation was even statistically possible (with no assumptions) given only polynomially many samples, let alone whether it could be computationally efficient. The building blocks of our algorithm are based on the work (Kalai et al, STOC 2010) that gives an efficient algorithm for learning mixtures of two Gaussians by considering a series of projections down to one dimension, and applying the method of moments to each univariate projection. A major technical hurdle in the previous work is showing that one can efficiently learn univariate mixtures of two Gaussians. In contrast, because pathological scenarios can arise when considering projections of mixtures of more than two Gaussians, the bulk of the work in this paper concerns how to leverage a weaker algorithm for learning univariate mixtures (of many Gaussians) to learn in high dimensions. Our algorithm employs hierarchical clustering and rescaling, together with methods for backtracking and recovering from the failures that can occur in our univariate algorithm. Finally, while the running time and data requirements of our algorithm depend exponentially on the number of Gaussians in the mixture, we prove that such a dependence is necessary.
[Additives, estimation theory, univariate mixtures, polynomials, Estimation, polynomial learnability, Probability, data requirements polynomial, learning, multivariate Gaussian mixture, polynomial time algorithm, Computer science, Accuracy, Clustering algorithms, Gaussian processes, proper density estimation, hierarchical clustering, Polynomials, method of moments, learning (artificial intelligence), mixture models, computational complexity]
Polynomial Learning of Distribution Families
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The question of polynomial learn ability of probability distributions, particularly Gaussian mixture distributions, has recently received significant attention in theoretical computer science and machine learning. However, despite major progress, the general question of polynomial learn ability of Gaussian mixture distributions still remained open. The current work resolves the question of polynomial learn ability for Gaussian mixtures in high dimension with an arbitrary fixed number of components. Specifically, we show that parameters of a Gaussian mixture distribution with fixed number of components can be learned using a sample whose size is polynomial in dimension and all other parameters. The result on learning Gaussian mixtures relies on an analysis of distributions belonging to what we call &#x201C;polynomial families&#x201D; in low dimension. These families are characterized by their moments being polynomial in parameters and include almost all common probability distributions as well as their mixtures and products. Using tools from real algebraic geometry, we show that parameters of any distribution belonging to such a family can be learned in polynomial time and using a polynomial number of sample points. The result on learning polynomial families is quite general and is of independent interest. To estimate parameters of a Gaussian mixture distribution in high dimensions, we provide a deterministic algorithm for dimensionality reduction. This allows us to reduce learning a high-dimensional mixture to a polynomial number of parameter estimations in low dimension. Combining this reduction with the results on polynomial families yields our result on learning arbitrary Gaussian mixtures in high dimensions.
[polynomials, Estimation, Gaussian distribution, Probability distribution, Covariance matrix, machine learning, Gaussian mixture learning, polynomial learning, Computer science, Geometry, theoretical computer science, computer science, distribution families, probability distributions, Polynomials, algebraic geometry, Moment methods, learning (artificial intelligence), Gaussian mixture distributions, Polynomial Learnability]
Agnostically Learning under Permutation Invariant Distributions
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We generalize algorithms from computational learning theory that are successful under the uniform distribution on the Boolean hypercube {0,1}n to algorithms successful on permutation invariant distributions. A permutation invariant distribution is a distribution where the probability mass remains constant upon permutations in the instances. While the tools in our generalization mimic those used for the Boolean hypercube, the fact that permutation invariant distributions are not product distributions presents a significant obstacle. Under the uniform distribution, halfspaces can be agnostically learned in polynomial time for constant e. The main tools used are a theorem of Peres [Per04] bounding the noise sensitivity of a halfspace, a result of [KOS04] that this theorem implies Fourier concentration, and a modification of the Low-Degree algorithm of Linial, Mansour, Nisan [LMN93] made by Kalai et. al. [KKMS08]. These results are extended to arbitrary product distributions in [BOW08]. We prove analogous results for permutation invariant distributions; more generally, we work in the domain of the symmetric group. We define noise sensitivity in this setting, and show that noise sensitivity has a nice combinatorial interpretation in terms of Young tableaux. The main technical innovations involve techniques from the representation theory of the symmetric group, especially the combinatorics of Young tableaux. We show that low noise sensitivity implies concentration on "simple" components of the Fourier spectrum, and that this fact will allow us to agnostically learn halfspaces under permutation invariant distributions to constant accuracy in roughly the same time as in the uniform distribution over the Boolean hypercube case.
[combinatorial mathematics, theorem of Peres, Noise, Young tableaux, low noise sensitivity, computational learning theory, Loss measurement, uniform distribution, arbitrary product distributions, Boolean functions, low-degree algorithm, Fourier spectrum, Hypercubes, Polynomials, polynomial time, learning (artificial intelligence), Fourier concentration, combinatorics, Fourier analysis, Boolean algebra, agnostic learning, probability mass, Support vector machines, Sensitivity, combinatorial interpretation, Tin, permutation invariant distributions, representation theory, agnostically learning, symmetric group, computational complexity, Boolean hypercube]
Corrigendum: A Random Sampling Algorithm for Learning an Intersection of Halfspaces
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We correct a claim from [Vem97] and provide a status update.
[random sampling algorithm, random processes, Linear programming, convex programming, learning, Complexity theory, set theory, Computer science, Neural networks, Approximation algorithms, Polynomials, learning (artificial intelligence), halfspace intersection, Principal component analysis, computational complexity]
Learning Convex Concepts from Gaussian Distributions with PCA
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present a new algorithm for learning a convex set in n-dimensional space given labeled examples drawn from any Gaussian distribution. The complexity of the algorithm is bounded by a fixed polynomial in n times a function of k and &#x03F5; where k is the dimension of the normal subspace (the span of normal vectors to supporting hyperplanes of the convex set) and the output is a hypothesis that correctly classifies at least 1 - &#x03F5; of the unknown Gaussian distribution. For the important case when the convex set is the intersection of k halfspaces, the complexity is poly(n, k, 1/&#x03F5;) + n &#x00B7; min k(O(log k/&#x03F5;4)), (k/&#x03F5;)O(k), improving substantially on the state of the art [Vem04], [KOS08] for Gaussian distributions. The key step of the algorithm is a Singular Value Decomposition after applying a normalization. The proof is based on a monotonicity property of Gaussian space under convex restrictions.
[Algorithm design and analysis, convex restriction, monotonicity property, Gaussian distribution, Gaussian space, learning, Complexity theory, Covariance matrix, set theory, PCA, polynomial complexity, Gaussians, convex set, Accuracy, convex, High-dimensional learning, Polynomials, polynomial time, learning (artificial intelligence), principal component analysis, singular value decomposition, Principal component analysis, computational complexity]
Deciding First-Order Properties for Sparse Graphs
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present a linear-time algorithm for deciding first-order logic (FOL) properties in classes of graphs with bounded expansion. Many natural classes of graphs have bounded expansion: graphs of bounded tree-width, all proper minor-closed classes of graphs, graphs of bounded degree, graphs with no sub graph isomorphic to a subdivision of a fixed graph, and graphs that can be drawn in a fixed surface in such a way that each edge crosses at most a constant number of other edges. We also develop an almost linear-time algorithm for deciding FOL properties in classes of graphs with locally bounded expansion, those include classes of graphs with locally bounded tree-width or locally excluding a minor. More generally, we design a dynamic data structure for graphs belonging to a fixed class of graphs of bounded expansion. After a linear-time initialization the data structure allows us to test an FOL property in constant time, and the data structure can be updated in constant time after addition/deletion of an edge, provided the list of possible edges to be added is known in advance and their addition results in a graph in the class. In addition, we design a dynamic data structure for testing existential properties or the existence of short paths between prescribed vertices in such classes of graphs. All our results also hold for relational structures and are based on the seminal result of Nesetril and Ossona de Mendez on the existence of low tree-depth colorings.
[TV, Gallium, graph theory, dynamic data structure, Mathematics, bounded tree-width, linear-time algorithm, formal logic, bounded degree, graphs with bounded expansion, Image color analysis, minor-closed classes of graphs, data structures, graphs with locally bounded tree-width, Testing, linear-time initialization, sparse graphs, Color, Data structures, first-order logic properties, algorithmic metatheorems, locally bounded expansion, minor-closed classes, graphs with bounded degree, computational complexity]
Logspace Versions of the Theorems of Bodlaender and Courcelle
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Bodlaender's Theorem states that for every k there is a linear-time algorithm that decides whether an input graph has tree width k and, if so, computes a width-k tree composition. Courcelle's Theorem builds on Bodlaender's Theorem and states that for every monadic second-order formula &#x03C6; and for every k there is a linear-time algorithm that decides whether a given logical structure A of tree width at most k satisfies &#x03C6;. We prove that both theorems still hold when "linear time" is replaced by "logarithmic space." The transfer of the powerful theoretical framework of monadic second-order logic and bounded tree width to logarithmic space allows us to settle a number of both old and recent open problems in the log space world.
[bounded tree width, tree width, Particle separators, trees (mathematics), monadic second-order formula, NP-complete problem, Bodlaender Theorem, linear-time algorithm, formal logic, Logspace version, deterministic logarithmic space, Automata, partial k-trees, Binary trees, logarithmic space, Approximation algorithms, Courcelle theorem, monadic second-order logic, width-k tree composition, Periodic structures]
A Separator Theorem in Minor-Closed Classes
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
It is shown that for each t, there is a separator of size O(t&#x221A;n) in any n-vertex graph G with no K<sub>t</sub>-minor. This settles a conjecture of Alon, Seymour and Thomas (J. Amer. Math. Soc, 1990 and STOC'90), and generalizes a result of Djidjev (1981), and Gilbert, Hutchinson and Tarjan (J. Algorithm, 1984), independently, who proved that every graph with n vertices and genus g has a separator of order O(&#x221A;gn), because K<sub>t</sub> has genus &#x03A9;(t2). The bound O(t&#x221A;n) is best possible because every 3-regular expander graph with n vertices is a graph with no K<sub>t</sub>-minor for t = cn1/2, and with no separator of size dn for appropriately chosen positive constants c, d. In addition, we give an O(n2) time algorithm to obtain such a separator, and then give a sketch how to obtain such a separator in O(n1+&#x03B5;) time for any &#x03B5; &gt; 0. Finally, we discuss several algorithm aspects of our separator theorem, including a possibility to obtain a separator of order g(t)&#x221A;n, for some function g of t, in an n-vertex graph G with no K<sub>t</sub>-minor in O(n) time.
[Algorithm design and analysis, Particle separators, Image edge detection, graph theory, time algorithm, excluded minor, Graph theory, Electronic mail, Partitioning algorithms, separator, and divide and conquer, minor closed classes, 3-regular expander graph, Adhesives, separator theorem, n-vertex graph]
Optimal Stochastic Planarization
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
It has been shown by Indyk and Sidiropoulos that any graph of genus g &gt; 0 can be stochastically embedded into a distribution over planar graphs with distortion 2O(g). This bound was later improved to O(g2) by Borradaile, Lee and Sidiropoulos. We give an embedding with distortion O(log g), which is asymptotically optimal. Apart from the improved distortion, another advantage of our embedding is that it can be computed in polynomial time. In contrast, the algorithm of requires solving an NP-hard problem. Our result implies in particular a reduction for a large class of geometric optimization problems from instances on genus-p graphs, to corresponding ones on planar graphs, with a O(log g) loss factor in the approximation guarantee.
[embeddings, optimal stochastic planarization, genus-p graphs, graph theory, planar graphs, Extraterrestrial measurements, Minimization, Generators, Partitioning algorithms, Optimization, geometric optimization problems, approximation guarantee, optimisation, asymptotically optimal, NP-hard problem, genus, Polynomials, polynomial time, stochastic processes, computational complexity]
Determinant Sums for Undirected Hamiltonicity
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present a Monte Carlo algorithm for Hamiltonicity detection in an n-vertex undirected graph running in O* (1.657n) time. To the best of our knowledge, this is the first superpolynomial improvement on the worst case runtime for the problem since the O*(2n) bound established for TSP almost fifty years ago (Bellman 1962, Held and Karp 1962). It answers in part the first open problem in Woeginger's 2003 survey on exact algorithms for NP-hard problems. For bipartite graphs, we improve the bound to O* (1.414n) time. Both the bipartite and the general algorithm can be implemented to use space polynomial in n. We combine several recently resurrected ideas to get the results. Our main technical contribution is a new reduction inspired by the algebraic sieving method for k-Path (Koutis ICALP 2008, Williams IPL 2009). We introduce the Labeled Cycle Cover Sum in which we are set to count weighted arc labeled cycle covers over a finite field of characteristic two. We reduce Hamiltonicity to Labeled Cycle Cover Sum and apply the determinant summation technique for Exact Set Covers (Bjo&#x0308;rklund STACS 2010) to evaluate it.
[exact set cover, worst case runtime, Monte Carlo algorithm, graph theory, determinant summation, space polynomial, undirected Hamiltonicity detection, algebraic sieving method, TSP, Interpolation, Runtime, Monte Carlo methods, graphs, optimisation, bipartite graphs, NP-hard problem, Hamiltonicity, labeled cycle cover sum, Polynomials, Bipartite graph, Dynamic programming, Mirrors, undirected graph, Exact algorithms]
Fighting Perebor: New and Improved Algorithms for Formula and QBF Satisfiability
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We investigate the possibility of finding satisfying assignments to Boolean formulae and testing validity of quantified Boolean formulae (QBF) asymptotically faster than a brute force search. Our first main result is a simple deterministic algorithm running in time 2n-&#x03A9;(n) for satisfiability of formulae of linear size in n, where n is the number of variables in the formula. This algorithm extends to exactly counting the number of satisfying assignments, within the same time bound. Our second main result is a deterministic algorithm running in time 2n-&#x03A9;(n/log(n)) for solving QBFs in which the number of occurrences of any variable is bounded by a constant. For instances which are "structured\
[Algorithm design and analysis, quantified Boolean formulas, QBF satisfiability, Force, computability, Search problems, Complexity theory, average case lower bounds, Satisfiability algorithms, quantified Boolean formulae, Boolean functions, decision trees, Approximation algorithms, parity function, Polynomials, random restrictions, Decision trees]
The Monotone Complexity of k-clique on Random Graphs
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
It is widely suspected that Erdo&#x0308;s-Renyi random graphs are a source of hard instances for clique problems. Giving further evidence for this belief, we prove the first average-case hardness result for the k-clique problem on monotone circuits. Specifically, we show that no monotone circuit of size O(nk/4) solves the k-clique problem with high probability on G(n,p) for two sufficiently far-apart threshold functions p(n) (for instance n-2/(k-1) and 2n-2/(k-1)). Moreover, the exponent k/4 in this result is tight up to an additive constant. One technical contribution of this paper is the introduction of quasi-sunflowers, a new relaxation of sunflowers in which petals may overlap slightly on average. A "quasi-sunflower lemma" (a&#x0300; la the Erdo&#x0308;s-Rado sunflower lemma) leads to our novel lower bounds within Razborov's method of approximations.
[circuit complexity, Additives, Razborov's method, graph theory, Lattices, probability, quasi-sunflowers, random processes, approximations, quasisunflower lemma, Complexity theory, Approximation methods, k-Clique, average-case complexity, monotone complexity, first average-case hardness, clique, Erdos-Renyi random graphs, Logic gates, Approximation algorithms, monotone circuits, Digital TV]
The Complexity of Distributions
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Complexity theory typically studies the complexity of computing a function h(x) : {0, 1}m &#x2192; {0,1}n of a given input x. We advocate the study of the complexity of generating the distribution h(x) for uniform x, given random bits. Our main results are: (1) Any function f : {0, 1}&#x2113; &#x2192; {0,1}n such that (i) each output bit f<sub>i</sub> depends on o(log n) input bits, and (ii) &#x2113; &#x2264; log<sub>2</sub> (<sub>&#x03B1;n</sub>n) + n0.99, has output distribution f(U) at statistical distance &#x2265; 1 - 1/n0.49 from the uniform distribution over n-bit strings of hamming weight &#x03B1;n. We also prove lower bounds for generating (X, b(X)) for boolean b, and in the case in which each bit f<sub>i</sub> is a small-depth decision tree. These lower bounds seem to be the first of their kind; the proofs use anti-concentration results for the sum of random variables. (2) Lower bounds for generating distributions imply succinct data structures lower bounds. As a corollary of (1), we obtain the first lower bound for the membership problem of representing a set S &#x2286; [n] of size &#x03B1;n, in the case where 1/&#x03B1; is a power of 2: If queries "i &#x2208; S?" are answered by non-adaptively probing o(log n) bits, then the representation uses &#x2265; log<sub>2</sub> (<sub>&#x03B1;n</sub>3) + &#x03A9;(log n) bits. (3) Upper bounds complementing the bounds in (1) for various settings of parameters. (4) Uniform randomized AC0 circuits of poly(n) size and depth d = O(1) with error &#x03F5; can be simulated by uniform randomized AC0 circuits of poly(n) size and depth d + 1 with error &#x03F5; + o(1) using &#x2264; (log n)O(log log n) random bits. Previous derandomizations [Ajtai and Wigderson '85; Nisan '91] increase the depth by a constant factor, or else have poor seed length.
[output distribution, data structure, upper bounds, Complexity theory, statistical distributions, distribution, uniform distribution, random bits, membership problem, Silicon, pseudorandomness, complexity theory, small-depth decision tree, n-bit strings, statistical distance, Data structures, Generators, lower bound, Hamming weight, lower bounds, Upper bound, hamming weight, k-wise independent distributions, Integrated circuit modeling, computational complexity]
Hardness of Finding Independent Sets in Almost 3-Colorable Graphs
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
For every &#x2208; &gt; 0, and integer q &#x2265; 3, we show that given an N-vertex graph that has an induced q-colorable subgraph of size (1 - &#x2208;)N, it is NP-hard to find an independent set of size N/q2.
[Weight measurement, approximation theory, independent sets finding, Color, hardness of approximation, Search problems, graph coloring, set theory, colorable graph, Approximation methods, graph colouring, NP-hardness, Coordinate measuring machines, optimisation, V-vertex graph, PCPs, Games, Polynomials]
Solving Linear Systems through Nested Dissection
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The generalized nested dissection method, developed by Lipton, Rose, and Tarjan, is a seminal method for solving a linear system Ax=b where A is a symmetric positive definite matrix. The method runs extremely fast whenever A is a well-separable matrix (such as matrices whose underlying support is planar or avoids a fixed minor). In this work we extend the nested dissection method to apply to any non-singular well-separable matrix over any field. The running times we obtain essentially match those of the nested dissection method.
[Linear systems, Transmission line matrix methods, Symmetric matrices, Gallium, symmetric positive definite matrix, nested dissection method, nonsingular well separable matrix, Particle separators, linear systems, Complexity theory, Sparse matrices, matrix algebra, linear system, Gaussian elimination, nested dissection]
Approaching Optimality for Solving SDD Linear Systems
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present an algorithm that on input of an n-vertex m-edge weighted graph G and a value k, produces an incremental sparsifier G with n-1+m/k edges, such that the condition number of G with G is bounded above by O&#x0303;(k log2 n), with probability 1-p. The algorithm runs in time O&#x0303;((m log n + n log n) log(1/p)). As a result, we obtain an algorithm that on input of an n &#x00D7; n symmetric diagonally dominant matrix A with m non-zero entries and a vector b, computes a vector x satisfying ||x-A+b||A &lt;; &#x03F5;||A+b||A, in expected time O&#x0303;(m log2 n log(1/&#x03F5;)). The solver is based on repeated applications of the incremental sparsifier that produces a chain of graphs which is then used as input to a recursive preconditioned Chebyshev iteration.
[Algorithm design and analysis, Linear systems, iterative methods, algorithms, graph theory, recursive preconditioned Chebyshev iteration, n-vertex m-edge weighted graph G, SDD linear systems, symmetric diagonally dominant matrix, combinatorial preconditioning, Chebyshev approximation, recursive estimation, vector, Symmetric matrices, Laplace equations, time complexity, linear systems, Partitioning algorithms, incremental sparsifier G, spectral graph theory, Resistance, Upper bound, vectors, incremental sparsifier, computational complexity]
Fast Approximation Algorithms for Cut-Based Problems in Undirected Graphs
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present a general method of designing fast approximation algorithms for cut-based minimization problems in undirected graphs. In particular, we develop a technique that given any such problem that can be approximated quickly on trees, allows approximating it almost as quickly on general graphs while only losing a poly-logarithmic factor in the approximation guarantee. To illustrate the applicability of our paradigm, we focus our attention on the undirected sparsest cut problem with general demands and the balanced separator problem. By a simple use of our framework, we obtain poly-logarithmic approximation algorithms for these problems that run in time close to linear. The main tool behind our result is an efficient procedure that decomposes general graphs into simpler ones while approximately preserving the cut-flow structure. This decomposition is inspired by the cut-based graph decomposition of R\\"acke that was developed in the context of oblivious routing schemes, as well as, by the construction of the ultrasparsifiers due to Spiel man and Teng that was employed to preconditioning symmetric diagonally-dominant matrices.
[Algorithm design and analysis, Context, approximation theory, Particle separators, polylogarithmic factor, graph theory, graph decomposition, trees (mathematics), Minimization, Partitioning algorithms, Approximation methods, fast approximation algorithms, trees, generalized sparsest cut, symmetric diagonally dominant matrices, balanced separator, cut based minimization problems, cut based graph decomposition, Approximation algorithms, cut-based problems, undirected sparsest cut problem, minimisation, undirected graphs, graph partitioning]
Metric Extension Operators, Vertex Sparsifiers and Lipschitz Extendability
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We study vertex cut and flow sparsifiers that were recently introduced by Moitra, and Leighton and Moitra. We improve and generalize their results. We give a new polynomial-time algorithm for constructing O(log k/log log k) cut and flow sparsifiers, matching the best known existential upper bound on the quality of a sparsifier, and improving the previous algorithmic upper bound of O(log2 k/log log k). We show that flow sparsifiers can be obtained from linear operators approximating minimum metric extensions. We introduce the notion of (linear) metric extension operators, prove that they exist, and give an exact polynomialtime algorithm for finding optimal operators. We then establish a direct connection between flow and cut sparsifiers and Lipschitz extendability of maps in Banach spaces, a notion studied in functional analysis since 1950s. Using this connection, we obtain a lower bound of &#x03A9; (&#x221A;log k/ log log k) for flow sparsifiers and a lower bound of &#x03A9;( &#x221A;g k/ log log k) for cut sparsifiers. We show that if a certain open question posed by Ball in 1992 has a positive answer, then there exist O&#x0303;(&#x221A;log k) cut sparsifiers. On the other hand, any lower bound on cut sparsifiers better than &#x03A9;&#x0303;(&#x221A;log k) would imply a negative answer to this question.
[algorithmic upper bound, lipschitz extendability, linear operators, functional analysis, minimum metric extensions, Extraterrestrial measurements, Functional analysis, Banach spaces, optimal operators, vertex cut, linear metric extension operators, Upper bound, Lipschitz extendability, Linear approximation, Approximation algorithms, polynomial-time algorithm, flow sparsifiers, vertex sparsifiers, computational complexity]
Vertex Sparsifiers and Abstract Rounding Algorithms
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The notion of vertex sparsification (in particular cut-sparsification) is introduced in, where it was shown that for any graph G = (V, E) and any subset of k terminals K &#x2282; V, there is a polynomial time algorithm to construct a graph H = (K, E<sub>H</sub>) on just the terminal set so that simultaneously for all cuts (A,K-A), the value of the minimum cut in G separating A from K-A is approximately the same as the value of the corresponding cut in H. Then approximation algorithms can be run directly on H as a proxy for running on G. We give the first super-constant lower bounds for how well a cut-sparsifier H can simultaneously approximate all minimum cuts in G. We prove a lower bound of &#x03A9;(log1/4 k) this is polynomially-related to the known upper bound of O(log k/log log k). Independently, a similar lower bound is given in. This is an exponential improvement on the &#x03A9;(log log k) bound given in which in fact was for a stronger vertex sparsification guarantee, and did not apply to cut sparsifiers. Despite this negative result, we show that for many natural optimization problems, we do not need to incur a multiplicative penalty for our reduction. Roughly, we show that any rounding algorithm which also works for the O-extension relaxation can be used to construct good vertex-sparsifiers for which the optimization problem is easy. Using this, we obtain optimal O(log k)-competitive Steiner oblivious routing schemes, which generalize the results in. We also demonstrate that for a wide range of graph packing problems (which includes maximum concurrent flow, maximum multiflow and multicast routing, among others, as a special case), the integrality gap of the linear program is always at most O(log k) times the integrality gap restricted to trees. Lastly, we use our ideas to give an efficient construction for vertex-sparsifiers that match the current best existential results - this was previously open. Our algorithm makes novel use of Earth-mover constraints.
[Measurement, approximation theory, vertex sparsification, cut-sparsification, optimal O(log k)-competitive Steiner oblivious routing schemes, graph theory, vertex sparsifier, Routing, approximation algorithms, Approximation methods, Earth-mover constraints, Hamming weight, polynomial time algorithm, Optimization, abstract rounding algorithms, graph packing problems, O-extension relaxation, Approximation algorithms, Polynomials, natural optimization problems, multicast routing, multiflow routing, computational complexity]
Approximation Algorithms for the Edge-Disjoint Paths Problem via Raecke Decompositions
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We study the Edge-Disjoint Paths with Congestion (EDPwC) problem in undirected networks in which we must integrally route a set of demands without causing large congestion on an edge. We present a (polylog(n),poly(log log n))approximation, which means that if there exists a solution that routes X demands integrally on edge-disjoint paths (i.e. with congestion 1), then the approximation algorithm can route X/polylog(n) demands with congestion poly(log log n). The best previous result for this problem was a (n1/&#x03B2;,&#x03B2;)approximation for &#x03B2; &lt;; log n.
[approximation theory, undirected network, Routing, Graph theory, Approximation methods, NP-hard problem, edge-disjoint path problem, Edge-disjoint paths, Clustering algorithms, algorithm theory, approximation algorithm, Approximation algorithms, Raecke decomposition, Random variables]
Computational Transition at the Uniqueness Threshold
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The hardcore model is a model of lattice gas systems which has received much attention in statistical physics, probability theory and theoretical computer science. It is the probability distribution over independent sets I of a graph weighted proportionally to &#x03BB;|I| with fugacity parameter &#x03BB;. We prove that at the uniqueness threshold of the hardcore model on the d-regular tree, approximating the partition function becomes computationally hard on graphs of maximum degree d. Specifically, we show that unless NP = RP there is no polynomial time approximation scheme for the partition function (the sum of such weighted independent sets) on graphs of maximum degree d for fugacity &#x03BB;<sub>c</sub>(d) &lt;; &#x03BB; &lt;; &#x03BB;<sub>c</sub>(d) + &#x03B5;(d) where &#x03BB;<sub>c</sub> = (d - 1)d-1/(d - 2)d is the uniqueness threshold on the d-regular tree and &#x03B5;(d) &gt; 0 is a positive constant. Weitz [36] produced an FPTAS for approximating the partition function when 0 &lt;; &#x03BB; &lt;; &#x03BB;<sub>c</sub>(d) so this result demonstrates that the computational threshold exactly coincides with the statistical physics phase transition thus confirming the main conjecture of [30]. We further analyze the special case of &#x03BB; = 1, d = 6 and show there is no polynomial time approximation scheme for approximately counting independent sets on graphs of maximum degree d = 6, which is optimal, improving the previous bound of d = 24. Our proof is based on specially constructed random bipartite graphs which act as gadgets in a reduction to MAX-CUT. Building on the involved second moment method analysis of [30] and combined with an analysis of the reconstruction problem on the tree our proof establishes a strong version of "replica" method heuristics developed by theoretical physicists. The result establishes the first rigorous correspondence between the hardness of approximate counting and sampling with statistical physics phase transitions.
[fugacity parameter, Correlation, Phase measurement, lattice gas system, set theory, Approximation methods, statistical distributions, Phase Transition, statistical physics phase transition, function approximation, probability distribution, Polynomials, phase transformations, partition becomes approximating, Approximate counting, Hardcore Model, uniqueness threshold, Computational modeling, trees (mathematics), Physics, Markov processes, d-regular tree, random bipartite graph, hardcore model, computational complexity]
Clustering with Spectral Norm and the k-Means Algorithm
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
There has been much progress on efficient algorithms for clustering data points generated by a mixture of k probability distributions under the assumption that the means of the distributions are well-separated, i.e., the distance between the means of any two distributions is at least &#x03A9;(k) standard deviations. These results generally make heavy use of the generative model and particular properties of the distributions. In this paper, we show that a simple clustering algorithm works without assuming any generative (probabilistic) model. Our only assumption is what we call a "proximity condition'': the projection of any data point onto the line joining its cluster center to any other cluster center is &#x03A9;(k) standard deviations closer to its own center than the other center. Here the notion of standard deviations is based on the spectral norm of the matrix whose rows represent the difference between a point and the mean of the cluster to which it belongs. We show that in the generative models studied, our proximity condition is satisfied and so we are able to derive most known results for generative models as corollaries of our main result. We also prove some new results for generative models - e.g., we can cluster all but a small fraction of points only assuming a bound on the variance. Our algorithm relies on the well known k-means algorithm, and along the way, we prove a result of independent interest - that the k-means algorithm converges to the "true centers'' even in the presence of spurious points provided the initial (estimated) centers are close enough to the corresponding actual centers and all but a small fraction of the points satisfy the proximity condition. Finally, we present a new technique for boosting the ratio of inter-center separation to standard deviation. This allows us to prove results for learning certain mixture of distributions under weaker separation conditions.
[k-means Algorithm, spectral norm, cluster center, probability, intercenter separation, Gaussian distribution, simple clustering algorithm, Classification algorithms, Approximation methods, k probability distributions, pattern clustering, Clustering algorithms, Approximation algorithms, Polynomials, Data models, clustering data points]
Stability Yields a PTAS for k-Median and k-Means Clustering
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We consider fc-median clustering in finite metric spaces and fc-means clustering in Euclidean spaces, in the setting where k is part of the input (not a constant). For the fc-means problem, Ostrovsky et al. show that if the optimal (k - 1)-means clustering of the input is more expensive than the optimal fc-means clustering by a factor of 1/&#x2208;2, then one can achieve a (1 + f(&#x2208;))-approximation to the fc-means optimal in time polynomial in n and k by using a variant of Lloyd's algorithm. In this work we substantially improve this approximation guarantee. We show that given only the condition that the (k - 1)-means optimal is more expensive than the fc-means optimal by a factor 1 + &#x03B1; for some constant &#x03B1; &gt; 0, we can obtain a PTAS. In particular, under this assumption, for any &#x2208; &gt; 0 we achieve a (1 + &#x2208;)-approximation to the fc-means optimal in time polynomial in n and k, and exponential in 1/e and 1/&#x03B1;. We thus decouple the strength of the assumption from the quality of the approximation ratio. We also give a PTAS for the fc-median problem in finite metrics under the analogous assumption as well. For fc-means, we in addition give a randomized algorithm with improved running time of no(1) (k log n)poly(1/&#x2208;,1/&#x03B1;) Our technique also obtains a PTAS under the assumption of Balcan et al. that all (1 + &#x03B1;) approximations are &#x03B4;-close to a desired target clustering, in the case that all target clusters have size greater than &#x03B4;n and &#x03B1; &gt; 0 is constant. Note that the motivation of Balcan et al. is that for many clustering problems, the objective function is only a proxy for the true goal of getting close to the target. From this perspective, our improvement is that for fc-means in Euclidean spaces we reduce the distance of the clustering found to the target from O(&#x03B4;) to &#x03B4; when all target clusters are large, and for fc-median we improve the "largeness" condition needed in to get exactly &#x03B4;-close from O(&#x03B4;n) to &#x03B4;n. Our results are based on a new notion of clustering stability.
[Optimized production technology, Extraterrestrial measurements, Approximation methods, k-median clustering, k-means clustering, PTAS, Euclidean spaces, pattern clustering, Clustering algorithms, Approximation algorithms, Lloyd algorithm, Polynomials, stability, finite metric spaces]
The Geometry of Manipulation: A Quantitative Proof of the Gibbard-Satterthwaite Theorem
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We prove a quantitative version of the Gibbard-Satterthwaite theorem. We show that a uniformly chosen voter profile for a neutral social choice function f of q &#x2265; 4 alternatives and n voters will be manipulable with probability at least 10-4&#x2208;2n-3q-30, where e is the minimal statistical distance between / and the family of dictator functions. Our results extend those of, which were obtained for the case of 3 alternatives, and imply that the approach of masking manipulations behind computational hardness (as considered in) cannot hide manipulations completely. Our proof is geometric. More specifically it extends the method of canonical paths to show that the measure of the profiles that lie on the interface of 3 or more outcomes is large. To the best of our knowledge our result is the first isoperimetric result to establish interface of more than two bodies.
[Context, quantitative version, neutral social choice function, computational hardness, Electronic mail, manipulation geometry, masking manipulations, Computer science, Geometry, quantitative proof, demography, Gibbard-Satterthwaite theorem, dictator functions, Polynomials, Robustness, geometry, computational complexity]
Efficient Volume Sampling for Row/Column Subset Selection
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We give efficient algorithms for volume sampling, i.e., for picking k-subsets of the rows of any given matrix with probabilities proportional to the squared volumes of the simplices defined by them and the origin (or the squared volumes of the parallelepipeds defined by these subsets of rows). This solves an open problem from the monograph on spectral algorithms by Kannan and Vempala (see Section 7.4 of [15], also implicit in [1], [5]). Our first algorithm for volume sampling k-subsets of rows from an m-by-n matrix runs in O(kmn&#x03C9; log n) arithmetic operations (where &#x03C9; is the exponent of matrix multiplication) and a second variant of it for (1 + &#x03F5;)-approximate volume sampling runs in O(mn log m &#x00B7; k2/&#x03F5;2 +m log&#x03C9; m &#x00B7; k2&#x03C9;+1/&#x03F5;2&#x03C9; &#x00B7; log(k&#x03F5;-1 log m)) arithmetic operations, which is almost linear in the size of the input (i.e., the number of entries) for small k. Our efficient volume sampling algorithms imply the following results for low-rank matrix approximation: 1) Given A &#x2208; Rm&#x00D7;n, in O(kmn&#x03C9; log n) arithmetic operations we can find k of its rows such that projecting onto their span gives a &#x221A;k + 1-approximation to the matrix of rank fc closest to A under the Frobenius norm. This improves the O(k&#x221A;log k)-approximation of Boutsidis, Drineas and Mahoney [1] and matches the lower bound shown in [5]. The method of conditional expectations gives a deterministic algorithm with the same complexity. The running time can be improved to O(mn log m &#x00B7; k2/e2 + m log&#x03C9; m&#x00B7;k2&#x03C9;+1&#x03F5;2&#x03C9;-log(k&#x03F5;-1 log m)) at the cost of losing an extra (1 + &#x03F5;) in the approximation factor. 2) The same rows and projection as in the previous point give a &#x221A;(k + 1)(n -k)-approximation to the matrix of rank k closest to A under the spectral norm. In this paper, we show an almost matching lower bound of &#x221A;n, even for k = 1.
[Algorithm design and analysis, volume sampling, approximation theory, complexity, sampling methods, probability, Frobenius norm, Vectors, low-rank matrix approximation, Approximation methods, Matrix decomposition, deterministic algorithm, deterministic algorithms, approximation factor, matrix multiplication, spectral algorithm, arithmetic operation, squared volume, Approximation algorithms, row-column subset selection, Polynomials, row/column subset selection, Principal component analysis, computational complexity]
A Non-linear Lower Bound for Planar Epsilon-Nets
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We show that the minimum possible size of an &#x03F5;-net for point objects and line (or rectangle)-ranges in the plane is (slightly) bigger than linear in 1/&#x03F5;. This settles a problem raised by Matousek, Seidel and Welzl in 1990.
[VC dimension, Poles and towers, &#x03F5;-net, Color, computational geometry, Vectors, nonlinear lower bound, planar epsilon-nets, Computer science, Epsilon nets, weak epsilon nets, Approximation algorithms, Polynomials]
The Sub-exponential Upper Bound for On-Line Chain Partitioning
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The main question in the on-line chain partitioning problem is to determine whether there exists an algorithm that partitions on-line posets of width at most w into polynomial number of chains see Trotter's chapter Partially ordered sets in the Handbook of Combinatorics. So far the best known on-line algorithm of Kierstead used at most (5&#x03C9; - 1)/4 chains; on the other hand Szemeredi proved that any on-line algorithm requires at least (&#x03C9;+1/2) chains. These results were obtained in the early eighties and since then no progress in the general case has been done. We provide an on-line algorithm that partitions orders of width &#x03C9; into at most &#x03C9;16 log &#x03C9; chains. This yields the first subexponential upper bound for on-line chain partitioning problem.
[polynomial number, polynomials, Color, chain partitioning, online chain partitioning problem, Partitioning algorithms, set theory, subexponential upper bound, Computer science, Upper bound, partially ordered sets, Games, on-line, Bismuth, Polynomials, algorithm]
Improved Bounds for Geometric Permutations
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We show that the number of geometric permutations of an arbitrary collection of n pairwise disjoint convex sets in Rd, for d &#x2265; 3, is O(n2d-3 log n), improving Wenger's 20 years old bound of O(n2d-2).
[geometric permutation, Shape, convex sets, line transversals, Complexity theory, set theory, Indexes, Geometry, Upper bound, geometric permutations, Polynomials, geometry, Face, arrangements, pairwise disjoint convex set]
On the Queue Number of Planar Graphs
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We prove that planar graphs have poly-logarithmic queue number, thus improving upon the previous polynomial upper bound. Consequently, planar graphs admit 3D straight-line crossing-free grid drawings in small volume.
[queueing theory, 3D straight-line crossing-free grid drawings, polynomials, graph theory, planar graphs, computational geometry, Partitioning algorithms, queue layout, volume, Upper bound, straight-line drawing, Layout, Heating, polynomial upper bound, poly-logarithmic queue number, Three dimensional displays, Books, Joining processes, computational complexity]
Polylogarithmic Approximation for Edit Distance and the Asymmetric Query Complexity
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present a near-linear time algorithm that approximates the edit distance between two strings within a polylogarithmic factor. For strings of length n and every fixed &#x03B5; &gt;; 0, the algorithm computes a (log n)O(1/&#x03B5;) approximation in n1+&#x03B5; time. This is an exponential improvement over the previously known approximation factor, 2O&#x0303;(&#x221A;log n), with a comparable running time [Ostrovsky and Rabani, J. ACM 2007; Andoni and Onak, STOC 2009]. This result arises naturally in the study of a new asymmetric query model. In this model, the input consists of two strings x and y, and an algorithm can access y in an unrestricted manner, while being charged for querying every symbol of x. Indeed, we obtain our main result by designing an algorithm that makes a small number of queries in this model. We then provide a nearly-matching lower bound on the number of queries. Our lower bound is the first to expose hardness of edit distance stemming from the input strings being &#x201C;repetitive&#x201D;, which means that many of their substrings are approximately identical. Consequently, our lower bound provides the first rigorous separation between edit distance and Ulam distance.
[Algorithm design and analysis, Heuristic algorithms, query complexity, Complexity theory, Approximation methods, Ulam distance, query processing, sublinear algorithms, nearly-matching lower bound, linear-time algorithms, near-linear time algorithm, edit distance stemming, approximation theory, edit distance, Computational modeling, sampling, rigorous separation, asymmetric query complexity, Upper bound, Approximation algorithms, asymmetric query model, string matching, symbol querying, polylogarithmic approximation factor, computational complexity]
Information Cost Tradeoffs for Augmented Index and Streaming Language Recognition
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
This paper makes three main contributions to the theory of communication complexity and stream computation. First, we present new bounds on the information complexity of AUGMENTED-INDEX. In contrast to analogous results for INDEX by Jain, Radhakrishnan and Sen [J. ACM, 2009], we have to overcome the significant technical challenge that protocols for AUGMENTED-INDEX may violate the "rectangle property" due to the inherent input sharing. Second, we use these bounds to resolve an open problem of Magniez, Mathieu and Nayak [STOC, 2010] on the multi-pass complexity of recognizing Dyck languages. This results in a natural separation between the standard multi-pass model and the multi-pass model that permits reverse passes. Third, we present the first passive memory checkers that verify the interaction transcripts of priority queues, stacks, and double-ended queues. We obtain tight upper and lower bounds for these problems, thereby addressing an important sub-class of the memory checking framework of Blum et al. [Algorithmica, 1994].
[augmented index, Protocols, computational linguistics, streaming language recognition, data streams, information cost tradeoffs, Educational institutions, Entropy, Complexity theory, Indexes, communication complexity, memory checking, lower bounds, rectangle property, passive memory checkers, Artificial intelligence, Dyck language recognition, Mutual information, computational complexity]
New Constructive Aspects of the Lovasz Local Lemma
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The Lova&#x0301;sz Local Lemma (LLL) is a powerful tool that gives sufficient conditions for avoiding all of a given set of "bad" events, with positive probability. A series of results have provided algorithms to efficiently construct structures whose existence is non-constructively guaranteed by the LLL, culminating in the recent breakthrough of Moser &amp; Tardos. We show that the output distribution of the Moser-Tardos algorithm well-approximates the conditional LLL-distribution - the distribution obtained by conditioning on all bad events being avoided. We show how a known bound on the probabilities of events in this distribution can be used for further probabilistic analysis and give new constructive and non-constructive results. We also show that when an LLL application provides a small amount of slack, the number of resamplings of the Moser-Tardos algorithm is nearly linear in the number of underlying independent variables (not events!), and can thus be used to give efficient constructions in cases where the underlying proof applies the LLL to super-polynomially many events. Even in cases where finding a bad event that holds is computationally hard, we show that applying the algorithm to avoid a polynomial-sized "core" subset of bad events leads to a desired outcome with high probability. We demonstrate this idea on several applications. We give the first constant-factor approximation algorithm for the Santa Claus problem by making an LLL-based proof of Feige constructive. We provide Monte Carlo algorithms for acyclic edge coloring, non-repetitive graph colorings, and Ramsey-type graphs. In all these applications the algorithm falls directly out of the non-constructive LLL-based proof. Our algorithms are very simple, often provide better bounds than previous algorithms, and are in several cases the first efficient algorithms known. As a second type of application we consider settings beyond the critical dependency threshold of the LLL: avoiding all bad events is impossible in these cases. As the first (even non-constructive) result of this kind, we show that by sampling from the LLL-distribution of a selected smaller core, we can avoid a fraction of bad events that is higher than the expectation. MAX k-SAT is an example of this.
[Algorithm design and analysis, Monte Carlo algorithm, output distribution, nonrepetitive graph coloring, probabilistic analysis, Approximation methods, graph colouring, Probabilistc Method, Ramsey type graph, Monte Carlo methods, Santa Claus Problem, Polynomials, Monte Carlo Algorithm, Moser-Tardos algorithm, polynomial sized core subset, sampling methods, polynomials, probability, Probabilistic logic, acyclic edge coloring, Computer science, MAX k-SAT, Lovasz Local Lemma, Linearity, Approximation algorithms, Santa Claus problem, computational complexity, constant factor approximation algorithm]
The Geometry of Scheduling
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We consider the following general scheduling problem: The input consists of n jobs, each with an arbitrary release time, size, and a monotone function specifying the cost incurred when the job is completed at a particular time. The objective is to find a preemptive schedule of minimum aggregate cost. This problem formulation is general enough to include many natural scheduling objectives, such as weighted flow, weighted tardiness, and sum of flow squared. The main contribution of this paper is a randomized polynomial-time algorithm with an approximation ratio O(log log n P), where P is the maximum job size. We also give an O(1) approximation in the special case when all jobs have identical release times. Initially, we show how to reduce this scheduling problem to a particular geometric set-cover problem. We then consider a natural linear programming formulation of this geometric set-cover problem, strengthened by adding knapsack cover inequalities, and show that rounding the solution of this linear program can be reduced to other particular geometric set-cover problems. We then develop algorithms for these sub-problems using the local ratio technique, and Varadarajan's quasi-uniform sampling technique. This general algorithmic approach improves the best known approximation ratios by at least an exponential factor (and much more in some cases) for essentially all of the nontrivial common special cases of this problem. We believe that this geometric interpretation of scheduling is of independent interest.
[Schedules, weighted tardiness, computational geometry, Varadarajan quasi-uniform sampling, linear programming, Complexity theory, Approximation methods, weighted flow, knapsack cover inequalities, scheduling, Polynomials, Weighted Flow Time, sum of flow squared, approximation ratio, general scheduling problem, geometric set-cover problem, local ratio technique, Linear programming, Scheduling, Geometric Set Cover, randomized polynomial-time algorithm, Processor scheduling, exponential factor, Approximation algorithms, computational complexity]
Strong Fault-Tolerance for Self-Assembly with Fuzzy Temperature
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We consider the problem of fault-tolerance in nanoscale algorithmic self-assembly. We employ a standard variant of Winfree's abstract Tile Assembly Model (aTAM), the two-handed aTAM, in which square &#x201C;tiles&#x201D; - a model of molecules constructed from DNA for the purpose of engineering self-assembled nanostructures - aggregate according to specific binding sites of varying strengths, and in which large aggregations of tiles may attach to each other, in contrast to the seeded aTAM, in which tiles aggregate one at a time to a single specially designated &#x201C;seed&#x201D; assembly. We focus on a major cause of errors in tile-based self-assembly: that of unintended growth due to &#x201C;weak&#x201D; strength-1 bonds, which if allowed to persist, may be stabilized by subsequent attachment of neighboring tiles in the sense that at least energy 2 is now required to break apart the resulting assembly, i.e., the errant assembly is stable at temperature 2. We study a common self-assembly benchmark problem, that of assembling an n&#x00D7;n square using O(log n) unique tile types, under the two-handed model of self-assembly. Our main result achieves a much stronger notion of fault-tolerance than those achieved previously. Arbitrary strength-1 growth is allowed, however, any assembly that grows sufficiently to become stable at temperature 2 is guaranteed to assemble into the correct final assembly of an n&#x00D7;n square. In other words, errors due to insufficient attachment, which is the cause of errors studied in earlier papers on fault-tolerance, are prevented absolutely in our main construction, rather than only with high probability and for sufficiently small structures, as in previous fault tolerance studies.
[fault tolerance, Radiation detectors, fuzzy set theory, Self-Assembly, Fault-Tolerance, abstract tile assembly model, Computer science, Fault tolerance, self-assembly, Self-assembly, self assembled nanostructure, Tiles, Fault tolerant systems, fuzzy temperature, fault tolerant computing, Assembly]
Holographic Algorithms with Matchgates Capture Precisely Tractable Planar_#CSP
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Valiant introduced match gate computation and holographic algorithms. A number of seemingly exponential time problems can be solved by this novel algorithmic paradigm in polynomial time. We show that, in a very strong sense, match gate computations and holographic algorithms based on them provide a universal methodology to a broad class of counting problems studied in statistical physics community for decades. They capture precisely those problems which are #P-hard on general graphs but computable in polynomial time on planar graphs. More precisely, we prove complexity dichotomy theorems in the framework of counting CSP problems. The local constraint functions take Boolean inputs, and can be arbitrary real-valued symmetric functions. We prove that, every problem in this class belongs to precisely three categories: (1) those which are tractable (i.e., polynomial time computable) on general graphs, or (2) those which are #P-hard on general graphs but ractable on planar graphs, or (3) those which are #P-hard even on planar graphs. The classification criteria are explicit. Moreover, problems in category (2) are tractable on planar graphs precisely by holographic algorithms with matchgates.
[general graphs, #P-hard, counting CSP problems, graph theory, planar graphs, holographic algorithms, exponential time problems, Complexity theory, real-valued symmetric functions, #CSP, Boolean functions, match gate computation, Polynomials, Bipartite graph, Boolean inputs, Software algorithms, constraint theory, statistical physics community, Interpolation, Tensile stress, polynomial time computable, matchgates, local constraint functions, complexity dichotomy theorems, classification criteria, computational complexity]
A Decidable Dichotomy Theorem on Directed Graph Homomorphisms with Non-negative Weights
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The complexity of graph homomorphism problems has been the subject of intense study. It is a long standing open problem to give a (decidable) complexity dichotomy theorem for the partition function of directed graph homomorphisms. In this paper, we prove a decidable complexity dichotomy theorem for this problem and our theorem applies to all non-negative weighted form of the problem: given any fixed matrix A with non-negative algebraic entries, the partition function Z<sub>A</sub>(G) of directed graph homomorphisms from any directed graph G is either tractable in polynomial time or #P-hard, depending on the matrix A. The proof of the dichotomy theorem is combinatorial, but involves the definition of an infinite family of graph homomorphism problems. The proof of its decidability is algebraic using properties of polynomials.
[Computers, Symmetric matrices, graph homomorphism, graph theory, dichotomy, nonnegative weights, Complexity theory, Indexes, directed graph homomorphisms, matrix algebra, Computer science, decidability, nonnegative algebraic entries, decidable complexity dichotomy theorem, Polynomials, Matrices, polynomial time, computational complexity]
Sublinear Optimization for Machine Learning
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We give sub linear-time approximation algorithms for some optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms can be extended to some kernelized versions of these problems, such as SVDD, hard margin SVM, and L<sub>2</sub>-SVM, for which sub linear-time algorithms were not known before. These new algorithms use a combination of a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model. We also give implementations of our algorithms in the semi-streaming setting, obtaining the first low pass polylogarithmic space and sub linear time algorithms achieving arbitrary approximation factor.
[Machine learning algorithms, Classification algorithms, SVM, Approximation methods, arbitrary approximation factor, Optimization, multiplicative update algorithm, optimisation, sublinear algorithms, polynomial approximation, optimization, learning (artificial intelligence), pattern classification, support vector machines, polylogarithmic space, Vectors, linear classifier, machine learning, classification, Support vector machines, sublinear time approximation, support vector machine, sublinear optimization, Approximation algorithms, sampling technique, computational complexity]
Estimating the Longest Increasing Sequence in Polylogarithmic Time
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Finding the length of the longest increasing subsequence (LIS) is a classic algorithmic problem. Let n denote the size of the array. Simple O(n log n) time algorithms are known that determine the LIS exactly. In this paper, we develop a randomized approximation algorithm, that for any constant &#x03B4; &gt; 0, runs in time polylogarithmic in n and estimates the length of the LIS of an array up to an additive error of &#x03B4;n. The algorithm presented in this extended abstract runs in time (log n)O(1/&#x03B4;). In the full paper, we will give an improved version of the algorithm with running time (log n)c(1/&#x03B4;)O(1/&#x03B4;) where the exponent c is independent of &#x03B4;. Previously, the best known polylogarithmic time algorithms could only achieve an additive n/2-approximation. Our techniques also yield a fast algorithm for estimating the distance to monotonicity to within a small multiplicative factor. The distance of f to monotonicity, &#x03B5;<sub>f</sub>, is equal to 1 - |LIS|/n (the fractional length of the complement of the LIS). For any &#x03B4; &gt; 0, we give an algorithm with running time O((&#x03B5;<sub>f</sub>-1 log n)O(1/&#x03B4;)) that outputs a (1 + &#x03B4;)-multiplicative approximation to &#x03B5;<sub>f</sub>. This can be improved so that the exponent is a fixed constant. The previously known polylogarithmic algorithms gave only a 2-approximation.
[randomized approximation algorithm, Additives, Protocols, polylogarithmic time algorithm, Heuristic algorithms, sequence estimation, multiplicative factor, Approximation methods, Indexes, Sublinear algorithms, randomised algorithms, running time, longest increasing subsequence, Longest Increasing Subsequence, Dynamic Programming, Approximation algorithms, algorithmic problem, Arrays, sequential estimation, Monotonicity]
Testing Properties of Sparse Images
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We initiate the study of testing properties of images that correspond to sparse 0/1-valued matrices of size n &#x00D7; n. Our study is related to but different from the study initiated by Raskhodnikova (Proceedings of RANDOM, 2003), where the images correspond to dense 0/1-valued matrices. Specifically, while distance between images in the model studied by Raskhodnikova is the fraction of entries on which the images differ taken with respect to all n2 entries, the distance measure in our model is defined by the fraction of such entries taken with respect to the actual number of 1's in the matrix. We study several natural properties: connectivity, convexity, monotonicity, and being a line. In all cases we give testing algorithms with sublinear complexity, and in some of the cases we also provide corresponding lower bounds.
[Algorithm design and analysis, image processing, sparse image, connectivity property, Shape, monotonicity property, Images, Complexity theory, Partitioning algorithms, Sparse matrices, sparse 0/1-valued matrix, Property Testing, convexity property, testing algorithm, testing property, distance measure, Sublinear Algorithms, Pixel, sparse matrices, dense 0/1-valued matrix, sublinear complexity, Testing, computational complexity]
A Unified Framework for Testing Linear-Invariant Properties
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
There has been a sequence of recent papers devoted to understanding the relation between the testability of properties of Boolean functions and the invariance of the properties with respect to transformations of the domain. Invariance with respect to F<sub>2</sub>-linear transformations is arguably the most common such symmetry for natural properties of Boolean functions on the hypercube. Hence, it is an important goal to find necessary and sufficient conditions for testability of linear-invariant properties. This is explicitly posed as an open problem in a recent survey of Sudan. We obtain the following results: 1. We show that every linear-invariant property that can be characterized by forbidding induced solutions to a (possibly infinite) set of linear equations can be tested with one-sided error. 2. We show that every linear-invariant property that can be tested with one-sided error can be characterized by forbidding induced solutions to a (possibly infinite) set of systems of linear equations. We conjecture that our result from item (1) can be extended to cover systems of linear equations. We further show that the validity of this conjecture would have the following implications: 1. It would imply that every linear-invariant property that is closed under restrictions to linear subspaces is testable with one-sided error. Such a result would unify several previous results on testing Boolean functions, such as the testability of low-degree polynomials and of Fourier dimensionality. 2. It would imply that a linear-invariant property P is testable with one-sided error if and only if P is closed under restrictions to linear subspaces, thus resolving Sudan's problem.
[linear-invariant property testing, linear equation, Boolean function, set theory, property testing, F<sub>2</sub>-linear transformation, Boolean functions, Linearity, one-sided error, linear subspace, linear invariance, Hypercubes, Polynomials, Mathematical model, linear algebra, Testing]
Optimal Testing of Reed-Muller Codes
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Abstract-We consider the problem of testing if a given function f:F<sub>2</sub>n &#x2192; F<sub>2</sub> is close to any degree d polynomial in n variables, also known as the Reed-Muller testing problem. Alon et al. [1] proposed and analyzed a natural 2d+1-query test for this problem. This test turned out to be intimately related to the Gowers norm. Alon et. al. showed that this test accepts every degree d polynomial with probability 1, while it rejects functions that are &#x03A9;(1)-far with probability &#x03A9;(1/(d2d)). We give an asymptotically optimal analysis of this test, and show that it rejects functions that are (even only) &#x03A9;(2-d)-far with &#x03A9;(1)probability (so the rejection probability is a universal constant independent of d and n). This implies a tight relationship between the (d + 1)st-Gowers norm of a function and its maximal correlation with degree d polynomials, when the correlation is close to 1. Our proof works by induction on n and yields a new analysis of even the classical Blum-Luby-Rubinfeld [2] linearity test, for the setting of functions mapping F<sub>2</sub>n to F<sub>2</sub>. The optimality follows from a tighter analysis of counterexamples to the "inverse conjecture for the Gowers norm" constructed by [3], [4]. Our result has several implications. First, it shows that the Gowers norm test is tolerant, in that it also accepts close codewords. Second, it improves the parameters of an XOR lemma for polynomials given by Viola and Wigderson [5]. Third, it implies a "query hierarchy" result for property testing of affine-invariant properties. That is, for every function q(n), it gives an affine-invariant property that is testable with O(q(n))-queries, but not with o(q(n))-queries, complementing an analogous result of [6] for graph properties.
[query hierarchy, Correlation, asymptotically optimal analysis, graph property, Gowers norm, graph theory, Complexity theory, Electronic mail, Low-degree tests, algorithm theory, Property testing, affine-invariant property, Polynomials, Sublinear-time algorithms, Testing, close codewords, polynomials, optimal testing, linearity test, probability, Reed-Muller codes, Computer science, Reed-Muller testing problem, Linearity, polynomial, function mapping, XOR lemma]
Overcoming the Hole in the Bucket: Public-Key Cryptography Resilient to Continual Memory Leakage
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
In recent years, there has been a major effort to design cryptographic schemes that remain secure even when arbitrary information about the secret key is leaked (e.g., via side-channel attacks). We explore the possibility of achieving security under \\emph{continual} leakage from the \\emph{entire} secret key by designing schemes in which the secret key is updated over time. In this model, we construct public-key encryption schemes, digital signatures, and identity-based encryption schemes that remain secure even if an attacker can leak a constant fraction of the secret memory (including the secret key) in each time period between key updates. We also consider attackers who may probe the secret memory during the updates themselves. We stress that we allow unrestricted leakage, without the assumption that ``only computation leaks information''. Prior to this work, constructions of public-key encryption schemes secure under continual leakage were not known even under this assumption.
[public-key cryptography, Identity-based encryption, public-key encryption schemes, Computational modeling, public key cryptography, identity-based encryption schemes, digital signatures, Encryption, cryptographic schemes, secret key, Resilience, continual memory leakage]
Cryptography against Continuous Memory Attacks
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We say that a cryptographic scheme is Continuous Leakage-Resilient (CLR), if it allows users to refresh their secret keys, using only fresh local randomness, such that: 1. The scheme remains functional after any number of key refreshes, although the public key never changes. Thus, the &#x201C;outside world'' is neither affected by these key refreshes, nor needs to know about their frequency. 2. The scheme remains secure even if the adversary can continuously leak arbitrary information about the current secret-key, as long as the amount of leaked information is bounded in between any two successive key refreshes. There is no bound on the total amount of information that can be leaked during the lifetime of the system. In this work, we construct a variety of practical CLR schemes, including CLR one-way relations, CLR signatures, CLR identification schemes, and CLR authenticated key agreement protocols. For each of the above, we give general constructions, and then show how to instantiate them efficiently using a well established assumption on bilinear groups, called the K-Linear assumption (for any constant K greater than or equal to 1). Our constructions are highly modular, and we develop many interesting techniques and building-blocks along the way, including: leakage-indistinguishable re-randomizable relations, homomorphic NIZKs, and leakage-of-cipher text non-malleable encryption schemes.
[Context, Leakage-Resilient Cryptography, cryptographic protocols, continuous memory attacks, public key, NIZK, Entropy, Encryption, continuous leakage resilient cryptographic scheme, CLR identification schemes, k-linear assumption, public key cryptography, Public key, CLR signatures, Syntactics, leakage-of-ciphertext nonmalleable encryption schemes, CLR authenticated key agreement protocols, Public-Key Cryptography, Signatures]
On the Insecurity of Parallel Repetition for Leakage Resilience
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
A fundamental question in leakage-resilient cryptography is: can leakage resilience always be amplified by parallel repetition? It is natural to expect that if we have a leakage-resilient primitive tolerating &#x2113; bits of leakage, we can take n copies of it to form a system tolerating n&#x2113; bits of leakage. In this paper, we show that this is not always true. We construct a public key encryption system which is secure when at most &#x2113; bits are leaked, but if we take n copies of the system and encrypt a share of the message under each using an n-out-of-n secret-sharing scheme, leaking n&#x2113; bits renders the system insecure. Our results hold either in composite order bilinear groups under a variant of the subgroup decision assumption or in prime order bilinear groups under the decisional linear assumption. We note that the n copies of our public key systems share a common reference parameter.
[leakage resilience, public key cryptography, Public key, Games, secret sharing scheme, cryptography, Entropy, Encryption, public key encryption system, insecurity, parallel repetition, Resilience]
Black-Box, Round-Efficient Secure Computation via Non-malleability Amplification
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present round-efficient protocols for secure multi-party computation with a dishonest majority that rely on black-box access to the underlying primitives. Our main contributions are as follows: &#x00B7; a O(log* n)-round protocol that relies on black-box access to dense cryptosystems, homomorphic encryption schemes, or lossy encryption schemes. This improves upon the recent O(1)log* n-round protocol of Lin, Pass and Venkitasubramaniam (STOC 2009) that relies on non-black-box access to a smaller class of primitives. &#x00B7; a O(1)-round protocol requiring in addition, black-box access to a one-way function with sub-exponential hardness, improving upon the recent work of Pass and Wee (Eurocrypt 2010). These are the first black-box constructions for secure computation with sublinear round complexity. Our constructions build on and improve upon the work of Lin and Pass (STOC 2009) on nonmalleability amplification, as well as that of Ishai et al. (STOC 2006) on black-box secure computation. In addition to the results on secure computation, we also obtain a simple construction of a 0(log* n)-round non-malleable commitment scheme based on one-way functions, improving upon the recent O(1)log* n-round protocol of Lin and Pass (STOC 2009). Our construction uses a novel transformation for handling arbitrary man-in-the-middle scheduling strategies which improves upon a previous construction of Barak (FOCS 2002).
[multiparty computation security, Protocols, Additives, man-in-the-middle scheduling strategies, secure multi-party computation, Receivers, sublinear round complexity, cryptography, Complexity theory, Encryption, round complexity, homomorphic encryption schemes, round efficient secure computation, non-malleable commitments, blackbox, nonmalleability amplification, Robustness, black-box constructions, round efficient protocols]
Adaptive Hardness and Composable Security in the Plain Model from Standard Assumptions
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We construct the first general secure computation protocols that require no trusted infrastructure other than authenticated communication, and that satisfy a meaningful notion of security that is preserved under universal composition- assuming only the existence of enhanced trapdoor permutations. The notion of security fits within a generalization of the "angelbased" framework of Prabhakaran and Sahai (STOC'04) and implies super-polynomial time simulation security. Security notions of this kind are currently known to be realizable only under strong and specific hardness assumptions. A key element in our construction is a commitment scheme that satisfies a new and strong notion of security. The notion, security against chosen-commitment-attacks (CCA security), means that security holds even if the attacker has access to a extraction oracle that gives the adversary decommitment information to commitments of the adversary's choice. This notion is stronger than concurrent non-malleability and is of independent interest. We construct CCA-secure commitments based on standard one-way functions, and with no trusted set-up. To the best of our knowledge, this provides the first construction of a natural cryptographic primitive requiring adaptive hardness from standard hardness assumptions, using no trusted set-up or public keys.
[Context, Protocols, cryptographic protocols, Computational modeling, secure multi-party computation, Receivers, adaptive hardness, cryptography, super polynomial time simulation security, Security, Data mining, computation protocols, chosen commitment attack, authenticated communication, security notion, authorisation, extraction oracle, composable security, Robustness, standard assumption, trapdoor permutation, public keys cryptography]
Bounds on Monotone Switching Networks for Directed Connectivity
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We prove that any monotone switching network solving directed connectivity on N vertices must have size N&#x03A9;(log N).
[Knowledge engineering, directed connectivity, Computational modeling, Switches, communication complexity, L, Computational complexity, switching networks, Polynomials, Labeling, monotone switching network, NL, computational complexity]
Subexponential Algorithms for Unique Games and Related Problems
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We give a subexponential time approximation algorithm for the Unique Games problem. The algorithms run in time that is exponential in an arbitrarily small polynomial of the input size, n&#x03B5;. The approximation guarantee depends on &#x03B5;, but not on the alphabet size or the number of variables. We also obtain a subexponential algorithms with improved approximations for SMALL-SET EXPANSION and MULTICUT. For MAX CUT, SPARSEST CUT, and VERTEX COVER, we give subexponential algorithms with improved approximations on some interesting subclasses of instances. Khot's Unique Games Conjecture (UGC) states that it is NP-hard to achieve approximation guarantees such as ours for the Unique Games. While our results stop short of refuting the UGC, they do suggest that Unique Games is significantly easier than NP-hard problems such as MAX 3SAT, MAX 3LIN, Label Cover and more, that are believed not to have a subexponential algorithm achieving a non-trivial approximation ratio. The main component in our algorithms is a new result on graph decomposition that may have other applications. Namely we show that for every &#x03B5; &gt; 0 and every regular n-vertex graph G, by changing at most &#x03B5; fraction of G's edges, one can break G into disjoint parts so that the stochastic adjacency matrix of the induced graph on each part has at most n&#x03B5; eigenvalues larger than 1 - &#x03B7;, where &#x03B7; depends polynomially on &#x03B5;.
[eigenvalues, subexponential time approximation algorithm, graph theory, Complexity theory, induced graph, Approximation methods, Unique Games conjecture states, eigenvalues and eigenfunctions, Constraint Satisfaction Problems, multicut, stochastic adjacency matrix, optimisation, NP-hard problems, polynomial approximation, small-set expansion, Eigenvalues, Eigenvalues and eigenfunctions, Polynomials, Graph Decompositions, vertex cover, graph decomposition, nontrivial approximation ratio, Spectral Methods, Graph theory, Approximation Algorithms, Subexponential Algorithms, polynomial, sparsest cut, Games, Unique Games, Approximation algorithms, stochastic games, max cut, computational complexity]
Dependent Randomized Rounding via Exchange Properties of Combinatorial Structures
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We consider the problem of randomly rounding a fractional solution x in an integer polytope P &#x2286; [0,1]n to a vertex X of P, so that E[X] = x. Our goal is to achieve concentration properties for linear and submodular functions of the rounded solution. Such dependent rounding techniques, with concentration bounds for linear functions, have been developed in the past for two poly topes: the assignment poly tope (that is, bipartite matchings and 6-matchings) [32], [19], [23], and more recently for the spanning tree poly tope [2]. These schemes have led to a number of new algorithmic results. In this paper we describe a new swap rounding technique which can be applied in a variety of settings including matroids and matroid intersection, while providing Chernoff-type concentration bounds for linear and submodular functions of the rounded solution. In addition to existing techniques based on negative correlation, we use a martingale argument to obtain an exponential tail estimate for monotone submodular functions. The rounding scheme explicitly exploits exchange properties of the underlying combinatorial structures, and highlights these properties as the basis for concentration bounds. Matroids and matroid intersection provide a unifying framework for several known applications [19], [23], [7], [22], [2] as well as new ones, and their generality allows a richer set of constraints to be incorporated easily. We give some illustrative examples, with a more comprehensive discussion deferred to a later version of the paper.
[Greedy algorithms, Correlation, combinatorial mathematics, monotone submodular function, Entropy, linear programming, Random processes, Electronic mail, Approximation methods, exponential tail estimate, spanning tree polytope, USA Councils, function approximation, linear function, Chernoff type concentration, integer polytope, stochastic processes, exchange property, swap rounding technique, approximation theory, matroid intersection, combinatorial structure, martingale argument, bipartite matching, matrix algebra, dependent randomized rounding]
Minimum-Cost Network Design with (Dis)economies of Scale
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Given a network, a set of demands and a cost function f(.), the min-cost network design problem is to route all demands with the objective of minimizing &#x03A3;<sub>e</sub> f(&#x2113;<sub>e</sub>), where &#x2113;<sub>e</sub> is the total traffic load under the routing. We focus on cost functions of the form f(x) = &#x03C3; + x&#x03B1; for x &gt; 0, with f(0) = 0. For &#x03B1; &#x2264; 1 f(.) is subadditive and exhibits behavior consistent with economies of scale. This problem corresponds to the well-studied Buy-at-Bulk network design problem and admits polylogarithmic approximation and hardness. In this paper, we focus on the less studied scenario of &#x03B1; &gt; 1 with a positive startup cost &#x03C3; &gt; 0. Now, the cost function f(.) is neither subadditive nor superadditive. This is motivated by minimizing network-wide energy consumption when supporting a set of traffic demands. It is commonly accepted that, for some computing and communication devices, doubling processing speed more than doubles the energy consumption. Hence, in Economics parlance, such a cost function reflects diseconomies of scale. We begin by discussing why existing routing techniques such as randomized rounding and tree-metric embedding fail to generalize directly. We then present our main contribution, which is a polylogarithmic approximation algorithm. We obtain this result by first deriving a bicriteria approximation for a related capacitated min-cost flow problem that we believe is interesting in its own right. Our approach for this problem builds upon the well-linked decomposition due to Chekuri-Khanna-Shepherd, the construction of expanders via matchings due to KhandekarRao-Vazirani, and edge-disjoint routing in well-connected graphs due to Rao-Zhou. However, we also develop new techniques that allow us to keep a handle on the total cost, which was not a concern in the aforementioned literature.
[Algorithm design and analysis, Energy consumption, network-wide energy consumption, network theory (graphs), traffic demands, approximation algorithms, Approximation methods, randomized rounding, min-cost network design problem, Bandwidth, capacitated min-cost flow problem, diseconomies of scale, cost function, Cost function, economies of scale, well-connected graphs, approximation theory, buy-at-bulk network design problem, energy-efficient networks, routing techniques, minimum-cost network design, tree-metric embedding, Routing, bicriteria approximation, hardness, edge-disjoint routing, Approximation algorithms, polylogarithmic approximation]
One Tree Suffices: A Simultaneous O(1)-Approximation for Single-Sink Buy-at-Bulk
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We study the single-sink buy-at-bulk problem with an unknown cost function. We wish to route flow from a set of demand nodes to a root node, where the cost of routing x total flow along an edge is proportional to f(x) for some concave, non-decreasing function f satisfying f(0)=0. We present a simple, fast, combinatorial algorithm that takes a set of demands and constructs a single tree T such that for all f the cost f(T) is a 47.45-approximation of the optimal cost for that f. This is within a factor of 2.33 of the best approximation ratio currently achievable when the tree can be optimized for a specific function. Trees achieving simultaneous O(1)-approximations for all concave functions were previously not known to exist regardless of computation time.
[Algorithm design and analysis, approximation ratio, approximation theory, combinatorial mathematics, buy-at-bulk, Piecewise linear approximation, trees (mathematics), nondecreasing function, concave functions, network theory (graphs), Routing, one tree suffices, single sink buy-at-bulk problem, approximation algorithms, Approximation methods, combinatorial algorithm, Bismuth, Approximation algorithms, Cost function, network design, computational complexity]
Min st-cut Oracle for Planar Graphs with Near-Linear Preprocessing Time
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
For an undirected n-vertex planar graph G with non-negative edge-weights, we consider the following type of query: given two vertices s and t in G, what is the weight of a min st-cut in G? We show how to answer such queries in constant time with O(n log5 n) preprocessing time and O(n log n) space. We use a Gomory-Hu tree to represent all the pairwise min st-cuts implicitly. Previously, no subquadratic time algorithm was known for this problem. Our oracle can be extended to report the min st-cuts in time proportional to their size. Since all-pairs min si-cut and the minimum cycle basis are dual problems in planar graphs, we also obtain an implicit representation of a minimum cycle basis in O(n log5 n) time and O(n log n) space and an explicit representation with additional O(C) time and space where G is the size of the basis. To obtain our results, we require that shortest paths be unique; this assumption can be removed deterministically with an additional O(log2 n) running-time factor.
[Algorithm design and analysis, Legged locomotion, Networks, Particle separators, Merging, trees (mathematics), near-linear preprocessing time, World Wide Web, Graph theory, Electronic mail, min st-cut oracle, undirected n-vertex planar graph, Computer science, Algorithms, nonnegative edge-weights, Gomory-Hu tree, computational complexity]
On the Computational Complexity of Coin Flipping
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Coin flipping is one of the most fundamental tasks in cryptographic protocol design. Informally, a coin flipping protocol should guarantee both (1) Completeness: an honest execution of the protocol by both parties results in a fair coin toss, and (2) Security: a cheating party cannot increase the probability of its desired outcome by any significant amount. Since its introduction by Blum, coin flipping has occupied a central place in the theory of cryptographic protocols. In this paper, we explore what are the implications of the existence of secure coin flipping protocols for complexity theory. As exposited recently by Impagliazzo, surprisingly little is known about this question. Previous work has shown that if we interpret the Security property of coin flipping protocols very strongly, namely that nothing beyond a negligible bias by cheating parties is allowed, then one-way functions must exist. However, for even a slight weakening of this security property (for example that cheating parties cannot bias the outcome by any additive constant &#x03B5; &gt; 0), the only complexity-theoretic implication that was known was that PSPACE &#x2288; BPP. We put forward a new attack to establish our main result, which shows that, informally speaking, the existence of any (weak) coin flipping protocol that prevents a cheating adversary from biasing the output by more than 1/4 - &#x03B5; implies that NP &#x2288; BPP. Furthermore, for constant-round protocols, we show that the existence of any (weak) coin flipping protocol that allows an honest party to maintain any noticeable chance of prevailing against a cheating party implies the existence of (infinitely often) one-way functions.
[Protocols, Additives, weak coin-?ipping, NP, cryptographic protocols, probability, Color, cryptographic protocol, Complexity theory, Security, one way function, Polynomials, secure coin flipping protocol, one-way functions, computational complexity]
Sequential Rationality in Cryptographic Protocols
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Much of the literature on rational cryptography focuses on analyzing the strategic properties of cryptographic protocols. However, due to the presence of computationally-bounded players and the asymptotic nature of cryptographic security, a definition of sequential rationality for this setting has thus far eluded researchers. We propose a new framework for overcoming these obstacles, and provide the first definitions of computational solution concepts that guarantee sequential rationality. We argue that natural computational variants of sub game perfection are too strong for cryptographic protocols. As an alternative, we introduce a weakening called threat-free Nash equilibrium that is more permissive but still eliminates the undesirable "empty threats'' of non-sequential solution concepts. To demonstrate the applicability of our framework, we revisit the problem of implementing a mediator for correlated equilibria (Dodis-Halevi-Rabin, Crypto'00), and propose a variant of their protocol that is sequentially rational for a non-trivial class of correlated equilibria. Our treatment provides a better understanding of the conditions under which mediators in a correlated equilibrium can be replaced by a stable protocol.
[sequential rationality, cryptographic protocols, nonsequential solution, game theory, Nash equilibrium, cryptographic protocol, History, correlated equilibria, Cryptographic protocols, Game Theory, Games, cryptographic security, stable protocol, Cryptography]
An Efficient Test for Product States with Applications to Quantum Merlin-Arthur Games
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We give a test that can distinguish efficiently between product states of n quantum systems and states which are far from product. If applied to a state |&#x03C6;) whose maximum overlap with a product state is 1- &#x03B5;, the test passes with probability 1-&#x0398;(&#x03B5;), regardless of n or the local dimensions of the individual systems. The test uses two copies of |&#x03C6;). We prove correctness of this test as a special case of a more general result regarding stability of maximum output purity of the depolarising channel. A key application of the test is to quantum Merlin-Arthur games with multiple Merlins, where we obtain several structural results that had been previously conjectured, including the fact that soundness amplification is possible and that two Merlins can simulate many Merlins: QMA(k)=QMA(2) for k &#x2265; 2. Building on a previous result of Aaronson et al, this implies that there is an efficient quantum algorithm to verify 3-SAT with constant soundness, given two unentangled proofs of O&#x0303;(&#x221A;n) qubits. Among other consequences, this result implies complexity-theoretic obstructions to finding a polynomial-time algorithm to determine separability of mixed quantum states, even up to constant error, and also to proving "weak" variants of the additivity conjecture for quantum channels. Finally, our test can also be used to construct an efficient test for determining whether a unitary operator is a tensor product, which is a generalisation of classical linearity testing.
[Protocols, SAT, computability, quantum algorithm, quantum channel, linearity testing, tensors, Complexity theory, Approximation methods, Quantum computing, quantum Merlin-Arthur, quantum state, Polynomials, polynomial-time algorithm, Testing, quantum Merlin-Arthur game, Quantum entanglement, depolarising channel, probability, game theory, maximum output purity, tensor product, product states, separability, quantum computing, quantum system, product state, complexity-theoretic obstruction, computational complexity]
Subcubic Equivalences between Path, Matrix and Triangle Problems
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We say an algorithm on n &#x00D7; n matrices with entries in [-M, M] (or n-node graphs with edge weights from [-M, M]) is truly subcubic if it runs in O(n3-&#x03B4; - poly(log M)) time for some &#x03B4; &gt; 0. We define a notion of subcubic reducibility, and show that many important problems on graphs and matrices solvable in O(n3) time are equivalent under subcubic reductions. Namely, the following weighted problems either all have truly subcubic algorithms, or none of them do: The all-pairs shortest paths problem (APSP). Detecting if a weighted graph has a triangle of negative total edge weight. Listing up to n2.99 negative triangles in an edge-weighted graph. Finding a minimum weight cycle in a graph of nonnegative edge weights. The replacement paths problem in an edge-weighted digraph. Finding the second shortest simple path between two nodes in an edge-weighted digraph. Checking whether a given matrix defines a metric. Verifying the correctness of a matrix product over the (min, +)-semiring. Therefore, if APSP cannot be solved in n3-&#x03B5; time for any &#x03B5; &gt; 0, then many other problems also need essentially cubic time. In fact we show generic equivalences between matrix products over a large class of algebraic structures used in optimization, verifying a matrix product over the same structure, and corresponding triangle detection problems over the structure. These equivalences simplify prior work on subcubic algorithms for all-pairs path problems, since it now suffices to give appropriate subcubic triangle detection algorithms. Other consequences of our work are new combinatorial approaches to Boolean matrix multiplication over the (OR, AND)semiring (abbreviated as BMM). We show that practical advances in triangle detection would imply practical BMM algorithms, among other results. Building on our techniques, we give two new BMM algorithms: a derandomization of the recent combinatorial BMM algorithm of Bansal and Williams (FOCS'09), and an improved quantum algorithm for BMM.
[replacement paths, matrix product, reductions, equivalences, triangle detection problem, quantum algorithm, edge-weighted digraph, Complexity theory, Shortest path problem, triangle detection, optimisation, APSP, optimization, subcubic algorithm, all pairs shortest paths, Matrices, Boolean matrix multiplication, Image edge detection, Size measurement, Partitioning algorithms, Boolean algebra, BMM algorithm, matrix multiplication, minimum cycle, directed graphs, subcubic reducibility, subcubic algorithms, subcubic equivalence, computational complexity, all-pairs shortest path problem]
Replacement Paths via Fast Matrix Multiplication
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Let G be a directed edge-weighted graph and let P be a shortest path from s to t in G. The replacement paths problem asks to compute, for every edge e on P, the shortest s-to-t path that avoids e. Apart from approximation algorithms and algorithms for special graph classes, the naive solution to this problem - removing each edge e on P one at a time and computing the shortest s-to-t path each time - is surprisingly the only known solution for directed weighted graphs, even when the weights are integrals. In particular, although the related shortest paths problem has benefited from fast matrix multiplication, the replacement paths problem has not, and still required cubic time. For an n-vertex graph with integral edge-lengths between -M and M, we give a randomized algorithm that uses fast matrix multiplication and is sub-cubic for appropriate values of M. We also show how to construct a distance sensitivity oracle in the same time bounds. A query (u,v,e) to this oracle requires sub-quadratic time and returns the length of the shortest u-to-v path that avoids the edge e. In fact, for any constant number of edge failures, we construct a data structure in sub-cubic time, that answer queries in sub-quadratic time. Our results also apply for avoiding vertices rather than edges.
[directed edge-weighted graph, sub-quadratic time, Frequency modulation, data structure, Data structures, Complexity theory, randomized algorithm, distance sensitivity oracle, Approximation methods, matrix algebra, randomised algorithms, Sensitivity, replacement path, Integral equations, directed graphs, approximation algorithm, Approximation algorithms, fast matrix multiplication]
All-Pairs Shortest Paths in O(n&#x0B2;) Time with High Probability
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability. This resolves a long standing open problem. The algorithm is a variant of the dynamic all-pairs shortest paths algorithm of Demetrescu and Italiano. The analysis relies on a proof that the number of locally shortest paths in such randomly weighted graphs is O(n2), in expectation and with high probability. We also present a dynamic version of the algorithm that recomputes all shortest paths after a random edge update in O(log2 n) expected time.
[Algorithm design and analysis, Heuristic algorithms, long standing open problem, probability, complete directed graph, high probability, Probabilistic logic, Data structures, Harmonic analysis, dynamic all-pairs shortest paths algorithm, random graphs, edge weights, Upper bound, running time, directed graphs, shortest paths, Random variables, computational complexity, graph algorithms]
Approximating Maximum Weight Matching in Near-Linear Time
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Given a weighted graph, the maximum weight matching problem (MWM) is to find a set of vertex-disjoint edges with maximum weight. In the 1960s Edmonds showed that MWMs can be found in polynomial time. At present the fastest MWM algorithm, due to Gabow and Tarjan, runs in O&#x0303;(m&#x221A;n) time, where m and n are the number of edges and vertices in the graph. Surprisingly, restricted versions of the problem, such as computing (1 - &#x03F5;)-approximate MWMs or finding maximum cardinality matchings, are not known to be much easier (on sparse graphs). The best algorithms for these problems also run in O&#x0303;(m&#x221A;n) time. In this paper we present the first near-linear time algorithm for computing (1 - e)-approximate MWMs. Specifically, given an arbitrary real-weighted graph and &#x03F5; &gt; 0, our algorithm computes such a matching in O(m&#x03F5;-2 log3 n) time. The previous best approximate MWM algorithm with comparable running time could only guarantee a (2/3 - &#x03F5;)-approximate solution. In addition, we present a faster algorithm, running in O(m log n log &#x03F5;-1) time, that computes a (3/4 - &#x03F5;)-approximate MWM.
[approximation theory, MWM algorithm, approximation, arbitrary real-weighted graph, maximum cardinality matchings, graph theory, Data structures, Approximation methods, graph, matching, maximum weight matching problem, Clustering algorithms, near-linear time algorithm, Approximation algorithms, Nickel, Polynomials, vertex-disjoint edges, polynomial time, Bipartite graph, computational complexity]
A Fourier-Analytic Approach to Reed-Muller Decoding
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We present a Fourier-analytic approach to list-decoding Reed-Muller codes over arbitrary finite fields. We use this to show that quadratic forms over any field are locally list-decodeable up to their minimum distance. The analogous statement for linear polynomials was proved in the celebrated works of Goldreich-Levin and Goldreich-Rubinfeld-Sudan. Previously, tight bounds for quadratic polynomials were known only for q = 2 or 3; the best bound known for other fields was the Johnson radius. Departing from previous work on Reed-Muller decoding which relies on some form of self- corrector, our work applies ideas from Fourier analysis of Boolean functions to low-degree polynomials over finite fields, in conjunction with results about the weight- distribution. We believe that the techniques used here could find other applications, we present some applications to testing and learning.
[Algorithm design and analysis, error correction codes, Reed Muller decoding, polynomials, Noise, Goldreich Rubinfeld Sudan work, weight distribution, Fourier analysis, Reed-Muller codes, Boolean function, Decoding, quadratic polynomial, decoding, linear polynomial, Computer science, error-correcting codes, Boolean functions, Johnson radius, self corrector form, Polynomials, Error correction codes, Error correction, Goldreich Levin work]
Pseudorandom Generators for CC0[p] and the Fourier Spectrum of Low-Degree Polynomials over Finite Fields
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
In this paper we give the first construction of a pseudorandom generator, with seed length O(log n), for CC0[p], the class of constant-depth circuits with unbounded fan-in MOD<sub>p</sub> gates, for some prime p. More accurately, the seed length of our generator is O(log n) for any constant error &#x03F5; &gt; 0. In fact, we obtain our generator by fooling distributions generated by low degree polynomials, over F<sub>p</sub>, when evaluated on the Boolean cube. This result significantly extends previous constructions that either required a long seed or that could only fool the distribution generated by linear functions over F<sub>p</sub>, when evaluated on the Boolean cube. Enroute of constructing our PRG, we prove two structural results for low degree polynomials over finite fields that can be of independent interest. 1) Let f be an n-variate degree d polynomial over F<sub>p</sub>. Then, for every &#x03F5; &gt; 0 there exists a subset S &#x2282; [n], whose size depends only on d and &#x03F5;, such that &#x03A3;<sub>&#x03B1;&#x2208;F</sub><sub>p</sub>n<sub>:&#x03B1;&#x2260;0,&#x03B1;</sub><sub>S</sub><sub>=0</sub> |f&#x0302;(&#x03B1;)|2 &#x2264; &#x03F5;. Namely, there is a constant size subset S such that the total weight of the nonzero Fourier coefficients that do not involve any variable from S is small. 2) Let f be an n-variate degree d polynomial over F<sub>p</sub>. If the distribution of f when applied to uniform zero-one bits is &#x03F5;-far (in statistical distance) from its distribution when applied to biased bits, then for every &#x03B4; &gt; 0, f can be approximated over zero-one bits, up to error &#x03B4;, by a function of a small number (depending only on &#x03F5;, &#x03B4; and d) of lower degree polynomials.
[circuit complexity, Correlation, pseudorandom generator, polynomials, pseudorandom generators, Fourier analysis, Generators, Vectors, Electronic mail, Boolean cube, random number generation, Galois fields, Fourier coefficients, Computer science, finite fields, fourier spectrum, Logic gates, Fourier spectrum, constant depth circuits, Polynomials, linear functions, low degree polynomials]
Matching Vector Codes
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
A locally decodable code encodes a message by a codeword, such that even if the codeword is corrupted by noise, each message bit can be recovered with high probability by a randomized decoding procedure that reads only few bits of the codeword. Recently a new class of locally decodable codes, based on families of vectors with restricted dot products has been discovered. We refer to those codes as Matching Vector (MV) codes. In this work we develop a new view of MV codes and uncover certain similarities between them and classical Reed Muller codes. Our view allows us to obtain a deeper insight into the power and limitations of MV codes. We use it to construct codes that can tolerate more errors or are shorter than previously known codes for certain parameter settings. We also show super-linear lower bounds on the codeword length of any MV code.
[locally decodable codes, codeword, Reed-Muller codes, Encoding, Vectors, Decoding, Complexity theory, encoding, matching vectors, Zinc, Interpolation, matching vector codes, Reed Muller codes, decodable code, Polynomials]
Local List Decoding with a Constant Number of Queries
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Efremenko showed locally-decodable codes of subexponential length that can handle close to 1/6 fraction of errors. In this paper we show that the same codes can be locally unique-decoded from error rate 1/2 - &#x03B1; for any &#x03B1; &gt; 0 and locally list-decoded from error rate 1 - &#x03B1; for any &#x03B1; &gt; 0, with only a constant number of queries and a constant alphabet size. This gives the first sub-exponential length codes that can be locally list-decoded with a constant number of queries.
[codes, locally decodable codes, Error analysis, local list decoding, Probabilistic logic, Decoding, alphabet, Computer science, List decodable codes, Linear code, Locally decodable codes, constant query number, Binary codes, Polynomials]
Codes for Computationally Simple Channels: Explicit Constructions with Optimal Rate
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
In this paper, we consider coding schemes for computationally bounded channels, which can introduce an arbitrary set of errors as long as (a) the fraction of errors is bounded with high probability by a parameter p and (b) the process which adds the errors can be described by a sufficiently "simple" circuit. Codes for such channel models are attractive since, like codes for standard adversarial errors, they can handle channels whose true behavior is unknown or varying over time. For three classes of channels, we provide explicit, efficiently encodable/decodable codes of optimal rate where only inefficiently decodable codes were previously known. In each case, we provide one encoder/decoder that works for every channel in the class. Unique decoding for additive errors: We give the first construction of a poly-time encodable/decodable code for additive (a.k.a. oblivious) channels that achieve the Shannon capacity 1-H(p). List-decoding for online log-space channels: A space-S(N) bounded channel reads and modifies the transmitted codeword as a stream, using at most S(N) bits of workspace on transmissions of N bits. For constant S, this captures many models from the literature, including "discrete channels with finite memory" and "arbitrarily varying channels". We give an efficient code with optimal rate (arbitrarily close to 1-H(p)) that recovers a short list containing the correct message with high probability for channels which read and modify the transmitted codeword as a stream, using at most O(\\log N) bits of workspace on transmissions of N bits. List-decoding for poly-time channels: For any constant c we give a similar list-decoding result for channels describable by circuits of size at most Nc, assuming the existence of pseudorandom generators.
[Additives, channel coding, Stochastic processes, explicit constructions, coding scheme, communication complexity, computationally bounded channel, online logspace channel, coding theory, unique decoding, list decoding, Polynomials, pseudorandomness, Shannon capacity, information theory, message passing, pseudorandom generator, error correction codes, adversarial errors, computationally bounded channels, Decoding, Channel coding, decoding, polytime encodable code, Automatic voltage control]
Pure and Bayes-Nash Price of Anarchy for Generalized Second Price Auction
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The Generalized Second Price Auction has been the main mechanism used by search companies to auction positions for advertisements on search pages. In this paper we study the social welfare of the Nash equilibria of this game in various models. In the full information setting, socially optimal Nash equilibria are known to exist (i.e., the Price of Stability is 1). This paper is the first to prove bounds on the price of anarchy, and to give any bounds in the Bayesian setting. Our main result is to show that the price of anarchy is small assuming that all bidders play un-dominated strategies. In the full information setting we prove a bound of 1.618 for the price of anarchy for pure Nash equilibria, and a bound of 4 for mixed Nash equilibria. We also prove a bound of 8 for the price of anarchy in the Bayesian setting, when valuations are drawn independently, and the valuation is known only to the bidder and only the distributions used are common knowledge. Our proof exhibits a combinatorial structure of Nash equilibria and uses this structure to bound the price of anarchy. While establishing the structure is simple in the case of pure and mixed Nash equilibria, the extension to the Bayesian setting requires the use of novel combinatorial techniques that can be of independent interest.
[Context, Bayesian setting, combinatorial mathematics, GSP, Companies, game theory, Nash equilibrium, combinatorial structure, commerce, Cost accounting, price of anarchy, Sponsored Search Auction, social welfare, Bayesian methods, generalized second price auction, combinatorial technique, Nash equilibria, Games, Random variables, Bayes methods, pricing]
Frugal and Truthful Auctions for Vertex Covers, Flows and Cuts
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We study truthful mechanisms for hiring a team of agents in three classes of set systems: Vertex Cover auctions, How auctions, and cut auctions. For Vertex Cover auctions, the vertices are owned by selfish and rational agents, and the auctioneer wants to purchase a vertex cover from them. For k-flow auctions, the edges are owned by the agents, and the auctioneer wants to purchase k edge-disjoint s-t paths, for given s and t. In the same setting, for cut auctions, the auctioneer wants to purchase an s-t cut. Only the agents know their costs, and the auctioneer needs to select a feasible set and payments based on bids made by the agents. We present constant-competitive truthful mechanisms for all three set systems. That is, the maximum overpayment of the mechanism is within a constant factor of the maximum overpayment of any truthful mechanism, for every set system in the class. The mechanism for Vertex Cover is based on scaling each bid by a multiplier derived from the dominant eigenvector of a certain matrix. The mechanism for k-flows prunes the graph to be minimally (k + 1)-connected, and then applies the Vertex Cover mechanism. Similarly, the mechanism for cuts contracts the graph until all s-t paths have length exactly 2, and then applies the Vertex Cover mechanism.
[spectral, multi-agent systems, k-flow auctions, constant-competitive truthful mechanisms, Flows, Nash equilibrium, mechanism design, set theory, truthful auctions, History, commerce, frugal auctions, Vertex Covers, eigenvalues and eigenfunctions, Cuts, frugality, Linearity, Polynomials, Eigenvalues and eigenfunctions, mechanism maximum overpayment, Monopoly, vertex cover auctions, dominant eigenvector, Monitoring]
Frugal Mechanism Design via Spectral Techniques
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We study the design of truthful mechanisms for set systems, i.e., scenarios where a customer needs to hire a team of agents to perform a complex task. In this setting, frugality [2] provides a measure to evaluate the "cost of truthfulness\
[fair payment, general set systems, multi-agent systems, frugal mechanism design, Nash equilibrium, r-out-of-k-system mechanism, single path systems, set theory, Electronic commerce, k-path systems, Young's inequality, respective interdependency matrix, frugality, overpayment, Benchmark testing, Eigenvalues and eigenfunctions, spectral techniques, &#x221A;-mechanism, edge-disjoint source-sink paths, Mechanical factors, Stability analysis, Equations, matrix algebra, matrix eigenvector, local sparsity condition, financial management, vertex cover systems, frugal truthful mechanisms]
Budget Feasible Mechanisms
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We study a novel class of mechanism design problems in which the outcomes are constrained by the payments. This basic class of mechanism design problems captures many common economic situations, and yet it has not been studied, to our knowledge, in the past. We focus on the case of procurement auctions in which sellers have private costs, and the auctioneer aims to maximize a utility function on subsets of items, under the constraint that the sum of the payments provided by the mechanism does not exceed a given budget. Standard mechanism design ideas such as the VCG mechanism and its variants are not applicable here. We show that, for general functions, the budget constraint can render mechanisms arbitrarily bad in terms of the utility of the buyer. However, our main result shows that for the important class of sub modular functions, a bounded approximation ratio is achievable. Better approximation results are obtained for subclasses of the sub modular functions. We explore the space of budget feasible mechanisms in other domains and give a characterization under more restricted conditions.
[Economics, Procurement, Frequency modulation, budget constraint, algorithmic game theory, mechanism design, Approximation methods, budgeting, Sorting, VCG mechanism, algorithmic mechanism design, submodular maximization, Space exploration, Resource management, procurement auction, bounded approximation ratio, budget]
Black-Box Randomized Reductions in Algorithmic Mechanism Design
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We give the first black-box reduction from arbitrary approximation algorithms to truthful approximation mechanisms for a non-trivial class of multi-parameter problems. Specifically, we prove that every packing problem that admits an FPTAS also admits a truthful-in-expectation randomized mechanism that is an FPTAS. Our reduction makes novel use of smoothed analysis, by employing small perturbations as a tool in algorithmic mechanism design. We develop a &#x201C;duality'' between linear perturbations of the objective function of an optimization problem and of its feasible set, and use the &#x201C;primal'' and &#x201C;dual'' viewpoints to prove the running time bound and the truthfulness guarantee, respectively, for our mechanism.
[Algorithm design and analysis, approximation theory, optimization problem, packing problem, polynomials, smoothed analysis, Smoothed Analysis, Truthful Approximation Algorithms, Approximation methods, Optimization, arbitrary approximation algorithms, multiparameter problems, Computer science, linear perturbations, black box randomized reductions, optimisation, algorithmic mechanism design, Approximation algorithms, Polynomials, Resource management, Mechanism Design, objective function]
Backyard Cuckoo Hashing: Constant Worst-Case Operations with a Succinct Representation
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
The performance of a dynamic dictionary is measured mainly by its update time, lookup time, and space consumption. In terms of update time and lookup time there are known constructions that guarantee constant-time operations in the worst case with high probability, and in terms of space consumption there are known constructions that use essentially optimal space. However, although the first analysis of a dynamic dictionary dates back more than 45 years ago (when Knuth analyzed linear probing in 1963), the trade-off between these aspects of performance is still not completely understood. In this paper we settle two fundamental open problems: &#x00B7; We construct the first dynamic dictionary that enjoys the best of both worlds: it stores n elements using (1 + &#x03F5;)n memory words, and guarantees constant-time operations in the worst case with high probability. Specifically, for any &#x03F5; = &#x03A9;((log log n/log n)1/2) and for any sequence of polynomially many operations, with high probability over the randomness of the initialization phase, all operations are performed in constant time which is independent of e. The construction is a two-level variant of cuckoo hashing, augmented with a "backyard" that handles a large fraction of the elements, together with a de-amortized perfect hashing scheme for eliminating the dependency on e. &#x00B7; We present a variant of the above construction that uses only (1 + o(1))B bits, where B is the information-theoretic lower bound for representing a set of size n taken from a universe of size u, and guarantees constant-time operations in the worst case with high probability, as before. This problem was open even in the amortized setting. One of the main ingredients of our construction is a permutation-based variant of cuckoo hashing, which significantly improves the space consumption of cuckoo hashing when dealing with a rather small universe.
[constant worst-case operation, Dictionaries, Redundancy, dynamic dictionary, Random access memory, Data structures, backyard cuckoo hashing, Memory management, permutation-based variant, Polynomials, data structures, Timing, succinct representation, computational complexity]
A Lower Bound for Dynamic Approximate Membership Data Structures
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
An approximate membership data structure is a randomized data structure for representing a set which supports membership queries. It allows for a small false positive error rate but has no false negative errors. Such data structures were first introduced by Bloom in the 1970's, and have since had numerous applications, mainly in distributed systems, database systems, and networks. The algorithm of Bloom is quite effective: it can store a set S of size n by using only &#x2248;1.44nlog<sub>2</sub>(1/&#x03B5;) bits while having false positive error &#x03B5;. This is within a constant factor of the entropy lower bound of nlog<sub>2</sub>(1/&#x03B5;) for storing such sets. Closing this gap is an important open problem, as Bloom filters are widely used is situations were storage is at a premium. Bloom filters have another property: they are dynamic. That is, they support the iterative insertions of up to n elements. In fact, if one removes this requirement, there exist static data structures which receive the entire set at once and can almost achieve the entropy lower bound; they require only nlog<sub>2</sub>(1/&#x03B5;)(1 + o(1)) bits. Our main result is a new lower bound for the memory requirements of any dynamic approximate membership data structure. We show that for any constant &#x03B5; &gt; 0, any such data structure which achieves false positive error rate of &#x03B5; must use at least C(&#x03B5;) &#x00B7; nlog<sub>2</sub>(1/&#x03B5;) memory bits, where C(&#x03B5;) &gt; 1 depends only on &#x03B5;. This shows that the entropy lower bound cannot be achieved by dynamic data structures for any constant error rate. In fact, our lower bound holds even in the setting where the insertion and query algorithms may use shared randomness, and where they are only required to perform well on average.
[iterative methods, Error analysis, Heuristic algorithms, dynamic data structure, Bloom algorithm, distributed system, Entropy, iterative insertion, set theory, query processing, Dynamic data structures, entropy, data structures, approximation theory, membership query, entropy lower bound, Data structures, Bloom filters, query algorithm, Computer science, Lower bounds, database system, Memory management, Approximation algorithms, approximate membership]
Lower Bounds on Near Neighbor Search via Metric Expansion
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
In this paper we show how the complexity of performing nearest neighbor (NNS) search on a metric space is related to the expansion of the metric space. Given a metric space we look at the graph obtained by connecting every pair of points within a certain distance r. We then look at various notions of expansion in this graph relating them to the cell probe complexity of NNS for randomized and deterministic, exact and approximate algorithms. For example if the graph has node expansion &#x03A6; then we show that any deterministic i-probe data structure for n points must use space S where (St/n)t &gt; &#x03A6;. We show similar results for randomized algorithms as well. These relationships can be used to derive most of the known lower bounds in the well known metric spaces such as l<sub>1</sub>, l<sub>2</sub>, l<sub>&#x221E;</sub>, and some new ones, by simply computing their expansion. In the process, we strengthen and generalize our previous results. Additionally, we unify the approach in and the communication complexity based approach. Our work reduces the problem of proving cell probe lower bounds of near neighbor search to computing the appropriate expansion parameter. In our results, as in all previous results, the dependence on t is weak; that is, the bound drops exponentially in t. We show a much stronger (tight) time-space tradeoff for the class of dynamic low contention data structures. These are data structures that supports updates in the data set and that do not look up any single cell too often. A full version of the paper could be found in.
[Measurement, approximation theory, graph theory, Data Structures, Metric Spaces, Artificial neural networks, random processes, Data structures, Complexity theory, Approximation methods, communication complexity, lower bounds, graph, approximate algorithms, nearest neighbor search, randomized algorithms, dynamic low contention data structures, Robustness, data structures, Probes, Expansion, search problems, metric expansion, computational complexity]
Distance Oracles beyond the Thorup-Zwick Bound
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
We give the first improvement to the space/approximation trade-off of distance oracles since the seminal result of Thorup and Zwick [STOC'01]. For unweighted graphs, our distance oracle has size O(n5/3) = O(n1.66&#x22EF;) and, when queried about vertices at distance d, returns a path of length 2d + 1. For weighted graphs with m = n2/&#x03B1; edges, our distance oracle has size O(n2/3&#x221A;&#x03B1;) and returns a factor 2 approximation. Based on a plausible conjecture about the hardness of set intersection queries, we show that a 2-approximate distance oracle requires space &#x03A9;&#x0303;(n2/&#x221A;&#x03B1;). For unweighted graphs, this implies a &#x03A9;&#x0303;(n1.5) space lower bound to achieve approximation 2d + 1.
[Algorithm design and analysis, approximation theory, Additives, graph theory, vertices, plausible conjecture, Artificial neural networks, computational geometry, Data structures, Distance oracles, Approximation methods, Thorup-Zwick Bound, set intersection queries, Upper bound, space-approximation, distance oracles, Graphs, Silicon, unweighted graphs]
[Roster]
2010 IEEE 51st Annual Symposium on Foundations of Computer Science
None
2010
Provides a listing of current committee members and society officers.
[]
Foreword
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Presents the introductory welcome message from the conference proceedings.
[]
Organizing Committee
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Provides a listing of current committee members.
[]
Program Committee
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Provides a listing of current committee members and society officers.
[]
The Promise of Differential Privacy: A Tutorial on Algorithmic Techniques
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Differential privacy describes a promise, made by a data curator to a data subject: you will not be affected, adversely or otherwise, by allowing your data to be used in any study, no matter what other studies, data sets, or information from other sources is available. At their best, differentially private database mechanisms can make confidential data widely available for accurate data analysis, without resorting to data clean rooms, institutional review boards, data usage agreements, restricted views, or data protection plans. To enjoy the fruits of the research described in this tutorial, the data analyst must accept that raw data can never be accessed directly and that eventually data utility is consumed: overly accurate answers to too many questions will destroy privacy. The goal of algorithmic research on differential privacy is to postpone this inevitability as long as possible.
[confidential data analysis, Data privacy, Data analysis, data analysis, Tutorials, privacy, Loss measurement, database management systems, Privacy, algorithmic technique, Databases, data sets, data privacy, data utility, private data analysis, Cryptography, differential private database mechanism, differential privacy]
Green Computing Algorithmics
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The converging trends of society's desire/need for more sustainable technologies, exponentially increasing power densities within computing devices, and exponentially more computing devices, have inevitably pushed power and energy management into the forefront of computing design and management for purely economic reasons. Thus we are in the midst of a green computing revolution involving the redesign of information technology hardware and software at all levels of the information technology stack. This revolution has spawned a multitude of technological challenges, many of which are algorithmic in nature. We provide pointers into the literature on the green computing algorithmics.
[Algorithm design and analysis, Computers, Google, Software algorithms, environmental factors, sustainable technologies, Computer science, sustainable development, Green products, energy conservation, power management, energy management, information technology stack, Software, green computing algorithmics]
Computing Blindfolded: New Developments in Fully Homomorphic Encryption
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
A fully homomorphic encryption scheme enables computation of arbitrary functions on encrypted data. Fully homomorphic encryption has long been regarded as cryptography's prized "holy grail" - extremely useful yet rather elusive. Starting with the groundbreaking work of Gentry in 2009, the last three years have witnessed numerous constructions of fully homomorphic encryption involving novel mathematical techniques, and a number of exciting applications. We will take the reader through a journey of these developments and provide a glimpse of the exciting research directions that lie ahead.
[Privacy, mathematical techniques, computing blindfolded, Databases, fully homomorphic encryption, data encryption, Public key, cryptography, Polynomials, arbitrary functions, Encryption, mathematical analysis]
Min-max Graph Partitioning and Small Set Expansion
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We study graph partitioning problems from a min-max perspective, in which an input graph on n vertices should be partitioned into k parts, and the objective is to minimize the maximum number of edges leaving a single part. The two main versions we consider are: (i) the k parts need to be of equal size, and (ii) the parts must separate a set of k given terminals. We consider a common generalization of these two problems, and design for it an O(&#x221A;log n log k)-approximation algorithm. This improves over an O(log2 n) approximation for the second version due to Svitkina and Tardos, and roughly O(k log n) approximation for the first version that follows from other previous work. We also give an improved O(1)-approximation algorithm for graphs that exclude any fixed minor. Our algorithm uses a new procedure for solving the Small Set Expansion problem. In this problem, we are given a graph G and the goal is to find a non-empty subset S of V of size at most pn with minimum edge-expansion. We give an O(&#x221A;log n log (1/p)) bicriteria approximation algorithm for the general case of Small Set Expansion and O(1) approximation algorithm for graphs that exclude any fixed minor.
[Algorithm design and analysis, approximation theory, min-max graph partitioning, Particle separators, graph theory, O(log2 n) approximation, Optimized production technology, Vectors, Partitioning algorithms, set theory, Approximation methods, minimax techniques, O(&#x221A;log n log (1/p)) bicriteria approximation algorithm, O(k log n) approximation, O(1)-approximation algorithm, small set expansion problem, O(&#x221A;log n log k)-approximation algorithm, Approximation algorithms, computational complexity]
Separator Theorems for Minor-Free and Shallow Minor-Free Graphs with Applications
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Alon, Seymour, and Thomas generalized Lipton and Tarjan's planar separator theorem and showed that a K<sub>h</sub>- minor free graph with n vertices has a separator of size at most h3/2 &#x221A;n. They gave an algorithm that, given a graph G with m edges and n vertices and given an integer h &#x2265; 1, outputs in O(&#x221A;hnm) time such a separator or a Ku-minor of G. Plotkin, Rao, and Smith gave an O(hm&#x221A;/n log n) time algorithm to find a separator of size O(h&#x221A;n log n). Kawarabayashi and Reed improved the bound on the size of the separator to h&#x221A;n and gave an algorithm that finds such a separator in O(n1+&#x03F5;) time for any constant &#x03F5; &gt;; 0, assuming h is constant. This algorithm has an extremely large dependency on h in the running time (some power tower of h whose height is itself a function of h), making it impractical even for small h. We are interested in a small polynomial time dependency on h and we show how to find an O (h&#x221A;n log n)-size separator or report that G has a Ku-minor in O(poly(h)n5/4+&#x03F5;) time for any constant &#x03F5; &gt;; 0. We also present the first O(poly(h)n) time algorithm to find a separator of size 0(nc) for a constant &#x03F5; &lt;; 1. As corollaries of our results, we get improved algorithms for shortest paths and maximum matching. Furthermore, for integers &#x2113; and h, we give an O(m ,2 + &#x03F5; /&#x2113;) time algorithm that either produces a K<sub>h</sub>-minor of depth O(&#x2113; log n) or a separator of size at most O(n/&#x2113; + &#x2113;h2log n). This improves the shallow minor algorithm of Plotkin, Rao, and Smith when m = &#x03A9;(n1+&#x03F5;). We get a similar running time improvement for an approximation algorithm for the problem of finding a largest K<sub>h</sub> -minor in a given graph.
[Algorithm design and analysis, separator theorems, Particle separators, Heuristic algorithms, polynomials, graph theory, time algorithm, Partitioning algorithms, Approximation methods, separator, shortest path, polynomial time dependency, Clustering algorithms, shallow minor-free graph, approximation algorithm, Approximation algorithms, minor-free graph, maximum matching, Ku-minor, computational complexity, shallow minor free graphs]
A Constant Factor Approximation Algorithm for Unsplittable Flow on Paths
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
In this paper, we present a constant-factor approximation algorithm for the unsplittable flow problem on a path. This improves on the previous best known approximation factor of O(log n). The approximation ratio of our algorithm is 7+e for any e&gt;;0. In the unsplittable flow problem on a path, we are given a capacitated path P and n tasks, each task having a demand, a profit, and start and end vertices. The goal is to compute a maximum profit set of tasks, such that for each edge e of P, the total demand of selected tasks that use e does not exceed the capacity of e. This is a well-studied problem that occurs naturally in various settings, and therefore it has been studied under alternative names, such as resource allocation, bandwidth allocation, resource constrained scheduling, temporal knapsack and interval packing. Polynomial time constant factor approximation algorithms for the problem were previously known only under the no-bottleneck assumption (in which the maximum task demand must be no greater than the minimum edge capacity). We introduce several novel algorithmic techniques, which might be of independent interest: a framework which reduces the problem to instances with a bounded range of capacities, and a new geometrically inspired dynamic program which solves a special case of the maximum weight independent set of rectangles problem to optimality. In addition, we show that the problem is strongly NP-hard even if all edge capacities are equal and all demands are either 1, 2, or 3.
[Algorithm design and analysis, approximation theory, unsplittable flow problem, NP-hard, unsplittable flow, Heuristic algorithms, dynamic programming, Partitioning algorithms, strong NP-hardness, Approximation methods, constant factor approximation, maximum weight independent set, resource allocation, Approximation algorithms, Polynomials, Resource management, O(log n), computational complexity, constant factor approximation algorithm]
How Bad is Forming Your Own Opinion?
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
A long-standing line of work in economic theory has studied models by which a group of people in a social network, each holding a numerical opinion, can arrive at a shared opinion through repeated averaging with their neighbors in the network. Motivated by the observation that consensus is rarely reached in real opinion dynamics, we study a related sociological model in which individuals' intrinsic beliefs counterbalance the averaging process and yield a diversity of opinions. By interpreting the repeated averaging as best-response dynamics in an underlying game with natural payoffs, and the limit of the process as an equilibrium, we are able to study the cost of disagreement in these models relative to a social optimum. We provide a tight bound on the cost at equilibrium relative to the optimum, our analysis draws a connection between these agreement models and extremal problems for generalized eigenvalues. We also consider a natural network design problem in this setting, where adding links to the underlying network can reduce the cost of disagreement at equilibrium.
[Laplace equations, economic theory, DeGroot Model, Social network services, Nash equilibrium, Vectors, Algorithmic Game Theory, sociological model, social network, Social Networks, Games, network design problem, Cost function, social networking (online), Eigenvalues and eigenfunctions]
The Complexity of the Homotopy Method, Equilibrium Selection, and Lemke-Howson Solutions
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We show that the widely used homotopy method for solving fix point problems, as well as the Harsanyi-Selten equilibrium selection process for games, are PSPACE-complete to implement. Extending our result for the Harsanyi-Selten process, we show that several other homotopy-based algorithms for finding equilibria of games are also PSPACE-complete to implement. A further application of our techniques yields the result that it is PSPACE-complete to compute any of the equilibria that could be found via the classical Lemke-How son algorithm, a complexity-theoretic strengthening of the result in [24]. These results show that our techniques can be widely applied and suggest that the PSPACE-completeness of implementing homotopy methods is a general principle.
[Color, game theory, Search problems, Nash equilibrium, PSPACE-complete, Complexity theory, equilibrium selection, Lemke-Howson solutions, Harsanyi-Selten process, Games, Logic gates, Polynomials, fix point problems, homotopy method complexity, computational complexity]
Welfare and Profit Maximization with Production Costs
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Combinatorial Auctions are a central problem in Algorithmic Mechanism Design: pricing and allocating goods to buyers with complex preferences in order to maximize some desired objective (e.g., social welfare, revenue, or profit). The problem has been well-studied in the case of limited supply (one copy of each item), and in the case of digital goods (the seller can produce additional copies at no cost). Yet in the case of resources -- oil, labor, computing cycles, etc. -- neither of these abstractions is just right: additional supplies of these resources can be found, but at increasing difficulty (marginal cost) as resources are depleted. In this work, we initiate the study of the algorithmic mechanism design problem of combinatorial pricing under increasing marginal cost. The goal is to sell these goods to buyers with unknown and arbitrary combinatorial valuation functions to maximize either the social welfare, or the seller's profit, specifically we focus on the setting of posted item prices with buyers arriving online. We give algorithms that achieve constant factor approximations for a class of natural cost functions - linear, low-degree polynomial, logarithmic - and that give logarithmic approximations for more general increasing marginal cost functions (along with a necessary additive loss). We show that these bounds are essentially best possible for these settings.
[Algorithm design and analysis, profit maximization, combinatorial pricing, combinatorial auctions, combinatorial valuation functions, Approximation methods, commerce, welfare maximization, Cost accounting, algorithmic mechanism design, production costs, social welfare, Production, Pricing, Approximation algorithms, Cost function, pricing]
Mechanism Design with Set-Theoretic Beliefs
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
In settings of incomplete information, we put forward (1) a very conservative -- indeed, purely set-theoretic -- model of the beliefs (including totally wrong ones) that each player may have about the payoff types of his opponents, and (2) a new and robust solution concept, based on mutual belief of rationality, capable of leveraging such conservative beliefs. We exemplify the applicability of our new approach for single-good auctions, by showing that, under our solution concept, a normal-form, simple, and deterministic mechanism guarantees -- up to an arbitrarily small, additive constant -- a revenue benchmark that is always greater than or equal to the second-highest valuation, and sometimes much greater. By contrast, we also prove that the same benchmark cannot even be approximated within any positive factor, under classical solution concepts.
[Context, incomplete information, robust solution concept, single-good auctions, game theory, deterministic mechanism, mechanism design, set theory, beliefs, Cost accounting, set theoretic beliefs, revenue, Games, Bismuth, Benchmark testing, Robustness, Silicon]
Efficient Fully Homomorphic Encryption from (Standard) LWE
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We present a fully homomorphic encryption scheme that is based solely on the (standard) learning with errors (LWE) assumption. Applying known results on LWE, the security of our scheme is based on the worst-case hardness of "short vector problems" on arbitrary lattices. Our construction improves on previous works in two aspects: 1) We show that "somewhat homomorphic" encryption can be based on LWE, using a new re-linearization technique. In contrast, all previous schemes relied on complexity assumptions related to ideals in various rings. 2) We deviate from the "squashing paradigm" used in all previous works. We introduce a new dimension-modulus reduction technique, which shortens the ciphertexts and reduces the decryption complexity of our scheme, without introducing additional assumptions. Our scheme has very short ciphertexts and we therefore use it to construct an asymptotically efficient LWE-based single-server private information retrieval (PIR) protocol. The communication complexity of our protocol (in the public-key model) is k &#x00B7; polylog(k) + log |DB| bits per single-bit query (here, A; is a security parameter).
[Protocols, ciphertext, Learning with Errors, cryptographic protocols, public key model, Lattices, information retrieval, learning with error assumption, dimension modulus reduction technique, somewhat homomorphic encryption, Encryption, Complexity theory, communication complexity, Fully Homomorphic Encryption, decryption complexity, Databases, public key cryptography, worst case hardness, LWE based single server private information retrieval protocol, data privacy, fully homomorphic encryption scheme, short vector problem, relinearization technique, squashing paradigm]
Fully Homomorphic Encryption without Squashing Using Depth-3 Arithmetic Circuits
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We describe a new approach for constructing fully homomorphic encryption (FHE) schemes. Previous FHE schemes all use the same blueprint from [Gentry 2009]: First construct a somewhat homomorphic encryption (SWHE) scheme, next "squash" the decryption circuit until it is simple enough to be handled within the homomorphic capacity of the SWHE scheme, and finally "bootstrap" to get a FHE scheme. In all existing schemes, the squashing technique induces an additional assumption: that the sparse subset sum problem (SSSP) is hard. Our new approach constructs FHE as a hybrid of a SWHE and a multiplicatively homomorphic encryption (MHE) scheme, such as Elgamal. Our construction eliminates the need for the squashing step, and thereby also removes the need to assume the SSSP is hard. We describe a few concrete instantiations of the new method, including a "simple" FHE scheme where we replace SSSP with Decision Diffle-Hellman, an optimization of the simple scheme that let us "compress" the FHE ciphertext into a single Elgamal ciphertext(J), and a scheme whose security can be (quantumly) reduced to the approximate ideal-SIVP. We stress that the new approach still relies on bootstrapping, but it shows how to bootstrap without having to "squash" the decryption circuit. The main technique is to express the decryption function of SWHE schemes as a depth-3 Q2 (&#x03A3; &#x03A0; &#x03A3;) arithmetic circuit of a particular form. When evaluating this circuit homomorphically (as needed for bootstrapping), we temporarily switch to a MHE scheme, such as Elgamal, to handle the &#x03A0; part. Due to the special form of the circuit, the switch to the MHE scheme can be done without having to evaluate anything homomorphically. We then translate the result back to the SWHE scheme by homomorphically evaluating the decryption function of the MHE scheme. Using our method, the SWHE scheme only needs to be capable of evaluating the MHE scheme's decryption function, not its own decryption function. We thereby avoid the circularity that necessitated squashing in the original blueprint.
[Elgamal ciphertext, decryption circuit, cryptography, Vectors, Encryption, squashing technique, decryption function, Homomorphic encryption, FHE scheme, SWHE scheme, sparse subset sum problem, Symmetric polynomials, fully homomorphic encryption schemes, digital arithmetic, Depth-3 arithmetic circuits, Public key, depth-3 arithmetic circuits, Logic gates, Polynomials, SSSP, decision Diffle-Hellman, somewhat homomorphic encryption scheme]
Coin Flipping with Constant Bias Implies One-Way Functions
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
It is well known (cf., Impagliazzo and Luby [FOCS '89]) that the existence of almost all "interesting" cryptographic applications, i.e., ones that cannot hold information theoretically, implies one-way functions. An important exception where the above implication is not known, however, is the case of coin-flipping protocols. Such protocols allow honest parties to mutually flip an unbiased coin, while guaranteeing that even a cheating (efficient) party cannot bias the output of the protocol by much. Impagliazzo and Luby proved that coin-flipping protocols that are safe against negligible bias do imply one-way functions, and, very recently, Maji, Prabhakaran, and Sahai [FOCS '10] proved the same for constant-round protocols (with any non-trivial bias). For the general case, however, no such implication was known. We make progress towards answering the above fundamental question, showing that (strong) coin-flipping protocols safe against a constant bias (concretely, (&#x221A;2 -1)/2 - o(1)) imply one-way functions.
[Protocols, coin-flipping protocols, cryptographic protocols, cryptographic applications, coin flipping protocols, Educational institutions, Inverters, Complexity theory, Security, constant bias, one way functions, Computer science, negligible bias, Random variables, constant round protocols, one-way functions]
How to Garble Arithmetic Circuits
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Yao's garbled circuit construction transforms a boolean circuit C : {0, 1}n &#x2192; {0, 1}m into a "garbled circuit" C&#x0302; along with n pairs of k-bit keys, one for each input bit, such that C&#x0302; together with the n keys corresponding to an input x reveal C(x) and no additional information about x. The garbled circuit construction is a central tool for constant-round secure computation and has several other applications. Motivated by these applications, we suggest an efficient arithmetic variant of Yao's original construction. Our construction transforms an arithmetic circuit C : &#x2124;n &#x2192; &#x2124;m over integers from a bounded (but possibly exponential) range into a garbled circuit C&#x0302; along with n affine functions L<sub>i</sub> : &#x2124; &#x2192; &#x2124;k such that C&#x0302; together with the n integer vectors L<sub>i</sub>(x<sub>i</sub>) reveal C(x) and no additional information about x. The security of our construction relies on the intractability of the learning with errors (LWE) problem.
[boolean circuit, central tool, Randomizing Polynomials, Encoding, Vectors, Encryption, Boolean algebra, garble arithmetic circuits, Yao garbled circuit construction, integer vectors, learning with errors, Wires, digital arithmetic, Polynomials, LWE, Cryptography, Garbled Circuit, arithmetic circuit]
Sharp Mixing Time Bounds for Sampling Random Surfaces
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We analyze the mixing time of a natural local Markov Chain (Gibbs sampler) for two commonly studied models of random surfaces: (i) discrete monotone surfaces with "almost planar" boundary conditions and(ii) the one-dimensional discrete Solid-on-Solid (SOS)model. In both cases we prove the first almost optimal bounds. Our proof is inspired by the so-called "meancurvature" heuristic: on a large scale, the dynamics should approximate a deterministic motion in which each point of the surface moves according to a drift proportional to the local inverse mean curvature radius. Key technical ingredients are monotonicity, coupling and an argument due to D. Wilson [17] in the framework of lozenge tiling Markov Chains. The novelty of our approach with respect to previous results consists in proving that, with high probability, the dynamics is dominated by a deterministic evolution which follows the mean curvature prescription. Our method works equally well for both models despite the fact that their equilibrium maximal deviations from the average height profile occur on very different scales.
[one dimensional discrete solid-on-solid model, Solid modeling, sampling methods, equilibrium maximal deviations, discrete monotone surfaces, Lattices, Boundary conditions, sharp mixing time bounds, random surface sampling, Glauber dynamics, Physics, Couplings, Gibbs sampler, mean curvature, Markov processes, natural local Markov chain, spectral gap, Monte Carlo Markov chains (MCMC), lozenge tilings, monotone surfaces, Clocks, mixing time]
Improved Mixing Condition on the Grid for Counting and Sampling Independent Sets
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The hard-core model has received much attention in the past couple of decades as a lattice gas model with hard constraints in statistical physics, a multicast model of calls in communication networks, and as a weighted independent set problem in combinatorics, probability and theoretical computer science. In this model, each independent set I in a graph G is weighted proportionally to &#x03BB;|I|, for a positive real parameter &#x03BB;. For large &#x03BB;, computing the partition function (namely, the normalizing constant which makes the weighting a probability distribution on a finite graph) on graphs of maximum degree &#x0394; &#x2265; 3, is a well known computationally challenging problem. More concretely, let &#x03BB;<sub>c</sub>(T<sub>&#x0394;</sub>) denote the critical value for the so-called uniqueness threshold of the hard-core model on the infinite &#x0394;-regular tree; recent breakthrough results of Dror Weitz (2006) and Allan Sly (2010) have identified &#x03BB;<sub>c</sub>(T<sub>&#x0394;</sub>) as a threshold where the hardness of estimating the above partition function undergoes a computational transition. We focus on the well-studied particular case of the square lattice Z2, and provide a new lower bound for the uniqueness threshold, in particular taking it well above &#x03BB;<sub>c</sub>(T<sub>4</sub>). Our technique refines and builds on the tree of self-avoiding walks approach of Weitz, resulting in a new technical sufficient criterion (of wider applicability) for establishing strong spatial mixing (and hence uniqueness) for the hard-core model. Our new criterion achieves better bounds on strong spatial mixing when the graph has extra structure, improving upon what can be achieved by just using the maximum degree. Applying our technique to Z2 we prove that strong spatial mixing holds for all &#x03BB; &lt;; 2.3882, improving upon the work of Weitz that held for &#x03BB; &lt;; 27/16 = 1.6875. Our results imply a fully-polynomial deterministic approximation algorithm for estimating the partition function, as well as rapid mixing of the associated Glauber dynamics to sample from the hard-core distribution.
[hard core model, Frequency modulation, Heuristic algorithms, Lattices, hard constraints, set theory, Glauber dynamics, Approximation methods, statistical distributions, theoretical computer science, finite graph, polynomial approximation, partition function, probability distribution, square lattice, combinatorics, independent set sampling, improved mixing condition, sampling methods, communication network, uniqueness threshold, Computational modeling, fully polynomial deterministic approximation algorithm, trees (mathematics), Probability, independent set counting, infinite regular tree, normalizing constant, lattice gas model, statistical physics, weighted independent set problem, Approximation algorithms, multicast model]
Solving Connectivity Problems Parameterized by Treewidth in Single Exponential Time
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
For the vast majority of local problems on graphs of small tree width (where by local we mean that a solution can be verified by checking separately the neighbourhood of each vertex), standard dynamic programming techniques give ctw |V|O(1) time algorithms, where tw is the tree width of the input graph G = (V, E) and c is a constant. On the other hand, for problems with a global requirement (usually connectivity) the best-known algorithms were naive dynamic programming schemes running in at least twtw time. We breach this gap by introducing a technique we named Cut&amp;Count that allows to produce ctw |V|O(1) time Monte Carlo algorithms for most connectivity-type problems, including Hamiltonian Path, Steiner Tree, Feedback Vertex Set and Connected Dominating Set. These results have numerous consequences in various fields, like parameterized complexity, exact and approximate algorithms on planar and H-minor-free graphs and exact algorithms on graphs of bounded degree. The constant c in our algorithms is in all cases small, and in several cases we are able to show that improving those constants would cause the Strong Exponential Time Hypothesis to fail. In contrast to the problems aiming to minimize the number of connected components that we solve using Cut&amp;Count as mentioned above, we show that, assuming the Exponential Time Hypothesis, the aforementioned gap cannot be breached for some problems that aim to maximize the number of connected components like Cycle Packing.
[Steiner trees, connectivity-type problems, connectivity problems, global requirement, Heuristic algorithms, ?xed parameter tractability, planar graphs, parameterized complexity, dynamic programming techniques, set theory, feedback vertex set, exact algorithm, Monte Carlo methods, ctw |V|O(1) time Monte Carlo algorithms, approximate algorithm, cut&amp;count, connected dominating set, strong exponential time hypothesis, H-minor-free graphs, Polynomials, Dynamic programming, treewidth, approximation theory, trees (mathematics), Hamiltonian path, dynamic programming, Educational institutions, exact algorithms, Steiner tree, cycle packing, randomized algorithms, Approximation algorithms, single exponential time, computational complexity]
The Minimum k-way Cut of Bounded Size is Fixed-Parameter Tractable
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We consider the minimum k-way cut problem for unweighted undirected graphs with a size bound s on the number of cut edges allowed. Thus we seek to remove as few edges as possible so as to split a graph into k components, or report that this requires cutting more than s edges. We show that this problem is fixed-parameter tractable (FPT) with the standard parameterization in terms of the solution size s. More precisely, for s=O(1), we present a quadratic time algorithm. Moreover, we present a much easier linear time algorithm for planar graphs and bounded genus graphs. Our tractability result stands in contrast to known W[1] hardness of related problems. Without the size bound, Downey et al. [2003] proved that the minimum k-way cut problem is W[1] hard with parameter k, and this is even for simple unweighted graphs. Downey et al. asked about the status for planar graphs. We get linear time with fixed parameter k for simple planar graphs since the minimum k-way cut of a planar graph is of size at most 6k. More generally, we get FPT with parameter k for any graph class with bounded average degree. A simple reduction shows that vertex cuts are at least as hard as edge cuts, so the minimum k-way vertex cut is also W[1] hard with parameter k. Marx [2004] proved that finding a minimum k-way vertex cut of size s is also W[1] hard with parameter s. Marx asked about the FPT status with edge cuts, which we prove tractable here. We are not aware of any other cut problem where the vertex version is W[1] hard but the edge version is FPT, e.g., Marx [2004] proved that the k-terminal cut problem is FPT parameterized by the cut size, both for edge and vertex cuts.
[Terminology, graph theory, planar graphs, bounded genus graphs, edge version, Minimization, FPT, Approximation methods, Helium, $k$-way-cut, linear time algorithm, Approximation algorithms, Polynomials, Kernel, unweighted undirected graphs, minimum k-way cut, fixed parameter tractability]
Multiple-Source Multiple-Sink Maximum Flow in Directed Planar Graphs in Near-Linear Time
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We give an O(n log3 n) algorithm that, given an n-node directed planar graph with arc capacities, a set of source nodes, and a set of sink nodes, finds a maximum flow from the sources to the sinks. Previously, the fastest algorithms known for this problem were those for general graphs.
[Computer vision, multiple-source multiple-sink maximum flow, Particle separators, Educational institutions, Data structures, maximum &#x0EF;&#130; ow, planar separator, Vectors, planar graph, Computer science, optimisation, pseudo&#x0EF;&#130; ow, directed graphs, planar duality, Face, directed planar graphs, 0(nlog3 n) algorithm, computational complexity]
Minimum Weight Cycles and Triangles: Equivalences and Algorithms
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We consider the fundamental algorithmic problem of finding a cycle of minimum weight in a weighted graph. In particular, we show that the minimum weight cycle problem in an undirected n-node graph with edge weights in {1,..., M} or in a directed n-node graph with edge weights in {-M,..., M} and no negative cycles can be efficiently reduced to finding a minimum weight triangle in an &#x0398;(n)- node undirected graph with weights in {1,..., O(M)}. Roughly speaking, our reductions imply the following surprising phenomenon: a minimum cycle with an arbitrary number of weighted edges can be "encoded" using only three edges within roughly the same weight interval! This resolves a longstanding open problem posed in a seminal work by Itai and Rodeh [SIAM J. Computing 1978] on minimum cycle in unweighted graphs. A direct consequence of our efficient reductions are O&#x0303;(Mn&#x03C9;) &#x2264; 6(Mn2.376)-time algorithms using fast matrix multiplication (FMM) for finding a minimum weight cycle in both undirected graphs with integral weights from the interval [1, M] and directed graphs with integral weights from the interval [-M,M]. The latter seems to reveal a strong separation between the all pairs shortest paths (APSP) problem and the minimum weight cycle problem in directed graphs as the fastest known APSP algorithm has a running time of O(M 0.681n2.575) by Zwick [J. ACM 2002]. In contrast, when only combinatorial algorithms are allowed (that is, without FMM) the only known solution to minimum weight cycle is by computing APSP. Interestingly, any separation between the two problems in this case would be an amazing breakthrough as by a recent paper by Vassilevska W. and Williams [FOCS'10], any O(n3-&#x03B5;)-time algorithm (&#x03B5; &gt;; 0) for minimum weight cycle immediately implies a O(n3-&#x03B4;)-time algorithm (&#x03B4; &gt;; 0) for APSP.
[weighted graph, Frequency modulation, all pairs shortest paths problem, reduction matrix multiplication, Approximation methods, minimum weight triangles, triangle, minimum weight cycle problem, Runtime, O&#x0303;(Mn&#x03C9;) &#x2264; 6(Mn2.376)-time algorithms, girth, algorithmic problem, undirected n-node graph, equivalence, Computer science, O(n3-&#x03B4;)-time algorithm, matrix multiplication, directed n-node graph, minimum cycle, Upper bound, directed graphs, Approximation algorithms, geometry, fast matrix multiplication, Manganese, computational complexity]
Graph Connectivities, Network Coding, and Expander Graphs
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We present a new algebraic formulation to compute edge connectivities in a directed graph, using the ideas developed in network coding. This reduces the problem of computing edge connectivities to solving systems of linear equations, thus allowing us to use tools in linear algebra to design new algorithms. Using the algebraic formulation we obtain faster algorithms for computing single source edge connectivities and all pairs edge connectivities, in some settings the amortized time to compute the edge connectivity for one pair is sub linear. Through this connection, we have also found an interesting use of expanders and super concentrators to design fast algorithms for some graph connectivity problems.
[Algorithm design and analysis, network coding, graph theory, Encoding, Vectors, Graph theory, algebraic formulation, edge connectivities, edge connectivity, graph connectivities, directed graph, Network coding, Polynomials, expander graphs, linear algebra, linear equations]
Maximum Edge-Disjoint Paths in Planar Graphs with Congestion 2
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We study the maximum edge-disjoint path problem (MEDP) in planar graphs. We are given a set of terminal pairs and wish to find a maximum routable subset of demands. That is, a subset of demands that can be connected by edge-disjoint paths. It is well-known that there is an integrality gap of order square root of the number of nodes for this problem even on a grid-like graph, and hence in planar graphs (Garg et al.). In contrast, Chekuri et al. show that for planar graphs, if LP is the optimal solution to the natural linear programming relaxation for MEDP, then there is a subset of size OPT over the logarithm of the number of nodes which is routable with congestion 2. Subsequently they showed that it is possible to get within a constant factor of the optimal solution with congestion 4 instead of 2. We strengthen this latter result to show that a constant approximation is possible also with congestion 2 (and this is tight via the integrality gap grid example). We use a basic framework from work by Chekuri et al. At the heart of their approach is a 2-phase algorithm that selects an Okamura-Seymour instance. Each of their phases incurs a factor 2 congestion. It is possible to reduce one of the phases to have congestion 1. In order to achieve an overall congestion 2, however, the two phases must share capacity more carefully. For the Phase 1 problem, we extract a problem called rooted clustering that appears to be an interesting problem class in itself.
[Algorithm design and analysis, rooted clustering, order square root, graph theory, Optimized production technology, planar graphs, edge-disjoint paths, confluent flows, Vectors, Approximation methods, terminal pairs, maximum rout&#x03B1;ble demands. subset, Network flows, OPT, Clustering algorithms, Approximation algorithms, Okamura-Seymour instance, clustering, factor 2 congestion, Face, maximum edge disjoint paths, grid like graph]
Online Node-Weighted Steiner Tree and Related Problems
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We obtain the first online algorithms for the node-weighted Steiner tree, Steiner forest and group Steiner tree problems that achieve a poly-logarithmic competitive ratio. Our algorithm for the Steiner tree problem runs in polynomial time, while those for the other two problems take quasi-polynomial time. Our algorithms can be viewed as online LP rounding algorithms in the framework of Buchbinder and Naor (Foundations and Trends in Theoretical Computer Science, 2009); however, while the natural LP formulation of these problems do lead to fractional algorithms with a poly-logarithmic competitive ratio, we are unable to round these LPs online without losing a polynomial factor. Therefore, we design new LP formulations for these problems drawing on a combination of paradigms such as spider decompositions, low-depth Steiner trees, generalized group Steiner problems, etc. and use the additional structure provided by these to round the more sophisticated LPs losing only a poly-logarithmic factor in the competitive ratio. As further applications of our techniques, we also design polynomial-time online algorithms with poly-logarithmic competitive ratios for two fundamental network design problems in edge-weighted graphs: the group Steiner forest problem (thereby resolving an open question raised by Chekuri et. al. (SODA 2008)) and the single source &#x2113;-vertex connectivity problem (which complements similar results for the corresponding edge-connectivity problem due to Gupta et. al. (STOC 2009)).
[Steiner trees, Algorithm design and analysis, Greedy algorithms, network theory (graphs), edge-connectivity problem, Approximation methods, polynomial-time online algorithms, online node-weighted Steiner tree algorithm, Steiner forest problem, Online Algorithm, Polynomials, spider decompositions, quasipolynomial time, generalized group Steiner tree problems, edge-weighted graphs, polynomials, trees (mathematics), polylogarithmic competitive ratio, Survivable Network Design, related problems, Vegetation, Approximation algorithms, online LP rounding algorithms, fractional algorithms, natural LP formulation]
Extractors for Circuit Sources
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We obtain the first deterministic extractors for sources generated (or sampled) by small circuits of bounded depth. Our main results are: (1) We extract k(k/nd)O(1) bits with exponentially small error from n-bit sources of min-entropy k that are generated by functions f : {0,1} &#x2192; {0,1}n where each output bit depends on &#x2264; d input bits. In particular, we extract from NC sources, corresponding to d = O(1). (2) We extract k(k/n1+&#x03B3;)O(1) bits with super-polynomially small error from ri-bit sources of min-entropy k that are generated by poly(n)-size ACO circuits, for any &#x03B3; &gt;; 0. As our starting point, we revisit the connection by Trevisan and Vadhan (FOCS 2000) between circuit lower bounds and extractors for sources generated by circuits. We note that such extractors (with very weak parameters) are equivalent to lower bounds for generating distributions (FOCS 2010; with Lovett, CCC 2011). Building on those bounds, we prove that the sources in (1) and (2) are (close to) a convex combination of high-entropy "bit-block" sources. Introduced here, such sources are a special case of affine ones. As extractors for (1) and (2) one can use the extractor for low-weight affine sources by Rao (CCC 2009). Along the way, we exhibit an explicit boolean function b : {0,1}n &#x2192; {0,1} such that poly(n)-size ACO circuits cannot generate the distribution (Y, b(Y)), solving a problem about the complexity of distributions. Independently, De and Watson (RANDOM 2011) obtain a result similar to (1) in the special case d = o(lg n).
[circuit complexity, Input variables, convex combination, circuit lower bounds, extractor, Entropy, Complexity theory, equivalent circuits, local, distribution complexity, Boolean functions, the complexity of distributions small-depth circuit, source generation, deterministic extractors, superpolynomially small error, minimum entropy methods, ACO circuits, exponentially small error, circuit source, sampling, min-entropy, Vectors, high entropy bit block source, Hamming weight, weak randomness source, Random variables, explicit boolean function, computational complexity]
Randomness Buys Depth for Approximate Counting
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We show that the promise problem of distinguishing n-bit strings of hamming weights 1/2 + / - &#x03A9;(1/lgd-1 n) can be solved by explicit, randomized (unbounded-fan-in) poly(n)- size depth-d circuits with error &#x2264; 1/3, but cannot be solved by deterministic poly(n)-size depth-(d +1) circuits, for every d &#x2265; 2; and the depth of both is tight. Previous results bounded the depth to within at least an additive 2. Our sharper bounds match Ajtai's simulation of randomized depth-d circuits by deterministic depth-(d+2) circuits (Ann. Pure Appl. Logic; ' 83), and provide an example where randomization (provably) buys resources. Techniques: To rule out deterministic circuits we combine the switching lemma with an earlier depth-3 lower bound by the author (Comp. Complexity 2009). To exhibit randomized circuits we combine recent analyses by Amano (ICALP '09) and Brody and Verbin (FOCS '10) with derandomization. To make these circuits explicit which we find important for the main message of this paper we construct a new pseudorandom generator for certain combinatorial rectangle tests. Based on expander walks, the generator for example fools tests A<sub>1</sub> &#x00D7; A<sub>2</sub> &#x00D7; ... &#x00D7; A<sub>lg n</sub> for A<sub>i</sub> &#x2286; [n], |A<sub>i</sub>| = n/2 with error 1/n and seed length O(lg n), improving on the seed length &#x03A9;(lg n lg lg n) of previous constructions.
[approximation theory, pseudorandom generator, approximate counting, Generators, Graph theory, Approximation methods, Hamming weight, O(lg n) algorithm, randomised algorithms, randomized depth-d circuits, Upper bound, depth, small-depth circuit, approximate majority, Logic gates, randomness vs. determinism, Integrated circuit modeling, hamming weights, combinatorial rectangle, computational complexity]
Pseudorandomness for Read-Once Formulas
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We give an explicit construction of a pseudorandom generator for read-once formulas whose inputs can be read in arbitrary order. For formulas in n inputs and arbitrary gates of fan-in at most d = O(n/ log n), the pseudorandom generator uses (1 - &#x03A9;(1))n bits of randomness and produces an output that looks 2-&#x03A9;(n)-pseudorandom to all such formulas. Our analysis is based on the following lemma. Let P = Mz+e, where M is the parity-check matrix of a sufficiently good binary error-correcting code of constant rate, z is a random string, e is a small-bias distribution, and all operations are modulo 2. Then for every pair of functions f, g: {0,1}n/2&#x2192; {0,1} and every equipartition (I, J) of [n], the distribution P is pseudorandom for the pair (f(x|<sub>I</sub>), g(x|<sub>J</sub>)), where x|<sub>I</sub> and x|<sub>J</sub> denote the restriction of x to the coordinates in / and J, respectively. More generally, our result applies to read-once branching pro- grams of bounded width with arbitrary ordering of the inputs. We show that such branching programs are more powerful distinguishers than those that read their inputs in sequential order: There exist (explicit) pseudorandom distributions that separate these two types of branching programs.
[pseudorandom generator, error correction codes, parity check codes, parity check matrix, Educational institutions, Generators, Vectors, binary error-correcting code, Boolean algebra, random number generation, bounded space computation, Boolean functions, read once formulas, branching programs, boolean formulas, Logic gates, Polynomials, Parity check codes, pseudorandomness, read once branching programs, computational complexity, pseudorandom distributions]
Dispersers for Affine Sources with Sub-polynomial Entropy
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We construct an explicit disperser for affine sources over F<sub>2</sub>n with entropy k = 2log0.9n = no(1). This is a polynomial time computable function D : F<sub>2</sub>n &#x2192; {0,1} such that for every affine space V of F<sub>2</sub>n that has dimension at least k, D(V) = {0,1}. This improves the best previous construction of Ben-Sasson and Kopparty (STOC 2009) that achieved k = &#x03A9;(n4/5). Our technique follows a high level approach that was developed in Barak, Kindler, Shaltiel, Sudakov and Wigderson (J. ACM 2010) and Barak, Rao, Shaltiel and Wigderson (STOC 2006) in the context of dispersers for two independent general sources. The main steps are: &#x00B7; Adjust the high level approach to make it suitable for affine sources. &#x00B7; Implement a "challenge-response game" for affine sources (in the spirit of the two aforementioned papers that introduced such games for two independent general sources). &#x00B7; In order to implement the game, we construct extractors for affine block-wise sources. For this we use ideas and components by Rao (CCC 2009). &#x00B7; Combining the three items above, we obtain dispersers for affine sources with entropy larger than &#x221A;n. We use a recursive win-win analysis in the spirit of Reingold, Shaltiel and Wigderson (SICOMP 2006) and Barak, Rao, Shaltiel and Wigderson (STOC 2006) to get affine dispersers with entropy less than &#x221A;n.
[Context, affine block-wise sources, explicit disperser, polynomials, game theory, Dispersers, Entropy, Indexes, Randomness extractors, Computer science, sub-polynomial entropy, entropy, polynomial time computable function, Explicit construction, Games, Polynomials, Random variables, challenge-response game, recursive win-win analysis, computational complexity, Pseudorandomness]
A Small PRG for Polynomial Threshold Functions of Gaussians
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We discuss a small-seed pseudorandom generator which fools arbitrary degree polynomial threshold functions with respect to the Gaussian distribution.
[pseudorandom generator, polynomials, polynomial threshold function, Gaussian distribution, gaussian distribution, Generators, Vectors, small PRG, Noise measurement, Approximation methods, random number generation, Gaussian polynomial threshold functions, small seed pseudorandom generator, Hypercubes, Polynomials, Random variables]
A Polylogarithmic-Competitive Algorithm for the k-Server Problem
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We give the first polylogarithmic-competitive randomized algorithm for the k-server problem on an arbitrary finite metric space. In particular, our algorithm achieves a competitive ratio of O&#x0303;(log3 n log2 k) for any metric space on n points. This improves upon the (2k-1)-competitive algorithm of Koutsoupias and Papadimitriou (J. ACM 1995) whenever n is sub-exponential in k.
[Algorithm design and analysis, arbitrary finite metric space, queueing theory, Extraterrestrial measurements, Vectors, Probability distribution, polylogarithmic competitive randomized algorithm, Servers, k-server problem, (2k-1) competitive algorithm, randomised algorithms, randomized algorithms, Resource management, competitive analysis]
3-SAT Faster and Simpler - Unique-SAT Bounds for PPSZ Hold in General
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The PPSZ algorithm by Paturi, Pudla&#x0301;k, Saks, and Zane [1998] is the fastest known algorithm for Unique k-SAT, where the input formula does not have more than one satisfying assignment. For k&#x2265;5 the same bounds hold for general k-SAT. We show that this is also the case for k=3,4, using a slightly modified PPSZ algorithm. We do the analysis by defining a cost for satisfiable CNF formulas, which we prove to decrease in each PPSZ step by a certain amount. This improves our previous best bounds with Moser and Scheder [2011] for 3-SAT to O(1.308") and for 4-SAT to O(1.469").
[Algorithm design and analysis, Data preprocessing, satisfiable CNF formulas, computability, 3-SAT, unique k-SAT, PPSZ algorithm, Random processes, polynomial time algorithm, Computer science, Upper bound, exponential time, satisfiability, Cost function, Polynomials, computational complexity, algorithm]
On the Power of Adaptivity in Sparse Recovery
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The goal of (stable) sparse recovery is to recover a k-sparse approximation x* of a vector x from linear measurements of x. Specifically, the goal is to recover x* such that &#x2225;x-x*&#x2225;<sub>p</sub> &#x2264; C min, k-sparse x, &#x2225;x-x'&#x2225;<sub>q</sub> for some constant C and norm parameters p and q. It is known that, for p = q=l or p = q = 2, this task can be accomplished using m = O(k log(n/k)) non-adaptive measurements [3] and that this bound is tight [9], [12], [28]. In this paper we show that if one is allowed to perform measurements that are adaptive, then the number of measurements can be considerably reduced. Specifically, for C = 1+&#x2208; and p = q = 2 we show &#x00B7; A scheme with m= O(1/&#x2208; log log (n&#x2208;/k)) measurements that uses O(log* k &#x00B7; log log(n&#x2208;/k)) rounds. This is a significant improvement over the best possible non-adaptive bound. &#x00B7; A scheme with m = O(1/&#x2208;k log(k/&#x2208;) + k log(n/k)) measurements that uses two rounds. This improves over the best possible non-adaptive bound. To the best of our knowledge, these are the first results of this type.
[Algorithm design and analysis, approximation theory, linear measurements, adaptivity power, sparse recovery, Vectors, Approximation methods, Equations, k-sparse approximation, Position measurement, Approximation algorithms, Signal to noise ratio, computational complexity]
(1 + eps)-Approximate Sparse Recovery
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The problem central to sparse recovery and compressive sensing is that of stable sparse recovery: we want a distribution A of matrices A&#x2208;Rm&#x00D7;n such that, for any c&#x2208;Rn and with probability 1-&#x03B4;&gt;;2/3 over A&#x2208;A, there is an algorithm to recover x&#x0302; from Ax with &#x2225;x&#x0302;-x&#x2225;<sub>p</sub> &#x2264; C<sub>k-sparsex'</sub>min&#x2225;x-x'&#x2225;<sub>p</sub> (1) for some constant C&gt;;1 and norm p. The measurement complexity of this problem is well understood for constant C&gt;;1. However, in a variety of applications it is important to obtain C=1+&#x03F5; for a small &#x03F5;&gt;;0, and this complexity is not well understood. We resolve the dependence on &#x03F5; in the number of measurements required of a k-sparse recovery algorithm, up to polylogarithmic factors for the central cases of p=1 and p=2. Namely, we give new algorithms and lower bounds that show the number of measurements required is k/&#x03F5;p/2polylog(n). For p = 2, our bound of 1/&#x03F5;klog(n/k) is tight up to constant factors. We also give matching bounds when the output is required to be fc-sparse, in which case we achieve k/&#x03F5;ppolylog(n). This shows the distinction between the complexity of sparse and non sparse outputs is fundamental.
[Noise, polylogarithmic factors, probability, Vectors, Complexity theory, Approximation methods, Sparse matrices, matrix algebra, Upper bound, k-sparse recovery algorithm, (1 + &#x03F5;) approximate sparse recovery, complexity measurement, sparse matrices, Geologic measurements, computational complexity]
Near Optimal Column-Based Matrix Reconstruction
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We consider low-rank reconstruction of a matrix using a subset of its columns and we present asymptotically optimal algorithms for both spectral norm and Frobenius norm reconstruction. The main tools we introduce to obtain our results are: (i) the use of fast approximate SVD-like decompositions for column-based matrix reconstruction, and (ii) two deterministic algorithms for selecting rows from matrices with orthonormal columns, building upon the sparse representation theorem for decompositions of the identity that appeared in [1].
[SVD, subset selection, Symmetric matrices, near optimal column based matrix reconstruction, spectral norm, Frobenius norm, sparse representation theorem, Vectors, low-rank matrix approximation, Approximation methods, Matrix decomposition, Sparse matrices, spectral sparsification, Accuracy, Approximation algorithms, approximate SVD, orthonormal columns, fast approximate SVD like decompositions, singular value decomposition, sparse matrices]
Near Linear Lower Bound for Dimension Reduction in L1
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Given a set of n points in &#x2113;<sub>1</sub>, how many dimensions are needed to represent all pair wise distances within a specific distortion? This dimension-distortion tradeoff question is well understood for the &#x2113;<sub>2</sub> norm, where O((log n)/&#x03F5;2) dimensions suffice to achieve 1+&#x03F5; distortion. In sharp contrast, there is a significant gap between upper and lower bounds for dimension reduction in &#x2113;<sub>1</sub>. A recent result shows that distortion 1+&#x03F5; can be achieved with n/&#x03F5;2 dimensions. On the other hand, the only lower bounds known are that distortion &#x03B4; requires n&#x03A9;(1/&#x03B4;2) dimensions and that distortion 1+&#x03F5; requires n1/2-O(&#x03F5; log(1/&#x03F5;)) dimensions. In this work, we show the first near linear lower bounds for dimension reduction in &#x2113;<sub>1</sub>. In particular, we show that 1+&#x03F5; distortion requires at least n1-O(1/log(1/&#x03F5;)) dimensions. Our proofs are combinatorial, but inspired by linear programming. In fact, our techniques lead to a simple combinatorial argument that is equivalent to the LP based proof of Brinkman-Charikar for lower bounds on dimension reduction in &#x2113;<sub>1</sub>.
[Measurement, Image edge detection, LP based proof, Diamond-like carbon, linear programming, metric embedding, Electronic mail, dimension reduction, Computer science, USA Councils, O((log n)/&#x03F5;2) dimensions, near linear lower bound, Labeling, computational complexity]
The 1D Area Law and the Complexity of Quantum States: A Combinatorial Approach
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The classical description of quantum states is in general exponential in the number of qubits. Can we get polynomial descriptions for more restricted sets of states such as ground states of interesting subclasses of local Hamiltonians? This is the basic problem in the study of the complexity of ground states, and requires an understanding of multi-particle entanglement and quantum correlations in such states. Area laws provide a fundamental ingredient in the study of the complexity of ground states, since they offer a way to bound in a quantitative way the entanglement in such states. Although they have long been conjectured for many body systems in arbitrary dimensions, a general rigorous was only recently proved in Hastings' seminal paper [8] for ID systems. In this paper, we give a combinatorial proof of the ID area law for the special case of frustration free systems, improving by an exponential factor the scaling in terms of the inverse spectral gap and the dimensionality of the particles. The scaling in terms of the dimension of the particles is a potentially important issue in the context of resolving the 2D case and higher dimensions, which is one of the most important open questions in Hamiltonian complexity. Our proof is based on a reformulation of the detectability lemma, introduced by us in the context of quantum gap amplification [1]. We give an alternative proof of the detectability lemma, which is not only simpler and more intuitive than the original proof, but also removes a key restriction in the original statement, making it more suitable for this new context. We also give a one page proof of Hastings' proof that the correlations in the ground states of gapped Hamiltonians decay exponentially with the distance, demonstrating the simplicity of the combinatorial approach for those problems.
[polynomial descriptions, quantum states complexity, Correlation, Stationary state, entanglement, quantum entanglement, Educational institutions, Entropy, Complexity theory, ID area law, Approximation methods, Upper bound, combinatorial approach, area law, detectability lemma, Hamiltonian, description complexity, ground state, computational complexity]
On the Complexity of Commuting Local Hamiltonians, and Tight Conditions for Topological Order in Such Systems
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The local Hamiltonian problem plays the equivalent role of SAT in quantum complexity theory. Understanding the complexity of the intermediate case in which the constraints are quantum but all local terms in the Hamiltonian commute, is of importance for conceptual, physical and computational complexity reasons. Bravyi and Vyalyi showed in 2003 [10], using a clever application of the representation theory of C*-algebras, that if the terms in the Hamiltonian are all two-local, the problem is in NP, and the entanglement in the ground states is local. The general case remained open since then. In this paper we extend this result beyond the two-local case, to the case of three-qubit interactions. We then extend our results even further, and show that NP verification is possible for three-wise interaction between qutrits as well, as long as the interaction graph is planar and also "nearly Euclidean" in some well-defined sense. The proofs imply that in all such systems, the entanglement in the ground states is local. These extensions imply an intriguing sharp transition phenomenon in commuting Hamiltonian systems: the ground spaces of 3-local "physical" systems based on qubits and qutrits are diagonalizable by a basis whose entanglement is highly local, while even slightly more involved interactions (the particle dimensionality or the locality of the interaction is larger) already exhibit an important long-range entanglement property called Topological Order. Our results thus imply that Kitaev's celebrated Toric code construction is, in a well defined sense, optimal as a construction of Topological Order based on commuting Hamiltonians.
[Stationary state, topology, SAT, computability, three qubit interactions, NP verification, algebra, long range entanglement property, Computational complexity, Quantum computing, Algebra, C*-algebras, commuting local Hamiltonians, topological order, quantum computing, qubits, qutrits, Hilbert space, Eigenvalues and eigenfunctions, Toric code construction, quantum complexity theory, computational complexity]
Quantum Query Complexity of State Conversion
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
State conversion generalizes query complexity to the problem of converting between two input-dependent quantum states by making queries to the input. We characterize the complexity of this problem by introducing a natural information-theoretic norm that extends the Schur product operator norm. The complexity of converting between two systems of states is given by the distance between them, as measured by this norm. In the special case of function evaluation, the norm is closely related to the general adversary bound, a semi-definite program that lower-bounds the number of input queries needed by a quantum algorithm to evaluate a function. We thus obtain that the general adversary bound characterizes the quantum query complexity of any function whatsoever. This generalizes and simplifies the proof of the same result in the case of boolean input and output. Also in the case of function evaluation, we show that our norm satisfies a remarkable composition property, implying that the quantum query complexity of the composition of two functions is at most the product of the query complexities of the functions, up to a constant. Finally, our result implies that discrete and continuous-time query models are equivalent in the bounded-error setting, even for the general state-conversion problem.
[Algorithm design and analysis, semi-definite program, adversary bound, function evaluation, quantum algorithm, quantum walk, Complexity theory, query processing, Boolean functions, state conversion, Boolean output, Matrix converters, information-theoretic norm, quantum query complexity, input-dependent quantum states, span program, Educational institutions, Vectors, composition property, Quantum mechanics, quantum computing, Boolean input, Schur product operator norm, discrete query models, computational complexity, continuous-time query models]
Optimal Bounds for Quantum Bit Commitment
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Bit commitment is a fundamental cryptographic primitive with numerous applications. Quantum information allows for bit commitment schemes in the information theoretic setting where no dishonest party can perfectly cheat. The previously best-known quantum protocol by Ambainis achieved a cheating probability of at most 3/4. On the other hand, Kitaev showed that no quantum protocol can have cheating probability less than 1/&#x221A;2 (his lower bound on coin flipping can be easily extended to bit commitment). Closing this gap has since been an important open question. In this paper, we provide the optimal bound for quantum bit commitment. First, we show a lower bound of approximately 0.739, improving Kitaev's lower bound. For this, we present some generic cheating strategies for Alice and Bob and conclude by proving a new relation between the trace distance and fidelity of two quantum states. Second, we present an optimal quantum bit commitment protocol which has cheating probability arbitrarily close to 0.739. More precisely, we show how to use any weak coin flipping protocol with cheating probability 1/2 + &#x03B5; in order to achieve a quantum bit commitment protocol with cheating probability 0.739 + O(&#x03B5;). We then use the optimal quantum weak coin flipping protocol described by Mochon. Last, in order to stress the fact that our protocol uses quantum effects beyond the weak coin flip, we show that any classical bit commitment protocol with access to perfect weak (or strong) coin flipping has cheating probability at least 3/4.
[Protocols, optimal bounds, probability, Kitaev lower bound, quantum information, information theoretic setting, Quantum computing, Algorithms, Quantum mechanics, quantum computing, quantum cryptography, quantum bit commitment, Cryptography, Joints, bit commitment, quantum protocol]
Streaming Algorithms via Precision Sampling
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
A technique introduced by Indyk and Woodruff (STOC 2005) has inspired several recent advances in data-stream algorithms. We show that a number of these results follow eas- ily from the application of a single probabilistic method called Precision Sampling. Using this method, we obtain simple data- stream algorithms that maintain a randomized sketch of an input vector x = (x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>), which is useful for the following applications: 1) Estimating the F<sub>k</sub>-moment of x, for k &gt;; 2. 2) Estimating the &#x2113;<sub>p</sub>-norm of x, for p &#x03F5; [1, 2], with small update time. 3) Estimating cascaded norms &#x2113;p(&#x2113;q) for all p,q &gt;; 0. 4) &#x2113;<sub>1</sub> sampling, where the goal is to produce an element i with probability (approximately) |x<sub>i</sub>|/||x||<sub>1</sub>. It extends to similarly defined &#x2113;<sub>p</sub>-sampling, for p &#x03F5; [1, 2]. For all these applications the algorithm is essentially the same: scale the vector x entry-wise by a well-chosen random vector, and run a heavy-hitter estimation algorithm on the resulting vector. Our sketch is a linear function of x, thereby allowing general updates to the vector x. Precision Sampling itself addresses the problem of estimating a sum &#x03A3;<sub>i=1</sub>n a<sub>i</sub> from weak estimates of each real a<sub>i</sub> &#x03F5; [0,1]. More precisely, the estimator first chooses a desired precision u<sub>i</sub> &#x03F5; (0,1] for each i &#x03F5; [n], and then it receives an estimate of every a<sub>i</sub> within additive u<sub>i</sub>. Its goal is to provide a good approximation to &#x03A3;a<sub>i</sub> while keeping a tab on the "approximation cost" &#x03A3;<sub>i</sub>(1/u<sub>i</sub>)- Here we refine previous work (Andoni, Krauthgamer, and Onak, FOCS 2010) which shows that as long as &#x03A3;a<sub>i</sub> = &#x03A9;(1), a good multiplicative approximation can be achieved using total precision of only O(n log n).
[Algorithm design and analysis, sampling methods, Estimation, probability, sampling, multiplicative approximation, Vectors, Approximation methods, Reactive power, streaming, moments, function approximation, heavy-hitter estimation algorithm, linear function, Approximation algorithms, Random variables, cascaded norm estimation, precision sampling, data-stream algorithms, cascaded norms, probabilistic method, computational complexity]
Steiner Shallow-Light Trees are Exponentially Lighter than Spanning Ones
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
For a pair of parameters &#x03B1;, &#x03B2; &#x2265; 1, a spanning tree T of a weighted undirected n-vertex graph G = (V, E, w) is called an (&#x03B1;,&#x03B2;)-shallow-light tree (shortly, (&#x03B1;,&#x03B2;-SLT) of G with respect to a designated vertex rt &#x2208; V if (1) it approximates all distances from rt to the other vertices up to a factor of &#x03B1;, and (2) its weight is at most &#x03B2; times the weight of the minimum spanning tree MST(G) of G. The parameter &#x03B1; (respectively, &#x03B2;) is called the root-distortion (resp., lightness) of the tree T. Shallow-light trees (SLTs) constitute a fundamental graph structure, with numerous theoretical and practical applications. In particular, they were used for constructing spanners, in network design, for VLSI-circuit design, for various data gathering and dissemination tasks in wireless and sensor networks, in overlay networks, and in the message-passing model of distributed computing. Tight tradeoffs between the parameters of SLTs were established by Awer buch et al. [5], [6] and Khuller et al. [33]. They showed that for any &#x03F5; &gt;; 0 there always exist (1+&#x03F5;, O(1/&#x03F5;))-SLTs, and that the upper bound &#x03B2; = O(1/&#x03F5;) on the lightness of SLTs cannot be improved. In this paper we show that using Steiner points one can build SLTs with logarithmic lightness, i.e., &#x03B2; = O(log 1/&#x03F5;). This establishes an exponential separation between spanning SLTs and Steiner ones. One particularly remarkable point on our tradeoff curve is &#x03F5; = 0. In this regime our construction provides a shortest-path tree with weight at most O(log n) &#x00B7; w(MST(G)). Moreover, we prove matching lower bounds that show that all our results are tight up to constant factors. Finally, on our way to these results we settle (up to constant factors) a number of open questions that were raised by Khuller et al. [33] in SODA'93.
[Steiner trees, Measurement, VLSI-circuit design, Merging, minimum spanning tree, tradeoff curve, sensor networks, distributed computing, optimisation, data gathering, SLT, network design, steiner shallow-light trees, fundamental graph structure, Steiner points, Context, message passing model, trees (mathematics), weighted undirected vertex graph, exponential separation, Probabilistic logic, wireless networks, data dissemination, root-distortion, Computer science, shortest path tree, overlay networks, shortest-path tree, Circuit synthesis]
Fully Dynamic Maximal Matching in O (log n) Update Time
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We present an algorithm for maintaining maximal matching in a graph under addition and deletion of edges. Our data structure is randomized that takes O(log n) expected amortized time for each edge update where n is the number of vertices in the graph. While there is a trivial O(n) algorithm for edge update, the previous best known result for this problem was due to Ivkovic and Llyod[4]. For a graph with n vertices and m edges, they give an O((n + m)0.7072) update time algorithm which is sublinear only for a sparse graph. For the related problem of maximum matching, Onak and Rubinfeld [6] designed a randomized data structure that achieves O(log2 n) expected amortized time for each update for maintaining a c-approximate maximum matching for some large constant c. In contrast, we can maintain a factor two approximate maximum matching in O(log n) expected amortized time per update as a direct corollary of the maximal matching scheme. This in turn also implies a two approximate vertex cover maintenance scheme that takes O(log n) expected amortized time per update.
[Algorithm design and analysis, approximation theory, Heuristic algorithms, graph theory, Data structures, Partitioning algorithms, Approximation methods, O(log n) update time, edge deletion, sparse graph, approximate vertex cover maintenance scheme, fully dynamic maximal matching, Approximation algorithms, Computational efficiency, expected amortized time, edge update, c-approximate maximum matching]
Which Networks are Least Susceptible to Cascading Failures?
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The spread of a cascading failure through a network is an issue that comes up in many domains - in the contagious failures that spread among financial institutions during a financial crisis, through nodes of a power grid or communication network during a widespread outage, or through a human population during the outbreak of an epidemic disease. Here we study a natural model of threshold contagion: each node v is assigned a numerical threshold &#x2113;(v) drawn independently from an underlying distribution μ, and v will fail as soon as &#x2113;(v) of its neighbors fail. Despite the simplicity of the formulation, it has been very challenging to analyze the failure processes that arise from arbitrary threshold distributions; even qualitative questions concerning which graphs are the most resilient to cascading failures in these models have been difficult to resolve. Here we develop a set of new techniques for analyzing the failure probabilities of nodes in arbitrary graphs under this model, and we compare different graphs G according to their μ-risk, defined as the maximum failure probability of any node in G when thresholds are drawn from μ. We find that the space of threshold distributions has a surprisingly rich structure when we consider the risk that these thresholds induce on different graphs: small shifts in the distribution of the thresholds can favor graphs with a maximally clustered structure (i.e., cliques), those with a maximally branching structure (trees), or even intermediate hybrids.
[financial institutions, arbitrary graphs, Electric shock, graph theory, probability, cascading failure, financial contagion, Educational institutions, voter models, networks, Power system faults, Resilience, contagion, Analytical models, percolation, Power system protection, numerical threshold, Labeling, finance, cascading failures, maximum failure probability]
The Power of Linear Estimators
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
For a broad class of practically relevant distribution properties, which includes entropy and support size, nearly all of the proposed estimators have an especially simple form. Given a set of independent samples from a discrete distribution, these estimators tally the vector of summary statistics -- the number of domain elements seen once, twice, etc. in the sample -- and output the dot product between these summary statistics, and a fixed vector of coefficients. We term such estimators linear. This historical proclivity towards linear estimators is slightly perplexing, since, despite many efforts over nearly 60 years, all proposed such estimators have significantly sub optimal convergence, compared to the bounds shown in [26], [27]. Our main result, in some sense vindicating this insistence on linear estimators, is that for any property in this broad class, there exists a near-optimal linear estimator. Additionally, we give a practical and polynomial-time algorithm for constructing such estimators for any given parameters. While this result does not yield explicit bounds on the sample complexities of these estimation tasks, we leverage the insights provided by this result to give explicit constructions of near-optimal linear estimators for three properties: entropy, L<sub>1</sub> distance to uniformity, and for pairs of distributions, L<sub>1</sub> distance. Our entropy estimator, when given O(n/&#x2208;log n) independent samples from a distribution of support at most n, will estimate the entropy of the distribution to within additive accuracy &#x2208;, with probability of failure o(1/poly(n)). From the recent lower bounds given in [26], [27], this estimator is optimal, to constant factor, both in its dependence on n, and its dependence on &#x2208;. In particular, the inverse-linear convergence rate of this estimator resolves the main open question of [26], [28], which left open the possibility that the error decreased only with the square root of the number of samples. Our distance to uniformity estimator, when given O(m/&#x2208;2 log m) independent samples from any distribution, returns an &#x2208;-accurate estimate of the L1 distance to the uniform distribution of support m. This is constant-factor optimal, for constant &#x2208;. Finally, our framework extends naturally to properties of pairs of distributions, including estimating the L1 distance and KL-divergence between pairs of distributions. We give an explicit linear estimator for estimating L1 distance to additive accuracy &#x2208; using O(n/&#x2208;2 log n) samples from each distribution, which is constant-factor optimal, for constant &#x2208;. This is the first sub linear-sample estimator for this fundamental property.
[Additives, sample complexities, suboptimal convergence, Entropy, Duality, Approximation methods, L1 Estimation, Histograms, Accuracy, entropy, linear estimators, distribution properties, summary statistics, polynomials, sublinear sample estimator, Estimation, Vectors, polynomial time algorithm, Property Testing, Entropy Estimation, uniformity estimator, KL-divergence, dot product, statistical analysis, computational complexity]
An Algebraic Proof of a Robust Social Choice Impossibility Theorem
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
An important element of social choice theory are impossibility theorems, such as Arrow's theorem [1] and Gibbard-Satterthwaite's theorem [2], [3], which state that under certain natural constraints, social choice mechanisms are impossible to construct. In recent years, beginning in Kalai [4], much work has been done in finding robust versions of these theorems, showing that impossibility remains even when the constraints are almost always satisfied. In this work we present an Algebraic scheme for producing such results. We demonstrate it for a variant of Arrow's theorem, found in Dokow and Holzman [5].
[Laplace equations, Discrete Fourier analysis, Vectors, Encoding, algebra, natural constraints, Robust impossibility theorems, Tensile stress, social sciences, algebraic proof, Arrow theorem, Gibbard-Satterthwaite theorem, Tin, Representation theory, Robustness, Arrow's theorem, Kernel, Social Choice, robust social choice impossibility theorem]
Planar Graphs: Random Walks and Bipartiteness Testing
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We initiate the study of the testability of properties in arbitrary planar graphs. We prove that bipartiteness can be tested in constant time. The previous bound for this class of graphs was O(&#x221A;n), and the constant-time testability was only known for planar graphs with bounded degree. Previously used transformations of unbounded-degree sparse graphs into bounded- degree sparse graphs cannot be used to reduce the problem to the testability of bounded-degree planar graphs. Our approach extends to arbitrary minor-free graphs. Our algorithm is based on random walks. The challenge here is to analyze random walks for a class of graphs that has good separators, i.e., bad expansion. Standard techniques that use a fast convergence to a uniform distribution do not work in this case. Roughly speaking, our analysis technique self-reduces the problem of finding an odd-length cycle in a multigraph G induced by a collection of cycles to another multigraph G' induced by a set of shorter odd-length cycles, in such a way that when a random walks finds a cycle in G' with probability p &gt;; 0, then it does so with probability &#x03BB;(p) &gt;; 0 in G. This reduction is applied until the cycles collapse to self-loops that can be easily detected.
[Algorithm design and analysis, random walks, graph theory, unbounded degree sparse graphs, planar graphs, constant time testability, Educational institutions, property testing, Semiconductor device modeling, bipartiteness testing, Awards activities, minor-free graphs, constant-time algorithms, Bipartite graph, bipartiteness, bounded degree sparse graphs, Testing]
Testing and Reconstruction of Lipschitz Functions with Applications to Data Privacy
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
A function f:D &#x2192; R has Lipschitz constant c if d<sub>R</sub>(f(x), f(y)) &#x2264; c&#x00B7;d<sub>D</sub>(x, y) for all x, y in D, where d<sub>R</sub> and d<sub>D</sub> denote the distance functions on the range and domain of f, respectively. We say a function is Lipschitz if it has Lipschitz constant 1. (Note that rescaling by a factor of 1/c converts a function with a Lipschitz constant c into a Lipschitz function.) In other words, Lipschitz functions are not very sensitive to small changes in the input. We initiate the study of testing and local reconstruction of the Lipschitz property of functions. A property tester has to distinguish functions with the property (in this case, Lipschitz) from functions that are &#x03F5;-far from having the property, that is, differ from every function with the property on at least an &#x03F5; fraction of the domain. A local filter reconstructs an arbitrary function f to ensure that the reconstructed function g has the desired property (in this case, is Lipschitz), changing f only when necessary. A local filter is given a function f and a query x and, after looking up the value of f on a small number of points, it has to output g(x) for some function g, which has the desired property and does not depend on x. If f has the property, g must be equal to f. We consider functions over domains of the form {1, &#x22EF;, n}d equipped with &#x2113;<sub>1</sub> distance. We design efficient testers of the Lipschitz property for functions of the form f:{1, 2}d &#x2192; &#x03B4;Z, where &#x03B4; &#x2208; (0, 1] and &#x03B4;Z is the set of integer multiples of &#x03B4;, and of the form f:{1, &#x22EF;, n}d &#x2192; R, where R is (discretely) metrically convex. We also present an efficient local filter of the Lipschitz property for functions of the form f:{1, &#x22EF;, n}d &#x2192; R. We give corresponding lower bounds on the complexity of testing and local reconstruction. The algorithms we design have applications to program analysis and data privacy. The application to privacy is based on the fact that a function f of entries in a database of sensitive information can be released with noise of magnitude proportional to a Lipschitz constant of f, while preserving the privacy of individuals whose data is stored in the database (Dwork, McSherry, Nissim and Smith, TCC 2006). We give a differentially private mechanism, based on local filters, for releasing a function f when a purported Lipschitz constant of f is provided by a distrusted client. We show that when no reliable Lipschitz constant of f is given, previously known differentially private mechanisms have either a substantially higher running time or a higher expected error, for a large class of symmetric functions f.
[Reconstruction, Data privacy, program testing, Noise, Extraterrestrial measurements, data privacy application, Lipschitz constant, mathematical analysis, private mechanism, Image reconstruction, Lipschitz function reconstruction, Property Testing, Lipschitz Constant, arbitrary function, local filter, Databases, program analysis, data privacy, distance functions, information database, Lipschitz property, Data Privacy, Testing]
How to Play Unique Games Against a Semi-random Adversary: Study of Semi-random Models of Unique Games
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
In this paper, we study the average case complexity of the Unique Games problem. We propose a semi-random model, in which a unique game instance is generated in several steps. First an adversary selects a completely satisfiable instance of Unique Games, then she chooses an &#x03B5;-fraction of all edges, and finally replaces ("corrupts") the constraints corresponding to these edges with new constraints. If all steps are adversarial, the adversary can obtain any (1 - &#x03B5;)-satisfiable instance, so then the problem is as hard as in the worst case. We show however that we can find a solution satisfying a (1 - &#x03B4;) fraction of all constraints in polynomial-time if at least one step is random (we require that the average degree of the graph is &#x03A9;&#x0303;(log k)). Our result holds only for &#x03B5; less than some absolute constant. We prove that if &#x03B5; &#x2265; 1/2, then the problem is hard in one of the models, that is, no polynomial-time algorithm can distinguish between the following two cases: (i) the instance is a (1 - &#x03B5;)-satisfiable semi-random instance and (ii) the instance is at most &#x03B4;-satisfiable (for every &#x03B4; &gt;; 0); the result assumes the 2-to-2 conjecture. Finally, we study semi-random instances of Unique Games that are at most (1 - &#x03B5;)-satisfiable. We present an algorithm that distinguishes between the case when the instance is a semi-random instance and the case when the instance is an (arbitrary) (1 - &#x03B4;)-satisfiable instances if &#x03B5; &gt;; c&#x03B4; (for some absolute constant c).
[graph theory, game theory, computability, Vectors, Approximation methods, semirandom adversary, Games, semirandom unique game models, Approximation algorithms, Polynomials, Mathematical model, polynomial-time algorithm, computational complexity]
The Grothendieck Constant is Strictly Smaller than Krivine's Bound
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The classical Grothendieck constant, denoted K<sub>G</sub>, is equal to the integrality gap of the natural semidefinite relaxation of the problem of computing max {&#x03A3;<sub>i-1</sub>m&#x03A3;<sub>j=1</sub>na<sub>ij</sub>&#x03B5;<sub>i</sub>&#x03B4;<sub>j</sub>: {&#x03B5;<sub>i</sub>}<sub>i=1</sub>m, {&#x03B4;<sub>j</sub>}<sub>j=1</sub>n&#x2286;{-1,1} } a generic and well-studied optimization problem with many applications. Krivine proved in 1977 that KG &#x2264; 2log (1+&#x221A;2)/&#x03C0; and conjectured that his estimate is sharp. We obtain a sharper Grothendieck inequality, showing that KG &lt;; 2log (1+&#x221A;2)/&#x03C0; for an explicit constant &#x03B5;<sub>o</sub> &gt;; 0. Our main contribution is conceptual: despite dealing with a binary rounding problem, random 2-dimensional projections combined with a careful partition of &#x211D;2 in order to round the projected vectors, beat the random hyperplane technique, contrary to Krivine's long-standing conjecture.
[Computer science, Upper bound, Limiting, Grothendieck constant, relaxation theory, random 2-dimensional projections, Vectors, Polynomials, natural semidefinite relaxation, binary rounding problem, Kernel, Physics]
A Parallel Approximation Algorithm for Positive Semidefinite Programming
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Positive semi definite programs are an important subclass of semi definite programs in which all matrices involved in the specification of the problem are positive semi definite and all scalars involved are non-negative. We present a parallel algorithm, which given an instance of a positive semi definite program of size N and an approximation factor &#x03B5; &gt;; 0, runs in (parallel) time poly(1/&#x03B5;)&#x00B7;polylog(N), using poly(N) processors, and outputs a value which is within multiplicative factor of (1+&#x03B5;) to the optimal. Our result generalizes analogous result of Luby and Nisan (1993) for positive linear programs and our algorithm is inspired by their algorithm of [10].
[Algorithm design and analysis, approximation theory, parallel algorithms, multiplicative weight update, parallel approximation algorithm, multiplicative factor, Fast parallel algorithms, linear programming, Approximation methods, Matrix decomposition, positive semidefinite programming, approximation factor, Bismuth, Approximation algorithms, Eigenvalues and eigenfunctions, Yttrium, positive linear programs, parallel time polygon]
Rounding Semidefinite Programming Hierarchies via Global Correlation
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We show a new way to round vector solutions of semidefinite programming (SDP) hierarchies into integral solutions, based on a connection between these hierarchies and the spectrum of the in- put graph. We demonstrate the utility of our method by providing a new SDP-hierarchy based algorithm for constraint satisfaction problems with 2-variable constraints (2-CSP's). More concretely, we show for every 2-CSP instance 3, a rounding algorithm for r rounds of the Lasserre SDP hierarchy for 3 that obtains an integral solution which is at most &#x03B5; worse than the relaxation's value (normalized to lie in [0, 1]), as long as r &gt;; k&#x00B7;rank<sub>&#x2265;&#x03B8;</sub>(3)/ poly(&#x03B5;), where k is the alphabet size of J, &#x03B8; = poly(&#x03B5;/k), and rank<sub>&#x2265;&#x03B8;</sub>(J) denotes the number of eigenvalues larger than &#x03B8; in the normalized adjacency matrix of the constraint graph of J. In the case that J is a Unique Games instance, the threshold &#x03B8; is only a polynomial in &#x03B5;, and is independent of the alphabet size. Also in this case, we can give a non-trivial bound on the number of rounds for every instance. In particular our result yields an SDP-hierarchy based algorithm that matches the performance of the recent subexponential algorithm of Arora, Barak and Steurer (FOCS 2010) in the worst case, but runs faster on a natural family of instances, thus further restricting the set of possible hard instances for Khot's Unique Games Conjecture. Our algorithm actually requires less than the nolO(r) constraints specified by the rth level of the Lasserre hierarchy, and in some cases r rounds of our program can be evaluated in time 2O(r) poly(n).
[semidefinite programming hierarchy rounding, integral solutions, Correlation, convex programming, Vectors, Approximation methods, Noise measurement, vector solution round, mathematical programming, matrix algebra, global correlation, constraint satisfaction problems, adjacency matrix, Games, Approximation algorithms, Random variables, subexponential algorithm, SDP]
Lasserre Hierarchy, Higher Eigenvalues, and Approximation Schemes for Graph Partitioning and Quadratic Integer Programming with PSD Objectives
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We present an approximation scheme for optimizing certain Quadratic Integer Programming problems with positive semidefinite objective functions and global lin- ear constraints. This framework includes well known graph problems such as Minimum graph bisection, Edge expansion, Uniform sparsest cut, and Small Set expansion, as well as the Unique Games problem. These problems are notorious for the existence of huge gaps between the known algorithmic results and NP-hardness results. Our algorithm is based on rounding semidefinite programs from the Lasserre hierarchy, and the analysis uses bounds for low-rank approximations of a matrix in Frobenius norm using columns of the matrix. For all the above graph problems, we give an algorithm running in time nO(r/&#x03B5;2) with approximation ratio (1+&#x03B5;)/min{1,&#x03BB;<sub>r</sub>}, where &#x03BB;<sub>r</sub> is the r'th smallest eigenvalue of the normalized graph Laplacian L. In the case of graph bisection and small set expansion, the number of vertices in the cut is within lower-order terms of the stipulated bound. Our results imply (1 + O(&#x03B5;)) factor approximation in time nO(r*/&#x03B5;2) where r* is the number of eigenvalues of L smaller than 1 - &#x03B5;. This perhaps gives some indication as to why even showing mere APX-hardness for these problems has been elusive, since the reduction must produce graphs with a slowly growing spectrum (and classes like planar graphs which are known to have such a spectral property often admit good algorithms owing to their nice structure). For Unique Games, we give a factor (1 + (2+&#x03B5;)/&#x03BB;<sub>r</sub>) approximation for minimizing the number of unsatisfied constraints in nO(r/&#x03B5;) time. This improves an earlier bound for solving Unique Games on expanders, and also shows that Lasserre SDPs are powerful enough to solve well-known integrality gap instances for the basic SDP. We also give an algorithm for independent sets in graphs that performs well when the Laplacian does not have too many eigenvalues bigger than 1 + o(1).
[global linear constraints, small set expansion, integer programming, low-rank approximations, graph theory, uniform sparsest cut, quadratic integer programming, PSD objectives, APX-hardness, approximation algorithms, Approximation methods, minimum graph bisection, unique games, eigenvalues and eigenfunctions, approximation schemes, semidefinite objective functions, semidefinite programming, factor approximation, Lasserre hierarchy, Eigenvalues and eigenfunctions, graph partitioning, approximation theory, Optimized production technology, game theory, Frobenius norm, Vectors, Partitioning algorithms, quadratic programming, matrix algebra, NP-hardness, normalized graph Laplacian, Games, Approximation algorithms, edge expansion, unique game problem, computational complexity]
Markov Layout
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Consider the problem of laying out a set of n images that match a query onto the nodes of a &#x221A;n&#x00D7;&#x221A;n grid. We are given a score for each image, as well as the distribution of patterns by which a user's eye scans the nodes of the grid and we wish to maximize the expected total score of images selected by the user. This is a special case of the Markov layout problem, in which we are given a Markov chain M together with a set of objects to be placed at the states of the Markov chain. Each object has a utility to the user if viewed, as well as a stopping probability with which the user ceases to look further at objects. This layout problem is prototypical in a number of applications in web search and advertising, particularly in an emerging genre of search results pages from major engines. In a different class of applications, the states of the Markov chain are web pages at a publishers website and the objects are advertisements. We study the approximability of the Markov layout problem. Our main result is an O(log n) approximation algorithm for the most general version of the problem. The core idea is to transform an optimization problem over partial permutations into an optimization problem over sets by losing a logarithmic factor in approximation, the latter problem is then shown to be sub modular with two matroid constraints, which admits a constant-factor approximation. In contrast, we also show the problem is APX-hard via a reduction from CUBIC MAX-BISECTION. We then study harder variants of greater practical interest of the problem in which no gaps - states of M with no object placed on them - are allowed. By exploiting the geometry, we obtain an O(log3/2 n) approximation algorithm when the digraph underlying M is a grid and an O(log n) approximation algorithm when it is a tree. These special cases are especially appropriate for our applications.
[optimization problem, Search problems, utility, advertising, set theory, partial permutation, Approximation methods, APX hard problem, Optimization, markov chain, Markov chain, optimisation, matroid constraint, cubic max bisection, stopping probability, publisher Website, approximation theory, logarithmic factor, probability, webpage layout, Markov layout problem, pattern distribution, Layout, Web pages, Web page, Markov processes, Approximation algorithms, Web sites, Web search, computational complexity, constant factor approximation algorithm]
Limitations of Randomized Mechanisms for Combinatorial Auctions
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The design of computationally efficient and incentive compatible mechanisms that solve or approximate fundamental resource allocation problems is the main goal of algorithmic mechanism design. A central example in both theory and practice is welfare-maximization in combinatorial auctions. Recently, a randomized mechanism has been discovered for combinatorial auctions that is truthful in expectation and guarantees a (1-1/e)-approximation to the optimal social welfare when players have coverage valuations [11]. This approximation ratio is the best possible even for non-truthful algorithms, assuming P &#x2260; NP [16]. Given the recent sequence of negative results for combinatorial auctions under more restrictive notions of incentive compatibility, [7], [2], [9], this development raises a natural question: Are truthful-in-expectation mechanisms compatible with polynomial-time approximation in a way that deterministic or universally truthful mechanisms are not? In particular, can polynomial-time truthful-in-expectation mechanisms guarantee a near-optimal approximation ratio for more general variants of combinatorial auctions? We prove that this is not the case. Specifically, the result of [11] cannot be extended to combinatorial auctions with sub modular valuations in the value oracle model. (Absent strategic considerations, a (1-1/e)-approximation is still achievable in this setting [25].) More precisely, we prove that there is a constant &#x03B3; &gt;; 0 such that there is no randomized mechanism that is truthful-in-expectation-or even approximately truthful-in-expectation-and guarantees an m^{-\\gamma}-approximation to the optimal social welfare for combinatorial auctions with sub modular valuations in the value oracle model. We also prove an analogous result for the flexible combinatorial public projects (CPP) problem, where a truthful-in-expectation (1-1/e)-approximation for coverage valuations has been recently developed [11]. We show that there is no truthful-in-expectation-or even approximately truthful-in-expectation-mechanism that achieves an m-&#x03B3;-approximation to the optimal social welfare for combinatorial public projects with sub modular valuations in the value oracle model. Both our results present an unexpected separation between coverage functions and sub modular functions, which does not occur for these problems without strategic considerations.
[approximation theory, combinatorial mathematics, combinatorial public projects problem, combinatorial auctions, polynomial-time approximation, mechanism design, Approximation methods, optimal social welfare, Cost accounting, submodular functions, randomised algorithms, randomized mechanisms, USA Councils, Tin, Approximation algorithms, approximate fundamental resource allocation problems, Polynomials, Resource management, computational complexity]
Bayesian Combinatorial Auctions: Expanding Single Buyer Mechanisms to Many Buyers
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
For Bayesian combinatorial auctions, we present a general framework for approximately reducing the mechanism design problem for multiple buyers to the mechanism design problem for each individual buyer. Our frame- work can be applied to any setting which roughly satisfies the following assumptions: (i) the buyer's types must be distributed independently (not necessarily identically), (ii) the objective function must be linearly separable over the set of buyers, and (iii) the supply constraints must be the only constraints involving more than one buyer. Our framework is general in the sense that it makes no explicit assumption about any of the following: (i) the buyer's valuations (e.g., submodular, additive, etc), (ii) The distribution of types for each buyer, and (iii) the other constraints involving individual buyers (e.g., budget constraints, etc). We present two generic ra-buyer mechanisms that use 1- buyer mechanisms as black boxes. Assuming that we have an &#x03B1;-approximate 1-buyer mechanism for each buyer and assuming that no buyer ever needs more than 1/k of all copies of each item for some integer k &#x2265; 1, then our generic n- buyer mechanisms are &#x03B3;<sub>k</sub> &#x00B7; &#x03B1;-approximation of the optimal n-buyer mechanism, in which &#x03B3;<sub>k</sub> is a constant which is at least 1 - 1/&#x221A;(k+3). Observe that &#x03B3;<sub>k</sub> is at least1/2 (for k = 1) and approaches 1 as k increases. As a byproduct of our construction, we improve a generalization of prophet inequalities. Furthermore, as applications of our main theorem, we improve several results from the literature.
[Bayesian Combinatorial Auction, combinatorial mathematics, Approximation, Mechanical factors, mechanism design, Approximation methods, commerce, supply constraints, Reduction, Bayesian methods, many buyers, budget constraints, Pricing, Benchmark testing, expanding single buyer mechanisms, Random variables, Bayes methods, Resource management, buyers valuation, Mechanism Design, Bayesian combinatorial auctions, objective function]
Extreme-Value Theorems for Optimal Multidimensional Pricing
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We provide a Polynomial Time Approximation Scheme for the multi-dimensional unit-demand pricing problem, when the buyer's values are independent (but not necessarily identically distributed.) For all &#x03F5; &gt;; 0, we obtain a (1 + &#x03F5;)-factor approximation to the optimal revenue in time polynomial, when the values are sampled from Monotone Hazard Rate (MHR) distributions, quasi-polynomial, when sampled from regular distributions, and polynomial in npoly(log r) when sampled from general distributions supported on a set [u<sub>min</sub>,ru<sub>min</sub>]. We also provide an additive PTAS for all bounded distributions. Our algorithms are based on novel extreme value theorems for MHR and regular distributions, and apply probabilistic techniques to understand the statistical properties of revenue distributions, as well as to reduce the size of the search space of the algorithm. As a byproduct of our techniques, we establish structural properties of optimal solutions. We show that, for all &#x03F5; &gt;; 0, g(1/&#x03F5;) distinct prices suffice to obtain a (1 + &#x03F5;)-factor approximation to the optimal revenue for MHR distributions, where g(1/&#x03F5;) is a quasi-linear function of 1/&#x03F5; that does not depend on the number of items. Similarly, for all &#x03F5; &gt;; 0 and n &gt;; 0, g(1/&#x03F5; &#x00B7; log n) distinct prices suffice for regular distributions, where n is the number of items and g(&#x00B7;) is a polynomial function. Finally, in the i.i.d. MHR case, we show that, as long as the number of items is a sufficiently large function of 1/&#x03F5;, a single price suffices to achieve a (1 + &#x03F5;)-factor approximation. Our results represent significant progress to the single-bidder case of the multidimensional optimal mechanism design problem, following Myerson's celebrated work on optimal mechanism design [Myerson 1981].
[monotone hazard rate distributions, approximation theory, probabilistic techniques, probability, Vectors, Hazards, revenue distributions, Approximation methods, Extreme-value Theorems, optimal multidimensional pricing, PTAS, polynomial time approximation scheme, Pricing, Approximation algorithms, Polynomials, Random variables, pricing, computational complexity]
Efficient Computation of Approximate Pure Nash Equilibria in Congestion Games
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Congestion games constitute an important class of games in which computing an exact or even approximate pure Nash equilibrium is in general PLS-complete. We present a surprisingly simple polynomial-time algorithm that computes O(1)-approximate Nash equilibria in these games. In particular, for congestion games with linear latency functions, our algorithm computes (2 +&#x03B5;)-approximate pure Nash equilibria in time polynomial in the number of players, the number of resources and 1/&#x03B5;. It also applies to games with polynomial latency functions with constant maximum degree d: there, the approximation guarantee is do(d). The algorithm essentially identifies a polynomially long sequence of best-response moves that lead to an approximate equilibrium; the existence of such short sequences is interesting in itself. These are the first positive algorithmic results for approximate equilibria in non-symmetric congestion games. We strengthen them further by proving that, for congestion games that deviate from our mild assumptions, computing &#x03C1;-approximate equilibria is PLS-complete for any polynomial-time computable &#x03C1;.
[linear latency functions, approximation theory, Heuristic algorithms, polynomials, game theory, Nash equilibrium, polynomial latency functions, Complexity theory, Approximation methods, polynomial time algorithm, approximate pure Nash equilibria, pure Nash equilibria approximation, computation and complexity, Games, congestion games, Approximation algorithms, Polynomials]
On Range Searching in the Group Model and Combinatorial Discrepancy
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
In this paper we establish an intimate connection between dynamic range searching in the group model and combinatorial discrepancy. Our result states that, for a broad class of range searching data structures (including all known upper bounds), it must hold that t<sub>u</sub>t<sub>q</sub> = &#x03A9;(disc2/lg n) where t<sub>u</sub> is the worst case update time, t<sub>q</sub> the worst case query time and $\\disc$ is the combinatorial discrepancy of the range searching problem in question. This relation immediately implies a whole range of exceptionally high and near-tight lower bounds for all of the basic range searching problems. We list a few of them in the following: 1.For halfspace range searching in d-dimensional space, we get a lower bound of t<sub>u</sub> t<sub>q</sub> = &#x03A9;(n1-1/d/lg n). This comes within a lg n lg lg n factor of the best known upper bound. 2. For orthogonal range searching in d-dimensional space, we get a lower bound of t<sub>u</sub> t<sub>q</sub> = &#x03A9;(lgd-2+μ(d)n), where μ(d) &gt;; 0 is some small but strictly positive function of d. 3. For ball range searching in d-dimensional space, we get a lower bound of t<sub>u</sub> t<sub>q</sub> = &#x03A9;(n1-1/d/lg n). We note that the previous highest lower bound for any explicit problem, due to Patrascu [STOC'07], states that t<sub>q</sub> = &#x03A9;((lg n/lg(lg n + t<sub>u</sub>))2), which does however hold for a less restrictive class of data structures. Our result also has implications for the field of combinatorial discrepancy. Using textbook range searching solutions, we improve on the best known discrepancy upper bound for axis-aligned rectangles in dimensions d &#x2265; 3.
[worst case query time, worst case update time, combinatorial mathematics, Computational modeling, Data preprocessing, Dynamic range, computational geometry, orthogonal range searching, Data structures, Search problems, dynamic range searching data structure, lower bounds, query processing, Upper bound, discrepancy, axis aligned rectangle, Data models, data structures, combinatorial discrepancy, range searching, search problems, group model, halfspace range searching, computational complexity]
A Randomized Rounding Approach to the Traveling Salesman Problem
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
For some positive constant &#x03F5;<sub>0</sub>, we give a (3/2-&#x03F5;<sub>0</sub>)-approximation algorithm for the following problem: given a graph G<sub>0</sub> = (V,V<sub>0</sub>), find the shortest tour that visits every vertex at least once. This is a special case of the metric traveling salesman problem when the underlying metric is defined by shortest path distances in Go. The result improves on the 3/2-approximation algorithm due to Christofides [13] for this special case. Similar to Christofides, our algorithm finds a spanning tree whose cost is upper bounded by the optimum, then it finds the minimum cost Eulerian augmentation (or T-join) of that tree. The main difference is in the selection of the spanning tree. Except in certain cases where the solution of LP is nearly integral, we select the spanning tree randomly by sampling from a maximum entropy distribution defined by the linear programming relaxation. Despite the simplicity of the algorithm, the analysis builds on a variety of ideas such as properties of strongly Rayleigh measures from probability theory, graph theoretical results on the structure of near minimum cuts, and the integrality of the T-join polytope from polyhedral theory. Also, as a byproduct of our result, we show new properties of the near minimum cuts of any graph, which may be of independent interest.
[Measurement, Algorithm design and analysis, probability theory, traveling salesman problem, polyhedral theory, graph theory, randomized rounding approach, positive constant, Eulerian augmentation, probability, Random Spanning Trees, Traveling salesman problems, Educational institutions, Entropy, linear programming, Approximation Algorithms, Approximation methods, Rayleigh measurement, travelling salesman problems, spanning tree, approximation algorithm, Approximation algorithms, Randomized Rounding, Traveling Salesman Problem]
Approximating Graphic TSP by Matchings
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We present a framework for approximating the metric TSP based on a novel use of matchings. Traditionally, matchings have been used to add edges in order to make a given graph Eulerian, whereas our approach also allows for the removal of certain edges leading to a decreased cost. For the TSP on graphic metrics (graph-TSP), the approach yields a 1.461-approximation algorithm with respect to the Held-Karp lower bound. For graph-TSP restricted to a class of graphs that contains degree three bounded and claw-free graphs, we show that the integrality gap of the Held-Karp relaxation matches the conjectured ratio 4/3. The framework allows for generalizations in a natural way and also leads to a 1.586-approximation algorithm for the traveling salesman path problem on graphic metrics where the start and end vertices are prespecified.
[Measurement, Algorithm design and analysis, approximation theory, approximation, graph theory, Traveling salesman problems, linear programming, Approximation methods, graphic metrics, Graphics, travelling salesman problems, TSP, Held-Karp relaxation matches, traveling salesman path problem, approximation algorithm, graphic TSP, Approximation algorithms, Polynomials, Held-Karp lower bound]
A Unified Continuous Greedy Algorithm for Submodular Maximization
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The study of combinatorial problems with a submodular objective function has attracted much attention in recent years, and is partly motivated by the importance of such problems to economics, algorithmic game theory and combinatorial optimization. Classical works on these problems are mostly combinatorial in nature. Recently, however, many results based on continuous algorithmic tools have emerged. The main bottleneck of such continuous techniques is how to approximately solve a non-convex relaxation for the sub- modular problem at hand. Thus, the efficient computation of better fractional solutions immediately implies improved approximations for numerous applications. A simple and elegant method, called "continuous greedy\
[Greedy algorithms, Algorithm design and analysis, combinatorial mathematics, knapsack constraints, Max-SAT, Approximation methods, unified continuous greedy algorithm, nonconvex relaxation, submodular welfare, Submodular Welfare, Submodular, Approximation, greedy algorithms, Optimized production technology, game theory, nonmonotone submodular objectives, convex programming, algorithmic game theory, Vectors, Continuous Greedy, combinatorial optimization, fractional solutions, submodular maximization, Approximation algorithms, submodular Max-SAT]
Enumerative Lattice Algorithms in any Norm Via M-ellipsoid Coverings
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We give a novel algorithm for enumerating lattice points in any convex body, and give applications to several classic lattice problems, including the Shortest and Closest Vector Problems (SVP and CVP, respectively) and Integer Programming (IP). Our enumeration technique relies on a classical concept from asymptotic convex geometry known as the M-ellipsoid, and uses as a crucial subroutine the recent algorithm of Micciancio and Voulgaris (STOC 2010)for lattice problems in the &#x2113;<sub>2</sub> norm. As a main technical contribution, which may be of independent interest, we build on the techniques of Klartag (Geometric and Functional Analysis, 2006) to give an expected 2O(n)-time algorithm for computing an M-ellipsoid for any n-dimensional convex body. As applications, we give deterministic 2O(n)-time and -space algorithms for solving exact SVP, and exact CVP when the target point is sufficiently close to the lattice, on n-dimensional lattices in any (semi-)norm given an M-ellipsoid of the unit ball. In many norms of interest, including all &#x2113;<sub>p</sub> norms, an M-ellipsoid is computable in deterministic poly(n) time, in which case these algorithms are fully deterministic. Here our approach may be seen as a derandomization of the "AKS sieve" for exact SVP and CVP (Ajtai, Kumar, and Siva Kumar, STOC2001 and CCC 2002). As a further application of our SVP algorithm, we derive an expected O(f*(n))n-time algorithm for Integer Programming, where f*(n) denotes the optimal bound in the so-called "flatnesstheorem," which satisfies f*(n) = O(n4/3 polylog(n))and is conjectured to be f*(n) = O(n). Our runtime improves upon the previous best of O(n2)n by Hildebrand and Koppe (2010).
[Algorithm design and analysis, enumerative lattice algorithms, integer programming, graph theory, Lattices, 2O(n)-time algorithms, Approximation methods, Ellipsoids, Lattice Point Enumeration, M-ellipsoid covering, shortest vector problems, M-ellipsoid, Integer Programming, convex programming, Vectors, Klartag techniques, closest vector problems, n-dimensional convex body, Approximation algorithms, Shortest/Closest Vector Problem, geometry, flatness theorem, enumeration technique, asymptotic convex geometry, computational complexity]
A Nearly-m log n Time Solver for SDD Linear Systems
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We present an improved algorithm for solving symmetrically diagonally dominant linear systems. On input of an n&#x00D7;n symmetric diagonally dominant matrix A with m non-zero entries and a vector b such that Ax&#x0305; = b for some (unknown) vector x&#x0305;, our algorithm computes a vector x such that &#x2225;x-x&#x0305;&#x2225;<sub>A</sub>&#x2264;&#x03F5;&#x2225;x&#x0305;&#x2225;<sub>A</sub>1 in time O&#x0303; (m log n log (1/&#x03F5;))2. The solver utilizes in a standard way a 'preconditioning' chain of progressively sparser graphs. To claim the faster running time we make a two-fold improvement in the algorithm for constructing the chain. The new chain exploits previously unknown properties of the graph sparsification algorithm given in [Koutis,Miller,Peng, FOCS 2010], allowing for stronger preconditioning properties.We also present an algorithm of independent interest that constructs nearly-tight low-stretch spanning trees in time O&#x0303; (m log n), a factor of O (log n) faster than the algorithm in [Abraham,Bartal,Neiman, FOCS 2008]. This speedup directly reflects on the construction time of the preconditioning chain.
[Algorithm design and analysis, Linear systems, algorithms, Laplace equations, Symmetric matrices, m log n time solver, graph theory, Vectors, linear systems, SDD linear systems, sparser graphs, spectral graph theory, Resistance, Computer science, combinatorial preconditioning, graph sparsification algorithm, computational complexity, symmetrically diagonally dominant]
Balls and Bins: Smaller Hash Families and Faster Evaluation
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
A fundamental fact in the analysis of randomized algorithms is that when n balls are hashed into n bins independently and uniformly at random, with high probability each bin contains at most O(log n/ log log n) balls. In various applications, however, the assumption that a truly random hash function is available is not always valid, and explicit functions are required. In this paper we study the size of families (or, equivalently, the description length of their functions) that guarantee a maximal load of O(log n/ log log n) with high probability, as well as the evaluation time of their functions. Whereas such functions must be described using Omega(log n) bits, the best upper bound was formerly O(log2 n/ log log n) bits, which is attained by O(log n/ log log n)-wise independent functions. Traditional constructions of the latter offer an evaluation time of O(log n/ log log n), which according to Siegel's lower bound [FOCS '89] can be reduced only at the cost of significantly increasing the description length. We construct two families that guarantee a maximal load of O(log n/ log log n) with high probability. Our constructions are based on two different approaches, and exhibit different trade-offs between the description length and the evaluation time. The first construction shows that O(log n/ log log n)-wise independence can in fact be replaced by "gradually increasing independence\
[pseudorandom generator, derandomization techniques, Computational modeling, random hash function, Random access memory, probability, Data structures, Generators, hash families, O(log n/ log log n) balls, randomised algorithms, space-bounded computations, Static VAr compensators, randomized algorithms, Polynomials, Random variables, computational complexity]
Lexicographic Products and the Power of Non-linear Network Coding
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We introduce a technique for establishing and amplifying gaps between parameters of network coding and index coding problems. The technique uses linear programs to establish separations between combinatorial and coding-theoretic parameters and applies hyper graph lexicographic products to amplify these separations. This entails combining the dual solutions of the lexicographic multiplicands and proving that this is a valid dual solution of the product. Our result is general enough to apply to a large family of linear programs. This blend of linear programs and lexicographic products gives a recipe for constructing hard instances in which the gap between combinatorial or coding-theoretic parameters is polynomially large. We find polynomial gaps in cases in which the largest previously known gaps were only small constant factors or entirely unknown. Most notably, we show a polynomial separation between linear and non-linear network coding rates. This involves exploiting a connection between matroids and index coding to establish a previously unknown separation between linear and non-linear index coding rates. We also construct index coding problems with a polynomial gap between the broadcast rate and the trivial lower bound for which no gap was previously known.
[combinatorial mathematics, trivial lower bound, nonlinear network coding, Entropy, linear programming, combinatorial parameters, polynomial gaps, index coding, hyper graph lexicographic products, lexicographic products, polynomial separation, linear programs, network coding, broadcast rate, polynomials, index coding problems, Receivers, Encoding, Vectors, Indexes, coding-theoretic parameters, matroids, lexicographic multiplicands, Network coding, Random variables, information inequalities]
Quadratic Goldreich-Levin Theorems
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Decomposition theorems in classical Fourier analysis enable us to express a bounded function in terms of few linear phases with large Fourier coefficients plus a part that is pseudorandom with respect to linear phases. The Goldreich-Levin algorithm [7] can be viewed as an algorithmic analogue of such a decomposition as it gives a way to efficiently find the linear phases associated with large Fourier coefficients. In the study of "quadratic Fourier analysis\
[Algorithm design and analysis, decomposition theorem, Correlation, Additives, polynomials, pseudorandomness property, local self-correction procedure, codeword, random processes, Fourier analysis, Reed-Muller codes, quadratic Fourier analysis, Hamming codes, Fourier coefficients, polynomial time algorithm, fractional Hamming distance, Computer science, quadratic Goldreich-Levin algorithm, Hafnium, Approximation algorithms, Polynomials, quadratic phase, list decoding radius, algorithmic analogue]
Optimal Testing of Multivariate Polynomials over Small Prime Fields
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We consider the problem of testing if a given function f : F<sub>q</sub>n&#x2192;F<sub>q</sub> is close to a n-variate degree d polynomial over the finite field F<sub>q</sub> of q elements. The natural, low-query, test for this property would be to pick the smallest dimension t = t<sub>q,d</sub>&#x2248; d/q such that every function of degree greater than d reveals this aspect on some i-dimensional affine subspace of F<sub>q</sub>n and to test that f when restricted to a random i-dimensional affine subspace is a polynomial of degree at most d on this subspace. Such a test makes only qt queries, independent of n. Previous works, by Alon et al. [1], and Kaufman and Ron [7] and Jutla et al. [6], showed that this natural test rejected functions that were &#x03A9;(1)-far from degree d-polynomials with probability at least &#x03A9;,(q-t). (The initial work [1] considered only the case of q = 2, while the work [6] only considered the case of prime q. The results in [7] hold for all fields.) Thus to get a constant probability of detecting functions that are at constant distance from the space of degree d polynomials, the tests made q2t queries. Kaufman and Ron also noted that when q is prime, then qt queries are necessary. Thus these tests were off by at least a quadratic factor from known lower bounds. Bhattacharyya et al. [2] gave an optimal analysis of this test for the case of the binary field and showed that the natural test actually rejects functions that were &#x03A9;(1)-far from degree d- polynomials with probability &#x03A9;(1). In this work we extend this result for all fields showing that the natural test does indeed reject functions that are &#x03A9;(1)-far from degree d polynomials with &#x03A9;(1)-probability, where the constants depend only on q the field size. Thus our analysis thus shows that this test is optimal (matches known lower bounds) when q is prime. The main technical ingredient in our work is a tight analysis of the number of "hyperplanes" (affine subspaces of co-dimension 1) on which the restriction of a degree d polynomial has degree less than d. We show that the number of such hyperplanes is at most O(qtq-d) - which is tight to within constant factors.
[Frequency modulation, constant distance, polynomials, optimal testing, Low-degree testing, optimal analysis, Probabilistic logic, small prime fields, Reed-Muller codes, Complexity theory, Electronic mail, multivariate polynomials, hyperplane number, Property Testing, Computer science, polynomial degree, Polynomials, n-variate degree, Testing]
Tight Lower Bounds for 2-query LCCs over Finite Fields
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
A Locally Correctable Code (LCC) is an error correcting code that has a probabilistic self-correcting algorithm that, with high probability, can correct any coordinate of the codeword by looking at only a few other coordinates, even if a fraction &#x03B4; of the coordinates are corrupted. LCCs are a stronger form of LDCs (Locally Decodable Codes) which have received a lot of attention recently due to their many applications and surprising constructions. In this work we show a separation between 2-query LDCs and LCCs over finite fields of prime order. Specifically, we prove a lower bound of the form p&#x03A9;(&#x03B4;d) on the length of linear 2-query LCCs over F<sub>p</sub>, that encode messages of length d. Our bound improves over the known bound of 2&#x03A9;(&#x03B4;d) [9], [12], [8] which is tight for LDCs. Our proof makes use of tools from additive combinatorics which have played an important role in several recent results in theoretical computer science. Corollaries of our main theorem are new incidence geometry results over finite fields. The first is an improvement to the Sylvester-Gallai theorem over finite fields [14] and the second is a new analog of Beck's theorem over finite fields.
[locally correctable code, locally decodable codes, Additives, tight lower bounds, error correction codes, Vectors, Decoding, Galois fields, Computer science, Geometry, 2-query LCC, finite fields, error correcting code, Error correction codes, additive combinatorics, Sylvester-Gallai theorem]
A Two Prover One Round Game with Strong Soundness
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We show that for any fixed prime q &#x2265; 5 and constant &#x03B6; &gt;; 0, it is NP-hard to distinguish whether a two prover one round game with q6 answers has value at least 1 - &#x03B6; or at most 4/q. The result is obtained by combining two techniques: (i) An Inner PCP based on the point versus subspace test for linear functions. The test is analyzed Fourier analytically, (ii) The Outer/Inner PCP composition that relies on a certain sub-code covering property for Hadamard codes. This is a new and essentially black-box method to translate a codeword test for Hadamard codes to a consistency test, leading to a full PCP construction. As an application, we show that unless NP has quasi-polynomial time deterministic algorithms, the Quadratic Programming Problem is inapproximable within factor (log n)1/6-o(1).
[quadratic programming problem, game theory, Fourier analysis, Quadratic programming, codeword test, NP-hardness, quasipolynomial time deterministic algorithms, inner PCP composition, Linearity, Games, black-box method, Polynomials, linear functions, Error correction codes, Error correction, Fourier analytics, Hadamard codes, two prover one round game, computational complexity]
The Randomness Complexity of Parallel Repetition
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Consider a m-round interactive protocol with soundness error 1/2. How much extra randomness is required to decrease the soundness error to &#x03B4; through parallel repetition? Previous work, initiated by Bell are, Goldreich and Goldwasser, shows that for public-coin interactive protocols with statistical soundness, m &#x00B7; O(log (1/&#x03B4;)) bits of extra randomness suffices. In this work, we initiate a more general study of the above question. We establish the first derandomized parallel repetition theorem for public-coin interactive protocols with computational soundness (a.k.a. arguments). The parameters of our result essentially matches the earlier works in the information-theoretic setting. We show that obtaining even a sub-linear dependency on the number of rounds m (i.e., o(m)&#x00B7;log(1/&#x03B4;)) is impossible in the information-theoretic, and requires the existence of one-way functions in the computational setting. We show that non-trivial derandomized parallel repetition for private-coin protocols is impossible in the information-theoretic setting and requires the existence of one-way functions in the computational setting. These results are tight in the sense that parallel repetition theorems in the computational setting can trivially be derandomized using pseudorandom generators, which are implied by the existence of one-way functions.
[public coin interactive protocols, interactive protocols, computational soundness, Protocols, randomness complexity, Entropy, Generators, derandomization, Complexity theory, soundness amplification, randomness extractors, Computer science, statistical soundness, Polynomials, Random variables, parallel repetition, computational complexity]
Privacy Amplification and Non-malleable Extractors via Character Sums
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
In studying how to communicate over a public channel with an active adversary, Dodis and Wichs introduced the notion of a non-malleable extractor. A non-malleable extractor dramatically strengthens the notion of a strong ex- tractor. A strong extractor takes two inputs, a weakly-random x and a uniformly random seed y, and outputs a string which appears uniform, even given y. For a non-malleable extractor nmExt, the output nmExt(x,y) should appear uniform given y as well as nmExt(x, A(y)), where A is an arbitrary function with A(y) &#x2260; y. We show that an extractor introduced by Chor and Goldreich is non-malleable when the entropy rate is above half. It outputs a linear number of bits when the entropy rate is 1/2 + &#x03B1;, for any &#x03B1; &gt;; 0. Previously, no nontrivial parameters were known for any non-malleable extractor. To achieve a polynomial running time when outputting many bits, we rely on a widely-believed conjecture about the distribution of prime numbers in arithmetic progressions. Our analysis involves a character sum estimate, which may be of independent interest. Using our non-malleable extractor, we obtain protocols for "privacy amplification": key agreement between two parties who share a weakly-random secret. Our protocols work in the presence of an active adversary with unlimited computational power, and have asymptotically optimal entropy loss. When the secret has entropy rate greater than 1/2, the protocol fol- lows from a result of Dodis and Wichs, and takes two rounds. When the secret has entropy rate &#x03B4; for any constant &#x03B4; &gt;; 0, our new protocol takes a constant (polynomial in 1/&#x03B4;) number of rounds. Our protocols run in polynomial time under the above well-known conjecture about primes.
[random secret, Protocols, prime number, entropy rate, privacy amplification, Educational institutions, arithmetic, cryptography, Entropy, character sum, Privacy, entropy, nontrivial parameter, nonmalleable strong extractor, Hafnium, Polynomials, data privacy, Random variables, public channel communication, arithmetic progression, computational complexity, polynomial running time, asymptotically optimal entropy loss]
Stateless Cryptographic Protocols
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Secure computation protocols inherently involve multiple rounds of interaction among the parties where, typically a party has to keep a state about what has happened in the protocol so far and then wait for the other party to respond. We study if this is inherent. In particular, we study the possibility of designing cryptographic protocols where the parties can be completely stateless and compute the outgoing message by applying a single fixed function to the incoming message (independent of any state). The problem of designing stateless secure computation protocols can be reduced to the problem of designing protocols satisfying the notion of resettable computation introduced by Canetti, Goldreich, Goldwasser and Micali (FOCS'01) and widely studied thereafter. The current start of art in resettable computation allows for construction of protocols which provide security only when a single predetermined party is resettable [15]. An exception is for the case of the zero-knowledge functionality for which a protocol in which both parties are resettable was recently obtained by Deng, Goyal and Sahai (FOCS'09). The fundamental question left open in this sequence of works is, whether fully-resettable computation is possible, when: 1) An adversary can corrupt any number of parties, and 2) The adversary can reset any party to its original state during the execution of the protocol and can restart the protocol. In this paper, we resolve the above problem by constructing secure protocols realizing any efficiently computable multi-party functionality in the plain model under standard cryptographic assumptions. First, we construct a Fully-Resettable Simulation Sound Zero-Knowledge (ss-rs-rZK) protocol. Next, based on these ss-rs-rZK protocols, we show how to compile any semi-honest secure protocol into a protocol secure against fully resetting adversaries. Next, we study a seemingly unrelated open question: "Does there exist a functionality which, in the concurrent setting, is impossible to securely realize using BB simulation but can be realized using NBB simulation?". We resolve the above question in the affirmative by giving an example of such a (reactive) functionality. Somewhat surprisingly, this is done by making a connection to the existence of a fully resettable simulation sound zero-knowledge protocol.
[single fixed function, stateless cryptographic protocols, secure computation protocols, cryptographic protocols, Computational modeling, ss-rs-rZK, fully resettable simulation sound zero knowledge, Public key, zero knowledge functionality, Polynomials, protocol construction, Cryptographic protocols]
Storing Secrets on Continually Leaky Devices
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We consider the question of how to store a value secretly on devices that continually leak information about their internal state to an external attacker. If the secret value is stored on a single device from which it is efficiently retrievable, and the attacker can leak even a single predicate of the internal state of that device, then she may learn some information about the secret value itself. Therefore, we consider a setting where the secret value is shared between multiple devices (or multiple components of a single device), each of which continually leaks arbitrary adaptively chosen predicates its individual state. Since leakage is continual, each device must also continually update its state so that an attacker cannot just leak it entirely one bit at a time. In our model, the devices update their state individually and asynchronously, without any communication between them. The update process is necessarily randomized, and its randomness can leak as well. As our main result, we construct a sharing scheme for two devices, where a constant fraction of the internal state of each device can leak in between and during updates. Our scheme has the structure of a public-key encryption, where one share is a secret key and the other is a ciphertext. As a contribution of independent interest, we also get public-key encryption in the continual leakage model, introduced by Brakerski et al. and Dodis et al. (FOCS '10). This scheme tolerates continual leakage on the secret key and the updates, and simplifies the recent construction of Lewko, Lewko and Waters (STOC '11). For our main result, we show how to update the ciphertexts of the encryption scheme so that the message remains hidden even if an attacker interleaves leakage on secret key and ciphertext shares. The security of our scheme is based on the linear assumption in prime-order bilinear groups. We also provide an extension to general access structures realizable by linear secret sharing schemes across many devices. The main advantage of this extension is that the state of some devices can be compromised entirely, while that of the all remaining devices is susceptible to continual leakage. Lastly, we show impossibility of information theoretic sharing schemes in our model, where continually leaky devices update their state individually.
[linear secret sharing schemes, information theoretic sharing schemes, ciphertext, Computational modeling, Encryption, message hiding, continual leakage model, public key cryptography, prime-order bilinear groups, Public key, message authentication, Games, continually leaky devices, Hardware, information theory, public-key encryption]
Medium Access Using Queues
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Consider a wireless network of n nodes represented by a (undirected) graph G where an edge (i,j) models the fact that transmissions of i and j interfere with each other, i.e. simultaneous transmissions of i and j become unsuccessful. Hence it is required that at each time instance a set of non-interfering nodes (corresponding to an independent set in G) access the wireless medium. To utilize wireless resources efficiently, it is required to arbitrate the access of medium among interfering nodes properly. Moreover, to be of practical use, such a mechanism is required to be totally distributed as well as simple. As the main result of this paper, we provide such a medium access algorithm. It is randomized, totally distributed and simple: each node attempts to access medium at each time with probability that is a function of its local information. We establish efficiency of the algorithm by showing that the corresponding network Markov chain is positive recurrent as long as the demand imposed on the network can be supported by the wireless network (using any algorithm). In that sense, the proposed algorithm is optimal in terms of utilizing wireless resources. The algorithm is oblivious to the network graph structure, in contrast with the so-called polynomial back-off algorithm by Hastad-Leighton-Rogoff (STOC '87, SICOMP '96) that is established to be optimal for the complete graph and bipartite graphs (by Goldberg-MacKenzie (SODA '96, JCSS '99)).
[Algorithm design and analysis, radio networks, queueing theory, network graph structure, Stability, Wireless Medium Access, Estimation, medium access algorithm, Hastad-Leighton-Rogoff, Vectors, wireless resources, noninterfering nodes, Wireless communication, Markov chain, polynomial back-off algorithm, Mixing Time, polynomial approximation, queues, Markov processes, Bismuth, Polynomials, wireless network]
Local Distributed Decision
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
A central theme in distributed network algorithms concerns understanding and coping with the issue of locality. Despite considerable progress, research efforts in this direction have not yet resulted in a solid basis in the form of a fundamental computational complexity theory for locality. Inspired by sequential complexity theory, we focus on a complexity theory for distributed decision problems. In the context of locality, solving a decision problem requires the processors to independently inspect their local neighborhoods and then collectively decide whether a given global input instance belongs to some specified language. We consider the standard LOCAL model of computation and define LD(t) (for local decision) as the class of decision problems that can be solved in t communication rounds. We first study the intriguing question of whether randomization helps in local distributed computing, and to what extent. Specifically, we define the corresponding randomized class BPLD(t,p,q), containing all languages for which there exists a randomized algorithm that runs in t rounds, accepts correct instances with probability at least p and rejects incorrect ones with probability at least q. We show that p2+q = 1 is a threshold for the containment of LD(t) in BPLD(t,p,q). More precisely, we show that there exists a language that does not belong to LD(t) for any t=o(n) but does belong to BPLD(0,p,q) for any p,q &#x2208; (0,1] such that p2+q&#x2264;1. On the other hand, we show that, restricted to hereditary languages, BPLD(t,p,q) = LD(O(t)), for any function t and any p,q &#x2208; (0,1] such that p2+q&gt;;1. In addition, we investigate the impact of non-determinism on local decision, and establish some structural results inspired by classical computational complexity theory. Specifically, we show that non-determinism does help, but that this help is limited, as there exist languages that cannot be decided non-deterministically. Perhaps surprisingly, it turns out that it is the combination of randomization with non-determinism that enables to decide all languages in constant time. Finally, we introduce the notion of local reduction, and establish some completeness results.
[Context, distributed network algorithms, Computational modeling, probability, randomized algorithm, local distributed algorithms, local decision, Distributed computing, Computational complexity, computational complexity theory, Program processors, oracles, distributed algorithms, randomized algorithms, hereditary languages, local distributed decision, sequential complexity theory, Distributed algorithms, computational complexity, nondeterminism]
The Complexity of Renaming
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We study the complexity of renaming, a fundamental problem in distributed computing in which a set of processes need to pick distinct names from a given namespace. We prove an individual lower bound of &#x03A9;( k ) process steps for deterministic renaming into any namespace of size sub-exponential in k, where k is the number of participants. This bound is tight: it draws an exponential separation between deterministic and randomized solutions, and implies new tight bounds for deterministic fetch-and-increment registers, queues and stacks. The proof of the bound is interesting in its own right, for it relies on the first reduction from renaming to another fundamental problem in distributed computing: mutual exclusion. We complement our individual bound with a global lower bound of &#x03A9;(k log (k/c)) on the total step complexity of renaming into a namespace of size ck, for any c &#x2265; 1. This applies to randomized algorithms against a strong adversary, and helps derive new global lower bounds for randomized approximate counter and fetch-and-increment implementations, all tight within logarithmic factors.
[Adaptation models, Adaptive systems, mutual exclusion, distributed processing, individual lower bound, Complexity theory, Registers, deterministic solution, distributed computing, Wires, total step complexity, randomized approximate counter, approximation theory, renaming, Radiation detectors, stacks, exponential separation, deterministic fetch and increment register, randomized algorithm, deterministic algorithms, lower bounds, Sorting, randomised algorithms, global lower bound, deterministic renaming, renaming complexity, queues, fetch-and-increment, namespace, deterministic fetch and increment queues, deterministic fetch and increment stacks, computational complexity]
Mutual Exclusion with O(log^2 Log n) Amortized Work
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
This paper presents a new algorithm for mutual exclusion in which each passage through the critical section costs amortized O(log2 log n) RMRs with high probability. The algorithm operates in a standard asynchronous, local spinning, shared memory model with an oblivious adversary. It guarantees that every process enters the critical section with high probability. The algorithm achieves its efficient performance by exploiting a connection between mutual exclusion and approximate counting.
[Protocols, Radiation detectors, probability, RMR, mutual exclusion, Approximation algorithms, Arrays, Approximation methods, Spinning, computational complexity, 0(log2logn)]
Algorithms for the Generalized Sorting Problem
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We study the generalized sorting problem where we are given a set of n elements to be sorted but only a subset of all possible pairwise element comparisons is allowed. The goal is to determine the sorted order using the smallest possible number of allowed comparisons. The generalized sorting problem may be equivalently viewed as follows. Given an undirected graph G(V, E) where V is the set of elements to be sorted and E defines the set of allowed comparisons, adaptively find the smallest subset E' &#x2286; E of edges to probe such that the directed graph induced by E' contains a Hamiltonian path. When G is a complete graph, we get the standard sorting problem, and it is well-known that &#x0398;(n log n) comparisons are necessary and sufficient. An extensively studied special case of the generalized sorting problem is the nuts and bolts problem where the allowed comparison graph is a complete bipartite graph between two equal-size sets. It is known that for this special case also, there is a deterministic algorithm that sorts using &#x0398;(n log n) comparisons. However, when the allowed comparison graph is arbitrary, to our knowledge, no bound better than the trivial O&#x0303;(n2) bound is known. Our main result is a randomized algorithm that sorts any allowed comparison graph using O(n3/2) comparisons with high probability (provided the input is sortable). We also study the sorting problem in randomly generated allowed comparison graphs, and show that when the edge probability is p, O&#x0303;(min{p2/n, n3/2 &#x221A;p}) comparisons suffice on average to sort.
[Additives, probability, Hamiltonian path, Fasteners, generalized sorting problem algorithms, pairwise element comparisons, randomized algorithm, bipartite graph, deterministic algorithm, deterministic algorithms, nuts and bolts problem, Sorting, randomised algorithms, Algorithms, Awards activities, directed graphs, comparison-sort, sorting, Bipartite graph, undirected graph, Probes, computational complexity, algorithm]
Information Equals Amortized Communication
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We show how to efficiently simulate the sending of a message to a receiver who has partial information about the message, so that the expected number of bits communicated in the simulation is close to the amount of additional information that the message reveals to the receiver who has some information about the message. This is a generalization and strengthening of the Slepian Wolf theorem, which shows how to carry out such a simulation with low amortized communication in the case that the message is a deterministic function of an input. A caveat is that our simulation is interactive. As a consequence, we prove that the internal information cost(namely the information revealed to the parties) involved in computing any relation or function using a two party interactive protocol is exactly equal to the amortized communication complexity of computing independent copies of the same relation or function. We also show that the only way to prove a strong direct sum theorem for randomized communication complexity is by solving a particular variant of the pointer jumping problem that we define. Our work implies that a strong direct sum theorem for communication complexity holds if and only if efficient compression of communication protocols is possible.
[Protocols, Compression, Direct Sum, Slepian-Wolf theorem, Entropy, Complexity theory, deterministic function, communication complexity, interactive simulation, message information, direct sum theorem, protocols, interactive protocol, message passing, internal information cost, Receivers, randomized communication complexity, Communication Complexity, deterministic algorithms, communication protocol compression, randomised algorithms, message sending, amortized communication complexity, pointer jumping problem, Random variables, Mutual information]
Delays and the Capacity of Continuous-Time Channels
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Any physical channel of communication offers two potential reasons why its capacity (the number of bits it can transmit in a unit of time) might be unbounded: (1) (Uncountably) infinitely many choices of signal strength at any given instant of time, and (2) (Uncountably) infinitely many instances of time at which signals may be sent. However channel noise cancels out the potential unboundedness of the first aspect, leaving typical channels with only a finite capacity per instant of time. The latter source of infinity seems less extensively studied. A potential source of unreliability that might restrict the capacity also from the second aspect is ``delay'': Signals transmitted by the sender at a given point of time may not be received with a predictable delay at the receiving end. In this work we examine this source of uncertainty by considering a simple discrete model of delay errors. In our model the communicating parties get to subdivide time as microscopically finely as they wish, but still have to cope with communication delays that are macroscopic and variable. The continuous process becomes the limit of our process as the time subdivision becomes infinitesimal. We taxonomize this class of communication channels based on whether the delays and noise are stochastic or adversarial, and based on how much information each aspect has about the other when introducing its errors. We analyze the limits of such channels and reach somewhat surprising conclusions: The capacity of a physical channel is finitely bounded only if at least one of the two sources of error (signal noise or delay noise) is adversarial. In particular the capacity is finitely bounded only if the delay is adversarial, or the noise is adversarial and acts with knowledge of the stochastic delay. If both error sources are stochastic, or if the noise is adversarial and independent of the stochastic delay, then the capacity of the associated physical channel is infinite!
[channel delays, associated physical channel, Noise, signal processing, Stochastic errors, channel capacity, stochastic delay, Delay, continuous-time channels, delay noise, stochastic systems, continuous time systems, communication channels, Communication, Channel capacity, Receivers, predictable delay, Probabilistic logic, simple discrete model, Encoding, Decoding, Physical channels, signal strength, time subdivision, delays, Adversarial errors, delay errors, Delays, communication delays, signal noise]
Efficient and Explicit Coding for Interactive Communication
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We revisit the problem of reliable interactive communication over a noisy channel, and obtain the first fully explicit (randomized) efficient constant-rate emulation procedure for reliable interactive communication. Our protocol works for any discrete memory less noisy channel with constant capacity, and fails with exponentially small probability in the total length of the protocol. Following a work by Schulman [Schulman 1993] our simulation uses a tree-code, yet as opposed to the non-constructive absolute tree-code used by Schulman, we introduce a relaxation in the notion of goodness for a tree code and define a potent tree code. This relaxation allows us to construct an explicit emulation procedure for any two-party protocol. Our results also extend to the case of interactive multiparty communication. We show that a randomly generated tree code (with suitable constant alphabet size) is an efficiently decodable potent tree code with overwhelming probability. Furthermore we are able to partially derandomize this result by means of epsilon-biased distributions using only O(N) random bits, where N is the depth of the tree.
[Protocols, Hamming distance, Channel capacity, tree-code, efficient coding, two-party protocol, tree codes, explicit coding, derandomization, explicit emulation procedure, Noise measurement, Channel coding, interactive communication with noise, Emulation, noisy channel, interactive communication]
Efficient Reconstruction of Random Multilinear Formulas
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
In the reconstruction problem for a multivariate polynomial f, we have black box access to f and the goal is to efficiently reconstruct a representation of f in a suitable model of computation. We give a polynomial time randomized algorithm for reconstructing random multilinear formulas. Our algorithm succeeds with high probability when given black box access to the polynomial computed by a random multilinear formula according to a natural distribution. This is the strongest model of computation for which a reconstruction algorithm is presently known, albeit efficient in a distributional sense rather than in the worst-case. Previous results on this problem considered much weaker models such as depth-3 circuits with various restrictions or read-once formulas. Our proof uses ranks of partial derivative matrices as a key ingredient and combines it with analysis of the algebraic structure of random multilinear formulas. Partial derivative matrices have earlier been used to prove lower bounds in a number of models of arithmetic complexity, including multilinear formulas and constant depth circuits. As such, our results give supporting evidence to the general thesis that mathematical properties that capture efficient computation in a model should also enable learning algorithms for functions efficiently computable in that model.
[arithmetic complexity, arithmetic circuits, Computational modeling, probability, Reconstruction algorithms, high probability, learning, reconstruction problem, randomised algorithms, multilinear formulas, random multilinear formulas, partial derivative matrices, Logic gates, Syntactics, constant depth circuits, reconstruction, Polynomials, Mathematical model, multivariate polynomial, Integrated circuit modeling, polynomial time randomized algorithm, computational complexity]
New Extension of the Weil Bound for Character Sums with Applications to Coding
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
The Weil bound for character sums is a deep result in Algebraic Geometry with many applications both in mathematics and in the theoretical computer science. The Weil bound states that for any polynomial f(x) over a finite field F and any additive character &#x03C7; : F &#x2192; &#x2102;, either &#x03C7;(f(x)) is a constant function or it is distributed close to uniform. The Weil bound is quite effective as long as deg (f) &#x226A; &#x221A;|F|, but it breaks down when the degree of f exceeds &#x221A;|F|. As the Weil bound plays a central role in many areas, finding extensions for polynomials of larger degree is an important problem with many possible applications. In this work we develop such an extension over finite fields F<sub>p</sub>n of small characteristic: we prove that if f(x) = g(x) + h(x) where deg(g) &#x226A; &#x221A;|F| and h(x) is a sparse polynomial of arbitrary degree but bounded weight degree, then the same conclusion of the classical Weil bound still holds: either &#x03C7;(f(x)) is constant or its distribution is close to uniform. In particular, this shows that the subcode of Reed-Muller codes of degree &#x03C9;(1) generated by traces of sparse polynomials is a code with near optimal distance, while Reed-Muller of such a degree has no distance (i.e. o(1) distance) ; this is one of the few examples where one can prove that sparse polynomials behave differently from non-sparse polynomials of the same degree. As an application we prove new general results for affine invariant codes. We prove that any affine-invariant subspace of quasi-polynomial size is (1) indeed a code (i.e. has good distance) and (2) is locally testable. Previous results for general affine invariant codes were known only for codes of polynomial size, and of length 2n where n needed to be a prime. Thus, our techniques are the first to extend to general families of such codes of super- polynomial size, where we also remove the requirement from n to be a prime. The proof is based on two main ingredients: the extension of the Weil bound for character sums, and a new Fourier-analytic approach for estimating the weight distribution of general codes with large dual distance, which may be of independent interest.
[affine invariant codes, super polynomial size, Additives, polynomials, constant function, Estimation, sparse polynomial, arbitrary degree, sparse polynomials, Reed-Muller codes, Weil bound, Complexity theory, additive character, property testing, character sums, Computer science, Linear code, computer science, Polynomials, Weil bound extension, algebraic geometry, coding application, Testing]
Maximizing Expected Utility for Stochastic Combinatorial Optimization Problems
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We study the stochastic versions of a broad class of combinatorial problems where the weights of the elements in the input dataset are uncertain. The class of problems that we study includes shortest paths, minimum weight spanning trees, and minimum weight matchings over probabilistic graphs, and other combinatorial problems like knapsack. We observe that the expected value is inadequate in capturing different types of risk averse or risk-prone behaviors, and instead we consider a more general objective which is to maximize the expected utility of the solution for some given utility function, rather than the expected weight (expected weight becomes a special case). We show that we can obtain a polynomial time approximation algorithm with additive error &#x03F5; for any &#x03F5; &gt;; 0, if there is a pseudopolynomial time algorithm for the exact version of the problem (This is true for the problems mentioned above) and the maximum value of the utility function is bounded by a constant. Our result generalizes several prior results on stochastic shortest path, stochastic spanning tree, and stochastic knapsack. Our algorithm for utility maximization makes use of the separability of exponential utility and a technique to decompose a general utility function into exponential utility functions, which may be useful in other stochastic optimization problems.
[stochastic combinatorial optimization, Vectors, Approximation methods, Fourier series, knapsack problems, Optimization, combinatorial problems, stochastic knapsack, optimisation, stochastic spanning tree, polynomial approximation, Approximation algorithms, Polynomials, Random variables, polynomial time approximation algorithm, stochastic shortest path]
Approximation Algorithms for Submodular Multiway Partition
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
We study algorithms for the SUBMODULAR Multiway PARTITION problem (SUB-MP). An instance of SUB-MP consists of a finite ground set V, a subset S = {s<sub>1</sub>, S<sub>2</sub>, ..., s<sub>k</sub>} &#x2286; V of k elements called terminals, and a non-negative submodular set function f : 2V &#x2192; &#x211D;<sub>+</sub> on V provided as a value oracle. The goal is to partition V into k sets A<sub>1</sub>,...,A<sub>k</sub> to minimize &#x03A3;<sub>i=1</sub>k f(A<sub>i</sub>) such that for 1 &#x2264; i &#x2264; k, s<sub>i</sub> &#x2208; A<sub>i</sub>. SUB-MP generalizes some well-known problems such as the MULTIWAY CUT problem in graphs and hypergraphs, and the NODE-WEIGHED MULTIWAY Cut problem in graphs. SUB-MP for arbitrary sub- modular functions (instead of just symmetric functions) was considered by Zhao, Nagamochi and Ibaraki [29]. Previous algorithms were based on greedy splitting and divide and conquer strategies. In recent work [5] we proposed a convex-programming relaxation for SUB-MP based on the Lovasz-extension of a submodular function and showed its applicability for some special cases. In this paper we obtain the following results for arbitrary submodular functions via this relaxation. (1) A 2-approximation for SUB-MP. This improves the (k - 1)-approximation from [29]. (2) A (1.5 - 1/k)-approximation for SUB-MP when f is symmetric. This improves the 2(1 - 1/k)-approximation from [23], [29].
[Algorithm design and analysis, approximation theory, submodular multiway partition, greedy splitting, greedy algorithms, graph theory, convex-programming relaxation, convex programming, Vectors, Partitioning algorithms, approximation algorithms, Approximation methods, hypergraphs, Approximation algorithms, node-weighed multiway cut problem, divide and conquer strategies, Polynomials, Resource management]
An FPTAS for #Knapsack and Related Counting Problems
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Given n elements with non-negative integer weights w<sub>1</sub>,..., w<sub>n</sub> and an integer capacity C, we consider the counting version of the classic knapsack problem: find the number of distinct subsets whose weights add up to at most C. We give the first deterministic, fully polynomial-time approximation scheme (FPTAS) for estimating the number of solutions to any knapsack constraint our estimate has relative error 1 &#x00B1; &#x03B5;. Our algorithm is based on dynamic programming. Previously, randomized polynomial-time approximation schemes (FPRAS) were known first by Morris and Sinclair via Markov chain Monte Carlo techniques, and subsequently by Dyer via dynamic programming and rejection sampling. In addition, we present a new method for deterministic approximate counting using read-once branching programs. Our approach yields an FPTAS for several other counting problems, including counting solutions for the multidimensional knapsack problem with a constant number of constraints, the general integer knapsack problem, and the contingency tables problem with a constant number of rows.
[Heuristic algorithms, integer programming, Approximation methods, knapsack problems, fully polynomial time approximation scheme, Monte Carlo methods, polynomial approximation, Polynomials, related counting problems, Dynamic programming, Derandomization, knapsack constraint, Multidimensional Knapsack, Knapsack, Approximate Counting, dynamic programming, Contingency Tables, Generators, Computer science, nonnegative integer, Markov processes, Approximation algorithms, FPRAS, FPTAS, Markov chain Monte Carlo techniques, #knapsack counting problems]
Approximation Algorithms for Correlated Knapsacks and Non-martingale Bandits
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
In the stochastic knapsack problem, we are given a knapsack of size B, and a set of items whose sizes and rewards are drawn from a known probability distribution. To know the actual size and reward we have to schedule the item-when it completes, we get to know these values. The goal is to schedule the items (possibly making adaptive decisions based on the sizes seen so far) to maximize the expected total reward of items which successfully pack into the knapsack. We know constant-factor approximations when (i) the rewards and sizes are independent, and (ii) we cannot prematurely cancel items after we schedule them. What if either or both assumptions are relaxed? Related stochastic packing problems are the multi-armed bandit (and budgeted learning) problems, here one is given several arms which evolve in a specified stochastic fashion with each pull, and the goal is to (adaptively) decide which arms to pull, in order to maximize the expected reward obtained after B pulls in total. Much recent work on this problem focuses on the case when the evolution of each arm follows a martingale, i.e., when the expected reward from one pull of an arm is the same as the reward at the current state. What if the rewards do not form a martingale? In this paper, we give O(1)-approximation algorithms for the stochastic knapsack problem with correlations and/or cancellations. Extending the ideas developed here, we give O(1)-approximations for MAB problems without the martingale assumption. Indeed, we can show that previously proposed linear programming relaxations for these problems have large integrality gaps. So we propose new time-indexed LP relaxations, using a decomposition and "gap-filling" approach, we convert these fractional solutions to distributions over strategies, and then use the LP values and the time ordering information from these strategies to devise randomized adaptive scheduling algorithms.
[stochastic knapsack problem, Approximation methods, knapsack problems, bin packing, constant-factor approximations, probability distribution, Multi-Armed Bandits, Silicon, correlated knapsacks, stochastic processes, approximation theory, Optimized production technology, probability, Stochastic Optimization, Approximation Algorithms, nonmartingale bandits, MAB problems, randomized adaptive scheduling algorithms, randomised algorithms, O(1)-approximation algorithms, Markov processes, Approximation algorithms, Random variables, time-indexed LP relaxations, computational complexity]
Evolution with Recombination
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Valiant (2007) introduced a computational model of evolution and suggested that Darwinian evolution be studied in the framework of computational learning theory. Valiant describes evolution as a restricted form of learning where exploration is limited to a set of possible mutations and feedback is received through the survival of the fittest mutation. In subsequent work Feldman (2008) showed that evolvability in Valiant's model is equivalent to learning in the correlational statistical query (CSQ) model. We extend Valiant's model to include genetic recombination and show that in certain cases, recombination can significantly speed-up the process of evolution in terms of the number of generations, though at the expense of population size. This follows via a reduction from parallel-CSQ algorithms to evolution with recombination. This gives an exponential speed-up (in terms of the number of generations) over previous known results for evolving conjunctions and half spaces with respect to restricted distributions.
[CSQ, Computational modeling, Biological system modeling, Evolutionary computation, computational learning theory, genetic algorithms, correlational statistical query, evolvability, Program processors, genetic recombination, Evolution (biology), computational model, Valiants model, Genetics, computational learning, Polynomials, learning (artificial intelligence), statistical analysis, Darwinian evolution]
[Publishers information]
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science
None
2011
Provides a listing of current committee members and society officers.
[]
Foreword
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Presents the introductory welcome message from the conference proceedings.
[]
Program Committee
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Provides a listing of current committee members.
[]
Learning Topic Models -- Going beyond SVD
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Topic Modeling is an approach used for automatic comprehension and classification of data in a variety of settings, and perhaps the canonical application is in uncovering thematic structure in a corpus of documents. A number of foundational works both in machine learning and in theory have suggested a probabilistic model for documents, whereby documents arise as a convex combination of (i.e. distribution on) a small number of topic vectors, each topic vector being a distribution on words (i.e. a vector of word-frequencies). Similar models have since been used in a variety of application areas, the Latent Dirichlet Allocation or LDA model of Blei et al. is especially popular. Theoretical studies of topic modeling focus on learning the model's parameters assuming the data is actually generated from it. Existing approaches for the most part rely on Singular Value Decomposition (SVD), and consequently have one of two limitations: these works need to either assume that each document contains only one topic, or else can only recover the {\\em span} of the topic vectors instead of the topic vectors themselves. This paper formally justifies Nonnegative Matrix Factorization (NMF) as a main tool in this context, which is an analog of SVD where all vectors are nonnegative. Using this tool we give the first polynomial-time algorithm for learning topic models without the above two limitations. The algorithm uses a fairly mild assumption about the underlying topic matrix called separability, which is usually found to hold in real-life data. Perhaps the most attractive feature of our algorithm is that it generalizes to yet more realistic models that incorporate topic-topic correlations, such as the Correlated Topic Model (CTM) and the Pachinko Allocation Model (PAM). We hope that this paper will motivate further theoretical results that use NMF as a replacement for SVD -- just as NMF has come to replace SVD in many applications.
[topic model learning, SVD, latent Dirichlet allocation, Dictionaries, Approximation methods, automatic data classification, topic vectors, probabilistic model, word-frequencies, topic matrix, correlated topic model, learning (artificial intelligence), polynomial-time algorithm, singular value decomposition, Pachinko allocation model, document handling, pattern classification, document corpus, Computational modeling, NMF, Vectors, Covariance matrix, thematic structure, Noise measurement, automatic data comprehension, machine learning, nonnegative matrix factorization, separability, LDA model, Data models, CTM, PAM, computational complexity]
Finding Correlations in Subquadratic Time, with Applications to Learning Parities and Juntas
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Given a set of n d-dimensional Boolean vectors with the promise that the vectors are chosen uniformly at random with the exception of two vectors that have Pearson-correlation &#x03C1; (Hamming distance d &#x00B7; 1-&#x03C1;/2), how quickly can one find the two correlated vectors? We present an algorithm which, for any constants &#x03B5;, &#x03C1; &gt;; 0 and d &gt;;&gt;; logn/&#x03C1;2 , finds the correlated pair with high probability, and runs in time O(n 3&#x03C9;/4 + &#x03F5;) &lt;; O(n1.8), where w &lt;; 2.38 is the exponent of matrix multiplication. Provided that d is sufficiently large, this runtime can be further reduced. These are the first subquadratic-time algorithms for this problem for which &#x03C1; does not appear in the exponent of n, and improves upon O(n2-O(&#x03C1;)), given by Paturi et al. [15], Locality Sensitive Hashing (LSH) [11] and the Bucketing Codes approach [6]. Applications and extensions of this basic algorithm yield improved algorithms for several other problems: ApproximateClosest Pair: For any sufficiently small constant &#x03F5; &gt;; 0, given n vectors in Rd, our algorithm returns a pair of vectors whose Euclidean distance differs from that of the closest pair by a factor of at most 1+&#x03F5;, and runs in time O(n2-&#x0398;(&#x221A;&#x03F5;)). The best previous algorithms (including LSH) have runtime O(n2-O(&#x03F5;)). Learning Sparse Parity with Noise: Given samples from an instance of the learning parity with noise problem where each example has length n, the true parity set has size at most k &lt;;&lt;; n, and the noise rate is &#x03B7;, our algorithm identifies the set of k indices in time n &#x03C9;+&#x03F5;/3 k poly(1/1-2&#x03B7;) &lt;; n0.8kpoly(1/1-2&#x03B7;). This is the first algorithm with no depenJence on &#x03B7; in the exponent of n, aside from the trivial brute-force algorithm. Learning k-Juntas with Noise: Given uniformly random length n Boolean vectors, together with a label, which is some function of just k &lt;;&lt;; n of the bits, perturbed by noise rate &#x03B7;, return the set of relevant indices. Leveraging the reduction of Feldman et al. [7] our result for learning k-parities implies an algorithm for this problem with runtime n &#x03C9;+&#x03F5;/3 k poly(1/1-2&#x03B7;) &lt;; n0.8k poly(1/1-2&#x03B7;), 2 which improves on the previous best of &gt;; nk(1-2/2k)poly( 1/1-2&#x03B7; ), from [8]. Learning k-Juntas without Noise:1 Our results for learning sparse parities with noise imply an algorithm for learning juntas without noise with runtime n &#x03C9;+&#x03F5;/4k poly(n) &lt;; n0.6 kpoly(n), which improves on the runtime n &#x03C9;+1/&#x03C9; poly(n) &#x2248; n0.7k poly(n) of Mossel n et al. [13].
[learning juntas, Correlation, Noise, computational geometry, noise problem, Runtime, approximate closest pair, learning parity with noise, nearest neighbor, Pearson-correlation, Chebyshev approximation, d-dimensional Boolean vectors, learning (artificial intelligence), learning sparse parity, LSH, subquadratic-time algorithms, Hamming distance, learning k-juntas, correlated vectors, locality sensitive hashing, cryptography, Vectors, metric embedding, Boolean algebra, Noise measurement, closest pair, matrix multiplication, vectors, learning k-parities, noise rate, correlation finding, Euclidean distance, Approximation algorithms, bucketing codes, computational complexity]
Active Property Testing
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
One motivation for property testing of boolean functions is the idea that testing can provide a fast preprocessing step before learning. However, in most machine learning applications, it is not possible to request for labels of arbitrary examples constructed by an algorithm. Instead, the dominant query paradigm in applied machine learning, called active learning, is one where the algorithm may query for labels, but only on points in a given (polynomial-sized) unlabeled sample, drawn from some underlying distribution D. In this work, we bring this well-studied model to the domain of testing. We develop both general results for this active testing model as well as efficient testing algorithms for several important properties for learning, demonstrating that testing can still yield substantial benefits in this restricted setting. For example, we show that testing unions of d intervals can be done with O(1) label requests in our setting, whereas it is known to &#x221A; require &#x03A9;(d) labeled examples for learning (and &#x03A9;(&#x221A;d) for passive testing [22] where the algorithm must pay for every example drawn from D). In fact, our results for testing unions of intervals also yield improvements on prior work in both the classic query model (where any point in the domain can be queried) and the passive testing model as well. For the problem of testing linear separators in Rn over the Gaussian distribution, we show that both active and passive testing can be done with O(&#x221A;n) queries, substantially less than the &#x03A9;(n) needed for learning, with near-matching lower bounds. We also present a general combination result in this model for building testable properties out of others, which we then use to provide testers for a number of assumptions used in semi-supervised learning. In addition to the above results, we also develop a general notion of the testing dimension of a given property with respect to a given distribution, that we show characterizes (up to constant factors) the intrinsic number of label requests needed to test that property. We develop such notions for both the active and passive testing models. We then use these dimensions to prove a number of lower bounds, including for linear separators and the class of dictator functions.
[Algorithm design and analysis, Machine learning algorithms, Noise, dominant query paradigm, Linear threshold functions, Unions of intervals, Gaussian distribution, linear separator testing problem, query processing, Boolean functions, active learning, machine learning applications, dictator functions, Property testing, learning (artificial intelligence), Testing, passive testing, Active learning, polynomial-sized unlabeled sample, active property testing, Sensitivity, passive testing models, Machine learning, query model, semi-supervised learning, computational complexity]
How to Compute in the Presence of Leakage
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We address the following problem: how to execute any algorithm P, for an unbounded number of executions, in the presence of an adversary who observes partial information on the internal state of the computation during executions. The security guarantee is that the adversary learns nothing, beyond P's input/output behavior. This general problem is important for running cryptographic algorithms in the presence of side-channel attacks, as well as for running non-cryptographic algorithms, such as a proprietary search algorithm or a game, on a cloud server where parts of the execution's internals might be observed. Our main result is a compiler, which takes as input an algorithm P and a security parameter &#x03BA;, and produces a functionally equivalent algorithm P'. The running time of P' is a factor of poly(&#x03BA;) slower than P. P' will be composed of a series of calls to poly(&#x03BA;)-time computable sub-algorithms. During the executions of P', an adversary algorithm A, which can choose the inputs of P', can learn the results of adaptively chosen leakage functions - each of bounded output size &#x03A9;&#x0303;(&#x03BA;) - on the sub-algorithms of P' and the randomness they use. We prove that any computationally unbounded A observing the results of computationally unbounded leakage functions, will learn no more from its observations than it could given blackbox access only to the input-output behavior of P. This result is unconditional and does not rely on any secure hardware components.
[proprietary search algorithm, cloud server, Computational modeling, Random access memory, computability, cryptography, unbounded leakage functions, program compilers, side-channel attacks, noncryptographic algorithms, input-output behavior, file servers, adversary algorithm, Hardware, Polynomials, poly(&#x03BA;)-time computable subalgorithms, Cryptography, cloud computing, compiler, Integrated circuit modeling]
Positive Results for Concurrently Secure Computation in the Plain Model
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We consider the question of designing concurrently self-composable protocols in the plain model. We first focus on the minimal setting where there is a party P<sub>1</sub> which might interact with several other parties in any unbounded (polynomial) number of concurrent sessions. P<sub>1</sub> holds a single input x which it uses in all the concurrent sessions. An analogy is a server interacting with various clients at the same time. In this &#x201C;single input&#x201D; setting, we show that many (or even most) functionalities can be securely realized in the plain model. More precisely, we are able to realize all ideal functionalities except ones which are a (weak form of) cryptographic pseudorandom functions. We complement our positive result by showing an impossibility result in this setting for a functionality which evaluates a pseudorandom function. Our security definition follows the standard ideal/real world simulation paradigm (with no super polynomial simulation etc). There is no apriori bound on the number of concurrent executions. We also show interesting extensions of our positive results to the more general setting where the honest parties may choose different inputs in different session (even adaptively), the roles that the parties assume in the protocol may be interchangeable, etc. Prior to our work, the only positive results known in the plain model in the fully concurrent setting were for zeroknowledge.
[Protocols, cryptographic protocols, Computational modeling, positive results, protocol composition, two-party computation, Security, Servers, Standards, concurrency, concurrently self-composable protocols, concurrently secure computation, Databases, standard ideal-real world simulation paradigm, cryptographic pseudorandom functions, Polynomials, plain model, zero knowledge]
Constructing Non-malleable Commitments: A Black-Box Approach
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We propose the first black-box construction of non-malleable commitments according to the standard notion of non-malleability with respect to commitment. Our construction additionally only requires a constant number of rounds and is based only on (black-box use of) one-way functions. Prior to our work, no black-box construction of non-malleable commitments was known (except for relaxed notions of security) in any (polynomial) number of rounds based on any cryptographic assumption. This closes the wide gap existent between black-box and non-black-box constructions for the problem of non-malleable commitments. Our construction relies on (and can be seen as a generalization of) the recent non-malleable commitment scheme of Goyal (STOC 2011). We also show how to get black-box constructions for a host of other cryptographic primitives. We extend our construction to get constant-round concurrent non-malleable commitments, constant-round multi-party coin tossing, and non-malleable statistically hiding commitments (satisfying the notion of non-malleability with respect to opening). All of the mentioned results make only a black-box use of one-way functions. Our primary technical contribution is a novel way of implementing the proof of consistency typically required in the constructions of non-malleable commitments (and other related primitives). We do this by relying on ideas from the ``zero-knowledge from secure multi-party computation" paradigm of Ishai, Kushilevitz, Ostrovsky, and Sahai (STOC 2007). We extend in a novel way this ``computation in the head" paradigm (which can be though of as bringing powerful error-correcting codes into purely computational setting). To construct a non-malleable commitment scheme, we apply our computation in the head techniques to the recent (constant-round) construction of Goyal. Along the way, we also present a simplification of the construction of Goyal where a part of the protocol is implemented in an information theoretic manner. Such a simplification is crucial for getting a black-box construction. This is done by making use of pair wise-independent hash functions and strong randomness extractors. We show that our techniques have multiple applications, as elaborated in the paper. Hence, we believe our techniques might be useful in other settings in future.
[Protocols, Complexity theory, black-box approach, cryptographic assumption, pair wise-independent hash functions, information theory, Cryptography, black-box use of cryptographic primitives, Computational modeling, cryptographic primitives, Receivers, nonmalleable statistically hiding commitments, cryptography, constant-round multiparty coin tossing, information theoretic manner, Standards, constant-round concurrent nonmalleable commitments, secure multiparty computation zero-knowledge paradigm, error-correcting codes, non-malleable commitments, Goyal nonmalleable commitment scheme, statistical analysis, one-way functions, head computation paradigm, computation in the head paradigm]
Constructive Discrepancy Minimization by Walking on the Edges
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Minimizing the discrepancy of a set system is a fundamental problem in combinatorics. One of the cornerstones in this area is the celebrated six standard deviations result of Spencer (AMS 1985): In any system of n sets in a universe of size n, there always exists a coloring which achieves discrepancy 6&#x221A;n. The original proof of Spencer was existential in nature, and did not give an efficient algorithm to find such a coloring. Recently, a breakthrough work of Bansal (FOCS 2010) gave an efficient algorithm which finds such a coloring. His algorithm was based on an SDP relaxation of the discrepancy problem and a clever rounding procedure. In this work we give a new randomized algorithm to find a coloring as in Spencer's result based on a restricted random walk we call Edge-Walk. Our algorithm and its analysis use only basic linear algebra and is &#x201C;truly&#x201D; constructive in that it does not appeal to the existential arguments, giving a new proof of Spencer's theorem and the partial coloring lemma.
[Algorithm design and analysis, restricted random walk, set system, Spencer proof, Gaussian distribution, Entropy, set theory, Spencer theorem, graph colouring, SDP relaxation, partial coloring lemma, linear algebra, combinatorics, discrepancy problem, random walks, Minimization, Vectors, randomized algorithm, Standards, randomised algorithms, constructive discrepancy minimization, clever rounding procedure, discrepancy, Gaussian, Random variables, edge-walk, computational complexity]
Combinatorial Coloring of 3-Colorable Graphs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We consider the problem of coloring a 3-colorable graph in polynomial time using as few colors as possible. We present a combinatorial algorithm getting down to O&#x0303;(n4/11) colors. This is the first combinatorial improvement of Blum's O&#x0303;(n3/8) bound from FOCS'90. Like Blum's algorithm, our new algorithm composes nicely with recent semi-definite programming approaches. The current best bound is O&#x0303;(n0.2072) colors by Chlamtac from FOCS'07. We now bring it down to O&#x0303;(n0. 2049) colors.
[Algorithm design and analysis, Color, 3-colorable graph, Electronic mail, Approximation Algorithms, Approximation methods, combinatorial coloring algorithm, Graph Coloring, graph colouring, mathematical programming, Computer science, O&#x0303;(n0. 2049) colors, semi-definite programming approaches, Approximation algorithms, Polynomials, polynomial time, O&#x0303;(n0.2072) colors, O&#x0303;(n4/11) colors, computational complexity]
A Permanent Approach to the Traveling Salesman Problem
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
A randomized polynomial time algorithm is presented which, for every simple, connected, k-regular graph on n vertices, finds a tour that visits every vertex and has length at most (1 + &#x221A;(64/1n k)) n with high probability. The proof follows simply from results developed in the context of permanents; Egorychev's and Falikman's theorem which lower bounds the permanent of a doubly stochastic matrix and the polynomial time algorithm of Jerrum, Sinclair and Vigoda which samples a near-random, perfect matching from a bipartite graph. The techniques in this paper suggest new permanent-based approaches for TSP which could be useful in attacking other interesting cases of TSP.
[Algorithm design and analysis, connected k-regular graph, Egorychev theorem, Permanent, graph theory, near-random perfect matching, Approximation methods, travelling salesman problems, permanent context, Polynomials, Bipartite graph, undirected graph, Traveling Salesman Problem, traveling salesman problem, probability, Falikman theorem, Traveling salesman problems, randomized polynomial time algorithm, Approximation Algorithms, bipartite graph, polynomial time algorithm, matrix algebra, randomised algorithms, doubly stochastic matrix, Upper bound, Approximation algorithms, computational complexity]
A Structure Theorem for Poorly Anticoncentrated Gaussian Chaoses and Applications to the Study of Polynomial Threshold Functions
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We prove a structural result for degree-d polynomials. In particular, we show that any degree-d polynomial, p can be approximated by another polynomial, p<sub>0</sub>, which can be decomposed as some function of polynomials q1,&#x00B7; &#x00B7; &#x00B7;, q<sub>m</sub> with q<sub>i</sub> normalized and m=O<sub>d</sub>(1), so that if X is a Gaussian random variable, the probability distribution on (q<sub>1</sub>(X), &#x00B7; &#x00B7; &#x00B7; , q<sub>m</sub>(X)) does not have too much mass in any small box. Using this result, we prove improved versions of a number of results about polynomial threshold functions, including producing better pseudorandom generators, obtaining a better invariance principle, and proving improved bounds on noise sensitivity.
[Gaussian random variable, degree-d polynomials, Gaussian distributions, pseudorandom generators, invariance principle, Probability distribution, Vectors, Complexity theory, poorly anticoncentrated Gaussian chaos, Approximation methods, random number generation, statistical distributions, polynomial threshold functions, Tensile stress, polynomial approximation, Gaussian processes, probability distribution, Polynomials, Threshold logic functions, structure theorem, noise sensitivity]
Large Deviation Bounds for Decision Trees and Sampling Lower Bounds for AC0-Circuits
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
There has been considerable interest lately in the complexity of distributions. Recently, Lovett and Viola (CCC 2011) showed that the statistical distance between a uniform distribution over a good code, and any distribution which can be efficiently sampled by a small bounded-depth AC0 circuit, is inverse-polynomially close to one. That is, such distributions are very far from each other. We strengthen their result, and show that the distance is in fact exponentially close to one. This allows us to strengthen the parameters in their application for data structure lower bounds for succinct data structures for codes. From a technical point of view, we develop new large deviation bounds for functions computed by small depth decision trees, which we then apply to obtain bounds for AC0 circuits via the switching lemma. We show that if such functions are Lipschitz on average in a certain sense, then they are in fact Lipschitz almost everywhere. This type of result falls into the extensive line of research which studies large deviation bounds for the sum of random variables, where while not independent, exhibit large deviation bounds similar to these obtained by independent random variables.
[statistical distance, Noise, large deviation bounds, switching lemma, Data structures, Complexity theory, bounded-depth AC0-circuits, Hamming weight, sampling lower bounds, distribution complexity, Small depth circuits, Vegetation, decision trees, data structure lower bounds, Concentration Bounds, Random variables, Decision trees, statistical analysis, Sampling Distributions, Lower Bounds, computational complexity]
Pseudorandomness from Shrinkage
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
One powerful theme in complexity theory and pseudorandomness in the past few decades has been the use lower bounds to give pseudorandom generators (PRGs). However, the general results using this hardness vs. randomness paradigm suffer a quantitative loss in parameters, and hence do not give nontrivial implications for models where we don't know superpolynomial lower bounds but do know lower bounds of a fixed polynomial. We show that when such lower bounds are proved using random restrictions, we can construct PRGs which are essentially best possible without in turn improving the lower bounds. More specifically, say that a circuit family has shrinkage exponent &#x0393; if a random restriction leaving a p fraction of variables unset shrinks the size of any circuit in the family by a factor of p&#x0393;+o(1). Our PRG uses a seed of length s1/(&#x0393;+1)+o(1) to fool circuits in the family of size s. By using this generic construction, we get PRGs with polynomially small error for the following classes of circuits of size s and with the following seed lengths: 1) For de Morgan formulas, seed length s1/3+o(1); 2) For formulas over an arbitrary basis, seed length s1/2+o(1); 3) For read-once de Morgan formulas, seed length s.234...; 4) For branching programs of size s, seed length s1/2+o(1). The previous best PRGs known for these classes used seeds of length bigger than n/2 to output n bits, and worked only when the size s = O(n) [1].
[complexity theory, Computational modeling, Input variables, polynomials, pseudorandom generators, PRG, average-case lowerbounds, Generators, Complexity theory, random number generation, de Morgan formulas, fixed polynomial lower bounds, seed length, branching programs, randomness paradigm, Polynomials, pseudorandomness, Random variables, random restrictions, Integrated circuit modeling, shrinkage, computational complexity]
Better Pseudorandom Generators from Milder Pseudorandom Restrictions
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We present an iterative approach to constructing pseudorandom generators, based on the repeated application of mild pseudorandom restrictions. We use this template to construct pseudorandom generators for combinatorial rectangles and read-once CNFs and a hitting set generator for width-3 branching programs, all of which achieve near-optimal seed-length even in the low-error regime: We get seed-length O&#x0303;(log (n/&#x03B5;)) for error &#x03B5;. Previously, only constructions with seed-length O(log3/2 n) or O(log2 n) were known for these classes with error &#x03B5; = 1/poly(n). The (pseudo)random restrictions we use are milder than those typically used for proving circuit lower bounds in that we only set a constant fraction of the bits at a time. While such restrictions do not simplify the functions drastically, we show that they can be derandomized using small-bias spaces.
[Algorithm design and analysis, combinatorial mathematics, Computational modeling, pseudorandom generators, near-optimal seed-length, combinatorial rectangles, width-3 branching programs, Educational institutions, pseudorandom restrictions, Generators, Approximation methods, random number generation, small-bias spaces, branching programs, iterative approach, Pseudorandom generators, Polynomials, Random variables, random restrictions, DNF formulas, read-once CNF, computational complexity, hitting set generator]
Optimal Multi-dimensional Mechanism Design: Reducing Revenue to Welfare Maximization
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We provide a reduction from revenue maximization to welfare maximization in multidimensional Bayesian auctions with arbitrary - possibly combinatorial - feasibility constraints and independent bidders with arbitrary - possibly combinatorial-demand constraints, appropriately extending Myerson's single-dimensional result [21] to this setting. We also show that every feasible Bayesian auction - including in particular the revenue-optimal one - can be implemented as a distribution over virtual VCG allocation rules. A virtual VCG allocation rule has the following simple form: Every bidder's type ti is transformed into a virtual type fi(ti), via a bidder-specific function. Then, the allocation maximizing virtual welfare is chosen. Using this characterization, we show how to find and run the revenue-optimal auction given only black-box access to an implementation of the VCG allocation rule. We generalize this result to arbitrarily correlated bidders, introducing the notion of a second-order VCG allocation rule. Our results are computationally efficient for all multidimensional settings where the bidders are additive, or can be efficiently mapped to be additive, albeit the feasibility and demand constraints may still remain arbitrary combinatorial. In this case, our mechanisms run in time polynomial in the number of items and the total number of bidder types, but not type profiles. This is polynomial in the number of items, the number of bidders, and the cardinality of the support of each bidder's value distribution. For generic correlated distributions, this is the natural description complexity of the problem. The runtime can be further improved to polynomial in only the number of items and the number of bidders in itemsymmetric settings by making use of techniques from [15].
[independent bidders, Additives, combinatorial-demand constraints, commerce, virtual welfare maximization, item symmetric settings, second-order VCG allocation rule, Game Theory, Runtime, resource allocation, optimal multidimensional mechanism design, virtual VCG allocation rules, revenue maximization, Auctions, Polynomials, polynomial time, combinatorial-feasibility constraints, revenue-optimal auction, revenue reduction, multidimensional Bayesian auctions, Vectors, Awards activities, Bayesian methods, Multi-Dimensional, Convex Optimization, bidder-specific function, Revenue, Bayes methods, Resource management, Mechanism Design, computational complexity]
The Exponential Mechanism for Social Welfare: Private, Truthful, and Nearly Optimal
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In this paper we show that for any mechanism design problem with the objective of maximizing social welfare, the exponential mechanism can be implemented as a truthful mechanism while still preserving differential privacy. Our instantiation of the exponential mechanism can be interpreted as a generalization of the VCG mechanism in the sense that the VCG mechanism is the extreme case when the privacy parameter goes to infinity. To our knowledge, this is the first general tool for designing mechanisms that are both truthful and differentially private.
[multiitem auctions, mechanism design problem, truthful mechanism, Entropy, mechanism design, social welfare maximization, commerce, Cost accounting, Temperature measurement, exponential mechanism, VCG mechanism, Privacy, Atmospheric measurements, Particle measurements, Resource management, differential privacy]
Concave Generalized Flows with Applications to Market Equilibria
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We consider a nonlinear extension of the generalized network How model, with the How leaving an arc being an increasing concave function of the How entering it, as proposed by Truemper [1] and Shigeno [2]. We give a polynomial time combinatorial algorithm for solving corresponding How maximization problems, finding an &#x03B5;-approximate solution in O(m(m + log n) log(MUm/&#x03B5;)) arithmetic operations and value oracle queries, where M and U are upper bounds on simple parameters. This also gives a new algorithm for linear generalized Hows, an efficient, purely scaling variant of the Fat-Path algorithm by Goldberg, Plotkin and Tardos [3], not using any cycle cancellations. We show that this general convex programming model serves as a common framework for several market equilibrium problems, including the linear Fisher market model and its various extensions. Our result immediately provides combinatorial algorithms for various extensions of these market models. This includes nonsymmetric Arrow-Debreu Nash bargaining, settling an open question by Vazirani [4].
[combinatorial mathematics, Transportation, market equilibria, nonsymmetric Arrow-Debreu Nash bargaining, Complexity theory, approximate solution, generalized network How model, value oracle queries, arithmetic operations, market equilibrium, Polynomials, general convex programming model, stock markets, concave function, linear Fisher market model, generalized flows, game theory, network flow algorithms, Educational institutions, convex programming, concave generalized flows, fat-path algorithm, How maximization problems, Linear approximation, polynomial time combinatorial algorithm, Approximation algorithms, computational complexity]
Efficient Interactive Coding against Adversarial Noise
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In this work, we study the problem of constructing interactive protocols that are robust to noise, a problem that was originally considered in the seminal works of Schulman (FOCS '92, STOC '93), and has recently regained popularity. Robust interactive communication is the interactive analogue of error correcting codes: Given an interactive protocol which is designed to run on an error-free channel, construct a protocol that evaluates the same function (or, more generally, simulates the execution of the original protocol) over a noisy channel. As in (non-interactive) error correcting codes, the noise can be either stochastic, i.e. drawn from some distribution, or adversarial, i.e. arbitrary subject only to a global bound on the number of errors. We show how to efficiently simulate any interactive protocol in the presence of constant-rate adversarial noise, while incurring only a constant blow-up in the communication complexity (CC). Our simulator is randomized, and succeeds in simulating the original protocol with probability at least 1 - 2-&#x03A9;(CC).
[CC, interactive protocols, Protocols, Error analysis, error correction codes, error correcting codes, Noise, probability, Synchronization, communication complexity, encoding, Computational complexity, error-free channel, interactive coding, constant-rate adversarial noise, noisy channel, Robustness, interactive communication, protocols, stochastic noise]
A Direct Product Theorem for the Two-Party Bounded-Round Public-Coin Communication Complexity
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
A strong direct product theorem for a problem in a given model of computation states that, in order to compute k instances of the problem, if we provide resource which is less than k times the resource required for computing one instance of the problem with constant success probability, then the probability of correctly computing all the k instances together, is exponentially small in k. In this paper, we consider the model of two-party bounded-round public-coin randomized communication complexity. We show a direct product theorem for the communication complexity of any relation in this model. In particular, our result implies a strong direct product theorem for the two-party constant-message public-coin randomized communication complexity of all relations. As an immediate application of our result, we get a strong direct product theorem for the pointer chasing problem. This problem has been well studied for understanding round v/s communication trade-offs in both classical and quantum communication protocols. Our result generalizes the result of Jain [2011] which can be regarded as the special case when t=1. Our result can be considered as an important progress towards settling the strong direct product conjecture for the two-party public-coin communication complexity, a major open question in this area. We show our result using information theoretic arguments. Our arguments and techniques build on the ones used in Jain~\\cite{Jain:2011}. %, where a strong direct product theorem for the %two-party one-way public-coin communication complexity of all %relations is shown (that is the special case of our result when $t=1$). One key tool used in our work and also in Jain~\\cite{Jain:2011} is a message compression technique due to Braver man and Rao~\\cite{Braverman2011}, who used it to show a {\\em direct sum} theorem in the same model of communication complexity as considered by us. Another important tool that we use is a correlated sampling protocol, which for example, has been used in Holenstein~\\cite{Holenstein2007} for proving a parallel repetition theorem for two-prover games.
[parallel repetition theorem, Protocols, quantum communication protocols, classical communication protocols, Entropy, Complexity theory, communication complexity, pointer chasing problem, two-party bounded-round public-coin randomized communication complexity, two-prover games, information theoretic arguments, information theory, protocols, message compression technique, Computational modeling, sampling protocol, probability, game theory, direct product, direct product theorem, Markov processes, Communication complexity, Random variables, direct product conjecture, bounded rounds, Integrated circuit modeling]
An Additive Combinatorics Approach Relating Rank to Communication Complexity
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
For a {0, 1}-valued matrix M let CC(M) denote the deterministic communication complexity of the boolean function associated with M. It is well-known since the work of Mehlhorn and Schmidt [STOC 1982] that CC(M) is bounded from above by rank(M) and from below by log rank(M) where rank(M) denotes the rank of M over the field of real numbers. Determining where in this range lies the true worst-case value of CC(M) is a fundamental open problem in communication complexity. The state of the art is log1.631 rank(M) &#x2264; CC(M) &#x2264; 0.415 rank(M), the lower bound is by Kushilevitz [unpublished, 1995] and the upper bound is due to Kotlov [Journal of Graph Theory, 1996]. Lovasz and Saks [FOCS 1988] conjecture that CC(M) is closer to the lower bound, i.e., CC(M)&#x2264; logcrank(M)) for some absolute constant c - this is the famous "log-rank conjecture'' - but so far there has been no evidence to support it, even giving a slightly non-trivial (o(rank(M))) upper bound on the communication complexity. Our main result is that, assuming the Polynomial Freiman-Ruzsa (PFR) conjecture in additive combinatorics, there exists a universal constant c such that CC(M) &#x2264; c &#x00B7;rank(M)/log rank(M). Although our bound is stated using the rank of M over the reals, our proof goes by studying the problem over the finite field of size 2, and there we bring to bear a number of new tools from additive combinatorics which we hope will facilitate further progress on this perplexing question. In more detail, our proof is based on the study of the "approximate duality conjecture'' which was suggested by Ben-Sasson and Zewi [STOC 2011] and studied there in connection to the PFR conjecture. First we improve the bounds on approximate duality assuming the PFR conjecture. Then we use the approximate duality conjecture (with improved bounds) to get our upper bound on the communication complexity of low-rank martices.
[approximate duality conjecture, polynomial Freiman-Ruzsa conjecture, Additives, combinatorial mathematics, additive combinatorics approach, Boolean function, log-rank conjecture, Vectors, Complexity theory, communication complexity, Communication Complexity, deterministic communication complexity, matrix algebra, Computer science, Upper bound, Boolean functions, low-rank martices, Log-rank Conjecture, PFR conjecture, Additive Combinatorics, Polynomials]
Approximating the Expansion Profile and Almost Optimal Local Graph Clustering
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Spectral partitioning is a simple, nearly-linear time, algorithm to find sparse cuts, and the Cheeger inequalities provide a worst-case guarantee of the quality of the approximation found by the algorithm. Local graph partitioning algorithms [1], [2], [3] run in time that is nearly linear in the size of the output set, and their approximation guarantee is worse than the guarantee provided by the Cheeger inequalities by a poly-logarithmic log&#x03A9;(1) n factor. It has been an open problem to design a local graph clustering algorithm with an approximation guarantee close to the guarantee of the Cheeger inequalities and with a running time nearly linear in the size of the output. In this paper we solve this problem; we design an algorithm with the same guarantee (up to a constant factor) as the Cheeger inequality, that runs in time slightly super linear in the size of the output. This is the first sublinear (in the size of the input) time algorithm with almost the same guarantee as the Cheeger's inequality. As a byproduct of our results, we prove a bicriteria approximation algorithm for the expansion profile of any graph. Let μ(S) = &#x03A3;<sub>v&#x2208;S</sub> d(v) be the volume, and &#x03C6;(S) := |E(S, S&#x0305;)|/μ(S), be the conductance of a set S of vertices. If there is a set of volume at most &#x03B3; and conductance &#x03C6;, we can find a set of volume at most &#x03B3;1+&#x03F5; and conductance V at most O(&#x221A;&#x03C6;/&#x03F5;), for any &#x03F5; &gt;; 0. Our proof techniques also provide a simpler proof of the structural result of Arora, Barak, Steurer [4], that can be applied to irregular graphs. Our main technical tool is a lemma stating that, for any set S of vertices of a graph, a lazy t-step random walk started from a randomly chosen vertex of S, will remain entirely inside S with probability at least (1-&#x03C6;(S)/2)t. The lemma also implies a new lower bound to the uniform mixing time of any finite states reversible markov chain.
[Algorithm design and analysis, poly-logarithmic log&#x03A9;(1) n factor, sublinear time algorithm, bicriteria approximation algorithm, graph theory, lazy t-step random walk, Approximation methods, local graph partitioning algorithms, finite states reversible Markov chain, Clustering algorithms, Cheeger Inequality, spectral partitioning, approximation theory, Symmetric matrices, Cheeger inequalities, expansion profile approximation, Small Set Expansion Problem, Vectors, Local Graph Clustering, pattern clustering, almost optimal local graph clustering, Markov processes, sparse cuts, Approximation algorithms, computational complexity]
Faster SDP Hierarchy Solvers for Local Rounding Algorithms
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Convex relaxations based on different hierarchies of linear/semi-definite programs have been used recently to devise approximation algorithms for various optimization problems. The approximation guarantee of these algorithms improves with the number of rounds r in the hierarchy, though the complexity of solving (or even writing down the solution for) the r'th level program grows as n&#x03A9;(r) where n is the input size. In this work, we observe that many of these algorithms are based on local rounding procedures that only use a small part of the SDP solution (of size nO(1)2O(r) instead of n&#x03A9;(r)). We give an algorithm to find the requisite portion in time polynomial in its size. The challenge in achieving this is that the required portion of the solution is not fixed a priori but depends on other parts of the solution, sometimes in a complicated iterative manner. Our solver leads to nO(1)2O(r) time algorithms to obtain the same guarantees in many cases as the earlier nO(r) time algorithms based on r rounds of the Lasserre hierarchy. In particular, guarantees based on O(log n) rounds can be realized in polynomial time. For instance, one can (i) get O(1/&#x03BB;<sub>r</sub>) approximations for graph partitioning problems such as minimum bisection and small set expansion in nO(1)2O(r) time, where &#x03BB;<sub>r</sub> is the r'th smallest eigenvalue of the graph's normalized Laplacian; (ii) a similar guarantee in nO(1)kO(r) for Unique Games where k is the number of labels (the polynomial dependence on k is new); and (iii) find an independent set of size &#x03A9;(n) in 3-colorable graphs in (n2r)O(1) time provided &#x03BB;<sub>n-r</sub> &lt;; 17/16. We develop and describe our algorithm in a fairly general abstract framework. The main technical tool in our work, which might be of independent interest in convex optimization, is an efficient ellipsoid algorithm based separation oracle for convex programs that can output a certificate of infeasibility with restricted support. This is used in a recursive manner to find a sequence of consistent points in nested convex bodies that &#x201C;fools&#x201D; local rounding algorithms.
[minimum bisection, small set expansion, linear programming, approximation algorithms, Approximation methods, Ellipsoids, graph colouring, unique games, eigenvalues and eigenfunctions, ellipsoid algorithm, ellipsoid method, SDP hierarchy solvers, Lasserre hierarchy, Polynomials, approximation theory, semi-definite programming, convex programming, Vectors, graph normalized Laplacian eigenvalue, Partitioning algorithms, local rounding, 3-colorable graphs, local rounding algorithms, general abstract framework, convex programs, Games, linear-semidefinite programs, Approximation algorithms, convex optimization, time polynomial, graph partitioning problems, optimization problems, computational complexity, convex relaxations]
Learning-Graph-Based Quantum Algorithm for k-Distinctness
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We present a quantum algorithm solving the k-distinctness problem in a less number of queries than the previous algorithm by Ambainis. The construction uses a modified learning graph approach. Compared to the recent paper by Belovs and Lee, the algorithm doesn't require any prior information on the input, and the complexity analysis is much simpler.
[Algorithm design and analysis, Input variables, graph theory, complexity analysis, query complexity, Complexity theory, queries, Learning systems, Quantum computing, learning-graph-based quantum algorithm, k-distinctness problem, Loading, quantum computing, Silicon, learning (artificial intelligence), computational complexity]
A PTAS for Computing the Supremum of Gaussian Processes
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We give a polynomial time approximation scheme (PTAS) for computing the supremum of a Gaussian process. That is, given a finite set of vectors V &#x2286; Rd, we compute a (1+&#x03B5;)-factor approximation to E<sub>X&#x2190;N</sub>d[sup<sub>v&#x2208;V</sub> |&#x2329;v, X&#x232A;|] deterministically in time poly(d) &#x00B7; |V|(O<sub>&#x03B5;</sub>)(1). Previously, only a constant factor deterministic polynomial time approximation algorithm was known due to the work of Ding, Lee and Peres [1]. This answers an open question of Lee [2] and Ding [3]. The study of supremum of Gaussian processes is of considerable importance in probability with applications in functional analysis, convex geometry, and in light of the recent breakthrough work of Ding, Lee and Peres [1], to random walks on finite graphs. As such our result could be of use elsewhere. In particular, combining with the recent work of Ding [3], our result yields a PTAS for computing the cover time of bounded degree graphs. Previously, such algorithms were known only for trees. Along the way, we also give an explicit oblivious estimator for semi-norms in Gaussian space with optimal query complexity. Our algorithm and its analysis are elementary in nature using two classical comparison inequalities in convex geometry- Slepian's lemma and Kanter's lemma.
[Algorithm design and analysis, classical comparison inequalities, constant factor deterministic polynomial time approximation algorithm, functional analysis, bounded degree graphs, Gaussian distribution, Approximation methods, trees, epsilon-nets, PTAS, gaussian processes, factor approximation, Polynomials, approximation theory, random walks, cover time, probability, trees (mathematics), Vectors, Slepian lemma, Kanter lemma, optimal query complexity, vectors, majorizing measures, seminorm explicit oblivious estimator, Gaussian processes, Approximation algorithms, geometry, convex geometry, finite graphs, Gaussian process supremum computation, computational complexity]
From the Impossibility of Obfuscation to a New Non-Black-Box Simulation Technique
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The introduction of a non-black-box simulation technique by Barak (FOCS 2001) has been a major landmark in cryptography, breaking the previous barriers of black-box impossibility. Barak's techniques were subsequently extended and have given rise to various powerful applications. We present the first non-black-box simulation technique that does not rely on Barak's technique (or on nonstandard assumptions). Our technique is based on essentially different tools: it does not invoke universal arguments, nor does it rely on collision-resistant hashing. Instead, the main ingredient we use is the impossibility of general program obfuscation (Barak et al., CRYPTO 2001). Using our technique, we construct a new resettably-sound zero-knowledge (rsZK) protocol. rsZK protocols remain sound even against cheating provers that can repeatedly reset the verifier to its initial state and random tape. Indeed, for such protocols black-box simulation is impossible. Our rsZK protocol is the first to be based solely on semi-honest oblivious transfer and does not rely on collision-resistant hashing; in addition, our protocol does not use PCP machinery. In the converse direction, we show a generic transformation from any rsZK protocol to a family of functions that cannot be obfuscated.
[Protocols, cryptographic protocols, Computational modeling, rsZK protocols, resettable-security, semihonest oblivious transfer, general program obfuscation impossibility, cryptography, nonblack-box simulation technique, zero-knowledge, Machinery, resettably-sound zero-knowledge protocol, Privacy, Barak techniques, Abstracts, non-black-box-simulation, black-box impossibility, Cryptography, cheating provers]
A Polylogarithmic Approximation Algorithm for Edge-Disjoint Paths with Congestion 2
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In the Edge-Disjoint Paths with Congestion problem (EDPwC), we are given an undirected n-vertex graph G, a collection M = {(s<sub>1</sub>, t<sub>1</sub>),..., (s<sub>k</sub>, t<sub>k</sub>)} of demand pairs and an integer c. The goal is to connect the maximum possible number of the demand pairs by paths, so that the maximum edge congestion - the number of paths sharing any edge - is bounded by c. When the maximum allowed congestion is c = 1, this is the classical Edge-Disjoint Paths problem (EDP). The best current approximation algorithm for EDP achieves an O(&#x221A;n)-approximation, by rounding the standard multicommodity How relaxation of the problem. This matches the &#x03A9;(&#x221A;n) lower bound on the integrality gap of this relaxation. We show an O(poly log k)-approximation algorithm for EDPwC with congestion c = 2, by rounding the same multi-commodity How relaxation. This gives the best possible congestion for a sub-polynomial approximation of EDPwC via this relaxation. Our results are also close to optimal in terms of the number of pairs routed, since EDPwC is known to be hard to approximate to within a factor of &#x03A9;&#x0305;(log n)1/(c+1)) for any constant congestion c. Prior to our work, the best approximation factor for EDPwC with congestion 2 was O&#x0305;(n3/7), and the best algorithm achieving a polylogarithmic approximation required congestion 14.
[network routing, graph theory, Optimized production technology, edge-disjoint paths, standard multicommodity how relaxation, Routing, approximation algorithms, Approximation methods, Standards, polylogarithmic approximation algorithm, edge-disjoint paths with congestion problem, undirected n-vertex graph, EDPwC, subpolynomial approximation, polynomial approximation, Clustering algorithms, Games, Approximation algorithms, maximum edge congestion]
A Multi-prover Interactive Proof for NEXP Sound against Entangled Provers
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We prove a strong limitation on the ability of entangled provers to collude in a multiplayer game. Our main result is the first nontrivial lower bound on the class MIP* of languages having multi-prover interactive proofs with entangled provers, namely MIP* contains NEXP, the class of languages decidable in non-deterministic exponential time. While Babai, Fort now, and Lund (Computational Complexity 1991) proved the celebrated equality MIP = NEXP in the absence of entanglement, ever since the introduction of the class MIP* it was open whether shared entanglement between the provers could weaken or strengthen the computational power of multi-prover interactive proofs. Our result shows that it does not weaken their computational power: MIP* contains MIP. At the heart of our result is a proof that Babai, Fort now, and Lund's multilinearity test is sound even in the presence of entanglement between the provers, and our analysis of this test could be of independent interest. As a byproduct we show that the correlations produced by any entangled strategy which succeeds in the multilinearity test with high probability can always be closely approximated using shared randomness alone.
[entangled provers, Protocols, Correlation, quantum entanglement, Complexity theory, NEXP sound, languages decidability, decidability, Polynomials, theorem proving, multiplayer game, nondeterministic exponential time, formal languages, Quantum entanglement, probability, multiprover interactive proof, game theory, entanglement, quantum interactive proofs, entangled strategy, MIP, Linearity, Games, Lund multilinearity test, multiple provers, computational complexity]
Beck's Three Permutations Conjecture: A Counterexample and Some Consequences
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Given three permutations on the integers 1 through n, consider the set system consisting of each interval in each of the three permutations. In 1982, Beck conjectured that the discrepancy of this set system is O(1). In other words, the conjecture says that each integer from 1 through n can be colored either red or blue so that the number of red and blue integers in each interval of each permutations differs only by a constant. (The discrepancy of a set system based on two permutations is at most two.) Our main result is a counterexample to this conjecture: for any positive integer n = 3k, we construct three permutations whose corresponding set system has discrepancy &#x03A9;(log n). Our counterexample is based on a simple recursive construction, and our proof of the discrepancy lower bound is by induction. This construction also disproves a generalization of Beck's conjecture due to Spencer, Srinivasan and Tetali, who conjectured that a set &#x221A; system corresponding to &#x00A3; permutations has discrepancy O(&#x221A;&#x2113;). Our work was inspired by an intriguing paper from SODA 2011 by Eisenbrand, Palvolgyi and Rothvo&#x00DF;, who show a surprising connection between the discrepancy of three permutations and the bin packing problem: They show that Beck's conjecture implies a constant worst-case bound on the additive integrality gap for the Gilmore-Gomory LP relaxation for bin packing in the special case when all items have sizes strictly between 1/4 and 1/2, also known as the three partition problem. Our counterexample shows that this approach to bounding the additive integrality gap for bin packing will not work. We can, however, prove an interesting implication of our construction in the reverse direction: there are instances of bin packing and corresponding optimal basic feasible solutions for the Gilmore-Gomory LP relaxation such that any packing that contains only patterns from the support of these solutions requires at least opt + &#x03A9;(log m) bins, where m is the number of items. Finally, we discuss some implications that our construction has for other areas of discrepancy theory.
[additive integrality gap, Additives, Law, positive integer, simple recursive construction, discrepancy lower bound, set system discrepancy, constant worst-case bound, Educational institutions, Beck three permutations conjecture, red integers, Partitioning algorithms, Electronic mail, set theory, permutations, bin packing, Gilmore-Gomory LP relaxation, Computer science, Upper bound, discrepancy, three partition problem, blue integers, bin packing problem, computational complexity]
Iterative Rounding Approximation Algorithms for Degree-Bounded Node-Connectivity Network Design
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We consider the problem of finding a minimum edge cost subgraph of an undirected or a directed graph satisfying given connectivity requirements and degree bounds b(&#x00B7;) on nodes. We present an iterative rounding algorithm for this problem. When the graph is undirected and the connectivity requirements are on the element-connectivity with maximum value k, our algorithm computes a solution that is an O(k)-approximation for the edge cost in which the degree of each node v is at most O(k) &#x00B7; b(v). We also consider the no edge cost case where the objective is to find a subgraph satisfying connectivity requirements and degree bounds. Our algorithm for this case outputs a solution in which the degree of each node v is at most 6&#x00B7;b(v)+O(k2). These algorithms can be extended to other well-studied undirected node-connectivity requirements such as uniform, subset and rooted connectivity. When the graph is directed and the connectivity requirement is k-out-connectivity from a root, our algorithm computes a solution that is a 2-approximation for the edge cost in which the degree of each node v is at most 2 &#x00B7; b(v) + O(k).
[approximation theory, iterative methods, degree-bounded node-connectivity network design, iterative rounding, undirected node-connectivity requirements, subset connectivity, rooted connectivity, iterative rounding approximation algorithms, element-connectivity, directed graphs, minimum edge cost subgraph, uniform connectivity, undirected graph, network design, node-connectivity, k-out-connectivity]
LP Rounding for k-Centers with Non-uniform Hard Capacities
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In this paper we consider a generalization of the classical k-center problem with capacities. Our goal is to select k centers in a graph, and assign each node to a nearby center, so that we respect the capacity constraints on centers. The objective is to minimize the maximum distance a node has to travel to get to its assigned center. This problem is NP-hard, even when centers have no capacity restrictions and optimal factor 2 approximation algorithms are known. With capacities, when all centers have identical capacities, a 6 approximation is known with no better lower bounds than for the infinite capacity version. While many generalizations and variations of this problem have been studied extensively, no progress was made on the capacitated version for a general capacity function. We develop the first constant factor approximation algorithm for this problem. Our algorithm uses an LP rounding approach to solve this problem, and works for the case of non-uniform hard capacities, when multiple copies of a node may not be chosen and can be extended to the case when there is a hard bound on the number of copies of a node that may be selected. Finally, for non-uniform soft capacities we present a much simpler 11-approximation algorithm, which we find as one more evidence that hard capacities are much harder to deal with.
[optimal factor 2 approximation algorithms, graph theory, capacity constraints, approximation algorithms, Approximation methods, facility location, LP rounding, LP rounding approach, classical k-center problem, nonuniform hard capacity, Polynomials, general capacity function, approximation theory, 11-approximation algorithm, maximum distance minimization, Educational institutions, non-uniform capacities, Standards, graph, facility location problem, hard capacities, Computer science, nonuniform soft capacity, NP-hard problem, Awards activities, Approximation algorithms, computational complexity, first constant factor approximation algorithm, k-center]
On-Line Indexing for General Alphabets via Predecessor Queries on Subsets of an Ordered List
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The problem of Text Indexing is a fundamental algorithmic problem in which one wishes to preprocess a text in order to quickly locate pattern queries within the text. In the ever evolving world of dynamic and on-line data, there is also a need for developing solutions to index texts which arrive online, i.e. a character at a time, and still be able to quickly locate said patterns. In this paper, a new solution for on-line indexing is presented by providing an on-line suffix tree construction in O(log log n + log log |&#x03A3;|) worst-case expected time per character, where n is the size of the string, and &#x03A3; is the alphabet. This improves upon all previously known on-line suffix tree constructions for general alphabets, at the cost of having the run time in expectation. The main idea is to reduce the problem of constructing a suffix tree on-line to an interesting variant of the order maintenance problem, which may be of independent interest. In the famous order maintenance problem, one wishes to maintain a dynamic list L of size n under insertions, deletions, and order queries. In an order query, one is given two nodes from L and must determine which node precedes the other in L. In an extension to this problem, named the Predecessor search on Dynamic Subsets of an Ordered Dynamic List problem (POLP for short), it is also necessary to maintain dynamic subsets S<sub>1</sub>, &#x00B7; &#x00B7; &#x00B7; , S<sub>k</sub> &#x2286; L, such that given some u &#x2208; L it will be possible to quickly locate the predecessor of u in Si, for any integer 1 &#x2264; i &#x2264; k. This paper provides an efficient data structure capable of locating the predecessor of u in Si in O(log log n) worst-case time and answering order queries on L in O(1) worst-case time, while allowing updates to L in O(1) worst-case expected time and updates to the subsets in O(log log n) worst-case expected time. This improves over a previous data structure which may be implicitly obtained from Dietz [8], in which the updates to the sets and L are done in O(log log n) amortized expected time. In addition, the bounds shown here match the currently best known bounds for predecessor search in the RAM model. Furthermore, this paper improves or simplifies bounds for several additional applications, including fully-persistent arrays, the monotonic list labeling problem, and the Order-Maintenance Problem.
[order-maintenance, text analysis, pattern matching, Heuristic algorithms, insertions, deletions, Random access memory, general alphabets, predecessor, fully-persistent arrays, query processing, text indexing, predecessor search on dynamic subsets of an ordered dynamic list problem, algorithmic problem, online suffix tree construction, Silicon, data structures, predecessor queries, RAM model, formal languages, indexing, trees (mathematics), POLP, order query answering, Maintenance engineering, online indexing, ordered list subset, order maintenance problem, monotonic list labeling problem, pattern queries, suffix tree, Arrays, Indexing]
Higher Cell Probe Lower Bounds for Evaluating Polynomials
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In this paper, we study the cell probe complexity of evaluating an n-degree polynomial P over a finite field F of size at least n1+&#x03A9;(1). More specifically, we show that any static data structure for evaluating P(x), where x &#x2208; F, must use &#x03A9;(lg |F|/ lg(Sw/n lg |F|)) cell probes to answer a query, where S denotes the space of the data structure in number of cells and w the cell size in bits. This bound holds in expectation for randomized data structures with any constant error probability &#x03B4; &lt;; 1/2. Our lower bound not only improves over the &#x03A9;(lg |F|/ lg S) lower bound of Miltersen [TCS'95], but is in fact the highest static cell probe lower bound to date: For linear space (i.e. S = O(n lg |F|/w)), our query time lower bound simplifies to &#x03A9;(lg |F|), whereas the highest previous lower bound for any static data structure problem having d different queries is &#x03A9;(lg d/ lg lg d), which was first achieved by Pa&#x0301;trascu and Thorup [SICOMP'10]. We also use the recent technique of Larsen [STOC'12] to show a lower bound of tq = &#x03A9;(lg |F| lg n/lg(wtu/ lg |F|) lg(wt<sub>u</sub>)) for dynamic data structures for polynomial evaluation over a finite field F of size &#x03A9;(n2). Here t<sub>q</sub> denotes the expected query time and tu the worst case update time. This lower bound holds for randomized data structures with any constant error probability &#x03B4; &lt;; 1/2. This is only the second time a lower bound beyond max{t<sub>u</sub>, t<sub>q</sub>} = &#x03A9;(max{lg n, lg d/ lg lg d}) has been achieved for dynamic data structures, where d denotes the number of different queries and updates to the problem. Furthermore, it is the first such lower bound that holds for randomized data structures with a constant probability of error.
[cell probe complexity, worst case update time, Error probability, n-degree polynomial, randomized data structures, Complexity theory, dynamic data structures, query processing, Polynomials, data structures, Probes, static data structure, error statistics, polynomials, error probability, query time, Data structures, Encoding, polynomial evaluation, lower bounds, higher cell probe lower bounds, query answering, finite field, cell probe model, Data models, computational complexity]
The Tile Assembly Model is Intrinsically Universal
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We prove that the abstract Tile Assembly Model (aTAM) of nanoscale self-assembly is intrinsically universal. This means that there is a single tile assembly system U that, with proper initialization, simulates any tile assembly system T. The simulation is "intrinsic" in the sense that the self-assembly process carried out by U is exactly that carried out by T, with each tile of T represented by an m &#x00D7; m "super tile" of U. Our construction works for the full aTAM at any temperature, and it faithfully simulates the deterministic or nondeterministic behavior of each T. Our construction succeeds by solving an analog of the cell differentiation problem in developmental biology: Each super tile of U, starting with those in the seed assembly, carries the "genome" of the simulated system T. At each location of a potential super tile in the self-assembly of U, a decision is made whether and how to express this genome, i.e., whether to generate a super tile and, if so, which tile of T it will represent. This decision must be achieved using asynchronous communication under incomplete information, but it achieves the correct global outcome(s).
[nanoscale self-assembly, Assembly systems, cell differentiation problem, deterministic behavior simulation, tile self-assembly, Genomics, simulation, asynchronous communication, abstract tile assembly model, aTAM, nondeterministic behavior simulation, structural DNA nanotechnology, nanotechnology, Bioinformatics, seed assembly, Assembly, biocomputing, information-processing capabilities, Computational modeling, developmental biology, single tile assembly system, nucleic acids, Self-assembly, Tiles, DNA, intrinsic universality]
The Dynamics of Influence Systems
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Influence systems form a large class of multiagent systems designed to model how influence, broadly defined, spreads across a dynamic network. We build a general analytical framework which we then use to prove that, while Turing-complete, influence dynamics of the diffusive type is almost surely asymptotically periodic. Besides resolving the dynamics of a popular family of multiagent systems, the other contribution of this work is to introduce a new type of renormalization-based bifurcation analysis for multiagent systems.
[Chaos, multi-agent systems, general analytical framework, algorithmic calculus, influence system dynamics, Encoding, Orbits, renormalization-based bifurcation analysis, dynamic network, Convergence, Influence systems, Space vehicles, Turing machines, renormalization. multiagent systems, natural algorithms, Polynomials, Turing-complete, Decision trees, multiagent systems]
The Locality of Distributed Symmetry Breaking
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We present new bounds on the locality of several classical symmetry breaking tasks in distributed networks. A sampling of the results include 1) A randomized algorithm for computing a maximal matching (MM) in O(log &#x0394; + (log log n)4) rounds, where &#x0394; is the maximum degree. This improves a 25-year old randomized algorithm of Israeli and Itai that takes O(log n) rounds and is provably optimal for all log &#x0394; in the range [(log log n)4, &#x221A;log n]. 2) A randomized maximal independent set (MIS) algorithm requiring O(log &#x0394;&#x221A;log n) rounds, for all &#x0394;, and only 2O(&#x221A;log log n) rounds when &#x0394; = poly(log n). These improve on the 25-year old O(log n)-round randomized MIS algorithms of Luby and Alon, Babai, and Itai when log &#x0394; &#x226B; &#x221A;log n. 3) A randomized (&#x0394; + 1)-coloring algorithm requiring O(log &#x0394; + 2O((&#x221A;log log n)) rounds, improving on an algorithm of Schneider and Wattenhofer that takes O(log &#x0394; + &#x221A;log n) rounds. This result implies that an O(&#x0394;)-coloring can be computed in 2O(&#x221A;log log n) rounds for all &#x0394;, improving on Kothapalli et al.'s O(&#x221A;log n)-round algorithm. We also introduce a new technique for reducing symmetry breaking problems on low arboricity graphs to low degree graphs. Corollaries of this reduction include MM and MIS algorithms for low arboricity graphs (e.g., planar graphs and graphs that exclude any fixed minor) requiring O(&#x221A;log n) and O(log2/3 n) rounds w.h.p., respectively.
[Algorithm design and analysis, maximal matching round, Maximal Independent Set, low arboricity graphs, Electronic mail, randomized (&#x0394; + 1)-coloring algorithm, graph colouring, Israeli, Itai, Maximal Matching, MIS algorithm, round algorithm, distributed symmetry breaking locality, low degree graphs, Coloring, Computational modeling, Color, Educational institutions, randomized maximal independent set algorithm, classical symmetry breaking tasks, randomised algorithms, Computer science, distributed algorithms, distributed networks, Random variables]
How to Allocate Tasks Asynchronously
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Asynchronous task allocation is a fundamental problem in distributed computing in which p asynchronous processes must execute a set of m tasks. Also known as write-all or do-all, this problem been studied extensively, both independently and as a key building block for various distributed algorithms. In this paper, we break new ground on this classic problem: we introduce the To-Do Tree concurrent data structure, which improves on the best known randomized and deterministic upper bounds. In the presence of an adaptive adversary, the randomized To-Do Tree algorithm has O(m+p log p log2 m) work complexity. We then show that there exists a deterministic variant of the To-Do Tree algorithm with work complexity O(m+p log5 m log2 max(m, p)). For all values of m and p, our algorithms are within log factors of the O(m + p log p) lower bound for this problem. The key technical ingredient in our results is a new approach for analyzing concurrent executions against a strong adaptive scheduler. This technique allows us to handle the complex dependencies between the processes' coin flips and their scheduling, and to tightly bound the work needed to perform subsets of the tasks.
[Measurement, asynchronous task allocation, do-all, adaptive scheduler, Registers, Complexity theory, distributed computing, randomized upper bounds, to-do tree concurrent data structure, resource allocation, Bismuth, scheduling, tree data structures, deterministic upper bounds, randomized to-do tree algorithm, Radiation detectors, write-all, Data structures, deterministic algorithms, coin flip processing, randomised algorithms, task allocation, distributed algorithms, randomized algorithms, work complexity, asynchronous process, Resource management, computational complexity]
Tight Bounds for Randomized Load Balancing on Arbitrary Network Topologies
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We consider the problem of balancing load items (tokens) on networks. Starting with an arbitrary load distribution, we allow in each round nodes to exchange tokens with their neighbors. The goal is to achieve a distribution where all nodes have nearly the same number of tokens. For the continuous case where tokens are arbitrarily divisible, most load balancing schemes correspond to Markov chains whose convergence is fairly well-understood in terms of their spectral gap. However, in many applications load items cannot be divided arbitrarily and we need to deal with the discrete case where the load is composed of indivisible tokens. This discretization entails a non-linear behavior due to its rounding errors, which makes the analysis much harder than in the continuous case. Therefore, it has been a major open problem to understand the limitations of discrete load balancing and its relation to the continuous case. We investigate several randomized protocols for different communication models in the discrete case. Our results demonstrate that there is almost no difference between the discrete and continuous case. For instance, for any regular network in the matching model, all nodes have the same load up to an additive constant in (asymptotically) the same number of rounds required in the continuous case. This generalizes and tightens the previous best result, which only holds for expander graphs.
[Protocols, load balancing, graph theory, load items, indivisible tokens, Markov chains, Convergence, resource allocation, parallel and distributed algorithms, token exchange, arbitrary load distribution, matching model, protocols, randomized load balancing, Load modeling, discrete load balancing, nonlinear behavior, Vectors, randomized protocols, Upper bound, tight bounds, randomized algorithms, Markov processes, communication models, Load management, Integrated circuit modeling, expander graphs, graph expansion, arbitrary network topology]
On the Complexity of Finding Narrow Proofs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We study the complexity of the following "resolution width problem": Does a given 3-CNF formula have a resolution refutation of width k? For fixed k, refutations of width k can easily be found in polynomial time. We prove a matching polynomial lower bound for the resolution width problem that shows that there is no significant faster way to decide the existence of a width-k refutation than exhaustively searching for it. This lower bound is unconditional and does not rely on any unproven complexity theoretic assumptions. We also prove that the resolution width problem is EXPTIME-complete (if k is part of the input). This confirms a conjecture by Vardi, who has first raised the question for the complexity of the resolution width problem. Furthermore, we prove that the variant of the resolution width problem for regular resolution is PSPACE-complete, confirming a conjecture by Urquhart.
[complexity, EXPTIME-complete problem, resolution width, Heuristic algorithms, resolution width problem, Length measurement, PSPACE-complete, Complexity theory, 3-CNF formula, conjunctive normal form, resolution refutation, lower bounds, Computer science, matching polynomial lower bound, Turing machines, narrow proofs, Games, Polynomials, polynomial time, theorem proving, computational complexity, width-k refutation]
The Computational Hardness of Counting in Two-Spin Models on d-Regular Graphs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The class of two-spin systems contains several important models, including random independent sets and the Ising model of statistical physics. We show that for both the hard-core (independent set) model and the anti-ferromagnetic Ising model with arbitrary external field, it is NP-hard to approximate the partition function or approximately sample from the model on regular graphs when the model has non-uniqueness on the corresponding regular tree. Together with results of Jerrum -- Sinclair, Weitz, and Sinclair -- Srivastava -- Thurley giving FPRAS's for all other two-spin systems except at the uniqueness threshold, this gives an almost complete classification of the computational complexity of two-spin systems on bounded-degree graphs. Our proof establishes that the normalized log-partition function of any two-spin system on bipartite locally tree-like graphs converges to a limiting ``free energy density'' which coincides with the (non-rigorous) Be the prediction of statistical physics. We use this result to characterize the local structure of two-spin systems on locally tree-like bipartite expander graphs, which then become the basic gadgets in a randomized reduction to approximate MAX-CUT. Our approach is novel in that it makes no use of the second moment method employed in previous works on these questions.
[antiferromagnetic Ising model, regular tree, Bethe free energy, Predictive models, regular graphs, Approximation methods, independent set, Convergence, hard-core model, second moment method, function approximation, two-spin models, normalized log-partition function, Ising model, spin system, Bipartite graph, d-regular graphs, locally tree-like bipartite expander graphs, Computational modeling, trees (mathematics), partition function approximation, Physics, free energy density, MAX-CUT problem, statistical physics, NP-hard problem, bounded-degree graphs, bipartite locally tree-like graphs, random independent sets, counting computational hardness, computational complexity]
Making the Long Code Shorter
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The long code is a central tool in hardness of approximation, especially in questions related to the unique games conjecture. We construct a new code that is exponentially more efficient, but can still be used in many of these applications. Using the new code we obtain exponential improvements over several known results, including the following: 1) For any &#x03B5; &gt;; 0, we show the existence of an n vertex graph G where every set of o(n) vertices has expansion 1 - &#x03B5;, but G's adjacency matrix has more than exp(log&#x03B4; n) eigenvalues larger than 1 - &#x03B5;, where &#x03B4; depends only on &#x03B5;. This answers an open question of Arora, Barak and Steurer (FOCS 2010) who asked whether one can improve over the noise graph on the Boolean hypercube that has poly(log n) such eigenvalues. 2) A gadget that reduces unique games instances with linear constraints modulo K into instances with alphabet k with a blowup of Kpolylog(K), improving over the previously known gadget with blowup of 2&#x03A9;(K). 3) An n variable integrality gap for Unique Games that survives exp(poly(log log n)) rounds of the SDP + Sherali Adams hierarchy, improving on the previously known bound of poly(log log n). We show a connection between the local testability of linear codes and small set expansion in certain related Cayley graphs, and use this connection to derandomize the noise graph on the Boolean hypercube.
[eigenvalues, linear constraints modulo, vertex graph, Locally Testable Codes, graph theory, long code local testability, hypercube networks, Electronic mail, Approximation methods, Unique games conjecture, eigenvalues and eigenfunctions, unique games conjecture, adjacency matrix, Small set expansion, Long Code, Hypercubes, Eigenvalues and eigenfunctions, Polynomials, approximation theory, game theory, Boolean algebra, Noise measurement, noise graph, matrix algebra, approximation hardness, SDP-Sherali Adams hierarchy, Games, variable integrality gap, Cayley graphs, Boolean hypercube]
Hardness of Finding Independent Sets in Almost q-Colorable Graphs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We show that for any &#x03B5; &gt;; 0, and positive integers k and q such that q &#x2265; 2k + 1, given a graph on N vertices that has a q-colorable induced subgraph of (1 - &#x03B5;)N vertices, it is NP-hard to find an independent set of N/qk+1 vertices. This substantially improves upon the work of Dinur et al. [1] who gave a corresponding bound of N/q2. Our result implies that for any positive integer k, given a graph that has an independent set of &#x2248; (2k + 1)-1 fraction of vertices, it is NP-hard to find an independent set of (2k + 1)-(k+1) fraction of vertices. This improves on the previous work of Engebretsen and Holmerin [2] who proved a gap of &#x2248; 2-k vs 2-(k:2), which is best possible using techniques (including those of [2]) based on the query efficient PCP of Samorodnitsky and Trevisan [3].
[vertex fraction, Coloring, Independent-Set, Color, Electronic mail, Hardness, graph colouring, Computer science, Coordinate measuring machines, Upper bound, q-colorable induced subgraph, NP-hard problem, USA Councils, independent set finding hardness, Approximation algorithms, Graphs, PCP, computational complexity]
Population Recovery and Partial Identification
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We study several problems in which an unknown distribution over an unknown population of vectors needs to be recovered from partial or noisy samples, each of which nearly completely erases or obliterates the original vector. For example, consider a distribution p over a population V &#x2286; {0, 1}n. A noisy sample v' is obtained by choosing v according to p and flipping each coordinate of v with probability say 0.49 independently. The problem is to recover V, p as efficiently as possible from noisy samples. Such problems naturally arise in a variety of contexts in learning, clustering, statistics, computational biology, data mining and database privacy, where loss and error may be introduced by nature, inaccurate measurements, or on purpose. We give fairly efficient algorithms to recover the data under fairly general assumptions. Underlying our algorithms is a new structure we call a partial identification (PID) graph for an arbitrary finite set of vectors over any alphabet. This graph captures the extent to which certain subsets of coordinates in each vector distinguish it from other vectors. PID graphs yield strategies for dimension reductions and re-assembly of statistical information. The quality of our algorithms (sequential and parallel runtime, as well as numerical stability) critically depends on three parameters of PID graphs: width, depth and cost. The combinatorial heart of this work is showing that every set of vectors posses a PID graph in which all three parameters are small (we prove some limitations on their trade-offs as well). We further give an efficient algorithm to find such near-optimal PID graphs for any set of vectors. Our efficient PID graphs imply general algorithms for these recovery problems, even when loss or noise are just below the information-theoretic limit! In the learning/clustering context this gives a new algorithm for learning mixtures of binomial distributions (with known marginals) whose running time depends only quasi-polynomially on the number of clusters. We discuss implications to privacy and coding as well.
[Noise, graph theory, population recovery, dimension reductions, privacy, near-optimal PID graphs, alphabet, depth parameter, coding, Databases, data recovery, Sociology, parallel runtime algorithm, Polynomials, learning (artificial intelligence), vector population, binomial distribution, sequential algorithm, parallel algorithms, information recovery, partial identification graph, Vectors, clustering context, statistical information reassembly, Noise measurement, binomial distributions, Statistics, vectors, noisy data, pattern clustering, width parameter, cost parameter, statistical analysis, learning theory, learning context]
The Privacy of the Analyst and the Power of the State
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We initiate the study of "privacy for the analyst" in differentially private data analysis. That is, not only will we be concerned with ensuring differential privacy for the data (i.e. individuals or customers), which are the usual concern of differential privacy, but we also consider (differential) privacy for the set of queries posed by each data analyst. The goal is to achieve privacy with respect to other analysts, or users of the system. This problem arises only in the context of stateful privacy mechanisms, in which the responses to queries depend on other queries posed (a recent wave of results in the area utilized cleverly coordinated noise and state in order to allow answering privately hugely many queries). We argue that the problem is real by proving an exponential gap between the number of queries that can be answered (with non-trivial error) by stateless and stateful differentially private mechanisms. We then give a stateful algorithm for differentially private data analysis that also ensures differential privacy for the analyst and can answer exponentially many queries.
[Algorithm design and analysis, Data privacy, data analysis, stateless differentially private mechanisms, analyst privacy, query processing, query answering, Privacy, Program processors, long code, Databases, Games, list decoding, stateful differentially private mechanisms, Libraries, data privacy, private data analysis, state power, differential data privacy, differential privacy, query set]
The Johnson-Lindenstrauss Transform Itself Preserves Differential Privacy
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
This paper proves that an "old dog\
[normal Gaussian process, Noise, graph theory, Transforms, transforms, Johnson-Lindenstrauss transform, query processing, Privacy, Databases, matrix dimensions, bounded norm rank-1 matrix, vector, covariance matrices, Differential privacy, Laplace equations, edge differential privacy preservation, JL transform, cut-query answering, Vectors, sanitized graph, Graph cuts, sanitized covariance matrix, vectors, Gaussian processes, Approximation algorithms, cut-query approximation, additive noise, data privacy]
On Range Searching with Semialgebraic Sets II
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Let P be a set of n points in Rd. We present a linear-size data structure for answering range queries on P with constant-complexity semialgebraic sets as ranges, in time close to O(n1-1/d). It essentially matches the performance of similar structures for simplex range searching, and, for d &#x2265; 5, significantly improves earlier solutions by the first two authors obtained in 1994. This almost settles a long-standing open problem in range searching. The data structure is based on the polynomial-partitioning technique of Guth and Katz [arXiv:1011.4105], which shows that for a parameter r, 1 &lt;; r &#x2264; n, there exists a d-variate polynomial f of degree O(r1/d) such that each connected component of Rd \\ Z(f) contains at most n/r points of P, where Z(f) is the zero set of f. We present an efficient randomized algorithm for computing such a polynomial partition, which is of independent interest and is likely to have additional applications.
[computational geometry, Search problems, Range searching, set theory, query processing, ham-sandwich cuts, polynomial-partitioning technique, Bismuth, Polynomials, data structures, search problems, open problem, polynomials, Data structures, Educational institutions, linear-size data structure, Partitioning algorithms, randomized algorithm, polynomial partition, randomised algorithms, Geometry, semialgebraic sets, constant-complexity semialgebraic sets, simplex range searching, d-variate polynomial, range query answering, computational complexity]
Down the Rabbit Hole: Robust Proximity Search and Density Estimation in Sublinear Space
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
For a set of n points in Rd, and parameters k and e, we present a data structure that answers (1 + e)-approximate k nearest neighbor queries in logarithmic time. Surprisingly, the space used by the data-structure is O&#x0303;(n/k), that is, the space used is sub linear in the input size if k is sufficiently large. Our approach provides a novel way to summarize geometric data, such that meaningful proximity queries on the data can be carried out using this sketch. Using this we provide a sub linear space data-structure that can estimate the density of a point set under various measures, including: (i) sum of distances of k closest points to the query point, and (ii) sum of squared distances of k closest points to the query point. Our approach generalizes to other distance based estimation of densities of similar flavor.
[point set density estimation, approximation theory, pattern classification, k closest points sum-of-squared distances, rabbit hole, k closest points sum-of-distances, Estimation, Artificial neural networks, computational geometry, k nearest neighbor queries, Complexity theory, set theory, Approximation methods, sublinear space data-structure, logarithmic time, Standards, query processing, Clustering algorithms, Approximation algorithms, robust proximity search, geometric data summarization, data structures, proximity queries, computational complexity]
On the Homotopy Test on Surfaces
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Let G be a graph cellularly embedded in a surface S. Given two closed walks c and d in G, we take advantage of the RAM model to describe linear time algorithms to decide if c and d are homotopic in S, either freely or with fixed base point. After O(|G|) time preprocessing independent of c and d, our algorithms answer the homotopy test in O(|c| + |d|) time, where |G|, |c| and |d| are the respective numbers of edges of G, c and d. These results were previously announced by Dey and Guha (1999). Their approach was based on small cancellation theory from combinatorial group theory. However, several flaws in their algorithms make their approach fail, leaving the complexity of the homotopy test problem still open. We present a geometric approach, based on previous works by Colin de Verdie&#x0300;re and Erickson, that provides optimal homotopy tests.
[RAM model, geometric approach, Computational modeling, graph theory, Random access memory, computational geometry, Generators, Complexity theory, Topology, curve homotopy, combinatorial surface, fixed base point, linear time algorithms, combinatorial group theory, group theory, homotopy test problem, computational topology, cancellation theory, Face, Mirrors, computational complexity]
Representative Sets and Irrelevant Vertices: New Tools for Kernelization
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The existence of a polynomial kernel for Odd Cycle Transversal was a notorious open problem in parameterized complexity. Recently, this was settled by the present authors (Kratsch and Wahlstrom, SODA 2012), with a randomized polynomial kernel for the problem, using matroid theory to encode How questions over a set of terminals in size polynomial in the number of terminals (rather than the total graph size, which may be superpolynomially larger). In the current work we further establish the usefulness of matroid theory to kernelization by showing applications of a result on representative sets due to Lovasz (Combinatorial Surveys 1977) and Marx (TCS 2009). We show how representative sets can be used to give a polynomial kernel for the elusive Almost 2-sat problem (where the task is to remove at most k clauses to make a 2-CNF formula satisfiable), solving a major open problem in kernelization. We further apply the representative sets tool to the problem of finding irrelevant vertices in graph cut problems, that is, vertices which can be made undeletable without affecting the status of the problem. This gives the first significant progress towards a polynomial kernel for the Multiway Cut problem; in particular, we get a polynomial kernel for Multiway Cut instances with a bounded number of terminals. Both these kernelization results have significant spin-off effects, producing the first polynomial kernels for a range of related problems. More generally, the irrelevant vertex results have implications for covering min-cuts in graphs. In particular, given a directed graph and a set of terminals, we can find a set of size polynomial in the number of terminals (a cut-covering set) which contains a minimum vertex cut for every choice of sources and sinks from the terminal set. Similarly, given an undirected graph and a set of terminals, we can find a set of vertices, of size polynomial in the number of terminals, which contains a minimum multiway cut for every partition of the terminals into a bounded number of sets. Both results are polynomial time. We expect this to have further applications; in particular, we get direct, reduction rule-based kernelizations for all problems above, in contrast to the indirect compression-based kernel previously given for Odd Cycle Transversal. All our results are randomized, with failure probabilities which can be made exponentially small in the size of the input, due to needing a representation of a matroid to apply the representative sets tool.
[polynomial time preprocessing, odd cycle transversal, reduction rule-based kernelizations, computability, parameterized complexity, multiway cut problem, graph cut problems, Complexity theory, Electronic mail, set theory, Approximation methods, spin-off effects, Runtime, directed graph, Polynomials, terminal set, undirected graph, failure probabilities, Kernel, open problem, matroid theory, 2-CNF formula, almost 2-sat problem, min-cuts covering, Particle separators, polynomials, randomized polynomial kernel, kernelization, matrix algebra, minimum vertex cut, almost 2-sat, matroids, directed graphs, irrelevant vertices, representative sets tool, graph cuts, computational complexity, multiway cut]
Designing FPT Algorithms for Cut Problems Using Randomized Contractions
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We introduce a new technique for designing fixed-parameter algorithms for cut problems, namely randomized contractions. With our framework: (1) We obtain the first FPT algorithm for the parameterized version of the UNIQUE LABEL COVER problem, with single exponential dependency on the size of the cutset and the size of the alphabet. As a consequence, we extend the set of the polynomial time solvable instances of UNIQUE GAMES to those with at most O(&#x221A;{log n}) violated constraints. (2) We obtain a new FPT algorithm for the STEINER CUT problem with exponential speed-up over the recent work of Kawarabayashi and Thorup (FOCS'11). (3) We show how to combine considering 'cut' and 'uncut' constraints at the same time. We define a robust problem NODE MULTIWAY CUT-UNCUT that can serve as an abstraction of introducing uncut constraints, and show that it admits an FPT algorithm with single exponential dependency on the size of the cutset. To the best of our knowledge, the only known way of tackling uncut constraints was via the approach of Marx, O'Sullivan and Razgon (STACS'10), which yields algorithms with double exponential running time. An interesting aspect of our algorithms is that they can handle real weights, to the best of our knowledge, the technique of important separators does not work in the weighted version.
[Algorithm design and analysis, randomized contractions, graph theory, single exponential dependency, uncut constraints, double exponential running time, graph cut problems, Steiner cut problem, Complexity theory, Electronic mail, unique games, fixed-parameter tractability algorithms, alphabet size, NP-hard problems, Polynomials, polynomial time, cut constraints, formal languages, Particle separators, Color, game theory, FPT algorithms, Educational institutions, cutset size, unique label cover, node multiway cut-uncut, unique label cover problem, randomised algorithms, fixed parameter tractability, computational complexity]
Planar F-Deletion: Approximation, Kernelization and Optimal FPT Algorithms
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Let F be a finite set of graphs. In the F-DELETION problem, we are given an n-vertex graph G and an integer k as input, and asked whether at most k vertices can be deleted from G such that the resulting graph does not contain a graph from F as a minor. F-DELETION is a generic problem and by selecting different sets of forbidden minors F, one can obtain various fundamental problems such as VERTEX COVER, FEEDBACK VERTEX SET or TREEWIDTH &#x03B7;-DELETION. In this paper we obtain a number of generic algorithmic results about F-DELETION, when F contains at least one planar graph. The highlights of our work are &#x00B7; A constant factor approximation algorithm for the optimization version of F-DELETION; &#x00B7; A linear time and single exponential parameterized algorithm, that is, an algorithm running in time O(2O(k)n), for the parameterized version of F-DELETION where all graphs in F are connected; &#x00B7; A polynomial kernel for parameterized F-DELETION. These algorithms unify, generalize, and improve a multitude of results in the literature. Our main results have several direct applications, but also the methods we develop on the way have applicability beyond the scope of this paper. Our results - constant factor approximation, polynomial kernelization and FPT algorithms - are stringed together by a common theme of polynomial time preprocessing.
[polynomial time preprocessing, algorithms, generic algorithmic, Complexity theory, set theory, planar graph, Approximation methods, Optimization, planar F-deletion problem, feedback vertex set, optimisation, linear time algorithm, graphs, Polynomials, Kernel, finite graph set, single exponential parameterized algorithm, approximation theory, fixed-parameter tractable, approximation, vertex cover, polynomials, trees (mathematics), treewidth &#x03B7;-deletion, fpt, Indexes, kernelization, optimization version, f-deletion, optimal FPT algorithm, n-vertex graph, Approximation algorithms, polynomial kernelization algorithm, computational complexity, constant factor approximation algorithm]
Approximation Limits of Linear Programs (Beyond Hierarchies)
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We develop a framework for proving approximation limits of polynomial-size linear programs from lower bounds on the nonnegative ranks of suitably defined matrices. This framework yields unconditional impossibility results that are applicable to any linear program as opposed to only programs generated by hierarchies. Using our framework, we prove that quadratic approximations for CLIQUE require linear programs of exponential size. (This lower bound applies to linear programs using a certain encoding of CLIQUE as a linear optimization problem) Moreover, we establish a similar result for approximations of semi definite programs by linear programs. Our main technical ingredient is a quantitative improvement of Razborov's rectangle corruption lemma (1992) for the high error regime, which gives strong lower bounds on the nonnegative rank of certain perturbations of the unique disjoint ness matrix.
[nonnegative ranks, CLIQUE, semi definite programs, linear programming, Complexity theory, approximation algorithms, Approximation methods, communication complexity, Polynomials, unique disjointness matrix, approximation limits, unconditional impossibility, polynomial-size linear programs, linear optimization problem, approximation theory, polynomials, Razborov rectangle corruption lemma, Linear programming, Encoding, Vectors, nonnegative rank, polyhedral combinatorics, matrix algebra, extended formulations, Approximation algorithms, quadratic approximations, nonnegative perturbation rank]
Formulas Resilient to Short-Circuit Errors
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We show how to efficiently convert any boolean formula F into a boolean formula E that is resilient to short-circuit errors (as introduced by Kleitman et al. [KLM94]). A gate has a short-circuit error when the value it computes is replaced by the value of one of its inputs. We guarantee that E computes the same function as F, as long as at most (1/10 - &#x03B5;) of the gates on each path from the output to an input have been corrupted in E. The corruptions may be chosen adversarially, and may depend on the formula E and even on the input. We obtain our result by extending the Karchmer-Wigderson connection between formulas and communication protocols to the setting of adversarial error. This enables us to obtain error-resilient formulas from error-resilient communication protocols.
[circuit complexity, Karchmer-Wigderson connection, Protocols, formal languages, short-circuit errors, Computational modeling, directed acyclic graph, Circuit faults, error-resilient formulas, Boolean functions, polynomial time computable function, directed graphs, error-resilient communication protocols, Boolean formula, Games, Logic gates, adversarial error, protocols, Integrated circuit modeling]
Lower Bounds on Information Complexity via Zero-Communication Protocols and Applications
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We show that almost all known lower bound methods for communication complexity are also lower bounds for the information complexity. In particular, we define a relaxed version of the partition bound of Jain and Klauck and prove that it lower bounds the information complexity of any function. Our relaxed partition bound subsumes all norm based methods (e.g. the &#x03B3;2 method) and rectangle-based methods (e.g. the rectangle/corruption bound, the smooth rectangle bound, and the discrepancy bound), except the partition bound. Our result uses a new connection between rectangles and zero-communication protocols where the players can either output a value or abort. We prove the following compression lemma: given a protocol for a function f with information complexity I, one can construct a zero-communication protocol that has non-abort probability at least 2-O(I) and that computes f correctly with high probability conditioned on not aborting. Then, we show how such a zero-communication protocol relates to the relaxed partition bound. We use our main theorem to resolve three of the open questions raised by Braver man. First, we show that the information complexity of the Vector in Subspace Problem is O(n1/3), which, in turn, implies that there exists an exponential separation between quantum communication complexity and classical information complexity. Moreover, we provide an O(n) lower bound on the information complexity of the Gap Hamming Distance Problem.
[Protocols, partition bound, nonabort probability, Complexity theory, Electronic mail, communication complexity, subspace problem, Integrated circuits, gap Hamming distance problem, norm based methods, smooth rectangle bound, information theory, protocols, quantum communication, lower bound methods, zero-communication protocols, rectangle-based methods, probability, quantum communication complexity, relaxed partition bound, exponential separation, Vectors, lemma compression, &#x03B3;2 method, information complexity, Quantum mechanics, Random variables, corruption bound, discrepancy bound]
Rarity for Semimeasures
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The notion of Kolmogorov-Martin-Lof Random sequences is extended from computable to enumerable distributions. This allows definitions of various other properties, such as mutual information in infinite sequences. Enumerable distributions (as well as distributions faced in some finite multi-party settings) are semi measures, handling those requires care.
[complexity, finite multiparty settings, Lattices, semimeasures, infinite sequences, computability, Kolmogorov-Martin-Lof random sequences, Educational institutions, Probability distribution, Complexity theory, set theory, statistical distributions, Computer science, random sequences, Kolmogorov complexity, probability distribution, Nickel, mutual information, Mutual information, computational complexity, enumerable distributions]
Faster Algorithms for Rectangular Matrix Multiplication
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Let &#x03B1; be the maximal value such that the product of an n &#x00D7; n&#x03B1; matrix by an n&#x03B1; &#x00D7; n matrix can be computed with n2+o(1) arithmetic operations. In this paper we show that &#x03B1; &gt;; 0.30298, which improves the previous record &#x03B1; &gt;; 0.29462 by Coppersmith (Journal of Complexity, 1997). More generally, we construct a new algorithm for multiplying an n &#x00D7; nk matrix by an nk &#x00D7; n matrix, for any value k &#x2260; 1. The complexity of this algorithm is better than all known algorithms for rectangular matrix multiplication. In the case of square matrix multiplication (i.e., for k = 1), we recover exactly the complexity of the algorithm by Coppersmith and Winograd (Journal of Symbolic Computation, 1990). These new upper bounds can be used to improve the time complexity of several known algorithms that rely on rectangular matrix multiplication. For example, we directly obtain a O(n2.5302)-time algorithm for the all-pairs shortest paths problem over directed graphs with small integer weights, where n denotes the number of vertices, and also improve the time complexity of sparse square matrix multiplication.
[shortest paths problem, algorithms, rectangular matrices, time complexity, sparse square matrix multiplication, Sparse matrices, Matrix decomposition, integer weights, Equations, matrix multiplication, Upper bound, Tensile stress, directed graphs, arithmetic operations, faster algorithms, rectangular matrix multiplication, sparse matrices, computational complexity]
Quasi-optimal Multiplication of Linear Differential Operators
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We show that linear differential operators with polynomial coefficients over a field of characteristic zero can be multiplied in quasi-optimal time. This answers an open question raised by van der Hoeven.
[linear differential operators, Vectors, Electronic mail, Linear differential operators, Computational complexity, algebraic algorithms, characteristic zero field, quasi-optimal time multiplication, Interpolation, Polynomials, polynomial coefficients, multiplication, computational complexity]
Algorithmic Applications of Baur-Strassen's Theorem: Shortest Cycles, Diameter and Matchings
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Consider a directed or undirected graph with integral edge weights in [-W, W]. This paper introduces a general framework for solving problems on such graphs using matrix multiplication. The framework is based on the Baur-Strassen Theorem and Strojohann's determinant algorithm. For directed and undirected graphs without negative cycles we obtain simple O&#x0303;(Wn&#x03C9;) running time algorithms for finding a shortest cycle, computing the diameter or radius, and detecting a negative weight cycle. For each of these problems we unify and extend the class of graphs for which O&#x0303;(Wn&#x03C9;) time algorithms are known. In particular no such algorithms were known for any of these problems in undirected graphs with (potentially) negative weights. We also present an O&#x0303;(Wn&#x03C9;) time algorithm for minimum weight perfect matching. This resolves an open problem posed by Sankowski in 2006, who presented such an algorithm for bipartite graphs. Our algorithm uses a novel combinatorial interpretation of the linear program dual for minimum perfect matching. We believe this framework will find applications for finding larger spectra of related problems. As an example we give a simple O&#x0303;(Wn&#x03C9;) time algorithm to find all the vertices that lie on cycles of length at most t, for given t. This improves an O&#x0303;(Wn&#x03C9;) time algorithm of Yuster.
[Strojohann determinant algorithm, linear programming, Electronic mail, algorithmic applications, negative weight cycle, shortest cycle, bipartite graphs, diameter, time algorithms, diameter computation, Polynomials, Matrices, radius computation, Bipartite graph, radius, shortest cycles, Educational institutions, linear program, Vectors, integral edge weights, minimum weight perfect matchings, Computer science, matrix multiplication, Baur-Strassen theorem, directed graphs, minimum perfect matching]
Almost Optimal Canonical Property Testers for Satisfiability
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In the (k, d)-Function-SAT problem we are given a set of n variables {X<sub>1</sub>, ... , X<sub>n</sub>} that can take values from the set {1, . .. , d} and a set of Boolean constraints on these variables, where each constraint is of the form f : {1, ... , d}k &#x2192; {0, 1}, i.e. the constraint depends on exactly k of these variables. We will treat k and d as constants. The goal is to determine whether the set of constraints has a satisfying assignment, i.e. an assignment to the variables such that all constraints simultanuously map to 1. In this paper, we study (k, d)-Function-SAT in the property testing model for dense instances. We call an instance &#x03B5;-far from satisfiable, if every assignment violates more than &#x03B5;nk constraints. A property testing algorithm is a randomized algorithm that, given oracle access to the set of constraints, must accept with probability at least 3/4 all satisfiable inputs and rejects with probability at least 3/4 all inputs, which are &#x03B5;-far from satisfiable. We analyze the canonical non-adaptive property testing algorithm with one-sided error: Sample r variables and accept, if and only if the induced set of constraints has a satisfying assignment. The value of r will be called the sample commlexity of the algorithm. We show that there is an r<sub>0</sub> = O(1/&#x03B5;) such that for any instance that is &#x03B5;-far from satisfiable, the probability, that a random sample on r &#x2265; r0 variables is satisfiable, is at most 1/4. This implies that the above algorithm is a property tester. The obtained sample complexity is nearly optimal for canonical testers as a lower bound of &#x03A9;(1/&#x03B5;) on the sample complexity is known. Previously, a tester with sample complexity o(1/&#x03B5;2) was only known for the very special case of testing bipartiteness in the dense graph model [3]. Our new general result improves the best previous result for testing satisfiability (and even for the special case of 3-colorability in graphs) from sample complexity O&#x0303;(1/&#x03B5;2) to O&#x0303;(1/&#x03B5;). It also slightly improves the sample complexity for the special case of bipartiteness. Improving the sample complexity for (k, d)-Function-SAT (or special cases of it) had been posed in several papers as an open problem [3], [4], [17]. This paper solves this problem nearly optimally for canonical testers and, in the case of k = 2, also for nonadaptive testers as there is a lower bound of &#x03A9;(1/&#x03B5;2) on the query complexity of non-adaptive testers for bipartiteness in the dense graph model [6], where the query complexity denotes the number of queries asked about the graph (for a canonical tester in graphs, the query complexity is the square of its sample complexity). As a byproduct, we obtain an algorithm, which, given a satisfiable set of constraints, computes in time O(n/&#x03B5;O(1) + 2O&#x0303;(1/&#x03B5;)) a solution, which violates at most &#x03B5;nk constraints.
[Algorithm design and analysis, Adaptation models, canonical nonadaptive property testing algorithm, (k, graph theory, query complexity, computability, Complexity theory, Satisfiability Problems, sample complexity, Boolean functions, satisfiability, Testing, Context, dense graph model, probability, Boolean constraints, Boolean algebra, randomized algorithm, instance &#x03B5;-far, randomised algorithms, &#x03B5;nk constraints, Property Testing, Computer science, bipartiteness testing, d)-function-SAT problem, optimal canonical property testers, computational complexity]
Partially Symmetric Functions Are Efficiently Isomorphism-Testable
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Given a Boolean function f, the f-isomorphism testing problem requires a randomized algorithm to distinguish functions that are identical to f up to relabeling of the input variables from functions that are far from being so. An important open question in property testing is to determine for which functions f we can test f-isomorphism with a constant number of queries. Despite much recent attention to this question, essentially only two classes of functions were known to be efficiently isomorphism testable: symmetric functions and juntas. We unify and extend these results by showing that all partially symmetric functions -- functions invariant to the reordering of all but a constant number of their variables -- are efficiently isomorphism-testable. This class of functions, first introduced by Shannon, includes symmetric functions, juntas, and many other functions as well. We conjecture that these functions are essentially the only functions efficiently isomorphism-testable. To prove our main result, we also show that partial symmetry is efficiently testable. In turn, to prove this result we had to revisit the junta testing problem. We provide a new proof of correctness of the nearly-optimal junta tester. Our new proof replaces the Fourier machinery of the original proof with a purely combinatorial argument that exploits the connection between sets of variables with low influence and intersecting families. Another important ingredient in our proofs is a new notion of symmetric influence. We use this measure of influence to prove that partial symmetry is efficiently testable and also to construct an efficient sample extractor for partially symmetric functions. We then combine the sample extractor with the testing-by-implicit-learning approach to complete the proof that partially symmetric functions are efficiently isomorphism-testable.
[symmetric influence, Input variables, combinatorial argument, f-isomorphism testing problem, Educational institutions, Boolean function, Vectors, Partitioning algorithms, randomized algorithm, property testing, junta testing problem, testing-by-implicit-learning approach, randomised algorithms, Computer science, partially symmetric functions, Boolean functions, partial symmetry, Fourier machinery, learning (artificial intelligence), isomorphism-testable, Testing, nearly-optimal junta tester]
Sparse Affine-Invariant Linear Codes Are Locally Testable
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We show that sparse affine-invariant linear properties over arbitrary finite fields are locally testable with a constant number of queries. Given a finite field F<sub>q</sub> and an extension field F<sub>q</sub>n, a property is a set of functions mapping F<sub>q</sub>n to F<sub>q</sub>. The property is said to be affine-invariant if it is invariant under affine transformations of F<sub>q</sub>n, and it is said to be sparse if its size is polynomial in the domain size. Our work completes a line of work initiated by Grigorescu et al. [RANDOM 2009] and followed by Kaufman and Lovett [FOCS 2011]. The latter showed such a result for the case when q was prime. Extending to non-prime cases turns out to be non-trivial and our proof involves some detours into additive combinatorics, as well as a new calculus for building property testers for affine-invariant linear properties.
[nontrivial cases, Additives, nonprime cases, linear codes, finite sets, Locally Testable Codes, Calculus, Orbits, Electronic mail, set theory, sparse affine-invariant linear codes, Affine Invariance, sparse affine-invariant linear properties, arbitrary finite fields, Testing, Context, functions mapping, locally testable, property testing, Sum-product Estimates, affine transformations, randomised algorithms, Computer science, Additive Combinatorics, computational complexity, number theory]
The Cutting Plane Method Is Polynomial for Perfect Matchings
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The cutting plane approach to optimal matchings has been discussed by several authors over the past decades, and its rate of convergence has been an open question. We prove that the cutting plane approach using Edmonds' blossom inequalities converges in polynomial time for the minimum-cost perfect matching problem. Our main insight is an LP-based method to select cutting planes. This cut selection procedure leads to a sequence of intermediate linear programs with a linear number of constraints whose optima are half-integral and supported by a disjoint union of odd cycles and edges. This structural property of the optima is instrumental in finding violated blossom inequalities (cuts) in linear time. Moreover, the number of cycles in the support of the half-integral optima acts as a potential function to show efficient convergence to an integral solution.
[Algorithm design and analysis, algorithms, minimum-cost perfect matching problem, intermediate linear programs, cutting plane method, Edmond blossom inequalities, odd cycles, linear programming, Electronic mail, Convergence, cut selection procedure, matching, Cost function, Polynomials, polynomial time, Image edge detection, polynomials, cutting plane methods, odd edge, half-integral optima, Linear programming, LP-based method, polynomial, optimal matchings, convergence rate, computational complexity, linear time]
A Weight-Scaling Algorithm for Min-Cost Imperfect Matchings in Bipartite Graphs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Call a bipartite graph G = (X, Y ; E) balanced when |X| = |Y |. Given a balanced bipartite graph G with edge costs, the assignment problem asks for a perfect matching in G of minimum total cost. The Hungarian Method can solve assignment problems in time O(mn+n2 log n), where n := |X| = |Y | and m := |E|. If the edge weights are integers bounded in magnitude by C &gt;; 1, then algorithms using weight scaling, such as that of Gabow and Tarjan, can lower the time to O(m&#x221A;n log(nC)). There are important applications in which G is unbalanced, with |X| &#x2260; |Y |, and we require a min-cost matching of size r := min(|X|, |Y |) or, more generally, of some specified size s &#x2264; r. The Hungarian Method extends easily to find such a matching in time O(ms + s2 log r), but weightscaling algorithms do not extend so easily. We introduce new machinery to find such a matching in time O(m&#x221A;s log(sC)) via weight scaling. Our results provide some insight into the design space of efficient weight-scaling matching algorithms.
[Algorithm design and analysis, weight-scaling matching algorithm, graph theory, edge costs, Quantization, assignment problem, edge weights, bipartite graphs, Handheld computers, Impedance matching, Vegetation, Hungarian method, min-cost imperfect matchings, Polynomials, Bipartite graph]
A New Direction for Counting Perfect Matchings
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In this paper, we present a new exact algorithm for counting perfect matchings, which relies on neither inclusion-exclusion principle nor tree-decompositions. For any bipartite graph of 2n nodes and &#x0394;n edges such that &#x0394; &#x2265; 3, our algorithm runs with O*(2(1-1/O(&#x0394; log &#x0394;))n) time and exponential space. Compared to the previous algorithms, it achieves a better time bound in the sense that the performance degradation to the increase of &#x0394; is quite slower. The main idea of our algorithm is a new reduction to the problem of computing the cut-weight distribution of the input graph. The primary ingredient of this reduction is MacWilliams Identity derived from elementary coding theory. The whole of our algorithm is designed by combining that reduction with a non-trivial fast algorithm computing the cut-weight distribution. To the best of our knowledge, the approach posed in this paper is new and may be of independent interest.
[inclusion-exclusion principle, time space, counting perfect matchings, elementary coding theory, Optical wavelength conversion, trees (mathematics), MacWilliams identity, Vectors, Generators, Partitioning algorithms, performance degradation, bipartite graph, exponential algorithm, exponential space, Linear code, exact algorithm, coding theory, nontrivial fast algorithm, cut-weight distribution, Polynomials, Bipartite graph, input graph, tree-decompositions, computational complexity]
Single Source -- All Sinks Max Flows in Planar Digraphs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Let G = (V, E) be a planar n-vertex digraph. Consider the problem of computing max st-flow values in G from a fixed source s to all sinks t &#x03F5; V \\ {s}. We show how to solve this problem in near-linear O(n log3 n) time. Previously, nothing better was known than running a single-source singlesink max How algorithm n-1 times, giving a total time bound of O(n2 log n) with the algorithm of Borradaile and Klein. An important implication is that all-pairs max st-How values in G can be computed in near-quadratic time. This is close to optimal as the output size is 8(n2). We give a quadratic lower bound on the number of distinct max How values and an &#x03A9;(n3) lower bound for the total size of all min cut-sets. This distinguishes the problem from the undirected case where the number of distinct max How values is O(n). Previous to our result, no algorithm which could solve the all-pairs max How values problem faster than the time of 8(n2) max-How computations for every planar digraph was known. This result is accompanied with a data structure that reports min cut-sets. For fixed s and all t, after O(n1.5 log2 n) preprocessing time, it can report the set of arcs C crossing a min st-cut in O(|C|) time.
[Borradaile-Klein algorithm, planar n-vertex digraph, max st-flow values, data structure, Educational institutions, Data structures, Partitioning algorithms, Electronic mail, planar graph, all-pairs max how values problem, near-quadratic time, Standards, Computer science, minimum cut, near-linear time, directed graphs, min cut-sets, maximum flow, all pairs, Informatics, computational complexity, single-source single sink max how algorithm]
New Limits to Classical and Quantum Instance Compression
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Given an instance of a hard decision problem, a limited goal is to compress that instance into a smaller, equivalent instance of a second problem. As one example, consider the problem where, given Boolean formulas &#x03C8;1,&#x22EF;,&#x03C8;t, we must determine if at least one &#x03C8;j is satisfiable. An OR-compression scheme for SAT is a polynomial-time reduction that maps (&#x03C8;1,&#x22EF;,&#x03C8;t) to a string z, such that z lies in some &#x201C;target&#x201D; language L' if and only if V<sub>j</sub>[&#x03C8;j &#x2208;SAT] holds. (Here, L' can be arbitrarily complex.) AND-compression schemes are defined similarly. A compression scheme is strong if |z| is polynomially bounded in n = max<sub>j</sub> |&#x03C8;j|, independent of t. Strong compression for SAT seems unlikely. Work of Harnik and Naor (FOCS '06/SICOMP '10) and Bodlaender, Downey, Fellows, and Hermelin (ICALP '08/JCSS '09) showed that the infeasibility of strong OR-compression for SAT would show limits to instance compression for a large number of natural problems. Bodlaender et al. also showed that the infeasibility of strong AND-compression for SAT would have consequences for a different list of problems. Motivated by this, Fortnow and Santhanam (STOC '08/JCSS '11) showed that if SAT is strongly OR-compressible, then NP C coNP/poly. Finding similar evidence against AND-compression was left as an open question. We provide such evidence: we show that strong AND- or OR-compression for SAT would imply non-uniform, statistical zero-knowledge proofs for SAT-an even stronger and more unlikely consequence than NP &#x2286; coNP/poly. Our method applies against probabilistic compression schemes of sufficient &#x201C;quality&#x201D; with respect to the reliability and compression amount (allowing for tradeoff). This greatly strengthens the evidence given by Fortnow and Santhanam against probabilistic OR-compression for SAT. We also give variants of these results for the analogous task of quantum instance compression, in which a polynomial-time quantum reduction must output a quantum state that, in an appropriate sense, &#x201C;preserves the answer&#x201D; to the input instance. The central idea in our proofs is to exploit the information bottleneck in an AND-compression scheme for a language L in order to fool a cheating prover in a proof system for L&#x0305;. Our key technical tool is a new method to &#x201C;disguise&#x201D; information being fed into a compressive mapping; we believe this method may find other applications.
[Protocols, decision theory, quantum instance compression, compressive mapping, SAT, probabilistic compression schemes, computability, Complexity theory, nonuniform statistical zero-knowledge proofs, Quantum computing, NP-hard problems, AND-compression schemes, satisfiability problem, information bottleneck, hard decision problem, Polynomials, theorem proving, data compression, proof system, polynomial hierarchy, Probabilistic logic, Encoding, polynomial-time quantum reduction, kernelization, quantum compression, Boolean formulas, instance compression, probabilistic OR-compression scheme, quantum computing, Reliability, classical instance compression, computational complexity]
Lower Bounds on Interactive Compressibility by Constant-Depth Circuits
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We formulate a new connection between instance compressibility [1]), where the compressor uses circuits from a class C, and correlation with circuits in C. We use this connection to prove the first lower bounds on general probabilistic multi-round instance compression. We show that there is no probabilistic multi-round compression protocol for Parity in which the computationally bounded party uses a non-uniform AC0-circuit and transmits at most n/(log(n))&#x03C9;(1) bits. This result is tight, and strengthens results of Dubrov and Ishai. We also show that a similar lower bound holds for Majority. We also consider the question of round separation, i.e., whether for each r &#x2265; 1, there are functions which can be compressed better with r rounds of compression than with r - 1 rounds. We answer this question affirmatively for compression using constant-depth polynomial-size circuits. Finally, we prove the first non-trivial lower bounds for 1-round compressibility of Parity by polynomial size ACC0[p] circuits where p is an odd prime.
[circuit complexity, Protocols, Correlation, probability, parity, game theory, nontrivial lower bounds, computationally bounded party, round separation, Probabilistic logic, Complexity theory, History, communication complexity, instance compressibility, general probabilistic multiround instance compression, interactive compressibility, bounded-depth circuits, Games, C-compression game, Logic gates, majority, compression, nonuniform AC0-circuit, constant-depth polynomial-size circuits]
Geometric Complexity Theory V: Equivalence between Blackbox Derandomization of Polynomial Identity Testing and Derandomization of Noether's Normalization Lemma
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
It is shown that black-box derandomization of polynomial identity testing (PIT) is essentially equivalent to derandomization of Noether's Normalization Lemma for explicit algebraic varieties, the problem that lies at the heart of the foundational classification problem of algebraic geometry. Specifically: (1) It is shown that in characteristic zero black-box derandomization of PIT for diagonal depth three circuits brings the problem of derandomizing Noether's Normalization Lemma, for the ring of invariants of any explicit linear action of a classical algebraic group of constant dimension, from EXPSPACE (where it is currently) to P. Next it is shown that assuming the Generalized Riemann Hypothesis (GRH), instead of the black-box derandomization hypothesis, brings the problem from EXPSPACE to quasi-PH, instead of P. Thus black-box derandomization of diagonal depth three circuits takes us farther than GRH here on the basis of the current knowledge. Variants of the main implication are also shown assuming, instead of the black-box derandomization hypothesis in characteristic zero, Boolean lower bounds for constant-depth threshold circuits or uniform Boolean conjectures, in conjunction with GRH. These results may explain in a unified way why proving lower bounds or derandomization results for constant-depth arithmetic circuits in characteristic zero or constant-depth Boolean threshold circuits, or proving uniform Boolean conjectures without relativizable proofs has turned out to be so hard, and also why GRH has turned out to be so hard from the complexity-theoretic perspective. Thus this investigation reveals that the foundational problems of Geometry (classification and GRH) and Complexity Theory (lower bounds and derandomization) share a common root difficulty that lies at the junction of these two fields. We refer to it as the GCT chasm. (2) It is shown that black-box derandomization of PIT in a strengthened form implies derandomization of Noether's Normalization Lemma in a strict form for any explicit algebraic variety. (3) Conversely, it is shown that derandomization of Noether's Normalization Lemma in a strict form for specific explicit varieties implies this strengthened form of black box derandomization of PIT and its various variants. (4) A unified geometric complexity theory (GCT) approach to derandomization and classification is formulated on the basis of this equivalence.
[characteristic zero black-box derandomization, Noether normalization lemma derandomization, explicit algebraic varieties, computational geometry, Complexity theory, common root difficulty, geometric complexity theory, algebraic geometry classification problem, Boolean functions, Integral equations, constant-depth arithmetic circuits, symbolic trace identity testing, EXPSPACE, Polynomials, generalized Riemann hypothesis, Derandomization, complexity-theoretic perspective, Testing, Polynomial Idenity Testing, Context, constant-depth threshold circuits, Geometric Complexity Theory, polynomial identity testing, STIT, constant-depth Boolean threshold circuits, Geometry, Computer science, diagonal depth three circuits, uniform Boolean conjectures, GCT, PIT, GRH, computational complexity]
Computing Multiplicities of Lie Group Representations
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
For fixed compact connected Lie groups H &#x2286; G, we provide a polynomial time algorithm to compute the multiplicity of a given irreducible representation of H in the restriction of an irreducible representation of G. Our algorithm is based on a finite difference formula which makes the multiplicities amenable to Barvinok's algorithm for counting integral points in polytopes. The Kronecker coefficients of the symmetric group, which can be seen to be a special case of such multiplicities, play an important role in the geometric complexity theory approach to the P vs. NP problem. Whereas their computation is known to be #P-hard for Young diagrams with an arbitrary number of rows, our algorithm computes them in polynomial time if the number of rows is bounded. We complement our work by showing that information on the asymptotic growth rates of multiplicities in the coordinate rings of orbit closures does not directly lead to new complexity-theoretic obstructions beyond what can be obtained from the moment polytopes of the orbit closures. Nonasymptotic information on the multiplicities, such as provided by our algorithm, may therefore be essential in order to find obstructions in geometric complexity theory.
[multiplicity computation, #P-hard, irreducible representation, asymptotic growth rates, Lattices, finite difference formula, computational geometry, finite difference methods, Complexity theory, integral points, geometric complexity theory, fixed compact connected Lie groups, Polynomials, P problem, moment polytopes, Barvinok algorithm, Extraterrestrial measurements, Vectors, Kronecker coefficients, polynomial time algorithm, Young diagrams, Physics, Lie group representations, NP problem, coordinate rings, Lie groups, symmetric group, computational complexity, orbit closures]
A Tight Linear Time (1/2)-Approximation for Unconstrained Submodular Maximization
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We consider the Unconstrained Submodular Maximization problem in which we are given a non-negative submodular function f : 2N &#x2192; &#x211D;+, and the objective is to find a subset S &#x2286; N maximizing f(S). This is one of the most basic submodular optimization problems, having a wide range of applications. Some well known problems captured by Unconstrained Submodular Maximization include MaxCut, Max-DiCut, and variants of Max-SAT and maximum facility location. We present a simple randomized linear time algorithm achieving a tight approximation guarantee of 1/2, thus matching the known hardness result of Feige et al. [11]. Our algorithm is based on an adaptation of the greedy approach which exploits certain symmetry properties of the problem. Our method might seem counterintuitive, since it is known that the greedy algorithm fails to achieve any bounded approximation factor for the problem.
[Algorithm design and analysis, Greedy algorithms, approximation theory, greedy algorithms, Optimized production technology, simple randomized linear time algorithm, Submodular Functions, Linear programming, tight approximation, Max-SAT, Approximation Algorithms, Approximation methods, maximum facility location, MaxCut, randomised algorithms, Computer science, unconstrained submodular maximization problem, optimisation, tight linear time (1/2)-approximation, Max-DiCut, Approximation algorithms, greedy approach, greedy algorithm, submodular optimization problems]
A Tight Combinatorial Algorithm for Submodular Maximization Subject to a Matroid Constraint
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We present an optimal, combinatorial 1-1/e approximation algorithm for monotone sub modular optimization over a matroid constraint. Compared to the continuous greedy algorithm (Calinescu, Chekuri, Pal and Vondrak, 2008), our algorithm is extremely simple and requires no rounding. It consists of the greedy algorithm followed by local search. Both phases are run not on the actual objective function, but on a related non-oblivious potential function, which is also monotone sub modular. In our previous work on maximum coverage (Filmus and Ward, 2011), the potential function gives more weight to elements covered multiple times. We generalize this approach from coverage functions to arbitrary monotone sub modular functions. When the objective function is a coverage function, both definitions of the potential function coincide. The parameters used to define the potential function are closely related to Pade approximants of exp(x) evaluated at x = 1. We use this connection to determine the approximation ratio of the algorithm.
[Greedy algorithms, Algorithm design and analysis, optimal combinatorial 1-1/e approximation algorithm, combinatorial mathematics, functions, Search problems, approximation algorithms, Approximation methods, monotone submodular optimization, local search, submodular functions, optimisation, matroid constraint, tight combinatorial algorithm, greedy algorithm, search problems, nonoblivious potential function, approximation ratio, approximation theory, greedy algorithms, Linear programming, coverage functions, Pade approximants, matrix algebra, matroids, submodular maximization, Approximation algorithms, potential function coincide]
The Power of Linear Programming for Valued CSPs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
A class of valued constraint satisfaction problems (VCSPs) is characterised by a valued constraint language, a fixed set of cost functions on a finite domain. An instance of the problem is specified by a sum of cost functions from the language with the goal to minimise the sum. This framework includes and generalises well-studied constraint satisfaction problems (CSPs) and maximum constraint satisfaction problems (Max-CSPs). Our main result is a precise algebraic characterisation of valued constraint languages whose instances can be solved exactly by the basic linear programming relaxation. Using this result, we obtain tractability of several novel and previously widely-open classes of VCSPs, including problems over valued constraint languages that are: (1) sub modular on arbitrary lattices, (2) bisubmodular (also known as k-sub modular) on arbitrary finite domains, (3) weakly (and hence strongly) tree-sub modular on arbitrary trees.
[valued constraint satisfaction, Lattices, fractional homomorphisms, algebra, linear programming, Complexity theory, linear programming relaxation, Cost function, maximum constraint satisfaction problems, Robustness, precise algebraic characterisation, IP networks, constraint handling, arbitrary trees, arbitrary finite domains, Max-CSP, VCSP, Cloning, trees (mathematics), Linear programming, arbitrary lattices, fractional polymorphisms, submodularity, bisubmodularity, valued constraint satisfaction problems, valued constraint language]
How to Construct Quantum Random Functions
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In the presence of a quantum adversary, there are two possible definitions of security for a pseudorandom function. The first, which we call standard-security, allows the adversary to be quantum, but requires queries to the function to be classical. The second, quantum-security, allows the adversary to query the function on a quantum superposition of inputs, thereby giving the adversary a superposition of the values of the function at many inputs at once. Existing techniques for proving the security of pseudorandom functions fail when the adversary can make quantum queries. We give the first quantum-security proofs for pseudorandom functions by showing that some classical constructions of pseudorandom functions are quantum-secure. Namely, we show that the standard constructions of pseudorandom functions from pseudorandom generators or pseudorandom synthesizers are secure, even when the adversary can make quantum queries. We also show that a direct construction from lattices is quantum-secure. To prove security, we develop new tools to prove the indistinguishability of distributions under quantum queries. In light of these positive results, one might hope that all standard-secure pseudorandom functions are quantum-secure. To the contrary, we show a separation: under the assumption that standard-secure pseudorandom functions exist, there are pseudorandom functions secure against quantum adversaries making classical queries, but insecure once the adversary can make quantum queries.
[pseudorandom function security, Pseudorandom Function, Synthesizers, pseudorandom generators, standard-security, Generators, random number generation, Standards, pseudorandom synthesizers, query processing, Quantum, Quantum computing, quantum-security proofs, security of data, quantum computing, quantum queries, quantum random functions, random functions, Polynomials, quantum adversary, Cryptography, quantum superposition]
Non-malleable Extractors, Two-Source Extractors and Privacy Amplification
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
In [1], Dodis and Wichs introduced the notion of a non-malleable extractor. A non-malleable extractor is a much stronger version of a seeded extractor. Dodis and Wichs showed that such an object can be used to give optimal privacy amplification protocols with an active adversary. Previously, there are only two known constructions of nonmalleable extractors [2], [3]. Both constructions only work for (n, k)-sources with k &gt;; n/2. Interestingly, both constructions are also two-source extractors. In this paper, we present a strong connection between nonmalleable extractors and two-source extractors. The first part of the connection shows that non-malleable extractors can be used to construct two-source extractors. This partially explains why previous constructions of non-malleable extractors only work for entropy rate &gt;; 1/2, and why explicit non-malleable extractors for small min-entropy may be hard to get. The second part of the connection shows that certain two-source extractors can be used to construct non-malleable extractors. Using this connection, we obtain the first construction of non-malleable extractors for k &lt;; n/2. Finally, despite the lack of explicit non-malleable extractors for arbitrarily linear entropy, we give the first 2-round privacy amplification protocol with asymptotically optimal entropy loss and communication complexity for (n, k) sources with k = &#x03B1;n for any constant &#x03B1; &gt;; 0. This dramatically improves previous results and answers an open problem in [2].
[Protocols, arbitrarily linear entropy, active adversary, entropy rate, privacy amplification, min-entropy, extractor, Entropy, Vectors, Complexity theory, 2-round privacy amplification protocols, communication complexity, Privacy, entropy, non-malleable, nonmalleable extractors, Agricultural machinery, data privacy, seeded extractor, Random variables, protocols, two-source extractors, asymptotically optimal entropy loss]
Constructing a Pseudorandom Generator Requires an Almost Linear Number of Calls
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
We show that a black-box construction of a pseudorandom generator from a one-way function needs to make &#x03A9;(n/log(n)) calls to the underlying one-way function. The bound even holds if the one-way function is guaranteed to be regular. In this case it matches the best known construction due to Gold Reich, Krawczyk, and Luby (SIAM J. Comp. 22, 1993), which uses O(n/log(n)) calls.
[Context, pseudorandom generator, functions, Pseudorandom Generators, Black-box separation, cryptography, black-box construction, Generators, Encoding, Security, random number generation, one-way function, Radio frequency, Computer science, &#x03A9;(n/log(n)) calls, O(n/log(n)) calls, One-way Functions, Polynomials, computational complexity]
Randomized Greedy Algorithms for the Maximum Matching Problem with New Analysis
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
It is a long-standing problem to lower bound the performance of randomized greedy algorithms for maximum matching. Aronson, Dyer, Frieze and Suen [1]studied the modified randomized greedy (MRG) algorithm and proved that it approximates the maximum matching within a factor of at least 1/2 + 1/400,000. They use heavy combinatorial methods in their analysis. We introduce a new technique we call Contrast Analysis, and show a 1/2 + 1/256 performance lower bound for the MRG algorithm. The technique seems to be useful not only for the MRG, but also for other related algorithms.
[Greedy algorithms, Algorithm design and analysis, long-standing problem, Schedules, approximation theory, approximation, combinatorial mathematics, greedy algorithms, Approximation methods, MRG algorithm, randomised algorithms, matching, maximum matching problem, combinatorial methods, modified randomized greedy algorithm, Weapons, Tin, Approximation algorithms, greedy, contrast analysis, randomized]
Matching with Our Eyes Closed
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Motivated by an application in kidney exchange, we study the following query-commit problem: we are given the set of vertices of a non-bipartite graph G. The set of edges in this graph are not known ahead of time. We can query any pair of vertices to determine if they are adjacent. If the queried edge exists, we are committed to match the two endpoints. Our objective is to maximize the size of the matching. This restriction in the amount of information available to the algorithm constraints us to implement myopic, greedy-like algorithms. A simple deterministic greedy algorithm achieves a factor 1/2 which is tight for deterministic algorithms. An important open question in this direction is to give a randomized greedy algorithm that has a significantly better approximation factor. This question was first asked almost 20 years ago by Dyer and Frieze [9] where they showed that a natural randomized strategy of picking edges uniformly at random doesn't help and has an approximation factor of 1/2 + o(1). They left it as an open question to devise a better randomized greedy algorithm. In subsequent work, Aronson, Dyer, Frieze, and Suen [2] gave a different randomized greedy algorithm and showed that it attains a factor 0.5 + o where o is 0.0000025. In this paper we propose and analyze a new randomized greedy algorithm for finding a large matching in a general graph and use it to solve the query commit problem mentioned above. We show that our algorithm attains a factor of at least 0.56, a significant improvement over 0.50000025. We also show that no randomized algorithm can have an approximation factor better than 0.7916 for the query commit problem. For another large and interesting class of randomized algorithms that we call vertex-iterative algorithms, we show that no vertex iterative algorithm can have an approximation factor better than 0.75.
[Greedy algorithms, Algorithm design and analysis, iterative methods, natural randomized strategy, graph theory, Stochastic processes, Randomized Algorithms, Approximation methods, Kidney, approximation factor, Matching, Combinatorial Optimization, large matching, simple deterministic greedy algorithm, matching problem, Probes, randomized greedy algorithm, non-bipartite graph G, vertex-iterative algorithms, approximation theory, greedy-like algorithms, query-commit problem, deterministic algorithms, kidney exchange, randomised algorithms, Approximation algorithms]
Online Matching with Stochastic Rewards
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The online matching problem has received significant attention in recent years because of its connections to allocation problems in Internet advertising, crowd-sourcing, etc. In these real-world applications, the typical goal is not to maximize the number of allocations, rather it is to maximize the number of successful allocations, where success of an allocation is governed by a stochastic process which follows the allocation. To address such applications, we propose and study the online matching problem with stochastic rewards (called the ONLINE STOCHASTIC MATCHING problem) in this paper. Our problem also has close connections to the existing literature on stochastic packing problems, in fact, our work initiates the study of online stochastic packing problems. We give a deterministic algorithm for the ONLINE STOCHASTIC MATCHING problem whose competitive ratio converges to (approximately) 0.567 for uniform and vanishing probabilities. We also give a randomized algorithm which outperforms the deterministic algorithm for higher probabilities. Finally, we complement our algorithms by giving an upper bound on the competitive ratio of any algorithm for this problem. This result shows that the best achievable competitive ratio for the ONLINE STOCHASTIC MATCHING problem is provably worse than that for the (non-stochastic) online matching problem.
[Algorithm design and analysis, online stochastic matching problem, allocation problems, advertising data processing, uniform probabilities, vanishing probabilities, Stochastic processes, Optimized production technology, Adaptive algorithms, probability, stochastic process, online stochastic packing problems, deterministic algorithm, deterministic algorithms, competitive ratio, Upper bound, stochastic rewards, Approximation algorithms, crowd-sourcing, Internet, Resource management, stochastic processes, Internet advertising]
A New Infinity of Distance Oracles for Sparse Graphs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Given a weighted undirected graph, our basic goal is to represent all pairwise distances using much less than quadratic space, such that we can estimate the distance between query vertices in constant time. We will study the inherent trade-off between space of the representation and the stretch (multiplicative approximation disallowing underestimates) of the estimates when the input graph is sparse with m = O&#x0303;(n) edges. In this paper, for any fixed positive integers k and &#x2113;, we obtain stretches = 2k + 1 &#x00B1; 2/&#x2113; = 2k + 1 - 2/&#x2113;, 2k + 1 + 2/&#x2113;, using space S(&#x03B1;, m) = O&#x0303;(m1+2/(&#x03B1;+1)). The query time is O(k + &#x2113;) = O(1). For integer stretches, this coincides with the previous bounds (odd stretches with &#x2113; = 1 and even stretches with &#x2113; = 2). The infinity of fractional stretches between consecutive integers are all new (even though &#x2113; is fixed as a constant independent of the input, the number of integers &#x2113; is still countably infinite). We will argue that the new fractional points are not just arbitrary, but that they, at least for fixed stretches below 3, provide a complete picture of the inherent trade-off between stretch and space in m. Consider any fixed stretch &#x03B1; &lt;; 3. Based on the hardness of set intersection, we argue that if &#x2113; is the largest integer such that 3-2/&#x2113; &#x2264; &#x03B1;, then &#x03A9;&#x0303;(S(3 - 2/&#x2113;, m)) space is needed for stretch . In particular, for fixed stretch below 22/3, we improve Patrascu and Roditty's lower bound from &#x03A9;&#x0303;(m3/2) to &#x03A9;&#x0303;(m5/3), thus matching their upper bound for stretch 2. For space in terms of m, this is the first hardness matching the space of a non-trivial/sub-quadratic distance oracle.
[Measurement, graph theory, multiplicative approximation, set theory, Approximation methods, weighted undirected graph, subquadratic distance oracle, query processing, set intersection, fractional points, Bismuth, distance estimation, Cost function, Silicon, nontrivial distance oracle, approximation theory, distances, fixed positive integers, sparse graphs, hardness matching, query vertices, query time, Data structures, constant time, Upper bound, distance oracles, shortest paths, pairwise distances, fractional stretch infinity, input graph, computational complexity]
Improved Distance Sensitivity Oracles via Fast Single-Source Replacement Paths
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
A distance sensitivity oracle is a data structure which, given two nodes s and t in a directed edge-weighted graph G and an edge e, returns the shortest length of an s-t path not containing e, a so called replacement path for the triple (s, t, e). Such oracles are used to quickly recover from edge failures. In this paper we consider the case of integer weights in the interval [-M, M], and present the first distance sensitivity oracle that achieves simultaneously subcubic preprocessing time and sublinear query time. More precisely, for a given parameter &#x03B1; &#x2208; [0, 1], our oracle has preprocessing time O&#x0303;(Mn&#x03C9;+1/2 +Mn&#x03C9;+&#x03B1;(4-&#x03C9;)) and query time O&#x0303;(n1-&#x03B1;). Here w &lt;; 2.373 denotes the matrix multiplication exponent. For a comparison, the previous best oracle for small integer weights has O&#x0303;(Mn&#x03C9;+1-&#x03B1;) preprocessing time and (superlinear) O&#x0303;(n1+&#x03B1;) query time [Weimann, Yuster-FOCS'10]. The main novelty in our approach is an algorithm to compute all the replacement paths from a given source s, an interesting problem on its own. We can solve the latter single-source replacement paths problem in O&#x0303;(APSP(n, M))) time, where APSP(n, M) &lt;; O&#x0303;(M0.681n2.575) [Zwick-JACM'02] is the runtime for computing all-pairs shortest paths in a graph with n vertices and integer edge weights in [-M, M]. For positive weights the runtime of our algorithm reduces to O&#x0303;(Mn&#x03C9;). This matches the best known runtime for the simpler replacement paths problem in which both the source s and the target t are fixed [Vassilevska-SODA'11].
[Algorithm design and analysis, directed edge-weighted graph, replacement paths, Transmission line matrix methods, distance sensitivity oracles, data structure, edge failures, sublinear query time, distance sensitivity oracle, Runtime, Bismuth, subcubic preprocessing time, data structures, matrix multiplication exponent, all-pairs shortest paths, fast single-source replacement paths problem, Data structures, Partitioning algorithms, n-node directed graph, integer weights, matrix multiplication, Sensitivity, directed graphs, shortest paths, computational complexity]
Everywhere-Sparse Spanners via Dense Subgraphs
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
The significant progress in constructing graph spanners that are sparse (small number of edges) or light (low total weight) has skipped spanners that are everywhere-sparse (small maximum degree). This disparity is in line with other network design problems, where the maximum-degree objective has been a notorious technical challenge. Our main result is for the LOWEST DEGREE 2-SPANNER (LD2S) problem, where the goal is to compute a 2-spanner of an input graph so as to minimize the maximum degree. We design a polynomial-time algorithm O&#x0305;(&#x0394;3-2&#x221A;2) &#x2248; O&#x0305;(&#x0394;0.172) achieving approximation factor , where &#x0394; is the maximum degree of the input graph. The previous O&#x0305;(&#x0394;1/4)-approximation was proved nearly two decades ago by Kortsarz and Peleg [SODA 1994, SICOMP 1998]. Our main conceptual contribution is to establish a formal connection between LD2S and a variant of the DENSEST k-SUBGRAPH (DkS) problem. Specifically, we design for both problems strong relaxations based on the Sherali-Adams linear programming (LP) hierarchy, and show that &#x201C;faithful&#x201D; randomized rounding of the DkS-variant can be used to round LD2S solutions. Our notion of faithfulness intuitively means that all vertices and edges are chosen with probability proportional to their LP value, but the precise formulation is more subtle. Unfortunately, the best algorithms known for DkS use the Lovasz-Schrijver LP hierarchy in a non-faithful way [Bhaskara, Charikar, Chlamtac, Feige, and Vijayaraghavan, STOC 2010]. Our main technical contribution is to overcome this shortcoming, while still matching the gap that arises in random graphs by planting a subgraph with same log-density.
[Algorithm design and analysis, everywhere-sparse spanners, Lovasz-Schrijver LP hierarchy, graph theory, linear programming, random graphs, Approximation methods, DkS, Sherali-Adams linear programming hierarchy, polynomial-time algorithm, dense subgraphs, lowest degree 2-spanner problem, DkS problem, Context, small maximum degree, approximation, Optimized production technology, densest k-subgraph problem, Linear programming, Routing, spanners, graph spanners, LD2S problem, Approximation algorithms, input graph, computational complexity]
[Publisher's information]
2012 IEEE 53rd Annual Symposium on Foundations of Computer Science
None
2012
Provides a listing of current committee members and society officers.
[]
Foreword
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Presents the welcome message from the conference proceedings.
[]
An Improved Competitive Algorithm for Reordering Buffer Management
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give an &amp;Otilde;(log log k)-competitive randomized online algorithm for reordering buffer management, where k is the buffer size. Our bound matches the lower bound of Adamaszek et al. (STOC 2011). Our algorithm has two stages which are executed online in parallel. The first stage computes deterministically a feasible fractional solution to an LP relaxation for reordering buffer management. The second stage "rounds" using randomness the fractional solution. The first stage is based on the online primal-dual schema, combined with a dual fitting charging scheme. As primal-dual steps and dual fitting steps are interleaved and in some sense conflicting, combining them is challenging. We also note that we apply the primal-dual schema to a relaxation with mixed packing and covering constraints. The first stage produces a fractional LP solution with cost within a factor of &amp;Otilde;(log log k) of the optimal LP cost. The second stage is an online algorithm that converts any LP solution to an integral solution, while increasing the cost by a constant factor. This stage generalizes recent results that gave a similar approximation guarantee using an offline rounding algorithm.
[Algorithm design and analysis, dual fitting steps, covering constraints, Schedules, fractional solution randomness, LP relaxation, competitive randomized online algorithm, mixed packing constraints, reordering buffer management, offline rounding algorithm, Color, Switches, dual fitting charging scheme, randomised algorithms, Computer science, Impedance matching, online primal-dual schema, randomized algorithms, scheduling, Approximation algorithms, online computing, computational complexity]
On Randomized Memoryless Algorithms for the Weighted K-Server Problem
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The weighted k-server problem is a generalization of the k-server problem in which the cost of moving a server of weight &#x03B2;<sub>i</sub> through a distance d is &#x03B2;<sub>i</sub> &#x00B7; d. The weighted server problem on uniform spaces models caching where caches have different write costs. We prove tight bounds on the performance of randomized memoryless algorithms for this problem on uniform metric spaces. We prove that there is an &#x03B1;<sub>k</sub> competitive memoryless algorithm for this problem, where &#x03B1;<sub>k</sub> = &#x03B1;2<sub>k-1</sub> + 3&#x03B1;<sub>k-1</sub> + 1; &#x03B1;1 = 1. On the other hand, we also prove a lower bound result, which is a strong evidence to our conjecture, that no randomized memoryless algorithm can have competitive ratio better than &#x03B1;<sub>k</sub>. To prove the upper bound of &#x03B1;<sub>k</sub>, we develop a framework to bound from above the competitive ratio of any randomized memoryless algorithm for this problem. The key technical contribution is a method for working with potential functions defined implicitly as the solution of a linear system. The result is robust in the sense that a small change in the probabilities used by the algorithm results in a small change in the upper bound on the competitive ratio. The above result has two important implications. Firstly this yields an &#x03B1;<sub>k</sub>-competitive memoryless algorithm for the weighted k-server problem on uniform spaces. This is the first competitive algorithm for k &gt; 2 which is memoryless. For k = 2, our algorithm agrees with the one given by Chrobak and Sgall [1]. Secondly, this helps us prove that the Harmonic algorithm, which chooses probabilities in inverse proportion to weights, has a competitive ratio of k&#x03B1;<sub>k</sub>. The only known competitive algorithm for every k before this work is a carefully crafted deterministic algorithm due to Fiat and Ricklin [2]. Their algorithm uses memory crucially and their bound on competitive ratio more than 24k . Our algorithm is not only memoryless, but also has a considerably improved competitive ratio of &#x03B1;<sub>k</sub> &lt;; 1.62k. Further, the derandomization technique by Ben-David et al. [3] implies that there exists a deterministic algorithm for this problem with competitive ratio &#x03B1;2<sub>k</sub> &lt;; 2.562k.
[Linear systems, queueing theory, Weighted k-server, weighted k-server problem, Extraterrestrial measurements, Probability distribution, cache storage, Servers, Equations, randomised algorithms, write costs, Competitive Ratio, Upper bound, randomized memoryless algorithms, &#x03B1;<sub>k</sub>-competitive memoryless algorithm, uniform spaces models caching, harmonic algorithm, uniform metric spaces, Memoryless Randomized Algorithms]
Approximating Bin Packing within O(log OPT * Log Log OPT) Bins
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
For bin packing, the input consists of n items with sizes between 0 and 1, which have to be assigned to a minimum number of bins of size 1. The seminal Karmarkar-Karp algorithm from '82 produces a solution with at most OPT + O(log2 OPT) bins. We provide the first improvement in now 3 decades and show that one can find a solution of cost OPT + O(log OPT * log log OPT) in polynomial time. This is achieved by rounding a fractional solution to the Gilmore-Gomory LP relaxation using the Entropy Method from discrepancy theory. The result is constructive via algorithms of Bansal and Lovett-Meka.
[Additives, combinatorial mathematics, Linear programming, Vectors, fractional solution, approximation algorithms, Approximation methods, Standards, bin packing, Gilmore-Gomory LP relaxation, discrepancy theory, optimisation, approximating bin packing, entropy method, seminal Karmarkar-Karp algorithm, Approximation algorithms, Polynomials, polynomial time, O(log OPT &#x00B7; Log Log OPT) bins, computational complexity]
Approximating Minimum-Cost k-Node Connected Subgraphs via Independence-Free Graphs
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We present a 6-approximation algorithm for the minimum-cost k-node connected spanning sub graph problem, assuming that the number of nodes is at least k3(k-1)+k. We apply a combinatorial preprocessing, based on the Frank-Tardos algorithm for k-out connectivity, to transform any input into an instance such that the iterative rounding method gives a 2-approximation guarantee. This is the first constant-factor approximation algorithm even in the asymptotic setting of the problem, that is, the restriction to instances where the number of nodes is lower bounded by a function of k.
[approximation theory, iterative methods, combinatorial preprocessing algorithm, graph theory, linear programming, Frank-Tardos algorithm, independence-free graphs, Iterative rounding, minimum-cost k-node connected subgraph problem, Graph connectivity, 2-approximation algorithm, Approximation algorithms, 6-approximation algorithm, iterative rounding method, constant-factor approximation algorithm, Linear Programming]
Candidate Indistinguishability Obfuscation and Functional Encryption for all Circuits
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
In this work, we study indistinguishability obfuscation and functional encryption for general circuits: Indistinguishability obfuscation requires that given any two equivalent circuits C<sub>0</sub> and C<sub>1</sub> of similar size, the obfuscations of C<sub>0</sub> and C<sub>1</sub> should be computationally indistinguishable. In functional encryption, cipher texts encrypt inputs x and keys are issued for circuits C. Using the key SK<sub>C</sub> to decrypt a cipher text CT<sub>x</sub> = Enc(x), yields the value C(x) but does not reveal anything else about x. Furthermore, no collusion of secret key holders should be able to learn anything more than the union of what they can each learn individually. We give constructions for indistinguishability obfuscation and functional encryption that supports all polynomial-size circuits. We accomplish this goal in three steps: - (1) We describe a candidate construction for indistinguishability obfuscation for NC1 circuits. The security of this construction is based on a new algebraic hardness assumption. The candidate and assumption use a simplified variant of multilinear maps, which we call Multilinear Jigsaw Puzzles. (2) We show how to use indistinguishability obfuscation for NC1 together with Fully Homomorphic Encryption (with decryption in NC1) to achieve indistinguishability obfuscation for all circuits. (3) Finally, we show how to use indistinguishability obfuscation for circuits, public-key encryption, and non-interactive zero knowledge to achieve functional encryption for all circuits. The functional encryption scheme we construct also enjoys succinct cipher texts, which enables several other applications.
[NC1 circuits, ciphertext decrypt, functional encryption scheme, noninteractive zero knowledge, Generators, SKC key, functional encryption, Encryption, Complexity theory, input encryption, secret key, equivalent circuits, polynomial-size circuits, public key cryptography, algebraic hardness assumption, fully-homomorphic encryption, Public key, computationally indistinguishable obfuscations, Software, multilinear maps, obfuscation, multilinear jigsaw puzzles, public-key encryption]
Simultaneous Resettability from One-Way Functions
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Resettable-security, introduced by Canetti, Goldreich, Goldwasser and Micali (STOC'00), considers the security of cryptographic two-party protocols (in particular zero-knowledge arguments) in a setting where the attacker may &#x201C;reset&#x201D; or &#x201C;rewind&#x201D; one of the players. The strongest notion of resettable security, simultaneous resettability, introduced by Barak, Goldreich, Goldwasser and Lindell (FOCS'01), requires resettable security to hold for both parties: in the context of zero-knowledge, both the soundness and the zero-knowledge conditions remain robust to resetting attacks. To date, all known constructions of protocols satisfying simultaneous resettable security rely on the existence of ZAPs; constructions of ZAPs are only known based on the existence of trapdoor permutations or number-theoretic assumptions. In this paper, we provide a new method for constructing protocols satisfying simultaneous resettable security while relying only on the minimal assumption of one-way functions. Our key results establish, assuming only one-way functions: Every language in NP has an &#x03C9;(1)-round simultaneously resettable witness indistinguishable argument system; Every language in NP has a (polynomial-round) simultaneously resettable zero-knowledge argument system. The key conceptual insight in our technique is relying on black-box impossibility results for concurrent zero-knowledge to achieve resettable-security.
[&#x03C9;(1)-round simultaneously resettable witness indistinguishable argument system, Schedules, Protocols, NP, black-box impossibility results, cryptographic protocols, proof systems, ZAP, Probabilistic logic, Standards, simultaneous resettable security, resettable WI/ZK/soundness, zero-knowledge conditions, polynomial-round simultaneously resettable zero-knowledge argument system, cryptographic two-party protocols, trapdoor permutations, Polynomials, Cryptography, one-way functions, number-theoretic assumptions, computational complexity, number theory]
From Unprovability to Environmentally Friendly Protocols
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
An important security concern for crypto-graphic protocols is the extent to which they adversely affect the security of the systems in which they run. In particular, can we rule out the possibility that introducing a new protocol to a system might, as a "side effect\
[Protocols, cryptographic protocols, universally composable security, black-box unprovability, external computational entities, protocol security properties, Polynomials, UC security, Cryptography, plain model, Black-Box Unprovability, Angel-Based Security, game theory, black box reduction, chosen-commitment-attack secure commitment scheme, Educational institutions, environmentally friendly protocols, game-based cryptographic hardness assumption, Environmentally Friendliness, CCA-secure commitment scheme, Awards activities, crypto-graphic protocols, Games, secure protocols, angel-based security notion]
How to Approximate a Set without Knowing Its Size in Advance
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The dynamic approximate membership problem asks to represent a set S of size n, whose elements are provided in an on-line fashion, supporting membership queries without false negatives and with a false positive rate at most &#x03B5;. That is, the membership algorithm must be correct on each x &#x2208; S, and may err with probability at most &#x03B5; on each x &#x2209; S. We study a well-motivated, yet insufficiently explored, variant of this problem where the size n of the set is not known in advance. Existing optimal approximate membership data structures require that the size is known in advance, but in many practical scenarios this is not a realistic assumption. Moreover, even if the eventual size n of the set is known in advance, it is desirable to have the smallest possible space usage also when the current number of inserted elements is smaller than n. Our contribution consists of the following results: (1) We show a super-linear gap between the space complexity when the size is known in advance and the space complexity when the size is not known in advance. When the size is known in advance, it is well-known that &#x0398;(n log(1/&#x03B5;)) bits of space are necessary and sufficient (Bloom '70, Carter et al. '78). However, when the size is not known in advance, we prove that at least (1 -o(1))n log(1/&#x03B5;)+&#x03A9;(n log log n) bits of space must be used. In particular, the average number of bits per element must depend on the size of the set. . We show that our space lower bound is tight, and can even be matched by a highly efficient data structure. We present a data structure that uses (1+o(1))n log(1/&#x03B5;)+O(n log log n) bits of space for approximating any set of any size n, without having to know n in advance. Our data structure supports membership queries in constant time in the worst case with high probability, and supports insertions in expected amortized constant time. Moreover, it can be &#x201C;de-amortized&#x201D; to support also insertions in constant time in the worst case with high probability by only increasing its space usage to O(n log(1/&#x03B5;) + n loglogn) bits.
[Dictionaries, false positive rate, Heuristic algorithms, probability, dynamic approximate membership problem, Data Structures, space lower bound, Data structures, Bloom filters, Complexity theory, Approximation methods, set approximation, Standards, query processing, membership queries, Approximate Set membership, Approximation algorithms, data structures, space complexity, computational complexity, approximate membership data structures]
Simple Tabulation, Fast Expanders, Double Tabulation, and High Independence
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Simple tabulation dates back to Zobrist in 1970 who used it for game playing programs. Keys are viewed as consisting of c characters from some alphabet &#x03A6;. We initialize c tables h<sub>0</sub>, ... , h<sub>c-1</sub> mapping characters to random hash values. A key x = (x<sub>0</sub>, ... , x<sub>c-1</sub>) is hashed to h<sub>0</sub>[x<sub>0</sub>]&#x2295;&#x00B7; &#x00B7; &#x00B7;&#x2295;h<sub>c-1</sub>[x<sub>c-1</sub>], where &#x2295; denotes bit-wise exclusive-or. The scheme is extremely fast when the character hash tables h<sub>i</sub> are in cache. Simple tabulation hashing is not even 4-independent, but we show here that if we apply it twice, then we do get high independence. First we hash to some intermediate keys that are 6 times longer than the original keys, and then we hash the intermediate keys to the final hash values. The intermediate keys have d = 6c characters from &#x03A6;. We can then view the hash function as a highly unbalanced bipartite graph with keys on one side, each with edges to d output characters on the other side. We show that this graph has nice expansion properties, and from that it follows that if we perform another level of simple tabulation on the intermediate keys, then the composition is a highly independent hash function. More precisely, the independence we get is |&#x03A6;|&#x03A9;(1/c). In our O-notation, we view both |&#x03A6;| and c is going to infinity, but with c much smaller than |&#x03A6;|. Our space is O(c|&#x03A6;|) and the hash function is evaluated in O(c) time. Siegel [FOCS'89, SICOMP'04] has proved that with this space, if the hash function is evaluated in o(c) time, then the independence can only be o(c), so our evaluation time is best possible for &#x03A9;(c) independence-our independence is much higher if c = |&#x03A6;|o(1/c). Siegel used O(c)c evaluation time to get the same independence with similar space. Siegel's main focus was c = O(1), but we are exponentially faster when c = &#x03C9;(1). Applying our scheme recursively, we can increase our independence to |&#x03A6;|&#x03A9;(1) with o(clogc) evaluation time. Compared with Siegel's scheme this is both faster and higher independence. Siegel states about his scheme that it is &#x201C;far too slow for any practical application&#x201D;. Our scheme is trivial to implement, and it does provide realistic implementations of 100-independent hashing for, say, 32-bit and 64-bit keys.
[unbalanced bipartite graph, graph theory, expanders, Random access memory, hashing, intermediate keys, independence, double tabulation, game playing programs, random hash values, Polynomials, Bipartite graph, Probes, character hash tables, Siegel scheme, independent hash function, fast expanders, Computational modeling, mapping characters, game theory, cryptography, bit-wise exclusive-or, evaluation time, Standards, simple tabulation, high independence, Concrete, computational complexity]
Extractors for a Constant Number of Independent Sources with Polylogarithmic Min-Entropy
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We study the problem of constructing explicit extractors for independent general weak random sources. Given weak sources on n bits, the probabilistic method shows that there exists a deterministic extractor for two independent sources with min-entropy as small as log n+O(1). However, even to extract from a constant number of independent sources, previously the best known extractors require the min-entropy to be at least n&#x03B4; for any constant &#x03B4; &gt; 0 [1], [2], [3]. For sources on n bits with min-entropy k &#x2265; polylog(n), previously the best known extractor needs to use O(log(log n/log k))+O(1) independent sources Li12d. In this paper, we construct the first explicit extractor for a constant number of independent sources on n bits with min-entropy k &#x2265; polylog(n). Thus, for the first time we get extractors for independent sources that are close to optimal. Our extractor is obtained by improving the condenser for structured somewhere random sources in [3], which is based on a connection between the problem of condensing somewhere random sources and the problem of leader election in distributed computing.
[first explicit extractor, Protocols, leader election, independent source, Nominations and elections, probability, constant number, Probabilistic logic, extractor, Entropy, randomness, Distributed computing, polylogarithmic min-entropy, distributed computing, O(log(log n/log k))+O(1) independent sources, Computer science, Cryptography, deterministic extractor, probabilistic method, computational complexity]
A Polynomial Time Algorithm for Lossy Population Recovery
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give a polynomial time algorithm for the lossy population recovery problem. In this problem, the goal is to approximately learn an unknown distribution on binary strings of length n from lossy samples: for some parameter &amp;mu; each coordinate of the sample is preserved with probability &amp;mu; and otherwise is replaced by a `?'. The running time and number of samples needed for our algorithm is polynomial in n and 1/&amp;epsi; for each fixed &amp;mu;&gt;0. This improves on algorithm of Wigderson and Yehudayoff that runs in quasi-polynomial time for any &amp;mu; &gt; 0 and the polynomial time algorithm of Dvir et al which was shown to work for &amp;mu; &gt; rapprox 0.30 by Batman et al. In fact, our algorithm also works in the more general framework of Batman et al. in which there is no a priori bound on the size of the support of the distribution. The algorithm we analyze is implicit in previous work; our main contribution is to analyze the algorithm by showing (via linear programming duality and connections to complex analysis) that a certain matrix associated with the problem has a robust local inverse even though its condition number is exponentially small. A corollary of our result is the first polynomial time algorithm for learning DNFs in the restriction access model of Dvir et al [9].
[Algorithm design and analysis, complex analysis, Linear programming, Vectors, linear programming, learning, Statistics, polynomial time algorithm, robust local inverse, Sensitivity, Sociology, lossy population recovery problem, inverse problems, Polynomials, statistical analysis, quasipolynomial time, DNF, linear programming duality]
OSNAP: Faster Numerical Linear Algebra Algorithms via Sparser Subspace Embeddings
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
An oblivious subspace embedding (OSE) given some parameters &#x03B5;, d is a distribution D over matrices &#x03A0; &#x2208; Rm&#x00D7;n such that for any linear subspace W &#x2286; Rn with dim(W) = d, P<sub>&#x03A0;~D</sub>(&#x2200;x &#x2208; W ||&#x03A0;x||<sub>2</sub> &#x2208; (1 &#x00B1; &#x03B5;)||x||<sub>2</sub>) &gt; 2/3. We show that a certain class of distributions, Oblivious Sparse Norm-Approximating Projections (OSNAPs), provides OSE's with m = O(d1+&#x03B3;/&#x03B5;2), and where every matrix &#x03A0; in the support of the OSE has only s = O<sub>&#x03B3;</sub>(1/&#x03B5;) non-zero entries per column, for &#x03B3; &gt; 0 any desired constant. Plugging OSNAPs into known algorithms for approximate least squares regression, &#x2113;<sub>p</sub> regression, low rank approximation, and approximating leverage scores implies faster algorithms for all these problems. Our main result is essentially a Bai-Yin type theorem in random matrix theory and is likely to be of independent interest: we show that for any fixed U &#x2208; Rn&#x00D7;d with orthonormal columns and random sparse &#x03A0;, all singular values of &#x03A0;U lie in [1 - &#x03B5;, 1 + &#x03B5;] with good probability. This can be seen as a generalization of the sparse Johnson-Lindenstrauss lemma, which was concerned with d = 1. Our methods also recover a slightly sharper version of a main result of [Clarkson-Woodruff, STOC 2013], with a much simpler proof. That is, we show that OSNAPs give an OSE with m = O(d2/&#x03B5;2), s = 1.
[Algorithm design and analysis, &#x2113;p regression, low rank approximation, regression analysis, sparser subspace embeddings, Johnson-Lindenstrauss lemma, type theory, Sparse matrices, numerical linear algebra, subspace embedding, linear subspace, OSNAP, sparse Johnson-Lindenstrauss lemma, random sparse, least squares regression, orthonormal columns, linear algebra, oblivious sparse norm-approximating projections, OSE, approximation theory, least squares approximations, Least squares approximations, leverage scores approximation, probability, Vectors, numerical linear algebra algorithms, nonzero entries, Bai-Yin type theorem, oblivious subspace embedding, random matrix theory, Approximation algorithms, computational complexity]
Iterative Row Sampling
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
There has been significant interest and progress recently in algorithms that solve regression problems involving tall and thin matrices in input sparsity time. Given a n * d matrix where n &amp;ge; d, these algorithms find an approximation with fewer rows, allowing one to solve a poly(d) sized problem instead. In practice, the best performances are often obtained by invoking these routines in an iterative fashion. We show these iterative methods can be adapted to give theoretical guarantees comparable to and better than the current state of the art. Our approaches are based on computing the importances of the rows, known as leverage scores, in an iterative manner. We show that alternating between computing a short matrix estimate and finding more accurate approximate leverage scores leads to a series of geometrically smaller instances. This gives an algorithm whose runtime is input sparsity plus an overhead comparable to the cost of solving a regression problem on the smaller approximation. Our results build upon the close connection between randomized matrix algorithms, iterative methods, and graph sparsification.
[Algorithm design and analysis, iterative methods, Symmetric matrices, sampling methods, regression problems, iterative row sampling method, regression analysis, Probability, Regression, Vectors, short matrix estimate, Approximation methods, matrix algebra, poly(d) sized problem, leverage scores, Runtime, randomized matrix algorithms, Well-conditioned Basis, tall matrices, Approximation algorithms, graph sparsification, thin matrices, Sampling]
Algebraic Algorithms for B-Matching, Shortest Undirected Paths, and F-Factors
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Let G = (V, E) be a graph with f : V &#x2192; Z<sub>+</sub> a function assigning degree bounds to vertices. We present the first efficient algebraic algorithm to find an f-factor. The time is O(f(V )&#x03C9;). More generally for graphs with integral edge weights of maximum absolute value W we find a maximum weight f-factor in time O&#x0303;(Wf(V )&#x03C9;). (The algorithms are correct with high probability and can be made Las Vegas.) We also present three specializations of these algorithms: For maximum weight perfect f-matching the algorithm is considerably simpler (and almost identical to its special case of ordinary weighted matching). For the single-source shortestpath problem in undirected graphs with conservative edge weights, we define a generalization of the shortest-path tree, and we compute it in &#x0303;O&#x0303;(Wn&#x03C9;) time. For bipartite graphs, we improve the known complexity bounds for vertex-capacitated max-flow and min-cost max-flow on a subclass of graphs.
[graph theory, O(Wn&#x03C9;) time, algebra, Complexity theory, F-factors, single-source shortestpath problem, min-cost max-flow, conservative edge weights, f-factors, Polynomials, Bipartite graph, maximum weight perfect f-matching algorithm, b-matching, capacitated max-flow, Educational institutions, integral edge weights, Standards, algebraic algorithms, Computer science, matrix multiplication, shortest undirected paths, b-matching algorithm, maximum absolute value, Periodic structures, computational complexity]
Efficient Accelerated Coordinate Descent Methods and Faster Algorithms for Solving Linear Systems
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
In this paper we show how to accelerate randomized coordinate descent methods and achieve faster convergence rates without paying per-iteration costs in asymptotic running time. In particular, we show how to generalize and efficiently implement a method proposed by Nesterov, giving faster asymptotic running times for various algorithms that use standard coordinate descent as a black box. In addition to providing a proof of convergence for this new general method, we show that it is numerically stable, efficiently implementable, and in certain regimes, asymptotically optimal. To highlight the power of this algorithm, we show how it can used to create faster linear system solvers in several regimes: - We show how this method achieves a faster asymptotic runtime than conjugate gradient for solving a broad class of symmetric positive definite systems of equations. - We improve the convergence guarantees for Kaczmarz methods, a popular technique for image reconstruction and solving over determined systems of equations, by accelerating an algorithm of Strohmer and Vershynin. - We achieve the best known running time for solving Symmetric Diagonally Dominant (SDD) system of equations in the unit-cost RAM model, obtaining a running time of O(m log3/2n (log log n)1/2 log((log n)/eps)) by accelerating a recent solver by Kelner et al. Beyond the independent interest of these solvers, we believe they highlight the versatility of the approach of this paper and we hope that they will open the door for further algorithmic improvements in the future.
[Linear systems, Algorithm design and analysis, linear system solvers, SDD equations system, conjugate gradient, coordinate descent, numerical stability, overdetermined equations systems, Convergence, Kaczmarz methods, symmetric diagonally dominant matrix, symmetric diagonally dominant equations system, Iterative methods, gradient methods, symmetric positive definite equations systems, unit-cost RAM model, Probabilistic logic, linear systems, randomized coordinate descent methods, image reconstruction, Standards, randomised algorithms, standard coordinate descent, asymptotically optimal, Kaczmarz method, convex optimization, convergence rates, black box, Acceleration, asymptotic running time, computational complexity, accelerated coordinate descent methods]
Faster Canonical Forms for Strongly Regular Graphs
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We show that a canonical form for strongly regular (s. r.) graphs can be found in time exp(O&#x0303;(n1/5))and therefore isomorphism of s. r. graphs can be tested within the same time bound, where n is the number of vertices and the tilde hides a polylogarithmic factor. The best previous bound for testing isomorphism of s. r. graphs was exp(O&#x0303;(n1/3)) (Spielman, STOC 1996) while the bound for GI in general has been standing lirmly at exp(O&#x0303;(n1/2)) for three decades. (These results, too, provided canonical forms.) The previous bounds on isomorphism of s. r. graphs (Babai 1980 and Spielman 1996) were based on the analysis of the classical individualization/relinement (I/R) heuristic. The present bound depends on a combination of a deeper analysis of the I/R heuristic with Luks's group theoretic divide-and-conquer methods following BabaiLuks (STOC 1983) and Miller (1983). Our analysis builds on Spielman's work that brought Neumaier's 1979 classilication of s. r. graphs to bear on the problem. One of Neumaier's classes, the line-graphs of Steiner 2-designs, has been eliminated as a bottleneck in recent work by the present authors (STOC'13). In the remaining hard cases, we have the benelit of &#x201C;Neumaier's claw bound&#x201D; and its asymptotic consequences derived by Spielman, some of which we improve via a new &#x201C;clique geometry.&#x201D; We also prove, by an analysis of the I/R heuristic, that, with known (trivial) exceptions, s. r. graphs have exp(O&#x0303;(n9/37)) automorphisms, improving Spielman's exp(O&#x0303;(n1/3)) bound. No knowledge of group theory is required for this paper. The group theoretic method is only used through an easily stated combinatorial consequence (Babai-Luks, 1983 combined with Miller, 1983). While the bulk of this paper is joint work by the live authors, it also includes two contributions by subsets of the authors: the clique geometry [BW] and the automorphism bound [CST].
[Neumaier's classes, algorithms, divide and conquer methods, polylogarithmic factor, graph theory, s. r. graphs, relinement heuristic, regular graphs, Electronic mail, History, Neumaier's claw bound, Steiner 2-designs, deeper analysis, line-graphs, graph isomorphism, strongly regular graphs, Polynomials, Luks's group theoretic divide-and-conquer methods, testing isomorphism, automorphism bound, combinatorial consequence, asymptotic consequences, group theoretic method, Color, Educational institutions, Geometry, Graphics, group theory, canonical forms, classical individualization heuristic, clique geometry, geometry, I/R heuristic, trivial exceptions, computational complexity]
Approximation Algorithms for Euler Genus and Related Problems
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The Euler genus of a graph is a fundamental and well-studied parameter in graph theory and topology. Computing it has been shown to be NP-hard by Thomassen [23], [24], and it is known to be fixed-parameter tractable. However, the approximability of the Euler genus is wide open. While the existence of an O(1)-approximation is not ruled out, only an O(&#x221A;n)-approximation [3] is known even in bounded degree graphs. In this paper we give a polynomialtime algorithm which on input a bounded-degree graph of Euler genus g, computes a drawing into a surface of Euler genus gO(1) &#x00B7; logO(1) n. Combined with the upper bound from [3], our result also implies a O(n1/2-&#x03B1;)-approximation, for some constant &#x03B1; &gt; 0. Using our algorithm for approximating the Euler genus as a subroutine, we obtain, in a unified fashion, algorithms with approximation ratios of the form OPTO(1) &#x00B7; logO(1) n for several related problems on bounded degree graphs. These include the problems of orientable genus, crossing number, and planar edge and vertex deletion problems. Our algorithm and proof of correctness for the crossing number problem is simpler compared to the long and difficult proof in the recent breakthrough by Chuzhoy [5], while essentially obtaining a qualitatively similar result. For planar edge and vertex deletion problems our results are the first to obtain a bound of form poly(OPT, log n). We also highlight some further applications of our results in the design of algorithms for graphs with small genus. Many such algorithms require that a drawing of the graph is given as part of the input. Our results imply that in several interesting cases, we can implement such algorithms even when the drawing is unknown.
[Algorithm design and analysis, graph theory, Euler problems, orientable genus, planar vertex deletion problem, O(1)-approximation, approximation algorithms, Approximation methods, planar edge deletion problem, NP-hard problems, O(&#x221A;n)-approximation, vertex deletion problems, approximation algorithm, gO(1) &#x00B7; logO(1) n Euler genus, Skeleton, polynomial-time algorithm, O(n1/2-&#x03B1;)-approximation, approximation ratio, approximation theory, fixed-parameter tractable, Optimized production technology, topology, poly(OPT, Graph theory, bounded-degree graph, correctness proof, Computer science, crossing number, Euler genus, minimum planarization, non-orientable genus, Approximation algorithms, crossing number problem, computational complexity, Euler genus approximability, log n) bound]
Non-positive Curvature and the Planar Embedding Conjecture
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The planar embedding conjecture asserts that any planar metric admits an embedding into L<sub>1</sub> with constant distortion. This is a well-known open problem with important algorithmic implications, and has received a lot of attention over the past two decades. Despite significant efforts, it has been verified only for some very restricted cases, while the general problem remains elusive. In this paper we make progress towards resolving this conjecture. We show that every planar metric of non-positive curvature admits a constant-distortion embedding into L<sub>1</sub>. This confirms the planar embedding conjecture for the case of non-positively curved metrics.
[planar embedding conjecture, open problem, graph theory, non-positive curvature, planar graphs, computational geometry, constant-distortion embedding, Extraterrestrial measurements, planar metric, planar graph, Geometry, Computer science, Upper bound, sparsest cut, nonpositive curvature, Nickel, Skeleton, constant distortion, multi-commodity flows, metric embeddings, L_1]
All-or-Nothing Multicommodity Flow Problem with Bounded Fractionality in Planar Graphs
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We study the following all-or-nothing multicommodity flow problem in planar graphs. Input: A graph G with n vertices and k pairs of vertices (s<sub>1</sub>, t<sub>1</sub>), (s<sub>2</sub>, t<sub>2</sub>),..., (s<sub>k</sub>, t<sub>k</sub>) in G. Find: A largest subset W of {1, ...., k such that for every i in W, we can send one unit of flow between s<sub>i</sub> and t<sub>i</sub>. This problem is different from the well-known maximum edge-disjoint paths problem in that we do not require integral flows for the pairs. This problem is APX-hard even for trees, and a 2-approximation algorithm is known for trees. For general graphs, Chekuri et al. (STOC'04) give a poly-logarithmic factor approximation algorithm and show that a natural LP-relaxation has a poly-logarithmic integrality gap. This result is in contrast with the integrality gap &#x03A9;(&#x221A;n) for the maximum edge-disjoint paths problem. Our main result considerably strengthens this result when an input graph is planar. Namely, for the all-or-nothing multicommodity flow problem in planar graphs, we give an O(1)-approximation algorithm and show that the integrality gap is O(1). In particular, in polynomial time, we can find an index set W with |W| = &#x03A9;(OPT) and eight s<sub>i</sub>-t<sub>i</sub> paths for each i in W such that each edge is used at most eight times in these paths (with multiplicity), where OPT is the optimal value of the LP-relaxation of the all-or-nothing multicommodity flow problem. Our result can be compared to the recent result by S'eguin-Charbonneau and Shepherd (FOCS'11) who give an O(1)-approximation algorithm for the maximum edge-disjoint paths problem in planar graphs with congestion 2 (but not implied by this result).
[all-or-nothing multicommodity flow problem, Optimized production technology, trees (mathematics), planar graphs, edge-disjoint paths, maximum edge-disjoint paths problem, natural LP-relaxation, linear programming, Indexes, Approximation methods, Upper bound, bounded fractionality, poly-logarithmic integrality gap, 2-approximation algorithm, Approximation algorithms, poly-logarithmic factor approximation algorithm, Polynomials, polynomial time, multicommodity flow, APX-hard problem, Joining processes, computational complexity]
The Planar Directed K-Vertex-Disjoint Paths Problem Is Fixed-Parameter Tractable
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Given a graph G and k pairs of vertices (s<sub>1</sub>, t<sub>1</sub>), ..., (s<sub>k</sub>, t<sub>k</sub>), the k-Vertex-Disjoint Paths problem asks for pair wise vertex-disjoint paths P<sub>1</sub>, ..., Pk such that Pi goes from si to ti. Schrijver [SICOMP'94] proved that the k-Vertex-Disjoint Paths problem on planar directed graphs can be solved in time nO(k). We give an algorithm with running time 22O(k2) * nO(1) for the problem, that is, we show the fixed-parameter tractability of the problem.
[planar graphs, Educational institutions, fixed-parameter tractability, Electronic mail, Standards, Computer science, planar directed k-vertex-disjoint path problem, directed graphs, planar directed graphs, Polynomials, 22O(k2) * nO(1) running time, disjoint paths, Joining processes, Informatics, fixed parameter tractability, computational complexity]
Bandits with Knapsacks
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Multi-armed bandit problems are the predominant theoretical model of exploration-exploitation tradeoffs in learning, and they have countless applications ranging from medical trials, to communication networks, to Web search and advertising. In many of these application domains the learner may be constrained by one or more supply (or budget) limits, in addition to the customary limitation on the time horizon. The literature lacks a general model encompassing these sorts of problems. We introduce such a model, called "bandits with knapsacks\
[Algorithm design and analysis, dynamic procurement, multiarmed bandit problems, Heuristic algorithms, integer programming, dynamic ad allocation, regret, stochastic programming, advertising, knapsack problems, exploration-exploitation tradeoff, routing, Pricing, Benchmark testing, bandits with knapsacks, scheduling, information theory, medical trials, stochastic packing, exploration-exploitation tradeoffs, electronic commerce, dynamic pricing, communication networks, Optimized production technology, Vectors, Multi-armed bandits, stochastic integer programming, information-theoretic optimum, Approximation algorithms, Web search, online learning]
Learning Sums of Independent Integer Random Variables
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Let S = X<sub>1</sub>+&#x00B7;&#x00B7;&#x00B7;+X<sub>n</sub> be a sum of n independent integer random variables X<sub>i</sub>, where each X<sub>i</sub> is supported on {0, 1, ..., k - 1} but otherwise may have an arbitrary distribution (in particular the Xi's need not be identically distributed). How many samples are required to learn the distribution S to high accuracy? In this paper we show that the answer is completely independent of n, and moreover we give a computationally efficient algorithm which achieves this low sample complexity. More precisely, our algorithm learns any such S to &#x03B5;-accuracy (with respect to the total variation distance between distributions) using poly(k, 1/&#x03B5;) samples, independent of n. Its running time is poly(k, 1/&#x03B5;) in the standard word RAM model. Thus we give a broad generalization of the main result of [DDS12b] which gave a similar learning result for the special case k = 2 (when the distribution S is a Poisson Binomial Distribution). Prior to this work, no nontrivial results were known for learning these distributions even in the case k = 3. A key difficulty is that, in contrast to the case of k = 2, sums of independent {0, 1, 2}-valued random variables may behave very differently from (discretized) normal distributions, and in fact may be rather complicated - they are not log-concave, they can be &#x0398;(n)-modal, there is no relationship between Kolmogorov distance and total variation distance for the class, etc. Nevertheless, the heart of our learning result is a new limit theorem which characterizes what the sum of an arbitrary number of arbitrary independent {0, 1, ... , k-1}-valued random variables may look like. Previous limit theorems in this setting made strong assumptions on the &#x201C;shift invariance&#x201D; of the random variables Xi in order to force a discretized normal limit. We believe that our new limit theorem, as the first result for truly arbitrary sums of independent {0, 1, ... , k-1}-valued random variables, is of independent interest.
[Gaussian distribution, independent integer random variables, discrete distribution learning, Complexity theory, Approximation methods, sums of independent integer random variables, sums of discrete distribution learning, Standards, limit theorem, Kolmogorov distance, Accuracy, normal distributions, Poisson binomial distribution, Random variables, stochastic processes, Digital TV, binomial distribution, computational complexity]
Optimal Bounds on Approximation of Submodular and XOS Functions by Juntas
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We investigate the approximability of several classes of real-valued functions by functions of a small number of variables (juntas). Our main results are tight bounds on the number of variables required to approximate a function f:{0, 1}n &#x2192; [0,1] within &#x2113;<sub>2</sub>-error &#x03F5; over the uniform distribution: If f is sub modular, then it is &#x03F5;-close to a function of O(1/&#x03F5;2 log 1/&#x03F5;) variables. This is an exponential improvement over previously known results FeldmanKV:13. We note that &#x03A9;(1/&#x03F5;2) variables are necessary even for linear functions. If f is fractionally sub additive (XOS) it is &amp;epsi;-close to a function of 2O(1/&#x03F5;2) variables. This result holds for all functions with low total &#x2113;<sub>1</sub>-influence and is a real-valued analogue of Fried gut's theorem for boolean functions. We show that 2&#x03A9;(1/&#x03F5;) variables are necessary even for XOS functions. As applications of these results, we provide learning algorithms over the uniform distribution. For XOS functions, we give a PAC learning algorithm that runs in time 21/poly(&#x03F5;) poly(n). For sub modular functions we give an algorithm in the more demanding PMAC learning model BalcanHarvey:[12] which requires a multiplicative (1 + &#x03B3;) factor approximation with probability at least 1 - &#x03F5; over the target distribution. Our uniform distribution algorithm runs in time 21/poly(&#x03B3;&#x03F5;) poly(n). This is the first algorithm in the PMAC model that can achieve a constant approximation factor arbitrarily close to 1 for all sub modular functions (even over the uniform distribution). It relies crucially on our approximation by junta result. As follows from the lower bounds in FeldmanKV:13 both of these algorithms are close to optimal. We also give applications for proper learning, testing and agnostic learning with value queries of these classes.
[optimal bounds, Juntas, exponential improvement, Approximation methods, statistical distributions, fractionally subadditive, Cost accounting, submodular function approximation, PAC learning algorithm, l<sub>2</sub>-error, Boolean functions, constant approximation factor, uniform distribution algorithm, function approximation, fractionally-subadditive, XOS function approximation, learning (artificial intelligence), Testing, PMAC learning model, Context, approximation, PAC learning, real-valued functions, testing, submodular, agnostic learning, Game theory, Friedgut theorem, multiplicative (1 + &#x03B3;) factor approximation, Approximation algorithms, junta, computational complexity]
Estimating the Distance from Testable Affine-Invariant Properties
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Let P be an affine invariant property of multivariate functions over a constant size finite field. We show that if P is locally testable with a constant number of queries, then one can estimate the distance of a function f from P with a constant number of queries. This was previously unknown even for simple properties such as cubic polynomials over the binary field. Our test is simple: take a restriction of f to a constant dimensional affine subspace, and measure its distance from P. We show that by choosing the dimension large enough, this approximates with high probability the global distance of f from P. The analysis combines the approach of Fischer and Newman [SIAM J. Comp 2007] who established a similar result for graph properties, with recently developed tools in higher order Fourier analysis, in particular those developed in Bhattacharyya et al. [STOC 2013].
[multivariate functions, Frequency modulation, Correlation, graph property, graph theory, affine invariant property, constant size finite field, affine transforms, higher-order fourier analysis, testable affine-invariant property, binary field, Polynomials, constant dimensional affine subspace, global distance, Testing, higher order Fourier analysis, polynomials, cubic polynomials, probability, Educational institutions, Probabilistic logic, Fourier analysis, property testing, Computer science, affine invariant properties]
Quasipolynomial-Time Identity Testing of Non-commutative and Read-Once Oblivious Algebraic Branching Programs
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We study the problem of obtaining efficient, deterministic, black-box polynomial identity testing algorithms (PIT) for algebraic branching programs (ABPs) that are read-once and oblivious. This class has an efficient, deterministic, white-box polynomial identity testing algorithm (due to Raz and Shpilka), but prior to this work there was no known such black-box algorithm. The main result of this work gives the first quasi-polynomial sized hitting sets for size S circuits from this class, when the order of the variables is known. As our hitting set is of size exp(lg2 S), this is analogous (in the terminology of boolean pseudorandom ness) to a seed-length of lg2 S, which is the seed length of the pseudorandom generators of Nisan and Impagliazzo-Nisan-Wigderson for read-once oblivious boolean branching programs. Thus our work can be seen as an algebraic analogue of these foundational results in boolean pseudorandom ness. Our results are stronger for branching programs of bounded width, where we give a hitting set of size exp(lg2 S/lglg S), corresponding to a seed length of lg2 S/lglg S. This is in stark contrast to the known results for read-once oblivious boolean branching programs of bounded width, where no pseudorandom generator (or hitting set) with seed length o(lg2 S) is known. Thus, while our work is in some sense an algebraic analogue of existing boolean results, the two regimes seem to have non-trivial differences. In follow up work, we strengthened a result of Mulmuley, and showed that derandomizing a particular case of the No ether Normalization Lemma is reducible to black-box PIT of read-once oblivious ABPs. Using the results of the present work, this gives a derandomization of No ether Normalization in that case, which Mulmuley conjectured would difficult due to its relations to problems in algebraic geometry. We also show that several other circuit classes can be black-box reduced to read-once oblivious ABPs, including set-multilinear ABPs (a generalization of depth-3 set-multilinear formulas), non-commutative ABPs (generalizing non-commutative formulas), and (semi-)diagonal depth-4 circuits (as introduced by Saxena). For set-multilinear ABPs and non-commutative ABPs, we give quasi-polynomial-time black-box PIT algorithms, where the latter case involves evaluations over the algebra of (D+1)x(D+1) matrices, where D is the depth of the ABP. For (semi-)diagonal depth-4 circuits, we obtain a black-box PIT algorithm (over any characteristic) whose run-time is quasi-polynomial in the runtime of Saxena's white-box algorithm, matching the concurrent work of Agrawal, Saha, and Saxena. Finally, by combining our results with the reconstruction algorithm of Klivans and Shpilka, we obtain deterministic reconstruction algorithms for the above circuit classes.
[Algorithm design and analysis, quasipolynomial-time identity testing, read-once oblivious algebraic branching programs, pseudorandom generators, computational geometry, directed acyclic graph, read-once oblivious ABP, set theory, (D+1)x(D+1) matrices, black-box PIT algorithm, branching programs, Polynomials, Noether normalization lemma, noncommutative formula generalization, deterministic reconstruction algorithms, algebraic geometry, noncommutative ABP, read-once oblivious Boolean branching programs, Testing, black-box polynomial identity testing algorithms, depth-3 set-multilinear formulas, Computational modeling, noncommutative algebraic branching programs, polynomial identity testing, Generators, derandomization, Boolean algebra, deterministic algorithms, deterministic polynomial identity testing algorithms, matrix algebra, Geometry, semidiagonal depth-4 circuits, set-multilinear ABP, quasi-polynomial sized hitting sets seed-length, directed graphs, noncommutative polynomials, Impagliazzo-Nisan-Wigderson, Integrated circuit modeling, white-box polynomial identity testing algorithm, computational complexity]
Navigating Central Path with Electrical Flows: From Flows to Matchings, and Back
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We present an O&#x0303;(m10/7) = O&#x0303;(m1.43)-time1 algorithm for the maximum s-t flow and the minimum s-t cut problems in directed graphs with unit capacities. This is the first improvement over the sparse-graph case of the long-standing O(m min{&#x221A;m, n2/3}) running time bound due to Even and Tarjan [16]. By well-known reductions, this also establishes an O&#x0303;(m107)-time algorithm for the maximum-cardinality bipartite matching problem. That, in turn, gives an improvement over the celebrated O(m&#x221A;n) running time bound of Hopcroft and Karp [25] whenever the input graph is sufficiently sparse. At a very high level, our results stem from acquiring a deeper understanding of interior-point methods - a powerful tool in convex optimization - in the context of flow problems, as well as, utilizing certain interplay between maximum flows and bipartite matchings.
[Context, Linear systems, Electric potential, Laplace equations, maximum-cardinality bipartite matching problem, interior-point methods, network theory (graphs), sparse-graph case, convex programming, Vectors, central path, bipartite matchings, directed graphs, Laplacian linear systems, maximum flow problem, Approximation algorithms, electrical flows, convex optimization, minimum s-t cut problem, central path navigation, unit capacities, Erbium]
Nearly Maximum Flows in Nearly Linear Time
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We introduce a new approach to the maximum flow problem in undirected, capacitated graphs using congestion-approximators: easy-to-compute functions that approximate the congestion required to route single-commodity demands in a graph to within some factor &#x03B1;. Our algorithm maintains an arbitrary flow that may have some residual excess and deficits, while taking steps to minimize a potential function measuring the congestion of the current flow plus an over-estimate of the congestion required to route the residual demand. Since the residual term over-estimates, the descent process gradually moves the contribution to our potential function from the residual term to the congestion term, eventually achieving a flow routing the desired demands with nearly minimal congestion after O&#x0303;(&#x03B1;2&#x03B5;-2 log2 n) iterations. Our approach is similar in spirit to that used by Spielman and Teng (STOC 2004) for solving Laplacian systems, and we summarize our approach as trying to do for &#x2113;<sub>&#x221E;</sub>-flows what they do for &#x2113;<sub>&#x221E;</sub>-flows. Together with a nearly linear time construction of a no(1)-congestion-approximator, we obtain 1 + &#x03B5;-optimal singlecommodity flows undirected graphs in time m1+o(1)&#x03B5;-2, yielding the fastest known algorithm for that problem. Our requirements of a congestion-approximator are quite low, suggesting even faster and simpler algorithms for certain classes of graphs. For example, an &#x03B1;-competitive oblivious routing tree meets our definition, even without knowing how to route the tree back in the graph. For graphs of conductance &#x03C6;, a trivial &#x03C6;-1-congestionapproximator gives an extremely simple algorithm for finding O&#x0303;(m&#x03C6;-1).
[iterative methods, nearly linear time construction, graph theory, graph conductance, nearly maximum flows, flow congestion, descent process, Approximation methods, optimisation, single-commodity demand routing, iteration, residual excess, undirected capacitated graph, Laplacian system, Laplace equations, nearly minimal congestion, congestion-approximators, Routing, Vectors, Equations, flow routing, function minimization, optimal single-commodity flow undirected graph, residual demand, Vegetation, maximum flow problem, Approximation algorithms, &#x03B1;-competitive oblivious routing tree, computational complexity]
Towards a Better Approximation for Sparsest Cut?
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give a new (1 + &#x03B5;)-approximation for SPARSEST CUT problem on graphs where small sets expand significantly more than the sparsest cut (expansion of sets of size n/r exceeds that of the sparsest cut by a factor &#x221A;log n log r, for some small r; this condition holds for many natural graph families). We give two different algorithms. One involves Guruswami-Sinop rounding on the level-r Lasserre relaxation. The other is combinatorial and involves a new notion called Small Set Expander Flows (inspired by the expander flows of [1]) which we show exists in the input graph. Both algorithms run in time 2O(r)poly(n). We also show similar approximation algorithms in graphs with genus g with an analogous local expansion condition. This is the first algorithm we know of that achieves (1 + &#x03B5;)-approximation on such general family of graphs.
[small set expansion, Particle separators, graph theory, Guruswami-Sinop rounding, small set expander flow, Vectors, Graph theory, approximation algorithms, Approximation methods, expander flows, sparsest cut, Games, Approximation algorithms, Eigenvalues and eigenfunctions, level-r Lasserre relaxation, graph partitioning, (1 + &#x03B5;)-approximation, computational complexity]
Layered Separators for Queue Layouts, 3D Graph Drawing and Nonrepetitive Coloring
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Graph separators are a ubiquitous tool in graph theory and computer science. However, in some applications, their usefulness is limited by the fact that the separator can be as large as &#x03A9;(&#x221A;n) in graphs with n vertices. This is the case for planar graphs, and more generally, for proper minor-closed families. We study a special type of graph separator, called a layered separator, which may have linear size in n, but has bounded size with respect to a different measure, called the breadth. We prove that a wide class of graphs admit layered separators of bounded breadth, including graphs of bounded Euler genus. We use layered separators to prove O&#x0303;(log n) bounds for a number of problems where O(&#x221A;n) was a long standing previous best bound. This includes the nonrepetitive chromatic number and queue-number of graphs with bounded Euler genus. We extend these results to all proper minor-closed families, with a O(log n) bound on the nonrepetitive chromatic number, and a logO(1)n bound on the queue-number. Only for planar graphs were logO(1)n bounds previously known. Our results imply that every graph from a proper minor-closed class has a 3-dimensional grid drawing with n logO(1)n volume, whereas the previous best bound was O(n3/2). Readers interested in the full details should consult arXiv:1302.0304 and arXiv:1306.1595, rather than the current extended abstract.
[queue layouts, minor, nonrepetitive coloring, planar graphs, linear size, graph colouring, bounded Euler genus graphs, nonrepetitive colouring, graph separators, graph vertices, ubiquitous tool, graph drawing, queue-number, O(&#x221A;n) bound, n logO(1)n volume, Particle separators, logO(1)n bound, Color, nonrepetitive chromatic number, Educational institutions, separator, graph, minor-closed families, bounded size, O(log n) bounds, Computer science, queue layout, layered separators, Upper bound, Layout, bounded breadth measurement, 3D graph drawing, 3-dimensional grid drawing, Queueing analysis, track layout]
Element Distinctness, Frequency Moments, and Sliding Windows
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We derive new time-space tradeoff lower bounds and algorithms for exactly computing statistics of input data, including frequency moments, element distinctness, and order statistics, that are simple to calculate for sorted data. We develop a randomized algorithm for the element distinctness problem whose time T and space S satisfy T &#x2208; O&#x0303; (n3/2/S1/2), smaller than previous lower bounds for comparison-based algorithms, showing that element distinctness is strictly easier than sorting for randomized branching programs. This algorithm is based on a new time and space efficient algorithm for finding all collisions of a function f from a finite set to itself that are reachable by iterating f from a given set of starting points. We further show that our element distinctness algorithm can be extended at only a polylogarithmic factor cost to solve the element distinctness problem over sliding windows, where the task is to take an input of length 2n-1 and produce an output for each window of length n, giving n outputs in total. In contrast, we show a time-space tradeoff lower bound of T &#x2208; &#x03A9;(n2/S) for randomized branching programs to compute the number of distinct elements over sliding windows. The same lower bound holds for computing the low-order bit of F<sub>0</sub> and computing any frequency moment F<sub>k</sub>, &#x2260; 1. This shows that those frequency moments and the decision problem F<sub>0</sub> mod 2 are strictly harder than element distinctness. We complement this lower bound with a T &#x2208; O&#x0303;(n2/S) comparison-based deterministic RAM algorithm for exactly computing F<sub>k</sub> over sliding windows, nearly matching both our lower bound for the sliding-window version and the comparison-based lower bounds for the single-window version. We further exhibit a quantum algorithm for F<sub>0</sub> over sliding windows with T &#x2208; O(n3/2/S1/2). Finally, we consider the computations of order statistics over sliding windows.
[Algorithm design and analysis, randomized branching program, randomized multiway branching programs, Collision finding, polylogarithmic factor, sliding-window version, Random access memory, Time-space tradeoffs, Complexity theory, set theory, space-efficient algorithm, time-efficient algorithm, comparison- based lower bounds, comparison-based algorithm, order statistics, word-RAM models, finite set, element distinctness, Educational institutions, Standards, Frequency moments, Sorting, Computer science, Branching programs, frequency moments, statistical analysis, computational complexity, RAM models]
Spatial Mixing and Approximation Algorithms for Graphs with Bounded Connective Constant
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The hard core model in statistical physics is a probability distribution on independent sets in a graph in which the weight of any independent set I is proportional to &#x03BB;|I|, where &#x03BB; &gt; 0 is the vertex activity. We show that there is an intimate connection between the connective constant of a graph and the phenomenon of strong spatial mixing (decay of correlations) for the hard core model; specifically, we prove that the hard core model with vertex activity &#x03BB; &lt;; &#x03BB;<sub>c</sub>(&#x0394;+1) exhibits strong spatial mixing on any graph of connective constant &#x0394;, irrespective of its maximum degree, and hence derive an FPTAS for the partition function of the hard core model on such graphs. Here &#x03BB;<sub>c</sub>(d) &#x00B7;&#x00B7;= dd/(d-1)d+1 is the critical activity for the uniqueness of the Gibbs measure of the hard core model on the infinite d-ary tree. As an application, we show that the partition function can be efficiently approximated with high probability on graphs drawn from the random graph model G (n, d/n) for all &#x03BB; &lt;; e/d, even though the maximum degree of such graphs is unbounded with high probability. We also improve upon Weitz's bounds for strong spatial mixing on bounded degree graphs [30] by providing a computationally simple method which uses known estimates of the connective constant of a lattice to obtain bounds on the vertex activities &#x03BB; for which the hard core model on the lattice exhibits strong spatial mixing. Using this framework, we improve upon these bounds for several lattices including the Cartesian lattice in dimensions 3 and higher. Our techniques also allow us to relate the threshold for the uniqueness of the Gibbs measure on a general tree to its branching factor [15].
[hard core model, Gibbs measure, Correlation, approximate counting, Lattices, bounded degree graphs, bounded connective constant, infinite d-ary tree, set theory, Approximation methods, statistical distributions, Weitz bounds, branching factor, approximation algorithm, probability distribution, random graph model, lattice connective constant estimation, approximation theory, Cartesian lattice, trees (mathematics), Boundary conditions, lattice theory, phase transitions, Partitioning algorithms, independent sets, connective constant, Physics, spatial mixing algorithm, statistical physics, vertex activity, Approximation algorithms, FPTAS, Decay of correlations]
Polar Codes: Speed of Polarization and Polynomial Gap to Capacity
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We prove that, for all binary-input symmetric memory less channels, polar codes enable reliable communication at rates within &#x03B5; &gt; 0 of the Shannon capacity with a block length, construction complexity, and decoding complexity all bounded by a polynomial in 1/&#x03B5;. Polar coding gives the first known explicit construction with rigorous proofs of all these properties. We give an elementary proof of the capacity achieving property of polar codes that does not rely on the martingale convergence theorem. As a result, we are able to explicitly show that polar codes can have block length (and consequently also encoding and decoding complexity) that is bounded by a polynomial in the gap to capacity. The generator matrix of such polar codes can be constructed in polynomial time using merging of channel output symbols to reduce the alphabet size of the channels seen at the decoder.
[Error probability, linear codes, channel coding, decoding complexity, Entropy, binary-input symmetric memoryless channels, encoding complexity, Complexity theory, channel output symbols, Convergence, symmetric capacity, alphabet size, entropy, polar coding, Polynomials, polarization, polynomial time, polar codes, Shannon capacity, channel polarization, polynomials, block length, codecs, Encoding, Decoding, decoder, maximum likelihood decoding, decoding, error-correction codes, polynomial gap, reliable communication, generator matrix, Information theory]
Constant Rate PCPs for Circuit-SAT with Sublinear Query Complexity
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The PCP theorem (Arora et. al., J. ACM 45(1, 3)) says that every NP-proof can be encoded to another proof, namely, a probabilistically checkable proof (PCP), which can be tested by a verifier that queries only a small part of the PCP. A natural question is how large is the blow-up incurred by this encoding, i.e., how long is the PCP compared to the original NP-proof. The state-of-the-art work of Ben-Sasson and Sudan (SICOMP 38(2)) and Dinur (J. ACM 54(3)) shows that one can encode proofs of length n by PCPs of quasi-linear length that can be verified using a constant number of queries. In this work, we show that if the query complexity is relaxed to polynomial, then one can construct PCPs of linear length for circuit-SAT, and PCPs of length O(tlog t) for any language in NTIME(t). Our PCPs have perfect completeness and constant soundness. This is the first constant-rate PCP construction that achieves constant soundness with nontrivial query complexity. Our proof replaces the low-degree polynomials in algebraic PCP constructions with tensors of transitive algebraic geometry (AG) codes. We show that the automorphisms of an AG code can be used to simulate the role of affine transformations which are crucial in earlier high-rate algebraic PCP constructions. Using this observation we conclude that any asymptotically good family of transitive AG codes over a constant-sized alphabet leads to a family of constant-rate PCPs with polynomially small query complexity. Such codes are constructed for the first time for every message length.
[Protocols, computability, computational geometry, sublinear query complexity, tensors, constant-sized alphabet, Complexity theory, quasilinear length, affine transforms, query processing, low-degree polynomials, NP-proof encoding, algebraic geometric codes, Polynomials, circuit-SAT, Testing, probabilistically checkable proofs, AG codes, nontrivial query complexity, probability, message length, affine transformations, probabilistically checkable proof, transitive algebraic geometry codes, Tensile stress, transitive AG codes, NTIME(t), Linear codes, algebraic PCP constructions, constant rate PCP theorem, computational complexity]
Strong LTCs with Inverse Poly-Log Rate and Constant Soundness
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
An error-correcting code C is called (q, &#x03F5;)-strong locally testable code (LTC) if there exists a tester that makes at most q queries to the input word. This tester accepts all code words with probability 1 and rejects all non-code words x with probability at least &#x03F5; &#x00B7; &#x03B4;(x, C), where &#x03B4;(x, C) denotes the relative Hamming distance between the word x and the code C. The parameter q is called the query complexity and the parameter &#x03F5; is called soundness. In this paper we resolve an open question raised by Gold Reich and Sudan (J. ACM 2006) and construct binary linear strong LTCs with query complexity 3, constant relative distance, constant soundness and inverse polylogarithmic rate. Our result is based on the previous paper of the author (Vide man, ECCC TR12-168), which presented binary linear strong LTCs with query complexity 3, constant relative distance, and inverse polylogarithmic soundness and rate. We show that the "gap amplification" procedure of Dinur (J. ACM 2007) can be used to amplify the soundness of these strong LTCs from inverse polylogarithmic up to a constant, while preserving the other parameters of these codes. Furthermore, we show that under a conceivable conjecture, there exist asymptotically good strong LTCs with poly-log query complexity.
[linear codes, Hamming codes, inverse polylogarithmic soundness, Complexity theory, error-correcting code C, code words, query processing, inverse poly-log rate, Parity check codes, theorem proving, binary codes, Hamming distance, error correction codes, probability, locally testable codes, Probabilistic logic, poly-log query complexity, constant relative distance, Computer science, gap amplification, constant soundness, inverse polylogarithmic rate, error-correcting codes, PCPs, binary linear strong LTC, Linear codes, Error correction codes, locally testable code, computational complexity]
PCPs via Low-Degree Long Code and Hardness for Constrained Hypergraph Coloring
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We develop new techniques to incorporate the recently proposed &#x201C;short code&#x201D; (a low-degree version of the long code) into the construction and analysis of PCPs in the classical &#x201C;Label Cover + Fourier Analysis&#x201D; framework. As a result, we obtain more size-efficient PCPs that yield improved hardness results for approximating CSPs and certain coloringtype problems. In particular, we show a hardness for a variant of hypergraph coloring (with hyperedges of size 6), with a gap between 2 and exp(2&#x03A9;(&#x221A;log log N)) number of colors where N is the number of vertices. This is the first hardness result to go beyond the O(log N) barrier for a coloring-type problem. Our hardness bound is a doubly exponential improvement over the previously known O(log log N)-coloring hardness for 2-colorable hypergraphs, and an exponential improvement over the (logN)&#x03A9;(1)-coloring hardness for O(1)-colorable hypergraphs. Stated in terms of &#x201C;covering complexity,&#x201D; we show that for 6-ary Boolean CSPs, it is hard to decide if a given instance is perfectly satisfiable or if it requires more than 2&#x03A9;(&#x221A;log log N) assignments for covering all of the constraints. While our methods do not yield a result for conventional hypergraph coloring due to some technical reasons, we also prove hardness of (log N)&#x03A9;(1)-coloring 2-colorable 6-uniform hypergraphs (this result relies just on the long code). A key algebraic result driving our analysis concerns a very low-soundness error testing method for Reed-Muller codes. We prove that if a function &#x03B2; : F<sub>2</sub>m &#x2192; F<sub>2</sub> is 2&#x03A9;(d) far in absolute distance from polynomials of degree m-d, then the probability that deg(&#x03B2;g) &#x2264; m-3d/4 for a random degree d/4 polynomial g is doubly exponentially small in d.
[Frequency modulation, coloring-type problem, low-soundness error testing method, low-degree long code, Noise, short code, Approximation methods, doubly exponential improvement, 6-ary Boolean CSP, graph colouring, colorable hypergraphs, coloring hardness, Polynomials, random degree, Testing, polynomials, probability, Color, hardness bound, Fourier analysis, Reed-Muller codes, constrained hypergraph coloring, Boolean algebra, label cover + Fourier analysis framework, Computer science, constraint satisfaction problems, PCPs, quasiNP-hardness, covering complexity, Hardness of approximation, PCP, computational complexity]
Approximate Constraint Satisfaction Requires Large LP Relaxations
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We prove super-polynomial lower bounds on the size of linear programming relaxations for approximation versions of constraint satisfaction problems. We show that for these problems, polynomial-sized linear programs are exactly as powerful as programs arising from a constant number of rounds of the Sherali-Adams hierarchy. In particular, any polynomial-sized linear program for MAX CUT has an integrality gap of 1/2 and any such linear program for MAX 3-SAT has an integrality gap of 7/8.
[LP hierarchies, LP relaxation, graph theory, Optimized production technology, polynomial-sized linear programs, approximation complexity, computability, Linear programming, Vectors, Entropy, linear programming, Complexity theory, Approximation methods, superpolynomial lower bounds, lower bounds, approximate constraint satisfaction problem, constraint satisfaction problems, extended formulations, Sherali-Adams hierarchy, MAX CUT, MAX 3-SAT, linear programming relaxations, Polynomials, computational complexity]
The Complexity of Approximating Vertex Expansion
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We study the complexity of approximating the vertex expansion of graphs G = (V, E), defined as &#x03A6;V def = minSCV n . |N(S)|/(|S||V\\S). We give a simple polynomialtime algorithm for finding a subset with vertex expansion O(&#x221A;(&#x03A6;V log d)) where d is the maximum degree of the graph. Our main result is an asymptotically matching lower bound: under the Small Set Expansion (SSE) hypothesis, it is hard to find a subset with expansion less than C(&#x221A;(&#x03A6;V log d)) for an absolute constant C. In particular, this implies for all constant &#x03B5; &gt; 0, it is SSE-hard to distinguish whether the vertex expansion &lt;; &#x03B5; or at least an absolute constant. The analogous threshold for edge expansion is &#x221A;&#x03A6; with no dependence on the degree (Here &#x03A6; denotes the optimal edge expansion). Thus our results suggest that vertex expansion is harder to approximate than edge expansion. In particular, while Cheeger's algorithm can certify constant edge expansion, it is SSE-hard to certify constant vertex expansion in graphs.
[approximation theory, Particle separators, graph theory, graphs vertex expansion, Small Set Expansion, vertex expansion approximation complexity, Approximation methods, Vertex Expansion, Graph Partitioning, Hardness of Approximation, Games, Markov processes, Approximation algorithms, Eigenvalues and eigenfunctions, polynomial-time algorithm, graph partitioning, small set expansion hypothesis, Testing, computational complexity]
Independent Set, Induced Matching, and Pricing: Connections and Tight (Subexponential Time) Approximation Hardnesses
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We present a series of almost settled inapproximability results for three fundamental problems. The first in our series is the subexponential-time inapproximability of the independent set problem, a question studied in the area of parameterized complexity. The second is the hardness of approximating the bipartite induced matching problem on bounded-degree bipartite graphs. The last in our series is the tight hardness of approximating the k-hypergraph pricing problem, a fundamental problem arising from the area of algorithmic game theory. In particular, assuming the Exponential Time Hypothesis, our two main results are: For any r larger than some constant, any r-approximation algorithm for the independent set problem must run in at least 2n1-&#x03B5;/r1+&#x03B5; time. This nearly matches the upper bound of 2n/r [23]. It also improves some hardness results in the domain of parameterized complexity (e.g., [26], [19]). For any k larger than some constant, there is no polynomial time min{k1-&#x03B5;, n1/2-&#x03B5;} time min -approximation algorithm for the k-hypergraph pricing problem , where n is the number of vertices in an input graph. This almost matches the upper bound of min{O(k), O&#x0303;(&#x221A;n) } min (by Balcan and Blum [3] and an algorithm in this paper). We note an interesting fact that, in contrast to n1/2-&#x03B5; hardness for polynomial-time algorithms, the k-hypergraph pricing problem admits n&#x03B4; approximation for any &#x03B4; &gt; 0 in quasi-polynomial time. This puts this problem in a rare approximability class in which approximability thresholds can be improved significantly by allowing algorithms to run in quasi-polynomial time. The proofs of our hardness results rely on unexpectedly tight connections between the three problems. First, we establish a connection between the first and second problems by proving a new graph-theoretic property related to an induced matching number of dispersers. Then, we show that the n1/2-&#x03B5; hardness of the last problem follows from nearly tight subexponential time inapproximability of the first problem, illustrating a rare application of the second type of inapproximability result to the first one. Finally, to prove the subexponential-time inapproximability of the first problem, we construct a new PCP with several properties; it is sparse and has nearly-linear size, large degree, and small free-bit complexity. Our PCP requires no ground-breaking ideas but rather a very careful assembly of the existing ingredients in the PCP literature.
[polynomial time min{k1-&#x03B5;, graph theory, Algorithmic Pricing, Subexponential-Time Algorithms, graph-theoretic property, parameterized complexity, Complexity theory, set theory, Approximation methods, tight subexponential time approximation hardnesses, r-approximation algorithm, small free-bit complexity, Pricing, Polynomials, Bipartite graph, bounded-degree bipartite graphs, approximation theory, almost settled inapproximability, bipartite induced matching problem approximation hardness, k-hypergraph pricing problem, game theory, exponential time hypothesis, algorithmic game theory, Approximation Algorithms, independent set problem, Upper bound, n1/2-&#x03B5;} time min-approximation algorithm, subexponential-time inapproximability, Approximation algorithms, quasi-polynomial time, pricing, computational complexity]
Chasing the K-Colorability Threshold
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
In this paper we establish a substantially improved lower bound on the k-color ability threshold of the random graph G(n, m) with n vertices and m edges. The new lower bound is &#x2248; 1.39 less than the 2k ln (k)-ln (k) first-moment upper bound (and approximately 0.39 less than the 2k ln (k) - ln(k) - 1 physics conjecture). By comparison, the best previous bounds left a gap of about 2+ln(k), unbounded in terms of the number of colors [Achlioptas, Naor: STOC 2004]. Furthermore, we prove that, in a precise sense, our lower bound marks the so-called condensation phase transition predicted on the basis of physics arguments [Krzkala et al.: PNAS 2007]. Our proof technique is a novel approach to the second moment method, inspired by physics conjectures on the geometry of the set of k-colorings of the random graph.
[random graph, condensation phase transition, Color, phase transitions, graph coloring, set theory, physics arguments, Physics, Optimization, graph colouring, Geometry, second moment method, set geometry, substantial lower bound improvement, Cavity resonators, k-colorability threshold, Random variables, random structures, Method of moments, physics conjectures]
On Clustering Induced Voronoi Diagrams
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
In this paper, we study a generalization of the classical Voronoi diagram, called clustering induced Voronoi diagram (CIVD). Different from the traditional model, CIVD takes as its sites the power set U of an input set P of objects. For each subset C of P, CIVD uses an influence function F(C, q) to measure the total (or joint) influence of all objects in C on an arbitrary point q in the space &#x211D;d, and determines the influence-based Voronoi cell in &#x211D;d for C. This generalized model offers a number of new features (e.g., simultaneous clustering and space partition) to Voronoi diagram which are useful in various new applications. We investigate the general conditions for the influence function which ensure the existence of a small-size (e.g., nearly linear) approximate CIVD for a set P of n points in &#x211D;d for some fixed d. To construct CIVD, we first present a standalone new technique, called approximate influence (AI) decomposition, for the general CIVD problem. With only O(n log n) time, the AI decomposition partitions the space &#x211D;d into a nearly linear number of cells so that all points in each cell receive their approximate maximum influence from the same (possibly unknown) site (i.e., a subset of P). Based on this technique, we develop assignment algorithms to determine a proper site for each cell in the decomposition and form various (1-&#x03B5;)-approximate CIVDs for some small fixed &#x20AC; &gt; 0. Particularly, we consider two representative CIVD problems, vector CIVD and density-based CIVD, and show that both of them admit fast assignment algorithms; consequently, their (1 - &#x20AC;)-approximate CIVDs can be built in O(n logd+1 n) and O(n log2 n) time, respectively.
[approximation theory, influence-based Voronoi cell, Social network services, Communities, computational geometry, Data structures, Vectors, CIVD, influence function, power set U, Voronoi diagram, clustering induced Voronoi diagram, approximate influence decomposition, pattern clustering, assignment algorithms, Approximation algorithms, clustering, Artificial intelligence, Joints, computational complexity]
Approximation Schemes for Maximum Weight Independent Set of Rectangles
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
In the Maximum Weight Independent Set of Rectangles (MWISR) problem we are given a set of n axis-parallel rectangles in the 2D-plane, and the goal is to select a maximum weight subset of pairwise non-overlapping rectangles. Due to many applications, e.g. in data mining, map labeling and admission control, the problem has received a lot of attention by various research communities. We present the first (1 + &#x03B5;)-approximation algorithm for the MWISR problem with quasipolynomial running time 2poly(log n/&#x03B5;). In contrast, the best known polynomial time approximation algorithms for the problem achieve superconstant approximation ratios of O(log log n) (unweighted case) and O(log n/log log n) (weighted case). Key to our results is a new geometric dynamic program which recursively subdivides the plane into polygons of bounded complexity. We provide the technical tools that are needed to analyze its performance. In particular, we present a method of partitioning the plane into small and simple areas such that the rectangles of an optimal solution are intersected in a very controlled manner. Together with a novel application of the weighted planar graph separator theorem due to Arora et al. [4] this allows us to upper bound our approximation ratio by 1 + &#x03B5;. Our dynamic program is very general and we believe that it will be useful for other settings. In particular, we show that, when parametrized properly, it provides a polynomial time (1 + &#x03B5;)-approximation for the special case of the MWISR problem when each rectangle is relatively large in at least one dimension. Key to this analysis is a method to tile the plane in order to approximately describe the topology of these rectangles in an optimal solution. This technique might be a useful insight to design better polynomial time approximation algorithms or even a PTAS for the MWISR problem. In particular, note that our results imply that the MWISR problem is not APX-hard, unless NP &#x2286; DTIME(2polylog (n)).
[approximation ratio, bounded complexity, quasipolynomial running time, Shape, weighted planar graph separator theorem, Heuristic algorithms, graph theory, dynamic programming, Partitioning algorithms, maximum weight independent set of rectangles problem, Approximation methods, superconstant approximation ratios, maximum weight subset, MWISR problem, PTAS, polynomial approximation, n axis-parallel rectangles, Approximation algorithms, polynomial time approximation algorithms, Polynomials, pairwise nonoverlapping rectangles, geometric dynamic program, Face, computational complexity]
Klee's Measure Problem Made Easy
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We present a new algorithm for a classic problem in computational geometry, Klee's measure problem: given a set of n axis-parallel boxes in d-dimensional space, compute the volume of the union of the boxes. The algorithm runs in O(nd/2) time for any constant d &#x2265; 3. Although it improves the previous best algorithm by &#x201C;just&#x201D; an iterated logarithmic factor, the real surprise lies in the simplicity of the new algorithm. We also show that it is theoretically possible to beat the O(nd/2) time bound by logarithmic factors for integer input in the word RAM model, and for other variants of the problem. With additional work, we obtain an O(nd/3 polylog n)-time algorithm for the important special case of orthants or unit hypercubes (which include the so-called &#x201C;hypervolume indicator problem&#x201D;), and an O(n(d+1)/3 polylog n)-time algorithm for the case of arbitrary hypercubes or fat boxes, improving a previous O(n(d+2)/3)-time algorithm by Bringmann.
[boxes, RAM model, orthants, d-dimensional space, Heuristic algorithms, Random access memory, computational geometry, iterated logarithmic factor, Complexity theory, Standards, unit hypercubes, volume, Volume measurement, Bismuth, Hypercubes, Klee measure problem, hypervolume indicator problem, axis-parallel box]
Playing Non-linear Games with Linear Oracles
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Linear optimization is many times algorithmically simpler than non-linear convex optimization. Linear optimization over matroid polytopes, matching polytopes and path polytopes are example of problems for which we have efficient combinatorial algorithms, but whose non-linear convex counterpart is harder and admit significantly less efficient algorithms. This motivates the computational model of online decision making and optimization using a linear optimization oracle. In this computational model we give the first efficient decision making algorithm with optimal regret guarantees, answering an open question of Kalai and Vempala, Hazan and Kale, in case the decision set is a polytope. We also give an extension of the algorithm for the partial information setting, i.e. the "bandit" model. Our method is based on a novel variant of the conditional gradient method, or Frank-Wolfe algorithm, that reduces the task of minimizing a smooth convex function over a domain to that of minimizing a linear objective. Whereas previous variants of this method give rise to approximation algorithms, we give such algorithm that converges exponentially faster and thus runs in polynomial-time for a large class of convex optimization problems over polyhedral sets, a result of independent interest.
[Algorithm design and analysis, Gradient methods, linear optimization oracle, convex optimization problems, online decision making algorithm, combinatorial mathematics, combinatorial algorithms, Online Algorithms, matching polytopes, Regret Minimization, approximation algorithms, Frank-Wolfe algorithm, Convergence, smooth convex function, matroid polytopes, conditional gradient method, optimisation, path polytopes, Games, Convex Optimization, Approximation algorithms, Convex functions, computational complexity]
Local Privacy and Statistical Minimax Rates
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Working under local differential privacy-a model of privacy in which data remains private even from the statistician or learner-we study the tradeoff between privacy guarantees and the utility of the resulting statistical estimators. We prove bounds on information-theoretic quantities, including mutual information and Kullback-Leibler divergence, that influence estimation rates as a function of the amount of privacy preserved. When combined with minimax techniques such as Le Cam's and Fano's methods, these inequalities allow for a precise characterization of statistical rates under local privacy constraints. In this paper, we provide a treatment of two canonical problem families: mean estimation in location family models and convex risk minimization. For these families, we provide lower and upper bounds for estimation of population quantities that match up to constant factors, giving privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds.
[Data privacy, Differential privacy, TV, Estimation, convex risk minimization, privacy-preserving mechanisms, statistical estimators, Kullback-Leibler divergence, minimax techniques, Zinc, information-theoretic quantities, estimation, Privacy, Upper bound, local privacy constraints, location family models, data privacy, information theory, statistical minimax rates, mutual information, minimax rates, statistical analysis, Mutual information, differential privacy]
Coupled-Worlds Privacy: Exploiting Adversarial Uncertainty in Statistical Data Privacy
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We propose a new framework for defining privacy in statistical databases that enables reasoning about and exploiting adversarial uncertainty about the data. Roughly, our framework requires indistinguishability of the real world in which a mechanism is computed over the real dataset, and an ideal world in which a simulator outputs some function of a "scrubbed" version of the dataset (e.g., one in which an individual user's data is removed). In each world, the underlying dataset is drawn from the same distribution in some class (specified as part of the definition), which models the adversary's uncertainty about the dataset. We argue that our framework provides meaningful guarantees in a broader range of settings as compared to previous efforts to model privacy in the presence of adversarial uncertainty. We also show that several natural, "noiseless" mechanisms satisfy our definitional framework under realistic assumptions on the distribution of the underlying data.
[Data privacy, Uncertainty, statistical data privacy, Educational institutions, coupled-worlds privacy, Computer science, natural noiseless mechanisms, Privacy, Databases, data privacy, statistical databases, Random variables, adversarial uncertainty]
Knowledge-Preserving Interactive Coding
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
How can we encode a communication protocol between two parties to become resilient to adversarial errors on the communication channel? If we encode each message in the communication protocol with a "good" error-correcting code (ECC), the error rate of the encoded protocol becomes poor (namely O(1/m) where m is the number of communication rounds). Towards addressing this issue, Schulman (FOCS'92, STOC'93) introduced the notion of interactive coding. We argue that whereas the method of separately encoding each message with an ECC ensures that the encoded protocol carries the same amount of information as the original protocol, this may no longer be the case if using interactive coding. In particular, the encoded protocol may completely leak a player's private input, even if it would remain secret in the original protocol. Towards addressing this problem, we introduce the notion of knowledge-preserving interactive coding, where the interactive coding protocol is required to preserve the "knowledge" transmitted in the original protocol. Our main results are as follows: The method of separately applying ECCs to each message has essentially optimal error rate: No knowledge-preserving interactive coding scheme can have an error rate of 1/m, where m is the number of rounds in the original protocol; If restricting to computationally-bounded (polynomial-time) adversaries, then assuming the existence of one-way functions (resp. sub exponentially-hard one-way functions), for every &#x03F5; &gt; 0, there exists a knowledge-preserving interactive coding schemes with constant error rate and information rate n-&#x03F5; (resp. 1/polylog(n)) where n is the security parameter; additionally to achieve an error of even 1/m requires the existence of one-way functions; Finally, even if we restrict to computationally-bounded adversaries, knowledge-preserving interactive coding schemes with constant error rate can have an information rate of at most o(1 log n). This results applies even to non-constructive interactive coding schemes.
[nonconstructive interactive coding scheme, Protocols, Error analysis, error correction codes, ECC, optimal error rate, message encoding, communication protocol, Encoding, Security, Information rates, knowledge-preserving interactive coding, Polynomials, encoded protocol, Error correction codes, protocols, error-correcting code]
Adaptive Seeding in Social Networks
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The algorithmic challenge of maximizing information diffusion through word-of-mouth processes in social networks has been heavily studied in the past decade. While there has been immense progress and an impressive arsenal of techniques has been developed, the algorithmic frameworks make idealized assumptions regarding access to the network that can often result in poor performance of state-of-the-art techniques. In this paper we introduce a new framework which we call Adaptive Seeding. The framework is a two-stage stochastic optimization model designed to leverage the potential that typically lies in neighboring nodes of arbitrary samples of social networks. Our main result is an algorithm which provides a constant factor approximation to the optimal adaptive policy for any influence function in the Triggering model.
[Algorithm design and analysis, adaptive seeding, word-of-mouth process, Adaptation models, Social network services, two-stage stochastic optimization model, Stochastic processes, stochastic programming, social networks, approximation algorithms, Approximation methods, constant factor approximation, influence function, Optimization, stochastic optimization, submodularity, optimal adaptive policy, Approximation algorithms, social networking (online), triggering model, influence maximization, algorithmic framework, information diffusion]
The Moser-Tardos Framework with Partial Resampling
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The resampling algorithm of Moser &amp; Tardos is a powerful approach to develop versions of the Lovasz Local Lemma. We develop a partial resampling approach motivated by this methodology: when a bad event holds, we resample an appropriately-random subset of the set of variables that define this event, rather than the entire set as in Moser &amp; Tardos. This leads to several improved algorithmic applications in scheduling, graph transversals, packet routing etc. For instance, we improve the approximation ratio of a generalized D-dimensional scheduling problem studied by Azar &amp; Epstein from O(D) to O(log D/ log log D), and settle a conjecture of Szabo &amp; Tardos on graph transversals asymptotically.
[Algorithm design and analysis, approximation ratio, graph transversals, graph theory, graph transversal, Routing, Educational institutions, Vectors, independent transversals, Moser-Tardos framework, approximation algorithms, Standards, partial resampling algorithm, Lovasz Local Lemma, generalized D-dimensional scheduling problem, randomized algorithms, scheduling, Silicon, Szabo &amp; Tardos conjecture, Random variables, packet routing, Lovasz local lemma]
A Satisfiability Algorithm for Sparse Depth Two Threshold Circuits
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give a nontrivial algorithm for the satisfiability problem for threshold circuits of depth two with a linear number of wires which improves over exhaustive search by an exponential factor. The independently interesting problem of the feasibility of sparse 0-1 integer linear programs is a special case. To our knowledge, our algorithm is the first to achieve constant savings even for the special case of Integer Linear Programming. The key idea is to reduce the satisfiability problem to the Vector Domination problem, the problem of checking whether there are two vectors in a given collection of vectors such that one dominates the other component-wise. Our result generalizes to formulas of arbitrary constant depth. We also provide a satisfiability algorithm with constant savings for depth two circuits with symmetric gates where the total weighted fan-in is at most linear in the number of variables. One of our motivations is proving strong lower bounds for TC0 circuits, exploiting the connection (established by Williams) between satisfiability algorithms and lower bounds. Our second motivation is to explore the connection between the expressive power of the circuits and the complexity of the corresponding circuit satisfiability problem.
[circuit complexity, nontrivial algorithm, integer programming, computability, linear programming, Complexity theory, Wires, Satisfiability Algorithms, Polynomials, search problems, Testing, sparse depth-two threshold circuits, arbitrary constant depth, TC0 circuits, search improvement, Vectors, lower bounds, Threshold Circuits, vector domination problem, satisfiability algorithm, logic gates, exponential factor, Logic gates, symmetric gates, total weighted fan-in, Integrated circuit modeling, sparse 0-1 integer linear programming]
Strong Backdoors to Bounded Treewidth SAT
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
There are various approaches to exploiting &#x201C;hidden structure&#x201D; in instances of hard combinatorial problems to allow faster algorithms than for general unstructured or random instances. For SAT and its counting version #SAT, hidden structure has been exploited in terms of decomposability and strong backdoor sets. Decomposability can be considered in terms of the treewidth of a graph that is associated with the given CNF formula, for instance by considering clauses and variables as vertices of the graph, and making a variable adjacent with all the clauses it appears in. On the other hand, a strong backdoor set of a CNF formula is a set of variables such that each assignment to this set moves the formula into a fixed class for which (#)SAT can be solved in polynomial time. In this paper we combine the two above approaches. In particular, we study the algorithmic question of finding a small strong backdoor set into the class W&#x03BD;&#x2264;<sub>t</sub> of CNF formulas whose associated graphs have treewidth at most t. The main results are positive: (1) There is a cubic-time algorithm that, given a CNF formula F and two constants k, t &#x2265; 0, either finds a strong W&#x03BD;&#x2264;<sub>t</sub>-backdoor set of size at most 2k, or concludes that F has no strong W&#x03BD;&#x2264;<sub>t</sub>-backdoor set of size at most k. (2) There is a cubic-time algorithm that, given a CNF formula F, computes the number of satisfying assignments of F or concludes that sbt(F) &gt; k, for any pair of constants k, t &#x2265; 0. Here, sbt(F) denotes the size of a smallest strong W&#x03BD;&#x2264;<sub>t</sub>-backdoor set of F. We establish both results by distinguishing between two cases, depending on whether the treewidth of the given formula is small or large. For both results the case of small treewidth can be dealt with relatively standard methods. The case of large treewidth is challenging and requires novel and sophisticated combinatorial arguments. The main tool is an auxiliary graph whose vertices represent subgraphs in F's associated graph. It captures various ways to assemble large-treewidth subgraphs in F's associated graph. This is used to show that every backdoor set of size k intersects a certain set of variables whose size is bounded by a function of k and t. For any other set of k variables, one can use the auxiliary graph to find an assignment &#x03C4; to these variables such that the graph associated with F[&#x03C4;] has treewidth at least t + 1. The significance of our results lies in the fact that they allow us to exploit algorithmically a hidden structure in formulas that is not accessible by any one of the two approaches (decomposability, backdoors) alone. Already a backdoor size 1 on top of treewidth 1 (i.e., sb<sub>1</sub>(F) = 1) entails formulas of arbitrarily large treewidth and arbitrarily large cycle cutsets (variables whose deletion makes the instance acyclic).
[combinatorial arguments, algorithms, graph minors, combinatorial mathematics, hidden structure, parameterized complexity, Educational institutions, bounded treewidth SAT, Complexity theory, Electronic mail, Standards, auxiliary graph, large cycle cutsets, strong backdoor sets, graphs, #SAT, strong backdoors, hard combinatorial problems, CNF formula, Model checking, decomposability, Polynomials, Bipartite graph]
An O(c^k n) 5-Approximation Algorithm for Treewidth
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give an algorithm that for an input n-vertex graph G and integer k &gt; 0, in time O(ckn) either outputs that the tree width of G is larger than k, or gives a tree decomposition of G of width at most 5k + 4. This is the first algorithm providing a constant factor approximation for tree width which runs in time single-exponential in k and linear in n. Tree width based computations are subroutines of numerous algorithms. Our algorithm can be used to speed up many such algorithms to work in time which is single-exponential in the tree width and linear in the input size.
[approximation theory, approximation, O(ckn) 5-approximation algorithm, Heuristic algorithms, Particle separators, trees (mathematics), fixed-parameter tractability, Partitioning algorithms, Approximation methods, single-exponential, factor approximation, n-vertex graph, Approximation algorithms, Polynomials, Dynamic programming, treewidth, tree decomposition]
Improved Approximation for 3-Dimensional Matching via Bounded Pathwidth Local Search
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
One of the most natural optimization problems is the k-SET PACKING problem, where given a family of sets of size at most k one should select a maximum size subfamily of pairwise disjoint sets. A special case of 3-SET PACKING is the well known 3-DIMENSIONAL MATCHING problem, which is a maximum hypermatching problem in 3-uniform tripartite hypergraphs. Both problems belong to the Karp's list of 21 NP-complete problems. The best known polynomial time approximation ratio for k-SET PACKING is (k + &#x03B5;)/2 and goes back to the work of Hurkens and Schrijver [SIDMA'89], which gives (1.5+&#x03B5;)-approximation for 3-DIMENSIONAL MATCHING. Those results are obtained by a simple local search algorithm, that uses constant size swaps. The main result of this paper is a new approach to local search for k-SET PACKING where only a special type of swaps is considered, which we call swaps of bounded pathwidth. We show that for a fixed value of k one can search the space of r-size swaps of constant pathwidth in crpoly(|F|) time. Moreover we present an analysis proving that a local search maximum with respect to O(log |F|)-size swaps of constant pathwidth yields a polynomial time (k+1+&#x03B5;)/3-approximation algorithm, improving the best known approximation ratio for k-SET PACKING. In particular we improve the approximation ratio for 3-DIMENSIONAL MATCHING from 3/2+&#x03B5; to 4/3+&#x03B5;.
[Algorithm design and analysis, graph theory, 3-set packing, constant size swaps, 3-dimensional matching, polynomial time approximation ratio, k-set packing, Approximation methods, local search, bin packing, optimisation, Bismuth, NP-complete problems, Polynomials, polynomial time, search problems, 3-dimensional matching problem, approximation, 3-uniform tripartite hypergraphs, Color, maximum hypermatching problem, Standards, bounded pathwidth local search, k-set packing problem, pairwise disjoint set maximum size subfamily, Approximation algorithms, optimization problems, fixed parameter tractability, computational complexity]
On Kinetic Delaunay Triangulations: A Near Quadratic Bound for Unit Speed Motions
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Let P be a collection of n points in the plane, each moving along some straight line at unit speed. We obtain an almost tight upper bound of O(n2+&#x03B5;), for any &#x03B5; &gt; 0, on the maximum number of discrete changes that the Delaunay triangulation DT(P) of P experiences during this motion. Our analysis is cast in a purely topological setting, where we only assume that (i) any four points can be co-circular at most three times, and (ii) no triple of points can be collinear more than twice; these assumptions hold for unit speed motions.
[topological setting, topology, Maintenance engineering, computational geometry, discrete changes, Probabilistic logic, Delaunay triangulation, Complexity theory, Indexes, near quadratic bound, combinatorial complexity, Upper bound, Voronoi diagram, unit speed motions, Kinetic theory, Trajectory, moving points, kinetic Delaunay triangulations:, computational complexity]
Interlacing Families I: Bipartite Ramanujan Graphs of All Degrees
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We prove that there exist infinite families of regular bipartite Ramanujan graphs of every degree bigger than 2. We do this by proving a variant of a conjecture of Bilu and Linial about the existence of good 2-lifts of every graph. We also establish the existence of infinite families of `irregular Ramanujan' graphs, whose eigenvalues are bounded by the spectral radius of their universal cover. Such families were conjectured to exist by Linial and others. In particular, we prove the existence of infinite families of (c, d)-biregular bipartite graphs with all non-trivial eigenvalues bounded by &#x221A;c-1+&#x221A;d-1, for all c, d &#x2265; q 3. Our proof exploits a new technique for demonstrating the existence of useful combinatorial objects that we call the "method of interlacing polynomials".
[bipartite Ramanujan graphs, Symmetric matrices, Matching Polynomial, graph theory, Computer science, Upper bound, Ramanujan Graph, Lifts of Graphs, Polynomials, Eigenvalues and eigenfunctions, Bipartite graph, spectral radius, nontrivial eigenvalues]
Dynamic Approximate All-Pairs Shortest Paths: Breaking the O(mn) Barrier and Derandomization
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We study dynamic (1 + &#x03F5;)-approximation algorithms for the all-pairs shortest paths problem in unweighted undirected n-node m-edge graphs under edge deletions. The fastest algorithm for this problem is a randomized algorithm with a total update time of O&#x0311;(mn) and constant query time by Roditty and Zwick (FOCS 2004). The fastest deterministic algorithm is from a 1981 paper by Even and Shiloach (JACM 1981); it has a total update time of O(mn2) and constant query time. We improve these results as follows: (1) We present an algorithm with a total update time of O&#x0311;(n5/2) and constant query time that has an additive error of two in addition to the 1 + &#x03F5; multiplicative error. This beats the previous O&#x0311;(mn) time when m = &#x03A9;(n3/2). Note that the additive error is unavoidable since, even in the static case, an O(n3-&#x03B4;)-time (a so-called truly sub cubic) combinatorial algorithm with 1 + &#x03F5; multiplicative error cannot have an additive error less than 2 - &#x03F5;, unless we make a major breakthrough for Boolean matrix multiplication (Dor, Halperin and Zwick FOCS 1996) and many other long-standing problems (Vassilevska Williams and Williams FOCS 2010). The algorithm can also be turned into a (2 + &#x03F5;)-approximation algorithm (without an additive error) with the same time guarantees, improving the recent (3 + &#x03F5;)-approximation algorithm with O&#x0311;(n5/2+O(1&#x221A;(log n))) running time of Bernstein and Roditty (SODA 2011) in terms of both approximation and time guarantees. (2) We present a deterministic algorithm with a total update time of O&#x0311;(mn) and a query time of O(log log n). The algorithm has a multiplicative error of 1 + &#x03F5; and gives the first improved deterministic algorithm since 1981. It also answers an open question raised by Bernstein in his STOC 2013 paper. In order to achieve our results, we introduce two new techniques: (1) A lazy Even-Shiloach tree algorithm which maintains a bounded-distance shortest-paths tree on a certain type of emulator called locally persevering emulator. (2) A derandomization technique based on moving Even-Shiloach trees as a way to derandomize the standard random set argument. These techniques might be of independent interest.
[Algorithm design and analysis, Additives, standard random set argument, combinatorial mathematics, Heuristic algorithms, locally persevering emulator, Approximation methods, unweighted undirected n-node m-edge graphs, bounded-distance shortest-paths tree, dynamic approximate all-pairs shortest paths algorithm, truly subcubic combinatorial algorithm, derandomization technique, all-pairs shortest paths, approximation theory, multiplicative error, moving Even-Shiloach trees, constant query time, dynamic graph algorithms, Educational institutions, total update time, derandomization, randomized algorithm, deterministic algorithm, deterministic algorithms, randomised algorithms, Computer science, emulator, edge deletions, additive error, Approximation algorithms, O(mn) barrier, monotone Even-Shiloach tree algorithm, computational complexity, dynamic approximation algorithms]
Fully Dynamic (1+ e)-Approximate Matchings
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We present the first data structures that maintain near optimal maximum cardinality and maximum weighted matchings on sparse graphs in sub linear time per update. Our main result is a data structure that maintains a (1+&#x03F5;) approximation of maximum matching under edge insertions/deletions in worst case O&#x0303;(&#x221A;m&#x03F5;-2) time per update. This improves the 3/2 approximation given by Neiman and Solomon [20] which runs in similar time. The result is based on two ideas. The first is to re-run a static algorithm after a chosen number of updates to ensure approximation guarantees. The second is to judiciously trim the graph to a smaller equivalent one whenever possible. We also study extensions of our approach to the weighted setting, and combine it with known frameworks to obtain arbitrary approximation ratios. For a constant &#x03F5; and for graphs with edge weights between 1 and N, we design an algorithm that maintains an (1+&#x03F5;) approximate maximum weighted matching in O&#x0303;(&#x221A;m log N) time per update. The only previous result for maintaining weighted matchings on dynamic graphs has an approximation ratio of 4.9108, and was shown by An and et al. [2], [3].
[Algorithm design and analysis, pattern matching, approximation, Heuristic algorithms, graph theory, sparse graphs, near optimal maximum cardinality, Data structures, fully dynamic (1+ &#x03F5;)-approximate matchings, Approximation methods, Optimization, matching, Computer science, dynamic graphs, maximum weighted matchings, dynamic algorithms, Approximation algorithms, data structures, static algorithm, computational complexity]
Online Node-Weighted Steiner Forest and Extensions via Disk Paintings
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give the first polynomial-time online algorithm for the node-weighted Steiner forest problem with a poly-logarithmic competitive ratio. The competitive ratio of our algorithm is optimal up to a logarithmic factor. For the special case of graphs with an excluded fixed minor (e.g., planar graphs), we obtain a logarithmic competitive ratio, which is optimal up to a constant, using a different online algorithm. Both these results are obtained as special cases of generic results for a large class of problems that can be encoded as online 0, 1-proper functions. Our results are obtained by using a new framework for online network design problems that we call disk paintings. The central idea in this technique is to amortize the cost of primal updates to a set of carefully selected mutually disjoint fixed-radius dual disks centered at a subset of terminals. We hope that this framework will be useful for other online network design problems.
[Algorithm design and analysis, disk paintings, logarithmic factor, trees (mathematics), Steiner Tree, Color, fixed-radius dual disk, Network Design, online network design problem, planar graph, Steiner Forest, Painting, Computer science, online node-weighted Steiner forest problem, logarithmic competitive ratio, polynomial-time online algorithm, Online Algorithm, Approximation algorithms, Polynomials, poly-logarithmic, Joining processes, computational complexity]
An LMP O(log n)-Approximation Algorithm for Node Weighted Prize Collecting Steiner Tree
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
In the node-weighted prize-collecting Steiner tree problem (NW-PCST) we are given an undirected graph G = (V, E), non-negative costs c(u) and penalties &#x03C0;(u) for each u &#x2208; V . The goal is to find a tree T that minimizes the total cost of the vertices spanned by T plus the total penalty of vertices not in T. This problem is well-known to be set-cover hard to approximate. Moss and Rabani (STOC'01) presented a primal-dual Lagrangean-multiplier-preserving O(ln |V |)-approximation algorithm for this problem. We show a serious problem with the algorithm, and present a new, fundamentally different primal-dual method achieving the same performance guarantee. Our algorithm introduces several novel features to the primal-dual method that may be of independent interest.
[Algorithm design and analysis, Steiner trees, LMP O(log n)-approximation algorithm, NW-PCST, graph theory, prize-collecting problems, primal-dual Lagrangean-multiplier-preserving O(ln |V|)-approximation algorithm, Linear programming, set-cover hard, approximation algorithms, Lagrangean multiplier preserving, Approximation methods, Standards, nonnegative costs, Node-weighted Steiner trees, Approximation algorithms, node weighted prize collecting steiner tree, undirected graph, Joining processes, computational complexity]
Arithmetic Circuits: A Chasm at Depth Three
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We show that, over Q, if an n-variate polynomial of degree d = nO(1) is computable by an arithmetic circuit of size s (respectively by an arithmetic branching program of size s) then it can also be computed by a depth three circuit (i.e. a &#x03A3;&#x03A0;&#x03A3;-circuit) of size exp(O(&#x221A;(d log n log d log s))) (respectively of size exp(O(&#x221A;(d log n log s))). In particular this yields a &#x03A3;&#x03A0;&#x03A3; circuit of size exp(O(&#x221A;(d log d))) computing the d &#x00D7; d determinant Det<sub>d</sub>. It also means that if we can prove a lower bound of exp(omega(&#x221A;(d log d))) on the size of any &#x03A3;&#x03A0;&#x03A3;-circuit computing the d &#x00D7; d permanent Perm<sub>d</sub> then we get super polynomial lower bounds for the size of any arithmetic branching program computing Perm<sub>d</sub>. We then give some further results pertaining to derandomizing polynomial identity testing and circuit lower bounds. The &#x03A3;&#x03A0;&#x03A3; circuits that we construct have the property that (some of) the intermediate polynomials have degree much higher than d. Indeed such a counterintuitive construction is unavoidable - it is known that in any &#x03A3;&#x03A0;&#x03A3; circuit C computing either Det<sub>d</sub> or Perm_d, if every multiplication gate has fanin at most d (or any constant multiple thereof) then C must have size at least exp(&#x03A9;(d)).
[circuit complexity, n-variate polynomial, arithmetic circuits, polynomials, depth three circuit, counterintuitive construction, arithmetic branching program computing, circuit lower bounds, Complexity theory, super polynomial lower bounds, derandomizing polynomial identity testing, Computer science, Tensile stress, digital arithmetic, permanent, &#x03A3;&#x03A0;&#x03A3;-circuit computing, Logic gates, depth reduction, VNP, Polynomials, VP, depth three circuits, Testing, determinant]
Improved Average-Case Lower Bounds for DeMorgan Formula Size
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give an explicit function h: {0, 1}n &#x2192; {0, 1} such that every deMorgan formula of size n3-o(1)/r2 agrees with h on at most a fraction of 1/2+2-&#x03A9;(r) of the inputs. This improves the previous average-case lower bound of Komargodski and Raz (STOC, 2013). Our technical contributions include a theorem that shows that the "expected shrinkage" result of Haastad (SIAM J. Comput., 1998) actually holds with very high probability (where the restrictions are chosen from a certain distribution that takes into account the structure of the formula), combining ideas of both Impagliazzo, Meka and Zuckerman (FOCS, 2012) and Komargodski and Raz. In addition, using a bit-fixing extractor in the construction of h allows us to simplify a major part of the analysis of Komargodski and Raz1.
[Correlation, bit-fixing extractor, average-case, Computational modeling, Input variables, DeMorgan formula size, probability, correlation bounds, bit-fixing extractors, Vectors, Boolean algebra, Approximation methods, Indexes, lower bounds, Boolean functions, Boolean formula, average-case lower bound, deMorgan formulas, random restrictions, shrinkage, computational complexity]
Average Case Lower Bounds for Monotone Switching Networks
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
An approximate computation of a Boolean function by a circuit or switching network is a computation in which the function is computed correctly on the majority of the inputs (rather than on all inputs). Besides being interesting in their own right, lower bounds for approximate computation have proved useful in many sub areas of complexity theory, such as cryptography and derandomization. Lower bounds for approximate computation are also known as correlation bounds or average case hardness. In this paper, we obtain the first average case monotone depth lower bounds for a function in monotone P. We tolerate errors that are asymptotically the best possible for monotone circuits. Specifically, we prove average case exponential lower bounds on the size of monotone switching networks for the GEN function. As a corollary, we separate the monotone NC hierarchy in the case of errors -- a result which was previously only known for exact computations. Our proof extends and simplifies the Fourier analytic technique due to Potechin, and further developed by Chan and Potechin. As a corollary of our main lower bound, we prove that the communication complexity approach for monotone depth lower bounds does not naturally generalize to the average case setting.
[circuit complexity, complexity theory, Switches, Boolean function, Vectors, Complexity theory, average case exponential lower bound, GEN function, monotone depth lower bound, Switching circuits, Fourier analytic technique, Boolean functions, switching networks, Games, Polynomials, monotone switching network, approximate computation]
Explicit Subspace Designs
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
A subspace design is a collection {H<sub>1</sub>, H<sub>2</sub>, . . . , H<sub>M</sub>} of subspaces of Fm<sub>q</sub> with the property that no low-dimensional subspace W of F<sub>q</sub>m intersects too many subspaces of the collection. Subspace designs were introduced by Guruswami and Xing (STOC 2013) who used them to give a randomized construction of optimal rate list-decodable codes over constant-sized large alphabets and sub-logarithmic (and even smaller) list size. Subspace designs are the only non-explicit part of their construction. In this paper, we give explicit constructions of subspace designs with parameters close to the probabilistic construction, and this implies the first deterministic polynomial time construction of list-decodable codes achieving the above parameters. Our constructions of subspace designs are natural and easily described, and are based on univariate polynomials over finite fields. Curiously, the constructions are very closely related to certain good list-decodable codes (folded RS codes and univariate multiplicity codes). The proof of the subspace design property uses the polynomial method (with multiplicities): Given a target low-dimensional subspace W, we construct a nonzero low-degree polynomial P<sub>W</sub> that has several roots for each H that non-trivially intersects W. The construction of P<sub>W</sub> is based on the classical Wronskian determinant and the folded Wronskian determinant, the latter being a recently studied notion that we make explicit in this paper. Our analysis reveals some new phenomena about the zeroes of univariate polynomials, namely that polynomials with many structured roots or many high multiplicity roots tend to be linearly independent.
[Frequency modulation, probabilistic construction, Polynomial Method, deterministic polynomial time construction, List-decoding, Reed-Solomon codes, structured roots, nonzero low-degree polynomial, multiplicity roots, Polynomials, folded Wronskian determinant, list-decodable codes, classical Wronskian determinant, determinants, polynomials, probability, Probabilistic logic, Decoding, low-dimensional subspace, Computer science, Algebraic coding, subspace design property, explicit subspace designs, univariate polynomials, Pseudorandomness]
Understanding Incentives: Mechanism Design Becomes Algorithm Design
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We provide a computationally efficient black-box reduction from mechanism design to algorithm design in very general settings. Specifically, we give an approximation-preserving reduction from truthfully maximizing any objective under arbitrary feasibility constraints with arbitrary bidder types to (not necessarily truthfully) maximizing the same objective plus virtual welfare (under the same feasibility constraints). Our reduction is based on a fundamentally new approach: we describe a mechanism's behavior indirectly only in terms of the expected value it awards bidders for certain behavior, and never directly access the allocation rule at all. Applying our new approach to revenue, we exhibit settings where our reduction holds both ways. That is, we also provide an approximation-sensitive reduction from (non-truthfully) maximizing virtual welfare to (truthfully) maximizing revenue, and therefore the two problems are computationally equivalent. With this equivalence in hand, we show that both problems are NP-hard to approximate within any polynomial factor, even for a single monotone sub modular bidder. We further demonstrate the applicability of our reduction by providing a truthful mechanism maximizing fractional max-min fairness.
[Algorithm design and analysis, Additives, mechanism behavior, mechanism design, polynomial factor, Approximation methods, single monotone submodular bidder, black-box reduction, Optimization, incentives, Optimal Auctions, Polynomials, arbitrary bidder types, tendering, approximation-sensitive reduction, approximation-preserving reduction, virtual welfare, truthful mechanism, Revenue Maximization, Black Box, incentive schemes, fractional max-min fairness maximization, NP-hard problem, arbitrary feasibility constraints, Reductions, Approximation algorithms, Bayes methods, Resource management, Mechanism Design, computational complexity, algorithm design]
The Simple Economics of Approximately Optimal Auctions
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The intuition that profit is optimized by maximizing marginal revenue is a guiding principle in microeconomics. In the classical auction theory for agents with quasi-linear utility and single-dimensional preferences, BR89 show that the optimal auction of M81 is in fact optimizing marginal revenue. In particular Myerson's virtual values are exactly the derivative of an appropriate revenue curve. This paper considers mechanism design in environments where the agents have multi-dimensional and non-linear preferences. Understanding good auctions for these environments is considered to be the main challenge in Bayesian optimal mechanism design. In these environments maximizing marginal revenue may not be optimal, and furthermore, there is sometimes no direct way to implement the marginal revenue maximization mechanism. Our contributions are three fold: we characterize the settings for which marginal revenue maximization is optimal (by identifying an important condition that we call revenue linearity), we give simple procedures for implementing marginal revenue maximization in general, and we show that marginal revenue maximization is approximately optimal. Our approximation factor smoothly degrades in a term that quantifies how far the environment is from an ideal one (i.e., where marginal revenue maximization is optimal). Because the marginal revenue mechanism is optimal for well-studied single-dimensional agents, our generalization immediately extends many approximation results for single-dimensional agents to more general preferences. Finally, one of the biggest open questions in Bayesian algorithmic mechanism design is in developing methodologies that are not brute-force in size of the agent type space (usually exponential in the dimension for multi-dimensional agents). Our methods identify a sub problem that, e.g., for unit-demand agents with values drawn from product distributions, enables approximation mechanisms that are polynomial in the dimension.
[M81, Bayesian optimal mechanism design, multidimensional preference, Approximation methods, commerce, classical auction theory, single-dimensional preference, BR89, single-dimensional agents, revenue curve, Pricing, marginal revenue, approximation theory, microeconomics, approximation, approximation mechanisms, Color, Bayesian mechanism design, Educational institutions, quasi-linear utility, revenue linearity, optimal auctions, Linearity, marginal revenue maximization mechanism, Myerson virtual values, Bayesian algorithmic mechanism design, nonlinear preference, Bayes methods, Resource management]
The Price of Stability for Undirected Broadcast Network Design with Fair Cost Allocation Is Constant
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We consider broadcast network design games in undirected networks in which every player is a node wishing to receive communication from a distinguished source node s and the cost of each communication link is equally shared among the downstream receivers according to the Shapley value. We prove that the Price of Stability of such games is constant, thus closing a long-standing open problem raised in [2]. Our result is obtained by means of homogenization, a new technique that, in any intermediate state locally diverging from a given optimal solution T*, is able to restore local similarity by exploiting cost differences between nearby players in T*.
[Network Design Games, broadcast network design games, game theory, Nash equilibrium, Educational institutions, Stability analysis, cost allocation, Upper bound, Shapley value, Nash equilibria, undirected broadcast network design, Games, downstream receivers, Resource management, Joining processes, source node, pricing, Price of Stability]
Rational Protocol Design: Cryptography against Incentive-Driven Adversaries
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Existing work on "rational cryptographic protocols" treats each party (or coalition of parties) running the protocol as a selfish agent trying to maximize its utility. In this work we propose a fundamentally different approach that is better suited to modeling a protocol under attack from an external entity. Specifically, we consider a two-party game between an protocol designer and an external attacker. The goal of the attacker is to break security properties such as correctness or privacy, possibly by corrupting protocol participants; the goal of the protocol designer is to prevent the attacker from succeeding. We lay the theoretical groundwork for a study of cryptographic protocol design in this setting by providing a methodology for defining the problem within the traditional simulation paradigm. Our framework provides ways of reasoning about important cryptographic concepts (e.g., adaptive corruptions or attacks on communication resources) not handled by previous game-theoretic treatments of cryptography. We also prove composition theorems that-for the first time-provide a sound way to design rational protocols assuming "ideal communication resources" (such as broadcast or authenticated channels) and then instantiate these resources using standard cryptographic tools. Finally, we investigate the problem of secure function evaluation in our framework, where the attacker has to pay for each party it corrupts. Our results demonstrate how knowledge of the attacker's incentives can be used to circumvent known impossibility results in this setting.
[Protocols, incentive-driven adversaries, correctness, Composition, cryptographic protocols, Secure Computation, cryptographic protocol design, game theory, external attacker, protocol participants corruption, rational protocol design, privacy, Game theory, protocol designer, Standards, secure function evaluation, Game Theory, Privacy, Games, security properties breakage, data privacy, two-party game, Cryptography, ideal communication resources]
Fourier Sparsity, Spectral Norm, and the Log-Rank Conjecture
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We study Boolean functions with sparse Fourier spectrum or small spectral norm, and show their applications to the Log-rank Conjecture for XOR functions f(x &#x2295; y) - a fairly large class of functions including well studied ones such as Equality and Hamming Distance. The rank of the communication matrix M<sub>f</sub> for such functions is exactly the Fourier sparsity of f. Let d = deg<sub>2</sub>(f) be the F<sub>2</sub>-degree of f and DCC(f &#x00B7; &#x2295;) stand for the deterministic communication complexity for f(x &#x2295; y). We show that 1) DCC(f &#x00B7; &#x2295;) = O(2d2/2 logd-2 &#x2225;f&#x0302;&#x2225;<sub>1</sub>). In particular, the Log-rank conjecture holds for XOR functions with constant F<sub>2</sub>-degree. 2) DCC(f &#x00B7; &#x2295;) = O(d&#x2225;f&#x0302;&#x2225;<sub>1</sub>) = O(&#x221A;(rank(M<sub>f</sub>))). This improves the (trivial) linear bound by nearly a quadratic factor. We obtain our results through a degree-reduction protocol based on a variant of polynomial rank, and actually conjecture that the communication cost of our protocol is at most logO(1) rank(M<sub>f</sub>). The above bounds are obtained from different analysis for the number of parity queries required to reduce f's F<sub>2</sub>-degree. Our bounds also hold for the parity decision tree complexity of f, a measure that is no less than the communication complexity. Along the way we also prove several structural results about Boolean functions with small Fourier sparsity &#x2225;f&#x0302;&#x2225;<sub>0</sub> or spectral norm &#x2225;f&#x0302;&#x2225;<sub>1</sub>, which could be of independent interest. For functions f with constant F<sub>2</sub>-degree, we show that: 1) f can be written as the summation of quasi-polynomially many indicator functions of subspaces with &#x00B1;-signs, improving the previous doubly exponential upper bound by Green and Sanders; 2) being sparse in Fourier domain is polynomially equivalent to having a small parity decision tree complexity; and 3) f depends only on polylog&#x2225;f&#x0302;&#x2225;<sub>1</sub> linear functions of input variables. For functions f with small spectral norm, we show that: 1) there is an affine subspace of co dimension &#x2225;f&#x0302;&#x2225;<sub>1</sub> on which f(x) is a constant, and 2) there is a parity decision &#x2225;f&#x0302;&#x2225;<sub>1</sub>log&#x2225;f&#x0302;&#x2225;<sub>0</sub> for computing f.
[Fourier sparsity, Protocols, spectral norm, communication matrix, Log-rank conjecture, Fourier analysis, log-rank conjecture, Complexity theory, deterministic algorithms, deterministic communication complexity, Standards, matrix algebra, low-degree polynomials, Boolean functions, Upper bound, decision trees, degree-reduction protocol, Polynomials, Decision trees, sparse Fourier spectrum, XOR functions, computational complexity, parity decision tree complexity]
A Tight Bound for Set Disjointness in the Message-Passing Model
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
In a multiparty message-passing model of communication, there are k players. Each player has a private input, and they communicate by sending messages to one another over private channels. While this model has been used extensively in distributed computing and in secure multiparty computation, lower bounds on communication complexity in this model and related models have been somewhat scarce. In recent work [25], [29], [30], strong lower bounds of the form &#x03A9;(n&#x00B7;k) were obtained for several functions in the message-passing model; however, a lower bound on the classical set disjointness problem remained elusive. In this paper, we prove a tight lower bound of &#x03A9;(n &#x00B7; k) for the set disjointness problem in the message passing model. Our bound is obtained by developing information complexity tools for the message-passing model and proving an information complexity lower bound for set disjointness.
[Protocols, message passing, Computational modeling, secure multiparty computation, set disjointness, Educational institutions, Complexity theory, multiparty message-passing model, communication complexity, Distributed computing, distributed computing, classical set disjointness problem, Computer science, private channels, information complexity tools, Integrated circuit modeling, computational complexity]
On the Communication Complexity of Sparse Set Disjointness and Exists-Equal Problems
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
In this paper we study the two player randomized communication complexity of the sparse set disjointness and the exists-equal problems and give matching lower and upper bounds (up to constant factors) for any number of rounds for both of these problems. In the sparse set disjointness problem, each player receives a k-subset of [m] and the goal is to determine whether the sets intersect. For this problem, we give a protocol that communicates a total of O(k log(r) k) bits over r rounds and errs with very small probability. Here we can take r = log* k to obtain a O(k) total communication log* k-round protocol with exponentially small error probability, improving on the O(k)-bits O(log k)-round constant error probability protocol of Hastad and Wigderson from 1997. In the exists-equal problem, the players receive vectors x, y &#x2208; [t]n and the goal is to determine whether there exists a coordinate i such that x<sub>i</sub> = y<sub>i</sub>. Namely, the exists-equal problem is the OR of n equality problems. Observe that exists-equal is an instance of sparse set disjointness with k = n, hence the protocol above applies here as well, giving an O(n log(r) n) upper bound. Our main technical contribution in this paper is a matching lower bound: we show that when t = &#x03A9;(n), any r-round randomized protocol for the exists-equal problem with error probability at most 1/3 should have a message of size &#x03A9;(n log(r) n). Our lower bound holds even for super-constant r &#x2264; log* n, showing that any O(n) bits exists-equal protocol should have log* n - O(1) rounds. Note that the protocol we give errs only with less than polynomially small probability and provides guarantees on the total communication for the harder set disjointness problem, whereas our lower bound holds even for constant error probability protocols and for the easier exists-equal problem with guarantees on the max-communication. Hence our upper and lower bounds match in a strong sense. Our lower bound on the constant round protocols for exist-sequal shows that solving the OR of n instances of the equality problems requires strictly more than n times the cost of a single instance. To our knowledge this is the first example of such a super-linear increase in complexity.
[Protocols, direct-sum, Error probability, exists-equal problems, game theory, O(k) total communication log* k-round protocol, exponentially small error probability, Vectors, Entropy, Complexity theory, set theory, communication complexity, OR equality problems, O(log k)-round constant error probability protocol, max-communication, sparse set disjointness problem, Games, n equality problems, isoperimetric inequality, Silicon, two player randomized communication complexity, constant round protocols, computational complexity, round-elimination]
Common Information and Unique Disjointness
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We provide a new framework for establishing strong lower bounds on the nonnegative rank of matrices by means of common information, a notion previously introduced in [1]. Common information is a natural lower bound for the nonnegative rank of a matrix and by combining it with He linger distance estimations we can compute the (almost) exact common information of UDISJ partial matrix. We also establish robustness of this estimation under various perturbations of the UDISJ partial matrix, where rows and columns are randomly or adversarially removed or where entries are randomly or adversarially altered. This robustness translates, via a variant of Yannakakis' Factorization Theorem, to lower bounds on the average case and adversarial approximate extension complexity. We present the first family of polytopes, the hard pair introduced in [2] related to the CLIQUE problem, with high average case and adversarial approximate extension complexity. We also provide an information theoretic variant of the fooling set method that allows us to extend fooling set lower bounds from extension complexity to approximate extension complexity.
[Correlation, UDISJ partial matrix, Entropy, Complexity theory, Linear matrix inequalities, matrix decomposition, exact common information, adversarial approximate extension complexity, information theory, CLIQUE problem, Yannakakis factorization theorem, unique disjointness, correlation polytope, Hellinger distance estimations, Linear programming, information theoretic variant, common information, nonnegative rank, extended formulations, extension complexity, nonnegative matrix rank, Random variables, fooling set method, Information theory, computational complexity]
A Linear Time Approximation Scheme for Euclidean TSP
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
The Traveling Salesman Problem (TSP) is among the most famous NP-hard optimization problems. The special case of TSP in bounded-dimensional Euclidean spaces has been a particular focus of research: The celebrated results of Arora [Aro98] and Mitchell [Mit99] - along with subsequent improvements of Rao and Smith [RS98] - demonstrated a polynomial time approximation scheme for this problem, ultimately achieving a runtime of O<sub>d,&#x03B5;</sub>(n log n). In this paper, we present a linear time approximation scheme for Euclidean TSP, with runtime O<sub>d,&#x03B5;</sub>(n). This improvement resolves a 15 year old conjecture of Rao and Smith, and matches for Euclidean spaces the bound known for a broad class of planar graphs [Kle08].
[approximation theory, Euclidean TSP, traveling salesman problem, bounded-dimensional Euclidean spaces, Heuristic algorithms, Computational modeling, graph theory, planar graphs, Approximation methods, Optimization, travelling salesman problems, Runtime, Geometrical problems and computations, Computations on discrete structures, Approximation algorithms, linear time approximation scheme, Joining processes, computational complexity, NP-hard optimization problems]
A Forward-Backward Single-Source Shortest Paths Algorithm
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We describe a new forward-backward variant of Dijkstra's and Spira's Single-Source Shortest Paths (SSSP) algorithms. While essentially all SSSP algorithm only scan edges forward, the new algorithm scans some edges backward. The new algorithm assumes that edges in the out-going and incoming adjacency lists of the vertices appear in nondecreasing order of weight. (Spira's algorithm makes the same assumption about the out-going adjacency lists, but does not use incoming adjacency lists.) The running time of the algorithm on a complete directed graph on n vertices with independent exponential edge weights is O(n), with very high probability. This improves on the previously best result of O(n log n), which is best possible if only forward scans are allowed, exhibiting an interesting separation between forward-only and forward-backward SSSP algorithms. As a consequence, we also get a new all-pairs shortest paths algorithm. The expected running time of the algorithm on complete graphs with independent exponential edge weights is O(n2), matching a recent result of Peres et al. Furthermore, the probability that the new algorithm requires more than O(n2) time is exponentially small, improving on the polynomially small probability of Peres et al.
[Algorithm design and analysis, independent exponential edge weights, Heuristic algorithms, Computational modeling, complete directed graph, Probabilistic logic, forward-only SSSP algorithms, forward-backward single-source shortest paths algorithm, running time, polynomially small probability, out-going adjacency lists, directed graphs, shortest paths, all-pairs shortest paths algorithm, Random variables, incoming adjacency lists, Arrays, search problems, computational complexity, graph algorithms]
Approximating Minimization Diagrams and Generalized Proximity Search
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We investigate the classes of functions whose minimization diagrams can be approximated efficiently in Red. We present a general framework and a data-structure that can be used to approximate the minimization diagram of such functions. The resulting data-structure has near linear size and can answer queries in logarithmic time. Applications include approximating the Voronoi diagram of (additively or multiplicatively) weighted points. Our technique also works for more general distance functions, such as metrics induced by convex bodies, and the nearest furthest-neighbor distance to a set of point sets. Interestingly, our framework works also for distance functions that do not obey the triangle inequality. For many of these functions no near-linear size approximation was known before.
[Measurement, proximity search, Voronoi diagrams, data-structure, Artificial neural networks, computational geometry, Minimization, Data structures, minimization diagrams, Complexity theory, approximation algorithms, Approximation methods, logarithmic time, Computer science, Computational geometry, Voronoi diagram, convex bodies, generalized proximity search, general distance functions, triangle inequality, nearest furthest-neighbor distance, minimisation, search problems]
The Parity of Directed Hamiltonian Cycles
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We present a deterministic algorithm that given any directed graph on n vertices computes the parity of its number of Hamiltonian cycles in O(1.619n) time and polynomial space. For bipartite graphs, we give a 1.5npoly(n) expected time algorithm. Our algorithms are based on a new combinatorial formula for the number of Hamiltonian cycles modulo a positive integer.
[directed Hamiltonian cycles, positive integer, polynomial space, Educational institutions, Vectors, Partitioning algorithms, deterministic algorithm, deterministic algorithms, Computer science, O(1.619n) time, bipartite graphs, directed graphs, 1.5npoly(n) expected time algorithm, directed graph, Polynomials, Bipartite graph, computational complexity]
Nondeterministic Direct Product Reductions and the Success Probability of SAT Solvers
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give two nondeterministic reductions which yield new direct product theorems (DPTs) for Boolean circuits. In these theorems one assumes that a target function F is mildly hard against nondeterministic circuits, and concludes that the direct product Ft is extremely hard against (only polynomially smaller) probabilistic circuits. The main advantage of these results compared with previous DPTs is the strength of the size bound in our conclusion. As an application, we show that if NP is not in coNP/poly then, for every PPT algorithm attempting to produce satisfying assigments to Boolean formulas, there are infinitely many instances where the algorithm's success probability is nearly-exponentially small. This furthers a project of Paturi and Pudla&#x0301;k [STOC'10].
[NP, SAT solvers, direct product theorems, probability, computability, Probabilistic logic, Search problems, nondeterministic reductions, Complexity theory, DPTs, hardness amplification, Boolean functions, nondeterministic direct product reductions, Polynomials, Cryptography, Boolean circuits, PPT algorithm, Integrated circuit modeling, success probability, Satisfiability, computational complexity]
Direct Products in Communication Complexity
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We give exponentially small upper bounds on the success probability for computing the direct product of any function over any distribution using a communication protocol. Let suc(μ, f, C) denote the maximum success probability of a 2-party communication protocol for computing the boolean function f(x, y) with C bits of communication, when the inputs (x, y) are drawn from the distribution μ. Let μn be the product distribution on n inputs and fn denote the function that computes n copies of f on these inputs. We prove that if T log3/2 T &#x226A; (C - 1)&#x221A;n and suc(μ, f, C) &lt;; 2/3, then suc(μn, fn, T) &#x2264; exp(-&#x03A9;(n)). When μ is a product distribution, we prove a nearly optimal result: as long as T log2 T &#x226A; Cn, we must have suc(μn, fn, T) &#x2264; exp(-&#x03A9;(n)).
[Direct Products, Protocols, product distribution, Computational modeling, 2-party communication protocol, Information Complexity, probability, Educational institutions, Boolean function, Complexity theory, communication complexity, Communication Complexity, Computer science, Upper bound, Boolean functions, Mutual information]
Quantum 3-SAT Is QMA1-Complete
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
Quantum satisfiability is a constraint satisfaction problem that generalizes classical boolean satisfiability. In the quantum k-SAT problem, each constraint is specified by a k-local projector and is satisfied by any state in its nullspace. Bravyi showed that quantum 2-SAT can be solved efficiently on a classical computer and that quantum k-SAT with k &#x2265; 4 is QMA<sub>1</sub>-complete [4]. Quantum 3-SAT was known to be contained in QMA<sub>1</sub> [4], but its computational hardness was unknown until now. We prove that quantum 3-SAT is QMA<sub>1</sub>-hard, and therefore complete for this complexity class.
[quantum satisfiability, computational hardness, Stationary state, complexity class, computability, constraint satisfaction problem, Registers, Complexity theory, quantum k-SAT problem, Computational complexity, quantum 3-SAT problem, Quantum computing, Boolean functions, constraint satisfaction problems, Logic gates, Hilbert space, classical Boolean satisfiability, QMA<sub>1</sub>-complete, Clocks, computational complexity]
Three-Player Entangled XOR Games Are NP-Hard to Approximate
2013 IEEE 54th Annual Symposium on Foundations of Computer Science
None
2013
We show that for any &#x03B5; &gt; 0 the problem of finding a factor (2 - &#x03B5;) approximation to the entangled value of a three-player XOR game is NP-hard. Equivalently, the problem of approximating the largest possible quantum violation of a tripartite Bell correlation inequality to within any multiplicative constant is NP-hard. These results are the first constant-factor hardness of approximation results for entangled games or quantum violations of Bell inequalities shown under the sole assumption that P&#x2260;NP. They can be thought of as an extension of Ha&#x0301;stad's optimal hardness of approximation results for MAX-E3-LIN2 (JACM'01) to the entangled-player setting. The key technical component of our work is a soundness analysis of a point-vs-plane low-degree test against entangled players. This extends and simplifies the analysis of the multilinearity test by Ito and Vidick (FOCS'12). Our results demonstrate the possibility for efficient reductions between entangled-player games and our techniques may lead to further hardness of approximation results.
[three-player entangled XOR games, Bell inequalities, Frequency modulation, quantum entanglement, MAX-E3-LIN2, entangled games, Approximation methods, largest possible quantum violation approximation, Ha&#x0301;stad's optimal hardness, Polynomials, approximation result constant-factor hardness, tripartite Bell correlation inequality, PCP theorem, approximation theory, Quantum entanglement, game theory, Bell theorem, multiplicative constant, XOR games, JACM'01, Q measurement, NP-hard problem, quantum computing, Games, computational complexity]
Foreword
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Program Committee
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Provides a listing of current committee members and society officers.
[]
(2 + epsilon)-Sat Is NP-Hard
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We prove the following hardness result for anatural promise variant of the classical CNF-satisfiabilityproblem: Given a CNF-formula where each clause has widthw and the guarantee that there exists an assignment satisfyingat least g = [w/2] - 1 literals in each clause, it is NP-hard tofind a satisfying assignment to the formula (that sets at leastone literal to true in each clause). On the other hand, when g = [w/2], it is easy to find a satisfying assignment via simplegeneralizations of the algorithms for 2-SAT. Viewing 2-SAT &#x2208; P as easiness of SAT when 1-in-2 literals are true in every clause, and NP-hardness of 3-SAT as intractability of SAT when 1-in-3 literals are true, our resultshows, for any fixed &#x03B5; &gt; 0, the hardness of finding a satisfyingassignment to instances of "(2 + &#x03B5;)-SAT" where the density ofsatisfied literals in each clause is promised to exceed 1/(2+&#x03B5;). We also strengthen the results to prove that given a (2k + 1)-uniform hypergraph that can be 2-colored such that each edgehas perfect balance (at most k + 1 vertices of either color), itis NP-hard to find a 2-coloring that avoids a monochromaticedge. In other words, a set system with discrepancy 1 is hard todistinguish from a set system with worst possible discrepancy.
[probabilistically checkable proofs, graph theory, Color, CNF-satisfiability problem, Constraint satisfaction, Probabilistic logic, complexity dichotomy, Complexity theory, monochromatic edge, polymorphisms, Standards, Computer science, (2 + &#x03B5;)-SAT, NP-hardness, CNF-formula, NP-hard problem, discrepancy, uniform hypergraph, Polynomials, Labeling, computational complexity, promise problems]
A Counter-example to Karlin's Strong Conjecture for Fictitious Play
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Fictitious play is a natural dynamic for equilibrium play in zero-sum games, proposed by Brown [6], and shown to converge by Robinson [33]. Samuel Karlin conjectured in 1959 that fictitious play converges at rate O(t-1/2) with respect to the number of steps t. We disprove this conjecture by showing that, when the payoff matrix of the row player is the n &#x00D7; n identity matrix, fictitious play may converge (for some tie-breaking) at rate as slow as &#x03A9;(t-1/n).
[equilibrium play, Karlin's strong conjecture, Heuristic algorithms, game theory, Nash equilibrium, Linear programming, Karlin's conjecture, Vectors, Convergence, matrix algebra, zero-sum games, fictitious play, Games, natural dynamic, payoff matrix]
A Simple and Approximately Optimal Mechanism for an Additive Buyer
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We consider a monopolist seller with n heterogeneous items, facing a single buyer. The buyer hasa value for each item drawn independently according to(non-identical) distributions, and his value for a set ofitems is additive. The seller aims to maximize his revenue.It is known that an optimal mechanism in this setting maybe quite complex, requiring randomization [19] and menusof infinite size [15]. Hart and Nisan [17] have initiated astudy of two very simple pricing schemes for this setting:item pricing, in which each item is priced at its monopolyreserve; and bundle pricing, in which the entire set ofitems is priced and sold as one bundle. Hart and Nisan [17]have shown that neither scheme can guarantee more thana vanishingly small fraction of the optimal revenue. Insharp contrast, we show that for any distributions, thebetter of item and bundle pricing is a constant-factorapproximation to the optimal revenue. We further discussextensions to multiple buyers and to valuations that arecorrelated across items.
[approximation theory, Additives, bundle pricing, single buyer, Approximation methods, Cost accounting, Computer science, Runtime, optimisation, optimal revenue, randomization, monopoly, pricing schemes, Pricing, revenue maximization, monopolist seller, Polynomials, constant-factor approximation, pricing, heterogeneous items, optimal mechanism]
Achieving Target Equilibria in Network Routing Games without Knowing the Latency Functions
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
The analysis of network routing games typically assumes, right at the onset, precise and detailed information about the latency functions. Such information may, however, be unavailable or difficult to obtain. Moreover, one is often primarily interested in enforcing a desirable target flow as the equilibrium by suitably influencing player behavior in the routing game. We ask whether one can achieve target flows as equilibria without knowing the underlying latency functions. Our main result gives a crisp positive answer to this question. We show that, under fairly general settings, one can efficiently compute edge tolls that induce a given target multicommodity flow in a nonatomic routing game using a polynomial number of queries to an oracle that takes candidate tolls as input and returns the resulting equilibrium flow. This result is obtained via a novel application of the ellipsoid method, and applies to arbitrary multicommodity settings and non-linear latency functions. Our algorithm extends easily to many other settings, such as (i) when certain edges cannot be tolled or there is an upper bound on the total toll paid by a user, and (ii) general nonatomic congestion games. We obtain tighter bounds on the query complexity for series-parallel networks, and single-commodity routing games with linear latency functions, and complement these with a query-complexity lower bound applicable even to single-commodity routing games on parallel-link graphs with linear latency functions. We also explore the use of Stackelberg routing to achieve target equilibria and obtain strong positive results for series-parallel graphs. Our results build upon various new techniques that we develop pertaining to the computation of, and connections between, different notions of approximate equilibrium, properties of multicommodity flows and tolls in series-parallel graphs, and sensitivity of equilibrium flow with respect to tolls. Our results demonstrate that one can indeed circumvent the potentially-onerous task of modeling latency functions, and yet obtain meaningful results for the underlying routing game.
[polynomial number, linear latency functions, network routing games, nonlinear latency functions, approximate equilibrium, graph theory, network theory (graphs), Complexity theory, series-parallel graphs, Ellipsoids, nonatomic routing game, tolls, single-commodity routing games, query processing, ellipsoid method, nonatomic congestion games, Polynomials, series-parallel networks, edge tolls, oracle, Computational modeling, query-complexity lower bound, game theory, arbitrary multicommodity settings, Routing, target equilibria, parallel-link graphs, approximate equilibria, Games, Network routing, multicommodity flows, multicommodity flow, Delays, Stackelberg routing, computational complexity]
An Algebraic Approach to Non-malleability
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
In their seminal work on non-malleable cryptography, Dolev, Dwork and Naor, showed how to construct a non-malleable commitment with logarithmically-many "rounds"/"slots\
[sigma protocols, Protocols, commitments, cryptographic protocols, Receivers, Educational institutions, Vectors, nonmalleable cryptography, non-malleability, zero-knowledge, Complexity theory, Security, round complexity, one-way function, nonmalleable commitment, standard homomorphic commitments, nonmalleable zero-knowledge argument, atomic subprotocols, constant-round protocols, Error correction codes, nonmalleable protocol]
An Automatic Inequality Prover and Instance Optimal Identity Testing
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We consider the problem of verifying the identity of a distribution: Given the description of a distribution over a discrete support p = (p<sub>1</sub>, p<sub>2</sub>, ... , p<sub>n</sub>), how many samples (independent draws) must one obtain from an unknown distribution, q, to distinguish, with high probability, the case that p = q from the case that the total variation distance (L<sub>1</sub> distance) ||p - q||1&#x2265; &#x03F5;? We resolve this question, up to constant factors, on an instance by instance basis: there exist universal constants c, c' and a function f(p, &#x03F5;) on distributions and error parameters, such that our tester distinguishes p = q from ||p-q||1&#x2265; &#x03F5; using f(p, &#x03F5;) samples with success probability &gt; 2/3, but no tester can distinguish p = q from ||p - q||1&#x2265; c &#x00B7; &#x03F5; when given c' &#x00B7; f(p, &#x03F5;) samples. The function f(p, &#x03F5;) is upperbounded by a multiple of ||p||2/3/&#x03F5;2, but is more complicated, and is significantly smaller in some cases when p has many small domain elements, or a single large one. This result significantly generalizes and tightens previous results: since distributions of support at most n have L<sub>2/3</sub> norm bounded by &#x221A;n, this result immediately shows that for such distributions, O(&#x221A;n/&#x03F5;2) samples suffice, tightening the previous bound of O(&#x221A;npolylog/n4) for this class of distributions, and matching the (tight) known results for the case that p is the uniform distribution over support n. The analysis of our very simple testing algorithm involves several hairy inequalities. To facilitate this analysis, we give a complete characterization of a general class of inequalities- generalizing Cauchy-Schwarz, Holder's inequality, and the monotonicity of L<sub>p</sub> norms. Specifically, we characterize the set of sequences (a)<sub>i</sub> = a<sub>1</sub>, . . . , ar, (b)i = b<sub>1</sub>, . . . , br, (c)i = c<sub>1</sub>, ... , cr, for which it holds that for all finite sequences of positive numbers (x)<sub>j</sub> = x<sub>1</sub>,... and (y)<sub>j</sub> = y<sub>1</sub>,...,&#x03A0;<sub>i=1</sub>r (&#x03A3;<sub>j</sub>xa<sub>j</sub>i<sub>y</sub><sub>i</sub>bi)ci&#x2265;1. For example, the standard Cauchy-Schwarz inequality corresponds to the sequences a = (1, 0, 1/2), b = (0,1, 1/2), c = (1/2 , 1/2 , -1). Our characterization is of a non-traditional nature in that it uses linear programming to compute a derivation that may otherwise have to be sought throu.gh trial and error, by hand. We do not believe such a characterization has appeared in the literature, and hope its computational nature will be useful to others, and facilitate analyses like the one here.
[Algorithm design and analysis, tight-known result matching, bounded L<sub>2/3</sub> norm, linear programming, Complexity theory, universal constants, finite positive number sequences, Automated Theorem Proving, O(&#x221A;n/&#x03B5;2), testing algorithm, Polynomials, O(&#x221A;npolylog/n4) bound, theorem proving, error parameters, generalized Cauchy-Schwarz inequality, Testing, total variation distance, automatic inequality prover, instance optimal identity testing, Cauchy-Schwarz inequality, probability, general inequalities, generalized Holder inequality, Linear programming, Vectors, Property Testing, domain elements, Identity Testing, constant factors, distribution identity verification, upper-bounded function, standard Cauchy-Schwarz inequality, Instance Optimal, hairy inequalities, unknown distribution parameters, computational complexity, L<sub>p</sub> norm monotonicity]
An Exponential Lower Bound for Homogeneous Depth Four Arithmetic Formulas
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We show here a 2&#x03A9;(&#x221A;d&#x00B7;log N) size lower bound for homogeneous depth four arithmetic formulas. That is, we give an explicit family of polynomials of degree d on N variables (with N = d3 in our case) with 0, 1-coefficients such that for any representation of a polynomial f in this family of the form f = &#x03A3;<sub>i</sub> &#x03A0;<sub>j</sub> Q<sub>ij</sub>, where the Qij's are homogeneous polynomials (recall that a polynomial is said to be homogeneous if all its monomials have the same degree), it must hold that &#x03A3;i,j (Number of monomials of Q<sub>ij</sub>) &#x2265; 2&#x03A9;(&#x221A;d&#x00B7;log N). The above mentioned family, which we refer to as the NisanWigderson design-based family of polynomials, is in the complexity class VNP. Our work builds on the recent lower bound results [1], [2], [3], [4], [5] and yields an improved quantitative bound as compared to the quasi-polynomial lower bound of [6] and the N&#x03A9;(log log N) lower bound in the independent work of [7].
[circuit complexity, complexity class VNP, Computational modeling, 2&#x03A9;(&#x221A;d&#x00B7;log N) size lower bound, shifted partial derivatives, Vectors, Complexity theory, Electronic mail, homogeneous polynomials, lower bounds, 0, homogeneous depth arithmetic formulas, Nisan-Wigderson design-based family, Arithmetic circuits, Logic gates, 1-coefficients, Polynomials, Silicon]
Barriers to Near-Optimal Equilibria
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
This paper explains when and how communication and computational lower bounds for algorithms for an optimization problem translate to lower bounds on the worst-case quality of equilibria in games derived from the problem. We give three families of lower bounds on the quality of equilibria, each motivated by a different set of problems: congestion, scheduling, and distributed welfare games, welfare-maximization in combinatorial auctions with "black-box" bidder valuations, and welfare-maximization in combinatorial auctions with succinctly described valuations. The most straightforward use of our lower bound framework is to harness an existing computational or communication lower bound to derive a lower bound on the worst-case price of anarchy (POA) in a class of games. This is a new approach to POA lower bounds, which relies on reductions in lieu of explicit constructions. More generally, the POA lower bounds implied by our framework apply to all classes of games that share the same underlying optimization problem, independent of the details of players' utility functions. For this reason, our lower bounds are particularly significant for problems of game design -- ranging from the design of simple combinatorial auctions to the computation of tolls for routing networks -- where the goal is to design a game that has only near-optimal equilibria. For example, our results imply that the simultaneous first-price auction format is optimal among all "simple combinatorial auctions" in several settings.
[optimization problem, Protocols, combinatorial mathematics, computational lower bounds, routing networks, congestion problem, combinatorial auctions, explicit constructions, scheduling problem, mechanism design, Approximation methods, Cost accounting, price of anarchy, optimisation, toll computation, first-price auction format, worst-case equilibrium quality, distributed welfare game problem, Cost function, Polynomials, player utility functions, near-optimal equilibrium barriers, worst-case price of anarchy, communication lower bounds, game theory, worst-case POA lower bounds, game design, complexity of equilbria, Games, welfare-maximization, black-box bidder valuations, computational complexity]
Bi-Lipschitz Bijection between the Boolean Cube and the Hamming Ball
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We construct a bi-Lipschitz bijection from the Boolean cube to the Hamming ball of equal volume. More precisely, we show that for all even n E N there exists an explicit bijection &#x03C8;: {0, 1}n &#x2192; {x E {0, 1}n+1 : |x| &gt; n/2} such that for every x &#x2260; y E {0, 1}n+1 it holds that 1/5 &#x2264; dist(&#x03C8;(x), &#x03C8;(y)) &#x2264; 4 5 - dist(x, y) where dist(&#x00B7;, &#x00B7;) denotes the Hamming distance. In particular, this implies that the Hamming ball is bi-Lipschitz transitive. This result gives a strong negative answer to an open problem of Lovett and Viola [CC 2012], who raised the question in the context of sampling distributions in low-level complexity classes. The conceptual implication is that the problem of proving lower bounds in the context of sampling distributions requires ideas beyond the sensitivity-based structural results of Boppana [IPL 97]. We study the mapping &#x03C8; further and show that it (and its inverse) are computable in DLOGTIME-uniform TC&#x00B0;, but not in AC&#x00B0;. Moreover, we prove that &#x03C8; is &#x201C;approximately local&#x201D; in the sense that all but the last output bit of &#x03C8; are essentially determined by a single input bit.
[Context, sampling methods, Hamming distance, approximately local &#x03C8;, sensitivity-based structural analysis, biLipschitz bijection, Complexity theory, Partitioning algorithms, Boolean cube, statistical distributions, explicit bijection, biLipschitz transitive Hamming ball, Equations, lower bounds, Computer science, low-level complexity classes, input bit, Boolean functions, sampling distributions, DLOGTIME-uniform TC&#x00B0;, computational complexity]
Bounds on the Permanent and Some Applications
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We give new lower and upper bounds on the permanent of a doubly stochastic matrix. Combined with previous work, this improves on the deterministic approximation factor. We also give a combinatorial application of the lower bound, proving S. Friedland's "Asymptotic Lower Matching Conjecture"for the monomer-dimer problem.
[approximation theory, bounds on the permanent, combinatorial mathematics, doubly-stochastic matrix permanent, asymptotic lower-matching conjecture, upper bounds, Educational institutions, Approximation methods, lower bounds, matrix algebra, Computer science, Upper bound, combinatorial application, monomer-dimer problem, Approximation algorithms, deterministic approximation factor improvement, Polynomials, approximation of the permanent, Bipartite graph, stochastic processes]
Chasing Ghosts: Competing with Stateful Policies
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We consider sequential decision making in a setting where regret is measured with respect to a set of stateful reference policies, and feedback is limited to observing the rewards of the actions performed (the so called &#x201C;bandit&#x201D; setting). If either the reference policies are stateless rather than stateful, or the feedback includes the rewards of all actions (the so called &#x201C;expert&#x201D; setting), previous work shows that the &#x221A; optimal regret grows like &#x0398;(&#x221A;T) in terms of the number of decision rounds T. The difficulty in our setting is that the decision maker unavoidably loses track of the internal states of the reference policies, and thus cannot reliably attribute rewards observed in a certain round to any of the reference policies. In fact, in this setting it is impossible for the algorithm to estimate which policy gives the highest (or even approximately highest) total reward. Nevertheless, we design an algorithm that achieves expected regret that is sublinear in T, of the form O(T/ log1/4 T). Our algorithm is based on a certain local repetition lemma that may be of independent interest. We also show that no algorithm can guarantee expected regret better than O(T/ log3/2 T).
[Algorithm design and analysis, Adaptation models, highest total reward, expert setting, Decision making, Switches, bandit setting, Vehicles, sequential decision making, Upper bound, O(T/ log1/4 T) algorithm, local repetition lemma, Games, decision making, stateful reference policies, computational complexity]
Complexity Classification of Local Hamiltonian Problems
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
The calculation of ground-state energies of physical systems can be formalised as the k-local Hamiltonian problem, which is the natural quantum analogue of classical constraint satisfaction problems. One way of making the problem more physically meaningful is to restrict the Hamiltonian in question by picking its terms from a fixed set S. Examples of such special cases are the Heisenberg and Ising models from condensed-matter physics. In this work we characterise the complexity of this problem for all 2-local qubit Hamiltonians. Depending on the subset S, the problem falls into one of the following categories: in P, NP-complete, polynomial-time equivalent to the Ising model with transverse magnetic fields, or QMA-complete. The third of these classes contains NP and is contained within StoqMA. The characterisation holds even if S does not contain any 1-local terms, for example, we prove for the first time QMA-completeness of the Heisenberg and XY interactions in this setting. If S is assumed to contain all 1-local terms, which is the setting considered by previous work, we have a characterisation that goes beyond 2-local interactions: for any constant k, all k-local qubit Hamiltonians whose terms are picked from a fixed set S correspond to problems either in P, polynomial-time equivalent to the Ising model with transverse magnetic fields, or QMA-complete. These results are a quantum analogue of Schaefer's dichotomy theorem for boolean constraint satisfaction problems.
[Lattices, Heisenberg models, Complexity theory, Hamiltonian complexity, quantum statistical mechanics, StoqMA, Quantum computing, Boolean functions, condensed-matter physics, Ising model, Eigenvalues and eigenfunctions, QMA-completeness, magnetic fields, Ising models, Heisenberg model, local Hamiltonian problems, Matrix decomposition, transverse magnetic fields, Physics, Computer science, physics computing, local qubit Hamiltonians, constraint satisfaction problems, Boolean constraint satisfaction problems, Schaefer dichotomy theorem, complexity classification, computational complexity]
Complexity of Counting Subgraphs: Only the Boundedness of the Vertex-Cover Number Counts
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
For a class C of graphs, #Sub(C) is the counting problem that, given a graph H from C and an arbitrary graph G, asks for the number of subgraphs of G isomorphic to H. It is known that if C has bounded vertex-cover number (equivalently, the size of the maximum matching in C is bounded), then #Sub(C) is polynomial-time solvable. We complement this result with a corresponding lower bound: if C is any recursively enumerable class of graphs with unbounded vertexcover number, then #Sub(C) is #W[1]-hard parameterized by the size of H and hence not polynomial-time solvable and not even fixed-parameter tractable, unless FPT is equal to #W[1]. As a first step of the proof, we show that counting kmatchings in bipartite graphs is #W[1]-hard. Recently, Curticapean [ICALP 2013] proved the #W[1]-hardness of counting k-matchings in general graphs; our result strengthens this statement to bipartite graphs with a considerably simpler proof and even shows that, assuming the Exponential Time Hypothesis (ETH), there is no f(k)*n^o(k/log(k)) time algorithm for counting k-matchings in bipartite graphs for any computable function f. As a consequence, we obtain an independent and somewhat simpler proof of the classical result of Flum and Grohe [SICOMP 2004] stating that counting paths of length k is #W[1]-hard, as well as a similar almost-tight ETH-based lower bound on the exponent.
[Context, arbitrary graph, graph theory, Color, vertex-cover number count boundedness, Complexity theory, unbounded vertex-cover number, Standards, Computer science, 1, parameterized #W[1, bounded vertex-cover number, polynomial-time solvable, counting problem, Polynomials, Bipartite graph, computational complexity]
Constructive Discrepancy Minimization for Convex Sets
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
A classical theorem of Spencer shows that any set system with n sets and n elements admits a coloring of discrepancy O(&#x221A;(n)). Recent exciting work of Bansal, Lovett and Meka shows that such colorings can be found in polynomial time. In fact, the Lovett-Meka algorithm finds a half integral point in any "large enough" polytope. However, their algorithm crucially relies on the facet structure and does not apply to general convex sets. We show that for any symmetric convex set K with measure at least e-n/500, the following algorithm finds a point y &#x2208; K &#x2229; [-1, 1]n with &#x03A9;(n) coordinates in &#x00B1;1: (1) take a random Gaussian vector x, (2) compute the point y in K &#x2229; [- 1, 1]n that is closest to x. (3) return y. This provides another truly constructive proof of Spencer's theorem and the first constructive proof of a Theorem of Giannopoulos.
[Strips, discrepancy coloring, Discrepancy theory, set system, Lovett-Meka algorithm, set theory, Spencer theorem, graph colouring, symmetric convex set, Polynomials, Convex functions, Silicon, polynomial time, combinatorics, random Gaussian vector, Gaussian measure, convex sets, random processes, Linear programming, convex programming, Vectors, Geometry, constructive discrepancy minimization, Gaussian processes, convex optimization, minimisation, computational complexity]
Decremental Single-Source Shortest Paths on Undirected Graphs in Near-Linear Total Update Time
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
The decremental single-source shortest paths (SSSP) problem concerns maintaining the distances between a given source node s to every node in an n-node m-edge graph G undergoing edge deletions. While its static counterpart can be easily solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In this case, the classic O(mn) total update time of Even and Shiloach (JACM 1981) has been the fastest known algorithm for three decades. With the loss of a (1 + &#x03B5;)-approximation factor, the running time was recently improved to O(n2+o(1)) by Bernstein and Roditty (SODA 2011), and more recently to O(n1.8+o(1) + m1+o(1)) by Henzinger, Krinninger, and Nanongkai (SODA 2014). In this paper, we finally bring the running time of this case down to near-linear: We give a (1 + &#x03B5;)-approximation algorithm with O(m1+o(1)) total update time, thus obtaining near-linear time. Moreover, we obtain O(m1+o(1) log W) time for the weighted case, where the edge weights are integers from 1 to W. The only prior work on weighted graphs in o(mn log W) time is the O(mn0.986 log W)-time algorithm by Henzinger, Krinninger, and Nanongkai (STOC 2014) which works for the general weighted directed case. In contrast to the previous results which rely on maintaining a sparse emulator, our algorithm relies on maintaining a so-called sparse (d, &#x03B5;)-hop set introduced by Cohen (JACM 2000) in the PRAM literature. A (d, &#x03B5;)-hop set of a graph G = (V, E) is a set E' of weighted edges such that the distance between any pair of nodes in G can be (1 + &#x03B5;)-approximated by their d-hop distance (given by a path containing at most d edges) on G'=(V, E&#x222A;E'). Our algorithm can maintain an (no(1), &#x03B5;)-hop set of near-linear size in near-linear time under edge deletions. It is the first of its kind to the best of our knowledge. To maintain the distances on this hop set, we develop a monotone bounded-hop Even-Shiloach tree. It results from extending and combining the monotone Even-Shiloach tree of Henzinger, Krinninger, and Nanongkai (FOCS 2013) with the bounded-hop SSSP technique of Bernstein (STOC 2013). These two new tools might be of independent interest.
[Algorithm design and analysis, approximation theory, monotone bounded-hop Even-Shiloach tree, Heuristic algorithms, dynamic graph algorithms, Europe, trees (mathematics), Educational institutions, Approximation methods, Computer science, edge deletions, decremental single-source shortest path problem, Approximation algorithms, single-source shortest paths, undirected graphs, decremental SSSP problem, computational complexity]
Digital Morphogenesis via Schelling Segregation
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Schelling's model of segregation looks to explain the way in which particles or agents of two types may come to arrange themselves spatially into configurations consisting of large homogeneous clusters, i.e. connected regions consisting of only one type. As one of the earliest agent based models studied by economists and perhaps the most famous model of self-organising behaviour, it also has direct links to areas at the interface between computer science and statistical mechanics, such as the Ising model and the study of contagion and cascading phenomena in networks. While the model has been extensively studied it has largely resisted rigorous analysis, prior results from the literature generally pertaining to variants of the model which are tweaked so as to be amenable to standard techniques from statistical mechanics or stochastic evolutionary game theory. In BK, Brandt, Immorlica, Kamath and Kleinberg provided the first rigorous analysis of the unperturbed model, for a specific set of input parameters. Here we provide a rigorous analysis of the model's behaviour much more generally and establish some surprising forms of threshold behaviour, notably the existence of situations where an increased level of intolerance for neighbouring agents of opposite type leads almost certainly to decreased segregation.
[neighbouring agents, stochastic evolutionary game theory, Ising, Schelling segregation, Computational modeling, digital morphogenesis, homogeneous clusters, Educational institutions, threshold behaviour, algorithmic game theory, Electronic mail, networks, spin glass, Computer science, Analytical models, input parameters, morphogenesis, computer science, unperturbed model, self-organising behaviour, Mathematical model, Schelling segregation model, stochastic games, statistical mechanics]
Dynamic Integer Sets with Optimal Rank, Select, and Predecessor Search
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We present a data structure representing a dynamic set S of w-bit integers on a w-bit word RAM. With S = n and w &#x2265; logn and space O(n), we support the following standard operations in O(log n/ log w) time: . insert(x) sets S = S &#x222A; {x}. .delete(x) sets S = S \\ {x}. . predecessor(x) returns max{y &#x2208; S \\ y &lt;; x}. . successor(x) returns min{y &#x2208; S y &#x2265; x}. . rank(x) returns # {y &#x2208; S \\ y &lt;; x}. . select(i) returns y &#x2208; S with rank(y) = i, if any. Our O (log n/ log w) bound is optimal for dynamic rank and select, matching a lower bound of Fredman and Saks [STOC'99]. When the word length is large, our time bound is also optimal for dynamic predecessor, matching a static lower bound of Beame and Fich [STOC' 99] whenever log n/ log w = O (log w/ log log w). Technically, the most interesting aspect of our data structure is that it supports all the above operations in constant time for sets of size n = wO(1). This resolves a main open problem of Ajtai, Komlos, and Fredman [FOCS'83]. Ajtai et al. presented such a data structure in Yao's abstract cell-probe model with w-bit cells/words, but pointed out that the functions used could not be implemented. As a partial solution to the problem, Fredman and Willard [STOC'90] introduced a fusion node that could handle queries in constant time, but used polynomial time on the updates. We call our small set data structure a dynamic fusion node as it does both queries and updates in constant time.
[w-bit integers, dynamic predecessor, random-access storage, Computational modeling, Random access memory, optimal rank, data structure, Data structures, integer data structures, Indexes, w-bit word RAM, dynamic fusion node, Standards, dynamic data structures, dynamic integer sets, predecessor search, Polynomials, data structures, polynomial time, Probes, computational complexity]
Exponential Separation of Information and Communication
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We show an exponential gap between communication complexity and information complexity, by giving an explicit example for a communication task (relation), with information complexity &#x2264; O(k), and distributional communication complexity &#x2265;2k. This shows that a communication protocol cannot always be compressed to its internal information. By a result of Braverman [1], our gap is the largest possible. By a result of Braverman and Rao [2], our example shows a gap between communication complexity and amortized communication complexity, implying that a tight direct sum result for distributional communication complexity cannot hold.
[Protocols, Noise, communication protocol, Entropy, Complexity theory, distributional communication complexity, direct sum, Noise measurement, communication complexity, information complexity, amortized communication complexity, Games, Random variables, protocols, communication compression]
Fixed-Parameter Tractable Canonization and Isomorphism Test for Graphs of Bounded Treewidth
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We give a fixed-parameter tractable algorithm that, given a parameter k and two graphs G<sub>1</sub>, G<sub>2</sub>, either concludes that one of these graphs has treewidth at least k, or determines whether G<sub>1</sub> and G<sub>2</sub> are isomorphic. The running time of the algorithm on an n-vertex graph is 2O(k5 log k) &#x00B7; n5, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth. Our algorithm in fact solves the more general canonization problem. We namely design a procedure working in 2OO(k5 log k) &#x00B7; n5 time that, for a given graph G on n vertices, either concludes that the treewidth of G is at least k, or finds an isomorphism-invariant construction term - an algebraic expression that encodes G together with a tree decomposition of G of width O(k4). Hence, a canonical graph isomorphic to G can be constructed by simply evaluating the obtained construction term, while the isomorphism test reduces to verifying whether the computed construction terms for G<sub>1</sub> and G<sub>2</sub> are equal.
[Algorithm design and analysis, canonical graph, Particle separators, Heuristic algorithms, graph theory, fixed-parameter tractable algorithm, fixed-parameter tractable isomorphism test, Complexity theory, Standards, O(k4) width, Adhesives, algebraic expression, canonization, fixed-parameter tractable canonization graph, n-vertex graph, 2OO(k5 log k) &#x00B7; n5 time, graph isomorphism, parameterized algorithms, isomorphism-invariant construction term, Polynomials, treewidth, computational complexity]
Generating k-Independent Variables in Constant Time
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
The generation of pseudorandom elements over finite fields is fundamental to the time, space and randomness complexity of randomized algorithms and data structures. We consider the problem of generating k-independent random values over a finite field F in a word RAM model equipped with constant time addition and multiplication in F, and present the first nontrivial construction of a generator that outputs each value in constant time, not dependent on k. Our generator has period length |F| poly log k and uses k poly (log k) log |F| bits of space, which is optimal up to a poly log k factor. We are able to bypass Siegel's lower bound on the time-space tradeoff for k-independent functions by a restriction to sequential evaluation.
[nontrivial generator construction, Random access memory, random number generation, k-independent functions, k poly(log k) log |F| space, finite fields, poly log k factor, Polynomials, data structures, time-space tradeoff, constant time addition, sequential evaluation, word RAM model, time complexity, randomness complexity, Data structures, Probabilistic logic, Generators, Graph theory, constant time, lower bound, pseudorandom element generation, randomized algorithms, constant time multiplication, k-independent random value generation, |F| poly log k period length, Time complexity, space complexity, computational complexity]
Hardness of Coloring 2-Colorable 12-Uniform Hypergraphs with exp(log^{Omega(1)} n) Colors
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We show that it is quasi-NP-hard to color 2-colorable 12-uniform hypergraphs with 2(log n) O(1) colors where n is the number of vertices. Previously, Guruswami et al. [1] showed that it is quasi-NP-hard to color 2-colorable 8-uniform hypergraphs with 22 O(vlog log n) colors. Their result is obtained by composing a standard Outer PCP with an Inner PCP based on the Short Code of super-constant degree. Our result is instead obtained by composing a new Outer PCP with an Inner PCP based on the Short Code of degree two.
[Hypergraph, Coloring, Symmetric matrices, 2-colorable 12-uniform hypergraph coloring, Color, 2(logn)&#x03A9;(1) colors, super-constant degree, Complexity theory, graph colouring, Inapproximability, graph vertices, standard outer PCP, Polynomials, degree-two short code, Error correction, Error correction codes, quasiNP-hard problem, inner PCP, PCP, computational complexity]
Improved Quantum Algorithm for Triangle Finding via Combinatorial Arguments
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
In this paper we present a quantum algorithm solving the triangle finding problem in unweighted graphs with query complexity O&#x0303;(n5/4), where n denotes the number of vertices in the graph. This improves the previous upper bound O(n9/7) = O(n1.285) recently obtained by Lee, Magniez and Santha. Our result shows, for the first time, that in the quantum query complexity setting unweighted triangle finding is easier than its edge-weighted version, since for finding an edge-weighted triangle Belovs and Rosmanis proved that any quantum algorithm requires O(n9/7/ &#x221A;log n) queries. Our result also illustrates some limitations of the non-adaptive learning graph approach used to obtain the previous O(n9/7) upper bound since, even over unweighted graphs, any quantum algorithm for triangle finding obtained using this approach requires v(n9/7/ &#x221A;log n) queries as well. To bypass the obstacles characterized by these lower bounds, our quantum algorithm uses combinatorial ideas exploiting the graph-theoretic properties of triangle finding, which cannot be used when considering edge-weighted graphs or the non-adaptive learning graph approach.
[Algorithm design and analysis, combinatorial arguments, O(n9/7) upper bound, graph theory, query complexity, Search problems, Complexity theory, Quantum computing, graph vertices, unweighted triangle finding problem, quantum algorithms, edge-weighted graphs, quantum query complexity, Computer science, nonadaptive learning graph approach, Upper bound, O(n9/7/&#x221A;log n) queries, Quantum mechanics, quantum computing, quantum algorithm improvement, O&#x0303;(n5/4) query complexity, graph-theoretic properties, edge-weighted triangle finding problem, triangle finding, unweighted graphs, computational complexity]
Interactive Channel Capacity Revisited
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We provide the first capacity approaching coding schemes that robustly simulate any interactive protocol over an adversarial channel that corrupts any fraction of the transmitted symbols. Our coding schemes achieve a communication rate of 1 - O(&#x2208;&#x221A;loglog1/&#x2208;) can be improved to 1 - O(&#x221A;&#x2208;) for random, oblivious, and over any adversarial channel. This computationally bounded channels, or if parties have shared randomness unknown to the channel. Surprisingly, these rates exceed the 1 - &#x03A9;( H(&#x03F5;)) = 1 - &#x03A9;(&#x03F5;&#x221A;log1/&#x03F5;) interactive channel capacity bound which [Kol and Raz; STOC'13] recently proved for random errors. We conjecture 1- &#x0398;(&#x03F5; log log 1/&#x03F5;) and 1- &#x0398;(&#x221A;&#x03F5;) to be the optimal rates for their respective settings and therefore to capture the interactive channel capacity for random and adversarial errors. In addition to being very communication efficient, our randomized coding schemes have multiple other advantages. They are computationally efficient, extremely natural, and significantly simpler than prior (non-capacity approaching) schemes. In particular, our protocols do not employ any coding but allow the original protocol to be performed as-is, interspersed only by short exchanges of hash values. When hash values do not match, the parties backtrack. Our approach is, as we feel, by far the simplest and most natural explanation for why and how robust interactive communication in a noisy environment is possible.
[noisy environment, Protocols, Error analysis, Noise, channel capacity, capacity approaching coding scheme, computationally bounded channel, Robustness, adversarial error, protocols, interactive protocol, adversarial channel, robust interactive communication, conding for interactive communications, Channel capacity, Redundancy, error analysis, hash value, transmitted symbol, communication rate, Encoding, encoding, randomized coding scheme, interactive channel capacity, backtracking, random error, computational complexity]
List and Unique Coding for Interactive Communication in the Presence of Adversarial Noise
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
In this paper we extend the notion of list-decoding to the setting of interactive communication and study its limits. In particular, we show that any protocol can be encoded, with a constant rate, into a list-decodable protocol which is resilient to a noise rate of up to 1/2-&#x03B5; and that this is tight. Using our list-decodable construction, we study a more nuanced model of noise where the adversary can corrupt up to a fraction &#x03B1; Alice's communication and up to a fraction &#x03B2; of Bob's communication. We use list-decoding in order to fully characterize the region R<sub>u</sub> of pairs (&#x03B1;, &#x03B2;) for which unique decoding with a constant rate is possible. The region R<sub>u</sub> turns out to be quite unusual in its shape. In particular, it is bounded by a piecewise-differentiable curve with infinitely many pieces. We show that outside this region, the rate must be exponential. This suggests that in some error regimes, list-decoding is necessary for optimal unique decoding. We also consider the setting where only one party of the communication must output the correct answer. We precisely characterize the region of all pairs (&#x03B1;, &#x03B2;) for which one-sided unique decoding is possible in a way that Alice will output the correct answer.
[List Decodable Codes, Protocols, Error analysis, list-decodable construction, Noise, optimal unique decoding, one-sided unique decoding, Encoding, tree codes, Decoding, Interactive Communication, fraction &#x03B1; Alice's communication, piecewise-differentiable curve, Bob's communication, list decoding, Tree Codes, interactive communication, Error correction codes, Error correction, protocols, fraction &#x03B2;, list-decodable protocol]
Local Tests of Global Entanglement and a Counterexample to the Generalized Area Law
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We introduce a technique for applying quantum expanders in a distributed fashion, and use it to solve two basic questions: testing whether a bipartite quantum state shared by two parties is the maximally entangled state and disproving a generalized area law. In the process these two questions which appear completely unrelated turn out to be two sides of the same coin. Strikingly in both cases a constant amount of resources are used to verify a global property.
[Protocols, Quantum entanglement, maximally entangled state, Stationary state, Lattices, local tests, quantum entanglement, Entropy, Vectors, communication complexity, global entanglement, global property, quantum expanders, quantum communication, generalized area law, bipartite quantum state, Testing]
LP-Based Algorithms for Capacitated Facility Location
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Linear programming has played a key role in the study of algorithms for combinatorial optimization problems. In the field of approximation algorithms, this is well illustrated by the uncapacitated facility location problem. A variety of algorithmic methodologies, such as LP-rounding and primal-dual method, have been applied to and evolved from algorithms for this problem. Unfortunately, this collection of powerful algorithmic techniques had not yet been applicable to the more general capacitated facility location problem. In fact, all of the known algorithms with good performance guarantees were based on a single technique, local search, and no linear programming relaxation was known to efficiently approximate the problem. In this paper, we present a linear programming relaxation with constant integrality gap for capacitated facility location. We demonstrate that the fundamental theories of multi-commodity flows and matchings provide key insights that lead to the strong relaxation. Our algorithmic proof of integrality gap is obtained by finally accessing the rich toolbox of LP-based methodologies: we present a constant factor approximation algorithm based on LP-rounding.
[Algorithm design and analysis, Measurement, algorithmic techniques, combinatorial mathematics, LP-rounding method, Linear programming, linear programming, approximation algorithms, Approximation methods, local search, Standards, facility location, LP-based algorithms, capacitated facility location, constant integrality gap, performance guarantees, linear programming relaxation, multicommodity flows, Approximation algorithms, Polynomials, combinatorial optimization problems, primaldual method, constant factor approximation algorithm]
Mechanism Design for Crowdsourcing: An Optimal 1-1/e Competitive Budget-Feasible Mechanism for Large Markets
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
In this paper we consider a mechanism design problem in the context of large-scale crowdsourcing markets such as Amazon's Mechanical Turk mturk, ClickWorker clickworker, CrowdFlower crowdflower. In these markets, there is a requester who wants to hire workers to accomplish some tasks. Each worker is assumed to give some utility to the requester on getting hired. Moreover each worker has a minimum cost that he wants to get paid for getting hired. This minimum cost is assumed to be private information of the workers. The question then is -- if the requester has a limited budget, how to design a direct revelation mechanism that picks the right set of workers to hire in order to maximize the requester's utility? We note that although the previous work (Singer (2010) chen et al. (2011)) has studied this problem, a crucial difference in which we deviate from earlier work is the notion of large-scale markets that we introduce in our model. Without the large market assumption, it is known that no mechanism can achieve a competitive ratio better than 0.414 and 0.5 for deterministic and randomized mechanisms respectively (while the best known deterministic and randomized mechanisms achieve an approximation ratio of 0.292 and 0.33 respectively). In this paper, we design a budget-feasible mechanism for large markets that achieves a competitive ratio of 1 - 1/e &#x2243; 0.63. Our mechanism can be seen as a generalization of an alternate way to look at the proportional share mechanism, which is used in all the previous works so far on this problem. Interestingly, we can also show that our mechanism is optimal by showing that no truthful mechanism can achieve a factor better than 1 - 1/e, thus, fully resolving this setting. Finally we consider the more general case of submodular utility functions and give new and improved mechanisms for the case when the market is large.
[Additives, MTRK, Approximation methods, randomized mechanisms, competitive ratio, submodular utility functions, proportional share mechanism, Budget-feasibility, Truthful Mechanisms, Pricing, CRDFLWR, Polynomials, utility theory, Crowdsourcing, approximation ratio, approximation theory, requester utility maximization, CrowdFlower, worker hiring, deterministic mechanisms, worker private information, Standards, budgeting, crowdsourcing mechanism design problem, optimal 1-1/e competitive budget-feasible mechanism, Large Markets, outsourcing, procurement, Amazon Mechanical Turk, ClickWorker, personnel, Resource management, pricing, large-scale crowdsourcing markets, CLKWRKR]
Network Sparsification for Steiner Problems on Planar and Bounded-Genus Graphs
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We propose polynomial-time algorithms that sparsify planar and bounded-genus graphs while preserving optimal or near-optimal solutions to Steiner problems. Our main contribution is a polynomial-time algorithm that, given an unweighted graph G embedded on a surface of genus g and a designated face f bounded by a simple cycle of length k, uncovers a set F in E(G) of size polynomial in g and k that contains an optimal Steiner tree for any set of terminals that is a subset of the vertices of f. We apply this general theorem to prove that: (2) given an unweighted graph G embedded on a surface of genus g and a terminal set S in V(G), one can in polynomial time find a set F in E(G) that contains an optimal Steiner tree T for S and that has size polynomial in g and |E(T)|; (2) an analogous result holds for an optimal Steiner forest for a set S of terminal pairs, (3) given an unweighted planar graph G and a terminal set S in V(G), one can in polynomial time find a set F in E(G) that contains an optimal (edge) multiway cut C separating S (i.e., a cutset that intersects any path with endpoints in different terminals from S) and has size polynomial in |C|. In the language of parameterized complexity, these results imply the first polynomial kernels for Steiner Tree and Steiner Forest on planar and bounded-genus graphs (parameterized by the size of the tree and forest, respectively) and for (Edge) Multiway Cut on planar graphs (parameterized by the size of the cutset). Steiner Tree and similar "subset" problems were identified in [Demaine, Hajiaghayi, Computer J., 2008] as important to the quest to widen the reach of the theory of bidimensionality ([Demaine et al., JACM 2005], [Fomin et al., SODA 2010]). Therefore, our results can be seen as a leap forward to achieve this broader goal. Additionally, we obtain a weighted variant of our main contribution: a polynomial-time algorithm that, given an edge-weighted planar graph G, a designated face f bounded by a simple cycle of weight w(f), and an accuracy parameter &amp;epsi; &gt; 0, uncovers a set F in E(G) of total weight at most poly(1/&amp;epsi;) w(f) that, for any set of terminal pairs that lie on f, contains a Steiner forest within additive error &amp;epsi; w(f) from the optimal Steiner forest. This result deepens the understanding of the recent framework of approximation schemes for network design problems on planar graphs ([Klein, SICOMP 2008], [Borradaile, Klein, Mathieu, ACM TALG 2009], and later works) by explaining the structure of the solution space within a brick of the so-called mortar graph -- the central notion of this framework.
[Steiner trees, Algorithm design and analysis, graph theory, Steiner Tree, Planar graphs, parameterized complexity, bidimensionality theory, Steiner Forest, optimal Steiner tree, (Edge) Multiway Cut, polynomial kernel, mortar graph, polynomial-time algorithms, parameterized algorithms, Polynomials, Face, Kernel, bounded-genus graphs, polynomial kernels, unweighted planar graphs, edge-weighted planar graph, Steiner problems, network design problems, optimal Steiner forest, Vegetation, Approximation algorithms, network sparsification, computational complexity]
New Algorithms and Lower Bounds for Monotonicity Testing
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We consider the problem of testing whether an unknown Boolean function f : {- 1, 1}n &#x2192; {-1, 1} is monotone versus &#x03B5;-far from every monotone function. The two main results of this paper are a new lower bound and a new algorithm for this well-studied problem. Lower bound: We prove an &#x03A9;&#x0305;(n1/5) lower bound on the query complexity of any non-adaptive two-sided error algorithm for testing whether an unknown Boolean function f is monotone versus constant-far from monotone. This gives an exponential improvement on the previous lower bound of &#x03A9;(log n) due to Fischer et al. [1]. We show that the same lower bound holds for monotonicity testing of Boolean-valued functions over hypergrid domains {1,&#x00B7;&#x00B7;&#x00B7;, m}n for all m &#x2265; 2. Upper bound: We present an O(n5/6) poly(1/&#x03B5;)-query algorithm that tests whether an unknown Boolean function f is monotone versus &#x03B5;-far from monotone. Our algorithm, which is non-adaptive and makes one-sided error, is a modified version of the algorithm of Chakrabarty and Seshadhri[2], which makes O(n7/8) poly(1/&#x03B5;) queries.
[Algorithm design and analysis, monotone function, hypergrid domains, query complexity, Vectors, Complexity theory, nonadaptive two-sided error algorithm, query algorithm, unknown Boolean function, Boolean functions, Upper bound, Monotonicity testing, monotonicity testing, Property testing, Boolean-valued functions, Random variables, Testing, computational complexity]
Noisy Interactive Quantum Communication
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We study the problem of simulating protocols in a quantum communication setting over noisy channels. This problem falls at the intersection of quantum information theory and quantum communication complexity, and will be of importance for eventual real-world applications of interactive quantum protocols, which can be proved to have exponentially lower communication costs than their classical counterparts for some problems. These are the first results concerning the quantum version of this problem, originally studied by Schulman in a classical setting (FOCS '92, STOC '93). We simulate a length N quantum communication protocol by a length O(N) protocol with arbitrarily small error. Our simulation strategy has a far higher communication rate than a naive one that encodes separately each particular round of communication to achieve comparable success. Such a strategy would have a communication rate going to 0 in the worst interaction case as the length of the protocols increases, in contrast to our strategy, which has a communication rate proportional to the capacity of the channel used. Under adversarial noise, our strategy can withstand, for arbitrarily small &amp;epsi; &gt; 0, error rates as high as 1/2 -- &amp;epsi; when parties preshare perfect entanglement, but the classical channel is noisy. We show that this is optimal. Note that in this model, the naive strategy would not work for any constant fraction of errors. We provide extension of these results in several other models of communication, including when also the entanglement is noisy, and when there is no pre-shared entanglement but communication is quantum and noisy. We also study the case of random noise, for which we provide simulation protocols with positive communication rates and no pre-shared entanglement over some quantum channels with quantum capacity Q = 0, proving that Q is in general not the right characterization of a channel's capacity for interactive quantum communication. Our results are stated for a general quantum communication protocol in which Alice and Bob collaborate, and hold in particular in the quantum communication complexity settings of the Yao and Cleve-Buhrman models.
[Adaptation models, quantum information theory, Protocols, telecommunication channels, Error analysis, quantum communication complexity, quantum channels, Quantum Computation and Information, Registers, Noise measurement, noisy interactive quantum communication, interactive quantum protocols, Communication Complexity, Coding Theory, adversarial noise, Quantum mechanics, protocols, noisy channels, quantum communication, simulation protocols, quantum communication protocol]
Non-malleable Codes against Constant Split-State Tampering
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Non-malleable codes were introduced by Dziembowski, Pietrzak and Wichs [1] as an elegant generalization of the classical notions of error detection, where the corruption of a codeword is viewed as a tampering function acting on it. Informally, a non-malleable code with respect to a family of tampering functions F consists of a randomized encoding function Enc and a deterministic decoding function Dec such that for any m, Dec(Enc(m)) = m. Further, for any tampering function f &#x2208; F and any message m, Dec(f(Enc(m))) is either m or is &#x2208;-close to a distribution D<sub>f</sub> independent of m, where &#x2208; is called the error. Of particular importance are non-malleable codes in the C-split-state model. In this model, the codeword is partitioned into C equal sized blocks and the tampering function family consists of functions (f<sub>1</sub>, . . . , f<sub>C</sub>) such that fi acts on the ith block. For C = 1 there cannot exist non-malleable codes. For C = 2, the best known explicit construction is by Aggarwal, Dodis and Lovett [2] who achieve rate = &#x03A9;(n-6/7) and error = 2-&#x03A9;(n-1/7), where n is the block length of the code. In our main result, we construct efficient non-malleable codes in the C-split-state model for C = 10 that achieve constant rate and error = 2-&#x03A9;(n). These are the first explicit codes of constant rate in the C-split-state model for any C = o(n), that do not rely on any unproven assumptions. We also improve the error in the explicit nonmalleable codes constructed in the bit tampering model by Cheraghchi and Guruswami [3]. Our constructions use an elegant connection found between seedless non-malleable extractors and non-malleable codes by Cheraghchi and Guruswami [3]. We explicitly construct such seedless non-malleable extractors for 10 independent sources and deduce our results on non-malleable codes based on this connection. Our constructions of extractors use encodings and a new variant of the sumproduct theorem.
[error detection codes, C-split-state model, codeword corruption, seedless nonmalleable extractors, error detection, nonmalleable codes, sum-product theorem, coding theory, Monte Carlo methods, &#x20AC;-close, bit tampering model, Robustness, explicit codes, tampering function, block length, Educational institutions, Encoding, Decoding, non-malleable codes, non-malleable extractors, randomness extractors, codeword partitioned, Computer science, Random variables, constant split-state tampering]
Novel Polynomial Basis and Its Application to Reed-Solomon Erasure Codes
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
In this paper, we present a new basis of polynomial over finite fields of characteristic two and then apply it to the encoding/decoding of Reed-Solomon erasure codes. The proposed polynomial basis allows that h-point polynomial evaluation can be computed in O(hlog<sub>2</sub>(h)) finite field operations with small leading constant. As compared with the canonical polynomial basis, the proposed basis improves the arithmetic complexity of addition, multiplication, and the determination of polynomial degree from O(hlog<sub>2</sub>(h)log<sub>2</sub>log<sub>2</sub>(h)) to O(hlog<sub>2</sub>(h)). Based on this basis, we then develop the encoding and erasure decoding algorithms for the (n=2r, k) Reed-Solomon codes. Thanks to the efficiency of transform based on the polynomial basis, the encoding can be completed in O(nlog<sub>2</sub>(k)) finite field operations, and the erasure decoding in O(nlog<sub>2</sub>(n)) finite field operations. To the best of our knowledge, this is the first approach supporting Reed-Solomon erasure codes over characteristic-2 finite fields while achieving a complexity of O(nlog<sub>2</sub>(n)), in both additive and multiplicative complexities. As the complexity leading factor is small, the algorithms are advantageous in practical applications.
[polynomials, h-point polynomial evaluation, Transforms, erasure decoding algorithms, Encoding, Decoding, Reed-Solomon erasure code, encoding, Computational complexity, Reed-Solomon codes, finite fields, finite field, encoding algorithms, Polynomial Basis, Polynomials, polynomial basis, Reed-Solomon erasure codes]
O(log log Rank) Competitive Ratio for the Matroid Secretary Problem
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
In the Matroid Secretary Problem (MSP), the elements of the ground set of a Matroid are revealed on-line one by one, each together with its value. An algorithm for the MSP is called Matroid-Unknown if, at every stage of its execution, it only knows (i) the elements that have been revealed so far and their values and (ii) an oracle for testing whether or not a subset the elements that have been revealed so far forms an independent set. An algorithm is called Known-Cardinality if it knows (i), (ii) and also knows from the start the cardinality n of the ground set of the Matroid. We present here a Known-Cardinality algorithm with a competitive-ratio of order log log the rank of the Matroid. The prior known results for a OC algorithm are a competitive-ratio of log the rank of the Matroid, by Babaioff et al. (2007), and a competitive-ratio of square root of log the rank of the Matroid, by Chakraborty and Lachish (2012).
[Algorithm design and analysis, combinatorial mathematics, oracle, O(log log Rank) competitive ratio, Data preprocessing, Known-Cardinality algorithm, Secretary, Optimized production technology, Matroid, competitive algorithms, Flyback transformers, set theory, matrix algebra, Matroid-Unknown, Computer science, matroid secretary problem, order log log, Competitive-Ratio, Approximation algorithms, Testing, computational complexity]
On Learning and Testing Dynamic Environments
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We initiate a study of learning and testing dynamic environments, focusing on environment that evolve according to a fixed local rule. The (proper) learning task consists of obtaining the initial configuration of the environment, whereas for non-proper learning it suffices to predict its future values. The testing task consists of checking whether the environment has indeed evolved from some initial configuration according to the known evolution rule. We focus on the temporal aspect of these computational problems, which is reflected in the requirement that only a small portion of the environment is inspected in each time slot (i.e., the time period between two consecutive applications of the evolution rule). We present some general observations, an extensive study of two special cases, two separation results, and a host of open problems. The two special cases that we study refer to linear rules of evolution and to rules of evolution that represent simple movement of objects. Specifically, we show that evolution according to any linear rule can be tested within a total number of queries that is sublinear in the size of the environment, and that evolution according to a simple one-dimensional movement can be tested within a total number of queries that is independent of the size of the environment.
[learning automata, linear rules, Observers, Encoding, learning, Complexity theory, property testing, multidimensional cellular automata, communication complexity, cellular automata, Property Testing, Learning, Three-dimensional displays, Emulation, Multi-dimensional cellular automata, Probes, Testing]
On the AC0 Complexity of Subgraph Isomorphism
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Let P be a fixed graph (hereafter called a &#x201C;pattern&#x201D;), and let SUBGRAPH(P) denote the problem of deciding whether a given graph G contains a subgraph isomorphic to P. We are interested in AC0-complexity of this problem, determined by the smallest possible exponent C(P) for which SUBGRAPH(P) possesses bounded-depth circuits of size nC(P)+o(1). Motivated by the previous research in the area, we also consider its &#x201C;colorful&#x201D; version SUBGRAPHcol(P) in which the target graph G is V(P)colored, and the average-case version SUBGRAP<sub>Have</sub>(P) under the distribution G(n, n-&#x03B8;(P)), where &#x03B8;(P) is the threshold exponent of P. Defining C<sub>col</sub>(P) and Cave(P) analogously to C(P), our main contributions can be summarized as follows. (1) C<sub>col</sub>(P) coincides with the tree-width of the pattern P within a logarithmic factor. This shows that the previously known upper bound by Alon, Yuster, Zwick [3] is almost tight. (2) We give a characterization of Cave(P) in purely combinatorial terms within a multiplicative factor of 2. This shows that the lower bound technique of Rossman [21] is essentially tight, for any pattern P whatsoever. (3) We prove that if Q is a minor of P then SUBGRAPH<sub>col</sub>(Q) is reducible to SUBGRAPH<sub>col</sub>(P) via a linear-size monotone projection. At the same time, we show that there is no monotone projection whatsoever that reduces SUBGRAPH(M<sub>3</sub>) to SUBGRAPH(P<sub>3</sub> + M<sub>2</sub>) (P<sub>3</sub> is a path on 3 vertices, Mk is a matching with k edges, and &#x201C;+&#x201D; stands for the disjoint union). This result strongly suggests that the colorful version of the subgraph isomorphism problem is much better structured and well-behaved than the standard (worstcase, uncolored) one.
[pattern matching, graph theory, pattern tree-width, Complexity theory, subgraph isomorphism problem, AC0, subgraph isomorphism, average-case complexity, bounded-depth circuits, edge matching, linear-size monotone projection, lower bound technique, treewidth, Context, logarithmic factor, trees (mathematics), Educational institutions, Standards, AC0 complexity, Computer science, Upper bound, disjoint union, average-case version SUBGRAPH, Integrated circuit modeling, computational complexity]
On the Hardness of Signaling
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
There has been a recent surge of interest in the role of information in strategic interactions. Much of this work seeks to understand how the realized equilibrium of a game is influenced by uncertainty in the environment and the information available to players in the game. Lurking beneath this literature is a fundamental, yet largely unexplored, algorithmic question: how should a "market maker" who is privy to additional information, and equipped with a specified objective, inform the players in the game? This is an informational analogue of the mechanism design question, and views the information structure of a game as a mathematical object to be designed, rather than an exogenous variable. We initiate a complexity-theoretic examination of the design of optimal information structures in general Bayesian games, a task often referred to as signaling. We focus on one of the simplest instantiations of the signaling question: Bayesian zero-sum games, and a principal who must choose an information structure maximizing the equilibrium payoff of one of the players. In this setting, we show that optimal signaling is computationally intractable, and in some cases hard to approximate, assuming that it is hard to recover a planted clique from an Erdos-Renyi random graph. This is despite the fact that equilibria in these games are computable in polynomial time, and therefore suggests that the hardness of optimal signaling is a distinct phenomenon from the hardness of equilibrium computation. Necessitated by the non-local nature of information structures, en-route to our results we prove an "amplification lemma" for the planted clique problem which may be of independent interest. Specifically, we show that even if we plant many cliques in an Erdos-Renyi random graph, so much so that most nodes in the graph are in some planted clique, recovering a constant fraction of the planted cliques is no easier than the traditional planted clique problem.
[planted clique problem, strategic interactions, optimal signaling, game theory, random processes, Linear programming, complexity-theoretic examination, Complexity theory, equilibrium payoff maximizing, equilibrium computation, Game theory, Signaling, Planted Clique, optimal information structures, Clustering algorithms, Games, Polynomials, polynomial time, Bayes methods, Erdos-Renyi random graph, Mechanism Design, computational complexity, Bayesian zero-sum games, amplification lemma]
On the Power of Homogeneous Depth 4 Arithmetic Circuits
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We prove exponential lower bounds on the size of homogeneous depth 4 arithmetic circuits computing an explicit polynomial in VP. Our results hold for the Iterated Matrix Multiplication polynomial - in particular we show that any homogeneous depth 4 circuit computing the (1, 1) entry in the product of n generic matrices of dimension nO(1) must have size n&#x03A9;(&#x221A;n). Our results strengthen previous works in two significant ways. 1) Our lower bounds hold for a polynomial in VP. Prior to our work, Kayal et al [KLSSa] proved an exponential lower bound for homogeneous depth 4 circuits (over fields of characteristic zero) computing a poly in VNP. The best known lower bounds for a depth 4 homogeneous circuit computing a poly in VP was the bound of n&#x03A9;(log n) by [KLSSb], [KLSSa]. Our exponential lower bounds also give the first exponential separation between general arithmetic circuits and homogeneous depth 4 arithmetic circuits. In particular they imply that the depth reduction results of Koiran [Koi12] and Tavenas [Tav13] are tight even for reductions to general homogeneous depth 4 circuits (without the restriction of bounded bottom fanin). 2) Our lower bound holds over all fields. The lower bound of [KLSSa] worked only over fields of characteristic zero. Prior to our work, the best lower bound for homogeneous depth 4 circuits over fields of positive characteristic was n&#x03A9;(log n) [KLSSb], [KLSSa].
[generic matrices, iterative methods, arithmetic circuits, Input variables, Educational institutions, Complexity theory, iterated matrix multiplication polynomial, Computer science, matrix multiplication, exponential lower bounds, Lower bounds, Upper bound, digital arithmetic, homogeneous depth circuits, Logic gates, depth reduction, VNP, homogeneous depth 4 circuits, Polynomials, computational complexity]
One-Way Functions and (Im)Perfect Obfuscation
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
A program obfuscator takes a program and outputs a "scrambled" version of it, where the goal is that the obfuscated program will not reveal much about its structure beyond what is apparent from executing it. There are several ways of formalizing this goal. Specifically, in indistinguishability obfuscation, first defined by Barak et al. (CRYPTO 2001), the requirement is that the results of obfuscating any two functionally equivalent programs (circuits) will be computationally indistinguishable. Recently, a fascinating candidate construction for indistinguishability obfuscation was proposed by Garg et al. (FOCS 2013). This has led to a flurry of discovery of intriguing constructions of primitives and protocols whose existence was not previously known (for instance, fully deniable encryption by Sahai and Waters, STOC 2014). Most of them explicitly rely on additional hardness assumptions, such as one-way functions. Our goal is to get rid of this extra assumption. We cannot argue that indistinguishability obfuscation of all polynomial-time circuits implies the existence of one-way functions, since if P &#x2260; NP, then program obfuscation (under the indistinguishability notion) is possible. Instead, the ultimate goal is to argue that if P &amp;ne; NP and program obfuscation is possible, then one-way functions exist. Our main result is that if NP &#x2288;; io-BPP and there is an efficient (even imperfect) indistinguishability obfuscator, then there are one-way functions. In addition, we show that the existence of an indistinguishability obfuscator implies (unconditionally) the existence of SZK-arguments for NP. This, in turn, provides an alternative version of our main result, based on the assumption of hard-on-the average NP problems. To get some of our results we need obfuscators for simple programs such as 3CNF formulas
[polynomial-time circuits, io-BPP, Probabilistic logic, NP problems, SZK-arguments, indistinguishability obfuscation, Inverters, Encryption, Electronic mail, 3CNF formulas, Awards activities, Polynomials, functionally equivalent program obfuscator, one-way functions, computational complexity]
Online Bipartite Matching in Offline Time
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
This paper investigates the problem of maintaining maximum size matchings in incremental bipartite graphs. In this problem a bipartite graph G between n clients and n servers is revealed online. The clients arrive in an arbitrary order and request to be matched to a subset of servers. In our model we allow the clients to switch between servers and want to maximize the matching size between them, i.e., after a client arrives we find an augmenting path from a client to a free server. Our goals in this model are twofold. First, we want to minimize the number of times clients are reallocated between the servers. Second, we want to give fast algorithms that recompute such reallocation. As for the number of changes, we propose a greedy algorithm that chooses an augmenting path &#x03C0; that minimizes the maximum number of times each server in &#x03C0; was used by augmenting paths so far. We show that in this algorithm each server has its client reassigned O(&#x221A;n) times. This gives an O(n3/2) bound on the total number of changes, what gives a progress towards the main open question risen by Chaudhuri et al. (INFOCOM'09) who asked to prove O(n log n) upper bound. Next, we argue that the same bound holds in the decremental case. Moreover, we show incremental and decremental algorithms that maintain (1 - &#x03B5;)-approximate matching with total of O(&#x03B5;-1n) reallocations, for any &#x03B5; &gt; 0. Finally, we address the question of how to efficiently compute paths given by this greedy algorithm. We show that by introducing proper amortization we can obtain an incremental algorithm that maintains the maximum size matching in total O(&#x221A;nm) time. This matches the running time of one of the fastest static maximum matching algorithms that was given by Hopcroft and Karp (SIAM J. Comput '73). We extend our result to decremental case where we give the same total bound on the running time. Additionally, we show O(&#x03B5;-1m) time incremental and decremental algorithms that maintain (1 - &#x03B5;)-approximate matching for any &#x03B5; &gt; 0. Observe that this bound matches the running time of the fastest approximate static solution as well.
[Greedy algorithms, Heuristic algorithms, graph theory, offline time, amortization, Servers, Approximation methods, matching, O(&#x03B5;-1n) reallocations, running time, incremental bipartite graphs, O(n log n) upper bound, static maximum matching algorithms, Bipartite graph, greedy algorithm, approximate static solution, client-server system, augmenting path, algorithm, (1 - &#x03B5;)-approximate matching, greedy algorithms, O(&#x03B5;-1m) time incremental algorithm, incremental, bipartite graph, Computer science, O(n3/2) bound, maximum size matchings, Approximation algorithms, online, &#x03B5;-1m) time decremental algorithm, O(&#x221A;n) time, total O(&#x221A;nm) time, computational complexity, online bipartite matching]
Optimal Error Rates for Interactive Coding II: Efficiency and List Decoding
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We study coding schemes for error correction in interactive communications. Such interactive coding schemes simulate any n-round interactive protocol using N rounds over an adversarial channel that corrupts up to &#x03C1;N transmissions. Important performance measures for a coding scheme are its maximum tolerable error rate &#x03C1;, communication complexity N, and computational complexity. We give the first coding scheme for the standard setting which performs optimally in all three measures: Our randomized non-adaptive coding scheme has a near-linear computational complexity and tolerates any error rate &#x03B4; &lt;; 1/4 with a linear N = &#x0398;(n) communication complexity. This improves over prior results [1]-[4] which each performed well in two of these measures. We also give results for other settings of interest, namely, the first computationally and communication efficient schemes that tolerate &#x03C1; &lt;; 2/7 adaptively, &#x03C1; &lt;; 1/3 if only one party is required to decode, and &#x03C1; &lt;; 1/2 if list decoding is allowed. These are the optimal tolerable error rates for the respective settings. These coding schemes also have near linear computational and communication complexity. These results are obtained via two techniques: We give a general black-box reduction which reduces unique decoding, in various settings, to list decoding. We also show how to boost the computational and communication efficiency of any list decoder to become near linear1.
[Protocols, telecommunication channels, Error-Rate, Error analysis, general black-box reduction, channel coding, optimal error rate, error correction code, communication complexity, maximum tolerable error rate, n-round interactive protocol, List-Decoding, interactive coding, Coding, Efficiency, list decoding, interactive systems, nonadaptive coding scheme, protocols, adversarial channel, error correction codes, linear computational complexity, Encoding, Decoding, Computational complexity, decoding, Interactive Communication, random codes, interactive communication, Error correction codes, linear communication complexity]
Outsourcing Private RAM Computation
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We construct the first schemes that allow a client to privately outsource arbitrary program executions to a remote server while ensuring that: (I) the client's work is small and essentially independent of the complexity of the computation being outsourced, and (II) the server's work is only proportional to the run-time of the computation on a random access machine (RAM), rather than its potentially much larger circuit size. Furthermore, our solutions are non-interactive and have the structure of reusable garbled RAM programs, addressing an open question of Lu and Ostrovsky (Eurocrypt 2013). We also construct schemes for an augmented variant of the above scenario, where the client can initially outsource a large private and persistent database to the server, and later outsource arbitrary program executions with read/write access to this database. Our solutions are built from non-reusable garbled RAM in conjunction with new types of reusable garbled circuits that are more efficient than prior solutions but only satisfy weaker security. For the basic setting without a persistent database, we can instantiate the required type of reusable garbled circuits from indistinguishability obfuscation or from functional encryption for circuits as a black-box. For the more complex setting with a persistent database, we can instantiate the required type of reusable garbled circuits using stronger notions of obfuscation. Our basic solution also requires the client to perform a one-time pre-processing step to garble a program at the cost of its RAM run-time, and we can avoid this cost using stronger notions of obfuscation. It remains an open problem to instantiate these new types of reusable garbled circuits under weaker assumptions, possibly avoiding obfuscation altogether. We show several simple extensions of our results and techniques to achieve: efficiency proportional to the input-specific RAM run-time, verifiability of outsourced RAM computation, functional encryption for RAMs, and a candidate obfuscation for RAMs.
[outsourced RAM computation verifiability, Protocols, reusable garbled RAM programs, Random access memory, functional encryption, Complexity theory, Servers, Security, program compilers, random access machine, read-write access, input-specific RAM run-time, Databases, remote server, obfuscation, private RAM computation outsourcing, nonreusable garbled RAM, private arbitrary program execution outsourcing, reusable garbled RAM, cryptography, reusable garbled circuits, computation complexity, outsourcing, software reusability, one-time preprocessing step, Outsourcing]
Parallel Repetition from Fortification
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
The Parallel Repetition Theorem upper-bounds the value of a repeated (tensored) two prover game in terms of the value of the base game and the number of repetitions. In this work we give a simple transformation on games -- "fortification" -- and show that for fortified games, the value of the repeated game decreases perfectly exponentially with the number of repetitions, up to an arbitrarily small additive error. Our proof is combinatorial and short. As corollaries, we obtain: (1) Starting from a PCP Theorem with soundness error bounded away from 1, we get a PCP with arbitrarily small constant soundness error. In particular, starting with the combinatorial PCP of Dinur, we get a combinatorial PCP with low error. The latter can be used for hardness of approximation as in the work of H&#x00E5;stad. (2) Starting from the work of the author and Raz, we get a projection PCP theorem with the smallest soundness error known today. The theorem yields nearly a quadratic improvement in the size compared to previous work. We then discuss the problem of derandomizing parallel repetition, and the limitations of the fortification idea in this setting. We point out a connection between the problem of derandomizing parallel repetition and the problem of composition. This connection could shed light on the so-called Projection Games Conjecture, which asks for projection PCP with minimal error.
[parallel repetition theorem, Additives, combinatorial mathematics, soundness error, game theory, projection PCP theorem, hardness of approximation, fortified games, Approximation methods, projection game conjecture, Computer science, Games, combinatorial PCP, projection game, Silicon, Polynomials, Bipartite graph, PCP, fortification, repeated two prover game, parallel repetition]
Path Finding Methods for Linear Programming: Solving Linear Programs in &#x00D5;(vrank) Iterations and Faster Algorithms for Maximum Flow
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
In this paper, we present a new algorithm for '/ solving linear programs that requires only O&#x0303;(&#x221A;rank(A)L) iterations where A is the constraint matrix of a linear program with m constraints, n variables, and bit complexity L. Each iteration of our method consists of solving O&#x0303;(1) linear systems and additional nearly linear time computation. Our method improves upon the previous best iteration bounds by factor &#x03A9;&#x0303;((m/rank (A)))1/4) of for methods with polynomial time computable iterations and by &#x03A9;&#x0303;((m/rank (A))1/2) for methods which solve at most O&#x0303;(1) linear systems in each iteration each achieved over 20 years ago. Applying our techniques to the linear program formulation of maximum flow yields an O&#x0303;(|E| &#x221A;|V| log2 U) time algorithm for solving the maximum flow problem on directed graphs with |E| edges, |V| vertices, and capacity ratio U. This improves upon the previous fastest running time of O(|E| min{|E|1/2, |V|2/3} log (|V|2/|E|) log(U)) achieved over 15 years ago by Goldberg and Rao and improves upon the previous best running times for solving dense directed unit capacity graphs of O&#x0303;(|E| min{|E|1/2, |V|2/3}) achieved by Even and Tarjan over 35 years ago and a running time of O&#x0303;(|E|10/7) achieved recently by Madry.
[Linear systems, linear time computation, Laplace equations, path-finding methods, constraint matrix, interior point, bit complexity, Linear programming, linear program, linear programming, linear systems, Approximation methods, Standards, Convergence, matrix algebra, dense directed unit capacity graphs, linear program formulation, directed graphs, maximum flow, Polynomials, polynomial time, computational complexity]
Popular Conjectures Imply Strong Lower Bounds for Dynamic Problems
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We consider several well-studied problems in dynamic algorithms and prove that sufficient progress on any of them would imply a breakthrough on one of five major open problems in the theory of algorithms: 1) Is the 3SUM problem on n numbers in O(n2-&#x03B5;) time for some &#x03B5; &gt; 0? 2) Can one determine the satisfiability of a CNF formula on n variables and poly n clauses in O((2 - &#x03B5;)npoly n) time for some &#x03B5; &gt; 0? 3) Is the All Pairs Shortest Paths problem for graphs on n vertices in O(n3-&#x03B5;) time for some &#x03B5; &gt; 0? 4) Is there a linear time algorithm that detects whether a given graph contains a triangle? 5) Is there an O(n3-&#x03B5;) time combinatorial algorithm for n&#x00D7;n Boolean matrix multiplication? The problems we consider include dynamic versions of bipartite perfect matching, bipartite maximum weight matching, single source reachability, single source shortest paths, strong connectivity, subgraph connectivity, diameter approximation and some nongraph problems such as Pagh's problem defined in a recent paper by Patrascu[STOC 2010].
[3SUM, Heuristic algorithms, computability, diameter approximation, bipartite perfect matching, bipartite maximum weight matching, Runtime, linear time algorithm, satisfiability, subgraph connectivity, dynamic algorithms, all pairs shortest paths, Polynomials, O(n3-&#x03B5;) time combinatorial algorithm, Boolean matrix multiplication, single source reachability, single source shortest paths, approximation theory, poly n clauses, reachability analysis, O((2-&#x03B5;)n poly n) time, 3SUM problem, Image edge detection, Pagh's problem, Boolean algebra, lower bounds, Computer science, strong connectivity, matrix multiplication, Upper bound, CNF formula, Approximation algorithms, all pair shortest path problem, computational complexity]
Pre-reduction Graph Products: Hardnesses of Properly Learning DFAs and Approximating EDP on DAGs
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
The study of graph products is a major research topic and typically concerns the term f(G * H), e.g., to show that f(G * H) = f(G)f(H). In this paper, we study graph products in a non-standard form f(R[G * H]) where R is a &#x201C;reduction&#x201D;, a transformation of any graph into an instance of an intended optimization problem. We resolve some open problems as applications. The first problem is minimum consistent deterministic finite automaton (DFA). We show a tight n1-&#x03F5; approximation hardness, improving the n1/14-&#x03F5; hardness of [Pitt and Warmuth, STOC 1989 and JACM 1993], where n is the sample size. (In fact, we also give improved hardnesses for the case of acyclic DFA and NFA.) Due to Board and Pitt [Theoretical Computer Science 1992], this implies the hardness of properly learning DFAs assuming NP &#x2260; RP (the weakest possible assumption). This affirmatively answers an open problem raised 25 years ago in the paper of Pitt and Warmuth and the survey of Pitt [All 1989]. Prior to our results, this hardness only follows from the stronger hardness of improperly learning DFAs, which requires stronger assumptions, i.e., either a cryptographic or an average case complexity assumption [Kearns and Valiant STOC 1989 and J. ACM 1994; Daniely et al. STOC 2014]. The second problem is edge-disjoint paths (EDP) on directed acyclic graphs (DAGs). This problem admits an O(&#x221A;n)-approximation algorithm [Chekuri, Khanna, and Shepherd, Theory of Computing 2006] and a matching &#x03A9;(&#x221A;n) integrality gap, but so far only an n1/26-&#x03F5; hardness factor is known [Chuzhoy et al., STOC 2007]. (n denotes the number of vertices.) Our techniques give a tight n1/2-&#x03F5; hardness for EDP on DAGs, thus resolving its approximability status. As by-products of our techniques: (i) We give a tight hardness of packing vertex-disjoint k-cycles for large k, complimenting [Guruswami and Lee, ECCC 2014] and matching [Krivelevich et al., SODA 2005 and ACM Transactions on Algorithms 2007]. (ii) We give an alternative (and perhaps simpler) proof for the hardness of properly learning DNF, CNF and intersection of halfspaces [Alekhnovich et al., FOCS 2004 and J. Comput.Syst. Sci. 2008]. Our new concept reduces the task of proving hardnesses to merely analyzing graph product inequalities, which are often as simple as textbook exercises. This concept was inspired by, and can be viewed as a generalization of, the graph product subadditivity technique we previously introduced in SODA 2013. This more general concept might be useful in proving other hardness results as well.
[learning automata, packing vertex-disjoint k-cycles, n1/2-&#x03F5; hardness, n1/14-&#x03F5; hardness improvement, optimization problem, n1/26-&#x03F5; hardness factor, tight n1-&#x03F5;-approximation hardness, acyclic NFA, Complexity theory, Approximation methods, O(&#x221A;n)-approximation algorithm, EDP approximation, optimisation, edge-disjoint path problem, deterministic automata, reduction method, &#x03A9;(&#x221A;n) integrality gap, graph vertices, directed acyclic graphs, Cryptography, approximation theory, graph product inequality analysis, properly learning DFA, Educational institutions, DAG, tight hardness, improperly learning DFA, Upper bound, acyclic DFA, graph transformation, directed graphs, generalization, Automata, Approximation algorithms, minimum consistent deterministic finite automaton problem, halfspace intersection, graph product subadditivity technique, computational complexity, prereduction graph products]
Preventing False Discovery in Interactive Data Analysis Is Hard
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We show that, under a standard hardness assumption, there is no computationally efficient algorithm that given n samples from an unknown distribution can give valid answers to n3+o(1) adaptively chosen statistical queries. A statistical query asks for the expectation of a predicate over the underlying distribution, and an answer to a statistical query is valid if it is "close" to the correct expectation over the distribution. Our result stands in stark contrast to the well known fact that exponentially many statistical queries can be answered validly and efficiently if the queries are chosen non-adaptively (no query may depend on the answers to previous queries). Moreover, Dwork et al. [1], showed how to accurately answer exponentially many adaptively chosen statistical queries via a computationally inefficient algorithm. They also gave efficient algorithm that can answer nearly n2 adaptively chosen queries, which shows our result is almost quantitatively tight. Conceptually, our result demonstrates that achieving statistical validity alone can be a source of computational intractability in adaptive settings. For example, in the modern large collaborative research environment, data analysts typically choose a particular approach based on previous findings. False discovery occurs if a research finding is supported by the data but not by the underlying distribution. While the study of preventing false discovery in Statistics is decades old, to the best of our knowledge our result is the first to demonstrate a computational barrier. In particular, our result suggests that the perceived difficulty of preventing false discovery in today's collaborative research environment may be inherent.
[Algorithm design and analysis, Adaptation models, data analysis, statistical queries, Encryption, Standards, query processing, Privacy, Accuracy, Polynomials, interactive data analysis, computational intractability, false discovery prevention, statistics]
Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Convex empirical risk minimization is a basic tool in machine learning and statistics. We provide new algorithms and matching lower bounds for differentially private convex empirical risk minimization assuming only that each data point's contribution to the loss function is Lipschitz and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex. Our algorithms run in polynomial time, and in some cases even match the optimal nonprivate running time (as measured by oracle complexity). We give separate algorithms (and lower bounds) for (&#x03B5;, 0)and (&#x03B5;, &#x03B4;)-differential privacy; perhaps surprisingly, the techniques used for designing optimal algorithms in the two cases are completely different. Our lower bounds apply even to very simple, smooth function families, such as linear and quadratic functions. This implies that algorithms from previous work can be used to obtain optimal error rates, under the additional assumption that the contributions of each data point to the loss function is smooth. We show that simple approaches to smoothing arbitrary loss functions (in order to apply previous techniques) do not yield optimal error rates. In particular, optimal algorithms were not previously known for problems such as training support vector machines and the high-dimensional median.
[Algorithm design and analysis, optimal nonprivate running time, smooth function families, (&#x03B5;, convex programming, Noise measurement, machine learning, Optimization, 0)-differential privacy, Support vector machines, Privacy, Lipschitz loss function, &#x03B4;)-differential privacy, private convex empirical risk minimization, oracle complexity, Convex functions, polynomial time, Risk management, learning (artificial intelligence), minimisation, arbitrary loss function smoothing, computational complexity, statistics]
Quantum Attacks on Classical Proof Systems: The Hardness of Quantum Rewinding
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Quantum zero-knowledge proofs and quantum proofs of knowledge are inherently difficult to analyze because their security analysis uses rewinding. Certain cases of quantum rewinding are handled by the results by Watrous (SIAM J Comput, 2009) and Unruh (Eurocrypt 2012), yet in general the problem remains elusive. We show that this is not only due to a lack of proof techniques: relative to an oracle, we show that classically secure proofs and proofs of knowledge are insecure in the quantum setting. More specifically, sigma-protocols, the Fiat-Shamir construction, and Fischlin's proof system are quantum insecure under assumptions that are sufficient for classical security. Additionally, we show that for similar reasons, computationally binding commitments provide almost no security guarantees in a quantum setting. To show these results, we develop the "pick-one trick\
[Protocols, cryptographic protocols, Computational modeling, quantum query complexity, Fiat-Shamir construction, classical proof systems, Registers, quantum attacks, quantum zero-knowledge proofs, random oracles, quantum rewinding, Quantum computing, Quantum mechanics, quantum computing, sigma protocol, quantum cryptography, rewinding, Cryptography, Fischlin proof system]
Ramanujan Complexes and Bounded Degree Topological Expanders
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Expander graphs have been a focus of attention in computer science in the last four decades. In recent years a high dimensional theory of expanders is emerging. There are several possible generalizations of the theory of expansion to simplicial complexes, among them stand out coboundary expansion and topological expanders. It is known that for every d there are unbounded degree simplicial complexes of dimension d with these properties. However, a major open problem, formulated by Gromov, is whether bounded degree high dimensional expanders, according to these definitions, exist for d &amp;ge; 2. We present an explicit construction of bounded degree complexes of dimension d = 2 which are high dimensional expanders. More precisely, our main result says that the 2-skeletons of the 3-dimensional Ramanujan complexes are topological expanders. Assuming a conjecture of Serre on the congruence subgroup property, infinitely many of them are also coboundary expanders.
[Measurement, topological overlapping, Buildings, graph theory, high dimensional theory, Lattices, topological expanders, Educational institutions, Graph theory, Stress, congruence subgroup property, Computer science, group theory, high dimensional expanders, Ramanujan complexes, computer science, bounded degree topological expanders, coboundary expansion, bounded degree complexes, bounded degree high dimensional expanders, 3-dimensional Ramanujan complexes, expander graphs]
Random Walks That Find Perfect Objects and the Lovasz Local Lemma
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We give an algorithmic local lemma by establishing a sufficient condition for the uniform random walk on a directed graph to reach a sink quickly. Our work is inspired by Moser's entropic method proof of the Lovasz Local Lemma (LLL) for satisfiability and completely bypasses the Probabilistic Method formulation of the LLL. In particular, our method works when the underlying state space is entirely unstructured. Similarly to Moser's argument, the key point is that algorithmic progress is measured in terms of entropy rather than energy (number of violated constraints) so that termination can be established even under the proliferation of states in which every step of the algorithm (random walk) increases the total number of violated constraints.
[algorithmic local lemma, random processes, computability, Probabilistic logic, Educational institutions, LLL, uniform random walk, Computer science, algorithmic progress, Lov&#x03B5;&#x0301;sz Local Lemma, Entropic Method, Image color analysis, entropy, Moser's entropic method proof, directed graphs, satisfiability, Lovasz Local Lemma, directed graph, Markov processes, Random Walks, Trajectory, theorem proving, Artificial intelligence]
Randomized Mutual Exclusion with Constant Amortized RMR Complexity on the DSM
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
In this paper we settle an open question by determining the remote memory reference (RMR) complexity of randomized mutual exclusion, on the distributed shared memory model (DSM) with atomic registers, in a weak but natural (and stronger than oblivious) adversary model. In particular, we present a mutual exclusion algorithm that has constant expected amortized RMR complexity and is deterministically deadlock free. Prior to this work, no randomized algorithm with o(log n/log log n) RMR complexity was known for the DSM model. Our algorithm is fairly simple, and compares favorably with one by Bender and Gilbert (FOCS 2011) for the CC model, which has expected amortized RMR complexity O(log2 log n) and provides only probabilistic deadlock freedom.
[Schedules, Adaptation models, distributed shared memory model, mutual exclusion, Registers, Complexity theory, deterministically deadlock free algorithm, CC model, DSM model, weak-natural adversary model, DSM, O(log n/log log n) RMR complexity, Nominations and elections, oblivious adversary, randomised algorithms, atomic registers, concurrency control, RMR complexity, System recovery, distributed shared memory systems, constant expected amortized RMR complexity, remote memory reference complexity, Arrays, randomized mutual exclusion, stronger-oblivious adversary model, shared memory, computational complexity, O(log2 log n) RMR complexity]
Sample-Optimal Fourier Sampling in Any Constant Dimension
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We give an algorithm for &#x2113;<sub>2</sub>/&#x2113;<sub>2</sub> sparse recovery from Fourier measurements using O(k log N) samples, matching the lower bound of Do Ba-Indyk-Price-Woodruff'10 for non-adaptive algorithms up to constant factors for any k &#x2264; N1-&#x03B4;. The algorithm runs in O&#x0303;(N) time. Our algorithm extends to higher dimensions, leading to sample complexity of O&#x0303;<sub>d</sub>(k log N), which is optimal up to constant factors for any d = O(1). These are the first sample optimal algorithms for these problems. A preliminary experimental evaluation indicates that our algorithm has empirical sampling complexity comparable to that of other recovery methods known in the literature, while providing strong provable guarantees on the recovery quality.
[Algorithm design and analysis, complexity, Fourier transforms, sparse Fourier Transform, compressed sensing, Fourier measurements, Discrete Fourier transforms, sparse recovery, Complexity theory, Approximation methods, sample complexity, empirical sampling complexity, sample-optimal Fourier sampling, Approximation algorithms, &#x2113;2/&#x2113;2 sparse recovery, sample optimal algorithms, Compressed sensing, computational complexity]
Satisfiability and Evolution
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We show that, if truth assignments on n variables reproduce through recombination so that satisfaction of a particular Boolean function confers a small evolutionary advantage, then a polynomially large population over polynomially many generations (polynomial in n and the inverse of the initial satisfaction probability) will end up almost certainly consisting exclusively of satisfying truth assignments. We argue that this theorem sheds light on the problem of the evolution of complex adaptations.
[algorithms, computability, Educational institutions, Boolean function, evolution, Statistics, Standards, Couplings, Boolean functions, Sociology, satisfiability, Genetics, truth assignments]
SelfishMigrate: A Scalable Algorithm for Non-clairvoyantly Scheduling Heterogeneous Processors
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We consider the classical problem of minimizing the total weighted flow-time for unrelated machines in the online non-clairvoyant setting. In this problem, a set of jobs J arrive over time to be scheduled on a set of M machines. Each job j has processing length pj, weight wj, and is processed at a rate of &#x2113;<sub>ij</sub> when scheduled on machine i. The online scheduler knows the values of wj and &#x2113;<sub>ij</sub> upon arrival of the job, but is not aware of the quantity pj. We present the first online algorithm that is scalable ((1 + &#x03F5;)-speed O(1/2)-competitive for any constant &#x03F5; &gt; 0) for the total weighted flow-time objective. No non-trivial results were known for this setting, except for the most basic case of identical machines. Our result resolves a major open problem in online scheduling theory. Moreover, we also show that no job needs more than a logarithmic number of migrations. We further extend our result and give a scalable algorithm for the objective of minimizing total weighted flow-time plus energy cost for the case of unrelated machines. In this problem, each machine can be sped up by a factor of f<sub>4</sub>-1 i (P) when consuming power P, wherefi is an arbitrary strictly convex power function. In particular, we get an O(&#x03B3;2)-competitive algorithm when all power functions are of form s&#x03B3;. These are the first non-trivial non-clairvoyant results in any setting with heterogeneous machines. The key algorithmic idea is to let jobs migrate selfishly until they converge to an equilibrium. Towards this end, we define a game where each job's utility which is closely tied to the instantaneous increase in the objective the job is responsible for, and each machine declares a policy that assigns priorities to jobs based on when they migrate to it, and the execution speeds. This has a spirit similar to coordination mechanisms that attempt to achieve near optimum welfare in the presence of selfish agents (jobs). To the best our knowledge, this is the first work that demonstrates the usefulness of ideas from coordination mechanisms and Nash equilibria for designing and analyzing online algorithms.
[Algorithm design and analysis, processor scheduling, non-clairvoyance, Program processors, power aware computing, Nash equilibria, execution speeds, Single machine scheduling, nonclairvoyantly scheduling, energy, arbitrary strictly convex consuming power, Flow-time, game theory, O(&#x03B3;2)-competitive algorithm, scalable algorithm, selfishmigrate, unrelated machines, logarithmic number, heterogeneous processors, Processor scheduling, total weighted flow-time plus energy cost, migrations, Games, Online Scheduling, optimum welfare, Delays, computational complexity, online scheduling theory]
Settling the APX-Hardness Status for Geometric Set Cover
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Weighted geometric set-cover problems arise naturally in several geometric and non-geometric settings (e.g. the breakthrough of Bansal and Pruhs (FOCS 2010) reduces a wide class of machine scheduling problems to weighted geometric set-cover). More than two decades of research has succeeded in settling the (1+&#x2208;)-approximability status for most geometric set-cover problems, except for four basic scenarios which are still lacking. One is that of weighted disks in the plane for which, after a series of papers, Varadarajan (STOC 2010) presented a clever quasi-sampling technique, which together with improvements by Chan et al(SODA 2012), yielded a O(1)-approximation algorithm. Even for the unweighted case, a PTAS for a fundamental class of objects called pseudodisks (which includes disks, unit-height rectangles, translates of convex sets etc.) is currently unknown. Another fundamental case is weighted halfspaces in R3, for which a PTAS is currently lacking. In this paper, we present a QPTAS for all of these remaining problems. Our results are based on the separator framework of Adamaszek and Wiese (FOCS 2013, SODA 2014), who recently obtained a QPTAS for weighted independent set of polygonal regions. This rules out the possibility that these problems are APX-hard, assuming NP DTIME(2polylog(n)). Together with the recent work of Chan-Grant (CGTA 2014), this settles the APX-hardness status for all natural geometric set-cover problems.
[approximation theory, Particle separators, k-admissible regions, Optimized production technology, weighted halfspaces, polygonal regions, weighted independent set, Complexity theory, set theory, Approximation methods, clever quasisampling technique, O(1)-approximation algorithm, Computer science, (1+&#x2208;)-approximability status, Pseudodisks, Hitting Sets, Quasi PTAS, weighted geometric set-cover problems, Approximation algorithms, pseudodisks, geometry, QPTAS, computational complexity, unit-height rectangles]
Shrinkage of De Morgan Formulae by Spectral Techniques
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We give a new and improved proof that the shrinkage exponent of De Morgan formulae is 2. Namely, we show that for any Boolean function f : {0, 1}n &#x2192; {0, 1}, setting each variable out of x1,.. . , xn with probability 1 - p to a randomly chosen constant, reduces the expected formula size of the function by a factor of O(p2). This result is tight and improves the work of Hastad [SIAM J. C., 1998] by removing logarithmic factors. As a consequence of our results, the function defined by Andreev [MUMB., 1987], A : {0, 1}n &#x2192; {0, 1}, which is in P, has formula size at least &#x03A9;(n3/log2 n log3 log n). This lower bound is tight (for the function A) up to the log3 log n factor, and is the best known lower bound for functions in P. In addition, we strengthen the average-case hardness result of Komargodski et al.; we show that the functions defined by Komargodski et al., h<sub>r</sub> : {0, 1}n &#x2192; {0, 1}, which are also in P, cannot be computed correctly on a fraction greater than 1/2 + 2-r of the inputs, by De n3 Morgan formulae of size at most n3/r2poly logn, for any parameter r &#x2264; n1/3. The proof relies on a result from quantum query complexity by Laplante et al. [CC, 2006], H&#x00F8;yer et al. [STOC, 2007] and Reichardt [SODA, 2011]: for any '/' Boolean function f, Q<sub>2</sub>(f) &#x2264; O( L(f)), where Q<sub>2</sub>(f) is the bounded-error quantum query complexity of f, and L(f) is the minimal size De Morgan formula computing f.
[De Morgan formulas, spectral techniques, probability, Switches, average-case hardness, Boolean function, Fourier analysis, Complexity theory, tight lower bound, Approximation methods, formula size reduction, query processing, bounded-error quantum query complexity, Boolean functions, quantum computing, Logic gates, De Morgan formula shrinkage exponent, logarithmic factor removal, Polynomials, random restrictions, log3 log n factor, shrinkage, computational complexity]
Single Pass Spectral Sparsification in Dynamic Streams
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We present the first single pass algorithm for computing spectral sparsifiers of graphs in the dynamic semi-streaming model. Given a single pass over a stream containing insertions and deletions of edges to a graph, G, our algorithm maintains a randomized linear sketch of the incidence matrix into dimension O(1/&#x2208;2npolylog(n)). Using this sketch, the algorithm can output a (1&#x00B1;&#x2208;) spectral sparsifier for G with high probability. While O(1/&#x2208;2n polylog(n)) space algorithms are known for computing cut sparsifiers in dynamic streams [1], [2] and spectral sparsifiers in insertion-only streams [3], prior to our work, the best known single pass algorithm for maintaining spectral sparsifiers in dynamic streams required sketches of dimension &#x03A9;(1/&#x2208;2n5/3). To achieve our result, we show that, using a coarse sparsifier of G and a linear sketch of G's incidence matrix, it is possible to sample edges by effective resistance, obtaining a spectral sparsifier of arbitrary precision. Sampling from the sketch requires a novel application of &#x2113;<sub>2</sub>/&#x2113;<sub>2</sub> sparse recovery, a natural extension of the &#x2113;<sub>0</sub> methods used for cut sparsifiers in [1]. Recent work of [2] on row sampling for matrix approximation gives a recursive approach for obtaining the required coarse sparsifiers. Under certain restrictions, our approach also extends to the problem of maintaining a spectral approximation for a general matrix AT A given a stream of updates to rows in A.
[single pass algorithm, Heuristic algorithms, graph theory, cut sparsifiers, Approximation methods, Sparse matrices, &#x2113;<sub>2</sub>/&#x2113;<sub>2</sub> sparse recovery, graph spectral sparsifiers, insertion-only streams, streaming, dimensionality reduction, sparsification, dynamic semistreaming model, single pass spectral sparsification, Laplace equations, sampling methods, stream containing insertions, Computational modeling, spectral approximation, probability, randomized linear sketch, sparse recovery, matrix approximation, matrix algebra, randomised algorithms, Resistance, sketching, space algorithms, incidence matrix, edge deletions, Approximation algorithms, computational complexity]
Solving Optimization Problems with Diseconomies of Scale via Decoupling
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We present a new framework for solving optimization problems with a diseconomy of scale. In such problems, our goal is to minimize the cost of resources used to perform a certain task. The cost of resources grows superlinearly, as xq, q &#x2265; 1, with the amount x of resources used. We define a novel linear programming relaxation for such problems, and then show that the integrality gap of the relaxation is A<sub>q</sub>, where A<sub>q</sub> is the q-th moment of the Poisson random variable with parameter 1. Using our framework, we obtain approximation algorithms for the Minimum Energy Efficient Routing, Minimum Degree Balanced Spanning Tree, Load Balancing on Unrelated Parallel Machines, and Unrelated Parallel Machine Scheduling with Nonlinear Functions of Completion Times problems. Our analysis relies on the decoupling inequality for nonnegative random variables. The inequality states that ||&#x03A3;<sub>i=1</sub>nX<sub>i</sub>||<sub>q</sub> &#x2264; Cq ||&#x03A3;<sub>i=1</sub>n Y<sub>i</sub>||<sub>q</sub>, where Xi are independent nonnegative random variables, Yi are possibly dependent nonnegative random variable, and each Y<sub>i</sub> has the same distribution as X<sub>i</sub>. The inequality was proved by de la Pen&#x0303;a in 1990. However, the optimal constant Cq was not known. We show that the optimal constant is C<sub>q</sub> = A<sub>q</sub>1/q.
[resource cost minimization, nonlinear functions, load balancing, decoupling inequality, diseconomy of scale, integrality gap, linear programming, approximation algorithms, Approximation methods, parallel machines, minimum degree balanced spanning tree, Optimization, processor scheduling, unrelated parallel machine scheduling, completion time problems, resource allocation, relaxation theory, linear programming relaxation, Polynomials, IP networks, stochastic processes, minimum energy efficient routing, nonnegative random variables, approximation theory, Poisson random variable, trees (mathematics), random processes, Linear programming, optimal constant, Approximation algorithms, Random variables]
Spectral Approaches to Nearest Neighbor Search
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We study spectral algorithms for the high-dimensional Nearest Neighbor Search problem (NNS). In particular, we consider a semi-random setting where a dataset is chosen arbitrarily from an unknown subspace of low dimension, and then perturbed by full-dimensional Gaussian noise. We design spectral NNS algorithms whose query time depends polynomially on the dimension and logarithmically on the size of the point set. These spectral algorithms use a repeated computation of the top PCA vector/subspace, and are effective even when the random-noise magnitude is much larger than the interpoint distances. Our motivation is that in practice, a number of spectral NNS algorithms outperform the random-projection methods that seem otherwise theoretically optimal on worst-case datasets. In this paper we aim to provide theoretical justification for this disparity. The full version of this extended abstract is available on arXiv.
[Algorithm design and analysis, Nearest neighbor search, worst-case datasets, high-dimensional NNS problem, random-noise magnitude, Noise, spectral NNS algorithms, Data structures, Vectors, Partitioning algorithms, full-dimensional Gaussian noise, spectral algorithms, Nearest neighbor searches, vectors, Gaussian noise, high-dimensional nearest neighbor search problem, data handling, principal component analysis, Principal component analysis, PCA vector]
The Communication Complexity of Distributed epsilon-Approximations
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Data summarization is an effective approach to dealing with the "big data" problem. While data summarization problems traditionally have been studied is the streaming model, the focus is starting to shift to distributed models, as distributed/parallel computation seems to be the only viable way to handle today's massive data sets. In this paper, we study &amp;epsi;-approximations, a classical data summary that, intuitively speaking, preserves approximately the density of the underlying data set over a certain range space. We consider the problem of computing &amp;epsi;-approximations for a data set which is held jointly by k players, and give general communication upper and lower bounds that hold for any range space whose discrepancy is known.
[parallel computation, Protocols, general communication upper bounds, range space, Complexity theory, distributed &#x03B5;-approximations, Approximation methods, communication complexity, data set density, Distributed databases, &amp;amp;epsi;-approximations, data summary, data summarization problems, message passing, massive data set handling, Computational modeling, general communication lower bounds, Big Data, distributed computation, Standards, distributed models, discrepancy, Data models, distributed data, Big Data problem]
The Complexity of Counting Edge Colorings and a Dichotomy for Some Higher Domain Holant Problems
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We show that an effective version of Siegel's Theorem on finiteness of integer solutions for a specific algebraic curve and an application of elementary Galois theory are key ingredients in a complexity classification of some Holant problems. These Holant problems, denoted by Holant(f), are defined by a symmetric ternary function f that is invariant under any permutation of the &#x03BA; &#x2265; 3 domain elements. We prove that Holant(f) exhibits a complexity dichotomy. The hardness, and thus the dichotomy, holds even when restricted to planar graphs. A special case of this result is that counting edge &#x03BA;-colorings is #P-hard over planar 3-regular multigraphs for all &#x03BA; &#x2265; 3. In fact, we prove that counting edge &#x03BA;-colorings is #P-hard over planar r-regular multigraphs for all &#x03BA; &#x2265; r &#x2265; 3. The problem is polynomial-time computable in all other parameter settings. The proof of the dichotomy theorem for Holant(f) depends on the fact that a specific polynomial p(x, y) has an explicitly listed finite set of integer solutions, and the determination of the Galois groups of some specific polynomials. In the process, we also encounter the Tutte polynomial, medial graphs, Eulerian partitions, Puiseux series, and a certain lattice condition on the (logarithm of) the roots of polynomials.
[Transmission line matrix methods, Tutte polynomial, edge coloring, Lattices, planar graphs, Complexity theory, algebraic curve, graph colouring, polynomial roots, Siegel theorem, Polynomials, Eigenvalues and eigenfunctions, symmetric ternary function, dichotomy theorem, elementary Galois theory, Color, complexity dichotomy, counting edge coloring complexity, Eulerian partitions, counting problems, Galois fields, Holant problems, Interpolation, higher domain Holant problems, medial graphs, Puiseux series, computational complexity]
The Dyck Language Edit Distance Problem in Near-Linear Time
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Given a string &#x03C3; over alphabet &#x03A3; and a grammar G defined over the same alphabet, how many minimum number of repairs (insertions, deletions and substitutions) are required to map &#x03C3; into a valid member of G? The seminal work of Aho and Peterson in 1972 initiated the study of this language edit distance problem providing a dynamic programming algorithm for context free languages that runs in O(|G|2n3) time, where n is the string length and G is the grammar size. While later improvements reduced the running time to O(G n3), the cubic time complexity on the input length held a major bottleneck for applying these algorithms to their multitude of applications. In this paper, we study the language edit distance problem for a fundamental context free language, DYCK(s) representing the language of well-balanced parentheses of s different types, that has been pivotal in the development of formal language theory. We provide the very first near-linear time algorithm to tightly approximate the DYCK(s) language edit distance problem for any arbitrary s. DYCK(s) language edit distance significantly generalizes the well-studied string edit distance problem, and appears in most applications of language edit distance ranging from data quality in databases, generating automated error-correcting parsers in compiler optimization to structure prediction problems in biological sequences. Its nondeterministic counterpart is known as the hardest context free language. Our main result is an algorithm for edit distance computation to DYCK(s) for any positive integer s that runs in O(n1+&#x03F5; polylog(n)) time and achieves an approximation factor of O(1/&#x03F5;&#x03B2;(n) log |OPT|), for any &#x03F5; &gt; 0. Here OPT is the optimal edit distance to DYCK(s) and &#x03B2;(n) is the best approximation factor known for the simpler problem of string edit distance running in analogous time. If we allow O(n1+&#x03F5; + |OPT|2n&#x03F5;) time, then the approximation factor can be reduced to O(1/&#x03F5; log |OPT|). Since the best known near-linear time algorithm for the string edit distance problem has &#x03B2;(n) = polylog(n), under near-linear time computation model both DYCK(s) language and string edit distance problems have polylog(n) approximation factors. This comes as a surprise since the former is a significant generalization of the latter and their exact computations via dynamic programming show a stark difference in time complexity. Rather less surprisingly, we show that the framework for efficiently approximating edit distance to DYCK(s) can be utilized for many other languages. We illustrate this by considering various memory checking languages (studied extensively under distributed verification) such as STACK, QUEUE, PQ and DEQUE which comprise of valid transcripts of stacks, queues, priority queues and double-ended queues respectively. Therefore, any language that can be recognized by these data structures, can also be repaired efficiently by our algorithm.
[Algorithm design and analysis, positive integer, Heuristic algorithms, string edit distance problem, context free languages, approximation algorithms, Approximation methods, program compilers, dynamic programming algorithm, near-linear time algorithm, context-free grammars, data structures, optimal edit distance, Dynamic programming, compiler optimization, formal language theory, error correction, Context, approximation factors, edit distance, formal languages, string edit distance problems, dynamic programming, time complexity, near-linear time computation model, cubic time complexity, Grammar, Dyck language edit distance problem, memory checking languages, biological sequences, data quality, Approximation algorithms, double-ended queues, grammar size, string length, linear time algorithm design, computational complexity, automated error-correcting parsers, formal language]
Threesomes, Degenerates, and Love Triangles
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
The 3SUM problem is to decide, given a set of n real numbers, whether any three sum to zero. It is widely conjectured that a trivial O(n2)-time algorithm is optimal and over the years the consequences of this conjecture have been revealed. This 3SUM conjecture implies &#x03A9;(n2) lower bounds on numerous problems in computational geometry and a variant of the conjecture implies strong lower bounds on triangle enumeration, dynamic graph algorithms, and string matching data structures. In this paper we refute the 3SUM conjecture. We prove that the decision tree complexity of 3SUM is O(n3/2 &#x221A;/log n) and give two subquadratic 3SUM algorithms, a deterministic one running in O(n2/(log n/ log log n)2/3) time and a randomized one running in O(n2 (log log n)2/ log n) time with high probability. Our results lead directly to improved bounds for k-variate linear degeneracy testing for all odd k &#x2265; 3. The problem is to decide, given a linear function f(x<sub>1</sub>, ... , x<sub>k</sub>) = &#x03B1;<sub>0</sub> + &#x03A3;<sub>1&#x2264;1&#x2264;k</sub> &#x03B1;<sub>i</sub>x<sub>i</sub> and a set A &#x2282; &#x211D;, whether 0 &#x2208; f(Ak). We show the decision tree complexity of this problem is O(nk/2 &#x221A;log n). Finally, we give a subcubic algorithm for a generalization of the (min, +)-product over real-valued matrices and apply it to the problem of finding zero-weight triangles in weighted graphs. We give a depth-O(n5/2 &#x221A;log n) decision tree for this problem, as well as an algorithm running in time O(n3(log log n)2/ log n).
[triangle enumeration, subquadratic 3SUM algorithms, decision tree complexity, depth-O(n5/2 &#x221A;log n) decision tree, Heuristic algorithms, graph theory, Random access memory, O(n2/(log n/ log log n)2/3) time, Complexity theory, 3SUM conjecture, weighted graphs, linear function, real-valued matrices, data structures, Decision trees, 3sum, 3SUM problem, dynamic graph algorithms, probability, Standards, +)-product, Sorting, matrix algebra, trivial O(n2)-time algorithm, Computational geometry, string matching data structures, &#x03A9;(n2) lower bounds, decision trees, zero-weight triangles, k-variate linear degeneracy testing, (min, string matching, computational complexity]
Topology Matters in Communication
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We consider the communication cost of computing functions when inputs are distributed among the vertices of an undirected graph. The communication is assumed to be point-to-point: a processor sends messages only to its neighbors. The processors in the graph act according to a pre-determined protocol, which can be randomized and may err with some small probability. The communication cost of the protocol is the total number of bits exchanged in the worst case. Extending recent work that assumed that the graph was the complete graph (with unit edge lengths), we develop a methodology for showing lower bounds that are sensitive to the graph topology. In particular, for a broad class of graphs, we obtain a lower bound of the form &#x03A9;(k2n), for computing a function of k inputs, each of which is n-bits long and located at a different vertex. Previous works obtained lower bounds of the form &#x03A9;(k n). This methodology yields a variety of other results including the following: A tight lower bound (ignoring poly-log factors) for Element Distinctness, settling a question of Phillips, Verbin and Zhang (SODA '12), a distributed XOR lemma, a lower bound for composed functions, settling a question of Phillips et al., new topology-dependent bounds for several natural graph problems considered by Woodruff and Zhang (DISC '13). To obtain these results we use tools from the theory of metric embeddings and represent the topological constraints imposed by the graph as a collection of cuts, each cut providing a setting where our understanding of two-party communication complexity can be effectively deployed.
[Measurement, Protocols, unit edge lengths, distributed computing functions, graph theory, Metric Embeddings, distributed processing, two-party communication complexity, Complexity theory, communication complexity, topological constraints, Network topology, distributed XOR lemma, undirected graph, graph topology, Computational modeling, element distinctness, communication cost, topology-dependent bounds, Topology, tight lower bound, Communication Complexity, predetermined protocol, Upper bound, Distributed Computing, complete graph, metric embeddings]
Total Space in Resolution
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
We show quadratic lower bounds on the total space used in resolution refutations of random k-CNFs over n variables, and of the graph pigeonhole principle and the bit pigeonhole principle for n holes. This answers the long-standing open problem of whether there are families of k-CNF formulas of polynomial size which require quadratic total space in resolution. The results follow from a more general theorem showing that, for formulas satisfying certain conditions, in every resolution refutation there is a memory configuration containing many clauses of large width.
[graph theory, quadratic total space, quadratic lower bounds, random processes, resolution refutations, random CNFs, memory configuration, conjunctive normal form, resolution, polynomial size, Computer science, bit pigeonhole principle, Turing machines, Semantics, Memory management, random k-CNF, total space, Polynomials, Bipartite graph, graph pigeonhole principle, computational complexity]
Understanding Alternating Minimization for Matrix Completion
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Alternating minimization is a widely used and empirically successful heuristic for matrix completion and related low-rank optimization problems. Theoretical guarantees for alternating minimization have been hard to come by and are still poorly understood. This is in part because the heuristic is iterative and non-convex in nature. We give a new algorithm based on alternating minimization that provably recovers an unknown low-rank matrix from a random subsample of its entries under a standard incoherence assumption. Our results reduce the sample size requirements of the alternating minimization approach by at least a quartic factor in the rank and the condition number of the unknown matrix. These improvements apply even if the matrix is only close to low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in the dimension of the matrix and, in a broad range of parameters, gives the strongest sample bounds among all subquadratic time algorithms that we are aware of. Underlying our work is a new robust convergence analysis of the well-known Power Method for computing the dominant singular vectors of a matrix. This viewpoint leads to a conceptually simple understanding of alternating minimization. In addition, we contribute a new technique for controlling the coherence of intermediate solutions arising in iterative algorithms based on a smoothed analysis of the QR factorization. These techniques may be of interest beyond their application here.
[Algorithm design and analysis, quartic factor, matrix completion, power method, convergence, sample size requirements, matrix decomposition, Convergence, subquadratic time algorithms, standard incoherence assumption, alternating minimization approach, robust convergence analysis, low-rank matrix, dominant singular vectors, random processes, Frobenius norm, Minimization, Vectors, Noise measurement, Standards, vectors, random subsample, Coherence, QR factorization, unknown matrix, minimisation, low-rank optimization problems, computational complexity]
Why Walking the Dog Takes Time: Frechet Distance Has No Strongly Subquadratic Algorithms Unless SETH Fails
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
The Fre&#x0301;chet distance is a well-studied and very popular measure of similarity of two curves. Many variants and extensions have been studied since Alt and Godau introduced this measure to computational geometry in 1991. Their original algorithm to compute the Fre&#x0301;chet distance of two polygonal curves with n vertices has a runtime of O(n^2 log n). More than 20 years later, the state of the art algorithms for most variants still take time more than O(n2 / log n), but no matching lower bounds are known, not even under reasonable complexity theoretic assumptions. To obtain a conditional lower bound, in this paper we assume the Strong Exponential Time Hypothesis or, more precisely, that there is no O*((2-&amp;delta;)N) algorithm for CNF-SAT for any delta &gt; 0. Under this assumption we show that the Fre&#x0301;chet distance cannot be computed in strongly subquadratic time, i.e., in time O(n2-&amp;delta;) for any delta &gt; 0. This means that finding faster algorithms for the Fre&#x0301;chet distance is as hard as finding faster CNF-SAT algorithms, and the existence of a strongly subquadratic algorithm can be considered unlikely. Our result holds for both the continuous and the discrete Fre&#x0301;chet distance. We extend the main result in various directions. Based on the same assumption we (1) show non-existence of a strongly subquadratic 1.001-approximation, (2) present tight lower bounds in case the numbers of vertices of the two curves are imbalanced, and (3) examine realistic input assumptions (c-packed curves).
[SETH, reasonable complexity theoretic assumptions, subquadratic algorithms, computational geometry, discrete Frechet distance, Vectors, Time measurement, inapproximability, Approximation methods, lower bounds, realistic input assumptions, polygonal curves, Runtime, curves, strong exponential time hypothesis, Bismuth, Approximation algorithms, Polynomials, computational complexity, CNF-SAT]
[Publisher's information]
2014 IEEE 55th Annual Symposium on Foundations of Computer Science
None
2014
Provides a listing of current committee members and society officers.
[]
Preface
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee and Sponsors
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Provides a listing of current committee members and society officers.
[]
Program Committee
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Provides a listing of current committee members and society officers.
[]
Approximating ATSP by Relaxing Connectivity
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
The standard LP relaxation of the asymmetric traveling salesman problem has been conjectured to have a constant integrality gap in the metric case. We prove this conjecture when restricted to shortest path metrics of node-weighted digraphs. Our arguments are constructive and give a constant factor approximation algorithm for these metrics. We remark that the considered case is more general than the directed analog of the special case of the symmetric traveling salesman problem for which there were recent improvements on Christofides' algorithm. The main idea of our approach is to first consider an easier problem obtained by significantly relaxing the general connectivity requirements into local connectivity conditions. For this relaxed problem, it is quite easy to give an algorithm with a guarantee of 3 on node-weighted shortest path metrics. More surprisingly, we then show that any algorithm (irrespective of the metric) for the relaxed problem can be turned into an algorithm for the asymmetric traveling salesman problem by only losing a small constant factor in the performance guarantee. This leaves open the intriguing task of designing a "good" algorithm for the relaxed problem on general metrics.
[Measurement, Algorithm design and analysis, node-weighted digraphs, approximation theory, LP relaxation, Christofide algorithm, Traveling salesman problems, any algorithm, shortest path metrics, approximation algorithms, Approximation methods, travelling salesman problems, combinatorial optimization, asymmetric traveling salesman problem, constant integrality gap, connectivity requirements, directed graphs, ATSP approximation, Cities and towns, Approximation algorithms, Polynomials, node-weighted shortest path metrics, computational complexity, constant factor approximation algorithm]
Effective-Resistance-Reducing Flows, Spectrally Thin Trees, and Asymmetric TSP
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show that the integrality gap of the natural LP relaxation of the Asymmetric Traveling Salesman Problem is polyloglog(n). In other words, there is a polynomial time algorithm that approximates the value of the optimum tour within a factor of polyloglog(n), where polyloglog(n) is a bounded degree polynomial of loglog(n). We prove this by showing that any k-edge-connected unweighted graph has a polyloglog(n)/k-thin spanning tree. Our main new ingredient is a procedure, albeit an exponentially sized convex program, that &#x201C;transforms&#x201D; graphs that do not admit any spectrally thin trees into those that provably have spectrally thin trees. More precisely, given a k-edge-connected graph G = (V, E) where k &#x2265; 7 log(n), we show that there is a matrix D that &#x201C;preserves&#x201D; the structure of all cuts of G such that for a set F &#x2286; E that induces an &#x03A9;(k)-edge-connected graph, the effective resistance of every edge in F w.r.t. D is at most polylog(k)/k. Then, we use our extension of the seminal work of Marcus, Spielman, and Srivastava [1], fully explained in [2], to prove the existence of a polylog(k)/k-spectrally thin tree with respect to D. Such a tree is polylog(k)/k-combinatorially thin with respect to G as D preserves the structure of cuts of G.
[Algorithm design and analysis, bounded degree polynomial, polyloglog(n)/k-thin spanning tree, integrality gap, asymmetric TSP, Approximation methods, effective-resistance-reducing flows, travelling salesman problems, polylog(k)/k-spectrally thin tree, Cost function, Kadison-Singer Problem, Polynomials, Asymmetric Traveling Salesman Problem, spectrally-thin trees, approximation theory, polylog(k)/k-combinatorially thin tree, trees (mathematics), Traveling salesman problems, k-edge-connected unweighted graph, natural LP relaxation, Thin Tree Conjecture, optimum tour value approximates, Approximation Algorithms, polynomial time algorithm, &#x03A9;(k)-edge-connected graph, Resistance, asymmetric traveling salesman problem, exponentially sized convex program, Approximation algorithms, computational complexity, Effective Resistance]
Compressing and Teaching for Low VC-Dimension
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
In this work we study the quantitative relation between VC-dimension and two other basic parameters related to learning and teaching. Namely, the quality of sample compression schemes and of teaching sets for classes of low VC-dimension. Let C be a binary concept class of size m and VC-dimension d. Prior to this work, the best known upper bounds for both parameters were log(m), while the best lower bounds are linear in d. We present significantly better upper bounds on both as follows. We construct sample compression schemes of size exp(d) for C. This resolves a question of Littlest one and Warmuth (1986). Roughly speaking, we show that given an arbitrary set of labeled examples from an unknown concept in C, one can retain only a subset of exp(d) of them, in a way that allows to recover the labels of all other examples in the set, using additional exp(d) information bits. We further show that there always exists a concept c in C with a teaching set (i.e. A list of c-labeled examples uniquely identifying c in C) of size exp(d) log log(m). This problem was studied by Kuhlmann (1999). Our construction also implies that the recursive teaching (RT) dimension of C is at most exp(d) log log(m) as well. The RT-dimension was suggested by Zilles et al. And Doliwa et al. (2010). The same notion (under the name partial-ID width) was independently studied by Wigderson and Yehuday off (2013). An upper bound on this parameter that depends only on d is known just for the very simple case d=1, and is open even for d=2. We also make small progress towards this seemingly modest goal.
[low VC-dimension, recursive teaching dimension, VC dimension, PAC learning, teaching, learning, Complexity theory, Electronic mail, set theory, Computer science, Geometry, Upper bound, Education, sample compression schemes, quantitative relation, recursive teaching, learning (artificial intelligence)]
On Monotonicity Testing and Boolean Isoperimetric Type Theorems
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show a directed and robust analogue of a boolean isoperimetric type theorem of Talagrand [13]. As an application, we give a monotonicity testing algorithm that makes O&#x0305;(&#x221A;n/&#x03B5;2) non-adaptive queries to a function f : {0, 1}n &#x2192; {0, 1}, always accepts a monotone function and rejects a function that is &#x03B5;-far from being monotone with constant probability.
[monotonicity testing algorithm, probability, Color, isoperimetry, Electronic mail, nonadaptive queries, property testing, Computer science, monotone functions, Boolean functions, Boolean isoperimetric type theorems, Hypercubes, Robustness, constant probability, Testing]
Tight Hardness Results for LCS and Other Sequence Similarity Measures
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Two important similarity measures between sequences are the longest common subsequence (LCS) and the dynamic time warping distance (DTWD). The computations of these measures for two given sequences are central tasks in a variety of applications. Simple dynamic programming algorithms solve these tasks in O(n2) time, and despite an extensive amount of research, no algorithms with significantly better worst case upper bounds are known. In this paper, we show that for any constant &#x03B5; &gt;0, an O(n2-&#x03B5;) time algorithm for computing the LCS or the DTWD of two sequences of length n over a constant size alphabet, refutes the popular Strong Exponential Time Hypothesis (SETH).
[O(n2) time algorithm, Heuristic algorithms, parameterized complexity, dynamic time warping distance, Biology, Complexity theory, sequences, sequence similarity measures, worst case upper bounds, Cost function, Dynamic programming, dynamic programming algorithms, SETH, sequence length, strong-exponential time hypothesis, constant size alphabet, sequence alignments, Time measurement, tight hardness, lower bounds, Computer science, longest common subsequence, O(n2-&#x03B5;) time algorithm, LCS, DTWD, computational complexity]
If the Current Clique Algorithms are Optimal, So is Valiant's Parser
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
The CFG recognition problem is: given a context-free grammar G and a string w of length n, decide if w can be obtained from G. This is the most basic parsing question and is a core computer science problem. Valiant's parser from 1975 solves the problem in O(nO) time, where ? &lt;; 2:373 is the matrix multiplication exponent. Dozens of parsing algorithms have been proposed over the years, yet Valiant's upper bound remains unbeaten. The best combinatorial algorithms have mildly subcubic O(n3= log3 n) complexity. Lee (JACM'01) provided evidence that fast matrix multiplication is needed for CFG parsing, and that very efficient and practical algorithms might be hard or even impossible to obtain. Lee showed that any algorithm for a more general parsing problem with running time O(|G| n3 -- e) can be converted into a surprising subcubic algorithm for Boolean Matrix Multiplication. Unfortunately, Lee' s hardness result required that the grammar size be |G| = O(n6). Nothing was known for the more relevant case of constant size grammars. In this work, we prove that any improvement on Valiant' s algorithm, even for constant size grammars, either in terms of runtime or by avoiding the inefficiencies of fast matrix multiplication, would imply a breakthrough algorithm for the k-Clique problem: given a graph on n nodes, decide if there are k that form a clique. Besides classifying the complexity of a fundamental problem, our reduction has led us to similar lower bounds for more modern and well-studied cubic time problems for which faster algorithms are highly desirable in practice: RNA Folding, a central problem in computational biology, and Dyck Language Edit Distance, answering an open question of Saha (FOCS'14).
[combinatorial algorithms, Heuristic algorithms, parsing question, graph theory, computer science problem, CFG recognition, RNA folding, parameterized complexity, Complexity theory, Classification algorithms, k-Clique problem, parsing, program compilers, Dyck Edit Distance, conditional lower bounds, clique algorithms, Runtime, computer science, CFG parsing, context-free grammars, Valiant parser, Boolean matrix multiplication, hardness in P, Valiant's algorithm, natural language processing, Grammar, Boolean algebra, tight hardness, graph, context-free grammar, Computer science, matrix multiplication, Upper bound, clique, Valiant algorithm, context free grammars, fast matrix multiplication]
Language Edit Distance and Maximum Likelihood Parsing of Stochastic Grammars: Faster Algorithms and Connection to Fundamental Graph Problems
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Given a context free language G over alphabet &#x03A3; and a string s &#x2208; &#x03A3;*, the language edit distance problem seeks the minimum number of edits (insertions, deletions and substitutions) required to convert s into a valid member of the language &#x00A3;(G). The well-known dynamic programming algorithm solves this problem in O(n3) time (ignoring grammar size) where n is the string length [Aho, Peterson 1972, Myers 1985]. Despite its numerous applications, to date there exists no algorithm that computes the exact or approximate language edit distance problem in true subcubic time. In this paper we give the first such algorithm that computes language edit distance almost optimally. For any arbitrary &#x03B5; &gt; 0, our algorithm runs in O&#x0303;(n&#x03C9;/poly(&#x2208;)) time and returns an estimate within a multiplicative approximation factor of (1 + &#x2208;) with high probability, where w is the exponent of ordinary matrix multiplication of n dimensional square matrices. It also computes the actual edits required. We further solve the local alignment problem; for all substrings of s, we can estimate their language edit distance within (1 &#x00B1; &#x03B5;) factor in O&#x0303;(n&#x03C9;/poly(&#x2208;)) time with high probability. Next, we design the very first subcubic (O&#x0303;(n&#x03C9;)) algorithm that given an arbitrary stochastic context free grammar, and a string returns a nearly-optimal maximum likelihood parsing of that string. Stochastic context free grammars significantly generalize hidden Markov models; they lie at the foundation of statistical natural language processing, and have found widespread applications in many other fields. To complement our upper bound result, we show that exact computation of maximum likelihood parsing of stochastic grammars or language edit distance in true subcubic time will imply a truly subcubic algorithm for all-pairs shortest paths, a long-standing open question. This will result in a breakthrough for a large range of problems in graphs and matrices due to subcubic equivalence. By a known lower bound result [Lee 2002], and a recent development [Abboud et al. 2015] even the much simpler problem of parsing a context free grammar requires &#x03A9;(n&#x03C9;) time. Therefore any nontrivial multiplicative approximation algorithms for either of the two problems in time o(n&#x03C9;) are unlikely to exist.
[multiplicative approximation factor, language edit distance problem, Stochastic processes, context free language, nearly-optimal maximum likelihood parsing, stochastic context free grammars, alphabet, approximation algorithms, Approximation methods, ordinary matrix multiplication, hidden Markov models, arbitrary stochastic context free grammar, graphs, context-free languages, context free grammar, dynamic programming algorithm, stochastic grammars, true subcubic time, subcubic algorithm, context-free grammars, dimensional square matrices, stochastic processes, Context, all-pairs shortest paths, approximation theory, edit distance, fundamental graph problems, local alignment problem, natural language processing, probability, statistical natural language processing, dynamic programming, Grammar, matrix multiplication, Hidden Markov models, context free grammar parsing, nontrivial multiplicative approximation algorithms, Approximation algorithms, Data models, fast algorithm, subcubic equivalence]
Probabilistic Polynomials and Hamming Nearest Neighbors
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show how to compute any symmetric Boolean function on n variables over any field (as well as '/ the integers) with a probabilistic polynomial of degree O( &#x221A;nlog(1/&#x03B5;)) and error at most &#x03B5;. The degree dependence on n and &#x03B5; is optimal, matching a lower bound of Razborov (1987) and Smolensky (1987) for the MAJORITY function. The proof is constructive: a low-degree polynomial can be efficiently sampled from the distribution. This polynomial construction is combined with other algebraic ideas to give the first subquadratic time algorithm for computing a (worst-case) batch of Hamming distances in superlogarithmic dimensions, exactly. To illustrate, let c(n) : &#x2115; &#x2192; &#x2115;. Suppose we are given a database D of n vectors in {0,1}c(n)logn and a collection of n query vectors Q in the same dimension. For all u &#x2208; Q, we wish to compute a v &#x2208; D with minimum Hamming distance from u. We solve this problem in n2-1/O(c(n)log2c(n)) randomized time. Hence, the problem is in &#x201C;truly subquadratic&#x201D; time for O(logn) dimensions, and in subquadratic time for d = o((log2 n)/(loglogn)2). We apply the algorithm to computing pairs with maximum inner product, closest pair in &#x2113;1 for vectors with bounded integer entries, and pairs with maximum Jaccard coefficients.
[probabilistic polynomial degree, majority function, truly-subquadratic time, O(logn) dimensions, Hamming nearest neighbors, superlogarithmic dimensions, Approximation methods, query processing, low-degree polynomial, n2-1/O(c(n)log2c(n)) randomized time, Boolean functions, Databases, Polynomials, minimum Hamming distance, subquadratic time algorithm, worst-case batch, maximum inner product, Hamming distance, maximum Jaccard coefficients, polynomials, probabilistic polynomials, probability, Probabilistic logic, nearest neighbors, lower bound, polynomial construction, query vectors, subquadratic time, degree dependence, d = o((log2 n)/(loglogn)2), bounded integer entries, symmetric Boolean function, Approximation algorithms, computational complexity]
Indistinguishability Obfuscation from the Multilinear Subgroup Elimination Assumption
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We revisit the question of constructing secure general-purpose indistinguishability obfuscation, with a security reduction based on explicit computational assumptions over multilinear maps. Previous to our work, such reductions were only known to exist based on meta-assumptions and/or ad-hoc assumptions: In the original constructive work of Garg et al. (FOCS 2013), the underlying explicit computational assumption encapsulated an exponential family of assumptions for each pair of circuits to be obfuscated. In the more recent work of Pass et al. (Crypto 2014), the underlying assumption is a meta-assumption that also encapsulates an exponential family of assumptions, and this meta-assumption is invoked in a manner that captures the specific pair of circuits to be obfuscated. The assumptions underlying both these works substantially capture (either explicitly or implicitly) the actual structure of the obfuscation mechanism itself. In our work, we provide the first construction of general-purpose indistinguishability obfuscation proven secure via a reduction to a natural computational assumption over multilinear maps, namely, the Multilinear Subgroup Elimination Assumption. This assumption does not depend on the circuits to be obfuscated (except for its size), and does not correspond to the underlying structure of our obfuscator. The technical heart of our paper is our reduction, which gives a new way to argue about the security of indistinguishability obfuscation.
[ad-hoc assumptions, Computational modeling, Buildings, security reduction, general-purpose indistinguishability obfuscation, cryptography, algebra, Encryption, Science - general, US Government, multilinear subgroup elimination assumption, multilinear maps, explicit computational assumptions]
Indistinguishability Obfuscation from Functional Encryption
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Indistinguishability obfuscation (IO) is a tremendous notion, powerful enough to give rise to almost any known cryptographic object. So far, candidate IO constructions were based on specific assumptions on algebraic objects called multi-linear graded encodings. We present a generic construction of indistinguishability obfuscation from public-key functional encryption with succinct cipher texts and sub-exponential security. This shows the equivalence of indistinguishability obfuscation and public-key functional encryption, a primitive that has so far seemed to be much weaker, lacking the power and the staggering range of applications of indistinguishability obfuscation. As an application, we obtain a new candidate IO construction based on the functional encryption scheme of Garg, Gentry, Halevi, and Zhan dry [Eprint 14] under their assumptions on multi-linear graded encodings. We also show that, under the Learning with Errors assumptions, our techniques imply that any indistinguishability obfuscator can be converted to one where obfuscated circuits are of linear size in the size of the original circuit plus a polynomial overhead in its depth. Our reduction highlights the importance of cipher text succinctness in functional encryption schemes, which we hope will serve as a pathway to new IO constructions based on solid cryptographic foundations.
[indistinguishability obfuscation, Encoding, algebra, functional encryption, Encryption, subexponential security, solid cryptographic foundations, public-key functional encryption, Standards, succinct cipher texts, public key cryptography, cryptographic object, multilinear graded encodings, Public key, Polynomials, algebraic objects, obfuscation, obfuscated circuits]
Limits on the Power of Indistinguishability Obfuscation and Functional Encryption
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Recent breakthroughs in cryptography have positioned indistinguishability obfuscation as a "central hub" for almost all known cryptographic tasks, and as an extremely powerful building block for new cryptographic tasks resolving long-standing and foundational open problems. However, constructions based on indistinguishability obfuscation almost always rely on non-black-box techniques, and thus the extent to which it can be used as a building block in cryptographic constructions has been completely unexplored so far. We present a framework for proving meaningful negative results on the power of indistinguishability obfuscation. By considering indistinguishability obfuscation for oracle-aided circuits, we capture the common techniques that have been used so far in constructions based on indistinguishability obfuscation. These include, in particular, non-black-box techniques such as the punctured programming approach of Sahai and Waters (STOC '14) and its variants, as well as sub-exponential security assumptions. Within our framework we prove the first negative results on the power of indistinguishability obfuscation and of the tightly related notion of functional encryption. Our results are as follows: - There is no fully black-box construction of a collision-resistant function family from an indistinguishability obfuscator for oracle-aided circuits. - There is no fully black-box construction of a key-agreement protocol with perfect completeness from a private-key functional encryption scheme for oracle-aided circuits. Specifically, we prove that any such potential constructions must suffer from an exponential security loss, and thus our results cannot be circumvented using sub-exponential security assumptions. Our framework captures constructions that may rely on a wide variety of primitives in a non-black-box manner (e.g., Obfuscating or generating a functional key for a function that uses the evaluation circuit of a puncturable pseudorandom function), and we only assume that the underlying indistinguishability obfuscator or functional encryption scheme themselves are used in a black-box manner.
[Protocols, oracle-aided circuit, indistinguishability obfuscation, exponential security loss, Encryption, Standards, pseudorandom function, exponential distribution, random sequences, private-key functional encryption scheme, Public key, private key cryptography, nonblack-box technique, Integrated circuit modeling]
Black-Box Garbled RAM
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Garbled RAM, introduced by Lu and Ostrovsky, enables the task of garbling a RAM (Random Access Machine) program directly, there by avoiding the inefficient process of first converting it into a circuit. Garbled RAM can be seen as a RAM analogue of Yao's garbled circuit construction, except that known realizations of Garbled RAM make non-black-box use of the underlying cryptographic primitives. In this paper we remove this limitation and provide the first black-box construction of Garbled RAM with polylogarithmic overhead. Our scheme allows for garbling multiple RAM programs being executed on a persistent database and its security is based only on the existence of one-way functions. We also obtain the first secure RAM computation protocol that is both constant round and makes only black-box use of one-way functions in the Oblivious Transfer hybrid model.
[polylogarithmic overhead, cryptographic protocols, Secure Computation, cryptographic primitives, Random access memory, Garbled RAM, black-box construction, Complexity theory, random access machine program, Computer science, oblivious transfer hybrid model, Databases, RAM computation protocol, black-box garbled RAM, One-Way Functions, persistent database, Central Processing Unit, Cryptography, one-way functions, Black-Box Cryptography]
Efficient Inverse Maintenance and Faster Algorithms for Linear Programming
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
In this paper, we consider the following inverse maintenance problem: given A &#x2208; Rn&#x00D7;d and a number of rounds r, at round k, we receive a n x n diagonal matrix D(k) and we wish to maintain an efficient linear system solver for ATD(k)A under the assumption D(k) does not change too rapidly. This inverse maintenance problem is the computational bottleneck in solving multiple optimization problems. We show how to solve this problem with O&#x0303; (nnz(A) + d&#x03C9;) preprocessing time and amortized O&#x0303;(nnz(A) + d2) time per round, improving upon previous running times. Consequently, we obtain the fastest known running times for solving multiple problems including, linear programming and computing a rounding of a polytope. In particular given a feasible point in a linear program with n variables, d constraints, and constraint matrix A &#x2208; Rd&#x00D7;n, we show how to solve the linear program in time O&#x0303;((nnz(A) + d2)&#x221A; d log(&#x2208;-1)). We achieve our results through a novel combination of classic numerical techniques of low rank update, preconditioning, and fast matrix multiplication as well as recent work on subspace embeddings and spectral sparsification that we hope will be of independent interest.
[Linear systems, O&#x0303;((nnz(A) + d2)&#x221A; d log(&#x2208;-1)), nxn diagonal matrix D(k), Maintenance engineering, Linear programming, amortized O&#x0303;(nnz(A) + d2) time, Stability analysis, linear programming, linear systems, Linear Systems, Approximation methods, Optimization, multiple optimization problem, matrix multiplication, ATD(k)A, linear system, inverse maintenance problem, O&#x0303; (nnz(A) + d&#x03C9;) preprocessing time, inverse problems, Approximation algorithms, Inverse Maintenance Problem, computational complexity, Linear Programming]
Constructing Linear-Sized Spectral Sparsification in Almost-Linear Time
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We present the first almost-linear time algorithm for constructing linear-sized spectral sparsification for graphs. This improves all previous constructions of linear-sized spectral sparsification, which requires &#x03A9;(n2) time [1], [2], [3]. A key ingredient in our algorithm is a novel combination of two techniques used in literature for constructing spectral sparsification: Random sampling by effective resistance [4], and adaptive constructions based on barrier functions [1], [3].
[Algorithm design and analysis, Laplace equations, &#x03A9;(n2) time algorithm, graph theory, linear-sized spectral sparsification, random sampling, Approximation methods, almost-linear time algorithm, barrier functions, Resistance, Computer science, spectral sparsification, graphs, adaptive constructions, algorithmic spectral graph theory, Approximation algorithms, Eigenvalues and eigenfunctions, computational complexity]
Guaranteed Matrix Completion via Nonconvex Factorization
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Matrix factorization is a popular approach for large-scale matrix completion. In this approach, the unknown low-rank matrix is expressed as the product of two much smaller matrices so that the low-rank property is automatically fulfilled. The resulting optimization problem, even with huge size, can be solved (to stationary points) very efficiently through standard optimization algorithms such as alternating minimization and stochastic gradient descent (SGD). However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of whether these algorithms will generate a good solution. In this paper, we establish a theoretical guarantee for the factorization based formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of the factorization based formulation, and recover the true low-rank matrix. A major difference of our work from the existing results is that we do not need resampling (i.e., Using independent samples at each iteration) in either the algorithm or its analysis. To the best of our knowledge, our result is the first one that provides exact recovery guarantee for many standard algorithms such as gradient descent, SGD and block coordinate gradient descent.
[Algorithm design and analysis, unknown low-rank matrix, optimization problem, block coordinate gradient descent, factorization based formulation, matrix completion, guaranteed matrix completion, Complexity theory, matrix decomposition, nonconvex optimization, Optimization, Manifolds, optimisation, gradient methods, factorization model, matrix factorization, standard optimization algorithms, perturbation analysis, large-scale matrix completion, alternating minimization, Minimization, Partitioning algorithms, stochastic gradient descent, Standards, nonconvex factorization]
Heavy-Tailed Independent Component Analysis
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Independent component analysis (ICA) is the problem of efficiently recovering a matrix A &#x2208; &#x211D;n&#x00D7;n from i.i.d. Observations of X=AS where S &#x2208; &#x211D;n is a random vector with mutually independent coordinates. This problem has been intensively studied, but all existing efficient algorithms with provable guarantees require that the coordinates Si have finite fourth moments. We consider the heavy-tailed ICA problem where we do not make this assumption, about the second moment. This problem also has received considerable attention in the applied literature. In the present work, we first give a provably efficient algorithm that works under the assumption that for constant &#x03B3; &gt; 0, each S<sub>i</sub> has finite (1+&#x03B3;)-moment, thus substantially weakening the moment requirement condition for the ICA problem to be solvable. We then give an algorithm that works under the assumption that matrix A has orthogonal columns but requires no moment assumptions. Our techniques draw ideas from convex geometry and exploit standard properties of the multivariate spherical Gaussian distribution in a novel way.
[Algorithm design and analysis, Damping, heavy-tailed distributions, Independent Component Analysis, Independent component analysis, Gaussian distribution, convex programming, random vector, Covariance matrices, Standards, heavy-tailed ICA problem, moment requirement condition, centroid body, independent component analysis, orthogonal columns, Signal processing algorithms, Silicon, convex geometry, multivariate spherical Gaussian distribution, heavy-tailed independent component analysis, mutually independent coordinates]
Input Sparsity and Hardness for Robust Subspace Approximation
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
In the subspace approximation problem, we seek a k-dimensional subspace F of Rd that minimizes the sum of p-th powers of Euclidean distances to a given set of n points a<sub>1</sub>,&#x22EF;, a<sub>n</sub> &#x2208; Rd, for p &#x2265; 1. More generally than minimizing &#x03A3;<sub>i</sub> dist(a<sub>i</sub> F)p, we may wish to minimize &#x03A3;<sub>i</sub> M(dist(a<sub>i</sub>, F)) for some loss function M(), for example, M-Estimators, which include the Huber and Tukey loss functions. Such subspaces provide alternatives to the singular value decomposition (SVD), which is the p = 2 case, finding such an F that minimizes the sum of squares of distances. For p E [1, 2), and for typical M-Estimators, the minimizing F gives a solution that is more robust to outliers than that provided by the SVD. We give several algorithmic results for these robust subspace approximation problems. We state our results as follows, thinking of the n points as forming an n &#x00D7; d matrix A, and letting nnz(A) denote the number of non-zero entries of A. Our results hold for p &#x2208; [1, 2). We use poly(n) to denote nO(1) as n &#x2192; &#x221E;. 1) For minimizing &#x03A3;<sub>i</sub> dist(a<sub>i</sub>, F)p, we give an algorithm running in O(nnz(A) + (n + d)poly(k/&#x03B5;) + exp(poly(k/&#x03B5;))) 2) We show that the problem of minimizing &#x03A3;<sub>i</sub> dist(a<sub>i</sub>, F)p is NP-hard, even to output a (1 + 1/poly(d))-approximation. This extends work of Deshpande et al. (SODA, 2011) which could only show NP-hardness or UGC-hardness for p &gt; 2; their proofs critically rely on p &gt; 2. Our work resolves an open question of [Kannan Vempala, NOW, 2009]. Thus, there cannot be an algorithm running in time polynomial in k and 1/&#x03B5; unless P = NP. Together with prior work, this implies that the problem is NP-hard for all p &#x2260; 2. 3) For loss functions for a wide class of M-Estimators, we give a problem-size reduction: for a parameter K = (log n)O(log k), our reduction takes O(nnz(A) logn + (n + d)poly(K/&#x03B5;)) time to reduce the problem to a constrained version involving matrices whose dimensions are poly(K&#x03B5;-1 log n). We also give bicriteria solutions. 4) Our techniques lead to the first O(mmz(A) + poly(d/&#x03B5;)) time algorithms for (1 + &#x03B5;)-approximate regression for a wide class of convex M-Estimators. This improves prior results [1], which were (1 + &#x03B5;)-approximation for Huber regression only, and O(1)-approximation for a general class of M-Estimators.
[SVD, (1 + &#x03B5;)-factor, UGC-hardness, robust subspace approximation problem, &#x03A3;<sub>i</sub> dist(a<sub>i</sub> F)p, low rank approximation, Tukey loss function, O(mmz(A) + poly(d/&#x03B5;)) time algorithm, O(1)-approximation, Approximation methods, nO(1), Huber regression, numerical linear algebra, Robustness, Polynomials, singular value decomposition, Singular value decomposition, O(mmz(A) logn + (n + d)poly(K/&#x03B5;)), approximation theory, k-dimensional subspace, F)), sampling, &#x03A3;<sub>i</sub> M(dist(a<sub>i</sub>, Huber loss function, poly(K&#x03B5;-1 log n), sketching, (1 + &#x03B5;)-approximate regression, NP-hard problem, O(mmz(A) + (n + d)poly(k/&#x03B5;) + exp(poly(k/&#x03B5;))), regression, Euclidean distance, Approximation algorithms, Time complexity, m-estimator, Principal component analysis, computational complexity, robust statistics]
The Minimum Principle of SINR: A Useful Discretization Tool for Wireless Communication
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Theoretical study of optimization problems in wireless communication often deals with zero-dimensional tasks. For example, the power control problem requires computing a power assignment guaranteeing that each transmitting station is successfully received at a single receiver point. This paper aims at addressing communication applications that require handling 2-dimensional tasks (e.g., Guaranteeing successful transmission in entire regions rather than in specific points). A natural approach to such tasks is to discretize the 2-dimensional optimization domain, e.g., By sampling points within the domain. This approach, however, might incur high time and memory requirements, and moreover, it cannot guarantee exact solutions. Towards this goal, we establish the minimum principle for the SINR function with free-space path loss (i.e., When the signal decays in proportion to the square of the distance between the transmitter and receiver). We then utilize it as a discretization technique for solving two-dimensional problems in the SINR model. This approach is shown to be useful for handling optimization problems over two dimensions (e.g., Power control, energy minimization), in providing tight bounds on the number of null-cells in the reception map, and in approximating geometrical and topological properties of the wireless reception map (e.g., Maximum inscribed sphere). Essentially, the minimum principle allows us to reduce the dimension of the optimization domain without losing anything in the accuracy or quality of the solution. More specifically, when the two dimensional optimization domain is bounded and free from any interfering station, the minimum principle implies that it is sufficient to optimize over the boundary of the domain, as the "hardest" points to be satisfied reside on boundary and not in the interior. We believe that the minimum principle, as well as the interplay between continuous and discrete analysis presented in this paper, may pave the way to future study of algorithmic SINR in higher dimensions.
[optimization problem, rational polynomials, minimum principle, power control problem, Optimization, wireless communication, Wireless communication, optimisation, power assignment, Polynomials, discretization technique, Testing, SINR function minimum principle, telecommunication power management, Interference, radio receivers, free-space path loss, discrete analysis, single receiver point, Power control, 2-dimensional optimization domain, power control, SINR, continuous analysis, discretization tool, Signal to noise ratio]
Enabling Robust and Efficient Distributed Computation in Dynamic Peer-to-Peer Networks
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Motivated by the need for designing efficient and robust fully-distributed computation in highly dynamic networks such as Peer-to-Peer (P2P) networks, we study distributed protocols for constructing and maintaining dynamic network topologies with good expansion properties. Our goal is to maintain a sparse (bounded degree) expander topology despite heavy churn (i.e., Nodes joining and leaving the network continuously over time). We assume that the churn is controlled by an adversary that has complete knowledge and control of what nodes join and leave and at what time and has unlimited computational power, but is oblivious to the random choices made by the algorithm. Our main contribution is a randomized distributed protocol that guarantees with high probability the maintenance of a constant degree graph with high expansion even under continuous high adversarial churn. Our protocol can tolerate a churn rate of up to O(n/polylog(n)) per round (where n is the stable network size). Our protocol is efficient, lightweight, and scalable, and it incurs only O(polylog(n)) overhead for topology maintenance: only polylogarithmic(in n) bits needs to be processed and sent by each node per round and any node's computation cost per round is also polylogarithmic. The given protocol is a fundamental ingredient that is needed for the design of efficient fully-distributed algorithms for solving fundamental distributed computing problems such as agreement, leader election, search, and storage in highly dynamic P2P networks and enables fast and scalable algorithms for these problems that can tolerate a large amount of churn.
[Protocols, leader election, Heuristic algorithms, fault-tolerance, dynamic network topologies, topology maintenance, churn, robust fully distributed computation, Network topology, sparse expander topology, continuous high adversarial churn, Robustness, dynamic peer-to-peer networks, expander graph, protocols, unlimited computational power, randomized protocol, fully distributed algorithms, peer-to-peer computing, randomized distributed protocols, distributed computing problems, Maintenance engineering, polylogarithmic, distributed computation, Topology, dynamic network, scalable algorithms, P2P computing, distributed algorithms, constant degree graph, Peer-to-peer computing, dynamic P2P networks]
Planar Reachability in Linear Space and Constant Time
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show how to represent a planar digraph in linear space so that reach ability queries can be answered in constant time. The data structure can be constructed in linear time. This representation of reach ability is thus optimal in both time and space, and has optimal construction time. The previous best solution used O(n log n) space for constant query time [Thorup FOCS'01].
[reachability queries, Transmission line matrix methods, linear space, Random access memory, data structure, Planar graphs, query processing, data structures, O(n log n) space, reachability, reachability analysis, constant query time, Computational modeling, planar reachability, Data structures, constant time, Computer science, planar digraph, Upper bound, directed graphs, Vegetation, reachability representation, optimal construction time, computational complexity, linear time]
Towards an Optimal Method for Dynamic Planar Point Location
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We describe a fully dynamic linear-space data structure for point location in connected planar subdivisions, or more generally vertical ray shooting among non-intersecting line segments, that supports queries in O(log n(log log n)2) time and updates in O(log nlog log n) time. This is the first data structure that achieves close to logarithmic query and update time simultaneously, ignoring log logn factors. We further show how to reduce the query time to O(logn log log n) in the RAM model with randomization. Alternatively, the query time can be lowered to O(log n) if the update time is increased to O(log1+&#x03B5; n) for any constant &#x03B5; &gt; 0, or vice versa.
[RAM model, Random access memory, dynamic linear-space data structure, query time, Data structures, connected planar subdivisions, Standards, Computer science, vertical ray shooting, randomization, nonintersecting line segments, Vegetation, optimal method, data structures, Face, Slabs, dynamic planar point location, logarithmic query]
Pattern-Avoiding Access in Binary Search Trees
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
The dynamic optimality conjecture is perhaps the most fundamental open question about binary search trees (BST). It postulates the existence of an asymptotically optimal online BST, i.e. one that is constant factor competitive with any BST on any input access sequence. The two main candidates for dynamic optimality in the literature are splay trees [Sleator and Tarjan, 1985], and GREEDY [Lucas, 1988; Munro, 2000; Demaine et al. 2009]. Despite BSTs being among the simplest data structures in computer science, and despite extensive effort over the past three decades, the conjecture remains elusive. Dynamic optimality is trivial for almost all sequences: the optimum access cost of most length-&#x03C4; sequences is &#x0398;(n log n), achievable by any balanced BST. Thus, the obvious missing step towards the conjecture is an understanding of the &#x201C;easy&#x201D; access sequences, and indeed the most fruitful research direction so far has been the study of specific sequences, whose &#x201C;easiness&#x201D; is captured by a parameter of interest. For instance, splay provably achieves the bound of O(nd) when d roughly measures the distances between consecutive accesses (dynamic finger), the average entropy (static optimality), or the delays between multiple accesses of an element (working set). The difficulty of proving dynamic optimality is witnessed by other highly restricted special cases that remain unresolved; one prominent example is the traversal conjecture [Sleator and Tarjan, 1985], which states that preorder sequences (whose optimum is linear) are linear-time accessed by splay trees; no online BST is known to satisfy this conjecture. In this paper, we prove two different relaxations of the traversal conjecture for GREEDY: (i) GREEDY is almost linear for preorder traversal, (ii) if a linear-time preprocessing1 is allowed, GREEDY is in fact linear. These statements are corollaries of our more general results that express the complexity of access sequences in terms of a pattern avoidance parameter k. Pattern avoidance is a well-established concept in combinatorics, and the classes of input sequences thus defined are rich, e.g. the k = 3 case includes preorder sequences. For any sequence X with parameter k, our most general result shows that GREEDY achieves the cost n2&#x03B1;(n)O(k) where &#x03B1; is the inverse Ackermann function. Furthermore, a broad subclass of parameter-k sequences has a natural combinatorial interpretation as k-decomposable sequences. For this class of inputs, we obtain an n2O(k2) bound for GREEDY when preprocessing is allowed. For k = 3, these results imply (i) and (ii). To our knowledge, these are the first upper bounds for GREEDY that are not known to hold for any other online BST. To obtain these results we identify an input-revealing property of GREEDY. Informally, this means that the execution log partially reveals the structure of the access sequence. This property facilitates the use of rich technical tools from forbidden submatrix theory. Further studying the intrinsic complexity of k-decomposable sequences, we make several observations. First, in order to obtain an offline optimal BST, it is enough to bound GREEDY on non-decomposable access sequences. Furthermore, we show that the optimal cost for k-decomposable sequences is &#x0398;(n log k), which is well below the proven performance of all known BST algorithms. Hence, sequences in this class can be seen as a &#x201C;candidate counterexample&#x201D; to dynamic optimality.
[combinatorial mathematics, Heuristic algorithms, static optimality, asymptotically optimal online BST, Complexity theory, splay trees, entropy, computer science, inverse Ackermann function, traversal conjecture, pattern-avoiding access, data structures, pattern avoidance, parameter-k sequences, dynamic optimality conjecture, pattern-avoidance, Computational modeling, Optimized production technology, Binary search trees, tree searching, GREEDY, BST algorithms, Computer science, k-decomposable sequences, combinatorial interpretation, Approximation algorithms, forbidden submatrix theory, binary search trees, access sequences]
The Average Sensitivity of Bounded-Depth Formulas
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show that unbounded fan-in boolean formulas of depth d + 1 and size s have average sensitivity O(1/d log s)d. In particular, this gives a tight 2&#x03A9;(d(n1/d-1)) lower bound on the size of depth d + 1 formulas computing the PARITY function. These results strengthen the corresponding O(log s)d and 2&#x03A9;(n1/d) bounds for circuits due to Boppana (1997) and Hastad (1986). Our proof technique studies a random process associated with formulas, in which the Switching Lemma is efficiently applied to subformulas.
[bounded-depth formulas, Switches, switching lemma, proof technique, average sensitivity, random process, Sensitivity, Boolean functions, unbounded fan-in Boolean formulas, Logic gates, Bismuth, PARITY function, Random variables, theorem proving, Decision trees]
The Power of Asymmetry in Constant-Depth Circuits
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
The threshold degree of a Boolean function f is the minimum degree of a real polynomial p that represents f in sign: f(x) = sgn p(x). Introduced in the seminal work of Minsky and Papert (1969), this notion is central to some of the strongest algorithmic and complexity-theoretic results for constant-depth circuits. One problem that has remained open for several decades, with applications to computational learning and communication complexity, is to determine the maximum threshold degree of a polynomial-size constant-depth circuit in n variables. The best lower bound prior to our work was &#x03A9;(n(d-1)/(2d-1)) for circuits of depth d. We obtain a polynomial improvement for every depth d, with a lower bound of &#x03A9;(n3/7) for depth 3 and &#x03A9;(&#x221A;n) for depth d &#x2265;4. The proof contributes a novel approximation-theoretic technique of independent interest, which exploits asymmetry in circuits to prove their hardness for polynomials.
[circuit complexity, constant-depth circuit, Transforms, computational learning theory, asymmetry, Complexity theory, Approximation methods, communication complexity, approximation-theoretic technique, Boolean functions, polynomial approximation, polynomial representations of Boolean functions, Polynomials, learning (artificial intelligence), Constant-depth circuits, threshold degree, approximation theory, maximum threshold degree, polynomials, Boolean function, Computer science, polynomial threshold functions, Upper bound, polynomial degree, computational learning]
Deterministic Divisibility Testing via Shifted Partial Derivatives
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Kayal has recently introduced the method of shifted partial derivatives as a way to give the first exponential lower bound for computing an explicit polynomial as a sum of powers of constant-degree polynomials. This method has garnered further attention because of the work of Gupta, Kamath, Kayal and Saptharishi who used this method to obtain lower bounds that approach the "chasm at depth-4". In this work, we investigate to what extent this method can be used to obtain deterministic polynomial identity testing (PIT) algorithms, which are algorithms that decide whether a given algebraic circuit computes the zero polynomial. In particular, we give a poly(s)(log s)-time deterministic black-box PIT algorithm for a size-s sum of powers of constant-degree polynomials. This is the first sub-exponential deterministic PIT algorithm for this model of computation and the first PIT algorithm based on the method of shifted partial derivatives. We also study the problem of divisibility testing, which when given multivariate polynomials f and g (as algebraic circuits) asks to decide whether f divides g. Using Strassen's technique for the elimination of divisions, we show that one can obtain deterministic divisibility testing algorithms via deterministic PIT algorithms, and this reduction does not dramatically increase the complexity of the underlying algebraic circuits. Using this reduction, we show that deciding divisibility of a constant-degree polynomial f into a sparse polynomial g reduces to PIT of sums of a monomial multiplied by a power of constant-degree polynomials. We then extend the method of shifted partial derivatives to give a poly(s)(log s)-time deterministic black-box PIT algorithm for this model of computation, and thus derive a corresponding deterministic divisibility algorithm. This is the first non-trivial deterministic algorithm for this problem. Previous work on multivariate divisibility testing due to Saha, Saptharishi and Saxena gave algorithms for when f is linear and g is sparse, and essentially worked via PIT algorithms for read-once (oblivious) algebraic branching programs (roABPs). We give explicit sums of powers of quadratic polynomials that require exponentially-large roABPs in a strong sense, showing that techniques known for roABPs have limited applicability in our regime. Finally, by combining our results with the algorithm of Forbes, Shpilka and Saptharishi we obtain poly(s)(log log s)-time deterministic black-box PIT algorithms for various models (translations of sparse polynomials, and sums of monomials multiplied by a power of a linear polynomial) where only poly(s) (Theta(log s))-time such algorithms were previously known.
[Algorithm design and analysis, deterministic divisibility testing algorithm, sparse polynomials, PIT algorithm, algebraic circuit, Complexity theory, roABP, multivariate divisibility testing, Algebra, Strassen technique, Polynomials, Testing, poly(s)&#x0398;(lg s)-time, read-once algebraic branching programs, Computational modeling, poly(s)O(lg s)-time deterministic black-box PIT algorithm, zero polynomial, sparse polynomial, shifted partial derivatives, polynomial identity testing, constant-degree polynomial, constant-degree polynomials, deterministic algorithms, deterministic polynomial identity testing algorithm, quadratic polynomial, explicit polynomial, divisibility testing, read-once algebraic branching program, sub-exponential deterministic PIT algorithm, shifted partial derivative, partial differential equations, nontrivial deterministic algorithm, Integrated circuit modeling, computational complexity]
Hardness of Approximation in PSPACE and Separation Results for Pebble Games
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We consider the pebble game on DAGs with bounded fan-in introduced in [Paterson and Hewitt '70] and the reversible version of this game in [Bennett '89], and study the question of how hard it is to decide exactly or approximately the number of pebbles needed for a given DAG in these games. We prove that the problem of deciding whether s pebbles suffice to reversibly pebble a DAG G is PSPACE-complete, as was previously shown for the standard pebble game in [Gilbert, Lengauer and Tarjan '80]. Via two different graph product constructions we then strengthen these results to establish that both standard and reversible pebbling space are PSPACE-hard to approximate to within any additive constant. To the best of our knowledge, these are the first hardness of approximation results for pebble games in an unrestricted setting (even for polynomial time). Also, since [Chan '13] proved that reversible pebbling is equivalent to the games in [Dymond and Tompa '85] and [Raz and McKenzie '99], our results apply to the Dymond -- Tompa and Raz -- McKenzie games as well, and from the same paper it follows that resolution depth is PSPACE-hard to determine up to any additive constant. We also obtain a multiplicative logarithmic separation between reversible and standard pebbling space. This improves on the additive logarithmic separation previously known and could plausibly be tight, although we are not able to prove this. We leave as an interesting open problem whether our additive hardness of approximation result could be strengthened to a multiplicative bound if the computational resources are decreased from polynomial space to the more common setting of polynomial time.
[pebbling, Additives, PSPACE-hard, pebble games, graph theory, multiplicative bound, Complexity theory, Approximation methods, reversible pebbling space, separation, Polynomials, separation results, Raz-McKenzie games, Raz-Mc Kenzie game, game theory, PSPACE-hardness of approximation, PSPACE-complete, DAG, Dymond-Tompa game, resolution depth, Standards, approximation hardness, Computer science, Games, multiplicative logarithmic separation, Dymond-Tompa games, graph product constructions, reversible pebbling, computational complexity]
The Submodular Secretary Problem Goes Linear
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
During the last decade, the matroid secretary problem (MSP) became one of the most prominent classes of online selection problems. The interest in MSP is twofold: on the one hand, there are many interesting applications of MSP, and on the other hand, there is strong hope that MSP admits O(1)-competitive algorithms, which is the claim of the well-known matroid secretary conjecture. Partially linked to its numerous applications in mechanism design, substantial interest arose also in the study of nonlinear versions of MSP, with a focus on the sub modular matroid secretary problem (SMSP). The fact that sub modularity captures the property of diminishing returns, a very natural property for valuation functions, is a key reason for the interest in SMSP. So far, O(1)-competitive algorithms have been obtained for SMSP over some basic matroid classes. This created some hope that, analogously to the matroid secretary conjecture, one may even obtain O(1)-competitive algorithms for SMSP over any matroid. However, up to now, most questions related to SMSP remained open, including whether SMSP may be substantially more difficult than MSP, and more generally, to what extend MSP and SMSP are related. Our goal is to address these points by presenting general black-box reductions from SMSP to MSP. In particular, we show that any O(1)-competitive algorithm for MSP, even restricted to a particular matroid class, can be transformed in a black-box way to an O(1)-competitive algorithm for SMSP over the same matroid class. This implies that the matroid secretary conjecture is equivalent to the same conjecture for SMSP. Hence, in this sense SMSP is not harder than MSP. Also, to find O(1)-competitive algorithms for SMSP over a particular matroid class, it suffices to consider MSP over the same matroid class. Using our reductions we obtain many first and improved O(1)-competitive algorithms for SMSP over various matroid classes by leveraging known algorithms for MSP. Moreover, our reductions imply an O(log log(rank))-competitive algorithm for SMSP, thus, matching the currently best asymptotic algorithm for MSP, and substantially improving on the previously best O(log(rank))-competitive algorithm for SMSP.
[Algorithm design and analysis, Computers, general black-box reduction, competitive algorithms, secretary problem, Partitioning algorithms, Electronic mail, online algorithms, Cost accounting, asymptotic algorithm, submodular functions, Computer science, matroids, competitive algorithm, submodular matroid secretary problem, computational complexity]
Competitive Flow Time Algorithms for Polyhedral Scheduling
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Many scheduling problems can be viewed as allocating rates to jobs, subject to convex packing constraints on the rates. In this paper, we consider the problem of rate allocation when jobs of unknown size arrive online (non-clairvoyant setting), with the goal of minimizing weighted delay or flow time. Though this problem has strong lower bounds on competitive ratio in its full generality, we show positive results for natural and fairly broad sub-classes. More specifically, the subclasses we consider not only generalize several well-studied models such as scheduling with speedup curves and related machine scheduling, but also capture as special cases hitherto unstudied scheduling problems such as routing multi-commodity flows, routing multicast (video-on-demand) trees, and multi-dimensional resource allocation. We establish several first positive results by making connections with two disparate disciplines: Economics and Queueing theory. First, we view the instantaneous allocation of rates as a resource allocation problem. We analyze the natural proportional fairness algorithm from economics. To do this, we extend results from market clearing literature, particularly the Eisenberg-Gale markets and the notions of Walrasian equilibria and Gross Substitutes. This yields the first constant competitive algorithm with constant speed augmentation for single-sink flow routing, routing multicast trees, and multidimensional resource allocation with substitutes resources. Next, we consider the general scheduling problem with packing constraints on rates, but with the restriction that the number of different job types is fixed. We model this problem as a non-stochastic queueing problem. We generalize a natural algorithm from queueing literature and analyze it by extending queueing theoretic ideas. We show that the competitive ratio, for any constant speed, depends polynomially only on the number of job types. Further, such a dependence on the number of job types is unavoidable for non-clairvoyant algorithms. This yields the first algorithm for scheduling multicommodity flows whose competitive ratio depends polynomially on the size of the underlying graph, and not on the number of jobs.
[Algorithm design and analysis, routing multicommodity flows, routing multicast trees, Online Algorithms, machine scheduling, competitive ratio, resource allocation, scheduling, video-on-demand, polyhedral scheduling, Single machine scheduling, gross substitutes, Economics, general scheduling problem, queueing theory, Biological system modeling, convex packing constraints, Eisenberg-Gale markets, trees (mathematics), Routing, single-sink flow routing, Scheduling, Proportional Fairness, rate allocation, nonstochastic queueing problem, competitive flow time algorithms, graph, resource allocation problem, Flow-time minimization, multidimensional resource allocation, economics, Walrasian equilibria, multicommodity flow scheduling, natural proportional fairness algorithm, Resource management, Queueing analysis]
Tight Bounds for Online Vector Scheduling
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Modern data centers face a key challenge of effectively serving user requests that arrive online. Such requests are inherently multi-dimensional and characterized by demand vectors over multiple resources such as processor cycles, storage space, and network bandwidth. Typically, different resources require different objectives to be optimized, and L<sub>r</sub> norms of loads are among the most popular objectives considered. Furthermore, the server clusters are also often heterogeneous making the scheduling problem more challenging. To address these problems, we consider the online vector scheduling problem in this paper. Introduced by Chekuri and Khanna (SIAM J. of Comp. 2006), vector scheduling is a generalization of classical load balancing, where every job has a vector load instead of a scalar load. The scalar problem, introduced by Graham in 1966, and its many variants (identical and unrelated machines, makespan and L<sub>r</sub>-norm optimization, offline and online jobs, etc.) have been extensively studied over the last 50 years. In this paper, we resolve the online complexity of the vector scheduling problem and its important generalizations - for all L<sub>r</sub> norms and in both the identical and unrelated machines settings. Our main results are: &#x00B7; For identical machines, we show that the optimal competitive ratio is &#x0398;(log d/ log log d) by giving an online lower bound and an algorithm with an asymptotically matching competitive ratio. The lower bound is technically challenging, and is obtained via an online lower bound for the minimum mono-chromatic clique problem using a novel online coloring game and randomized coding scheme. Our techniques also extend to asymptotically tight upper and lower bounds for general L<sub>r</sub> norms. &#x00B7; For unrelated machines, we show that the optimal competitive ratio is &#x0398;(log m + log d) by giving an online lower bound that matches a previously known upper bound. Unlike identical machines, however, extending these results, particularly the upper bound, to general L<sub>r</sub> norms requires new ideas. In particular, we use a carefully constructed potential function that balances the individual L<sub>r</sub> objectives with the overall (convexified) min-max objective to guide the online algorithm and track the changes in potential to bound the competitive ratio.
[load balancing, asymptotically matching competitive ratio, data centers, server clusters, online coloring game, demand vectors, Electronic mail, Approximation methods, minimax techniques, resource allocation, scheduling, min-max objective, Color, game theory, online vector scheduling, computer centres, minimum monochromatic clique problem, randomised algorithms, Computer science, randomized coding scheme, Online algorithms, Upper bound, vectors, Processor scheduling, Load management, load balancing generalization]
Online Buy-at-Bulk Network Design
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We present the first non-trivial online algorithms for the non-uniform, multicommodity buy-at-bulk (MC-BB) network design problem. Our competitive ratios qualitatively match the best known approximation factors for the corresponding offline problems. In particular, we show:1. A polynomial time online algorithm with a poly-logarithmic competitive ratio for the MC-BB problem in undirected edge-weighted graphs.2. A quasi-polynomial time online algorithm with a poly-logarithmic competitive ratio for the MC-BB problem in undirected node-weighted graphs.3. For any fixed &#x03B5; &gt; 0, a polynomial time online algorithm with a competitive ratio of O&#x0305;(k{1/2+&#x03B5;} polylog(n)) (where k is the number of demands) for MC-BB in directed graphs.4. Algorithms with matching competitive ratios for the prize-collecting variants of all the above problems. Prior to our work, a logarithmic competitive ratio was known for undirected, edge-weighted graphs only for the special case of uniform costs (Awerbuch and Azar, FOCS 1997), and a polylogarithmic competitive ratio was known for the edge-weighted single-sink problem (Meyerson, SPAA 2004). To the best of our knowledge, no previous online algorithm was known, even for uniform costs, in the node-weighted and directed settings. Our main engine for the results above is an online reduction theorem of MC-BB problems to their single-sink (SS-BB) counterparts. We use the concept of junction-tree solutions (Chekuri et al., FOCS 2006) that play an important role in solving the offline versions of the problem via a greedy subroutine -- an inherently offline procedure. Our main technical contribution is in designing an online algorithm using only the existence of good junction-trees to reduce an MC-BB instance to multiple SS-BB sub-instances. Along the way, we also give the first non-trivial online node-weighted/directed single-sink buy-at-bulk algorithms. In addition to the new results, our generic reduction also yields new proofs of recent results for the online node-weighted Steiner forest and online group Steiner forest problems.
[Algorithm design and analysis, Steiner trees, polylogarithmic competitive ratios, prize-collecting variants, graph theory, MC-BB network design problem, quasipolynomial time online algorithm, edge-weighted single-sink problem, Electronic mail, junction-tree solutions, Approximation methods, multicommodity buy-at-bulk network design problem, online reduction theorem, buy at bulk costs, online group Steiner forest problems, offline problems, undirected edge-weighted graphs, Polynomials, network design, nontrivial online algorithms, online node-weighted Steiner forest, greedy subroutine, Computer science, Online algorithms, undirected node-weighted graphs, multicommodity flows, Approximation algorithms, Internet, online buy-at-bulk network design]
Solving the Closest Vector Problem in 2^n Time -- The Discrete Gaussian Strikes Again!
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We give a 2n+o(n)-time and space randomized algorithm for solving the exact Closest Vector Problem (CVP) on n-dimensional Euclidean lattices. This improves on the previous fastest algorithm, the deterministic O&#x0303;(4n)-time and O&#x0303;(2n)-space algorithm of Micciancio and Voulgaris [1]. We achieve our main result in three steps. First, we show how to modify the sampling algorithm from [2] to solve the problem of discrete Gaussian sampling over lattice shifts, L - t, with very low parameters. While the actual algorithm is a natural generalization of [2], the analysis uses substantial new ideas. This yields a 2n+o(n)-time algorithm for approximate CVP with the very good approximation factor &#x03B3; = 1 + 2-o(n/ log n). Second, we show that the approximate closest vectors to a target vector t can be grouped into &#x201C;lower-dimensional clusters,&#x201D; and we use this to obtain a recursive reduction from exact CVP to a variant of approximate CVP that &#x201C;behaves well with these clusters.&#x201D; Third, we show that our discrete Gaussian sampling algorithm can be used to solve this variant of approximate CVP. The analysis depends crucially on some new properties of the discrete Gaussian distribution and approximate closest vectors, which might be of independent interest.
[Algorithm design and analysis, Discrete Gaussian, Lattices, Gaussian distribution, 2^n time algorithm, Electronic mail, Approximation methods, n-dimensional Euclidean lattices, target vector, approximation factor, deterministic O&#x0303;(4n)-time algorithm, approximate closest vectors, Lattice Problems, Clustering algorithms, approximate CVP, approximation theory, sampling methods, discrete Gaussian sampling algorithm, lattice shifts, lattice theory, discrete Gaussian distribution, 2n+o(n)-time randomized algorithm, 2n+o(n)-space randomized algorithm, deterministic O&#x0303;(2n)-space algorithm, Closest Vector Problem, deterministic algorithms, randomised algorithms, Computer science, exact-closest vector problem, Gaussian processes, Approximation algorithms, recursive reduction, lower-dimensional clusters, computational complexity]
A Robust Sparse Fourier Transform in the Continuous Setting
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
In recent years, a number of works have studied methods for computing the Fourier transform in sublinear time if the output is sparse. Most of these have focused on the discrete setting, even though in many applications the input signal is continuous and naive discretization significantly worsens the sparsity level. We present an algorithm for robustly computing sparse Fourier transforms in the continuous setting. Let x*(t) = x(t)+g(t), where x* has a k-sparse Fourier transform and g is an arbitrary noise term. Given sample access to x(t) for some duration T, we show how to find a k-Fourier-sparse reconstruction x'(t) with 1/T &#x222B;<sub>0</sub>T|x'(t) - x(t)|2dt &#x2272; 1/T&#x222B;<sub>0</sub>T|g(t)|2dt. The sample complexity is linear in k and logarithmic in the signal-to-noise ratio and the frequency resolution. Previous results with similar sample complexities could not tolerate an infinitesimal amount of i.i.d. Gaussian noise, and even algorithms with higher sample complexities increased the noise by a polynomial factor. We also give new results for how precisely the individual frequencies of x* can be recovered.
[discrete setting, Fourier transforms, continuous setting, Superresolution, naive discretization, Complexity theory, k-Fourier-sparse reconstruction, Signal resolution, Standards, continuous discretization, robust sparse fourier transform, Gaussian noise, Approximation algorithms, Robustness, Fourier Transform, Sparse Recovery, sparse Fourier transforms, k-sparse Fourier transform, Signal to noise ratio]
Breaking the Variance: Approximating the Hamming Distance in 1/&amp;amp;#x3B5; Time Per Alignment
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
The algorithmic tasks of computing the Hamming distance between a given pattern of length m and each location in a text of length n is one of the most fundamental algorithmic tasks in string algorithms. Unfortunately, there is evidence that for a text T of sizen and a pattern P of size m, one cannot compute the exact Hamming distance for all locations in T in time which is less than O(n&#x221A;m). However, Karloff [30] showed that if one is willing to suffer a 1 &#x00B1; &#x20AC; approximation, then it is possible to solve the problem with high probability, in O(2/n) time. Due to related lower bounds for computing the Hamming distance of two strings in the one-way communication complexity model, it is strongly believed that obtaining an algorithm for solving the approximation version cannot be done much faster as a function of 1/&#x03B5;. We show here that this belief is false by introducing a new O(n/&#x03B5; ) time algorithm that succeeds with high probability. The main idea behind our algorithm, which is common in sparse recovery problems, is to reduce the variance of a specific randomized experiment by (approximately) separating heavy hitters from non-heavy hitters. However, while known sparse recovery techniques work very well on vectors, they do not seem to apply here, where we are dealing with mismatches between pairs of characters. We introduce two main algorithmic ingredients. The first is a new sparse recovery method that applies for pair inputs (such as in our setting). The second is a new construction of hash/projection functions, for which have which allows us to count the number of projections that induce mismatches between two characters exponentially faster than brute force. We expect that these algorithmic techniques will be of independent interest.
[Measurement, text analysis, string algorithms, projection functions, Complexity theory, hash functions, Approximation methods, communication complexity, Runtime, Stringology, function approximation, sparse recovery techniques, Hamming distance, probability, one-way communication complexity model, sparse recovery problems, Computer science, Approximate Hamming distance, non-heavy hitters, 1/&#x03B5; time per alignment, Approximation algorithms, string matching, Hamming distance approximation, Sparse Recovery, sparse matrices]
Approximately Counting Triangles in Sublinear Time
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We consider the problem of estimating the number of triangles in a graph. This problem has been extensively studied in both theory and practice, but all existing algorithms read the entire graph. In this work we design a sublinear-time algorithm for approximating the number of triangles in a graph, where the algorithm is given query access to the graph. The allowed queries are degree queries, vertex-pair queries and neighbor queries. We show that for any given approximation parameter 0&lt;;epsilon&lt;;1, the algorithm provides an estimate hat{t} such that with high constant probability, (1-epsilon) t&lt;;hat{t}&#x03BA;(1+epsilon)t, where t is the number of triangles in the graph G. The expected query complexity of the algorithm is O(n/t&#x0302;{1/3} + min {m, m&#x0302;{3/2}/t}) poly(log n, 1/epsilon), where n is the number of vertices in the graph and m is the number of edges, and the expected running time is (n/t&#x0302;{1/3} + m&#x0302;{3/2}/t) poly(log n, 1/epsilon). We also prove that Omega(n/t&#x0302;{1/3} + min {m, m&#x0302;{3/2}/t}) queries are necessary, thus establishing that the query complexity of this algorithm is optimal up to polylogarithmic factors in n (and the dependence on 1/epsilon).
[Algorithm design and analysis, approximation theory, TV, expected query complexity, approximate triangle counting, Sublinear Approximation Algorithm, graph theory, polylogarithmic factors, vertex-pair queries, Complexity theory, Approximation methods, approximation parameter, Triangles Counting, Computer science, query processing, neighbor queries, Approximation algorithms, Yttrium, query access, sublinear-time algorithm, computational complexity]
Differentially Private Release and Learning of Threshold Functions
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove new upper and lower bounds on the sample complexity of (&#x03B5;, &#x03B4;) differentially private algorithms for releasing approximate answers to threshold functions. A threshold function c over a totally ordered domain X evaluates to c<sub>z</sub>(y) = 1 if y &#x2264; x, and evaluates to 0 otherwise. We give the first nontrivial lower bound for releasing thresholds with (&#x03B5;, &#x03B4;) differential privacy, showing that the task is impossible over an infinite domain X, and moreover requires sample complexity n &#x2265; &#x03A9;(log* |X|), which grows with the size of the domain. Inspired by the techniques used to prove this lower bound, we give an algorithm for releasing thresholds with n &#x2264; 2(1+&#x03BF;(1)) log* |X| samples. This improves the previous best upper bound of 8(1+&#x03BF;(1)) log* |X| (Beimel et al., RANDOM '13). Our sample complexity upper and lower bounds also apply to the tasks of learning distributions with respect to Kolmogorov distance and of properly PAC learning thresholds with differential privacy. The lower bound gives the first separation between the sample complexity of properly learning a concept class with (&#x03B5;, &#x03B4;) differential privacy and learning without privacy. For properly learning thresholds in &#x2113; dimensions, this lower bound extends to n &#x2265; &#x03A9;(&#x2113; &#x00B7; log* |X|). To obtain our results, we give reductions in both directions from releasing and properly learning thresholds and the simpler interior point problem. Given a database D of elements from X, the interior point problem asks for an element between the smallest and largest elements in D. We introduce new recursive constructions for bounding the sample complexity of the interior point problem, as well as further reductions and techniques for proving impossibility results for other basic problems in differential privacy.
[Data privacy, PAC learning, fingerprinting codes, interior point problem, Complexity theory, learning distributions, threshold function learning, lower bounds, sample complexity, Computer science, Kolmogorov distance, Privacy, Upper bound, Databases, PAC learning thresholds, Approximation algorithms, data privacy, differentially private release, differential privacy, threshold functions, computational complexity, recursive constructions]
Robust Traceability from Trace Amounts
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
The privacy risks inherent in the release of a large number of summary statistics were illustrated by Homer et al. (PLoS Genetics, 2008), who considered the case of 1-way marginals of SNP allele frequencies obtained in a genome-wide association study: Given a large number of minor allele frequencies from a case group of individuals diagnosed with a particular disease, together with the genomic data of a single target individual and statistics from a sizable reference dataset independently drawn from the same population, an attacker can determine with high confidence whether or not the target is in the case group. In this work we describe and analyze a simple attack that succeeds even if the summary statistics are significantly distorted, whether due to measurement error or noise intentionally introduced to protect privacy. Our attack only requires that the vector of distorted summary statistics is close to the vector of true marginals in &#x2113;<sub>1</sub> norm. Moreover, the reference pool required by previous attacks can be replaced by a single sample drawn from the underlying population. The new attack, which is not specific to genomics and which handles Gaussian as well as Bernouilli data, significantly generalizes recent lower bounds on the noise needed to ensure differential privacy (Bun, Ullman, and Vadhan, STOC 2014, Steinke and Ullman, 2015), obviating the need for the attacker to control the exact distribution of the data.
[Data privacy, Genomics, privacy, Statistics, Bernouilli data, Computer science, Privacy, privacy risks, measurement error, sizable reference dataset, Sociology, privacy protection, genome-wide association study, robust traceability, 1-way marginals, data privacy, noise intentionally, genomic data, Bioinformatics, SNP allele frequencies, fingerprinting, differential privacy, trace amounts]
Community Detection in General Stochastic Block models: Fundamental Limits and Efficient Algorithms for Recovery
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
New phase transition phenomena have recently been discovered for the stochastic block model, for the special case of two non-overlapping symmetric communities. This gives raise in particular to new algorithmic challenges driven by the thresholds. This paper investigates whether a general phenomenon takes place for multiple communities, without imposing symmetry. In the general stochastic block model SBM(n,p,W), n vertices are split into k communities of relative size {p<sub>i</sub>}<sub>i&#x2208;[k]</sub>, and vertices in community i and j connect independently with probability {W<sub>ij</sub>}<sub>i,j&#x2208;[k]</sub>. This paper investigates the partial and exact recovery of communities in the general SBM (in the constant and logarithmic degree regimes), and uses the generality of the results to tackle overlapping communities. The contributions of the paper are: (i) an explicit characterization of the recovery threshold in the general SBM in terms of a new f-divergence function D<sub>+</sub>, which generalizes the Hellinger and Chernoff divergences, and which provides an operational meaning to a divergence function analog to the KL-divergence in the channel coding theorem, (ii) the development of an algorithm that recovers the communities all the way down to the optimal threshold and runs in quasi-linear time, showing that exact recovery has no information-theoretic to computational gap for multiple communities, (iii) the development of an efficient algorithm that detects communities in the constant degree regime with an explicit accuracy bound that can be made arbitrarily close to 1 when a prescribed signal-to-noise ratio [defined in terms of the spectrum of diag(p)W] tends to infinity.
[partial-community recovery, graph theory, Stochastic processes, network theory (graphs), Complexity theory, optimal threshold, W) model, logarithmic degree regimes, Chernoff divergences, clustering algorithms, Clustering algorithms, recovery threshold, constant degree regimes, explicit accuracy bound, information measures, general stochastic block models, Mathematical model, stochastic processes, channel coding theorem, community detection, phase transition phenomena, explicit characterization, nonoverlapping symmetric communities, constant degree communities, Computational modeling, probability, exact-community recovery, Community detection, phase transitions, graph-based codes, overlapping communities, network vertices, Hellinger divergences, SBM (n, Computer science, stochastic block model, p, quasilinear time, pattern clustering, signal-to-noise ratio, KL-divergence, stochastic block models, f-divergence function, general stochastic block model, Signal to noise ratio, computational complexity]
How to Refute a Random CSP
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Let P be a k-ary predicate over a finite alphabet. Consider a random CSP(P) instance I over n variables with m constraints. When m &#x226B; n the instance will be unsatisfiable with high probability and we want to find a certificate of unsatisfiability. When P is the 3-ary OR predicate, this is the well-studied problem of refuting random 3-SAT formulas and an efficient algorithm is known only when m &#x226B; n3/2. Understanding the density required for refutation of other predicates is important in cryptography, proof complexity, and learning theory. Previously, it was known that for a k-ary predicate, having m &#x226B;&#x226B;n[k/2] constraints suffices for refutation. We give a criterion for predicates that often yields efficient refutation algorithms at much lower densities. Specifically, if P fails to support a t-wise uniform distribution, then there is an efficient algorithm that refutes random CSP(P) instances whp when m &#x226B; nt/2. Indeed, our algorithm will &#x201C;somewhat strongly&#x201D; refute the instance I, certifying Opt(I) &#x2264; 1 - &#x03A9;k(1). If t = k then we get the strongest possible refutation, certifying Opt(I) &#x2264; E[P] + o(1). This last result is new even for random k-SAT. Prior work on SDP hierarchies has given some evidence that efficient refutation of random CSP(P) may be impossible when m &#x226B; nt/2; thus there is an indication that our algorithm's dependence on m is optimal for every P, at least in the context of SDP hierarchies. As an application of our result, we falsify assumptions used to show hardness-of-learning in recent work of Daniely, Linial, and Shalev-Shwartz.
[formal languages, Terminology, random CSP(P) instance, probability, refutation, computability, Complexity theory, finite alphabet, Approximation methods, Physics, 3-ary OR predicate, Computer science, constraint satisfaction problems, Approximation algorithms, random 3-SAT formulas, Cryptography, k-ary predicate, unsatisfiability certificate, t-wise uniform distribution]
An O(1)-Approximation for Minimum Spanning Tree Interdiction
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Network interdiction problems are a natural way to study the sensitivity of a network optimization problem with respect to the removal of a limited set of edges or vertices. One of the oldest and best-studied interdiction problems is minimum spanning tree (MST) interdiction. Here, an undirected multigraph with nonnegative edge weights and positive interdiction costs on its edges is given, together with a positive budget B. The goal is to find a subset of edges R, whose total interdiction cost does not exceed B, such that removing R leads to a graph where the weight of an MST is as large as possible. Frederickson and Solis-Oba (SODA 1996) presented an O(log m)-approximation for MST interdiction, where m is the number of edges. Since then, no further progress has been made regarding approximations, and the question whether MST interdiction admits an O(1)-approximation remained open. We answer this question in the affirmative, by presenting a 14-approximation that overcomes two main hurdles that hindered further progress so far. Moreover, based on a well-known 2-approximation for the metric traveling salesman problem (TSP), we show that our O(1)-approximation for MST interdiction implies an O(1)-approximation for a natural interdiction version of metric TSP.
[Algorithm design and analysis, natural interdiction version, positive interdiction, approximation, metric TSP, Sensitivity analysis, trees (mathematics), nonnegative edge weights, Approximation methods, undirected multigraph, Optimization, minimum spanning tree interdiction, travelling salesman problems, network interdiction problems, network optimization problem, metric traveling salesman problem, MST interdiction, interdiction cost, Approximation algorithms]
Reality Distortion: Exact and Approximate Algorithms for Embedding into the Line
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We describe algorithms for the problem of minimum distortion embeddings of finite metric spaces into the real line (or a finite subset of the line). The time complexities of our algorithms are parametrized by the values of the minimum distortion, &#x03B4;, and the spread, &#x0394;, of the point set we are embedding. We consider the problem of finding the minimum distortion bijection between two finite subsets of IR. This problem was known to have an exact polynomial time solution when &#x03B4; is below a specific small constant, and hard to approximate within a factor of &#x03B4;1-E, when &#x03B4; is polynomially large. Let D be the largest adjacent pair distance, a value potentially much smaller than &#x0394;. Then we provide a &#x03B4;O(&#x03B4;2log2D)nO(1) time exact algorithm for this problem, which in particular yields a quasipolynomial running time for constant &#x03B4;, and polynomial D. For the more general problem of embedding any finite metric space (X, dX) into a finite subset of the line, Y , we provide a &#x0394;O(&#x03B4;2)(mn)O(1) time O(1)-approximation algorithm (where X = n and Y = m), which runs in polynomial time provided &#x03B4; is a constant and &#x0394; is polynomial. This in turn allows us to get a &#x0394;O(&#x03B4;2)(n)O(1) time O(1)-approximation algorithm for embedding (X, dX) into the continuous real line.
[finite subset, minimum distortion bijection, Metric Embedding, Fixed Parameter Tractable, Distortion, Extraterrestrial measurements, set theory, minimum distortion embedding problem, time complexities, &#x0394;O(&#x03B4;2)(n)O(1) time O(1)-approximation algorithm, Approximation Algorithms, Approximation methods, time exact algorithm, polynomial approximation, Approximation algorithms, finite metric space, Polynomials, reality distortion, polynomial time solution, Yttrium, computational complexity, finite metric spaces]
Polylogarithmic Approximations for the Capacitated Single-Sink Confluent Flow Problem
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
A single-sink confluent flow is a routing of multiple demands to a sink r such that any flow exiting a node v must use a single arc. Hence, a confluent flow routes on a tree within the network. In uncapacitated (or uniform-capacity) networks, there is an O(1)-approximation algorithm for demand maximization and a logarithmic approximation algorithm for congestion minimization [6]. We study the case of capacitated networks, where each node v has its own capacity μ(v). Indeed, it was recently shown that demand maximization is in approximable to within polynomial factors in capacitated networks [20]. We circumvent this lower bound in two ways. First, we prove that there is a polylogarithmic approximation algorithm for demand maximization in networks that satisfy the ubiquitous no-bottleneck assumption (NBA). Second, we show a bicriteria result for capacitated networks without the NBA: there is a polylog factor approximation guarantee for demand maximization provided we allow congestion 2. We model the capacitated confluent flows problem using a multilayer linear programming formulation. At the heart of our approach for demand maximization is a rounding procedure for flows on multilayer networks which can be viewed as a proposal algorithm for an extension of stable matchings. In addition, the demand maximization algorithms require, as a subroutine, an algorithm for approximate congestion minimization in a special class of capacitated networks that may be of independent interest. Specifically, we present a polylogarithmic approximation algorithm for congestion minimization in monotonic networks - those networks with the property that μ(u) &#x2264; μ(v) for each arc (u, v).
[approximation theory, confluent flow, multilayer linear programming formulation, Routing, Minimization, Nonhomogeneous media, linear programming, polylog factor approximation guarantee, Approximation methods, Combinatorial optimization, networks, demand maximization algorithms, stable matching, approximate congestion minimization, polylogarithmic approximation algorithm, O(1)-approximation algorithm, ubiquitous NBA, capacitated single-sink confluent flow problem, Approximation algorithms, Polynomials, Routing protocols, confluent flow route, computational complexity]
A Light Metric Spanner
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
It has long been known that d-dimensional Euclidean point sets admit (1+&#x03B5;)-stretch spanners with lightness W<sub>E</sub> = &#x03B5;-O(d), that is the total edge weight is at most WE times the weight of the minimum spanning tree of the set [DHN93]. Whether or not a similar result holds for metric spaces with low doubling dimension has remained an open problem. In this paper, we resolve the question in the affirmative, and show that doubling spaces admit(1 + &#x03B5;)-stretch spanners with lightness WD = (ddim /&#x03B5;)O(ddim).
[light metric spanner, low doubling dimension, open problem, Spanners, Buildings, trees (mathematics), minimum spanning tree, Extraterrestrial measurements, doubling dimension, Mathematics, metric spaces, set theory, Computer science, d-dimensional Euclidean point sets, stretch spanners, Yttrium, Erbium]
Near-Optimal Bounds on Bounded-Round Quantum Communication Complexity of Disjointness
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove a near optimal round-communication tradeoff for the two-party quantum communication complexity of disjointness. For protocols with r rounds, we prove a lower bound of Omega(n/r) on the communication required for computing disjointness of input size n, which is optimal up to logarithmic factors. The previous best lower bound was Omega(n/r&#x0302;2) due to Jain, Radhakrishnan and Sen. Along the way, we develop several tools for quantum information complexity, one of which is a lower bound for quantum information complexity in terms of the generalized discrepancy method. As a corollary, we get that the quantum communication complexity of any boolean function f is at most 2 &#x0302;O(QIC(f)), where QIC(f) is the prior-free quantum information complexity of f (with error 1/3).
[near-optimal bounds, Protocols, generalized discrepancy method, Boolean function, Complexity theory, Boolean algebra, prior-free quantum information complexity, near optimal round-communication, Computer science, Boolean functions, disjointness, Weapons, bounded-round quantum communication complexity, Quantum mechanics, protocols, quantum communication, logarithmic factors, two-party quantum communication complexity, computational complexity]
Hamiltonian Simulation with Nearly Optimal Dependence on all Parameters
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We present an algorithm for sparse Hamiltonian simulation whose complexity is optimal (up to log factors) as a function of all parameters of interest. Previous algorithms had optimal or near-optimal scaling in some parameters at the cost of poor scaling in others. Hamiltonian simulation via a quantum walk has optimal dependence on the sparsity at the expense of poor scaling in the allowed error. In contrast, an approach based on fractional-query simulation provides optimal scaling in the error at the expense of poor scaling in the sparsity. Here we combine the two approaches, achieving the best features of both. By implementing a linear combination of quantum walk steps with coefficients given by Bessel functions, our algorithm's complexity (as measured by the number of queries and 2-qubit gates) is logarithmic in the inverse error, and nearly linear in the product tau of the evolution time, the sparsity, and the magnitude of the largest entry of the Hamiltonian. Our dependence on the error is optimal, and we prove a new lower bound showing that no algorithm can have sub linear dependence on tau.
[Bessel function, Computational modeling, Heuristic algorithms, sparse Hamiltonian simulation, quantum walk, Complexity theory, fractional-query simulation, Bessel functions, Computer science, Quantum computing, near-optimal scaling, quantum computing, Hilbert space, Eigenvalues and eigenfunctions, Hamiltonian simulation, quantum algorithms]
Quantum Expander Codes
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We present an efficient decoding algorithm for constant rate quantum hyper graph-product LDPC codes which provably corrects adversarial errors of weight proportional to the code minimum distance, or equivalently to the square-root of the block length. The algorithm runs in time linear in the number of qubits, which makes its performance the strongest to date for linear-time decoding of quantum codes. The algorithm relies on expanding properties, not of the quantum code's factor graph directly, but of the factor graph of the original classical code it is constructed from.
[constant rate quantum hypergraph-product LDPC expander code, error correction codes, graph theory, parity check codes, Graph theory, Generators, linear-time decoding, Decoding, LDPC codes, Iterative decoding, decoding, weight proportional error correction code, quantum codes, Quantum computing, Expander codes, CSS codes, Robustness, quantum communication]
Robust Testing of Lifted Codes with Applications to Low-Degree Testing
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
A local tester for a code probabilistically views a small set of coordinates of a given word and based on this local view accepts code words with probability one while rejecting words far from the code with constant probability. A local tester for a code is said to be "robust" if the local views of the tester are far from acceptable views when the word being tested is far from the code. Robust testability of codes play a fundamental role in constructions of probabilistically checkable proofs where robustness is a critical element in composition. In this work we consider a broad class of codes, called lifted codes, that include codes formed by low-degree polynomials, and show that an almost natural test, extending a low-degree test proposed by Raz and Safra (STOC 1997), is robust. Our result is clean and general -- the robustness of the test depends only on the distance of the code being lifted, and is positive whenever the distance is positive. We use our result to get the first robust low-degree test that works when the degree of the polynomial being tested is more than half the field size. Our results also show that the high-rate codes of Guo et al. (ITCS 2013) are robustly locally testable with sub linear query complexity. Guo et al. Also show several other interesting classes of locally testable codes that can be derived from lifting and our result shows all such codes have robust testers, at the cost of a quadratic blow up in the query complexity of the tester. Of technical interest is an intriguing relationship between tensor product codes and lifted codes that we explore and exploit.
[Locally testable codes, Frequency modulation, tensor product code, Affine-invariance, error correction codes, polynomials, probability, query complexity, Probabilistic logic, sublinear query complexity, tensors, Complexity theory, product codes, low-degree polynomial, Tensile stress, error correcting code, Error-correcting codes, Robustness, Polynomials, constant probability, low-degree testing, lifted code robust testing, Testing]
Local Correlation Breakers and Applications to Three-Source Extractors and Mergers
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We introduce and construct a pseudorandom object which we call a local correlation breaker (LCB). Informally speaking, an LCB is a function that gets as input a sequence of r (arbitrarily correlated) random variables and an independent weak-source. The output of the LCB is a sequence of r random variables with the following property. If the i'th input random variable is uniform then the i'th output variable is uniform even given a bounded number of any other output variables. That is, an LCB uses the weak-source to break local correlations between random variables. Our construction of LCBs has applications to three-source extractors, mergers with weak-seeds, and a variant of non-malleable extractors, that we introduce.
[Correlation, Corporate acquisitions, Input variables, random variable, Entropy, pseudorandom object, local correlations, random number generation, non-malleable extractors, independent weak-source, mergers, nonmalleable extractors, Random variables, three-source extractors, Yttrium, Face, multi-source extractors, local correlation breakers]
Three-Source Extractors for Polylogarithmic Min-Entropy
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We continue the study of constructing explicit extractors for independent general weak random sources. The ultimate goal is to give a construction that matches what is given by the probabilistic method - an extractor for two independent n-bit weak random sources with min-entropy as small as log n + O(1). Previously, the best known result in the two-source case is an extractor by Bourgain [1], which works for min-entropy 0.49n; and the best known result in the general case is an earlier work of the author [2], which gives an extractor for a constant number of independent sources with min-entropy polylog(n). However, the constant in the construction of [2] depends on the hidden constant in the best known seeded extractor, and can be large; moreover the error in that construction is only 1/poly(n). In this paper, we make two important improvements over the result in [2]. First, we construct an explicit extractor for three independent sources on n bits with min-entropy k &#x2265; polylog(n). In fact, our extractor works for one source with poly-logarithmic min-entropy and another independent block source with two blocks each having poly-logarithmic min-entropy. This significantly improves previous constructions, and the next step would be to break the 0.49n barrier in two-source extractors. Second, we improve the error of the extractor from 1/poly(n) to 2-k&#x03A9;(1), which is almost optimal and crucial for cryptographic applications. Some of our techniques may be of independent interests.
[cryptographic applications, independent sources, independent source, probability, Probabilistic logic, extractor, Entropy, randomness, Distributed computing, polylogarithmic min-entropy, Computer science, min-entropy polylog(n), independent general weak random sources, Bipartite graph, three-source extractors, independent n-bit weak random sources, Yttrium, Cryptography, probabilistic method, computational complexity, minimum entropy methods, number theory, two-source extractors]
Beyond the Central Limit theorem: Asymptotic Expansions and Pseudorandomness for Combinatorial Sums
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove a new asymptotic expansion in the central limit theorem for sums of discrete independent random variables. The classical central limit theorem asserts that if {X<sub>i</sub>}<sub>i=1</sub>n is a sequence of i.i.d. random variables, then S = &#x03A3;<sub>i=1</sub>n X<sub>i</sub> converges to a Gaussian whose first two moments match those of . Further, the rate of convergence is O(n-1/2). Roughly speaking, asymptotic expansions of the central limit theorem show that by considering a family of limiting distributions specified by &#x2265; 2 moments (k = 2 corresponds to Gaussians) and matching the first moments of to such a limiting distribution, one can achieve a convergence of n-(-1)/2. While such asymptotic expansions have been known since Crame&#x0301;r [1], they did not apply to discrete and non-identical random variables. Further, the error bounds in nearly all cases was non-explicit (in their dependence on {X<sub>i</sub>}), thus limiting their applicability. In this work, we prove a new asymptotic expansions of the central limit theorem which applies to discrete and non-identical random variables and the error bounds are fully explicit. Given the wide applicability of the central limit theorem in probability theory and theoretical computer science, we believe that this new asymptotic expansion theorem will be applicable in several settings. As a main application in this paper, we give an application in derandomization: Namely, we construct PRGs for the class of combinatorial sums, a class of functions first studied by [2] and which generalize many previously studied classes such as combinatorial rectangles [3], small-biased spaces [4] and modular sums [5] among others. A function f : [m],n &#x2192; {0, 1} is said to be a combinatorial sum if there exists functions f<sub>1</sub>,..., f<sub>n</sub> : [m] &#x2192; {0, 1} such that (x<sub>1</sub>, ... , x<sub>n</sub>) = f<sub>1</sub>(x<sub>1</sub>) + ... + ,f<sub>n</sub>(x,<sub>n</sub>). For this class, we give a seed length of (log + log3/2(n/&#x2208;)), thus improving upon [2] whenever &#x03F5; &#x2264; 2-(log n)3/4.
[Measurement, Limiting, probability theory, asymptotic expansions, PRG, probability, asymptotic expansion theorem, Gaussian distribution, derandomization, n(k-1)/2 convergence, combinatorial sum, Convergence, Computer science, Reactive power, theoretical computer science, central limit theorem, Central limit theorem, discrete independent random variable, pseudorandomness, Random variables, nonidentical random variable, computational complexity, asymptotic expansion]
Pseudorandomness via the Discrete Fourier Transform
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We present a new approach to constructing unconditional pseudorandom generators against classes of functions that involve computing a linear function of the inputs. We give an explicit construction of a pseudorandom generator that fools the discrete Fourier transforms of linear functions with seed-length that is nearly logarithmic (up to polyloglog factors) in the input size and the desired error parameter. Our result gives a single pseudorandom generator that fools several important classes of tests computable in log space that have been considered in the literature, including half spaces (over general domains), modular tests and combinatorial shapes. For all these classes, our generator is the first that achieves near logarithmic seed-length in both the input length and the error parameter. Getting such a seed-length is a natural challenge in its own right, which needs to be overcome in order to derandomize RL -- a central question in complexity theory. Our construction combines ideas from a large body of prior work, ranging from a classical construction of [1] to the recent gradually increasing independence paradigm of [2] -- [4], while also introducing some novel analytic machinery which might find other applications.
[complexity theory, Shape, discrete Fourier transforms, Discrete Fourier transforms, probability, Chernoff bound, Gaussian distribution, error parameter, Fourier transform, Generators, Electronic mail, random number generation, limited independence, discrete Fourier transform, unconditional pseudorandom generators, halfspaces, linear function, Polynomials, pseudorandomness, Random variables, computational complexity]
Tight Bounds on Low-Degree Spectral Concentration of Submodular and XOS Functions
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Submodular and fractionally subadditive (or equivalently XOS) functions play a fundamental role in combinatorial optimization, algorithmic game theory and machine learning. Motivated by learnability of these classes of functions from random examples, we consider the question of how well such functions can be approximated by low-degree polynomials in &#x2113;<sub>2</sub> norm over the uniform distribution. This question is equivalent to understanding the concentration of Fourier weight on low-degree coefficients, a central concept in Fourier analysis. Denoting the smallest degree sufficient to approximate f in &#x2113;<sub>2</sub> norm within &#x2208; by deg<sub>&#x2208;</sub>(&#x2113;<sub>2</sub>)(f), we show that : For any submodular function f : {0, 1}n &#x2192; [0, 1], deg<sub>&#x2208;</sub>(&#x2113;<sub>2</sub>)(f) = O(log(1/&#x2208;)/&#x2208;4/5) and there is a submodular function that requires degree &#x03A9;(1/&#x2208;4/5). : For any XOS function f : {0, 1} &#x2192; [0, 1], deg<sub>&#x2208;</sub>(&#x2113;<sub>2</sub>) (f) = O(1/&#x2208;) and there exists an XOS function that requires degree &#x03A9;(1/&#x2208;). This improves on previous approaches that all showed an upper bound of O(1/&#x2208;2) for submodular [CKKL12], [FKV13], [FV13] and XOS [FV13] functions. The best previous lower bound was &#x03A9;(1/&#x2208;2/3) for monotone submodular functions [FKV13]. Our techniques reveal new structural properties of submodular and XOS functions and the upper bounds lead to nearly optimal PAC learning algorithms for these classes of functions.
[Algorithm design and analysis, FV13 function, low-degree coefficients, Machine learning algorithms, submodular function, l<sub>2</sub> norm, Approximation methods, low-degree spectral concentration, CKKL12 function, uniform distribution, low-degree polynomials, fractionally subadditive functions, function approximation, polynomial approximation, function class learnability, Polynomials, learning (artificial intelligence), nearly-optimal PAC learning algorithms, spectral concentration, Fourier analysis, upper bound, lower bound, Game theory, XOS function, Upper bound, Fourier weight, monotone submodular functions, tight bounds, XOS functions, Approximation algorithms, computational complexity, FKV13 function]
Equivalence of Deterministic Top-Down Tree-to-String Transducers is Decidable
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show that equivalence of deterministic top-down tree-to-string transducers is decidable, thus solving a long standing open problem in formal language theory. We also present efficient algorithms for subclasses: polynomial time for total transducers with unary output alphabet (over a given top-down regular domain language), and co-randomized polynomial time for linear transducers, these results are obtained using techniques from multi-linear algebra. For our main result, we prove that equivalence can be certified by means of inductive invariants using polynomial ideals. This allows us to construct two semi-algorithms, one searching for a proof of equivalence, one for a witness of non-equivalence.
[inductive invariants, Input variables, multilinear algebra, Electronic mail, corandomized polynomial time, equivalence proof, polynomial ideals, Presses, nonequivalence witness, Polynomials, polynomial time, theorem proving, formal language theory, linear algebra, linear transducers, deterministic top-down tree-to-string transducers, Transducers, formal languages, equivalence, unary output alphabet, transducers, XML, Vegetation, semialgorithms, automata]
FO Model Checking on Posets of Bounded Width
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Over the past two decades the main focus of research into first-order (FO) model checking algorithms have been sparse relational structures-culminating in the FPT-algorithm by Grohe, Kreutzer and Siebertz for FO model checking of nowhere dense classes of graphs [STOC'14], with dense structures starting to attract attention only recently. Bova, Ganian and Szeider [CSL-LICS'14] initiated the study of the complexity of FO model checking on partially ordered sets (posets). Bova, Ganian and Szeider showed that model checking existential FO logic is fixed-parameter tractable (FPT) on posets of bounded width, where the width of a poset is the size of the largest antichain in the poset. The existence of an FPT algorithm for general FO model checking on posets of bounded width, however, remained open. We resolve this question in the positive by giving an algorithm that takes as its input an n-element poset P of width w and an FO logic formula &#x03C6;, and determines whether &#x03C6; holds on P in time f(&#x03C6;, w) &#x00B7; n2.
[Algorithm design and analysis, posets, fixed-parameter tractable, graph theory, first-order logic, sparse relational structures-culminating, algorithmic meta-theorems, parameterized complexity, Data structures, Complexity theory, Electronic mail, set theory, FO model checking, formal logic, Boolean functions, graphs, formal verification, partially ordered sets, FO logic formula, Games, FPT-algorithm, Model checking, first-order model checking algorithms, bounded width]
Satisfiability of Ordering CSPs above Average is Fixed-Parameter Tractable
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We study the satisfiability of ordering constraint satisfaction problems (CSPs) above average. We prove the conjecture of Gutin, van Iersel, Mnich, and Yeo that the satisfiability above average of ordering CSPs of arity k is fixed-parameter tractable for every k. Previously, this was only known for k=2 and k=3. We also generalize this result to more general classes of CSPs, including CSPs with predicates defined by linear equations. To obtain our results, we prove a new Bonami-type inequality for the Efron -- Stein decomposition. The inequality applies to functions defined on arbitrary product probability spaces. In contrast to other variants of the Bonami Inequality, it does not depend on the mass of the smallest atom in the probability space. We believe that this inequality is of independent interest.
[Atomic measurements, CSP, fixed-parameter tractable, advantage over random, Efron--Stein decomposition, Lattices, probability, ordering CSP, computability, fixed-parameter tractability, Approximation methods, combinatorial optimization, constraint satisfaction problems, satisfiability, Bonami-type inequality, Games, Approximation algorithms, Polynomials, Kernel, arbitrary product probability spaces]
Parameterizing the Permanent: Genus, Apices, Minors, Evaluation Mod 2k
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We identify and study relevant structural parameters for the problem PerfMatch of counting perfect matchings in a given input graph C. These generalize the well-known tractable planar case, and they include the genus of C, its apex number (the minimum number of vertices whose removal renders C planar), and its Hadwiger number (the size of a largest clique minor). To study these parameters, we first introduce the notion of combined matchgates, a general technique that bridges parameterized counting problems and the theory of so-called Holants and matchgates: Using combined matchgates, we can simulate certain nonexisting gadgets F as linear combinations of L = O(1) existing gadgets. If a graph C features k occurrences of F, we can then reduce C to tk graphs that feature only existing gadgets, thus enabling parameterized reductions. As applications of this technique, we simplify known 4gnO(1) time algorithms for PerfMatch on graphs of genus g. Orthogonally to this, we show #W[1]-hardness of the permanent on k-apex graphs, implying its &#x2295;W[1]-hardness under the Hadwiger number. Additionally, we rule out no(k/ log k) time algorithms under the counting exponential-time hypothesis #ETH. Finally, we use combined matchgates to prove $W[1]-hardness of evaluating the permanent modulo 2k, complementing an O(n4k-3) time algorithm by Valiant and answering an open question of Bj&#x03C5;&#x0308;rklund. We also obtain a lower bound of n&#x03A9;(k/ log k) under the parity version $ETH of the exponential-time hypothesis.
[Transmission line matrix methods, $ETH, perfect matchings, graph theory, Complexity theory, modular counting complexity, Hadwiger number, parameterized counting complexity, Polynomials, permanent 2k modulo evaluation, Bipartite graph, graph minors, O(n4k-3) time algorithm, Partitioning algorithms, Structural engineering, lower bound, Holant problem, Computer science, 1, apex number, genus, permanent, matchgates, $W[1, computational complexity]
Isomorphism Testing for Graphs of Bounded Rank Width
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We give an algorithm that, for every fixed k, decides isomorphism of graphs of rank width at most k in polynomial time. As the rank width of a graph is bounded in terms of its clique width, we also obtain a polynomial time isomorphism test for graph classes of bounded clique width.
[Context, bounded clique width, Computational modeling, graph classes, graph theory, rank width, polynomial time isomorphism testing, Matrix decomposition, canonical decomposition theorems, Machinery, bounded rank width, connectivity functions, graph isomorphism, Polynomials, Yttrium, clique width, Testing, computational complexity]
An Average-Case Depth Hierarchy Theorem for Boolean Circuits
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove an average-case depth hierarchy theorem for Boolean circuits over the standard basis of AND, OR, and NOT gates. Our hierarchy theorem says that for every d &#x2265; 2, there is an explicit n-variable Boolean function f, computed by a linear-size depth-d formula, which is such that any depth-(d - 1) circuit that agrees with f on (1/2 + o<sub>n</sub>(1)) fraction of all inputs must have size exp(n&#x03A9;(1/d)). This answers an open question posed by Hastad in his Ph.D. thesis [Has86b]. Our average-case depth hierarchy theorem implies that the polynomial hierarchy is infinite relative to a random oracle with probability 1, confirming a conjecture of Hastad [Has86a], Cai [Cai86], and Babai [Bab87]. We also use our result to show that there is no &#x201C;approximate converse&#x201D; to the results of Linial, Mansour, Nisan [LMN93] and Boppana [Bop97] on the total influence of constant-depth circuits, thus answering a question posed by Kalai [Kal12] and Hatami [Hat14]. A key ingredient in our proof is a notion of random projections which generalize random restrictions.
[Correlation, average-case depth hierarchy theorem, depth hierarchy theorem, average-case, random projections, Computational modeling, probability, Polynomial Hierarchy, Complexity theory, Boolean functions, n-variable Boolean function, Logic gates, Polynomials, Small-depth circuits, Boolean circuits, Integrated circuit modeling, logic circuits]
A Faster Cutting Plane Method and its Implications for Combinatorial and Convex Optimization
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
In this paper we improve upon the running time for finding a point in a convex set given a separation oracle. In particular, given a separation oracle for a convex set K &#x2282; Rn that is contained in a box of radius R we show how to either compute a point in K or prove that K does not contain a ball of radius &#x03F5; using an expected O(n log(nR/&#x03F5;)) evaluations of the oracle and additional time O(n3 logO(1)(nR/&#x03F5;)). This matches the oracle complexity and improves upon the O(n&#x03C9;+1 log(nR/&#x03F5;)) additional time of the previous fastest algorithm achieved over 25 years ago by Vaidya [91] for the current value of the matrix multiplication constant w &lt;; 2.373 [98], [36] when R/&#x03F5; = O(poly(n)). Using a mix of standard reductions and new techniques we show how our algorithm can be used to improve the running time for solving classic problems in continuous and combinatorial optimization. In particular we provide the following running time improvements: &#x00B7; Submodular Function Minimization: n is the size of the ground set, M is the maximum absolute value of function values and EO is the time for function evaluation. Our weakly and strongly polynomial time algorithms have a running time of O(n2 log nM &#x00B7; EO + n3 logO(1) nM) and O(n3 log2 n &#x00B7; EO + n4 logO(1) n), improving upon the previous best of O((n4 &#x00B7; EO + n5)logM) and O(n5 &#x00B7; EO + n6) respectively. &#x00B7; Submodular Flow: n = |V|, m = |E|, C is the maximum edge cost in absolute value and U is maximum edge capacity in absolute value. We obtain a faster weakly polynomial running time of O(n2 log nCU &#x00B7; EO + n3 logO(1) nCU), improving upon the previous best of O(mn5 log nU &#x00B7; EO) and O (n4h min {log C, log U}) from 15 years ago by a factor of O&#x0303;(n4). We also achieve faster strongly polynomial time algorithms as a consequence of our result on submodular minimization. &#x00B7; Matroid Intersection: n is the size of the ground set, r is the maximum size of independent sets, M is the maximum absolute value of element weight, T<sub>rank</sub> and T<sub>ind</sub> are the time for each rank and independence oracle query. We obtain a running time of O((nr log2 nT<sub>rank</sub>+n3 logO(1) n) log nM) and O((n2 log nT<sub>ind</sub>+n3 logO(1) n) log nM), achieving the first quadratic bound on the query complexity for the independence and rank oracles. In the unweighted case, this is the first improvement since 1986 for independence oracle. &#x00B7; Semidefinite Programming: n is the number of constraints, m is the number of dimensions and S is the total number of non-zeros in the constraint matrices. We obtain a running time of O(n(n2 + m&#x03C9; + S)), improving upon the previous best of O&#x0303;(n(n&#x03C9; + m&#x03C9; + S)) for the regime S is small.
[Algorithm design and analysis, combinatorial mathematics, cutting plane method, separation oracle, Complexity theory, submodular minimization, Ellipsoids, Optimization, semidefinite programming, Submodular Flow, polynomial time algorithms, oracle complexity, Matroid Intersection, Polynomials, Convex functions, Ellipsoid Method, Minimization, convex programming, Semidefinite Programming, mathematical programming, matrix multiplication, combinatorial optimization, convex optimization, Submodular Function Minimization, Cutting Plane Method, computational complexity]
Lower Bounds for Clique vs. Independent Set
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove an &#x03C9;(log n) lower bound on the Conon deterministic communication complexity of the Clique vs. Independent Set problem introduced by Yannakakis (STOC 1988, JCSS 1991). As a corollary, this implies super polynomial lower bounds for the Alon - Saks - Seymour conjecture in graph theory. Our approach is to first exhibit a query complexity separation for the decision tree analogue of the UP vs. coNP question - namely, unambiguous DNF width vs. CNF width - and then "lift" this separation over to communication complexity using a result from prior work.
[Protocols, graph theory, Graph theory, unambiguous DNF width, Complexity theory, set theory, superpolynomial lower bounds, communication complexity, clique set problem, query complexity separation, independent set problem, Computer science, query processing, Upper bound, Boolean functions, CNF width, conondeterministic communication complexity, decision tree analogue, decision trees, Decision trees, &#x03C9;(log n) lower bound, Alon-Saks-Seymour conjecture, computational complexity]
Deterministic Communication vs. Partition Number
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show that deterministic communication complexity can be super logarithmic in the partition number of the associated communication matrix. We also obtain near-optimal deterministic lower bounds for the Clique vs. Independent Set problem, which in particular yields new lower bounds for the log-rank conjecture. All these results follow from a simple adaptation of a communication-to-query simulation theorem of Raz and McKenzie (Combinatorica 1999) together with lower bounds for the analogous query complexity questions.
[associated communication matrix, Protocols, log-rank conjecture, Complexity theory, set theory, near-optimal deterministic lower bounds, communication-to-query simulation theorem, communication complexity, deterministic communication complexity, analogous query complexity questions, matrix algebra, independent set problem, Computer science, Upper bound, Boolean functions, clique, Decision trees, Yttrium, partition number, number theory]
New Unconditional Hardness Results for Dynamic and Online Problems
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
There has been a resurgence of interest in lower bounds whose truth rests on the conjectured hardness of well known computational problems. These conditional lower bounds have become important and popular due to the painfully slow progress on proving strong unconditional lower bounds. Nevertheless, the long term goal is to replace these conditional bounds with unconditional ones. In this paper we make progress in this direction by studying the cell probe complexity of two conjectured to be hard problems of particular importance: matrix-vector multiplication and a version of dynamic set disjointness known as Patrascu's Multiphase Problem. We give improved unconditional lower bounds for these problems as well as introducing new proof techniques of independent interest. These include a technique capable of proving strong threshold lower bounds of the following form: If we insist on having a very fast query time, then the update time has to be slow enough to compute a lookup table with the answer to every possible query. This is the first time a lower bound of this type has been proven.
[strong unconditional lower bounds, cell probe complexity, Patrascu multiphase problem, Computational modeling, dynamic problems, query time, Data structures, online problems, Complexity theory, proof techniques, lower bounds, matrix-vector multiplication, Computer science, dynamic set disjointness, matrix multiplication, conjectured hardness, threshold lower bounds, Polynomials, Yttrium, unconditional hardness results, Probes, update time, computational complexity, cell-probe model]
Tight Hardness of the Non-commutative Grothendieck Problem
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove that it is NP-hard to approximate the non-commutative Grothendieck problem to within any constant factor larger than one-half, which matches the approximation ratio of the algorithm of Naor, Regev, and Vidick (STOC'13). Our proof uses an embedding of finite-dimensional Hilbert spaces into the space of matrices endowed with the trace norm with the property that the image of standard basis vectors is longer than that of unit vectors with no large coordinates.
[approximation ratio, approximation theory, NP-hard, Naor-Regev-and-Vidick algorithm, noncommutative Grothendieck problem, Hilbert spaces, Linear matrix inequalities, standard basis vector, Approximation methods, Standards, Computer science, semidefinite programming, hardness, vectors, finite-dimensional Hilbert space, Algebra, Games, Approximation algorithms, Grothendieck inequality, computational complexity]
No Small Linear Program Approximates Vertex Cover within a Factor 2 -- e
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
The vertex cover problem is one of the most important and intensively studied combinatorial optimization problems. Khot and Regev [30], [31] proved that the problem is NP-hard to approximate within a factor 2 - &#x03B5;, assuming the Unique Games Conjecture (UGC). This is tight because the problem has an easy 2-approximation algorithm. Without resorting to the UGC, the best in approximability result for the problem is due to Dinur and Safra [16], [17]: vertex cover is NP-hard to approximate within a factor 1.3606. We prove the following unconditional result about linear programming (LP) relaxations of the problem: every LP relaxation that approximates vertex cover within a factor of 2 - &#x03B5; has super-polynomially many inequalities. As a direct consequence of our methods, we also establish that LP relaxations (as well as SDP relaxations) that approximate the independent set problem within any constant factor have super-polynomially many inequalities.
[NP-hard, combinatorial mathematics, vertex cover approximation, Predictive models, linear programming, Electronic mail, set theory, Approximation methods, independent set, unique games conjecture, LP relaxations, relaxation theory, SDP relaxations, easy 2-approximation algorithm, linear programming relaxations, Polynomials, approximation theory, vertex cover, game theory, hardness of approximation, Linear programming, independent set problem approximation, Computer science, small linear program approximation, UGC, extended formulations, Games, combinatorial optimization problems, computational complexity]
Approximate Modularity
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
A set function on a ground set of size n is approximately modular if it satisfies every modularity requirement to within an additive error, approximate modularity is the set analog of approximate linearity. In this paper we study how close, in additive error, can approximately modular functions be to truly modular functions. We first obtain a polynomial time algorithm that makes O(n2 log n) queries to any approximately modular function to reconstruct a modular function that is O(&#x221A;n)-close. We also show an almost matching lower bound: any algorithm world need super polynomially many queries to construct a modular function that is o(&#x221A;(n/log n))-close. In a striking contrast to these near-tight computational reconstruction bounds, we then show that for any approximately modular function, there exists a modular function that is O(log n)-close.
[ground set, set analog, Additives, approximate modularity, polynomials, modularity requirement, approximately modular functions, set theory, Approximation methods, polynomial time algorithm, duality, approximate linearity, modularity, super polynomially, Linearity, additive error, Approximation algorithms, Polynomials, set function, Yttrium, probabilistic method, Manganese]
Trading Query Complexity for Sample-Based Testing and Multi-testing Scalability
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show that every non-adaptive property testing algorithm making a constant number of queries, over a fixed alphabet, can be converted to a sample-based (as per [Gold Reich and Ron, 2015]) testing algorithm whose average number of queries is a fixed, smaller than 1, power of n. Since the query distribution of the sample-based algorithm is not dependent at all on the property, or the original algorithm, this has many implications in scenarios where there are many properties that need to be tested for concurrently, such as testing (relatively large) unions of properties, or converting a Merlin-Arthur Proximity proof (as per [Gur and Rothblum, 2013]) to a proper testing algorithm. The proof method involves preparing the original testing algorithm for a combinatorial analysis. For the analysis we develop a structural lemma for hyper graphs that may be of independent interest. When analyzing a hyper graph that was extracted from a 2-sided test, it allows for finding generalized sunflowers that provide for a large-deviation type analysis. For 1-sided tests the bounds can be improved further by applying Janson's inequality directly over our structures.
[Algorithm design and analysis, 2-sided test extraction, structural lemma, Scalability, graph theory, query complexity, 1-sided tests, Complexity theory, fixed alphabet, sample-based testing, query processing, sample-based testing algorithm, generalized sunflowers, combinatorial analysis, nonadaptive property testing algorithm, Janson inequality, Polynomials, query distribution, Testing, hypergraph analysis, sampling, average query number, property testing, Indexes, Merlin-Arthur proximity proof, large-deviation type analysis, Computer science, multitesting scalability, hypergraphs, constant queries, computational complexity]
Optimal Algorithms and Lower Bounds for Testing Closeness of Structured Distributions
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We give a general unified method that can be used for L<sub>1</sub> closeness testing of a wide range of univariate structured distribution families. More specifically, we design a sample optimal and computationally efficient algorithm for testing the equivalence of two unknown (potentially arbitrary) univariate distributions under the Ak-distance metric: Given sample access to distributions with density functions p, q : I &#x2192; R, we want to distinguish between the cases that p = q and &#x2225;p - q&#x2225;<sub>Ak</sub> &#x2265; &#x2208; with probability at least 2/3. We show that for any k &#x2265; 2, &#x2208; &gt; 0, the optimal sample complexity of the Ak-closeness testing problem is &#x0398;(max{k4/5/&#x2208;6/5, k1/2/&#x2208;2}). This is the first o(k) sample algorithm for this problem, and yields new, simple L1 closeness testers, in most cases with optimal sample complexity, for broad classes of structured distributions.
[Measurement, A<sub>k</sub>-distance metric, univariate distributions, optimal sample complexity, Complexity theory, Partitioning algorithms, statistical distributions, univariate structured distribution families, density functions, Upper bound, distribution testing, Probability density function, A<sub>k</sub>-closeness testing problem, Polynomials, L<sub>1</sub> closeness testing, structured distributions, general unified method, testing closeness, Testing, computational complexity]
On the Structure, Covering, and Learning of Poisson Multinomial Distributions
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
An (n, k)-Poisson Multinomial Distribution (PMD) is the distribution of the sum of n independent random vectors supported on the set Bk={e<sub>1</sub>,...,ek} of standard basis vectors in Rk. We prove a structural characterization of these distributions, showing that, for all &#x03B5; &gt; 0, any (n, k)-Poisson multinomial random vector is &#x03B5;-close, in total variation distance, to the sum of a discretized multidimensional Gaussian and an independent (poly(k/&#x03B5;), k)-Poisson multinomial random vector. Our structural characterization extends the multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to all approximation requirements &#x03B5;. In particular, it overcomes factors depending on log n and, importantly, the minimum Eigen value of the PMD's covariance matrix. We use our structural characterization to obtain an &#x03B5;-cover, in total variation distance, of the set of all (n, k)-PMDs, significantly improving the cover size of Daskalakis and Papadimitriou, and obtaining the same qualitative dependence of the cover size on n and &#x03B5; as the k=2 cover of Daskalakis and Papadimitriou. We further exploit this structure to show that (n, k)-PMDs can be learned to within &#x03B5; in total variation distance from O&#x0303;<sub>k</sub>(1/&#x03B5;) samples, which is near-optimal in terms of dependence on &#x03B5; and independent of n. In particular, our result generalizes the single-dimensional result of Daskalakis, Diakonikolas and Servedio for Poisson binomials to arbitrary dimension. Finally, as a corollary of our results on PMDs, we give a O&#x0303;k(1/&#x03B5;2) sample algorithm for learning (n, k)-sums of independent integer random variables (SIIRVs), which is near-optimal for constant k.
[covariance matrices, discretized multidimensional Gaussian, Poisson binomial, Multivariate statistics, Poisson multinomial distribution, sums of independent integer random variable, Gaussian distribution, random vector, Approximation methods, Poisson distribution, Covariance matrices, Standards, eigenvalues and eigenfunctions, covariance matrix, Limit theorem, Learning, Applied probability, Games, Polynomials, Eigenvalues and eigenfunctions, Yttrium, Structure, eigenvalue]
Uniform Generation of Random Regular Graphs
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We develop a new approach for uniform generation of combinatorial objects, and apply it to derive a uniform sampler REG for d-regular graphs. REG can be implemented such that each graph is generated in expected time O(nd3), provided that d = o(&#x221A;n). Our result significantly improves the previously best uniform sampler, which works efficiently only when d = O(n1/3), with essentially the same running time for the same d. We also give a linear-time approximate sampler REG*, which generates a random d-regular graph whose distribution differs from the uniform by o(1) in total variation distance, when d = o(&#x221A;n).
[Algorithm design and analysis, switching, approximation theory, graph theory, Switches, regular graphs, Approximation methods, Markov chain, uniform sampler REG, random d-regular graph, Markov processes, Approximation algorithms, Silicon, uniform generation, combinatorial objects, Time complexity, d-regular graphs, linear-time approximate sampler REG]
Symbolic Integration and the Complexity of Computing Averages
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We study the computational complexity of several natural problems arising in statistical physics and combinatorics. In particular, we consider the following problems: the mean magnetization and mean energy of the Ising model (both the ferromagnetic and the anti-ferromagnetic settings), the average size of an independent set in the hard core model, and the average size of a matching in the monomer-dimer model. We prove that for all non-trivial values of the underlying model parameters, exactly computing these averages is #P-hard. In contrast to previous results of Sinclair and Srivastava (2013) for the mean magnetization of the ferromagnetic Ising model, our approach does not use any Lee-Yang type theorems about the complex zeros of partition functions. Indeed, it was due to the lack of suitable Lee-Yang theorems for models such as the anti-ferromagnetic Ising model that some of the problems we study here were left open by Sinclair and Srivastava. In this paper, we instead use some relatively simple and well-known ideas from the theory of automatic symbolic integration to complete our hardness reductions.
[hard core model, automatic symbolic integration, #P-hard, Statistical Mechanics, Magnetic cores, graph theory, monomer-dimer model, partition functions, Counting Problems, magnetisation, ferromagnetic Ising model, monomer-dimer, symbol manipulation, Ising model, mean magnetization, Lee-Yang type theorems, combinatorics, Computational Complexity, Computational modeling, Magnetization, mean energy, Computational complexity, Physics, Interpolation, #P-hardness, statistical physics, hardness reductions, computational complexity]
The Complexity of General-Valued CSPs
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
An instance of the Valued Constraint Satisfaction Problem (VCSP) is given by a finite set of variables, a finite domain of labels, and a sum of functions, each function depending on a subset of the variables. Each function can take finite values specifying costs of assignments of labels to its variables or the infinite value, which indicates an infeasible assignment. The goal is to find an assignment of labels to the variables that minimizes the sum. We study, assuming that P &#x2260; NP, how the complexity of this very general problem depends on the set of functions allowed in the instances, the so-called constraint language. The case when all allowed functions take values in {0, &#x221E;} corresponds to ordinary CSPs, where one deals only with the feasibility issue and there is no optimization. This case is the subject of the Algebraic CSP Dichotomy Conjecture predicting for which constraint languages CSPs are tractable (i.e. solvable in polynomial time) and for which NP-hard. The case when all allowed functions take only finite values corresponds to finite-valued CSP, where the feasibility aspect is trivial and one deals only with the optimization issue. The complexity of finite-valued CSPs was fully classified by Thapper and Zivny. An algebraic necessary condition for tractability of a general-valued CSP with a fixed constraint language was recently given by Kozik and Ochremiak. As our main result, we prove that if a constraint language satisfies this algebraic necessary condition, and the feasibility CSP (i.e. the problem of deciding whether a given instance has a feasible solution) corresponding to the VCSP with this language is tractable, then the VCSP is tractable. The algorithm is a simple combination of the assumed algorithm for the feasibility CSP and the standard LP relaxation. As a corollary, we obtain that a dichotomy for ordinary CSPs would imply a dichotomy for general-valued CSPs.
[complexity, NP-hard, algebraic CSP dichotomy conjecture, valued constraint satisfaction problem, finite-valued CSP, dichotomy, fractional polymorphism, Linear programming, algebra, Complexity theory, Electronic mail, Standards, Computer science, Valued constraint satisfaction problem, constraint satisfaction problems, general-valued CSP, Cost function, polynomial time, constraint language, computational complexity]
A Holant Dichotomy: Is the FKT Algorithm Universal?
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove a complexity dichotomy for complex-weighted Holant problems with an arbitrary set of symmetric constraint functions on Boolean variables. In the study of counting complexity, such as #CSP, there are problems which are #P-hard over general graphs but P-time solvable over planar graphs. A recurring theme has been that a holographic reduction [36] to FKT precisely captures these problems. This dichotomy answers the question: Is this a universal strategy? Surprisingly, we discover new planar tractable problems in the Holant framework (which generalizes #CSP) that are not expressible by a holographic reduction to FKT. In particular, the putative form of a dichotomy for planar Holant problems is false. Nevertheless, we prove a dichotomy for #CSP2, a variant of #CSP where every variable appears even times, that the presumed universality holds for #CSP2. This becomes an important tool in the proof of the full dichotomy, which refutes this universality in general. The full dichotomy says that the new P-time algorithms and the strategy of holographic reductions to FKT together are universal for these locally defined counting problems. As a special case of our new planar tractable problems, counting perfect matchings (#PM) over k-uniform hypergraphs is P-time computable when the incidence graph is planar and k &#x2265; 5. The same problem is #P-hard when k = 3 or k = 4, also a consequence of the dichotomy. More generally, over hypergraphs with specified hyperedge sizes and the same planarity assumption, #PM is P-time computable if the greatest common divisor (gcd) of all hyperedge sizes is at least 5.
[Computers, Holant dichotomy, complex-weighted Holant problem, graph theory, Counting Problems, planar graph, planar tractable problem, Systematics, P-time algorithm, Polynomials, Boolean variable, k-uniform hypergraph, Computational Complexity, #P-hard problem, Dichotomy Theorem, Holant Problems, complexity dichotomy, Boolean algebra, Computational complexity, FKT, Holographic Algorithms, Computer science, FKT algorithm, #CSP2, computational complexity]
Sample (x) = (a*x&lt;=t) is a Distinguisher with Probability 1/8
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
A random sampling function Sample: U &#x2192; {0, 1} for a key universe U is a distinguisher with probability. If for any given assignment of values v (x) to the keys x &#x2208; U, including at least one non-zero v (x) &#x2260; 0, the sampled sum &#x03A3;{v(x) | x &#x2208; U &#x2227; Sample(x) = 1} is non-zero with probability at least &#x03B1;. Here the key values may come from any commutative monoid (addition is commutative and associative and zero is neutral). Such distinguishers were introduced by Vazirani [PhD thesis 1986], and Naor and Naor used them for their small bias probability spaces [STOC'90]. Constant probability distinguishers are used for testing in contexts where the key values are not computed directly, yet where the sum is easily computed. A simple example is when we get a stream of key value pairs (x<sub>1</sub>, v<sub>1</sub>), (x<sub>2</sub>, v<sub>2</sub>), ..., (x<sub>n</sub>, v<sub>n</sub>) where the same key may appear many times. The accumulated value of key x is v(x) = &#x03A3;{v<sub>1</sub> | x<sub>i</sub> = x}. For space reasons, we may not be able to maintain x(x) for every key x, but the sampled sum is easily maintained as the single value &#x03A3;{v<sub>i</sub> | Sample(x<sub>i</sub>) = 1}. Here we show that when dealing with w-bit integers, if a is a uniform odd w-bit integer and t is a uniform w-bit integer, then Sample(x) = [ax mod 2w &#x2264; t] is a distinguisher with probability 1/8. Working with standard units, that is w = 8,16,32,64, we exploit that w-bit multiplication works modulo 2w, discarding overflow automatically, and then the sampling decision is implemented by the C-code a*x&lt;;=t. Previous such samplers were much less computer-friendly, e.g. The distinguisher of Naor and Naor [STOC'90] was more complicated and involved a 7-independent hash function.
[Computers, source code (software), bias probability spaces, uniform w-bit integer, C-code, Electronic mail, C language, hash function, Small Bias Sampling Spaces, random sampling function, key-value pair stream, w-bit multiplication, Testing, Context, Distinguishers, accumulated key value, probability, standard units, Standards, Computer science, Algorithms, constant probability distinguishers, Hashing, file organisation, commutative monoid, uniform odd w-bit integer]
Hashing for Statistics over K-Partitions
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
In this paper we analyze a hash function for k-partitioning a set into bins, obtaining strong concentration bounds for standard algorithms combining statistics from each bin. This generic method was originally introduced by Flajolet and Martin [FOCS'83] in order to save a factor &#x03A9;(k) of time per element over k independent samples when estimating the number of distinct elements in a data stream. It was also used in the widely used HyperLogLog algorithm of Flajolet et al. [AOFA'97] and in large-scale machine learning by Li et al. [NIPS'12] for minwise estimation of set similarity. The main issue of k-partition, is that the contents of different bins may be highly correlated when using popular hash functions. This means that methods of analyzing the marginal distribution for a single bin do not apply. Here we show that a tabulation based hash function, mixed tabulation, does yield strong concentration bounds on the most popular applications of k-partitioning similar to those we would get using a truly random hash function. The analysis is very involved and implies several new results of independent interest for both simple and double tabulation, e.g. a simple and efficient construction for invertible bloom filters and uniform hashing on a given set.
[Correlation, Machine learning algorithms, estimation theory, minwise estimation, constant moments, Frequency estimation, hash function, joint distributions, Polynomials, data structures, large-scale machine learning, learning (artificial intelligence), concentration bounds, Estimation, HyperLogLog algorithm, k-partitioning, tabulation hashing, Computer science, k-partition, peelability, Hashing, Yttrium, uniform hashing, invertible Bloom filters, invertible bloom filters, statistics]
Optimal Induced Universal Graphs and Adjacency Labeling for Trees
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show that there exists a graph G with O(n) nodes, where any forest of n nodes is a node-induced subgraph of G. Furthermore, for constant arboricity k, the result implies the existence of a graph with O(nk) nodes that contains all n-node graphs as node-induced subgraphs, matching a &#x03A9;(nk) lower bound. The lower bound and previously best upper bounds were presented in Alstrup and Rauhe (FOCS'02). Our upper bounds are obtained through a log<sub>2</sub> n + O(1) labeling scheme for adjacency queries in forests. We hereby solve an open problem being raised repeatedly over decades, e.g. in Kannan, Naor, Rudich (STOC 1988), Chung (J. of Graph Theory 1990), Fraigniaud and Korman (SODA 2010).
[all n-node graphs, adjacency labeling, graph theory, trees (mathematics), Encoding, Graph theory, Decoding, trees, forests, Computer science, Upper bound, node-induced subgraphs, XML, Vegetation, optimal induced universal graphs, Labeling, adjacency queries, Induced universal graphs]
An Algorithmic Proof of the Lovasz Local Lemma via Resampling Oracles
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
The Lovasz Local Lemma is a seminal result in probabilistic combinatorics. It gives a sufficient condition on a probability space and a collection of events for the existence of an outcome that simultaneously avoids all of those events. Finding such an outcome by an efficient algorithm has been an active research topic for decades. Breakthrough work of Moser and Tardos (2009) presented an efficient algorithm for a general setting primarily characterized by a product structure on the probability space. In this work we present an efficient algorithm for a much more general setting. Our main assumption is that there exist certain functions, called resampling oracles, that can be invoked to address the undesired occurrence of the events. We show that, in all scenarios to which the original Lovasz Local Lemma applies, there exist resampling oracles, although they are not necessarily efficient. Nevertheless, for essentially all known applications of the Lovasz Local Lemma and its generalizations, we have designed efficient resampling oracles. As applications of these techniques, we present new results for packings of Latin transversals, rainbow matchings and rainbow spanning trees.
[Algorithm design and analysis, Heuristic algorithms, algorithmic proof, Transportation, probability, trees (mathematics), rainbow matchings, Probabilistic logic, Extraterrestrial measurements, resampling oracles, general probability spaces, rainbow spanning trees, Computer science, Latin transversals, probability space, Lov&#x00E1;sz local lemma, probabilistic combinatorics, Lovasz local lemma]
Non-backtracking Spectrum of Random Graphs: Community Detection and Non-regular Ramanujan Graphs
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
A non-backtracking walk on a graph is a directed path such that no edge is the inverse of its preceding edge. The non-backtracking matrix of a graph is indexed by its directed edges and can be used to count on-backtracking walks of a given length. It has been used recently in the context of community detection and has appeared previously in connection with the Ihara zeta function and in some generalizations of Ramanujan graphs. In this work, we study the largest eigen valus of the non-backtracking matrix of the Erdos-Renyi random graph and of the Stochastic Block Model in the regime where the number of edges is proportional to the number of vertices. Our results confirm the "spectral redemption conjecture" that community detection can be made on the basis of the leading eigenvectors above the feasibility threshold.
[Context, Stochastic Block Model, Symmetric matrices, Image edge detection, graph theory, Stochastic processes, Electronic mail, eigenvalues and eigenfunctions, matrix algebra, stochastic block model, Ihara zeta function, directed path, eigenvectors, non-backtracking matrix, nonbacktracking matrix, Eigenvalues and eigenfunctions, Ramanujan graphs, stochastic processes, Erdos-Renyi random graph, nonregular Ramanujan graphs, nonbacktracking spectrum, community detection, Belief propagation]
Interlacing Families IV: Bipartite Ramanujan Graphs of All Sizes
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove that there exist bipartite Ramanujan graphs of every degree and every number of vertices. The proof is based on analyzing the expected characteristic polynomial of a union of random perfect matchings, and involves three ingredients: (1) a formula for the expected characteristic polynomial of the sum of a regular graph with a random permutation of another regular graph, (2) a proof that this expected polynomial is real rooted and that the family of polynomials considered in this sum is an interlacing family, and (3) strong bounds on the roots of the expected characteristic polynomial of a union of random perfect matchings, established using the framework of finite free convolutions introduced recently by the authors.
[Context, bipartite Ramanujan graphs, Symmetric matrices, pattern matching, finite free convolutions, graph theory, expected characteristic polynomials, random perfect matchings, Computer science, characteristic polynomial, finite free probability, interlacing family, Polynomials, Eigenvalues and eigenfunctions, Ramanujan graphs, Bipartite graph, regular graph, computational complexity]
Incidences between Points and Lines in R^4
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We show that the number of incidences between m distinct points and n distinct lines in R4 is O(2c&#x221A;log m(m2/5n4/5 + m) + m1/2n1/2q1/4 + m2/3n1/3s1/3 + n), for a suitable absolute constant c, provided that no 2-plane contains more than s input lines, and no hyperplane or quadric contains more than q lines. The bound holds without the extra factor 2c&#x221A;log m when m &#x2264; n6/7 or m &#x2265; n5/3. Except for this possible factor, the bound is tight in the worst case. The context of this work is incidence geometry, a topic that has been widely studied for more than three decades, with strong connections to a variety of topics, from range searching in computational geometry to the Kakeya problem in harmonic analysis and geometric measure theory. The area has picked up considerable momentum in the past seven years, following the seminal works of Guth and Katz [12, 13], where the later work solves the point-line incidence problem in three dimensions, using new tools and techniques from algebraic geometry. This work extends their result to four dimensions. In doing so, it had to overcome many new technical hurdles that arise from the higher-dimensional context, by developing and adapting more advanced tools from algebraic geometry.
[Context, distinct lines, geometric measure theory, Kakeya problem, computational geometry, Search problems, algebra, incidence geometry, point-line incidence problem, Combinatorial geometry, harmonic analysis, Computer science, Computational geometry, Upper bound, Polynomials, algebraic geometry, the polynomial method, hyperplane, ruled surfaces, incidences]
Talagrand's Convolution Conjecture on Gaussian Space
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Smoothing properties of the noise operator on the discrete cube and on Gaussian space have played a pivotal role in many fields. In particular, these smoothing effects have seena broad range of applications in theoretical computer science. We exhibit new regularization properties of the noise operator on Gaussian space. More specifically, we show that the mass on level sets of a probability density decays uniformly under the Ornstein-Uhlenbeck semi group. This confirms positively the Gaussian case of Talagrand's convolution conjecture (1989)on the discrete cube. A major theme is our use of an It o process (the "F"ollmer drift")which can be seen as an entropy-optimal coupling between the Gaussian measure and another given measure on Gaussian space. To analyze this process, we employ stochastic calculus and Girsanov's change of measure formula. The ideas and tools employed here provide a new perspective on hyper contractivity in Gaussian space and the discrete cube. In particular, our work gives a new way of studying "small" sets in product spaces (e.g., Sets of size 2o(n) in the discrete cube) using a form of regularized online gradient descent.
[discrete cube, Smoothing methods, stochastic calculus, Level set, probability, smoothing methods, Talagrand convolution conjecture, Gaussian space, noise operator, the Ornstein-Uhlenbeck semigroup, Space heating, Temperature measurement, Computer science, Ornstein-Uhlenbeck semi group, entropy-optimal coupling, Girsanov change of measure formula, Convolution, computer science, probability density decays, hypercontractivity, Gaussian processes, smoothing properties, regularized online gradient descent]
Random Matrices: l1 Concentration and Dictionary Learning with Few Samples
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Let X be a sparse random matrix of size n &#x00D7; p (p &#x226B; n). We prove that if p &#x2265; Cn log4 n, then with probability 1 - o(1), &#x2225;XT v&#x2225;<sub>1</sub> is close to its expectation for all vectors v &#x2208; Rn (simultaneously). The bound on p is sharp up to the polylogarithmic factor. The study of this problem is directly motivated by an application. Let A be an n&#x00D7;n matrix, X be an n&#x00D7;p matrix and Y = AX. A challenging and important problem in data analysis, motivated by dictionary learning and other practical problems, is to recover both A and X, given Y . Under normal circumstances, it is clear that this problem is underdetermined. However, in the case when X is sparse and random, Spielman, Wang and Wright showed that one can recover both A and X efficiently from Y with high probability, given that p (the number of samples) is sufficiently large. Their method works for p &#x2265; Cn2 log2 n and they conjectured that p &#x2265; Cn log n suffices. The bound n log n is sharp for an obvious information theoretical reason. The matrix concentration result verifies the Spielman et. al. conjecture up to a log3 n factor. Our proof of the concentration result is based on two ideas. The first is an economical way to apply the union bound. The second is a refined version of Bernstein's concentration inequality for a sum of independent variables. Both have nothing to do with random matrices and are applicable in general settings.
[Algorithm design and analysis, Dictionaries, data analysis, polylogarithmic factor, mathematics computing, probability, random processes, sparse random matrix, union bound, Matrix Concentration, concentration inequality, Linear matrix inequalities, dictionary learning, matrix concentration, Sparse matrices, Standards, l<sub>1</sub> concentration, log3 n factor, optimisation, Dictionary Learning, Random variables, Yttrium, learning (artificial intelligence), sparse matrices]
Mixture Selection, Mechanism Design, and Signaling
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We pose and study a fundamental algorithmic problem which we term mixture selection, arising as a building block in a number of game-theoretic applications: Given a function g from the n-dimensional hypercube to the bounded interval [-1, 1], and an n &#x00D7; rn matrix A with bounded entries, maximize g(Ax) over x in the m-dimensional simplex. This problem arises naturally when one seeks to design a lottery over items for sale in an auction, or craft the posterior beliefs for agents in a Bayesian game through the provision of information (a.k.a. signaling). We present an approximation algorithm for this problem when g simultaneously satisfies two &#x201C;smoothness&#x201D; properties: Lipschitz continuity with respect to the L&#x221E; norm, and noise stability. The latter notion, which we define and cater to our setting, controls the degree to which low-probability - and possibly correlated - errors in the inputs of g can impact its output. The approximation guarantee of our algorithm degrades gracefully as a function of the Lipschitz continuity and noise stability of g. In particular, when g is both 0(1)-Lipschitz continuous and 0(1)-stable, we obtain an (additive) polynomial-time approximation scheme (PTAS) for mixture selection. We also show that neither assumption suffices by itself for an additive PTAS, and both assumptions together do not suffice for an additive fully polynomial-time approximation scheme (FPTAS). We apply our algorithm for mixture selection to a number of different game-theoretic applications, focusing on problems from mechanism design and optimal signaling. In particular, we make progress on a number of open problems suggested in prior work by easily reducing them to mixture selection: we resolve an important special case of the small-menu lottery design problem posed by Dughmi, Han, and Nisan [10]; we resolve the problem of revenue-maximizing signaling in Bayesian secondprice auctions posed by Emek et al. [12] and Miltersen and Sheffet [5]; we design a quasipolynomial-time approximation scheme for the optimal signaling problem in normal form games suggested by Dughmi [9]; and we design an approximation algorithm for the optimal signaling problem in the voting model of Alonso and Camara [3].
[Algorithm design and analysis, Additives, Bayesian game, signaling, signal processing, mechanism design, Approximation methods, matrix, Lipschitz continuity, additive fully polynomial-time approximation scheme, mixture selection, quasipolynomial-time approximation scheme, game-theoretic applications, game theory, Stability analysis, Game theory, matrix algebra, n-dimensional hypercube, optimal signaling problem, Games, Approximation algorithms, Bayes methods, FPTAS, noise stability, computational complexity]
Optimal Auctions vs. Anonymous Pricing
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
For selling a single item to agents with independent but non-identically distributed values, the revenue optimal auction is complex. With respect to it, Hartline and Rough garden showed that the approximation factor of the second-price auction with an anonymous reserve is between two and four. We consider the more demanding problem of approximating the revenue of the ex ante relaxation of the auction problem by posting an anonymous price (while supplies last) and prove that their worst-case ratio is e. As a corollary, the upper-bound of anonymous pricing or anonymous reserves versus the optimal auction improves from four to e. We conclude that, up to an e factor, discrimination and simultaneity are unimportant for driving revenue in single-item auctions.
[approximation theory, Hartline garden, revenue optimal auction, Rough garden, Electronic mail, Approximation methods, commerce, Algorithmic Game Theory, Anonymous Pricing, Standards, Optimization, Computer science, optimal auctions, approximation factor, Upper bound, anonymous pricing, Pricing, Auction Theory, nonidentically distributed values, Mechanism Design, pricing]
On the Complexity of Optimal Lottery Pricing and Randomized Mechanisms
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We study the optimal lottery problem and the optimal mechanism design problem in the setting of a single unit-demand buyer with item values drawn from independent distributions. Optimal solutions to both problems are characterized by a linear program with exponentially many variables. For the menu size complexity of the optimal lottery problem, we present an explicit, simple instance with distributions of support size 2, and show that exponentially many lotteries are required to achieve the optimal revenue. We also show that, when distributions have support size 2 and share the same high value, the simpler scheme of item pricing can achieve the same revenue as the optimal menu of lotteries. The same holds for the case of two items with support size 2 (but not necessarily the same high value). For the computational complexity of the optimal mechanism design problem, we show that unless the polynomial-time hierarchy collapses (more exactly, PNP = P#P), there is no universal efficient randomized algorithm to implement an optimal mechanism even when distributions have support size 3.
[Algorithm design and analysis, optimal mechanism design, Additives, optimal lottery pricing complexity, menu size complexity, unit-demand buyer, linear program, linear programming, Complexity theory, Standards, Cost accounting, randomised algorithms, randomized mechanism, polynomial-time hierarchy collapse, optimal mechanism design problem, Pricing, optimal lottery problem, single unit-demand buyer, Polynomials, pricing, Lottery pricing, computational complexity]
On the Cryptographic Hardness of Finding a Nash Equilibrium
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We prove that finding a Nash equilibrium of a game is hard, assuming the existence of indistinguishability obfuscation and one-way functions with sub-exponential hardness. We do so by showing how these cryptographic primitives give rise to a hard computational problem that lies in the complexity class PPAD, for which finding Nash equilibrium is complete. Previous proposals for basing PPAD-hardness on program obfuscation considered a strong "virtual black-box" notion that is subject to severe limitations and is unlikely to be realizable for the programs in question. In contrast, for indistinguishability obfuscation no such limitations are known, and recently, several candidate constructions of indistinguishability obfuscation were suggested based on different hardness assumptions on multilinear maps. Our result provides further evidence of the intractability of finding a Nash equilibrium, one that is extrinsic to the evidence presented so far.
[hard computational problem, PPAD complexity class, game theory, Nash equilibrium, Search problems, cryptography, indistinguishability obfuscation, Complexity theory, subexponential hardness, complete Nash equilibrium, nash equilibrium, Computer science, Games, cryptographic hardness, multilinear maps, Cryptography, one-way functions, hard-Nash equilibrium, obfuscation, computational complexity]
Welfare Maximization with Limited Interaction
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
We continue the study of welfare maximization in unit-demand (matching) markets, in a distributed information model where agent's valuations are unknown to the central planner, and therefore communication is required to determine an efficient allocation. Dobzinski, Nisan and Oren (STOC'14) showed that if the market size is n, then r rounds of interaction (with logarithmic bandwidth) suffice to obtain an n1/(r+1)-approximation to the optimal social welfare. In particular, this implies that such markets converge to a stable state (constant approximation) in time logarithmic in the market size. We obtain the first multi-round lower bound for this setup. We show that even if the allowable per-round bandwidth of each agent is n&#x03B5;(r), the approximation ratio of any r-round (randomized) protocol is no better than &#x03A9;(n1/5r+1), implying an &#x03A9;(log log n) lower bound on the rate of convergence of the market to equilibrium. Our construction and technique may be of interest to round-communication tradeoffs in the more general setting of combinatorial auctions, for which the only known lower bound is for simultaneous (r = 1) protocols [DNO14].
[central planner, Protocols, combinatorial mathematics, combinatorial auctions, Complexity theory, Electronic mail, Approximation methods, communication complexity, time logarithmic, optimisation, limited interaction, round-communication tradeoffs, approximation ratio, Computational modeling, welfare maximization, distributed information model, optimal social welfare, Distributed matchings, simultaneous protocols, matching markets, logarithmic bandwidth, Upper bound, unit-demand markets, Welfare maximization, Resource management, Multiparty Communication complexity, Information theory]
[Publisher's information]
2015 IEEE 56th Annual Symposium on Foundations of Computer Science
None
2015
Provides a listing of current committee members and society officers.
[]
Preface
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee and Sponsors
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Provides a listing of current committee members and society officers.
[]
Program Committee
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Provides a listing of current committee members and society officers.
[]
Bounded-Communication Leakage Resilience via Parity-Resilient Circuits
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We consider the problem of distributing a computation between two parties, such that any bounded-communication leakage function applied to the local views of the two parties reveals essentially nothing about the input. This problem can be motivated by the goal of outsourcing computations on sensitive data to two servers in the cloud, where both servers can be simultaneously corrupted by viruses that have a limited communication bandwidth. We present a simple and efficient reduction of the above problem to that of constructing parity-resilient circuits, namely circuits that map an encoded input to an encoded output so that the parity of any subset of the wires is essentially independent of the input. We then construct parity-resilient circuits from circuits that are resilient to local leakage, which can in turn be obtained from protocols for secure multiparty computation. Our main reduction builds on a novel generalization of the &#x03B5;-biased masking lemma that applies to interactive protocols. Applying the above, we obtain two-party protocols with resilience to bounded-communication leakage either in the information-theoretic setting, relying on random oblivious transfer correlations, or in the computational setting, relying on non-committing encryption which can be based on a variety of standard cryptographic assumptions.
[interactive protocols, Protocols, distributed processing, noncommitting encryption, Servers, communication complexity, bounded communication leakage function, Hardware, Cryptography, cloud computing, local leakage, Viruses (medical), bounded communication leakage resilience, encoded input, communication bandwidth, secure multiparty computation, cryptography, e-biased masking, Standards, parity resilient circuits, random oblivious transfer correlations, standard cryptographic assumptions, sensitive data, outsourcing computations, information-theoretic setting, Leakage-resilient cryptography]
Indistinguishability Obfuscation from DDH-Like Assumptions on Constant-Degree Graded Encodings
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
All constructions of general purpose indistinguishability obfuscation (IO) rely on either meta-assumptions that encapsulate an exponential family of assumptions (e.g., Pass, Seth and Telang, CRYPTO 2014 and Lin, EUROCRYPT 2016), or polynomial families of assumptions on graded encoding schemes with a high polynomial degree/multilinearity (e.g., Gentry, Lewko, Sahai and Waters, FOCS 2014). We present a new construction of IO, with a security reduction based on two assumptions: (a) a DDH-like assumption - called the sSXDH assumption - on constant degree graded encodings, and (b) the existence of polynomial-stretch pseudorandom generators (PRG) in NC0. Our assumption on graded encodings is simple, has constant size, and does not require handling composite-order rings. This narrows the gap between the mathematical objects that exist (bilinear maps, from elliptic curve groups) and ones that suffice to construct general purpose indistinguishability obfuscation.
[PRG, security reduction, Complexity theory, Electronic mail, meta-assumptions, Graded Encodings, NC0, sSXDH assumption, Cryptography, polynomial degree/multilinearity, polynomials, probability, IO, cryptography, Encoding, Generators, Noise measurement, DDH-like assumptions, assumption polynomial families, assumption exponential family, Program Obfuscation, probabilistic polynomial-time algorithm, polynomial-stretch pseudorandom generators, general purpose indistinguishability obfuscation, constant-degree graded encodings, computational complexity]
Breaking the Three Round Barrier for Non-malleable Commitments
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We construct two-message non-malleable commitments with respect to opening in the standard model, assuming only one-to-one one-way functions. Our protocol consists of two unidirectional messages by the committer (with no message from the receiver), and is secure against all polynomial-time adversaries in the standard synchronous setting. Pass (TCC 2013) proved that any commitment scheme with non-malleability with respect to commitment, using only 2 rounds of communication, cannot be proved secure via a black-box reduction to any "standard" intractability assumption. We extend this by showing a similar impossibility result for commitments with non-malleability with respect to opening, another standard notion of non-malleability for commitments, for any 2-message challenge-response protocol, as well. However, somewhat surprisingly, we show that this barrier breaks down in the setting of two unidirectional messages by the committer (with no message from the receiver), for non-malleability with respect to opening. <sub>&#x00B0;</sub> Our protocol makes only black-box use of any non-interactive statistically binding commitment scheme. Such a scheme can be based on any one-to-one one-way function. <sub>&#x00B0;</sub> Our techniques depart significantly from the commit-challenge-response structure followed by nearly all prior works on non-malleable protocols in the standard model. Our methods are combinatorial in nature. <sub>&#x00B0;</sub> Our protocol resolves the round complexity of commitments with non-malleability with respect to opening via natural (non-embedding) black-box security reductions. We show that completely non-interactive non-malleable commitments w.r.t. opening cannot be proved secure via most natural black-box reductions. This result extends to also rule out bi-directional two-message non-malleable commitments w.r.t. opening in the synchronous or asynchronous setting. <sub>&#x00B0;</sub> Our protocol, together with our impossibility result, also resolves the round complexity of block-wise non-malleable codes (Chandran et al) w.r.t. natural black-box reductions.
[2-message challenge-response protocol, Protocols, nonmalleable commitments, polynomials, commit-challenge-response structure, Receivers, commitment scheme, Complexity theory, Security, black-box reduction, Standards, three round barrier, Computer science, polynomial-time adversaries, security of data, Bidirectional control, unidirectional messages, block-wise nonmalleable codes]
Zero-Knowledge Proof Systems for QMA
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Prior work has established that all problems in NP admit classical zero-knowledge proof systems, and under reasonable hardness assumptions for quantum computations, these proof systems can be made secure against quantum attacks. We prove a result representing a further quantum generalization of this fact, which is that every problem in the complexity class QMA has a quantum zero-knowledge proof system. More specifically, assuming the existence of an unconditionally binding and quantum computationally concealing commitment scheme, we prove that every problem in the complexity class QMA has a quantum interactive proof system that is zero-knowledge with respect to efficient quantum computations. Our QMA proof system is sound against arbitrary quantum provers, but only requires an honest prover to perform polynomial-time quantum computations, provided that it holds a quantum witness for a given instance of the QMA problem under consideration.
[quantum witness, Protocols, quantum computationally concealing commitment scheme, polynomials, zero-knowledge proof systems, quantum generalization, Encoding, polynomial-time quantum computations, Computer science, quantum zero-knowledge proof system, Quantum computing, Quantum mechanics, Authentication, quantum computing, complexity class QMA, quantum interactive proof system, theorem proving, Cryptography, computational complexity]
Strong Fooling Sets for Multi-player Communication with Applications to Deterministic Estimation of Stream Statistics
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We develop a paradigm for studying multi-player deterministic communication, based on a novel combinatorial concept that we call a strong fooling set. Our paradigm leads to optimal lower bounds on the per-player communication required for solving multi-player EQUALITY problems in a private-message setting. This in turn gives a very strong - O(1) versus &#x03A9;(n) - separation between private-message and one-way blackboard communication complexities. Applying our communication complexity results, we show that for deterministic data streaming algorithms, even loose estimations of some basic statistics of an input stream require large amounts of space. For instance, approximating the frequency moment F<sub>k</sub> within a factor &#x03B1; requires &#x03A9;(n/&#x03B1;1/(1-k)) space for k &gt; 1 and roughly &#x03A9;(n/&#x03B1;k/(k-1)) space for k &gt; 1. In particular, approximation within any constant factor &#x03B1;, however large, requires linear space, with the trivial exception of k = 1. This is in sharp contrast to the situation for randomized streaming algorithms, which can approximate F<sub>k</sub> to within (1&#x00B1;&#x03B5;) factors using O&#x0303;(1) space for k &#x2264; 2 and o(n) space for all finite k and all constant &#x03B5; &gt; 0. Previous linear-space lower bounds for deterministic estimation were limited to small factors &#x03B1;, such as &#x03B1; &lt;; 2 for approximating F<sub>0</sub> or F<sub>2</sub>. We also provide certain space/approximation tradeoffs in a deterministic setting for the problems of estimating the empirical entropy of a stream as well as the size of the maximum matching and the edge connectivity of a streamed graph.
[Protocols, linear space, streamed graph, graph theory, approximation tradeoffs, randomized streaming algorithms, Frequency estimation, multiplayer equality problems, Complexity theory, constant factor, stream statistics, private-message setting, maximum matching, multiplayer deterministic communication, one-way blackboard communication complexities, data analysis, Estimation, strong fooling set, Receivers, space tradeoffs, deterministic algorithms, Computer science, edge connectivity, deterministic data streaming algorithms, Games, statistical analysis, computational complexity, empirical entropy]
Edit Distance: Sketching, Streaming, and Document Exchange
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We show that in the document exchange problem, where Alice holds x &#x03F5; {0, 1}n and Bob holds y &#x03F5; {0, 1}n, Alice can send Bob a message of size O(K(log2 K + log n)) bits such that Bob can recover x using the message and his input y if the edit distance between x and y is no more than K, and output "error" otherwise. Both the encoding and decoding can be done in time O&#x0303;(n + poly(K)). This result significantly improves on the previous communication bounds under polynomial encoding/decoding time. We also show that in the referee model, where Alice and Bob hold x and y respectively, they can compute sketches of x and y of sizes poly(K log n) bits (the encoding), and send to the referee, who can then compute the edit distance between x and y together with all the edit operations if the edit distance is no more than K, and output "error" otherwise (the decoding). To the best of our knowledge, this is the first result for sketching edit distance using poly(K log n) bits. Moreover, the encoding phase of our sketching algorithm can be performed by scanning the input string in one pass. Thus our sketching algorithm also implies the first streaming algorithm for computing edit distance and all the edits exactly using poly(K log n) bits of space.
[document handling, edit distance, Protocols, Computational modeling, polynomials, Encoding, Decoding, Synchronization, streaming algorithm, document exchange, polynomial decoding time, sketching algorithm, Computer science, poly(K log n) bits, sketching, streaming, Databases, polynomial encoding time, computational complexity]
Optimal Quantile Approximation in Streams
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
This paper resolves one of the longest standing basic problems in the streaming computational model. Namely, optimal construction of quantile sketches. An &#x03B5; approximate quantile sketch receives a stream of items x1,<sub>&#x22EF;</sub>,xn and allows one to approximate the rank of any query item up to additive error &#x03B5; n with probability at least 1-&#x03B4;.The rank of a query x is the number of stream items such that x<sub>i</sub> &#x2264; x. The minimal sketch size required for this task is trivially at least 1/&#x03B5;.Felber and Ostrovsky obtain a O((1/&#x03B5;)log(1/&#x03B5;)) space sketch for a fixed &#x03B4;.Without restrictions on the nature of the stream or the ratio between &#x03B5; and n, no better upper or lower bounds were known to date. This paper obtains an O((1/&#x03B5;)log log (1/&#x03B4;)) space sketch and a matching lower bound. This resolves the open problem and proves a qualitative gap between randomized and deterministic quantile sketching for which an &#x03A9;((1/&#x03B5;)log(1/&#x03B5;)) lower bound is known. One of our contributions is a novel representation and modification of the widely used merge-and-reduce construction. This modification allows for an analysis which is both tight and extremely simple. The same technique was reported, in private communications, to be useful for improving other sketching objectives and geometric coreset constructions.
[Additives, merge-and-reduce construction, deterministic quantile sketching, streaming quantiles, optimal quantile sketch construction, Data structures, query item ranking, Complexity theory, Compaction, Partitioning algorithms, set theory, additive error probability, geometric coreset constructions, Computer science, query processing, quantiles, optimal quantile approximation, streaming median, Approximation algorithms, geometry, randomized quantile sketching, streaming computational model, computational complexity, error statistics]
An Average-Case Depth Hierarchy Theorem for Higher Depth
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We extend the recent hierarchy results of Rossman, Servedio and Tan [1] to address circuits of almost logarithmic depth. Our proof uses the same basic approach as [1] but a number of small differences enables us to obtain a stronger result by a significantly shorter proof.
[circuit complexity, logarithmic depth circuits, Correlation, average-case depth hierarchy theorem, Complexity theory, Electronic mail, average case complexity, hierarchy theorems, Switching circuits, Computer science, Systematics, Logic gates, small-depth circuits]
A Better-Than-3n Lower Bound for the Circuit Complexity of an Explicit Function
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We consider Boolean circuits over the full binary basis. We prove a (3+1/86)n-o(n) lower bound on the size of such a circuit for an explicitly defined predicate, namely an affine disperser for sublinear dimension. This improves the 3n-o(n) bound of Norbert Blum (1984).The proof is based on the gate elimination technique extended with the following three ideas. We generalize the computational model by allowing circuits to contain cycles, this in turn allows us to perform affine substitutions. We use a carefully chosen circuit complexity measure to track the progress of the gate elimination process. Finally, we use quadratic substitutions that may be viewed as delayed affine substitutions.
[circuit complexity, gate elimination, affine disperser, Computational modeling, Complexity theory, Boolean algebra, lower bounds, affine transforms, Computer science, delayed affine substitutions, Boolean functions, Upper bound, logic gates, explicit function, better-than-3n lower bound, quadratic substitutions, 3n-O(n) bound, Logic gates, computational model, Boolean circuits, Integrated circuit modeling, sublinear dimension]
Depth-Reduction for Composites
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We obtain a new depth-reduction construction, which implies a super-exponential improvement in the depth lower bound separating NEXP from non-uniform ACC. In particular, we show that every circuit with AND, OR, NOT, and MOD<sub>m</sub> gates, m &#x03B5; Z+, of polynomial size and depth d can be reduced to a depth-2, SYM-AND, circuit of size 2(log n)O(d). This is an exponential size improvement over the traditional Yao-Beigel-Tarui, which has size blowup 2(log n)2O(d). Therefore, depth-reduction for composite m matches the size of the Allender-Hertrampf construction for primes from 1989. One immediate implication of depth reduction is an improvement of the depth from o(loglog n) to o(log n/loglog n), in Williams' program for ACC circuit lower bounds against NEXP. This is just short of O(log n/loglog n) and thus pushes William's program to the NC1 barrier, since NC1 is contained in ACC of depth O(log n/loglog n). A second, but non-immediate, implication regards the strengthening of the ACC lower bound in the Chattopadhyay-Santhanam interactive compression setting.
[depth-reduction, depth reduction construction, MOD<sub>m</sub> gates, OR, Allender-Hertrampf construction, SYM-AND, circuit lower bound, Williams' program, interactive compression, Probabilistic logic, Electronic mail, Chattopadhyay-Santhanam interactive compression setting, Computational complexity, lower bound separating NEXP, polynomial size, Computer science, Yao-Beigel-Tarui, Boolean functions, logic gates, NOT, super-exponential improvement, AND, Logic gates, composite modulus]
A Deterministic Polynomial Time Algorithm for Non-commutative Rational Identity Testing
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Symbolic matrices in non-commuting variables, and the related structural and algorithmic questions, have a remarkable number of diverse origins and motivations. They arise independently in (commutative) invariant theory and representation theory, linear algebra, optimization, linear system theory, quantum information theory, and naturally in non-commutative algebra. In this paper we present a deterministic polynomial time algorithm for testing if a symbolic matrix in non-commuting variables over Q is invertible or not. The analogous question for commuting variables is the celebrated polynomial identity testing (PIT) for symbolic determinants. In contrast to the commutative case, which has an efficient probabilistic algorithm, the best previous algorithm for the non-commutative setting required exponential time [1] (whether or not randomization is allowed). The main (simple!) technical contribution of this paper is an analysis of an existing &#x201C;operator scaling&#x201D; algorithm due to Gurvits [2], which solved some special cases of the same problem we do (these already include optimization problems like matroid intersection). This analysis of the running time of Gurvits' algorithm combines results from some of these different fields. It lower bounds a parameter of quantum maps called capacity, via degree bounds from algebraic geometry on the Left Right group action, which in turn is relevant due to certain characterization of the free skew (non-commutative) field. Via the known connections above, our algorithm efficiently solves several problems in different areas which had only exponential-time algorithms prior to this work. These include the &#x201C;word problem&#x201D; for the free skew field (namely identity testing for rational expressions over non-commuting variables), testing if a quantum operator is &#x201C;rank decreasing&#x201D;, and the membership problem in the null-cone of a natural group action arising in Geometric Complexity Theory (GCT). Moreover, extending our algorithm to actually compute the non-commutative rank of a symbolic matrix, yields an efficient factor-2 approximation to the standard commutative rank. This naturally suggests the challenge to improve this approximation factor, noting that a fully polynomial approximation scheme may lead to a deterministic PIT algorithm. Finally, our algorithm may also be viewed as efficiently solving a family of structured systems of quadratic equations, which seem general enough to encode interesting decision and optimization problems1.
[Algorithm design and analysis, fully-polynomial approximation scheme, noncommuting variables, Complexity theory, Electronic mail, quantum operator, Gurvit algorithm, Optimization, geometric complexity theory, approximation factor, optimisation, Algebra, polynomial approximation, quadratic equations, optimization, algebraic geometry, quantum maps, Testing, standard commutative rank, noncommutative rational identity testing, approximation theory, probability, symbolic determinants, deterministic polynomial time algorithm, polynomial identity testing, left-right group action, operator scaling algorithm, matrix algebra, factor-2 approximation, probabilistic algorithm, Quantum mechanics, deterministic PIT algorithm, GCT, computational complexity, symbolic matrix, Keywords-Non-commutative computation; Rational identity]
The Salesman's Improved Paths: A 3/2+1/34 Approximation
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We give a new, strongly polynomial algorithm and improved analysis of the metric s-t path TSP. It finds a tour of cost less than 1.53 times the optimum of the subtour elimination LP, while known examples show that 1.5 is a lower bound for the integrality gap. A key new idea is the deletion of some edges of Christofides' trees, and we show that the arising "reconnection" problems can be solved for a minor extra cost. On the one hand our algorithm and analysis extend previous tools, at the same time simplifying the framework. On the other hand new tools are introduced, such as a flow problem used for analyzing the reconnection cost, and the use of a set of more and more restrictive minimum cost spanning trees, each of which can still be found by the greedy algorithm. The latter leads to a simple Christofides-like algorithm completely avoiding the computation of a convex combination of spanning trees. Furthermore, the 3/2 target-bound is easily reached in some relevant new cases.
[Algorithm design and analysis, Measurement, integrality gap, polynomial algorithm, path traveling salesman problem, Christofides' heuristic, Christofides-like algorithm, T-joins, travelling salesman problems, path traveling salesman problem (TSP), subtour elimination LP, polynomial approximation, 3/2+1/34 approximation, approximation algorithm, Cost function, the Chinese postman problem, matching theory, greedy algorithm, greedy algorithms, Urban areas, trees (mathematics), Traveling salesman problems, metric s-t path, Christofides trees, restrictive minimum cost spanning trees, edge deletion, reconnection cost, TSP, matroids, 3/2 target-bound, Approximation algorithms, polyhedra, computational complexity]
Hopsets with Constant Hopbound, and Applications to Approximate Shortest Paths
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
A (&#x03B2;, &#x2208;)-hopset for a weighted undirected n-vertex graph G = (V, E) is a set of edges, whose addition to the graph guarantees that every pair of vertices has a path between them that contains at most &#x03B2; edges, whose length is within 1 + &#x2208; of the shortest path. In her seminal paper, Cohen [8, JACM 2000] introduced the notion of hopsets in the context of parallel computation of approximate shortest paths, and since then it has found numerous applications in various other settings, such as dynamic graph algorithms, distributed computing, and the streaming model. Cohen [8] devised efficient algorithms for constructing hopsets with polylogarithmic in n number of hops. Her constructions remain the state-of-the-art since the publication of her paper in STOC'94, i.e., for more than two decades. In this paper we exhibit the first construction of sparse hopsets with a constant number of hops. We also find efficient algorithms for hopsets in various computational settings, improving the best known constructions. Generally, our hopsets strictly outperform the hopsets of [8], both in terms of their parameters, and in terms of the resources required to construct them. We demonstrate the applicability of our results for the fundamental problem of computing approximate shortest paths from s sources. Our results improve the running time for this problem in the parallel, distributed and streaming models, for a vast range of s.
[parallel algorithms, parallel computation, streaming models, Computational modeling, Heuristic algorithms, graph theory, approximate shortest paths, hopsets, Phase change random access memory, Routing, sparse hopsets, Computer science, Program processors, computational settings, graphs, distributed models, constant hopbound, shortest paths, Approximation algorithms, parallel models, weighted undirected n-vertex graph]
Better Unrelated Machine Scheduling for Weighted Completion Time via Random Offsets from Non-uniform Distributions
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
In this paper we consider the classic scheduling problem of minimizing total weighted completion time on unrelated machines when jobs have release times, i.e, R|r<sub>ij</sub>| &#x03A3;<sub>j</sub> w<sub>j</sub>C<sub>j</sub> using the three-field notation. For this problem, a 2-approximation is known based on a novel convex programming (J. ACM 2001 by Skutella). It has been a long standing open problem if one can improve upon this 2-approximation (Open Problem 8 in J. of Sched. 1999 by Schuurman and Woeginger). We answer this question in the affirmative by giving a 1.8786-approximation. We achieve this via a surprisingly simple linear programming, but a novel rounding algorithm and analysis. A key ingredient of our algorithm is the use of random offsets sampled from non-uniform distributions. We also consider the preemptive version of the problem, i.e, R|r<sub>ij</sub>, pmtn|&#x03A3;<sub>j</sub>w<sub>j</sub>C<sub>j</sub>. We again use the idea of sampling offsets from non-uniform distributions to give the first better than 2-approximation for this problem. This improvement also requires use of a configuration LP with variables for each job's complete schedules along with more careful analysis. For both non-preemptive and preemptive versions, we break the approximation barrier of 2 for the first time.
[Algorithm design and analysis, Schedules, approximation theory, unrelated machine scheduling, Optimal scheduling, Programming, convex programming, random offsets, weighted completion time, completion time, approximation algorithms, nonuniform distributions, unrelated machines, release times, Processor scheduling, minimizing total weighted completion time, classic scheduling problem, scheduling, Approximation algorithms, simple linear programming]
Online Algorithms for Covering and Packing Problems with Convex Objectives
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We present online algorithms for covering and packing problems with (non-linear) convex objectives. The convex covering problem is defined as: min<sub>x&#x03F5;</sub>R<sub>+</sub>nf(x) s.t. Ax &#x2265; 1, where f:R<sub>+</sub>n &#x2192; R<sub>+</sub> is a monotone convex function, and A is an m&#x00D7;n matrix with non-negative entries. In the online version, a new row of the constraint matrix, representing a new covering constraint, is revealed in each step and the algorithm is required to maintain a feasible and monotonically non-decreasing assignment x over time. We also consider a convex packing problem defined as: max<sub>y&#x03F5;R+</sub>m &#x03A3;<sub>j=1</sub>m yj - g(AT y), where g:R<sub>+</sub>n&#x2192;R<sub>+</sub> is a monotone convex function. In the online version, each variable yj arrives online and the algorithm must decide the value of yj on its arrival. This represents the Fenchel dual of the convex covering program, when g is the convex conjugate of f. We use a primal-dual approach to give online algorithms for these generic problems, and use them to simplify, unify, and improve upon previous results for several applications.
[constraint matrix, primal-dual algorithm, online algorithm, convex covering program, Linear programming, Routing, convex programming, Electronic mail, online algorithms, bin packing, primal-dual approach, matrix algebra, Computer science, convex objectives, Production, monotone convex function, Cost function, convex optimization, Convex functions, convex packing problem, convex covering problem]
Explicit Non-malleable Extractors, Multi-source Extractors, and Almost Optimal Privacy Amplification Protocols
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We make progress in the following three problems: 1. Constructing optimal seeded non-malleable extractors, 2. Constructing optimal privacy amplification protocols with an active adversary, for any possible security parameter, 3. Constructing extractors for independent weak random sources, when the min-entropy is extremely small (i.e., near logarithmic). For the first two problems, the best known non-malleable extractors by Chattopadhyay, Goyal and Li, and by Cohen all require seed length and min-entropy with quadratic loss in parameters. As a result, the best known explicit privacy amplification protocols with an active adversary, which achieve two rounds of communication and optimal entropy loss was sub-optimal in the min-entropy of the source. In this paper we give an explicit non-malleable extractor that works for nearly optimal seed length and min-entropy, and yields a two-round privacy amplification protocol with optimal entropy loss for almost all ranges of the security parameter. For the third problem, we improve upon a very recent result by Cohen and Schulman and give an explicit extractor that uses an absolute constant number of sources, each with almost logarithmic min-entropy. The key ingredient in all our constructions is a generalized, and much more efficient version of the independence preserving merger introduced by Cohen, which we call non-malleable independence preserving merger. Our construction of the merger also simplifies that of Cohen and Schulman, and may be of independent interest.
[Protocols, cryptographic protocols, optimal entropy loss, quadratic loss, privacy amplification, Entropy, security parameter, Security, Privacy, entropy, independent source extractors, almost optimal privacy amplification protocols, two-round privacy amplification protocol, explicit privacy amplification protocols, Corporate acquisitions, communication loss, non-malleable extractors, optimal seeded nonmalleable extractors, independent weak random sources, Computer science, seed length, almost logarithmic min-entropy, Agricultural machinery, data privacy, multisource extractors, explicit nonmalleable extractors]
Improved Two-Source Extractors, and Affine Extractors for Polylogarithmic Entropy
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
In a recent breakthrough [1], Chattopadhyay and Zuckerman gave an explicit two-source extractor for min-entropy k &#x2265; logC n for some large enough constant C, where n is the length of the source. However, their extractor only outputs one bit. In this paper, we improve the output of the two-source extractor to k&#x03A9;(1), while the error remains n-&#x03A9;(1) and the extractor remains strong in the second source. In the non-strong case, the output can be increased to k. Our improvement is obtained by giving a better extractor for (q, t, &#x03B3;) non-oblivious bit-fixing sources, which can output t&#x03A9;(1) bits instead of one bit as in [1]. We also give the first explicit construction of deterministic extractors for affine sources over F<sub>2</sub>, with entropy k &#x2265; logC n for some large enough constant C, where n is the length of the source. Previously the best known results are by Bourgain [2], Yehudayoff [3] and Li [4], which require the affine source to have entropy at least &#x03A9;(n/&#x221A;log log n). Our extractor outputs k&#x03A9;(1) bits with error n-&#x03A9;(1). This is done by reducing an affine source to a non-oblivious bit-fixing source, where we adapt the alternating extraction based approach in previous work on independent source extractors [5] to the affine setting. Our affine extractors also imply improved extractors for circuit sources studied in [6]. We further extend our results to the case of zero-error dispersers, and give two applications in data structures that rely crucially on the fact that our two-source or affine extractors have large output size.
[extractor outputs, Probabilistic logic, Data structures, extractor, Entropy, Data mining, affine, Standards, polylogarithmic entropy, two-source extractor, Computer science, Chattopadhyay, bit-fixing sources, affine extractors, affine sources, independent source extractors, Zuckerman, Agricultural machinery, deterministic extractors, two-source, computational complexity, two-source extractors]
Extractors for Near Logarithmic Min-Entropy
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
The main contribution of this work is an explicit construction of extractors for near logarithmic min-entropy. For any &#x03B4; &gt; 0 we construct an extractor for O(1/&#x03B4;) n-bit sources with min-entropy (logn)1+&#x03B4;. This is most interesting when &#x03B4; is set to a small constant, though the result also yields an extractor for O(log logn) sources with logarithmic min-entropy. Prior to this work, the best explicit extractor in terms of supporting least-possible min-entropy, due to Li (FOCS'15), requires min-entropy (logn)2+&#x03B4; from its O(1/&#x03B4;) sources. Further, all current techniques for constructing multi-source extractors "break" below min-entropy (log n)2. In fact, existing techniques do not provide even a disperser for o(log n) sources each with min-entropy (log n)1.99. Apart from being a natural problem, supporting logarithmic min-entropy has applications to combinatorics. A two-source disperser, let alone an extractor, for min-entropy O(log n) induces a (log, nO(1))-Ramsey graph on n vertices. Thus, constructing such dispersers would be a significant step towards constructively matching Erdo&#x0308;s' proof for the existence of (2log n)-Ramsey graphs on n vertices. Our construction does not rely on the sophisticated primitives that were key to the substantial recent progress on multi-source extractors, such as non-malleable extractors, correlation breakers, the lightest-bin condenser, or extractors for non-oblivious bit-fixing sources, although some of these primitives can be combined with our construction so to improve the output length and the error guarantee. Instead, at the heart of our construction is a new primitive called an independence-preserving merger. The construction of the latter builds on the alternating extraction technique.
[Heart, Correlation, Erdos proof, independence-preserving merger, Corporate acquisitions, extractors, graph theory, Entropy, independence-preserving mergers, near logarithmic min-entropy, logarithmic min-entropy, min-entropy (log n)2, (polylog n)-Ramsey graph, (2 log n)-Ramsey graphs, Standards, O(log log n) sources, correlation breakers, min-entropy (log n)1+&#x03B4;, o(log n) sources, Random variables, Bipartite graph, multisource extractors, O(1/&#x03B4;) n-bit sources, computational complexity]
Making the Most of Advice: New Correlation Breakers and Their Applications
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
A typical obstacle one faces when constructing pseudorandom objects is undesired correlations between random variables. Identifying this obstacle and constructing certain types of &#x201C;correlation breakers&#x201D; was central for recent exciting advances in the construction of multi-source and nonmalleable extractors. One instantiation of correlation breakers is correlation breakers with advice. These are algorithms that break the correlation a &#x201C;bad&#x201D; random variable Y ' has with a &#x201C;good&#x201D; random variable Y using an &#x201C;advice&#x201D; - a fixed string &#x03B1; that is associated with Y which is guaranteed to be distinct from the corresponding string &#x03B1;' associated with Y '. Prior to this work, explicit constructions of correlation breakers with advice require the entropy of the involved random variables to depend linearly on the advice length. In this work, building on independence-preserving mergers, a pseudorandom primitive that was recently introduced by Cohen and Schulman, we devise a new construction of correlation breakers with advice that has optimal, logarithmic, dependence on the advice length. This enables us to obtain the following results. . We construct an extractor for 5 independent n-bit sources with min-entropy (log n)1+o(1). This result puts us tantalizingly close to the goal of constructing extractors for 2 sources with min-entropy O(log n), which would have exciting implications to Ramsey theory. . We construct non-malleable extractors with error guarantee &#x03B5; for n-bit sources, with seed length d = O(log n)+ (log(1/&#x03B5;))1+o(1) for any min-entropy k = &#x03A9;(d). Prior to this work, all constructions require either very high minentropy or otherwise have seed length &#x03C9;(log n) for any &#x03B5;. Further, our extractor has near-optimal output length. Prior constructions that achieve comparable output length work only for very high min-entropy k &#x2248; n/2. . By instantiating the Dodis-Wichs framework with our non-malleable extractor, we obtain near-optimal privacy amplification protocols against active adversaries, improving upon all (incomparable) known protocols.
[Correlation, Protocols, extractors, privacy amplification, random variable, Entropy, Ramsey theory, error guarantee, near-optimal privacy amplification protocols, Privacy, entropy, pseudorandom primitive, Corporate acquisitions, independent n-bit sources, Buildings, min-entropy, fixed string, independence-preserving mergers, random sequences, correlation breakers, non-malleable, nonmalleable extractors, advice length, data privacy, Random variables]
The Hilbert Function, Algebraic Extractors, and Recursive Fourier Sampling
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
In this paper, we apply tools from algebraic geometry to prove new results concerning extractors for algebraic sets, the recursive Fourier sampling problem, and VC dimension. We present a new construction of an extractor which works for algebraic sets defined by polynomials over GF(2) of substantially higher degree than the current state-of-the-art construction. We also exactly determine the GF(2)-polynomial degree of the recursive Fourier sampling problem and use this to provide new partial results towards a circuit lower bound for this problem. Finally, we answer a question concerning VC dimension, interpolation degree and the Hilbert function.
[Computers, VC dimension, polynomials, recursive Fourier sampling problem, Fourier analysis, Hilbert spaces, Oracle Separations, Complexity theory, algebraic sets, Hilbert transforms, Interpolation, Quantum computing, Extractors, Polynomial Degree, Quantum mechanics, Logic gates, Hilbert function, Approximation algorithms, polynomial degree, algebraic geometry, algebraic extractors]
Computational Efficiency Requires Simple Taxation
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We characterize the communication complexity of truthful mechanisms. Our departure point is the well known taxation principle. The taxation principle asserts that every truthful mechanism can be interpreted as follows: every player is presented with a menu that consists of a price for each bundle (the prices depend only on the valuations of the other players). Each player is allocated a bundle that maximizes his profit according to this menu. We define the taxation complexity of a truthful mechanism to be the logarithm of the maximum number of menus that may be presented to a player. Our main finding is that in general the taxation complexity essentially equals the communication complexity. The proof consists of two main steps. First, we prove that for rich enough domains the taxation complexity is at most the communication complexity. We then show that the taxation complexity is much smaller than the communication complexity only in "pathological" cases and provide a formal description of these extreme cases. Next, we study mechanisms that access the valuations via value queries only. In this setting we establish that the menu complexity - a notion that was already studied in several different contexts - characterizes the number of value queries that the mechanism makes in exactly the same way that the taxation complexity characterizes the communication complexity. Our approach yields several applications, including strengthening the solution concept with low communication overhead, fast computation of prices, and hardness of approximation by computationally efficient truthful mechanisms.
[Algorithm design and analysis, Protocols, combinatorial mathematics, value queries, game theory, truthful mechanism, computational efficiency, taxation principle, formal description, taxation complexity, Complexity theory, communication complexity, Communication Complexity, Cost accounting, pathological cases, Computer science, Approximation algorithms, Resource management, simple taxation, Mechanism Design, computational complexity]
Learning in Auctions: Regret is Hard, Envy is Easy
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
An extensive body of recent work studies the welfare guarantees of simple and prevalent combinatorial auction formats, such as selling m items via simultaneous second price auctions (SiSPAs) [1], [2], [3]. These guarantees hold even when the auctions are repeatedly executed and the players use no-regret learning algorithms to choose their actions. Unfortunately, off-the-shelf no-regret learning algorithms for these auctions are computationally inefficient as the number of actions available to the players becomes exponential. We show that this obstacle is inevitable: there are no polynomial-time no-regret learning algorithms for SiSPAs, unless RP &#x2287; NP, even when the bidders are unit-demand. Our lower bound raises the question of how good outcomes polynomially-bounded bidders may discover in such auctions. To answer this question, we propose a novel concept of learning in auctions, termed "no-envy learning." This notion is founded upon Walrasian equilibrium, and we show that it is both efficiently implementable and results in approximately optimal welfare, even when the bidders have valuations from the broad class of fractionally subadditive (XOS) valuations (assuming demand oracle access to the valuations) or coverage valuations (even without demand oracles). No-envy learning outcomes are a relaxation of no-regret learning outcomes, which maintain their approximate welfare optimality while endowing them with computational tractability. Our positive and negative results extend to several auction formats that have been studied in the literature via the smoothness paradigm. Our positive results for XOS valuations are enabled by a novel Follow-The-Perturbed-Leader algorithm for settings where the number of experts and states of nature are both infinite, and the payoff function of the learner is non-linear. We show that this algorithm has applications outside of auction settings, establishing significant gains in a recent application of no-regret learning in security games. Our efficient learning result for coverage valuations is based on a novel use of convex rounding schemes and a reduction to online convex optimization.
[Algorithm design and analysis, Additives, simultaneous second price auctions, mechanism design, smoothness paradigm, commerce, SiSPA, Cost accounting, price of anarchy, demand oracle access, Silicon, learning (artificial intelligence), auctions, Economics, welfare optimality, no-regret learning, polynomials, learning algorithms, convex programming, Walrasian equilibrium, follow-the-perturbed-leader algorithm, Partitioning algorithms, computational tractability, convex rounding schemes, polynomially bounded bidders, Resource management, online convex optimization, combinatorial auction formats, online learning, computational complexity]
On the Communication Complexity of Approximate Fixed Points
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We study the two-party communication complexity of finding an approximate Brouwer fixed point of a composition of two Lipschitz functions g o f: [0,1]n &#x2192; [0,1]n, where Alice holds f and Bob holds g. We prove an exponential (in n) lower bound on the deterministic communication complexity of this problem. Our technical approach is to adapt the Raz-McKenzie simulation theorem (FOCS 1999) into geometric settings, thereby "smoothly lifting" the deterministic query lower bound for finding an approximate fixed point (Hirsch, Papadimitriou and Vavasis, Complexity 1989) from the oracle model to the two-party model. Our results also suggest an approach to the well-known open problem of proving strong lower bounds on the communication complexity of computing approximate Nash equilibria. Specifically, we show that a slightly "smoother" version of our fixed-point computation lower bound (by an absolute constant factor) would imply that: The deterministic two-party communication complexity of finding an &#x2208; = &#x03A9;(1/log2 N)-approximate Nash equilibrium in an N &#x00D7; N bimatrix game (where each player knows only his own payoff matrix) is at least N&#x03B3; for some constant &#x03B3; &gt; 0. (In contrast, the nondeterministic communication complexity of this problem is only O(log6 N)). ; The deterministic (Number-In-Hand) multiparty communication complexity of finding an &#x2208; = &#x03A9;(1)-Nash equilibrium in a k-player constant-action game is at least 2&#x03A9;(k/log k) (while the nondeterministic communication complexity is only O(k)).
[Adaptation models, approximate fixed points, geometric settings, Nash equilibrium, Complexity theory, communication complexity, Raz-McKenzie simulation theorem, approximate Brouwer fixed point, two Lipschitz functions, payoff matrix, Brouwer's Fixed-Point Theorem, approximation theory, bimatrix game, Computational modeling, game theory, multiparty communication complexity, deterministic communication complexity, Computer science, Simulation Theorems, absolute constant factor, approximate Nash equilibria, Games, Approximation algorithms, k-player constant-action game, computational complexity, Approximate Nash Equilibrium]
Informational Substitutes
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We propose definitions of substitutes and complements for pieces of information ("signals") in the context of a decision or optimization problem, with game-theoretic and algorithmic applications. In a game-theoretic context, substitutes capture diminishing marginal value of information to a rational decision maker. There, we address the main open problem in a fundamental strategic-information-revelation setting, prediction markets. We show that substitutes characterize "best-possible" equilibria with immediate information aggregation, while complements characterize "worst-possible\
[Algorithm design and analysis, decision problem, optimization problem, Lattices, information acquisition, value of information, Optimization, Rain, optimisation, game theoretic applications, complements, Prediction algorithms, information aggregation, marginal value, Context, rational decision maker, algorithmic context, information signals, substitutes, game theory, game theoretic context, informational substitutes, question-and-answer forums, prediction markets, submodularity, encourage substitutability, Approximation algorithms, crowdsourcing contests, decision problems]
Constrained Submodular Maximization: Beyond 1/e
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
In this work, we present a new algorithm for maximizing a non-monotone submodular function subject to a general constraint. Our algorithm finds an approximate fractional solution for maximizing the multilinear extension of the function over a down-closed polytope. The approximation guarantee is 0.372 and it is the first improvement over the 1/e approximation achieved by the unified Continuous Greedy algorithm [Feldman et al., FOCS 2011].
[Greedy algorithms, Algorithm design and analysis, greedy algorithms, Optimized production technology, maximization, down-closed polytope, Electronic mail, unified continuous greedy algorithm, nonmonotone submodular function, Standards, submodular functions, Computer science, optimisation, constrained submodular maximization, 1/e approximation, general constraint, Approximation algorithms]
Settling the Complexity of Computing Approximate Two-Player Nash Equilibria
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We prove that there exists a constant &#x03B5; &gt; 0 such that, assuming the Exponential Time Hypothesis for PPAD, computing an &#x03B5;-approximate Nash equilibrium in a two-player (n &#x00D7; n) game requires quasi-polynomial time, nlog1-o(1) n. This matches (up to the o(1) term) the algorithm of Lipton, Markakis, and Mehta [54]. Our proof relies on a variety of techniques from the study of probabilistically checkable proofs (PCP), this is the first time that such ideas are used for a reduction between problems inside PPAD. En route, we also prove new hardness results for computing Nash equilibria in games with many players. In particular, we show that computing an &#x03B5;-approximate Nash equilibrium in a game with n players requires 2&#x03A9;(n) oracle queries to the payoff tensors. This resolves an open problem posed by Hart and Nisan [43], Babichenko [13], and Chen et al. [28]. In fact, our results for n-player games are stronger: they hold with respect to the (&#x03B5;,&#x03B4;)-WeakNash relaxation recently introduced by Babichenko et al. [15].
[probabilistically checkable proofs, probability, PPAD, game theory, exponential time hypothesis, (&#x03B5;, Nash equilibrium, Probabilistic logic, Encoding, payoff tensors, Complexity theory, &#x03B4;)-WeakNash relaxation, Computational complexity, query processing, Games, Approximation algorithms, Error correction codes, theorem proving, &#x03B5;-approximate two-player Nash equilibria complexity, 2&#x03A9;(n) oracle queries, computational complexity]
Fast Learning Requires Good Memory: A Time-Space Lower Bound for Parity Learning
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We prove that any algorithm for learning parities requires either a memory of quadratic size or an exponential number of samples. This proves a recent conjecture of Steinhardt, Valiant and Wager [15] and shows that for some learning problems a large storage space is crucial. More formally, in the problem of parity learning, an unknown string x &#x03F5; {0,1}n was chosen uniformly at random. A learner tries to learn x from a stream of samples (a<sub>1</sub>, b<sub>1</sub>), (a<sub>2</sub>, b<sub>2</sub>)..., where each at is uniformly distributed over {0,1}n and bt is the inner product of a<sub>t</sub> and x, modulo 2. We show that any algorithm for parity learning, that uses less than n2/25 bits of memory, requires an exponential number of samples. Previously, there was no non-trivial lower bound on the number of samples needed, for any learning problem, even if the allowed memory size is O(n) (where n is the space needed to store one sample). We also give an application of our result in the field of bounded-storage cryptography. We show an encryption scheme that requires a private key of length n, as well as time complexity of n per encryption/decryption of each bit, and is provenly and unconditionally secure as long as the attacker uses less than n2/25 memory bits and the scheme is used at most an exponential number of times. Previous works on bounded-storage cryptography assumed that the memory size used by the attacker is at most linear in the time needed for encryption/decryption.
[Protocols, Computational modeling, parity learning, bit encryption, time complexity, cryptography, Encryption, bounded-storage cryptography, lower bounds, parity-learning, storage management, bit decryption, Memory management, quadratic size memory, storage space, time-space lower bound, memory size, time-space tradeoff, learning (artificial intelligence), bounded storage cryptography, Time complexity, branching-program, computational complexity]
Ramanujan Graphs in Polynomial Time
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Recent work by Marcus, Spielman and Srivastava proves the existence of bipartite Ramanujan (multi) graphs of all degrees and all sizes. However, that paper did not provide a polynomial time algorithm to actually compute such graphs. Here, we provide a polynomial time algorithm to compute certain expected characteristic polynomials related to this construction. This leads to a deterministic polynomial time algorithm to compute bipartite Ramanujan (multi) graphs of all degrees and all sizes.
[Symmetric matrices, graph theory, deterministic polynomial time algorithm, constructive, Probability distribution, Ramanujan graph, Indexes, bipartite Ramanujan multigraphs, Computer science, characteristic polynomial, interlacing family, Eigenvalues and eigenfunctions, Nickel, Bipartite graph, characteristic polynomials, computational complexity]
Structure of Protocols for XOR Functions
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Let f be a boolean function on n variables. Its associated XOR function is the two-party function F(x, y) = f(x xor y). We show that, up to polynomial factors, the deterministic communication complexity of F is equal to the parity decision tree complexity of f. This relies on a novel technique of entropy reduction for protocols, combined with existing techniques in Fourier analysis and additive combinatorics.
[Protocols, Additives, entropy reduction, Fourier analysis, Boolean function, Entropy, Complexity theory, communication complexity, two-party function, deterministic communication complexity, Computer science, polynomial factors, Boolean functions, entropy, Parity decision tree, decision trees, Communication complexity, Decision trees, protocols, XOR functions, additive combinatorics, parity decision tree complexity]
The Multiparty Communication Complexity of Interleaved Group Products
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Party A<sub>i</sub> of k parties A<sub>1</sub>,...,A<sub>k</sub> receives on its forehead a t-tuple (a<sub>i1</sub>,...,a<sub>it</sub>) of elements from the group G = SL(2, q). The parties are promised that the interleaved product a<sub>11</sub>...a<sub>k1</sub>a<sub>12</sub>...a<sub>k2</sub>...a<sub>1t</sub>...a<sub>kt</sub> is equal either to the identity e or to some other fixed element g &#x2208; G. Their goal is to determine which of e and g the interleaved product is equal to, using the least amount of communication. We show that for all fixed k and all sufficiently large t the communication is &#x03A9;(t log |G|), which is tight. As an application, we establish the security of the leakage-resilient circuits studied by Miles and Viola (STOC 2013) in the "only computation leaks" model. Our main technical contribution is of independent interest. We show that if X is a probability distribution on Gm such that any two coordinates are uniform in G2, then a pointwise product of s independent copies of X is nearly uniform in Gm, where s depends on m only.
[Protocols, special linear group, Computational modeling, interleaved group product, mixing, Complexity theory, multiparty communication complexity, communication complexity, pointwise product, quasirandom group, leakage-resilient, group theory, Convolution, interleaved group products, Wires, number-on-forehead, iterated group products, probability distribution, Cryptography, leakage-resilient circuits security, Integrated circuit modeling]
How Limited Interaction Hinders Real Communication (and What It Means for Proof and Circuit Complexity)
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We obtain the first true size-space trade-offs for the cutting planes proof system, where the upper bounds hold for size and total space for derivations with constantsize coefficients, and the lower bounds apply to length and formula space (i.e., number of inequalities in memory) even for derivations with exponentially large coefficients. These are also the first trade-offs to hold uniformly for resolution, polynomial calculus and cutting planes, thus capturing the main methods of reasoning used in current state-of-the-art SAT solvers. We prove our results by a reduction to communication lower bounds in a round-efficient version of the real communication model of [Kraj&#x0301;i&#x0303;cek '98], drawing on and extending techniques in [Raz and McKenzie '99] and [G&#x0308;o&#x0308;os et al. '15]. The communication lower bounds are in turn established by a reduction to trade-offs between cost and number of rounds in the game of [Dymond and Tompa '85] played on directed acyclic graphs. As a by-product of the techniques developed to show these proof complexity trade-off results, we also obtain an exponential separation between monotone-ACi-1 and monotone-ACi, improving exponentially over the superpolynomial separation in [Raz and McKenzie '99]. That is, we give an explicit Boolean function that can be computed by monotone Boolean circuits of depth logi n and polynomial size, but for which circuits of depth O(logi-1 n) require exponential size.
[circuit complexity, SAT solvers, proof complexity, Calculus, Cognition, Complexity theory, Boolean functions, proof complexity; communication complexity; circuit, theorem proving, directed acyclic graphs, cutting plane proof system, communication lower bounds, polynomial calculus, Computational modeling, communication model, constant-size coefficients, exponential separation, Boolean function, monotone Boolean circuits, Computer science, Upper bound, directed graphs, monotone-ACi-1, Games, size-space trade-offs, superpolynomial separation]
Amortized Dynamic Cell-Probe Lower Bounds from Four-Party Communication
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
This paper develops a new technique for proving amortized, randomized cell-probe lower bounds on dynamic data structure problems. We introduce a new randomized nondeterministic four-party communication model that enables "accelerated\
[2D-ORC, four-party communication, Computational modeling, deterministic data structure, accelerated error-preserving simulations, dynamic data structure problems, Data structures, Complexity theory, Computer science, randomized nondeterministic four-party communication model, Data models, data structures, 2D weighted orthogonal range counting problem, randomized cell-probe lower bounds, Probes, Integrated circuit modeling, amortized dynamic cell-probe lower bounds, computational complexity]
Decremental Single-Source Reachability and Strongly Connected Components in &#xd5;(m&#x221a;n) Total Update Time
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We present randomized algorithms with a total update time of O&#x0303;(m &#x221A;n) for the problems of decremental single source reachability and decremental strongly connected components on directed graphs. This improves recent breakthrough results of Henzinger, Krinninger and Nanongkai [STOC 14, ICALP 15]. In addition, our algorithms are arguably simpler.
[Algorithm design and analysis, reachability analysis, decremental strongly connected components, Heuristic algorithms, Particle separators, single-source reachability, Maintenance engineering, Data structures, Electronic mail, O&#x0303;(m&#x221A;n) total update time, randomised algorithms, Computer science, decremental single-source reachability, strongly connected components, directed graphs, randomized algorithms, dynamic algorithm, computational complexity]
Fully Dynamic Maximal Matching in Constant Update Time
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Baswana, Gupta and Sen [FOCS'11] showed that fully dynamic maximal matching can be maintained in general graphs with logarithmic amortized update time. More specifically, starting from an empty graph on n fixed vertices, they devised a randomized algorithm for maintaining maximal matching over any sequence of t edge insertions and deletions with a total runtime of O(t log n) in expectation and O(t log n + n log2 n) with high probability. Whether or not this runtime bound can be improved towards O(t) has remained an important open problem. Despite significant research efforts, this question has resisted numerous attempts at resolution even for basic graph families such as forests. In this paper, we resolve the question in the affirmative, by presenting a randomized algorithm for maintaining maximal matching in general graphs with constant amortized update time. The optimal runtime bound O(t) of our algorithm holds both in expectation and with high probability. As an immediate corollary, we can maintain 2-approximate vertex cover with constant amortized update time. This result is essentially the best one can hope for (under the unique games conjecture) in the context of dynamic approximate vertex cover, culminating a long line of research. Our algorithm builds on Baswana et al.'s algorithm, but is inherently different and arguably simpler. As an implication of our simplified approach, the space usage of our algorithm is linear in the (dynamic) graph size, while the space usage of Baswana et al.'s algorithm is always at least &#x03A9;(n log n). Finally, we present applications to approximate weighted matchings and to distributed networks.
[general graphs, Heuristic algorithms, dynamic matching, graph theory, simplified approach, approximate weighted matchings, graph families, Runtime, dynamic algorithm, logarithmic amortized update time, dynamic graph size, edge insertions, Fading channels, Context, approximation theory, constant update time, vertex cover, Radiation detectors, probability, randomized algorithm, maximal matching, fixed vertices, edge deletions, approximate vertex, fully dynamic maximal matching, Games, Approximation algorithms, distributed networks, Baswana Gupta, computational complexity]
On Fully Dynamic Graph Sparsifiers
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We initiate the study of fast dynamic algorithms for graph sparsification problems and obtain fully dynamic algorithms, allowing both edge insertions and edge deletions, that take polylogarithmic time after each update in the graph. Our three main results are as follows. First, we give a fully dynamic algorithm for maintaining a (1 &#x00B1; &#x03F5;)-spectral sparsifier with amortized update time poly(log n, &#x03F5;-1). Second, we give a fully dynamic algorithm for maintaining a (1 &#x00B1; &#x03F5;)-cut sparsifier with worst-case update time poly(log n, &#x03F5;-1). Both sparsifiers have size n &#x00B7; poly(log n, &#x03F5;-1). Third, we apply our dynamic sparsifier algorithm to obtain a fully dynamic algorithm for maintaining a (1 - &#x03F5;)-approximation to the value of the maximum flow in an unweighted, undirected, bipartite graph with amortized update time poly(log n, &#x03F5;-1).
[Algorithm design and analysis, Dynamic Graph Algorithms, Laplace equations, Heuristic algorithms, graph theory, Data structures, time poly(log n, unweighted graph, bipartite graph, Sparsification, graph sparsification problems, n&#x00B7;poly(log n, dynamic sparsifier algorithm, dynamic graph sparsifiers, Clustering algorithms, &#x03B5;-1), Approximation algorithms, (1&#x00B1;&#x03B5;)-spectral sparsifier, Bipartite graph, polylogarithmic time, undirected graph, computational complexity]
Linear Hashing Is Awesome
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
The most classic textbook hash function, e.g. taught in CLRS [MIT Press'09], is h(x) = ((ax + b) mod p) mod m, (&#x25C7;) where x, a, b &#x03F5; {0, 1, ..., p-1} and a, b are chosen uniformly at random. It is known that (&#x25C7;) is 2-independent and almost uniform provided p is a prime and p &#x226B; m. This implies that when using (&#x25C7;) to build a hash table with chaining that contains n &#x2264; m keys, the expected query time is O(1) and the expected length of the longest chain is O(&#x221A;n). This result holds for any 2-independent hash function. No hash function can improve on the expected query time, but the upper bound on the expected length of the longest chain is not known to be tight for (&#x25C7;). Partially addressing this problem, Alon et al. [STOC'97] proved the existence of a class of linear hash functions such that the expected length of the longest chain is (&#x221A;n) and leave as an open problem to decide which nontrivial properties (&#x25C7;) has. We make the first progress on this fundamental problem, by showing that the expected length of the longest chain is at most n1/3o(1) which means that the performance of (&#x25C7;) is similar to that of a independent hash function for which we can prove an upper bound of O(n1/3). As a lemma we show that within a fixed set of integers there are few pairs such that the height of the ratio of the pairs are small. Given two non-zero coprime integers n, m &#x03F5; &#x2124; with the height of n/m is max t{|n|, |m|}, and the height is a way of measuring how complex a fraction is. This is proved using a mixture of techniques from additive combinatorics and number theory, and we believe that the result might be of independent interest. For a natural variation of (&#x25C7;), we show that it is possible to apply second order moment bounds even when a hash value is fixed. As a consequence: For min-wise hashing it was known that any key from a set of n keys has the smallest hash value with probability O (1&#x221A;n). We improve this to n-1+o(1). For linear probing it was known that the worst case expected query time is O (&#x221A;n). We improve this to no(1).
[Additives, combinatorial mathematics, O (1&#x221A;n), nonzero coprime integers, 2-independent hash function, hashing, textbook hash function, hashing with chaining, Electronic mail, n1/3o(1), Presses, 3-independent hash function, additive combinatorics, no(1), n-1+o(1), hash table, probability, query time, Zinc, Computer science, Upper bound, O(&#x221A;n), min-wise hashing, CLRS, file organisation, Distance measurement, O(n1/3), linear hashing, computational complexity]
Local Search Yields Approximation Schemes for k-Means and k-Median in Euclidean and Minor-Free Metrics
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We give the first polynomial-time approximation schemes (PTASs) for the following problems: (1) uniform facility location in edge-weighted planar graphs, (2) k-median and k-means in edge-weighted planar graphs, (3) k-means in Euclidean space of bounded dimension. Our first and second results extend to minor-closed families of graphs. All our results extend to cost functions that are the pth power of the shortest-path distance. The algorithm is local search where the local neighborhood of a solution S consists of all solutions obtained from S by removing and adding 1/&#x03B5;O(1) centers.
[Algorithm design and analysis, graph theory, edge-weighted planar graphs, planar graph, local search, uniform facility location, facility location, approximation schemes, PTAS, minor-free graphs, polynomial-time approximation schemes, Clustering algorithms, k-means, cost functions, Cost function, Euclidean space, search problems, Euclidean metric, graph minor-closed families, shortest-path distance, Extraterrestrial measurements, Linear programming, Euclidean metrics, k-median, Approximation algorithms, minor-free metrics, computational complexity]
Local Search Yields a PTAS for k-Means in Doubling Metrics
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
The most well known and ubiquitous clustering problem encountered in nearly every branch of science is undoubtedly k-MEANS: given a set of data points and a parameter k, select k centres and partition the data points into k clusters around these centres so that the sum of squares of distances of the points to their cluster centre is minimized. Typically these data points lie in Euclidean space Rd for some d &#x2265; 2. k-MEANS and the first algorithms for it were introduced in the 1950's. Over the last six decades, hundreds of papers have studied this problem and different algorithms have been proposed for it. The most commonly used algorithm in practice is known as Lloyd-Forgy, which is also referred to as "the" k-MEANS algorithm, and various extensions of it often work very well in practice. However, they may produce solutions whose cost is arbitrarily large compared to the optimum solution. Kanungo et al. [2004] analyzed a very simple local search heuristic to get a polynomial-time algorithm with approximation ratio 9 + &#x03B5; for any fixed &#x03B5; &gt; 0 for k-Umeans in Euclidean space. Finding an algorithm with a better worst-case approximation guarantee has remained one of the biggest open questions in this area, in particular whether one can get a true PTAS for fixed dimension Euclidean space. We settle this problem by showing that a simple local search algorithm provides a PTAS for k-MEANS for Rd for any fixed d. More precisely, for any error parameter &#x03B5; &gt; 0, the local search algorithm that considers swaps of up to &#x03C1; = dO(d) &#x00B7; &#x03B5;-O(d/&#x03B5;) centres at a time will produce a solution using exactly k centres whose cost is at most a (1+&#x03B5;)-factor greater than the optimum solution. Our analysis extends very easily to the more general settings where we want to minimize the sum of q'th powers of the distances between data points and their cluster centres (instead of sum of squares of distances as in k-MEANS) for any fixed q &#x2265; 1 and where the metric may not be Euclidean but still has fixed doubling dimension.
[Algorithm design and analysis, fixed doubling dimension, fixed dimension Euclidean space, Heuristic algorithms, data points, Search problems, error parameter, doubling metrics, Lloyd-Forgy, ubiquitous computing, optimum solution, #NAME?, PTAS, local search yields, pattern clustering, Clustering algorithms, Euclidean distance, Approximation algorithms, ubiquitous clustering problem, data handling, simple local search algorithm, polynomial-time algorithm, search problems, cluster centre]
Truly Sub-cubic Algorithms for Language Edit Distance and RNA-Folding via Fast Bounded-Difference Min-Plus Product
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
It is a major open problem whether the (min,+)-product of two n by n matrices has a truly sub-cubic time algorithm, as it is equivalent to the famous All-Pairs-Shortest-Paths problem (APSP) in n-vertex graphs. There are some restrictions of the (min,+)-product to special types of matrices that admit truly sub-cubic algorithms, each giving rise to a special case of APSP that can be solved faster. In this paper we consider a new, different and powerful restriction in which one matrix can be arbitrary, as long as the other matrix has "bounded differences" in either its columns or rows, i.e. any two consecutive entries differ by only a small amount. We obtain the first truly sub-cubic algorithm for this Bounded Differences (min,+)-product (answering an open problem of Chan and Lewenstein). Our new algorithm, combined with a strengthening of an approach of L. Valiant for solving context-free grammar parsing with matrix multiplication, yields the first truly sub-cubic algorithms for the following problems: Language Edit Distance (a major problem in the parsing community), RNA-folding (a major problem in bioinformatics) and Optimum Stack Generation (answering an open problem of Tarjan).
[Context, min-plus matrix multiplication, Additives, RNA-folding, graph theory, language edit distance, bounded differences, context-free grammar parsing, RNA folding, Grammar, Electronic mail, n-vertex graphs, optimum stack generation, truly sub-cubic algorithm, Computer science, matrix multiplication, APSP, grammars, Production, Approximation algorithms, fast bounded-difference min-plus product, fast matrix multiplication, truly subcubic time algorithm, all-pairs-shortest-paths problem]
Knuth Prize Lecture: Complexity of Communication in Markets
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Summary form only given. A classical point of view in Economic Theory is that prices in markets serve as a communication mechanism between the participants (buyers and sellers) in the market. I will analyze the communication complexity (in the standard sense used in Theoretical Computer Science) required for obtaining efficiency and equilibrium in several scenarios of markets of indivisible goods.
[Economics, Knuth prize lecture, economic theory, Complexity theory, communication complexity, Standards, Computer science, marketing, economics, markets, theoretical computer science, communication mechanism, indivisible goods, prices, pricing, computational complexity]
No Occurrence Obstructions in Geometric Complexity Theory
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
The permanent versus determinant conjecture is a major problem in complexity theory that is equivalent to the separation of the complexity classes VP ws and VNP. Mulmuley and Sohoni [SIAM J Comput 2001] suggested 8to study a strengthened version of this conjecture over the complex numbers that amounts to separating the orbit closures of the determinant and padded permanent polynomials. In that paper it was also proposed to separate these orbit closures by exhibiting occurrence obstructions, which are irreducible representations of GLn2(C), which occur in one coordinate ring of the orbit closure, but not in the other. We prove that this approach is impossible. However, we do not rule out the approach to the permanent versus determinant problem via multiplicity obstructions as proposed by in [32].
[determinant orbit closures, polynomials, Young tableaux, padded permanent polynomials, VP<sub>ws</sub>, Orbits, Stability analysis, tensors, Complexity theory, Electronic mail, Topology, representations, geometric complexity theory, Space vehicles, Geometry, occurrence obstructions, permanent-determinant conjecture, plethysms, highest weight vectors, VNP, computational complexity, Permanent versus determinant, orbit closures]
Rectangular Kronecker Coefficients and Plethysms in Geometric Complexity Theory
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
The geometric complexity theory program is an approach to separate algebraic complexity classes, more precisely to show the superpolynomial growth of the determinantal complexity dc(per<sub>m</sub>) of the permanent polynomial. Mulmuley and Sohoni showed that the vanishing behaviour of rectangular Kronecker coefficients could in principle be used to show some lower bounds on dc(per<sub>m</sub>) and they conjectured that superpolynomial lower bounds on dc(per<sub>m</sub>) could be shown in this way. In this paper we disprove this conjecture by Mulmuley and Sohoni, i.e., we prove that the vanishing of rectangular Kronecker coefficients cannot be used to prove superpolynomial lower bounds on dc(per<sub>m</sub>).
[Shape, rectangular Kronecker coefficients, algebraic complexity classes, permanent polynomial, plethysm coefficients, Superluminescent diodes, Orbits, superpolynomial growth, Kronecker coefficients, Complexity theory, Electronic mail, superpolynomial lower bounds, geometric complexity theory, Space vehicles, Computer science, geometric complexity theory program, occurrence obstructions, plethysms, determinantal complexity, permanent, geometry, computational complexity, determinant]
Exponential Lower Bounds for Monotone Span Programs
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Monotone span programs are a linear-algebraic model of computation which were introduced by Karchmer and Wigderson in 1993 [1]. They are known to be equivalent to linear secret sharing schemes, and have various applications in complexity theory and cryptography. Lower bounds for monotone span programs have been difficult to obtain because they use non-monotone operations to compute monotone functions, in fact, the best known lower bounds are quasipolynomial for a function in (nonmonotone) P [2]. A fundamental open problem is to prove exponential lower bounds on monotone span program size for any explicit function. We resolve this open problem by giving exponential lower bounds on monotone span program size for a function in monotone P. This also implies the first exponential lower bounds for linear secret sharing schemes. Our result is obtained by proving exponential lower bounds using Razborov's rank method [3], a measure that is strong enough to prove lower bounds for many monotone models. As corollaries we obtain new proofs of exponential lower bounds for monotone formula size, monotone switching network size, and the first lower bounds for monotone comparator circuit size for a function in monotone P. We also obtain new polynomial degree lower bounds for Nullstellensatz refutations using an interpolation theorem of Pudlak and Sgall [4]. Finally, we obtain quasipolynomial lower bounds on the rank measure for the st-connectivity function, implying tight bounds for st-connectivity in all of the computational models mentioned above.
[linear secret sharing schemes, nonmonotone operations, Secret Sharing, Switches, Complexity theory, monotone comparator circuit size, monotone formula size, monotone switching network size, Boolean functions, Switching Networks, interpolation theorem, Cryptography, linear algebra, complexity theory, Monotone, Span Programs, Computational modeling, linear algebraic model, cryptography, monotone span programs, Nullstellensatz, Sorting, exponential lower bounds, interpolation, Comparator Circuits, fundamental open problem, Nullstellensatz refutations, Integrated circuit modeling, Lower Bounds]
A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of Agents
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We consider the well-studied cake cutting problem in which the goal is to find an envy-free allocation based on queries from n agents. The problem has received attention in computer science, mathematics, and economics. It has been a major open problem whether there exists a discrete and bounded envy-free protocol. We resolve the problem by proposing a discrete and bounded envy-free protocol for any number of agents. The maximum number of queries required by the protocol is nnnnnn. Even if we do not run our protocol to completion, it can find in at most nn+1 queries an envy-free partial allocation of the cake in which each agent gets at least 1/n of the value of the whole cake.
[Protocols, multi-agent systems, envy-free, Mathematics, Electronic mail, Cost accounting, Computer science, query processing, agent queries, discrete envy-free cake cutting protocol, resource allocation, cake cutting, envy-free partial cake allocation, fair division, Resource management, Australia, protocols, bounded envy-free cake cutting protocol]
A Nearly Tight Sum-of-Squares Lower Bound for the Planted Clique Problem
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We prove that with high probability over the choice of a random graph G from the Erdo&#x0308;s-Re&#x0301;nyi distribution G(n,1/2), the nO(d)-time degree d Sum-of-Squares semidefinite programming relaxation for the clique problem will give a value of at least n1/2-c(d/log n)1/2 for some constant c &gt; 0. This yields a nearly tight n1/2-o(1) bound on the value of this program for any degree d = o(log n). Moreover we introduce a new framework that we call pseudo-calibration to construct Sum-of-Squares lower bounds. This framework is inspired by taking a computational analogue of Bayesian probability theory. It yields a general recipe for constructing good pseudo-distributions (i.e., dual certificates for the Sum-of-Squares semidefinite program), and sheds further light on the ways in which this hierarchy differs from others.
[Algorithm design and analysis, planted clique problem, pseudo-calibration, computational analogue, random graph, graph theory, algorithm design and analysis, Sum-of-Squares semidefinite programming relaxation, Observers, Programming, Complexity theory, statistical distributions, Bayesian probability theory, mathematical programming, Erdos-Renyi distribution, nO(d)-time degree, n1/2-c(d/log n)1/2, Bayes methods, n1/2-o(1), computational complexity]
Polynomial-Time Tensor Decompositions with Sum-of-Squares
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We give new algorithms based on the sum-of-squares method for tensor decomposition. Our results improve the best known running times from quasi-polynomial to polynomial for several problems, including decomposing random overcomplete 3-tensors and learning overcomplete dictionaries with constant relative sparsity. We also give the first robust analysis for decomposing overcomplete 4-tensors in the smoothed analysis model. A key ingredient of our analysis is to establish small spectral gaps in moment matrices derived from solutions to sum-of-squares relaxations. To enable this analysis we augment sum-of-squaresrelaxations with spectral analogs of maximum entropy constraints.
[Algorithm design and analysis, polynomial-time tensor decompositions, Entropy, tensors, sum-of-squares relaxations, smoothed analysis model, sum-of-squares method, constant relative sparsity, relaxation theory, Lasserre hierarchy, Robustness, random overcomplete 3-tensors decomposition, maximum entropy constraints, spectral gaps, running times, smoothed analysis, Matrix decomposition, moment matrices, matrix algebra, overcomplete dictionaries, Computer science, overcomplete 4-tensors decomposition, Tensile stress, Approximation algorithms, FOOBI algorithm, computational complexity, tensor decomposition, simultaneous diagonalization]
Towards Strong Reverse Minkowski-Type Inequalities for Lattices
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We present a natural reverse Minkowski-type inequality for lattices, which gives upper bounds on the number of lattice points in a Euclidean ball in terms of sublattice determinants, and conjecture its optimal form. The conjecture exhibits a surprising wealth of connections to various areas in mathematics and computer science, including a conjecture motivated by integer programming by Kannan and Lovasz (Annals of Math. 1988), a question from additive combinatorics asked by Green, a question on Brownian motions asked by Saloff-Coste (Colloq. Math. 2010), a theorem by Milman and Pisier from convex geometry (Ann. Probab. 1987), worst-case to average-case reductions in lattice-based cryptography, and more. We present these connections, provide evidence for the conjecture, and discuss possible approaches towards a proof. Our main technical contribution is in proving that our conjecture implies the l2 case of the Kannan and Lovasz conjecture. The proof relies on a novel convex relaxation for the covering radius, and a rounding procedure based on "uncrossing" lattice subspaces.
[Additives, reverse Minkowski-type inequalities, integer programming, Lattices, upper bounds, Euclidean ball, covering radius, uncrossing lattice subspaces, Covariance matrices, Minkowski's first theorem, relaxation theory, additive combinatorics, sublattice determinants, Smoothing methods, cryptography, Zinc, Geometry, Computer science, average-case reductions, Brownian motions, geometry, lattice points, convex geometry, convex relaxation, lattices, lattice-based cryptography, computational complexity]
Which Regular Expression Patterns Are Hard to Match?
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Regular expressions constitute a fundamental notion in formal language theory and are frequently used in computer science to define search patterns. In particular, regular expression matching and membership testing are widely used computational primitives, employed in many programming languages and text processing utilities. A classic algorithm for these problems constructs and simulates a non-deterministic finite automaton corresponding to the expression, resulting in an O(m n) running time (where m is the length of the pattern and n is the length of the text). This running time can be improved slightly (by a polylogarithmic factor), but no significantly faster solutions are known. At the same time, much faster algorithms exist for various special cases of regular expressions, including dictionary matching, wildcard matching, subset matching, word break problem etc. In this paper, we show that the complexity of regular expression matching can be characterized based on its depth (when interpreted as a formula). Our results hold for expressions involving concatenation, OR, Kleene star and Kleene plus. For regular expressions of depth two (involving any combination of the above operators), we show the following dichotomy: matching and membership testing can be solved in near-linear time, except for "concatenations of stars\
[SETH, Dictionaries, pattern matching, OR, Heuristic algorithms, regular expression patterns, dynamic programming, Kleene star, Complexity theory, classification, regular expression matching complexity, Standards, Computer languages, Kleene plus, concatenation, dynamic programming algorithm, Regular expressions, strong exponential time hypothesis, Pattern matching, membership testing, conditional hardness, Testing, computational complexity]
Polynomial Representations of Threshold Functions and Algorithmic Applications
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We design new polynomials for representing threshold functions in three different regimes: probabilistic polynomials of low degree, which need far less randomness than previous constructions, polynomial threshold functions (PTFs) with "nice" threshold behavior and degree almost as low as the probabilistic polynomials, and a new notion of probabilistic PTFs where we combine the above techniques to achieve even lower degree with similar "nice" threshold behavior. Utilizing these polynomial constructions, we design faster algorithms for a variety of problems: &#x00B7; Offline Hamming Nearest (and Furthest) Neighbors: Given n red and n blue points in d-dimensional Hamming space for d = c log n, we can find an (exact) nearest (or furthest) blue neighbor for every red point in randomized time n2-1/O(&#x221A;clog2/3 c) or deterministic time n2-1/O(c log2 c). These improve on a randomized n2-1/O(c log2 c) bound by Alman and Williams (FOCS'15), and also lead to faster MAX-SAT algorithms for sparse CNFs. &#x00B7; Offline Approximate Nearest (and Furthest) Neighbors: Given n red and n blue points in d-dimensional &#x2113;<sub>1</sub> or Euclidean space, we can find a (1+&#x03B5;)-approximate nearest (or furthest) blue neighbor for each red point in randomized time near dn+n2-&#x03A9;(&#x03B5;1/3/log(1/&#x03B5;)). This improves on an algorithm by Valiant (FOCS'12) with randomized time near dn+n2-&#x03A9;(&#x221A;&#x03B5;), which in turn improves previous methods based on locality-sensitive hashing. &#x00B7; SAT Algorithms and Lower Bounds for Circuits With Linear Threshold Functions: We give a satisfiability algorithm for AC0[m] o LTF LTF circuits with a subquadratic number of LTF gates on the bottom layer, and a subexponential number of gates on the other layers, that runs in deterministic 2n-n&#x03B5; time. This strictly generalizes a SAT algorithm for ACC0 oLTF circuits of subexponential size by Williams (STOC'14) and also implies new circuit lower bounds for threshold circuits, improving a recent gate lower bound of Kane and Williams (STOC'16). We also give a randomized 2n-n&#x03B5;-time SAT algorithm for subexponential-size MAJ o AC<sub>0</sub> oLTF o AC<sub>0</sub> oLTF circuits, where the top MAJ gate and middle LTF gates have O(n6/5-&#x03B4;) fan-in.
[Algorithm design and analysis, subquadratic number, PTF, polynomials, probabilistic polynomials, probability, Offline Hamming nearest neighbors, Probabilistic logic, threshold behavior, algorithmic applications, MAX-SAT algorithms, offline hamming furthest neighbors, subexponential number, Computer science, polynomial threshold functions, Boolean functions, polynomial representations, polynomial constructions, Logic gates, Chebyshev approximation, Approximation algorithms, Euclidean space, threshold functions, computational complexity]
Popular Conjectures as a Barrier for Dynamic Planar Graph Algorithms
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
The dynamic shortest paths problem on planar graphs asks us to preprocess a planar graph G such that we may support insertions and deletions of edges in G as well as distance queries between any two nodes u, v subject to the constraint that the graph remains planar at all times. This problem has been extensively studied in both the theory and experimental communities over the past decades. The best known algorithm performs queries and updates in O&#x0303;(n2/3) time, based on ideas of a seminal paper by Fakcharoenphol and Rao [FOCS'01]. A (1+&#x03B5;)-approximation algorithm of Abraham et al. [STOC'12] performs updates and queries in O&#x0303;(&#x221A;n) time. An algorithm with a more practical O(polylog(n)) runtime would be a major breakthrough. However, such runtimes are only known for a (1+&#x03B5;)-approximation in a model where only restricted weight updates are allowed due to Abraham et al. [SODA'16], or for easier problems like connectivity. In this paper, we follow a recent and very active line of work on showing lower bounds for polynomial time problems based on popular conjectures, obtaining the first such results for natural problems in planar graphs. Such results were previously out of reach due to the highly non-planar nature of known reductions and the impossibility of "planarizing gadgets". We introduce a new framework which is inspired by techniques from the literatures on distance labelling schemes and on parameterized complexity. Using our framework, we show that no algorithm for dynamic shortest paths or maximum weight bipartite matching in planar graphs can support both updates and queries in amortized O(n1/2-&#x03B5;) time, for any &#x03B5;&gt;0, unless the classical all-pairs-shortest-paths problem can be solved in truly subcubic time, which is widely believed to be impossible. We extend these results to obtain strong lower bounds for other related problems as well as for possible trade-offs between query and update time. Interestingly, our lower bounds hold even in very restrictive models where only weight updates are allowed.
[pattern matching, polynomial time problems, Heuristic algorithms, Roads, graph theory, dynamic planar graph, planar graphs, parameterized complexity, popular conjectures, Complexity theory, (1+&#x03B5;)-approximation algorithm, all-pairs-shortest-path problem, conditional lower bounds, Shortest path problem, query processing, truly subcubic time, Runtime, all pairs shortest paths, Labeling, distance queries, approximation theory, distance labelling, dynamic shortest paths, edge insertion, edge deletion, dynamic distance oracles, planarizing gadgets, Computer science, restrictive models, maximum weight bipartite matching, hardness in p, computational complexity]
Max-Information, Differential Privacy, and Post-selection Hypothesis Testing
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
In this paper, we initiate a principled study of how the generalization properties of approximate differential privacy can be used to perform adaptive hypothesis testing, while giving statistically valid p-value corrections. We do this by observing that the guarantees of algorithms with bounded approximate max-information are sufficient to correct the p-values of adaptively chosen hypotheses, and then by proving that algorithms that satisfy (&#x2208;,&#x03B4;)-differential privacy have bounded approximate max information when their inputs are drawn from a product distribution. This substantially extends the known connection between differential privacy and max-information, which previously was only known to hold for (pure) (&#x2208;,0)-differential privacy. It also extends our understanding of max-information as a partially unifying measure controlling the generalization properties of adaptive data analyses. We also show a lower bound, proving that (despite the strong composition properties of max-information), when data is drawn from a product distribution, (&#x2208;,&#x03B4;)-differentially private algorithms can come first in a composition with other algorithms satisfying max-information bounds, but not necessarily second if the composition is required to itself satisfy a nontrivial max-information bound. This, in particular, implies that the connection between (&#x2208;,&#x03B4;)-differential privacy and max-information holds only for inputs drawn from product distributions, unlike the connection between (&#x2208;,0)-differential privacy and max-information.
[Algorithm design and analysis, Data privacy, approximation theory, Data analysis, data analysis, product distribution, Probability, adaptive data analyses, post-selection hypothesis testing, adaptive data analysis, adaptive hypothesis testing, Computer science, Privacy, differentially private algorithms, bounded approximate max-information, data privacy, differential privacy, Testing, p-value corrections, nontrivial max-information bound]
Lipschitz Extensions for Node-Private Graph Statistics and the Generalized Exponential Mechanism
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Lipschitz extensions were proposed as a tool for designing differentially private algorithms for approximating graph statistics. However, efficiently computable Lipschitz extensions were known only for 1-dimensional functions (that is, functions that output a single real value). We study efficiently computable Lipschitz extensions for multi-dimensional (that is, vector-valued) functions on graphs. We show that, unlike for 1-dimensional functions, Lipschitz extensions of higher-dimensional functions on graphs do not always exist, even with a non-unit stretch. We design Lipschitz extensions with small stretch for the sorted degree list and degree distribution of a graph, viewed as functions from the space of graphs equipped with the node distance into real space equipped with l1. Our extensions are from the space of bounded-degree graphs to the space of arbitrary graphs. The extensions use convex programming and are efficiently computable. We also develop a new tool for employing Lipschitz extensions in differentially private algorithms that operate with no prior knowledge of the graph (and, in particular, no knowledge of the degree bound). Specifically, we generalize the exponential mechanism, a widely used tool in data privacy. The exponential mechanism is given a collection of score functions that map datasets to real values. It returns the name of the function with nearly minimum value on the dataset. Our generalized exponential mechanism provides better accuracy than the standard exponential mechanism when the sensitivity of an optimal score function is much smaller than the maximum sensitivity over all score functions. We use our Lipschitz extensions and the generalized exponential mechanism to design a node differentially private algorithm for approximating the degree distribution of a sensitive graph. Our algorithm is much more accurate than those from previous work. In particular, our algorithm is accurate on all graphs whose degree distributions decay at least as fast as those of "scale-free" graphs. Using our methodology, we also obtain more accurate node-private algorithms for 1-dimensional statistics.
[Algorithm design and analysis, Data privacy, degree distributions decay, graph theory, score functions, private algorithms, higher dimensional functions, approximating graph statistics, graph distribution, single real value, Privacy, scale-free graphs, graph algorithms, approximation theory, node private graph statistics, convex programming, Functional analysis, degree bound, computable Lipschitz extensions, node distance, Sensitivity, generalized exponential mechanism, Approximation algorithms, data privacy, statistical analysis, Lipschitz extensions, differential privacy]
The Constant Inapproximability of the Parameterized Dominating Set Problem
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We prove that there is no fpt-algorithm that can approximate the dominating set problem with any constant ratio, unless FPT = W[1]. Our hardness reduction is built on the second author's recent W[1]-hardness proof of the biclique problem [25]. This yields, among other things, a proof without the PCP machinery that the classical dominating set problem has no polynomial time constant approximation under the exponential time hypothesis.
[hardness reduction, exponential time hypothesis, parameterized dominating set, Electronic mail, fpt-algorithm, FPT, fpt inapproximability, W[1, Machinery, dominating set, Standards, constant inapproximability, Computer science, constant ratio, 1, NP-hard problem, biclique problem, polynomial approximation, Approximation algorithms, computational complexity, polynomial time constant approximation]
Subexponential Parameterized Algorithms for Planar and Apex-Minor-Free Graphs via Low Treewidth Pattern Covering
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We prove the following theorem. Given a planar graph G and an integer k, it is possible in polynomial time to randomly sample a subset A of vertices of G with the following properties: 1) A induces a subgraph of G of treewidth O(&#x221A;(k log k)), and 2) for every connected subgraph H of G on at most k vertices, the probability that A covers the whole vertex set of H is at least (2O(&#x221A;k log2 k) &#x00B7; nO(1))-1, where n is the number of vertices of G. Together with standard dynamic programming techniques for graphs of bounded treewidth, this result gives a versatile technique for obtaining (randomized) subexponential parameterized algorithms for problems on planar graphs, usually with running time bound 2O(&#x221A;(k log2 k))nO(1). The technique can be applied to problems expressible as searching for a small, connected pattern with a prescribed property in a large host graph, examples of such problems include DIRECTED k-Path, WEIGHTED k-Path, VERTEX COVER LOCAL SEARCH, and SUBGRAPH ISOMORPHISM, among others. Up to this point, it was open whether these problems can be solved in subexponential parameterized time on planar graphs, because they are not amenable to the classic technique of bidimensionality. Furthermore, all our results hold in fact on any class of graphs that exclude a fixed apex graph as a minor, in particular on graphs embeddable in any fixed surface.
[Algorithm design and analysis, Heuristic algorithms, planar graphs, vertex cover local search, random sampling, Electronic mail, subgraph isomorphism, graph vertices, Dynamic programming, search problems, apex-minor-free graphs, sampling methods, directed k-path problem, Particle separators, weighted k-path problem, subexponential parameterized algorithms, probability, trees (mathematics), random processes, dynamic programming, Standards, subexponential parameterized time, treewidth pattern covering, Approximation algorithms, subexponential algorithms, computational complexity]
Testing Assignments to Constraint Satisfaction Problems
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
For a finite relational structure A, let CSP(A) denote the CSP instances whose constraint relations are taken from A. The resulting family of problems CSP(A) has been considered heavily in a variety of computational contexts. In this article, we consider this family from the perspective of property testing: given an instance of a CSP and query access to an assignment, one wants to decide whether the assignment satisfies the instance, or is far from so doing. While previous work on this scenario studied concrete templates or restricted classes of structures, this article presents comprehensive classification theorems. Our first contribution is a dichotomy theorem completely characterizing the structures A such that CSP(A) is constant-query testable: (i) If A has a majority polymorphism and a Maltsev polymorphism, then CSP(A) is constant-query testable with one-sided error. (ii) Else, testing CSP(A) requires a super-constant number of queries. Let &#x2203;CSP(A) denote the extension of CSP(A) to instances which may include existentially quantified variables. Our second contribution is to classify all structures A in terms of the number of queries needed to test assignments to instances of &#x2203;CSP(A), with one-sided error. More specifically, we show the following trichotomy (i) If A has a majority polymorphism and a Maltsev polymorphism, then &#x2203;CSP(A) is constant-query testable with one-sided error. (ii) Else, if A has a (k + 1)-ary near-unanimity polymorphism for some k &#x2265; 2, and no Maltsev polymorphism then &#x2203;CSP(A) is not constant-query testable (even with two-sided error) but is sublinear-query testable with one-sided error. (iii) Else, testing &#x2203;CSP(A) with one-sided error requires a linear number of queries.
[Context, Algorithm design and analysis, CSP, computational contexts, dichotomy theorem, Complexity theory, near unanimity polymorphism, property testing, constraint relations, finite relational structure, Constraint Satisfaction Problems, Property Testing, query processing, testing assignments, constraint satisfaction problems, Algebra, Databases, comprehensive classification theorems, one-sided error, Concrete, constant query testable, query access, Maltsev polymorphism, Testing]
Compressing Interactive Communication under Product Distributions
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We study the problem of compressing interactive communication to its information content I, defined as the amount of information that the participants learn about each other's inputs. We focus on the case when the participants' inputs are distributed independently and show how to compress the communication to O(I log2 I) bits, with no dependence on the original communication cost. This result improves quadratically on previous work by Kol (STOC 2016) and essentially matches the well-known lower bound &#x03A9;(I).
[data compression, Protocols, interactive compression, Probability distribution, Entropy, communication cost, Complexity theory, set theory, lower bound, History, communication complexity, statistical distributions, information complexity, Computer science, information content, product distributions, Random variables, interactive communication compression, protocol compression]
Decidability of Non-interactive Simulation of Joint Distributions
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We present decidability results for a sub-class of "non-interactive" simulation problems, a well-studied class of problems in information theory. A non-interactive simulation problem is specified by two distributions P(x, y) and Q(u, v): The goal is to determine if two players, Alice and Bob, that observe sequences Xn and Yn respectively where {(Xi, Yi)}ni = 1 are drawn i.i.d. from P(x, y) can generate pairs U and V respectively (without communicating with each other) with a joint distribution that is arbitrarily close in total variation to Q(u, v). Even when P and Q are extremely simple: e.g., P is uniform on the triples (0, 0), (0,1), (1,0) and Q is a "doubly symmetric binary source\
[Regularity Lemma, Correlation, Protocols, Additives, Computational modeling, simulation, doubly symmetric binary source, Complexity theory, Boolean function analysis, Boolean functions, decidability, joint distributions, Approximation algorithms, noninteractive simulation decidability, information theory, finite domain, Information theory, Invariance Principle, Non-Interactive Simulation]
Separations in Communication Complexity Using Cheat Sheets and Information Complexity
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
While exponential separations are known between quantum and randomized communication complexity for partial functions (Raz, STOC 1999), the best known separation between these measures for a total function is quadratic, witnessed by the disjointness function. We give the first super-quadratic separation between quantum and randomized communication complexity for a total function, giving an example exhibiting a power 2.5 gap. We further present a 1.5 power separation between exact quantum and randomized communication complexity, improving on the previous &#x2248; 1.15 separation by Ambainis (STOC 2013). Finally, we present a nearly optimal quadratic separation between randomized communication complexity and the logarithm of the partition number, improving upon the previous best power 1.5 separation due to Goos, Jayram, Pitassi, and Watson. Our results are the communication analogues of separations in query complexity proved using the recent cheat sheet framework of Aaronson, Ben-David, and Kothari (STOC 2016). Our main technical results are randomized communication and information complexity lower bounds for a family of functions, called lookup functions, that generalize and port the cheat sheet framework to communication complexity.
[Protocols, partial function, Computational modeling, super-quadratic separation, Sea measurements, quantum communication complexity, query complexity, disjointness function, Complexity theory, set theory, randomized communication complexity, communication complexity, optimal quadratic separation, Computer science, Upper bound, lookup functions, function approximation, randomized algorithms, cheat sheets, quadratic function, exponential separations, quantum algorithms, information complexity lower bounds]
Extension Complexity of Independent Set Polytopes
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We exhibit an n-node graph whose independent set polytope requires extended formulations of size exponential in &#x03A9;(n/log n). Previously, no explicit examples of n-dimensional 0/1-polytopes were known with extension complexity larger than exponential in &#x0398;(&#x221A;n). Our construction is inspired by a relatively little-known connection between extended formulations and (monotone) circuit depth.
[independent set polytopes, Protocols, n-dimensional 0/1-polytopes, graph theory, size exponential, n-node graph, Search problems, Complexity theory, set theory, circuit depth, monotone depth, Matrix decomposition, communication complexity, Optimization, monotone circuit depth, Computer science, extended formulations, Games, extension complexity, computational complexity]
Approximate Gaussian Elimination for Laplacians - Fast, Sparse, and Simple
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We show how to perform sparse approximate Gaussian elimination for Laplacian matrices. We present a simple, nearly linear time algorithm that approximates a Laplacian by the product of a sparse lower triangular matrix with its transpose. This gives the first nearly linear time solver for Laplacian systems that is based purely on random sampling, and does not use any graph theoretic constructions such as low-stretch trees, sparsifiers, or expanders. Our algorithm performs a subsampled Cholesky factorization, which we analyze using matrix martingales. As part of the analysis, we give a proof of a concentration inequality for matrix martingales where the differences are sums of conditionally independent variables.
[Algorithm design and analysis, subsampled Cholesky factorization, graph theory, sparse approximate Gaussian elimination, Linear system solvers, random sampling, Linear matrix inequalities, Sparse matrices, Laplacian approximation, linear time algorithm, Gaussian elimination, matrix martingales, graph theoretic constructions, Cholesky factorization, approximation theory, Laplace equations, Symmetric matrices, sampling methods, Randomized numerical linear algebra, random processes, concentration inequality, Matrix decomposition, Matrix martingales, nearly linear time solver, Gaussian processes, Approximation algorithms, sparse lower triangular matrix, Laplacian systems, conditionally independent variables, Laplacian matrices, sparse matrices, computational complexity]
Faster Algorithms for Computing the Stationary Distribution, Simulating Random Walks, and More
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
In this paper, we provide faster algorithms for computing various fundamental quantities associated with random walks on a directed graph, including the stationary distribution, personalized PageRank vectors, hitting times, and escape probabilities. In particular, on a directed graph with n vertices and m edges, we show how to compute each quantity in time O&#x0303;(m3/4n + mn2/3), where the O&#x0303; notation suppresses polylog factors in n, the desired accuracy, and the appropriate condition number (i.e. the mixing time or restart probability). Our result improves upon the previous fastest running times for these problems; previous results either invoke a general purpose linear system solver on a n &#x00D7; n matrix with m nonzero entries, or depend polynomially on the desired error or natural condition number associated with the problem (i.e. the mixing time or restart probability). For sparse graphs, we obtain a running time of O&#x0303;(n7/4), breaking the O(n2) barrier of the best running time one could hope to achieve using fast matrix multiplication. We achieve our result by providing a similar running time improvement for solving directed Laplacian systems, a natural directed or asymmetric analog of the well studied symmetric or undirected Laplacian systems. We show how to solve such systems in time O&#x0303;(m3/4n + mn2/3), and efficiently reduce a broad range of problems to solving O&#x0303;(1) directed Laplacian systems on Eulerian graphs. We hope these results and our analysis open the door for further study into directed spectral graph theory.
[Linear systems, Algorithm design and analysis, O&#x0303;(n7/4) running time, diagonally dominant, hitting times, restart probability, Sparse matrices, polylog factors, personalized PageRank vectors, Markov chain, Clustering algorithms, O&#x0303;(m3/4n + mn2/3) time, directed spectral graph theory, solver, escape probabilities, random walks, Laplace equations, random processes, PageRank, Graph theory, Partitioning algorithms, O(n2) barrier, Laplacian, nonzero entries, matrix multiplication, natural directed analog, stationary distribution, undirected Laplacian systems, linear system solver, directed Laplacian systems, directed graphs, asymmetric analog, Eulerian graphs, fast matrix multiplication, computational complexity, mixing time, symmetric Laplacian systems]
Computing Maximum Flow with Augmenting Electrical Flows
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We present an O&#x0303; (m 7/10 U 1/7)-time algorithm for the maximum s-t flow problem (and the minimum s-t cut problem) in directed graphs with m arcs and largest integer capacity U. This matches the running time of the O&#x0303; (mU)10/7)- time algorithm of Madry [30] in the unit-capacity case, and improves over it, as well as over the O&#x0303; (m&#x221A;n log U)-time algorithm of Lee and Sidford [25], whenever U is moderately large and the graph is sufficiently sparse. By well-known reductions, this also implies similar running time improvements for the maximum-cardinality bipartite b-matching problem. One of the advantages of our algorithm is that it is significantly simpler than the ones presented in [30] and [25]. In particular, these algorithms employ a sophisticated interior-point method framework, while our algorithm is cast directly in the classic augmenting path setting that almost all the combinatorial maximum flow algorithms use. At a high level, the presented algorithm takes a primal dual approach in which each iteration uses electrical flows computations both to find an augmenting s-t flow in the current residual graph and to update the dual solution. We show that by maintain certain careful coupling of these primal and dual solutions we are always guaranteed to make significant progress.
[Algorithm design and analysis, Context, interior-point method framework, Heuristic algorithms, maximum s-t flow problem, residual graph, electrical flows computations, Couplings, Computer science, Perturbation methods, directed graphs, Approximation algorithms, minimum s-t cut problem, maximum-cardinality bipartite b-matching problem, maximum flow problem; augmenting paths; minimum, maximum flow computing, computational complexity]
Optimizing Star-Convex Functions
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Star-convexity is a significant relaxation of the notion of convexity, that allows for functions that do not have (sub)gradients at most points, and may even be discontinuous everywhere except at the global optimum. We introduce a polynomial time algorithm for optimizing the class of star-convex functions, under no Lipschitz or other smoothness assumptions whatsoever, and no restrictions except exponential boundedness on a region about the origin, and Lebesgue measurability. The algorithm's performance is polynomial in the requested number of digits of accuracy and the dimension of the search domain. This contrasts with the previous best known algorithm of Nesterov and Polyak which has exponential dependence on the number of digits of accuracy, but only n! dependence on the dimension n (where ! is the matrix multiplication exponent), and which further requires Lipschitz second differentiability of the function [1].
[Algorithm design and analysis, global optimum, search domain, convex programming, star-convex function optimization, Time measurement, Lebesgue measurable star-convex functions, polynomial time algorithm, Optimization, Standards, randomised algorithms, Computer science, algorithm performance, Approximation algorithms, Convex functions, randomized cutting plane algorithm, gradient-based optimization algorithm, gradient methods, search problems, computational complexity]
An Exponential Separation between Randomized and Deterministic Complexity in the LOCAL Model
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Over the past 30 years numerous algorithms have been designed for symmetry breaking problems in the LOCAL model, such as maximal matching, MIS, vertex coloring, and edge coloring. For most problems the best randomized algorithm is at least exponentially faster than the best deterministic algorithm. We prove that these exponential gaps are necessary and establish numerous connections between the deterministic and randomized complexities in the LOCAL model. Each of our results has a very compelling take-away message: 1) Building on the recent randomized lower bounds of Brandt et al. [1], we prove that the randomized complexity of &#x0394;-coloring a tree with maximum degree &#x0394; is O(log &#x0394; log n + log*n), for any &#x0394; &gt; = 55, whereas its deterministic complexity is &#x03A9;(log &#x0394; n) for any &#x0394; &gt; = 3. This also establishes a large separation between the deterministic complexity of &#x0394;-coloring and (&#x0394;+1)-coloring trees. 2) We prove that any deterministic algorithm for a natural class of problems that runs in O(1) + o(log &#x0394; n) rounds can be transformed to run in O(log*n - log*&#x0394; + 1) rounds. If the transformed algorithm violates a lower bound (even allowing randomization), then one can conclude that the problem requires &#x03A9;(log &#x0394; n) time deterministically. This gives an alternate proof that deterministically &#x0394;-coloring a tree with small &#x0394; takes &#x03A9;(log &#x0394; n) rounds. 3) We prove that the randomized complexity of any natural problem on instances of size n is at least its deterministic complexity on instances of size &#x221A;log n. This shows that a deterministic &#x03A9;(log &#x0394; n) lower bound for any problem (&#x0394;-coloring a tree, for example) implies a randomized &#x03A9;(log &#x0394; log n) lower bound. It also illustrates that the graph shattering technique employed in recent randomized symmetry breaking algorithms is absolutely essential to the LOCAL model. For example, it is provably impossible to improve the 2O(&#x221A;log log n) term in the complexities of the best MIS and (&#x0394;+1)-coloring algorithms without also improving the 2O(&#x221A;log n)-round Panconesi-Srinivasan algorithm.
[Algorithm design and analysis, edge coloring, 2O(&#x221A;log n)-round Panconesi-Srinivasan algorithm, &#x0394;-coloring, Complexity theory, symmetry breaking problems, (&#x0394;+1)-coloring trees, graph colouring, graph shattering technique, LOCAL model, coloring, randomized symmetry breaking algorithms, Labeling, Distributed algorithms, symmetry breaking, Computational modeling, trees (mathematics), Color, exponential separation, randomized complexity, deterministic algorithm, exponential gaps, vertex coloring, randomized lower bounds, randomised algorithms, Computer science, maximal matching, distributed algorithm, deterministic complexity, MIS, local model, computational complexity]
Local Conflict Coloring
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Locally finding a solution to symmetry-breaking tasks such as vertex-coloring, edge-coloring, maximal matching, maximal independent set, etc., is a long-standing challenge in distributed network computing. More recently, it has also become a challenge in the framework of centralized local computation. We introduce conflict coloring as a general symmetry-breaking task that includes all the aforementioned tasks as specific instantiations - conflict coloring includes all locally checkable labeling tasks from [Naor &amp; Stockmeyer, STOC 1993]. Conflict coloring is characterized by two parameters l and d, where the former measures the amount of freedom given to the nodes for selecting their colors, and the latter measures the number of constraints which colors of adjacent nodes are subject to. We show that, in the standard LOCAL model for distributed network computing, if l/d &gt; &#x0394;, then conflict coloring can be solved in O&#x0303;(&#x221A;&#x0394;)+log*n rounds in n-node graphs with maximum degree &#x0394;, where O&#x0303; ignores the polylog factors in &#x0394;. The dependency in n is optimal, as a consequence of the &#x03A9;(log*n) lower bound by [Linial, SIAM J. Comp. 1992] for (&#x0394; + 1)-coloring. An important special case of our result is a significant improvement over the best known algorithm for distributed (&#x0394; + 1)-coloring due to [Barenboim, PODC 2015], which required O&#x0303;(&#x0394;3/4) + log*n rounds. Improvements for other variants of coloring, including (&#x0394; + 1)-list-coloring, (2&#x0394;-1)-edge-coloring, coloring with forbidden color distances, etc., also follow from our general result on conflict coloring. Likewise, in the framework of centralized local computation algorithms (LCAs), our general result yields an LCA which requires a smaller number of probes than the previously best known algorithm for vertex-coloring, and works for a wide range of coloring problems.
[Algorithm design and analysis, edge coloring, Symmetry Breaking, vertex-coloring, maximal independent set, set theory, Distributed Network Computing, graph colouring, Image color analysis, adjacent nodes, coloring, Labeling, List-coloring, Distributed algorithms, general symmetry breaking, Computational modeling, Color, distributed network computing, forbidden color distances, maximal matching, centralized local computation, distributed algorithms, Signal processing algorithms, local conflict coloring, Local Computation Algorithm]
A Fast and Simple Unbiased Estimator for Network (Un)reliability
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
The following procedure yields an unbiased estimator for the disconnection probability of an n-vertex graph with minimum cut c if every edge fails independently with probability p: (i) contract every edge independently with probability 1- n-2/c, then (ii) recursively compute the disconnection probability of the resulting tiny graph if each edge fails with probability n2/cp. We give a short, simple, self-contained proof that this estimator can be computed in linear time and has relative variance O(n2). Combining these two facts with a standard sparsification argument yields an O(n3 log n)-time algorithm for estimating the (un)reliability of a network. We also show how the technique can be used to create unbiased samples of disconnected networks.
[Algorithm design and analysis, linear time computation, graph theory, probability, disconnection probability, reliability theory, fast unbiased estimator, network reliability, Monte Carlo methods, Runtime, graph edge, Computer network reliability, standard sparsification argument, n-vertex graph, Approximation algorithms, network unreliability, Reliability, Contracts, computational complexity]
A New Framework for Distributed Submodular Maximization
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. A lot of recent effort has been devoted to developing distributed algorithms for these problems. However, these results suffer from high number of rounds, suboptimal approximation ratios, or both. We develop a framework for bringing existing algorithms in the sequential setting to the distributed setting, achieving near optimal approximation ratios for many settings in only a constant number of MapReduce rounds. Our techniques also give a fast sequential algorithm for non-monotone maximization subject to a matroid constraint.
[Greedy algorithms, nonmonotone maximization, document summarization, approximation algorithms, MapReduce, Distributed submodular maximization, fast sequential algorithm, optimisation, sensor placement, Clustering algorithms, matroid constraint, learning (artificial intelligence), Distributed algorithms, approximation theory, sequential setting, distributed submodular maximization, Partitioning algorithms, machine learning, suboptimal approximation ratio, Standards, Computer science, MapReduce rounds, exemplar clustering, distributed algorithms, Approximation algorithms]
Robust Estimators in High Dimensions without the Computational Intractability
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We study high-dimensional distribution learning in an agnostic setting where an adversary is allowed to arbitrarily corrupt an epsilon fraction of the samples. Such questions have a rich history spanning statistics, machine learning and theoretical computer science. Even in the most basic settings, the only known approaches are either computationally inefficient or lose dimension dependent factors in their error guarantees. This raises the following question: Is high-dimensional agnostic distribution learning even possible, algorithmically? In this work, we obtain the first computationally efficient algorithms for agnostically learning several fundamental classes of high-dimensional distributions: (1) a single Gaussian, (2) a product distribution on the hypercube, (3) mixtures of two product distributions (under a natural balancedness condition), and (4) mixtures of k Gaussians with identical spherical covariances. All our algorithms achieve error that is independent of the dimension, and in many cases depends nearly-linearly on the fraction of adversarially corrupted samples. Moreover, we develop a general recipe for detecting and correcting corruptions in high-dimensions, that may be applicable to many other problems.
[Algorithm design and analysis, statistical learning, estimation theory, product distribution, high-dimensional distribution learning, Programming, Gaussian distribution, machine learning, robust estimators, unsupervised learning, lose dimension dependent factors, Hidden Markov models, epsilon fraction, Approximation algorithms, Hypercubes, density estimation robust algorithm, Robustness, learning (artificial intelligence), computational intractability, high-dimensional agnostic distribution learning, Digital TV]
Agnostic Estimation of Mean and Covariance
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We consider the problem of estimating the mean and covariance of a distribution from i.i.d. samples in the presence of a fraction of malicious noise. This is in contrast to much recent work where the noise itself is assumed to be from a distribution of known type. The agnostic problem includes many interesting special cases, e.g., learning the parameters of a single Gaussian (or finding the best-fit Gaussian) when a fraction of data is adversarially corrupted, agnostically learning mixtures, agnostic ICA, etc. We present polynomial-time algorithms to estimate the mean and covariance with error guarantees in terms of information-theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value Decomposition.
[information-theoretic lower bounds, Estimation, Gaussian distribution, covariance, Complexity theory, Noise measurement, agnostic learning, Mean estimation, Gaussian parameters, PCA, Computer science, covariance estimation, polynomial-time algorithms, covariance analysis, malicious noise, Robustness, Data models, singular value decomposition, Principal component analysis, computational complexity, mean estimation, robust statistics]
Noisy Population Recovery in Polynomial Time
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
In the noisy population recovery problem of Dvir et al. [6], the goal is to learn an unknown distribution f on binary strings of length n from noisy samples. A noisy sample with parameter μ &#x2208; [0,1] is generated by selecting a sample from f, and independently flipping each coordinate of the sample with probability (1-μ)/2. We assume an upper bound k on the size of the support of the distribution, and the goal is to estimate the probability of any string to within some given error &#x03B5;. It is known that the algorithmic complexity and sample complexity of this problem are polynomially related to each other. We describe an algorithm that for each μ &gt; 0, provides the desired estimate of the distribution in time bounded by a polynomial in k, n and 1/&#x03B5; improving upon the previous best result of poly(klog log k, n, 1/&#x03B5;) due to Lovett and Zhang [9]. Our proof combines ideas from [9] with a noise attenuated version of Mo&#x0308;bius inversion. The latter crucially uses the robust local inverse construction of Moitra and Saks [11].
[Algorithm design and analysis, Smoothing methods, probability, noise attenuated Mo&#x0308;bius inversion, sample selection, binary strings, Fourier transform, Complexity theory, Noise measurement, statistical distributions, Statistics, Unsupervised learning, sample complexity, algorithmic complexity, noisy samples, Sociology, noisy population recovery, polynomial time, robust local inverse construction, Reverse Bonami-Beckner inequality, Population recovery, computational complexity]
A New Approach for Testing Properties of Discrete Distributions
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We study problems in distribution property testing: Given sample access to one or more unknown discrete distributions, we want to determine whether they have some global property or are epsilon-far from having the property in L1 distance (equivalently, total variation distance, or "statistical distance").In this work, we give a novel general approach for distribution testing. We describe two techniques: our first technique gives sample-optimal testers, while our second technique gives matching sample lower bounds. As a consequence, we resolve the sample complexity of a wide variety of testing problems. Our upper bounds are obtained via a modular reduction-based approach. Our approach yields optimal testers for numerous problemsby using a standard L2-identity tester as a black-box. Using this recipe, we obtain simple estimators for a wide range of problems, encompassing many problems previously studied in the TCS literature, namely: (1) identity testing to a fixed distribution, (2) closeness testing between two unknown distributions (with equal/unequal sample sizes), (3) independence testing (in any number of dimensions), (4) closeness testing for collections of distributions, and(5) testing histograms. For all of these problems, our testers are sample-optimal, up to constant factors. With the exception of (1), ours are the first sample-optimal testers for the corresponding problems. Moreover, our estimators are significantly simpler to state and analyze compared to previous results. As an important application of our reduction-based technique, we obtain the first adaptive algorithm for testing equivalence betweentwo unknown distributions. The sample complexity of our algorithm depends on the structure of the unknown distributions - as opposed to merely their domain size -and is significantly better compared to the worst-case optimal L1-tester in many natural instances. Moreover, our technique naturally generalizes to other metrics beyond the L1-distance. As an illustration of its flexibility, we use it to obtain the first near-optimal equivalence testerunder the Hellinger distance. Our lower bounds are obtained via a direct information-theoretic approach: Given a candidate hard instance, our proof proceeds by boundingthe mutual information between appropriate random variables. While this is a classical method in information theory, prior to our work, it had not been used in this context. Previous lower bounds relied either on the birthday paradox, oron moment-matching and were thus restricted to symmetric properties. Our lower bound approach does not suffer from any such restrictions and gives tight sample lower bounds for the aforementioned problems.
[Algorithm design and analysis, Measurement, symmetric properties, L2-identity tester, hypothesis testing, Complexity theory, statistical distributions, testing histograms, sample-optimal testers, moment-matching, adaptive algorithm, information theory, statistical testing, direct information-theoretic approach, Testing, total variation distance, independence testing, L1 distance, statistical distance, lower bound approach, discrete distribution property testing, random processes, closeness testing, random variables, modular reduction-based approach, property testing, TCS literature, Standards, worst-case optimal L1-tester, Upper bound, distribution testing, birthday paradox, distribution collections, Information theory, computational complexity]
How to Determine if a Random Graph with a Fixed Degree Sequence Has a Giant Component
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
The traditional Erdos-Renyi model of a random network is of little use in modelling the type of complex networks which modern researchers study. In this graph, every pair of vertices is equally likely to be connected by an edge. However, 21st century networks are of diverse nature and usually exhibit inhomogeneity among their nodes. This motivates the study, for a fixed degree sequence D=(d1, ..., dn), of a uniformly chosen simple graph G(D) on {1, ..., n} where the vertex i has degree di. In this paper, we study the existence of a giant component in G(D). A heuristic argument suggests that a giant component in G(D) will exist provided that the sum of the squares of the degrees is larger than twice the sum of the degrees. In 1995, Molloy and Reed essentially proved this to be the case when the degree sequence D under consideration satisfies certain technical conditions [Random Structures &amp; Algorithms, 6:161-180]. This work has attracted considerable attention, has been extended to degree sequences under weaker conditions and has been applied to random models of a wide range of complex networks such as the World Wide Web or biological systems operating at a sub-molecular level. Nevertheless, the technical conditions on D restrict the applicability of the result to sequences where the vertices of high degree play no important role. This is a major problem since it is observed in many real-world networks, such as scale-free networks, that vertices of high degree (the so-called hubs) are present and play a crucial role. In this paper we characterize when a uniformly random graph with a fixed degree sequence has a giant component. Our main result holds for every degree sequence of length n provided that a minor technical condition is satisfied. The typical structure of G(D) when D does not satisfy this condition is relatively simple and easy to understand. Our result gives a unified criterion that implies all the known results on the existence of a giant component in G(D), including both the generalizations of the Molloy-Reed result and results on more restrictive models. Moreover, it turns out that the heuristic argument used in all the previous works on the topic, does not extend to general degree sequences.
[degree sequences, random graph, Biological system modeling, random processes, network theory (graphs), Nonhomogeneous media, random graphs, complex networks, Next generation networking, heuristic argument, Computer science, Molloy-Reed result, Complex networks, Biological systems, graph vertices, fixed degree sequence, giant component, random models, Web sites]
Convergence of MCMC and Loopy BP in the Tree Uniqueness Region for the Hard-Core Model
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We study the hard-core (gas) model defined on independent sets of an input graph where the independent sets are weighted by a parameter (aka fugacity) &#x03BB; &gt; 0. For constant &#x0394;, previous work of Weitz (2006) established an FPTAS for the partition function for graphs of maximum degree &#x0394; when &#x03BB; &lt;; &#x03BB;<sub>c</sub>(&#x0394;). Sly (2010) showed that there is no FPRAS, unless NP=RP, when &#x03BB; &gt; &#x03BB;<sub>c</sub>(&#x0394;). The threshold &#x03BB;<sub>c</sub>(&#x0394;) is the critical point for the statistical physics phase transition for uniqueness/non-uniqueness on the infinite &#x0394;-regular tree. The running time of Weitz's algorithm is exponential in log &#x0394;. Here we present an FPRAS for the partition function whose running time is O* (n2). We analyze the simple single-site Markov chain known as the Glauber dynamics for sampling from the associated Gibbs distribution. We prove there exists a constant &#x0394;<sub>0</sub> such that for all graphs with maximum degree &#x0394; &gt; &#x0394;<sub>0</sub> and girth &gt; 7 (i.e., no cycles of length &#x2264; 6), the mixing time of the Glauber dynamics is O(nlog n) when &#x03BB; &lt;; &#x03BB;<sub>c</sub>(&#x0394;). Our work complements that of Weitz which applies for small constant &#x0394; whereas our work applies for all &#x0394; at least a sufficiently large constant &#x0394;<sub>0</sub> (this includes &#x0394; depending on n = IVI). Our proof utilizes loopy BP (belief propagation) which is a widely-used algorithm for inference in graphical models. A novel aspect of our work is using the principal eigenvector for the BP operator to design a distance function which contracts in expectation for pairs of states that behave like the BP fixed point. We also prove that the Glauber dynamics behaves locally like loopy BP. As a byproduct we obtain that the Glauber dynamics, after a short burn-in period, converges close to the BP fixed point, and this implies that the fixed point of loopy BP is a close approximation to the Gibbs distribution. Using these connections we establish that loopy BP quickly converges to the Gibbs distribution when the girth &#x2265; 6 and &#x03BB; &lt;; &#x03BB;<sub>c</sub>(&#x0394;).
[Algorithm design and analysis, hard core model, Gibbs Tree Uniqueness, belief propagation, Heuristic algorithms, graph theory, convergence, MCMC, associated Gibbs distribution, tree uniqueness region, Electronic mail, simple single-site Markov chain, Glauber dynamics, statistical physics phase transition, Gibbs distribution, partition function, rapid mixing, Loopy BP, distance function, loopy BP, Partitioning algorithms, independent sets, Couplings, Hard-Core model, Belief Propagation, graphical models, Markov processes, Approximation algorithms, Weitz algorithm, input graph, FPTAS]
Simulated Quantum Annealing Can Be Exponentially Faster Than Classical Simulated Annealing
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Can quantum computers solve optimization problems much more quickly than classical computers? One major piece of evidence for this proposition has been the fact that Quantum Annealing (QA) finds the minimum of some cost functions exponentially more quickly than classical Simulated Annealing (SA). One such cost function is the simple &#x201C;Hamming weight with a spike&#x201D; function in which the input is an n-bit string and the objective function is simply the Hamming weight, plus a tall thin barrier centered around Hamming weight n/4. While the global minimum of this cost function can be found by inspection, it is also a plausible toy model of the sort of local minima that arise in realworld optimization problems. It was shown by Farhi, Goldstone and Gutmann [1] that for this example SA takes exponential time and QA takes polynomial time, and the same result was generalized by Reichardt [2] to include barriers with width n&#x03B6; and height n&#x03B1; for &#x03B6; + &#x03B1; &#x2264; 1/2. This advantage could be explained in terms of quantummechanical &#x201C;tunneling.&#x201D; Our work considers a classical algorithm known as Simulated Quantum Annealing (SQA) which relates certain quantum systems to classical Markov chains. By proving that these chains mix rapidly, we show that SQA runs in polynomial time on the Hamming weight with spike problem in much of the parameter regime where QA achieves an exponential advantage over SA. While our analysis only covers this toy model, it can be seen as evidence against the prospect of exponential quantum speedup using tunneling. Our technical contributions include extending the canonical path method for analyzing Markov chains to cover the case when not all vertices can be connected by low-congestion paths. We also develop methods for taking advantage of warm starts and for relating the quantum state in QA to the probability distribution in SQA. These techniques may be of use in future studies of SQA or of rapidly mixing Markov chains in general.
[classical Markov chains, Markov chains, SA, QA, Quantum computing, Simulated annealing, cost functions, cost function, probability distribution, Cost function, polynomial time, simulated quantum annealing, objective function, SQA, simulated annealing, polynomials, quantum computers, Hamming weight, classical computers, Quantum mechanics, quantum computing, Markov processes, realworld optimization problems, optimization problems, classical simulated annealing]
The Number of Solutions for Random Regular NAE-SAT
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Recent work has made substantial progress in understanding the transitions of random constraint satisfaction problems (CSPs). In particular, for several of these models, the exact satisfiability threshold has been rigorously determined, confirming predictions from the statistical physics literature. Here we revisit one of these models, random regular NAE-SAT: knowing the satisfiability threshold, it is natural to study, in the satisfiable regime, the number of solutions in a typical instance. We prove here that these solutions have a well-defined free energy (limiting exponential growth rate), with explicit value matching the one-step replica symmetry breaking prediction. The proof develops new techniques for analyzing a certain "survey propagation model" associated to this problem. We believe that these methods may be applicable in a wide class of related problems.
[random regular NAE-SAT, random processes, Probability, Predictive models, computability, replica symmetry, satisfiability threshold, Electronic mail, random constraint satisfaction, Sun, Physics, Computer science, Analytical models, one-step replica symmetry breaking prediction, constraint satisfaction problems, statistical physics, free energy, survey propagation model, random constraint satisfaction problem, one-step replica symmetry breaking, condensation transition, explicit value matching]
Accelerated Newton Iteration for Roots of Black Box Polynomials
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We study the problem of computing the largest root of a real rooted polynomial p(x) to within error 'z' given only black box access to it, i.e., for any x, the algorithm can query an oracle for the value of p(x), but the algorithm is not allowed access to the coefficients of p(x). A folklore result for this problem is that the largest root of a polynomial can be computed in O(n log (1/z)) polynomial queries using the Newton iteration. We give a simple algorithm that queries the oracle at only O(log n log(1/z)) points, where n is the degree of the polynomial. Our algorithm is based on a novel approach for accelerating the Newton method by using higher derivatives.
[Symmetric matrices, black box access, Computational modeling, polynomials, black box polynomial roots, accelerated Newton iteration, Complexity theory, Upper bound, polynomial roots, O(n log (1/z)) polynomial queries, polynomial degree, Newton's method, Eigenvalues and eigenfunctions, real rooted polynomial, Acceleration, Newton method, computational complexity]
Fourier-Sparse Interpolation without a Frequency Gap
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We consider the problem of estimating a Fourier-sparse signal from noisy samples, where the sampling is done over some interval [0, T] and the frequencies can be "off-grid". Previous methods for this problem required the gap between frequencies to be above 1/T, the threshold required to robustly identify individual frequencies. We show the frequency gap is not necessary to estimate the signal as a whole: for arbitrary k-Fourier-sparse signals under l2 bounded noise, we show how to estimate the signal with a constant factor growth of the noise and sample complexity polynomial in k and logarithmic in the bandwidth and signal-to-noise ratio. As a special case, we get an algorithm to interpolate degree d polynomials from noisy measurements, using O(d) samples and increasing the noise by a constant factor in l2.
[sample complexity polynomial, Fourier transforms, super-resolution, Fourier transform, Frequency estimation, Complexity theory, l<sub>2</sub> bounded noise, noisy samples, noise complexity polynomial, signal sampling, polynomial interpolation, Robustness, Fourier-sparse interpolation, arbitrary k-Fourier-sparse signals, constant factor growth, O(d) samples, sparse recovery, Fourier-sparse signal, Noise measurement, Computer science, Interpolation, noisy measurements, interpolation, signal-to-noise ratio, frequency gap, compressive sensing, computational complexity]
Robust Fourier and Polynomial Curve Fitting
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We consider the robust curve fitting problem, for both algebraic and Fourier (trigonometric) polynomials, in the presence of outliers. In particular, we study the model of Arora and Khot (STOC 2002), who were motivated by applications in computer vision. In their model, the input data consists of ordered pairs (x<sub>i</sub>, y<sub>i</sub>) &#x03B5; [-1, 1] &#x00D7; [-1, 1], i = 1, 2,..., N, and there is an unknown degree-d polynomial p such that for all but &#x03C1; fraction of the i, we have |p(x<sub>i</sub>) - y<sub>i</sub>|&#x2264; &#x03B4;. Unlike Arora-Khot, we also study the trigonometric setting, where the input is from T &#x00D7; [-1, 1], where T is the unit circle. In both scenarios, the i corresponding to errors are chosen randomly, and for such i the errors in the yi can be arbitrary. The goal is to output a degree-d polynomial q such that ||p - q||<sub>&#x221E;</sub> is small (for example, O(&#x03B4;)). Arora and Khot could achieve a polynomial-time algorithm only for &#x03C1; = 0. Daltrophe et al. observed that a simple median-based algorithm can correct errors if the desired accuracy &#x03B4; is large enough. (Larger &#x03B4; makes the output guarantee easier to achieve, which seems to typically outweigh the weaker input promise.) We dramatically expand the range of parameters for which recovery of q is possible in polynomial time. Specifically, we show that there are polynomial-time algorithms in both settings that recover q up to l&#x221E; error O(&#x03B4;.99) provided 1) &#x03C1; &#x2264;/c1log d and &#x03B4; &#x2265; 1/(log d)c, or 2) &#x03C1; &#x2264; c1/log log d/log2 d and &#x03B4; &#x2265; 1/dc. Here c is any constant and c1 is a small enough constant depending on c. The number of points that suffices is N = O&#x0303;(d) in the trigonometric setting for random x<sub>i</sub> or arbitrary x<sub>i</sub> that are roughly equally spaced, or in the algebraic setting when the x<sub>i</sub> are chosen according to the Chebyshev distribution, and N = O&#x0303;(d2) in the algebraic setting with random (or roughly equally spaced) x<sub>i</sub>.
[Computer vision, Computational modeling, algebraic polynomials, Error-correction, Electronic mail, statistical distributions, Computer science, Reed-Solomon codes, l<sub>&#x221E;</sub> error O(&#x03B4;.99), Chebyshev distribution, Chebyshev approximation, Fourier polynomials, trigonometric polynomials, Robustness, Data models, curve fitting, robust Fourier curve fitting, polynomial-time algorithm, polynomial curve fitting, computational complexity, polynomial regression]
NP-Hardness of Reed-Solomon Decoding and the Prouhet-Tarry-Escott Problem
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Establishing the complexity of Bounded Distance Decoding for Reed-Solomon codes is a fundamental open problem in coding theory, explicitly asked by Guruswami and Vardy (IEEE Trans. Inf. Theory, 2005). The problem is motivated by the large current gap between the regime when it is NP-hard, and the regime when it is efficiently solvable (i.e., the Johnson radius). We show the first NP-hardness results for asymptotically smaller decoding radii than the maximum likelihood decoding radius of Guruswami and Vardy. Specifically, for Reed-Solomon codes of length N and dimension K = O(N), we show that it is NP-hard to decode more than N-K-O/log N log log N) errors. Moreover, we show that the problem is NP-hard under quasipolynomial-time reductions for an error amount &gt; N-K-c log N (with c &gt; 0 an absolute constant). An alternative natural reformulation of the Bounded Distance Decoding problem for Reed-Solomon codes is as a Polynomial Reconstruction problem. In this view, our results show that it is NP-hard to decide whether there exists a degree K polynomial passing through K + O(log N / log log N) points from a given set of points (a1, b1), (a2, b2) ..., (aN, bN). Furthermore, it is NP-hard under quasipolynomial-time reductions to decide whether there is a degree K polynomial passing through K + c log N many points (with c &gt; 0 an absolute constant). These results follow from the NP-hardness of a generalization of the classical Subset Sum problem to higher moments, called Moments Subset Sum, which has been a known open problem, and which may be of independent interest. We further reveal a strong connection with the well-studied Prouhet-Tarry-Escott problem in Number Theory, which turns out to capture a main barrier in extending our techniques. We believe the Prouhet-Tarry-Escott problem deserves further study in the theoretical computer science community.
[subset sum problem, Nonhomogeneous media, Electronic mail, Complexity theory, set theory, Reed-Solomon codes, coding theory, bounded distance decoding, N-K-O/log N log log N) errors, asymptotically smaller decoding radii, Reed-Solomon decoding, Maximum likelihood decoding, Bounded Distance Decoding, Computer science, NP-hardness, Interpolation, K + O(log N / log log N), quasipolynomial-time reductions, Prouhet-Tarry-Escott problem, Moments Subset Sum, Reed-Solomon Codes, N-K-clog N, computational complexity, number theory, moment subset sum]
Amplification and Derandomization without Slowdown
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We present techniques for decreasing the error probability of randomized algorithms and for converting randomized algorithms to deterministic (nonuniform) algorithms. Unlike most existing techniques that involve repetition of the randomized algorithm and hence a slowdown, our techniques produce algorithms with a similar run-time to the original randomized algorithms. The amplification technique is related to a certain stochastic multi-armed bandit problem. The derandomization technique - which is the main contribution of this work - points to an intriguing connection between derandomization and sketching/sparsification. We demonstrate the techniques by showing algorithms for approximating free games (constraint satisfaction problems on dense bipartite graphs).
[Algorithm design and analysis, Error probability, graph theory, slowdown, stochastic multiarmed bandit problem, stochastic processes, error statistics, free game approximation, Amplification; derandomization; Free game;, game theory, error probability, Generators, derandomization, Partitioning algorithms, dense bipartite graphs, deterministic algorithms, randomised algorithms, Computer science, amplification, sketching, constraint satisfaction problems, nonuniform algorithms, randomized algorithms, Games, Approximation algorithms]
Commutativity in the Algorithmic Lov&#xe1;sz Local Lemma
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We consider the recent formulation of the Algorithmic Lova&#x0301;sz Local Lemma [1], [2] for finding objects that avoid "bad features\
[Algorithm design and analysis, parallel algorithms, algorithmic Lov&#x03B1;sz local Lemma, Probabilistic logic, Data structures, Probability distribution, resampling oracle, set theory, Indexes, Computer science, Moser-Tardos resampling algorithm, specific flaw selection rule, styling, arbitrary rule, commutativity condition, Data models, style, formatting]
An Algorithm for Koml&#xf3;s Conjecture Matching Banaszczyk's Bound
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We consider the problem of finding a low discrepancy coloring for sparse set systems where each element lies in at most t sets. We give an efficient algorithm that finds a coloring with discrepancy O((t log n)1/2), matching the best known non-constructive bound for the problem due to Banaszczyk. The previous algorithms only achieved an O(t1/2 log n) bound. Our result also extends to the more general Komlo&#x0301;s setting and gives an algorithmic O(log1/2 n) bound.
[Computers, Linear systems, Banaszczyk's bound, discrepancy coloring, Komlo&#x0301;s conjecture, sparse set systems, Optimization, graph colouring, Computer science, Integral equations, Approximation algorithms, algorithmic O(log1/2 n) bound, O((t log n)1/2) discrepancy, nonconstructive bound, computational complexity]
Universal Simulation of Directed Systems in the Abstract Tile Assembly Model Requires Undirectedness
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
As a mathematical model of tile-based self-assembling systems, Winfree's abstract Tile Assembly Model (aTAM) has proven to be a remarkable platform for studying and understanding the behaviors and powers of self-assembling systems. Furthermore, as it is capable of Turing universal computation, the aTAM allows algorithmic self-assembly, in which the components can be designed so that the rules governing their behaviors force them to inherently execute prescribed algorithms as they combine. This power has yielded a wide variety of theoretical results in the aTAM utilizing algorithmic self-assembly to design systems capable of performing complex computations and forming extremely intricate structures. Adding to the completeness of the model, in FOCS 2012 the aTAM was shown to also be intrinsically universal, which means that there exists one single tile set such that for any arbitrary input aTAM system, that tile set can be configured into a "seed" structure which will then cause self-assembly using that tile set to simulate the input system, capturing its full dynamics modulo only a scale factor. However, the "universal simulator" of that result makes use of nondeterminism in terms of the tiles placed in several key locations when different assembly sequences are followed. This nondeterminism remains even when the simulator is simulating a system which is directed, meaning that it has exactly one unique terminal assembly and for any given location, no matter which assembly sequence is followed, the same tile type is always placed there. The question which then arose was whether or not that nondeterminism is fundamentally required, and if any universal simulator must in fact utilize more nondeterminism than directed systems when simulating them. In this paper, we answer that question in the affirmative: the class of directed systems in the aTAM is not intrinsically universal, meaning there is no universal simulator for directed systems which itself is always directed. This result provides a powerful insight into the role of nondeterminism in self-assembly, which is itself a fundamentally nondeterministic process occurring via unguided local interactions. Furthermore, to achieve this result we leverage powerful results of computational complexity hierarchies, including tight bounds on both best and worst-case complexities of decidable languages, to tailor design systems with precisely controllable space resources available to computations embedded within them. We also develop novel techniques for designing systems containing subsystems with disjoint, mutually exclusive computational powers. The main result will be important in the development of future simulation systems, and the supporting design techniques and lemmas will provide powerful tools for the development of future aTAM systems as well as proofs of their computational abilities.
[Algorithm design and analysis, directed systems, simulation, Winfree abstract tile assembly model, Complexity theory, set theory, best-case complexities, nondeterministic process, Jacobian matrices, decidable languages, unguided local interactions, aTAM, decidability, computational abilities, Mathematical model, universal simulation, terminal assembly, Computational modeling, universal simulator, Tile Assembly Model, Computer science, computational complexity hierarchies, self-assembly, Self-assembly, tile-based self-assembling system, intrinsic universality, Turing universal computation, algorithmic self-assembly, computational complexity, worst-case complexities]
A PTAS for the Steiner Forest Problem in Doubling Metrics
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We achieve a (randomized) polynomial-time approximation scheme (PTAS) for the Steiner Forest Problem in doubling metrics. Before our work, a PTAS is given only for the Euclidean plane in [FOCS 2008: Borradaile, Klein and Mathieu]. Our PTAS also shares similarities with the dynamic programming for sparse instances used in [STOC 2012: Bartal, Gottlieb and Krauthgamer] and [SODA 2016: Chan and Jiang]. However, extending previous approaches requires overcoming several non-trivial hurdles, and we make the following technical contributions. (1) We prove a technical lemma showing that Steiner points have to be "near" the terminals in an optimal Steiner tree. This enables us to define a heuristic to estimate the local behavior of the optimal solution, even though the Steiner points are unknown in advance. This lemma also generalizes previous results in the Euclidean plane, and may be of independent interest for related problems involving Steiner points. (2) We develop a novel algorithmic technique known as "adaptive cells" to overcome the difficulty of keeping track of multiple components in a solution. Our idea is based on but significantly different from the previously proposed "uniform cells" in the FOCS 2008 paper, whose techniques cannot be readily applied to doubling metrics.
[Steiner trees, approximation theory, trees (mathematics), sparse instances, dynamic programming, Euclidean plane, Extraterrestrial measurements, doubling metrics, doubling dimension, Electronic mail, optimal Steiner tree, randomised algorithms, Computer science, PTAS, Steiner forest problem, polynomial time approximation scheme, approximation algorithm, Approximation algorithms, Dynamic programming, Steiner points, computational complexity, randomized polynomial-time approximation]
On Approximating Maximum Independent Set of Rectangles
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
We study the Maximum Independent Set of Rectangles (MISR) problem: given a set of n axis-parallel rectangles, find a largest-cardinality subset of the rectangles, such that no two of them overlap. MISR is a basic geometric optimization problem with many applications, that has been studied extensively. Until recently, the best approximation algorithm for it achieved an O(log log n)-approximation factor. In a recent breakthrough, Adamaszek and Wiese provided a quasi-polynomial time approximation scheme: a (1-&#x03B5;)-approximation algorithm with running time nO(poly(log n)/&#x03B5;). Despite this result, obtaining a PTAS or even a polynomial-time constant-factor approximation remains a challenging open problem. In this paper we make progress towards this goal by providing an algorithm for MISR that achieves a (1 - &#x03B5;)-approximation in time nO(poly(log logn/&#x03B5;)). We introduce several new technical ideas, that we hope will lead to further progress on this and related problems.
[Shape, Heuristic algorithms, quasipolynomial time approximation, (1-&#x03B5;)-approximation algorithm, set theory, nO(poly(log n)/&#x03B5;) running time, approximation algorithms, Rectangle independent set, optimisation, polynomial approximation, approximation algorithm, Dynamic programming, geometric optimization, largest-cardinality subset, MISR, axis-parallel rectangles, Partitioning algorithms, O(log log n)-approximation factor, Vegetation, maximum independent set of rectangles, Approximation algorithms, Fats, running PTAS, polynomial-time constant-factor approximation, computational complexity]
[Publishers' information]
2016 IEEE 57th Annual Symposium on Foundations of Computer Science
None
2016
Provides a listing of current committee members and society officers.
[]
Preface
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee and Sponsors
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Provides a listing of current committee members and society officers.
[]
Program Committee
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Provides a listing of current committee members and society officers.
[]
A Nearly Optimal Lower Bound on the Approximate Degree of AC^0
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
The approximate degree of a Boolean function f : {-1, 1}n &#x2192; {-1, 1} is the least degree of a real polynomial that approximates f pointwise to error at most 1/3. We introduce a generic method for increasing the approximate degree of a given function, while preserving its computability by constant-depth circuits. Specifically, we show how to transform any Boolean function f with approximate degree d into a function F on O(n &#x00B7; polylog(n)) variables with approximate degree at least D = &#x03A9;(n1/3 &#x00B7; d2/3). In particular, if d = n1-&#x03A9;(1), then D is polynomially larger than d. Moreover, if f is computed by a constant-depth polynomial-size Boolean circuit, then so is F. By recursively applying our transformation, for any constant &#x03B4; &gt; 0 we exhibit an AC0 function of approximate degree &#x03A9;(n1-&#x03B4;). This improves over the best previous lower bound of &#x03A9;(n2/3) due to Aaronson and Shi (J. ACM 2004), and nearly matches the trivial upper bound of n that holds for any function. Our lower bounds also apply to (quasipolynomial-size) DNFs of polylogarithmic width. We describe several applications of these results. We give: &#x00B7; For any constant &#x03B4; &gt; 0, an &#x03A9;(n1-&#x03B4;) lower bound on the quantum communication complexity of a function in AC0. &#x00B7; A Boolean function f with approximate degree at least C(f)2-o(1), where C(f) is the certificate complexity of f. This separation is optimal up to the o(1) term in the exponent. &#x00B7; Improved secret sharing schemes with reconstruction procedures in AC0.
[O(n &#x00B7; polylog(n)) variables, approximation theory, Frequency modulation, nearly optimal lower bound, polynomials, quantum communication complexity, constant-depth polynomial-size Boolean circuit, computability, Boolean function, Complexity theory, communication complexity, quasipolynomial-size DNF, Computer science, approximate degree, Boolean functions, Upper bound, polynomial approximation, trivial upper bound, improved secret sharing schemes, certificate complexity, Cryptography, secret sharing, computational complexity, reconstruction procedures]
On the Quantitative Hardness of CVP
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
For odd integers p &#x2265; 1 (and p = &#x221E;), we show that the Closest Vector Problem in the &#x2113;<sub>p</sub> norm (CVP<sub>p</sub>) over rank n lattices cannot be solved in 2(1-&#x03B5;)n time for any constant &#x03B5; &gt; 0 unless the Strong Exponential Time Hypothesis (SETH) fails. We then extend this result to &#x201C;almost all&#x201D; values of p &#x2265; 1, not including the even integers. This comes tantalizingly close to settling the quantitative time complexity of the important special case of CVP<sub>2</sub> (i.e., CVP in the Euclidean norm), for which a 2n+o(n)-time algorithm is known. In particular, our result applies for any p = p(n) &#x2260; 2 that approaches 2 as n &#x2192; &#x221E;. We also show a similar SETH-hardness result for SVP<sub>&#x221E;</sub>; hardness of approximating CVP<sub>p</sub> to within some constant factor under the so-called Gap-ETH assumption; and other hardness results for CVP<sub>p</sub> and CVPP<sub>p</sub> for any 1 &#x2264; p &lt;; &#x221E; under different assumptions.
[SETH, approximation theory, Heuristic algorithms, similar SETH-hardness result, Lattices, Linear programming, Closest Vector Problem, Complexity theory, Electronic mail, Euclidean norm, hardness, vectors, closest vector problem, odd integers, Fine-grained complexity, CVP quantitative hardness, strong exponential time hypothesis, Approximation algorithms, Cryptography, quantitative time complexity, CVP, computational complexity]
Distributed PCP Theorems for Hardness of Approximation in P
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We present a new distributed model of probabilistically checkable proofs (PCP). A satisfying assignment x &#x2208; {0, 1}n to a CNF formula &#x03C6; is shared between two parties, where Alice knows x<sub>1</sub>, ... , x<sub>n/2</sub>, Bob knows x<sub>n/2+1</sub>, . . . , xn, and both parties know &#x03C6;. The goal is to have Alice and Bob jointly write a PCP that x satisfies &#x03C6;, while exchanging little or no information. Unfortunately, this model as-is does not allow for nontrivial query complexity. Instead, we focus on a non-deterministic variant, where the players are helped by Merlin, a third party who knows all of x. Using our framework, we obtain, for the first time, PCP-like reductions from the Strong Exponential Time Hypothesis (SETH) to approximation problems in P. In particular, under SETH we show that there are no trulysubquadratic approximation algorithms for Maximum Inner Product over {0, 1}-vectors, LCS Closest Pair over permutations, Approximate Partial Match, Approximate Regular Expression Matching, and Diameter in Product Metric. All our inapproximability factors are nearly-tight. In particular, for the first three problems we obtain nearly-polynomial factors of 2(log n)1-o(1); only (1+o(1))-factor lower bounds (under SETH) were known before. As an additional feature of our reduction, we obtain new SETH lower bounds for the exact &#x201C;monochromatic&#x201D; Closest Pair problem in the Euclidean, Manhattan, and Hamming metrics.
[Algorithm design and analysis, fine-grained complexity, Protocols, CNF formula &#x03C6;, similarity search, distributed model, Maximum Inner Product, Product Metric, Search problems, inapproximability, Complexity theory, inapproximability factors, LCS Closest Pair, polynomial approximation, approximate partial matching, distributed PCP theorems, strong exponential-time hypothesis, probabilistically checkable proofs, nontrivial query complexity, probability, Approximate Regular Expression Matching, Probabilistic logic, nearly-polynomial factors, Strong Exponential Time Hypothesis, Computer science, SETH lower bounds, closest pair, longest common subsequence, exact monochromatic Closest Pair problem, trulysubquadratic approximation algorithms, Approximation algorithms, PCP-like reductions, computational complexity]
Short Presburger Arithmetic Is Hard
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study the computational complexity of short sentences in Presburger arithmetic (SHORT-PA). Here by &#x201C;short&#x201D; we mean sentences with a bounded number of variables, quantifiers, inequalities and Boolean operations; the input consists only of the integer coefficients involved in the linear inequalities. We prove that satisfiability of SHORT-PA sentences with m+2 alternating quantifiers is &#x03A3;<sub>m</sub>P-complete or &#x03A0;<sub>m</sub>P-complete, when the first quantifier is &#x2203; or &#x2200;, respectively. Counting versions and restricted systems are also analyzed.
[Linear systems, Boolean operations, integer coefficients, Linear programming, m+2 alternating quantifiers, Electronic mail, completeness, quantifier, Computational complexity, Presburger arithmetic, linear inequalities, short Presburger arithmetic, short sentences, Boolean functions, SHORT-PA sentences, IP networks, computational complexity]
On the Local Structure of Stable Clustering Instances
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study the classic k-median and k-means clustering objectives in the beyond-worst-case scenario. We consider three well-studied notions of structured data that aim at characterizing real-world inputs: Distribution Stability (introduced by Awasthi, Blum, and Sheffet, FOCS 2010); Spectral Separability (introduced by Kumar and Kannan, FOCS 2010); Perturbation Resilience (introduced by Bilu and Linial, ICS 2010). We prove structural results showing that inputs satisfying at least one of the conditions are inherently local. Namely, for any such input, any local optimum is close both in term of structure and in term of objective value to the global optima. As a corollary we obtain that the widely-used Local Search algorithm has strong performance guarantees for both the tasks of recovering the underlying optimal clustering and obtaining a clustering of small cost. This is a significant step toward understanding the success of local search heuristics in clustering applications.
[Algorithm design and analysis, local structure, beyond-worst-case scenario, Local Search algorithm, Distribution Stability, local search heuristics, Extraterrestrial measurements, Stability analysis, clustering applications, Resilience, optimisation, Spectral Separability, k-means clustering, structured data, pattern clustering, Clustering algorithms, classic k-median clustering, Approximation algorithms, Perturbation Resilience, search problems, optimal clustering]
Better Guarantees for k-Means and Euclidean k-Median by Primal-Dual Algorithms
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Clustering is a classic topic in optimization with k-means being one of the most fundamental such problems. In the absence of any restrictions on the input, the best known algorithm for k-means with a provable guarantee is a simple local search heuristic yielding an approximation guarantee of 9 + &#x03B5;, a ratio that is known to be tight with respect to such methods. We overcome this barrier by presenting a new primal-dual approach that allows us to (1) exploit the geometric structure of k-means and (2) to satisfy the hard constraint that at most k clusters are selected without deteriorating the approximation guarantee. Our main result is a 6.357-approximation algorithm with respect to the standard LP relaxation. Our techniques are quite general and we also show improved guarantees for the general version of k-means where the underlying metric is not required to be Euclidean and for k-median in Euclidean metrics.
[Algorithm design and analysis, Measurement, 6.357-approximation algorithm, primal-dual algorithms, Electronic mail, primal-dual approach, Clustering algorithms, k-means, k-means algorithm, primal-dual, search problems, approximation theory, local search heuristic, Euclidean k-median algorithm, Standards, Computer science, approximation guarantee, pattern clustering, Euclidean metrics, geometric structure, standard LP relaxation, k-median, Approximation algorithms, clustering, computational complexity]
Statistical Query Lower Bounds for Robust Estimation of High-Dimensional Gaussians and Gaussian Mixtures
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We describe a general technique that yields the first Statistical Query lower bounds for a range of fundamental high-dimensional learning problems involving Gaussian distributions. Our main results are for the problems of (1) learning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of a single unknown Gaussian distribution. For each of these problems, we show a super-polynomial gap between the (information-theoretic) sample complexity and the computational complexity of any Statistical Query algorithm for the problem. Statistical Query (SQ) algorithms are a class of algorithms that are only allowed to query expectations of functions of the distribution rather than directly access samples. This class of algorithms is quite broad: a wide range of known algorithmic techniques in machine learning are known to be implementable using SQs. Moreover, for the unsupervised learning problems studied in this paper, all known algorithms with non-trivial performance guarantees are SQ or are easily implementable using SQs. Our SQ lower bound for Problem (1) is qualitatively matched by known learning algorithms for GMMs. At a conceptual level, this result implies that - as far as SQ algorithms are concerned - the computational complexity of learning GMMs is inherently exponential in the dimension of the latent space - even though there is no such information-theoretic barrier. Our lower bound for Problem (2) implies that the accuracy of the robust learning algorithm in [29] is essentially best possible among all polynomial-time SQ algorithms. On the positive side, we also give a new (SQ) learning algorithm for Problem (2) achieving the information-theoretically optimal accuracy, up to a constant factor, whose running time essentially matches our lower bound. Our algorithm relies on a filtering technique generalizing [29] that removes outliers based on higher-order tensors. Our SQ lower bounds are attained via a unified moment-matching technique that is useful in other contexts and may be of broader interest. Our technique yields nearly-tight lower bounds for a number of related unsupervised estimation problems. Specifically, for the problems of (3) robust covariance estimation in spectral norm, and (4) robust sparse mean estimation, we establish a quadratic statistical- computational tradeoff for SQ algorithms, matching known upper bounds. Finally, our technique can be used to obtain tight sample complexity lower bounds for high-dimensional testing problems. Specifically, for the classical problem of robustly testing an unknown mean (known covariance) Gaussian, our technique implies an information-theoretic sample lower bound that scales linearly in the dimension. Our sample lower bound matches the sample complexity of the corresponding robust learning problem and separates the sample complexity of robust testing from standard (non-robust) testing. This separation is surprising because such a gap does not exist for the corresponding learning problem.problem.
[Machine learning algorithms, polynomial-time SQ algorithms, estimation theory, information-theoretic barrier, Gaussian distribution, robust sparse mean estimation, single unknown Gaussian distribution, statistical queries, GMMs, Statistical Query algorithm, query processing, robust learning algorithm, Robustness, unsupervised learning problems, learning (artificial intelligence), Testing, approximation theory, covariance matrices, Gaussian distributions, statistical learning, Estimation, high-dimensional Gaussians, Statistical query lower bounds, machine learning, Computational complexity, robust algorithm, unsupervised learning, high-dimensional learning problems, robust estimation, Gaussian processes, Gaussian mixture models, mixture models, robust covariance estimation, computational complexity]
On Small-Depth Frege Proofs for Tseitin for Grids
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We prove a lower bound on the size of a small depth Frege refutation of the Tseitin contradiction on the grid. We conclude that polynomial size such refutations must use formulas of almost logarithmic depth.
[small-depth Frege proofs, Input variables, polynomials, proof complexity, Switches, switching lemma, Tseitin formulas, Tseitin contradiction, Cognition, Graph theory, grids, Switching circuits, polynomial size, Computer science, depth Frege refutation, logarithmic depth, refutations, Decision trees, Frege proofs, computational complexity]
Random &#x398;(log n)-CNFs Are Hard for Cutting Planes
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
The random k-SAT model is the most important and well-studied distribution over k-SAT instances. It is closely connected to statistical physics and is a benchmark for satisfiability algorithms. We show that when k = &#x0398;(log n), any Cutting Planes refutation for random k-SAT requires exponential size in the interesting regime where the number of clauses guarantees that the formula is unsatisfiable with high probability.
[satisfiability algorithms, random k-SAT model, probability, exponential size, computability, Complexity theory, random k-SAT, Physics, Proof Complexity, Computer science, Interpolation, Boolean functions, statistical physics, Semantics, Games, Random &#x0398;(log n)-CNF, Cutting Planes refutation, computational complexity, Cutting Planes]
Random Formulas, Monotone Circuits, and Interpolation
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We prove new lower bounds on the sizes of proofs in the Cutting Plane proof system, using a concept that we call unsatisfiability certificate. This approach is, essentially, equivalent to the well-known feasible interpolation method, but is applicable to CNF formulas that do not seem suitable for interpolation. Specifically, we prove exponential lower bounds for random k-CNFs, where k is the logarithm of the number of variables, and for the Weak Bit Pigeon Hole Principle. Furthermore, we prove a monotone variant of a hypothesis of Feige [12]. We give a superpolynomial lower bound on monotone real circuits that approximately decide the satisfiability of k-CNFs, where k = &#x03C9;(1). For k &#x2248; log n, the lower bound is exponential.
[Cutting Plane proof system, Computational modeling, computability, CNF formulas, Weak Bit Pigeon Hole Principle, Complexity theory, monotone variant, Interpolation, exponential lower bounds, Boolean functions, interpolation, Semantics, random k-CNFs, Games, superpolynomial lower, interpolation method, theorem proving, unsatisfiability certificate, random formulas, monotone real circuits, monotone circuits, computational complexity, Cutting Planes]
Query-to-Communication Lifting for BPP
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
For any n-bit boolean function f, we show that the randomized communication complexity of the composed function f o gn, where g is an index gadget, is characterized by the randomized decision tree complexity of f. In particular, this means that many query complexity separations involving randomized models (e.g., classical vs. quantum) automatically imply analogous separations in communication complexity.
[Protocols, BPP, query complexity separations, query, Complexity theory, Electronic mail, randomized communication complexity, randomized models, Indexes, communication complexity, Query-to-communication lifting, composed function, Computer science, randomized decision tree complexity, Boolean functions, index gadget, lifting, decision trees, n-bit boolean function, Decision trees, communication]
A Rounds vs. Communication Tradeoff for Multi-Party Set Disjointness
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In the set disjointess problem, we have k players, each with a private input Xi &#x2286; [n], and the goal is for the players to determine whether or not their sets have a global intersection. The players communicate over a shared blackboard, and we charge them for each bit that they write on the board. We study the trade-off between the number of interaction rounds we allow the players, and the total number of bits they must send to solve set disjointness. We show that if R rounds of interaction are allowed, the communication cost is &#x03A9;&#x0303;(nk1/R/R4), which is nearly tight. We also leverage our proof to show that wellfare maximization with unit demand bidders cannot be solved efficiently in a small number of rounds: here, we have k players bidding on n items, and the goal is to find a matching between items and player that bid on them which approximately maximizes the total number of items assigned. It was previously shown by Alon et. al. that &#x03A9;(log log k) rounds of interaction are required to find an assignment which achieves a constant approximation to the maximum-wellfare assignment, even if each player is allowed to write n&#x03F5;(R) bits on the board in each round, where &#x03F5;(R) = exp(-R). We improve this lower bound to &#x03A9;(log k/log log k), which is known to be tight up to a log log k factor.
[approximation theory, Protocols, round complxity, Estimation, set disjointness, communication cost, Complexity theory, Electronic mail, set theory, maximum-wellfare assignment, global intersection, multiparty set disjointness, item bidding, Computer science, Upper bound, Bandwidth, interaction rounds, maximum matching, computational complexity, wellfare maximization]
A Time Hierarchy Theorem for the LOCAL Model
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
The celebrated Time Hierarchy Theorem for Turing machines states, informally, that more problems can be solved given more time. The extent to which a time hierarchy-type theorem holds in the classic distributed LOCAL model has been open for many years. In particular, it is consistent with previous results that all natural problems in the LOCAL model can be classified according to a small constant number of complexities, such as O(1), O(log* n), O(log n), 2O&#x221A;log n), etc.In this paper we establish the first time hierarchy theorem for the LOCAL model and prove that several gaps exist in the LOCAL time hierarchy. Our main results are as follows.
[Computational modeling, Ports (Computers), type theory, Complexity theory, Topology, Electronic mail, time hierarchy-type theorem, time hierarchy theorem, Turing machine states, LOCAL time hierarchy, distributed algorithm, Upper bound, Turing machines, local model, Labeling, computational complexity, distributed LOCAL model, locally checkable labeling]
Distributed Exact Weighted All-Pairs Shortest Paths in &#xd5;(n^{5/4}) Rounds
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study computing all-pairs shortest paths (APSP) on distributed networks (the CONGEST model). The goal is for every node in the (weighted) network to know the distance from every other node using communication. The problem admits (1+o(1))-approximation O&#x0303;(n)-time algorithms [2], [3], which are matched with &#x03A9;(n)-time lower bounds [3], [4], [5]1. No &#x03C9;(n) lower bound or o(m) upper bound were known for exact computation. In this paper, we present an O&#x0303;(n5/4)-time randomized (Las Vegas) algorithm for exact weighted APSP; this provides the first improvement over the naive O(m)-time algorithm when the network is not so sparse. Our result also holds for the case where edge weights are asymmetric (a.k.a. the directed case where communication is bidirectional). Our techniques also yield an O&#x0303;(n3/4k1/2 + n)-time algorithm for the k-source shortest paths problem where we want every node to know distances from k sources; this improves Elkin's recent bound [6] when k = &#x03C9;&#x0303;(n1/4). We achieve the above results by developing distributed algorithms on top of the classic scaling technique, which we believe is used for the first time for distributed shortest paths computation. One new algorithm which might be of an independent interest is for the reversed r-sink shortest paths problem, where we want every of r sinks to know its distances from all other nodes, given that every node already knows its distance to every sink. We show an O&#x0303;(n&#x221A;r)-time algorithm for this problem. Another new algorithm is called short range extension, where we show that in O&#x0303;(n&#x221A;h) time the knowledge about distances can be &#x201C;extended&#x201D; for additional h hops. For this, we use weight rounding to introduce small additive errors which can be later fixed.
[Additives, distributed exact weighted all-pairs shortest paths, weight rounding, k-source shortest paths problem, directed case, exact distributed algorithms, exact weighted APSP, Distributed algorithms, CONGEST model, reversed r-sink shortest paths problem, all-pairs shortest paths, approximation theory, Computational modeling, Routing, Las Vegas algorithm, O&#x0303;(n&#x221A;r)-time algorithm, edge weights, exact computation, distributed graph algorithms, Upper bound, distributed shortest paths computation, directed graphs, distributed algorithms, Approximation algorithms, distributed networks, naive O(m)-time algorithm, &#x03A9;(n)-time lower bounds, Time complexity, computational complexity]
Deterministic Distributed Edge-Coloring via Hypergraph Maximal Matching
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We present a deterministic distributed algorithm that computes a (2&#x0394;-1)-edge-coloring, or even list-edge-coloring, in any n-node graph with maximum degree &#x0394;, in O(log8 &#x0394;&#x00B7;log n) rounds. This answers one of the long-standing open questions of distributed graph algorithms} from the late 1980s, which asked for a polylogarithmic-time algorithm. See, e.g., Open Problem 4 in the Distributed Graph Coloring book of Barenboim and Elkin. The previous best round complexities were 2O(&#x221A;(log n) by Panconesi and Srinivasan [STOC'92] and O&#x0303;(&#x221A;(&#x0394;)) + O(log* n) by Fraigniaud, Heinrich, and Kosowski [FOCS'16]. A corollary of our deterministic list-edge-coloring also improves the randomized complexity of (2&#x0394;-1)-edge-coloring to poly(log log n) rounds. The key technical ingredient is a deterministic distributed algorithm for hypergraph maximal matching, which we believe will be of interest beyond this result. In any hypergraph of rank r - where each hyperedge has at most r vertices - with n nodes and maximum degree &#x0394;, this algorithm computes a maximal matching in O(r5 log6+log r &#x0394;&#x00B7;log n) rounds. This hypergraph matching algorithm and its extensions also lead to a number of other results. In particular, we obtain a polylogarithmic-time deterministic distributed maximal independent set (MIS) algorithm for graphs with bounded neighborhood independence, hence answering Open Problem 5 of Barenboim and Elkins book, a ((log &#x0394;/&#x03B5;)O(log 1/&#x03B5;))-round deterministic algorithm for (1+&#x03B5;)-approximation of maximum matching, and a quasi-polylogarithmic-time deterministic distributed algorithm for orienting &#x03BB;-arboricity graphs with out-degree at most &#x2308;(1+&#x03B5;)&#x03BB;&#x2309;, for any constant &#x03B5;&gt;0, hence partially answering Open Problem 10 of Barenboim and Elkin's book.
[hypergraph matching algorithm, quasipolylogarithmic-time deterministic distributed algorithm, Complexity theory, set theory, graph colouring, edge-coloring, hypergraph, deterministic list-edge-coloring, maximal independent set algorithm, Distributed Graph Coloring book, Distributed algorithms, Computational modeling, n-node graph, deterministic distributed algorithms, Color, Partitioning algorithms, deterministic algorithm, rounding linear programs, deterministic algorithms, Standards, randomised algorithms, distributed graph algorithms, Computer science, maximal matching, (1+&#x03B5;)-approximation, distributed algorithms, hypergraph maximal matching, polylogarithmic-time algorithm, local algorithms, &#x03BB;-arboricity graphs, computational complexity]
Fine-Grained Complexity of Analyzing Compressed Data: Quantifying Improvements over Decompress-and-Solve
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Can we analyze data without decompressing it? As our data keeps growing, understanding the time complexity of problems on compressed inputs, rather than in convenient uncompressed forms, becomes more and more relevant. Suppose we are given a compression of size n of data that originally has size N, and we want to solve a problem with time complexity T(&#x00B7;). The naive strategy of &#x201C;decompress-and-solve&#x201D; gives time T(N), whereas &#x201C;the gold standard&#x201D; is time T(n): to analyze the compression as efficiently as if the original data was small. We restrict our attention to data in the form of a string (text, files, genomes, etc.) and study the most ubiquitous tasks. While the challenge might seem to depend heavily on the specific compression scheme, most methods of practical relevance (Lempel-Ziv-family, dictionary methods, and others) can be unified under the elegant notion of Grammar-Compressions. A vast literature, across many disciplines, established this as an influential notion for Algorithm design. We introduce a direly needed framework for proving (conditional) lower bounds in this field, allowing us to assess whether decompress-and-solve can be improved, and by how much. Our main results are: (1) The O(nN&#x221A;(log N/n)) bound for LCS and the O(min{N log N, nM}) bound for Pattern Matching with Wildcards are optimal up to No(1) factors, under the Strong Exponential Time Hypothesis. (Here, M denotes the uncompressed length of the compressed pattern.) (2) Decompress-and-solve is essentially optimal for ContextFree Grammar Parsing and RNA Folding, under the k-Clique conjecture. (3) We give an algorithm showing that decompress-and-solve is not optimal for Disjointness.
[Gold, data compression, fine-grained complexity, grammar-compressions, pattern matching, Grammar, Standards, k-Clique conjecture, Computer science, compressed inputs, compressed data analysis, strong exponential time hypothesis, context-free grammars, convenient uncompressed forms, Time complexity, LCS, computational complexity, grammar-compression]
Local List Recovery of High-Rate Tensor Codes &amp; Applications
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In this work, we give the first construction of high-rate locally list-recoverable codes. List-recovery has been an extremely useful building block in coding theory, and our motivation is to use these codes as such a building block. In particular, our construction gives the first capacity-achieving locally list-decodable codes (over constant-sized alphabet); the first capacity achieving globally list-decodable codes with nearly linear time list decoding algorithm (once more, over constant-sized alphabet); and a randomized construction of binary codes on the Gilbert-Varshamov bound that can be uniquely decoded in near-linear-time, with higher rate than was previously known. Our techniques are actually quite simple, and are inspired by an approach of Gopalan, Guruswami, and Raghavendra (Siam Journal on Computing, 2011) for list-decoding tensor codes. We show that tensor powers of (globally) list-recoverable codes are `approximately' locally list-recoverable, and that the `approximately' modifier may be removed by pre-encoding the message with a suitable locally decodable code. Instantiating this with known constructions of high-rate globally list-recoverable codes and high-rate locally decodable codes finishes the construction.
[linear codes, randomized construction, list recovery, capacity-achieving locally list-decodable codes, high-rate tensor codes, high-rate globally list-recoverable codes, constant-sized alphabet, Complexity theory, linear time list decoding algorithm, coding theory, local list recovery, building block, Silicon, binary codes, error correction codes, error correcting codes, high-rate locally decodable codes, list-decoding tensor codes, Decoding, decoding, tensor codes, Computer science, Tensile stress, globally list-decodable codes, Gilbert-Varshamov bound, high-rate locally list-recoverable codes, near-linear-time, Error correction codes, computational complexity]
Optimal Repair of Reed-Solomon Codes: Achieving the Cut-Set Bound
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
The repair problem for an (n, k) error-correcting code calls for recovery of an unavailable coordinate of the codeword by downloading as little information as possible from a subset of the remaining coordinates. Using the terminology motivated by coding in distributed storage, we attempt to repair a failed node by accessing information stored on d helper nodes, where k &#x2264; d &#x2264; n - 1, and using as little repair bandwidth as possible to recover the lost information. By the so-called cut-set bound (Dimakis et al., 2010), the repair bandwidth of an (n,k = n - r) MDS code using d helper nodes is at least dl/(d + 1 - k), where l is the size of the node. A number of constructions of MDS array codes have been shown to meet this bound with equality. In a related but separate line of work, Guruswami and Wootters (2016) studied repair of Reed-Solomon (RS) codes, showing that it is possible to perform repair using a smaller bandwidth than under the trivial approach. At the same time, their work as well as follow-up papers stopped short of constructing RS codes (or any scalar MDS codes) that meet the cut-set bound with equality, which has been an open problem in coding theory. In this work we present a solution to this problem, constructing RS codes of length n over the field of size ql, l = exp((1 + o(1))n log n) that meet the cut-set bound. We also prove an almost matching lower bound on l, showing that super-exponential scaling is both necessary and sufficient for achieving the cut-set bound using linear repair schemes. More precisely, we prove that for scalar MDS codes (including the RS codes) to meet this bound, the sub-packetization l must satisfy l &#x2265; exp((1 + o(1))k log k).
[RS codes, super-exponential scaling, error correction codes, optimal repair, Maintenance engineering, MDS array codes, set theory, Reed-Solomon codes, Repair bandwidth, coding theory, linear repair schemes, helper nodes, repair bandwidth, cut-set bound, Cut-set bound, Distributed databases, Bandwidth, Optimal sub-packetization, Linear codes, scalar MDS codes, reed-solomon codes, computational complexity]
Average-Case Reconstruction for the Deletion Channel: Subpolynomially Many Traces Suffice
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
The deletion channel takes as input a bit string x &#x2208; {0, 1}n, and deletes each bit independently with probability q, yielding a shorter string. The trace reconstruction problem is to recover an unknown string x from many independent outputs (called &#x201C;traces&#x201D;) of the deletion channel applied to x. We show that if x is drawn uniformly at random and q &lt;; 1/2, then eO(log1/2 n) traces suffice to reconstruct x with high probability. The previous best bound, established in 2008 by Holenstein, Mitzenmacher, Panigrahy, and Wieder [1], uses nO(1) traces and only applies for q less than a smaller threshold (it seems that q &lt;; 0.07 is needed). Our algorithm combines several ideas: 1) an alignment scheme for &#x201C;greedily&#x201D; fitting the output of the deletion channel as a subsequence of the input; 2) a version of the idea of &#x201C;anchoring&#x201D; used in [1]; and 3) complex analysis techniques from recent work of Nazarov and Peres [2] and De, O'Donnell, and Servedio [3].
[Greedy algorithms, Algorithm design and analysis, sequence alignment, probability, random processes, trace reconstruction problem, Electronic mail, Indexes, complex analysis technique, alignment scheme, Computer science, subpolynomially many trace suffice, bit string, deletion channel, string matching, average-case reconstruction, trace reconstruction, Computational biology, computational complexity]
Optimal Interactive Coding for Insertions, Deletions, and Substitutions
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Interactive coding, pioneered by Schulman (FOCS 92, STOC 93), is concerned with making communication protocols resilient to adversarial noise. The canonical model allows the adversary to alter a small constant fraction of symbols, chosen at the adversarys discretion, as they pass through the communication channel. Braverman, Gelles, Mao, and Ostrovsky (2015) proposed a far-reaching generalization of this model, whereby the adversary can additionally manipulate the channel by removing and inserting symbols. They showed how to faithfully simulate any protocol in this model with corruption rate up to 1/18, using a constant-size alphabet and a constant-factor overhead in communication. We give an optimal simulation of any protocol in this generalized model of substitutions, insertions, and deletions, tolerating a corruption rate up to 1/4 while keeping the alphabet to a constant size and the communication overhead to a constant factor. Our corruption tolerance matches an impossibility result for corruption rate 1/4 which holds even for substitutions alone (Braverman and Rao, STOC 11).
[Protocols, insertions, deletions, communication channel, tree codes, corruption tolerance, communication complexity, communication overhead, symbols, Schulman, STOC 93, substitutions, adversarial noise, constant factor, interactive coding, optimal interactive coding, insertions and deletions, protocols, constant size, edit distance, error correction codes, optimal simulation, generalized model, Computational modeling, communication protocols, constant-size alphabet, canonical model, constant-factor overhead, Channel coding, Computer science, constant fraction, adversarys discretion, Communication channels, FOCS 92, corruption rate, computational complexity]
The Independence Number of the Birkhoff Polytope Graph, and Applications to Maximally Recoverable Codes
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Maximally recoverable codes are codes designed for distributed storage which combine quick recovery from single node failure and optimal recovery from catastrophic failure. Gopalan et al [SODA 2017] studied the alphabet size needed for such codes in grid topologies and gave a combinatorial characterization for it. Consider a labeling of the edges of the complete bipartite graph K<sub>n,n</sub> with labels coming from F<sub>2</sub>d, that satisfies the following condition: for any simple cycle, the sum of the labels over its edges is nonzero. The minimal d where this is possible controls the alphabet size needed for maximally recoverable codes in n &#x00D7; n grid topologies. Prior to the current work, it was known that d is between log(n)2 and n log n. We improve both bounds and show that d is linear in n. The upper bound is a recursive construction which beats the random construction. The lower bound follows by first relating the problem to the independence number of the Birkhoff polytope graph, and then providing tight bounds for it using the representation theory of the symmetric group.
[Birkhoff polytope graph, grid topologies, maximally recoverable codes, linear codes, distributed storage, complete bipartite graph K, graph theory, single node failure, coding theory, independence number, alphabet size, Network topology, labeling, catastrophic failure, Eigenvalues and eigenfunctions, Bipartite graph, Labeling, quick recovery, optimal recovery, topology, Topology, Birkhoff polytope, matrix algebra, labels, Computer science, Upper bound, representation theory, symmetric group, computational complexity]
Approximating Geometric Knapsack via L-Packings
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study the two-dimensional geometric knapsack problem (2DK) in which we are given a set of n axis-aligned rectangular items, each one with an associated profit, and an axis-aligned square knapsack. The goal is to find a (non-overlapping) packing of a maximum profit subset of items inside the knapsack (without rotating items). The best-known polynomial-time approximation factor for this problem (even just in the cardinality case) is 2+ &#x03B5; [Jansen and Zhang, SODA 2004]. In this paper we break the 2 approximation barrier, achieving a polynomialtime 17/9 + &#x03B5; &lt;; 1.89 approximation, which improves to 558/325 + &#x03B5; &lt;; 1.72 in the cardinality case. Essentially all prior work on 2DK approximation packs items inside a constant number of rectangular containers, where items inside each container are packed using a simple greedy strategy. We deviate for the first time from this setting: we show that there exists a large profit solution where items are packed inside a constant number of containers plus one L-shaped region at the boundary of the knapsack which contains items that are high and narrow and items that are wide and thin. The items of these two types possibly interact in a complex manner at the corner of the L. The above structural result is not enough however: the best-known approximation ratio for the subproblem in the L-shaped region is 2 + &#x03B5; (obtained via a trivial reduction to one-dimensional knapsack by considering tall or wide items only). Indeed this is one of the simplest special settings of the problem for which this is the best known approximation factor. As a second major, and the main algorithmic contribution of this paper, we present a PTAS for this case. We believe that this will turn out to be useful in future work in geometric packing problems. We also consider the variant of the problem with rotations (2DKR), where items can be rotated by 90 degrees. Also in this case the best-known polynomial-time approximation factor (even for the cardinality case) is 2+&#x03B5;[Jansen and Zhang, SODA 2004]. Exploiting part of the machinery developed for 2DK plus a few additional ideas, we obtain a polynomial-time 3/2 + &#x03B5;-approximation for 2DKR, which improves to 4/3 + &#x03B5; in the cardinality case.
[Algorithm design and analysis, Strips, greedy algorithms, Containers, rectangular containers, Electronic mail, Partitioning algorithms, Approximation Algorithms, polynomial-time approximation factor, bin packing, greedy strategy, 2DKR, Computer science, geometric packing problems, axis-aligned square knapsack, 2DK approximation, Rectangle Packing, polynomial approximation, Geometric Packing, Approximation algorithms, Two-dimensional Knapsack, two-dimensional geometric knapsack problem, computational complexity, L-Packings]
Removing Depth-Order Cycles among Triangles: An Efficient Algorithm Generating Triangular Fragments
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
More than 25 years ago, inspired by applications in computer graphics, Chazelle et al. (FOCS 1991) studied the following question: Is it possible to cut any set of n lines or other objects in R3 into a subquadratic number of fragments such that the resulting fragments admit a depth order? They managed to prove an O(n9/4) bound on the number of fragments, but only for the very special case of bipartite weavings of lines. Since then only little progress was made, until a recent breakthrough by Aronov and Sharir (STOC 2016) who showed that O(n3/2 polylog n) fragments suffice for any set of lines. In a follow-up paper Aronov, Miller and Sharir (SODA 2017) proved an O(n3/2+&#x03B5;) bound for triangles, but their method uses high-degree algebraic arcs to perform the cuts. Hence, the resulting pieces have curved boundaries. Moreover, their method uses polynomial partitions, for which currently no algorithm is known. Thus the most natural version of the problem is still wide open: Is it possible to cut any collection of n disjoint triangles in R3 into a subquadratic number of triangular fragments that admit a depth order? And if so, can we compute the cuts efficiently? We answer this question by presenting an algorithm that cuts any set of n disjoint triangles in R3 into O(n7/4 polylog n) triangular fragments that admit a depth order. The running time of our algorithm is O(n3.69). We also prove a refined bound that depends on the number, K, of intersections between the projections of the triangle edges onto the xy-plane: we show that O(n1+&#x03B5; + n1/4K3/4 polylog n) fragments suffice to obtain a depth order. This result extends to xy-monotone surface patches bounded by a constant number of bounded-degree algebraic arcs in general position, constituting the first subquadratic bound for surface patches. Finally, as a byproduct of our approach we obtain a faster algorithm to cut a set of lines into O(n3/2 polylog n) fragments that admit a depth order. Our algorithm for lines runs in O(n5.38) time, while the previous algorithm uses O(n8.77) time.
[subquadratic number, depth orders, polynomials, Optimized production technology, xy-monotone surface patches, computational geometry, Partitioning algorithms, Electronic mail, set theory, Computer science, high-degree algebraic arcs, cyclic overlap, Upper bound, computer graphics, triangular fragments, Computer graphics, bounded-degree algebraic arcs, Weaving, computational complexity, triangle edges]
Scheduling to Minimize Total Weighted Completion Time via Time-Indexed Linear Programming Relaxations
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study approximation algorithms for scheduling problems with the objective of minimizing total weighted completion time, under identical and related machine models with job precedence constraints. We give algorithms that improve upon many previous 15 to 20-year-old state-of-art results. A major theme in these results is the use of time-indexed linear programming relaxations. These are natural relaxations for their respective problems, but surprisingly are not studied in the literature.We also consider the scheduling problem of minimizing total weighted completion time on unrelated machines. The recent breakthrough result of [Bansal-Srinivasan-Svensson, STOC 2016] gave a (1.5-c)-approximation for the problem, based on some lift-and-project SDP relaxation. Our main result is that a (1.5 - c)-approximation can also be achieved using a natural and considerably simpler time-indexed LP relaxation for the problem. We hope this relaxation can provide new insights into the problem.
[Algorithm design and analysis, Schedules, approximation theory, Job shop scheduling, time-indexed linear programming relaxations, Computational modeling, Linear programming, linear programming, scheduling problem, timeindexed, weighted completion time, approximation algorithms, total weighted completion time, time-indexed LP relaxation, relaxation theory, natural relaxations, scheduling, Approximation algorithms, job precedence constraints, identical machine models, minimisation, computational complexity]
Fast &amp; Space-Efficient Approximations of Language Edit Distance and RNA Folding: An Amnesic Dynamic Programming Approach
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Dynamic programming is a basic, and one of the most systematic techniques for developing polynomial time algorithms with overwhelming applications. However, it often suffers from having high running time and space complexity due to (a) maintaining a table of solutions for a large number of sub-instances, and (b) combining/comparing these solutions to successively solve larger sub-instances. In this paper, we consider a canonical cubic time and quadratic space dynamic programming, and show how improvements in both its time and space uses are possible. As a result, we obtain fast small-space approximation algorithms for the fundamental problems of context free grammar recognition (the basic computer science problem of parsing), the language edit distance (a significant generalization of string edit distance and parsing), and RNA folding (a classical problem in bioinformatics). For these problems, ours are the first algorithms that break the cubic-time barrier of any combinatorial algorithm, and quadratic-space barrier of &#x201C;any&#x201D; algorithm significantly improving upon their long-standing space and time complexities. Our technique applies to many other problems as well including string edit distance computation, and finding longest increasing subsequence. Our improvements come from directly grinding the dynamic programming and looking through the lens of language edit distance which generalizes both context free grammar recognition, and RNA folding. From known conditional lower bound results, neither of these problems can have an exact combinatorial algorithm (one that does not use fast matrix multiplication) running in truly subcubic time. Moreover, for language edit distance such an algorithm cannot exist even when nontrivial multiplicative approximation is allowed. We overcome this hurdle by designing an additive-approximation algorithm that for any parameter k &gt; 0, uses O(nk log n) space and O(n2k log n) time and provides an additive O(nk log n)approximation. In particular, in O&#x0303;(n)1 space and O&#x0303;(n2) time it can solve deterministically whether a string belongs to a context free grammar, or &#x03F5;-far from it for any constant &#x03F5; &gt; 0. We also improve the above results to obtain an algorithm that outputs an &#x03F5; &#x00B7; n-additive approximation to the above problems with space complexity O(n2/3 log n). The space complexity remains sublinear in n, as long as &#x03F5; = o(n-1/<sub>4</sub> ). Moreover, we provide the first MapReduce and streaming algorithms for them with multiple passes and sublinear space complexity.
[Algorithm design and analysis, additive-approximation algorithm, RNA, Heuristic algorithms, Language Edit Distance, Context Free Grammar, graph theory, amnesic dynamic programming approach, RNA folding, time complexities, RNA-Folding, quadratic space dynamic programming, small-space approximation algorithms, polynomial time algorithms, context-free grammars, context free grammar recognition, Dynamic programming, canonical cubic time, approximation theory, polynomials, algorithm running time, Parsing, language edit distance, dynamic programming, Edit Distance, Light emitting diodes, Grammar, streaming algorithms, exact combinatorial algorithm, matrix multiplication, bioinformatics, Approximation algorithms, string edit distance computation, sublinear space complexity, computational complexity]
A Dichotomy for Regular Expression Membership Testing
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study regular expression membership testing: Given a regular expression of size m and a string of size n, decide whether the string is in the language described by the regular expression. Its classic O(nm) algorithm is one of the big success stories of the 70s, which allowed pattern matching to develop into the standard tool that it is today. Many special cases of pattern matching have been studied that can be solved faster than in quadratic time. However, a systematic study of tractable cases was made possible only recently, with the first conditional lower bounds reported by Backurs and Indyk [FOCS'16]. Restricted to any &#x201C;type&#x201D; of homogeneous regular expressions of depth 2 or 3, they either presented a near-linear time algorithm or a quadratic conditional lower bound, with one exception known as the Word Break problem. In this paper we complete their work as follows: (1) We present two almost-linear time algorithms that generalize all known almost-linear time algorithms for special cases of regular expression membership testing. (2) We classify all types, except for the Word Break problem, into almost-linear time or quadratic time assuming the Strong Exponential Time Hypothesis. This extends the classification from depth 2 and 3 to any constant depth. (3) For the Word Break problem we give an improved O&#x0303;(nm1/3 + m) algorithm. Surprisingly, we also prove a matching conditional lower bound for combinatorial algorithms. This establishes Word Break as the only intermediate problem. In total, we prove matching upper and lower bounds for any type of bounded-depth homogeneous regular expressions, which yields a full dichotomy for regular expression membership testing.
[algorithms, Dictionaries, pattern matching, combinatorial mathematics, combinatorial algorithms, quadratic conditional lower bound, Electronic mail, word break problem, Fast Fourier transforms, near-linear time algorithm, strong exponential time hypothesis, regular expressions, quadratic time algorithm, Informatics, improved O&#x0303;(nm1/3 + m) algorithm, conditional hardness, Testing, Computer science, bounded-depth homogeneous regular expressions, string matching, Pattern matching, regular expression membership testing, improved upper bounds, computational complexity, almost-linear time algorithms]
A Dichotomy Theorem for Nonuniform CSPs
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In a non-uniform Constraint Satisfaction problem CSP(&#x0393;), where &#x0393; is a set of relations on a unite set A, the goal is to und an assignment of values to variables subject to constraints imposed on speciued sets of variables using the relations from &#x0393;. The Dichotomy Conjecture for the non-uniform CSP states that for every constraint language &#x0393; the problem CSP(&#x0393;) is either solvable in polynomial time or is NP-complete. It was proposed by Feder and Vardi in their seminal 1993 paper. In this paper we confirm the Dichotomy Conjecture.
[Algorithm design and analysis, Terminology, polynomials, dichotomy theorem, nonuniform CSP states, speciued sets, Complexity theory, Electronic mail, NP-complete problem, Standards, nonuniform Constraint Satisfaction problem, Computer science, dichotomy conjecture, constraint satisfaction problems, Algebra, Dichotomy Conjecture, polynomial time, Constraint Satisfaction problem, constraint handling, constraint language, computational complexity]
A Proof of CSP Dichotomy Conjecture
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Many natural combinatorial problems can be expressed as constraint satisfaction problems. This class of problems is known to be NP-complete in general, but certain restrictions on the form of the constraints can ensure tractability. The standard way to parametrize interesting subclasses of the constraint satisfaction problem is via finite constraint languages. The main problem is to classify those subclasses that are solvable in polynomial time and those that are NP-complete. It was conjectured that if a core of a constraint language has a weak near unanimity polymorphism then the corresponding constraint satisfaction problem is tractable, otherwise it is NP-complete.In the paper we present an algorithm that solves Constraint Satisfaction Problem in polynomial time for constraint languages having a weak near unanimity polymorphism, which proves the remaining part of the conjecture.
[Constraint satisfaction problem, combinatorial mathematics, natural combinatorial problems, constraint theory, Cloning, constraint satisfaction problem, Electronic mail, CSP dichotomy conjecture, NP-complete problem, polynomial time algorithm, Standards, Computer science, constraint satisfaction problems, Algebra, finite constraint languages, weak-near-unanimity polymorphism, Time factors, constraint language, computational complexity, CSP dichotomy]
Learning Graphical Models Using Multiplicative Weights
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We give a simple, multiplicative-weight update algorithm for learning undirected graphical models or Markov random fields (MRFs). The approach is new, and for the well-studied case of Ising models or Boltzmann machines we obtain an algorithm that uses a nearly optimal number of samples and has running time O&#x0303;(n2) (where n is the dimension), subsuming and improving on all prior work. Additionally, we give the first efficient algorithm for learning Ising models over non-binary alphabets. Our main application is an algorithm for learning the structure of t-wise MRFs with nearly-optimal sample complexity (up to polynomial losses in necessary terms that depend on the weights) and running time that is nO(t). In addition, given nO(t) samples, we can also learn the parameters of the model and generate a hypothesis that is close in statistical distance to the true MRF. All prior work runs in time n&#x03A9;(d) for graphs of bounded degree d and does not generate a hypothesis close in statistical distance even for t = 3. We observe that our runtime has the correct dependence on n and t assuming the hardness of learning sparse parities with noise. Our algorithm- the Sparsitron- is easy to implement (has only one parameter) and holds in the on-line setting. Its analysis applies a regret bound from Freund and Schapires classic Hedge algorithm. It also gives the first solution to the problem of learning sparse Generalized Linear Models (GLMs).
[Algorithm design and analysis, O(t) samples, learning Ising models, generalized linear models, Machine learning algorithms, graph theory, polynomial losses, learning, sparsity, Complexity theory, nonbinary alphabets, Boltzmann machines, multiplicative-weight update algorithm, Graphical models, sparse Generalized Linear Models, t-wise MRFs, Ising model, Mathematical model, learning (artificial intelligence), Graphical Models, undirected graphical models, Markov Random Fields, statistical distance, Markov random fields, Computer science, Sparsitron, MRFs, sigmoid, Markov processes, nearly-optimal sample complexity, multiplicative weights, computational complexity]
Active Classification with Comparison Queries
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study an extension of active learning in which the learning algorithm may ask the annotator to compare the distances of two examples from the boundary of their label-class. For example, in a recommendation system application (say for restaurants), the annotator may be asked whether she liked or disliked a specific restaurant (a label query); or which one of two restaurants did she like more (a comparison query). We focus on the class of half spaces, and show that under natural assumptions, such as large margin or bounded bit-description of the input examples, it is possible to reveal all the labels of a sample of size n using approximately O(log n) queries. This implies an exponential improvement over classical active learning, where only label queries are allowed. We complement these results by showing that if any of these assumptions is removed then, in the worst case, &#x03A9;(n) queries are required. Our results follow from a new general framework of active learning with additional queries. We identify a combinatorial dimension, called the inference dimension, that captures the query complexity when each additional query is determined by O(1) examples (such as comparison queries, each of which is determined by the two compared examples). Our results for half spaces follow by bounding the inference dimension in the cases discussed above.
[pattern classification, learning algorithm, active classification, query complexity, Complexity theory, Standards, &#x03A9;(n) queries, Computer science, query processing, recommender systems, active learning, catering industry, label-class, restaurants, Inference algorithms, Labeling, learning (artificial intelligence), comparison query, Context modeling, computational complexity, recommendation system application, label query]
Capacity of Neural Networks for Lifelong Learning of Composable Tasks
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We investigate neural circuits in the exacting setting that (i) the acquisition of a piece of knowledge can occur from a single interaction, (ii) the result of each such interaction is a rapidly evaluatable subcircuit, (iii) hundreds of thousands of such subcircuits can be acquired in sequence without substantially degrading the earlier ones, and (iv) recall can be in the form of a rapid evaluation of a composition of subcircuits that have been so acquired at arbitrary different earlier times.We develop a complexity theory, in terms of asymptotically matching upper and lower bounds, on the capacity of a neural network for executing, in this setting, the following action, which we call {\\it association}: Each action sets up a subcircuit so that the excitation of a chosen set of neurons A will in future cause the excitation of another chosen set B.% As model of computation we consider the neuroidal model, a fully distributed model in which the quantitative resources n, the neuron numbers, d, the number of other neurons each neuron is connected to, and k, the inverse of the maximum synaptic strength, are all accounted for.A succession of experiences, possibly over a lifetime, results in the realization of a complex set of subcircuits. The composability requirement constrains the model to ensure that, for each association as realized by a subcircuit, the excitation in the triggering set of neurons A is quantitatively similar to that in the triggered set B, and also that the unintended excitation in the rest of the system is negligible. These requirements ensure that chains of associations can be triggeredWe first analyze what we call the Basic Mechanism, which uses only direct connections between neurons in the triggering set A and the target set B. We consider random networks of n neurons with expected number d of connections to and from each. We show that in the composable context capacity growth is limited by d2, a severe limitation if the network is sparse, as it is in cortex. We go on to study the Expansive Mechanism, that additionally uses intermediate relay neurons which have high synaptic weights. For this mechanism we show that the capacity can grow as dn, to within logarithmic factors. From these two results it follows that in the composable regime, for the realistic cortical estimate of d=n1/2, superlinear capacity of order n3/2 in terms of the neuron numbers can be realized by the Expansive Mechanism, instead of the linear order n to which the Basic Mechanism is limited. More generally, for both mechanisms, we establish matching upper and lower bounds on capacity in terms of the parameters n, d, and the inverse maximum synaptic strength k.The results as stated above assume that in a set of associations, a target B can be triggered by at most one set A. It can be shown that the capacities are similar if the number m of As that can trigger a B is greater than one but small, but become severely constrained if m exceeds a certain threshold.
[Knowledge engineering, associations, triggering set, neural circuits, fully distributed model, expected number, superlinear capacity, intermediate relay neurons, inverse maximum synaptic strength, learning (artificial intelligence), asymptotically matching upper bound, composability requirement, composable context capacity growth, complexity theory, Computational modeling, Neurons, knowledge acquisition, neuroidal model, neuroscience, neural computation, composable tasks, Biological neural networks, expansive mechanism, asymptotically matching lower bound, neuron numbers, random networks, Brain modeling, neurophysiology, triggered set, neural nets, computational complexity]
Efficient Bayesian Estimation from Few Samples: Community Detection and Related Problems
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We propose an efficient meta-algorithm for Bayesian inference problems based on low-degree polynomials, semidefinite programming, and tensor decomposition. The algorithm is inspired by recent lower bound constructions for sum-of-squares and related to the method of moments. Our focus is on sample complexity bounds that are as tight as possible (up to additive lower-order terms) and often achieve statistical thresholds or conjectured computational thresholds. Our algorithm recovers the best known bounds for partial recovery in the stochastic block model, a widely-studied class of inference problems for community detection in graphs. We obtain the first partial recovery guarantees for the mixed-membership stochastic block model (Airoldi et el.) for constant average degree-up to what we conjecture to be the computational threshold for this model. We show that our algorithm exhibits a sharp computational threshold for the stochastic block model with multiple communities beyond the Kesten-Stigum bound-giving evidence that this task may require exponential time. The basic strategy of our algorithm is strikingly simple: we compute the best-possible low-degree approximation for the moments of the posterior distribution of the parameters and use a robust tensor decomposition algorithm to recover the parameters from these approximate posterior moments.
[Algorithm design and analysis, Correlation, graph theory, Stochastic processes, meta-algorithm, average-case hardness, tensors, sum of squares algorithms, Complexity theory, sample complexity bounds, low-degree polynomials, semidefinite programming, low-degree approximation, Prediction algorithms, approximate posterior moments, stochastic processes, statistical thresholds, Bayesian inference, community detection, partial recovery guarantees, Bayesian estimation, Bayesian inference problems, polynomials, mixed-membership stochastic block model, Estimation, sum-of-squares, lower bound constructions, phase transitions, inference mechanisms, mathematical programming, stochastic blockmodel, lower-order terms, robust tensor decomposition algorithm, Inference algorithms, Bayes methods, constant average degree, computational complexity, sharp computational threshold, tensor decomposition]
Robust Polynomial Regression up to the Information Theoretic Limit
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We consider the problem of robust polynomial regression, where one receives samples that are usually within a small additive error of a target polynomial, but have a chance of being arbitrary adversarial outliers. Previously, it was known how to efficiently estimate the target polynomial only when the outlier probability was subconstant in the degree of the target polynomial. We give an algorithm that works for the entire feasible range of outlier probabilities, while simultaneously improving other parameters of the problem. We complement our algorithm, which gives a factor 2 approximation, with impossibility results that show, for example, that a 1.09 approximation is impossible even with infinitely many samples.
[Additives, outlier probability, Approximation, probability, robust polynomial regression, Polynomial Regression, information theoretic limit, Decoding, Complexity theory, Computer science, Learning, polynomial approximation, arbitrary adversarial outliers, Chebyshev approximation, Approximation algorithms, Robustness, target polynomial, Robust regression, computational complexity, Robust recovery]
Quantum SDP-Solvers: Better Upper and Lower Bounds
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Brandao and Svore recently gave quantum algorithms for approximately solving semidefinite programs, which in some regimes are faster than the best-possible classical algorithms in terms of the dimension n of the problem and the number m of constraints, but worse in terms of various other parameters. In this paper we improve their algorithms in several ways, getting better dependence on those other parameters. To this end we develop new techniques for quantum algorithms, for instance a general way to efficiently implement smooth functions of sparse Hamiltonians, and a generalized minimum-finding procedure.We also show limits on this approach to quantum SDP-solvers, for instance for combinatorial optimizations problems that have a lot of symmetry. Finally, we prove some general lower bounds showing that in the worst case, the complexity of every quantum LP-solver (and hence also SDP-solver) has to scale linearly with mn when m is approximately n, which is the same as classical.
[combinatorial mathematics, best-possible classical algorithms, Optimized production technology, generalized minimum-finding procedure, Complexity theory, mathematical programming, quantum LP-solver, Quantum algorithms, Lower bounds, Quantum computing, Upper bound, Runtime, semidefinite programs, Semidefinite programs, Linear programs, Approximation algorithms, combinatorial optimization problems, quantum algorithms, SDP-solver, Manganese, computational complexity, quantum SDP-solvers]
Quantum Speed-Ups for Solving Semidefinite Programs
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We give a quantum algorithm for solving semidefinite programs (SDPs). It has worst-case running time n1/2 m1/2 s2 poly(log(n), log(m), R, r, 1/&#x03B4;), with n and s the dimension and row-sparsity of the input matrices, respectively, m the number of constraints, &#x03B4; the accuracy of the solution, and R, r upper bounds on the size of the optimal primal and dual solutions, respectively. This gives a square-root unconditional speed-up over any classical method for solving SDPs both in n and m. We prove the algorithm cannot be substantially improved (in terms of n and m) giving a &#x03A9;(n1/2 + m2) quantum lower bound for solving semidefinite programs with constant s, R, r and &#x03B4;. The quantum algorithm is constructed by a combination of quantum Gibbs sampling and the multiplicative weight method. In particular it is based on a classical algorithm of Arora and Kale for approximately solving SDPs. We present a modification of their algorithm to eliminate the need for solving an inner linear program which may be of independent interest.
[quantum Gibbs sampling, quantum algorithm, linear programming, Electronic mail, dual solutions, Optimization, square-root unconditional speed-up, Quantum computing, SDPs, semidefinite programs, worst-case running time, quantum speed-ups, optimal primal solutions, quantum algorithms, inner linear program, approximation theory, Size measurement, Gibbs sampling, input matrices, matrix algebra, row-sparsity, Upper bound, Quantum mechanics, quantum computing, Approximation algorithms, classical algorithm, computational complexity]
Local Hamiltonians Whose Ground States Are Hard to Approximate
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Ground states of local Hamiltonians can be generally highly entangled: any quantum circuit that generates them, even approximately, must be sufficiently deep to allow coupling (entanglement) between any pair of qubits. Until now this property was not known to be &#x201C;robust&#x201D; - the marginals of such states to a subset of the qubits containing all but a small constant fraction of them may be only locally entangled, and hence approximable by shallow quantum circuits. In this work we construct a family of 16-local Hamiltonians for which any marginal of a ground state to a fraction at least 1-10-8 of the qubits must be globally entangled. This provides evidence that quantum entanglement is not very fragile, and perhaps our intuition about its instability is an artifact of considering local Hamiltonians which are not only local but spatially local. Formally, it provides positive evidence for two wide-open conjectures in condensed-matter physics and quantum complexity theory which are the qLDPC conjecture, positing the existence of &#x201C;good&#x201D; quantum LDPC codes, and the NLTS conjecture [1] positing the existence of local Hamiltonians in which any low-energy state is highly entangled. Our Hamiltonian is based on applying the hypergraph product by Tillich-Zemor [2] to the repetition code with checks from an expander graph. A key tool in our proof is a new lower bound on the vertex expansion of the output of low-depth quantum circuits, which may be of independent interest.
[Quantum entanglement, ground states, Stationary state, graph theory, highdimensional expander, quantum entanglement, Graph theory, Complexity theory, local Hamiltonians, robust codes, low-depth quantum circuits, quantum circuit, quantum computing, condensed-matter physics, shallow quantum circuits, NLTS, Robustness, qLDPC conjecture, PCP, ground state]
On Preparing Ground States of Gapped Hamiltonians: An Efficient Quantum Lov&#xe1;sz Local Lemma
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
A frustration-free local Hamiltonian has the property that its ground state minimises the energy of all local terms simultaneously. In general, even deciding whether a Hamiltonian is frustration-free is a hard task, as it is closely related to the QMA<sub>1</sub>-complete quantum satisfiability problem (QSAT) - the quantum analogue of SAT, which is the archetypal NP-complete problem in classical computer science. This connection shows that the frustration-free property is not only relevant to physics but also to computer science. The Quantum Lovasz Local Lemma (QLLL) provides a sufficient condition for frustration-freeness. Is there an efficient way to prepare a frustration-free state under the conditions of the QLLL? Previous results showed that the answer is positive if all local terms commute. These works were based on Moser's &#x201C;compression argument&#x201D; which was the original analysis technique of the celebrated resampling algorithm. We generalise and simplify the &#x201C;compression argument&#x201D;, so that it provides a simplified version of the previous quantum results, and improves on some classical results as well. More importantly, we improve on the previous constructive results by designing an algorithm that works efficiently for non-commuting terms as well, assuming that the system is &#x201C;uniformly&#x201D; gapped, by which we mean that the system and all its subsystems have an inverse polynomial energy gap. Similarly to the previous results, our algorithm has the charming feature that it uses only local measurement operations corresponding to the local Hamiltonian terms.
[Algorithm design and analysis, frustration-freeness, frustration-free local Hamiltonian, graph theory, local Hamiltonian terms, computability, Search problems, Local Hamiltonians, celebrated resampling algorithm, frustration-free property, Gapped Hamiltonians, Quantum computing, QMA<sub>1</sub>-complete quantum satisfiability problem, archetypal NP-complete problem, quantum analogue-of-SAT, sufficient condition, Mosers compression argument, polynomials, QMA, Stationary state, Quantum Lov&#x00E1;sz Local Lemma, frustration-free state, NP-complete problem, Computer science, Quantum Algorithm, Moser-Tardos Algorithm, classical computer science, QSAT, noncommuting terms, quantum computing, Quantum Lovasz Local Lemma, QLLL, local measurement operations, inverse polynomial energy gap]
Variable-Version Lov&#xe1;sz Local Lemma: Beyond Shearer's Bound
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
A tight criterion under which the abstract version Lovasz Local Lemma (abstract-LLL) holds was given by Shearer [42] decades ago. However, little is known about that of the variable version LLL (variable-LLL) where events are generated by independent random variables, though this model of events is applicable to almost all applications of LLL. We introduce a necessary and sufficient criterion for variable-LLL, in terms of the probabilities of the events and the eventvariable graph specifying the dependency among the events. Based on this new criterion, we obtain boundaries for two families of event-variable graphs, namely, cyclic and treelike bigraphs. These are the first two non-trivial cases where the variable-LLL boundary is fully determined. As a byproduct, we also provide a universal constructive method to find a set of events whose union has the maximum probability, given the probability vector and the event-variable graph. Though it is #P-hard in general to determine variable-LLL boundaries, we can to some extent decide whether a gap exists between a variable-LLL boundary and the corresponding abstract-LLL boundary. In particular, we show that the gap existence can be decided without solving Shearer's conditions or checking our variable-LLL criterion. Equipped with this powerful theorem, we show that there is no gap if the base graph of the event-variable graph is a tree, while gap appears if the base graph has an induced cycle of length at least 4. The problem is almost completely solved except when the base graph has only 3-cliques, in which case we also get partial solutions. A set of reduction rules are established that facilitate to infer gap existence of an event-variable graph from known ones. As an application, various event-variable graphs, in particular combinatorial ones, are shown to be gapful/gapless.
[Algorithm design and analysis, variable version LLL, event-variable graph, probability, trees (mathematics), abstract-LLL boundary, Electronic mail, Physics, treelike bigraphs, variable-LLL, Computer science, variable-LLL boundary, gap of LLL, cyclic bigraphs, variable-LLL criterion, Clustering algorithms, abstract version Lovasz Local Lemma, Approximation algorithms, Random variables, Lov&#x00E1;sz Local Lemma, base graph, independent random variables, computational complexity]
Linear Algebraic Analogues of the Graph Isomorphism Problem and the Erd&#x151;s-R&#xe9;nyi Model
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
A classical difficult isomorphism testing problem is to test isomorphism of p-groups of class 2 and exponent p in time polynomial in the group order. It is known that this problem can be reduced to solving the alternating matrix space isometry problem over a finite field in time polynomial in the underlying vector space size. We propose a venue of attack for the latter problem by viewing it as a linear algebraic analogue of the graph isomorphism problem. This viewpoint leads us to explore the possibility of transferring techniques for graph isomorphism to this long-believed bottleneck case of group isomorphism. In 1970's, Babai, Erdo&#x0303;s, and Selkow presented the first average-case efficient graph isomorphism testing algorithm (SIAM J Computing, 1980). Inspired by that algorithm, we devise an average-case efficient algorithm for the alternating matrix space isometry problem over a key range of parameters, in a random model of alternating matrix spaces in vein of the Erdo&#x0303;s-Re&#x0301;nyi model of random graphs. For this, we develop a linear algebraic analogue of the classical individualisation technique, a technique belonging to a set of combinatorial techniques that has been critical for the progress on the worstcase time complexity for graph isomorphism, but was missing in the group isomorphism context. This algorithm also enables us to improve Higman's 57-year-old lower bound on the number of p-groups (Proc. of the LMS, 1960). We finally show that Luks' dynamic programming technique for graph isomorphism (STOC 1999) can be adapted to slightly improve the worstcase time complexity of the alternating matrix space isometry problem in a certain range of parameters. Most notable progress on the worst-case time complexity of graph isomorphism, including Babai's recent breakthrough (STOC 2016) and Babai and Luks' previous record (STOC 1983), has relied on both group theoretic and combinatorial techniques. By developing a linear algebraic analogue of the individualisation technique and demonstrating its usefulness in the average-case setting, the main result opens up the possibility of adapting that strategy for graph isomorphism to this hard instance of group isomorphism. The linear algebraic Erdo&#x0303;s-Re&#x0301;nyi model is of independent interest and may deserve further study.
[Algorithm design and analysis, Frequency modulation, graph theory, matrix space isometry problem, Quantum computing, graph isomorphism problem, Erdos-R&#x00E9;nyi model, graph isomorphism, linear algebra, Testing, worst-case time complexity, isomorphism testing problem, Computational modeling, polynomials, dynamic programming, linear algebraic Erdo&#x0303;s-Re&#x0301;nyi model, group isomorphism context, linear algebraic analogue, individualisation and refinement, group theory, combinatorial techniques, average-case efficient graph isomorphism, Software, time polynomial, group isomorphism, Time complexity, computational complexity]
Optimal Lower Bounds for Universal Relation, and for Samplers and Finding Duplicates in Streams
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In the communication problem UR (universal relation) [25], Alice and Bob respectively receive x, y &#x2208; {0, 1}n with the promise that x &#x2260; y. The last player to receive a message must output an index i such that x<sub>i</sub> &#x2260; y<sub>i</sub>. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly &#x0398;(min{n, log(1/&#x03B4;) log2(n/log(1/&#x03B4;) )}) for failure probability &#x03B4;. Our lower bound holds even if promised support(y) &#x2282; support(x). As a corollary, we obtain optimal lower bounds for &#x2113;<sub>p</sub>-sampling in strict turnstile streams for 0 &#x2264; p &lt;; 2, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if promised x &#x2208; {0, 1}n at all points in the stream. We give two different proofs of our main result. The first proof demonstrates that any algorithm A solving sampling problems in turnstile streams in low memory can be used to encode subsets of [n] of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to A throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with A, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of A to correctly answer adaptive queries which have positive but bounded mutual information with A's internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens. Our second proof is via a novel randomized reduction from Augmented Indexing [30] which needs to interact with A adaptively. To handle the adaptivity we identify certain likely interaction patterns and union bound over them to guarantee correct interaction on all of them. To guarantee correctness, it is important that the interaction hides some of its randomness from A in the reduction.
[Heuristic algorithms, &#x2113;p-sampling, correctness analysis, Complexity theory, communication complexity, public coin model, query processing, adaptive queries, streaming, optimal lower bounds, approximation theory, data analysis, one-way communication complexity, probability, Data structures, random noise, information theoretic minimum, internal randomness, Indexes, adaptive data analysis, lower bounds, algorithm A solving sampling problems, Computer science, Upper bound, communication problem UR, failure probability, computational complexity, strict turnstile streams]
First Efficient Convergence for Streaming k-PCA: A Global, Gap-Free, and Near-Optimal Rate
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study streaming principal component analysis (PCA), that is to find, in O(dk) space, the top k eigenvectors of a d &#x00D7; d hidden matrix &#x03A3; with online vectors drawn from covariance matrix &#x03A3;. We provide global convergence for Oja's algorithm which is popularly used in practice but lacks theoretical understanding for k &gt; 1. We also provide a modified variant Oja++ that runs even faster than Oja's. Our results match the information theoretic lower bound in terms of dependency on error, on eigengap, on rank k, and on dimension d, up to poly-log factors. In addition, our convergence rate can be made gap-free, that is proportional to the approximation error and independent of the eigengap. In contrast, for general rank k, before our work (1) it was open to design any algorithm with efficient global convergence rate; and (2) it was open to design any algorithm with (even local) gap-free convergence rate in O(dk) space.
[Algorithm design and analysis, hidden matrix, convergence, near-optimal rate, efficient global convergence rate, Complexity theory, nonconvex optimization, Covariance matrices, approximation error, Convergence, eigenvalues and eigenfunctions, online vectors, covariance matrix, stochastic optimization, optimisation, Eigenvalues and eigenfunctions, theoretical understanding, k eigenvectors, optimal algorithm, Ojas algorithm, approximation theory, covariance matrices, online algorithm, global convergence, streaming algorithm, matrix algebra, pattern clustering, k-PCA, gap-free convergence rate, Approximation algorithms, principal component analysis, Principal component analysis, computational complexity]
Weighted k-Server Bounds via Combinatorial Dichotomies
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
The weighted k-server problem is a natural generalization of the k-server problem where each server has a different weight. We consider the problem on uniform metrics, which corresponds to a natural generalization of paging. Our main result is a doubly exponential lower bound on the competitive ratio of any deterministic online algorithm, that essentially matches the known upper bounds for the problem and closes a large and long-standing gap. The lower bound is based on relating the weighted k-server problem to a certain combinatorial problem and proving a Ramsey-theoretic lower bound for it. This combinatorial connection also reveals several structural properties of low cost feasible solutions to serve a sequence of requests. We use this to show that the generalized Work Function Algorithm achieves an almost optimum competitive ratio, and to obtain new refined upper bounds on the competitive ratio for the case of d different weight classes.
[Algorithm design and analysis, natural generalization, combinatorial mathematics, weighted k-server problem, combinatorial problem, Extraterrestrial measurements, weighted k-server, Servers, deterministic algorithms, known upper bounds, online algorithms, Standards, uniform metrics, refined upper bounds, competitive ratio, Ramsey-theoretic lower bound, Upper bound, weight classes, combinatorial dichotomies, generalized work function algorithm, Labeling, computational complexity, competitive analysis]
An Input Sensitive Online Algorithm for the Metric Bipartite Matching Problem
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We present a novel input sensitive analysis of a deterministic online algorithm [1] for the minimum metric bipartite matching problem. We show that, in the adversarial model, for any metric space M and a set of n servers S, the competitive ratio of this algorithm is O(μ<sub>M</sub>(S) log2 n); here μ<sub>M</sub>(S) is the maximum ratio of the traveling salesman tour and the diameter of any subset of S. It is straight-forward to show that any algorithm, even with complete knowledge of M and S, will have a competitive ratio of &#x03A9;(μ<sub>M</sub>(S)). So, the performance of this algorithm is sensitive to the input and near-optimal for any given S and M. As consequences, we also achieve the following results: 1) If S is a set of points on a line, then μ<sub>M</sub>(S) = &#x0398;(1) and the competitive ratio is O(log2 n), and, 2) If S is a set of points spanning a subspace with doubling dimension d, then μ<sub>M</sub>(S) = O(n1-1/d) and the competitive ratio is O(n1-1/d log2 n). Prior to this result, the previous best-known algorithm for the line metric has a competitive ratio of O(n0.59) and requires both S and the request set R to be on a line. There is also an O(log n) competitive algorithm in the weaker oblivious adversary model. To obtain our results, we partition the requests into well-separated clusters and replace each cluster with a small and a large weighted ball; the weight of a ball is the number of requests in the cluster. We show that the cost of the online matching can be expressed as the sum of the weight times radius of the smaller balls. We also show that the cost of edges of the optimal matching inside each larger ball can be shown to be proportional to the weight times the radius of the larger ball. We then use a simple variant of the well-known Vitali's covering lemma to relate the radii of these balls and obtain the competitive ratio.
[Algorithm design and analysis, pattern matching, sensitive analysis, doubling dimension, set theory, Servers, travelling salesman problems, competitive ratio, Analytical models, deterministic online algorithm, competitive algorithm, large weighted ball, minimum metric bipartite matching problem, approximation theory, input sensitive analysis, competitive algorithms, Extraterrestrial measurements, Partitioning algorithms, deterministic algorithms, Vitali's covering lemma, online algorithms, minimum metric matching, randomised algorithms, online matching, input sensitive online algorithm, Optimal matching, computational complexity, small weighted ball]
Learning Multi-Item Auctions with (or without) Samples
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We provide algorithms that learn simple auctions whose revenue is approximately optimal in multi-item multi-bidder settings, for a wide range of bidder valuations including unit-demand, additive, constrained additive, XOS, and subadditive. We obtain our learning results in two settings. The first is the commonly studied setting where sample access to the bidders' distributions over valuations is given, for both regular distributions and arbitrary distributions with bounded support. Here, our algorithms require polynomially many samples in the number of items and bidders. The second is a more general max-min learning setting that we introduce, where we are given &#x201C;approximate distributions,&#x201D; and we seek to compute a mechanism whose revenue is approximately optimal simultaneously for all &#x201C;true distributions&#x201D; that are close to the ones we were given. These results are more general in that they imply the sample-based results, and are also applicable in settings where we have no sample access to the underlying distributions but have estimated them indirectly via market research or by observation of bidder behavior in previously run, potentially non-truthful auctions. All our results hold for valuation distributions satisfying the standard (and necessary) independence-across-items property. They also generalize and improve upon recent works of Goldner and Karlin [25] and Morgenstern and Roughgarden [32], which have provided algorithms that learn approximately optimal multi-item mechanisms in more restricted settings with additive, subadditive and unit-demand valuations using sample access to distributions. We generalize these results to the complete unit-demand, additive, and XOS setting, to i.i.d. subadditive bidders, and to the max-min setting. Our results are enabled by new uniform convergence bounds for hypotheses classes under product measures. Our bounds result in exponential savings in sample complexity compared to bounds derived by bounding the VC dimension and are of independent interest.
[Additives, approximate distributions, subadditive bidders, Max-min Learning, independence-across-items property, additive unit-demand valuations, Complexity theory, commerce, minimax techniques, Cost accounting, valuation distributions, sample complexity, bidder valuations, XOS setting, approximately optimal multiitem mechanisms, Uniform Convergence Bounds under Product Measures, learning (artificial intelligence), max-min setting, subadditive unit-demand valuations, approximation theory, learning multiitem auctions, regular distributions, Sample Complexity, game theory, sample-based results, Revenue Maximization, multiitem multibidder settings, general max-min learning, bidder behavior, Standards, Computer science, arbitrary distributions, Multi-item Auctions, Approximation algorithms, Bayes methods, computational complexity, nontruthful auctions, complete unit-demand]
Oracle-Efficient Online Learning and Auction Design
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We consider the design of computationally efficient online learning algorithms in an adversarial setting in which the learner has access to an offline optimization oracle. We present an algorithm called Generalized Followthe- Perturbed-Leader and provide conditions under which it is oracle-efficient while achieving vanishing regret. Our results make significant progress on an open problem raised by Hazan and Koren [1], who showed that oracle-efficient algorithms do not exist in full generality and asked whether one can identify conditions under which oracle-efficient online learning may be possible. Our auction-design framework considers an auctioneer learning an optimal auction for a sequence of adversarially selected valuations with the goal of achieving revenue that is almost as good as the optimal auction in hindsight, among a class of auctions. We give oracle-efficient learning results for: (1) VCG auctions with bidder-specific reserves in singleparameter settings, (2) envy-free item-pricing auctions in multiitem settings, and (3) the level auctions of Morgenstern and Roughgarden [2] for single-item settings. The last result leads to an approximation of the overall optimal Myerson auction when bidders' valuations are drawn according to a fast-mixing Markov process, extending prior work that only gave such guarantees for the i.i.d. setting.We also derive various extensions, including: (1) oracleefficient algorithms for the contextual learning setting in which the learner has access to side information (such as bidder demographics), (2) learning with approximate oracles such as those based on Maximal-in-Range algorithms, and (3) no-regret bidding algorithms in simultaneous auctions, which resolve an open problem of Daskalakis and Syrgkanis [3].
[Algorithm design and analysis, adversarial setting, Oracle-efficient online learning, auction design, optimal Myerson auction, Electronic mail, single-item settings, History, Optimization, Cost accounting, envy-free item-pricing auctions, optimisation, revenue maximization, learning (artificial intelligence), auction-design framework, electronic commerce, contextual learning setting, approximation theory, tendering, Oracle-efficient learning results, offline optimization oracle, Stability analysis, VCG auctions, Markov processes, Approximation algorithms, generalized followthe- perturbed-leader, Follow-the-Perturbed-Leader, pricing, computational complexity, online learning]
Prophet Inequalities Made Easy: Stochastic Optimization by Pricing Non-Stochastic Inputs
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We present a general framework for stochastic online maximization problems with combinatorial feasibility constraints. The framework establishes prophet inequalities by constructing price-based online approximation algorithms, a natural extension of threshold algorithms for settings beyond binary selection. Our analysis takes the form of an extension theorem: we derive sufficient conditions on prices when all weights are known in advance, then prove that the resulting approximation guarantees extend directly to stochastic settings. Our framework unifies and simplifies much of the existing literature on prophet inequalities and posted price mechanisms, and is used to derive new and improved results for combinatorial markets (with and without complements), multi-dimensional matroids, and sparse packing problems. Finally, we highlight a surprising connection between the smoothness framework for bounding the price of anarchy of mechanisms and our framework, and show that many smooth mechanisms can be recast as posted price mechanisms with comparable performance guarantees.
[combinatorial mathematics, smoothness framework, mechanism design, Electronic mail, approximation algorithms, stochastic online maximization problems, Cost accounting, Optimization, stochastic optimization, binary selection, price of anarchy, optimisation, natural extension, smooth mechanisms, Pricing, posted price mechanisms, stochastic processes, approximation theory, sparse packing problems, combinatorial feasibility constraints, combinatorial markets, Computer science, smoothness, extension theorem, Approximation algorithms, threshold algorithms, stochastic settings, prophet inequalities, posted prices, Resource management, pricing nonstochastic inputs, pricing]
Tight Lower Bounds for Differentially Private Selection
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
A pervasive task in the differential privacy literature is to select the k items of &#x201C;highest quality&#x201D; out of a set of d items, where the quality of each item depends on a sensitive dataset that must be protected. Variants of this task arise naturally in fundamental problems like feature selection and hypothesis testing, and also as subroutines for many sophisticated differentially private algorithms. The standard approaches to these tasks-repeated use of the exponential mechanism or the sparse vector technique-approximately solve this problem given a dataset of n = O(&#x221A;k log d) samples. We provide a tight lower bound for some very simple variants of the private selection problem. Our lower bound shows that a sample of size n = &#x03A9;(&#x221A;k log d) is required even to achieve a very minimal accuracy guarantee. Our results are based on an extension of the fingerprinting method to sparse selection problems. Previously, the fingerprinting method has been used to provide tight lower bounds for answering an entire set of d queries, but often only some much smaller set of k queries are relevant. Our extension allows us to prove lower bounds that depend on both the number of relevant queries and the total number of queries.
[differentially private selection, hypothesis testing, Selection, smaller set, differential privacy literature, sparse vector technique, private selection problem, query processing, Privacy, fingerprinting method, Sociology, k items, Genetics, feature selection, Testing, tight lower bounds, pervasive task, Multiple Hypothesis Testing, sophisticated differentially private algorithms, Statistics, highest quality, query answering, selection problems, fundamental problems, Top-k Problem, Fingerprinting, relevant queries, Differential Privacy, Approximation algorithms, data privacy, sensitive dataset, computational complexity]
How to Achieve Non-Malleability in One or Two Rounds
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Non-malleable commitments, introduced by Dolev, Dwork and Naor (STOC 1991), are a fundamental cryptographic primitive, and their round complexity has been a subject of great interest. And yet, the goal of achieving non-malleable commitments with only one or two rounds has been elusive. Pass (TCC 2013) captured this difficulty by proving important impossibility results regarding two-round non-malleable commitments. This led to the widespread belief that achieving two-round nonmalleable commitments was impossible from standard assumptions. We show that this belief was false. Indeed, we obtain the following positive results: We construct two-message non-malleable commitments satisfying non-malleability with respect to commitment, based on standard sub-exponential assumptions, namely: sub-exponential one-way permutations, sub-exponential ZAPs, and sub-exponential DDH. Furthermore, our protocol is public-coin.; We obtain two-message private-coin non-malleable commitments with respect to commitment, assuming only sub-exponential DDH or QR or Nth-residuosity.; We bootstrap the above protocols (under the same assumptions) to obtain two round constant boundedconcurrent non-malleable commitments. In the simultaneous message model, we obtain unbounded concurrent non-malleability in two rounds.; In the simultaneous messages model, we obtain oneround non-malleable commitments, with unbounded concurrent security with respect to opening, under standard sub-exponential assumptions.; This implies non-interactive non-malleable commitments with respect to opening, in a restricted model with a broadcast channel, and a-priori bounded polynomially many parties such that every party is aware of every other party in the system. To the best of our knowledge, this is the first protocol to achieve completely non-interactive non-malleability in any plain model setting from standard assumptions.; As an application of this result, in the simultaneous exchange model, we obtain two-round multi-party pseudorandom coin-flipping.; We construct two-message zero-knowledge arguments with super-polynomial strong simulation (SPSS-ZK), which also serve as an important tool for our constructions of non-malleable commitments.; In order to obtain our results, we develop several techniques that may be of independent interest.; We give the first two-round black-box rewinding strategy based on standard sub-exponential assumptions, in the plain model.;- We also give a two-round tag amplification technique for non-malleable commitments, that amplifies a 4-tag scheme to a scheme for all tags, while relying on sub-exponential DDH. This includes a more efficient alternative to the DDN encoding.
[Protocols, cryptographic protocols, Nth-residuosity, sub-exponential DDH, broadcast channel, sub-exponential ZAPs, public-coin protocols, Complexity theory, two-round multiparty pseudorandom coin-flippin, two-message nonmalleable commitments, cryptographic primitive, sub-exponential one-way permutations, two-round tag amplification technique, simultaneous exchange model, Cryptography, simultaneous message model, two-message private-coin nonmalleable commitments, SPSS-ZK, standard sub-exponential assumptions, polynomials, Receivers, unbounded concurrent security, Standards, first two-round black-box rewinding strategy, Computer science, noninteractive nonmalleable commitments, constant bounded concurrent nonmalleable commitments, DDN encoding, two-round nonmalleable commitments, super-polynomial strong simulation, broadcast channels, two-message zero-knowledge arguments]
Two-Round and Non-Interactive Concurrent Non-Malleable Commitments from Time-Lock Puzzles
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Non-malleable commitments are a fundamental cryptographic tool for preventing against (concurrent) man-in-the-middle attacks. Since their invention by Dolev, Dwork, and Naor in 1991, the round-complexity of non-malleable commitments has been extensively studied, leading up to constant-round concurrent non-malleable commitments based only on one-way functions, and even 3-round concurrent non-malleable commitments based on subexponential one-way functions. But constructions of two-round, or non-interactive, nonmalleable commitments have so far remained elusive; the only known construction relied on a strong and non-falsifiable assumption with a non-malleability flavor. Additionally, a recent result by Pass shows the impossibility of basing two-round non-malleable commitments on falsifiable assumptions using a polynomial-time black-box security reduction. In this work, we show how to overcome this impossibility, using super-polynomial-time hardness assumptions. Our main result demonstrates the existence of a two-round concurrent non-malleable commitment based on subexponential &#x201C;standard-type&#x201D; assumptions-notably, assuming the existence of the following primitives (all with subexponential security): (1) non-interactive commitments, (2) ZAPs (i.e., 2-round witness indistinguishable proofs), (3) collision-resistant hash functions, and (4) a &#x201C;weak&#x201D; time-lock puzzle. Primitives (1),(2),(3) can be based on e.g., the discrete log assumption and the RSA assumption. Time-lock puzzles-puzzles that can be solved by &#x201C;brute-force&#x201D; in time 2t, but cannot be solved significantly faster even using parallel computers-were proposed by Rivest, Shamir, and Wagner in 1996, and have been quite extensively studied since; the most popular instantiation relies on the assumption that 2t repeated squarings mod N = pq require &#x201C;roughly&#x201D; 2t parallel time. Our notion of a &#x201C;weak&#x201D; time-lock puzzle, requires only that the puzzle cannot be solved in parallel time 2t&#x03F5; (and thus we only need to rely on the relatively mild assumption that there are no huge improvements in the parallel complexity of repeated squaring algorithms). We additionally show that if replacing assumption (2) for a non-interactive witness indistinguishable proof (NIWI), and (3) for a uniform collision-resistant hash function, then a non-interactive (i.e., one-message) version of our protocol satisfies concurrent non-malleability w.r.t. uniform attackers.
[nonfalsifiable assumption, non-interactive, Protocols, RSA assumption, super-polynomial-time hardness, cryptographic protocols, discrete log assumption, subexponential security, polynomial-time black-box security reduction, time-lock puzzles-puzzles, Non-malleable commitment, 2-round witness indistinguishable proofs, public key cryptography, time-lock puzzles, subexponential standard-type assumptions, parallel computers, weak time-lock puzzle, Cryptography, uniform collision-resistant hash function, parallel complexity, Engineering profession, polynomials, collision-resistant hash functions, subexponential one-way functions, cryptographic tool, round complexity, Standards, constant-round concurrent nonmalleable commitments, Resistance, Computer science, noninteractive concurrent nonmalleable commitments, 2-message, noninteractive witness indistinguishable proof, noninteractive commitments, computational complexity]
Garbled Protocols and Two-Round MPC from Bilinear Maps
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In this paper, we initiate the study of garbled protocols - a generalization of Yao's garbled circuits construction to distributed protocols. More specifically, in a garbled protocol construction, each party can independently generate a garbled protocol component along with pairs of input labels. Additionally, it generates an encoding of its input. The evaluation procedure takes as input the set of all garbled protocol components and the labels corresponding to the input encodings of all parties and outputs the entire transcript of the distributed protocol. We provide constructions for garbling arbitrary protocols based on standard computational assumptions on bilinear maps (in the common random string model). Next, using garbled protocols we obtain a general compiler that compresses any arbitrary round multiparty secure computation protocol into a two-round UC secure protocol. Previously, two-round multiparty secure computation protocols were only known assuming witness encryption or learning-with errors. Benefiting from our generic approach we also obtain protocols (i) for the setting of random access machines (RAM programs) while keeping communication and computational costs proportional to running times, while (ii) making only a black-box use of the underlying group, eliminating the need for any expensive non-black-box group operations. Our results are obtained by a simple but powerful extension of the non-interactive zero-knowledge proof system of Groth, Ostrovsky and Sahai [Journal of ACM, 2012].
[RAM programs, noninteractive zero-knowledge proof system, Bilinear Maps, Protocols, cryptographic protocols, Computational modeling, Random access memory, garbled protocol construction, two-round UC secure protocol, Circuit Garbling, Encoding, Universal Composability, Encryption, distributed protocol, multiparty secure computation protocols, two-round MPC, Standards, bilinear maps, random access machines, witness encryption, Yao's garbled circuits construction, garbling arbitrary protocols]
Obfuscating Compute-and-Compare Programs under LWE
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show how to obfuscate a large and expressive class of programs, which we call compute-and-compare programs, under the learning-with-errors (LWE) assumption. Each such program CC[f, y] is parametrized by an arbitrary polynomial-time computable function f along with a target value y and we define CC[f,y](x) to output 1 if f(x) = y and 0 otherwise. In other words, the program performs an arbitrary computation f and then compares its output against a target y. Our obfuscator satisfies distributional virtual-blackbox security, which guarantees that the obfuscated program does not reveal any partial information about the function f or the target value y, as long as they are chosen from some distribution where y has sufficient pseudo-entropy given f. We also extend our result to multi-bit compute-and-compare programs MBCC[f, y, z](x) which output a message z if f(x) = y. Compute-and-compare programs are powerful enough to capture many interesting obfuscation tasks as special cases. This includes obfuscating conjunctions, and therefore we improve on the prior work of Brakerski et al. (ITCS '16) which constructed a conjunction obfuscator under a non-standard &#x201C;entropic&#x201D; ring-LWE assumption, while here we obfuscate a significantly broader class of programs under standard LWE. We show that our obfuscator has several interesting applications. For example, we can take any encryption scheme and publish an obfuscated plaintext equality tester that allows users to check whether a ciphertext decrypts to some target value y; as long as y has sufficient pseudo-entropy this will not harm semantic security. We can also use our obfuscator to generically upgrade attribute-based encryption to predicate encryption with one-sided attribute-hiding security, and to upgrade witness encryption to indistinguishability obfuscation which is secure for all &#x201C;null circuits&#x201D;. Furthermore, we show that our obfuscator gives new circular-security counterexamples for public-key bit encryption and for unbounded length key cycles. Our result uses the graph-induced multi-linear maps of Gentry, Gorbunov and Halevi (TCC '15), but only in a carefully restricted manner which is provably secure under LWE. Our technique is inspired by ideas introduced in a recent work of Goyal, Koppula and Waters (EUROCRYPT '17) in a seemingly unrelated context.
[obfuscated plaintext equality tester, polynomials, obfuscating conjunctions, cryptography, indistinguishability obfuscation, learning-with-errors assumption, Encryption, Standards, encryption scheme, arbitrary polynomial-time computable function, entropy, Public key, Program Obfuscation, LWE assumption, compute-and-compare programs, learning (artificial intelligence), Integrated circuit modeling, obfuscated program]
Lockable Obfuscation
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In this paper we introduce the notion of lockable obfuscation. In a lockable obfuscation scheme there exists an obfuscation algorithm Obf that takes as input a security parameter, a program P, a message msg and lock value lck and outputs an obfuscated program oP. One can evaluate the obfuscated program oP on any input x where the output of evaluation is the message msg if P(x) = lck and otherwise receives a rejecting symbol. We proceed to provide a construction of lockable obfuscation and prove it secure under the Learning with Errors (LWE) assumption. Notably, our proof only requires LWE with polynomial hardness and does not require complexity leveraging. We follow this by describing multiple applications of lockable obfuscation. First, we show how to transform any attribute-based encryption (ABE) scheme into one in which the attributes used to encrypt the message are hidden from any user that is not authorized to decrypt the message. (Such a system is also know as predicate encryption with one-sided security.) The only previous construction due to Gorbunov, Vaikuntanathan and Wee is based off of a specific ABE scheme of Boneh. By enabling the transformation of any ABE scheme we can inherent different forms and features of the underlying scheme such as: multi-authority, adaptive security from polynomial hardness, regular language policies, etc. We also show applications of lockable obfuscation to separation and uninstantiability results. We first show how to create new separation results in circular encryption that were previously based on indistinguishability obfuscation. This results in new separation results from learning with error including a public key bit encryption scheme that it IND-CPA secure and not circular secure. The tool of lockable obfuscation allows these constructions to be almost immediately realized by translation from previous indistinguishability obfuscation based constructions. In a similar vein we provide random oracle uninstantiability results of the Fujisaki-Okamoto transformation (and related transformations) from the lockable obfuscation combined with fully homomorphic encryption. Again, we take advantage that previous work used indistinguishability obfuscation that obfuscated programs in a form that could easily be translated to lockable obfuscation.
[lockable obfuscation, attribute-based encryption, indistinguishability obfuscation based constructions, IND-CPA secure, security parameter, Encryption, Electronic mail, Complexity theory, polynomial hardness, Standards, Computer science, lockable obfuscation scheme, obfuscated programs, fully homomorphic encryption, public key cryptography, learning with errors, computational complexity]
White-Box vs. Black-Box Complexity of Search Problems: Ramsey and Graph Property Testing
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Ramsey theory assures us that in any graph there is a clique or independent set of a certain size, roughly logarithmic in the graph size. But how difficult is it to find the clique or independent set? If the graph is given explicitly, then it is possible to do so while examining a linear number of edges. If the graph is given by a black-box, where to figure out whether a certain edge exists the box should be queried, then a large number of queries must be issued. But what if one is given a program or circuit for computing the existence of an edge? This problem was raised by Buss and Goldberg and Papadimitriou in the context of TFNP, search problems with a guaranteed solution. We examine the relationship between black-box complexity and white-box complexity for search problems with guaranteed solution such as the above Ramsey problem. We show that under the assumption that collision resistant hash function exist (which follows from the hardness of problems such as factoring, discrete-log and learning with errors) the white-box Ramsey problem is hard and this is true even if one is looking for a much smaller clique or independent set than the theorem guarantees. In general, one cannot hope to translate all black-box hardness for TFNP into white-box hardness: we show this by adapting results concerning the random oracle methodology and the impossibility of instantiating it. Another model we consider is the succinct black-box, where there is a known upper bound on the size of the black-box (but no limit on the computation time). In this case we show that for all TFNP problems there is an upper bound on the number of queries proportional to the description size of the box times the solution size. On the other hand, for promise problems this is not the case. Finally, we consider the complexity of graph property testing in the white-box model. We show a property which is hard to test even when one is given the program for computing the graph. The hard property is whether the graph is a two-source extractor.
[graph theory, hard property, collision-resistant hashing, Search problems, Ramsey theory, Complexity theory, graph size, Cryptography, the Ramsey problem, search problems, box times, Testing, graph property testing, Computational modeling, white-box complexity, description size, black-box complexity, TFNP problems, white-box hardness, Resistance, black-box hardness, white-box Ramsey problem, white-box model, Integrated circuit modeling, succinct black-box, computational complexity]
Optimality of the Johnson-Lindenstrauss Lemma
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
For any d, n &#x2265; 2 and 1/(min{n, d})0.4999 &lt;; &#x03B5; &lt;; 1, we show the existence of a set of n vectors X &#x2282; &#x211D;d such that any embedding f : X &#x2192; &#x211D;m satisfying &#x2200;x, y &#x2208; X, (1-&#x03B5;)&#x2225;x-y&#x2225;<sub>2</sub>2 &#x2264; &#x2225;f(x)-f(y)&#x2225;<sub>2</sub>2 &#x2264; (1+&#x03B5;)&#x2225;x-y&#x2225;<sub>2</sub>2 must have m = &#x03A9;(&#x03B5;-2 lg n). This lower bound matches the upper bound given by the Johnson-Lindenstrauss lemma [JL84]. Furthermore, our lower bound holds for nearly the full range of &#x03B5; of interest, since there is always an isometric embedding into dimension min{d, n} (either the identity map, or projection onto span(X)). Previously such a lower bound was only known to hold against linear maps f, and not for such a wide range of parameters &#x03B5;, n, d [LN16]. The best previously known lower bound for general f was m = &#x03A9;(&#x03B5;-2 lg n/ lg(1/&#x03B5;)) [Wel74], [Alo03], which is suboptimal for any &#x03B5; = o(1).
[Additives, random projections, Johnson-Lindenstrauss, Distortion, Encoding, Complexity theory, set theory, Johnson-Lindenstrauss Lemma optimality, identity map, Computer science, Upper bound, isometric embedding, dimensionality reduction, JL84, computational complexity]
Optimal Compression of Approximate Inner Products and Dimension Reduction
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Let X be a set of n points of norm at most 1 in the Euclidean space Rk, and suppose &#x03B5; &gt; 0. An &#x03B5;-distance sketch for X is a data structure that, given any two points of X enables one to recover the square of the (Euclidean) distance between them up to an additive error of &#x03B5;. Let f(n, k, &#x03B5;) denote the minimum possible number of bits of such a sketch. Here we determine f(n, k, &#x03B5;) up to a constant factor for all n &#x2265; k &#x2265; 1 and all &#x03B5; &#x2265; 1/n0.49. Our proof is algorithmic, and provides an efficient algorithm for computing a sketch of size O(f(n, k, &#x03B5;)/n) for each point, so that the square of the distance between any two points can be computed from their sketches up to an additive error of &#x03B5; in time linear in the length of the sketches. We also discuss the case of smaller &#x03B5; &gt; 2/&#x221A;n and obtain some new results about dimension reduction in this range. In particular, we show that for any such &#x03B5; and any k &#x2264; t = log(2+&#x03B5;2n)/&#x03B5;2 there are configurations of n points in Rk that cannot be embedded in R&#x2113; for &#x2113; &lt;; ck with c a small absolute positive constant, without distorting some inner products (and distances) by more than &#x03B5;. On the positive side, we provide a randomized polynomial time algorithm for a bipartite variant of the Johnson-Lindenstrauss lemma in which scalar products are approximated up to an additive error of at most &#x03B5;. This variant allows a reduction of the dimension down to O(log(2+&#x03B5;2n)/&#x03B5;2), where n is the number of points.
[approximation theory, Additives, Correlation, optimal compression, polynomials, data structure, Tools, randomized polynomial time algorithm, Data structures, Johnson-Lindenstrauss lemma, set theory, dimension reduction, Gaussian correlation, approximate inner products, Computer science, scalar products approximation, constant factor, additive error, Euclidean distance, &#x03B5;-distance sketch, Euclidean space, epsilon-net, computational complexity, compression scheme]
Sample Efficient Estimation and Recovery in Sparse FFT via Isolation on Average
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
The problem of computing the Fourier Transform of a signal whose spectrum is dominated by a small number k of frequencies quickly and using a small number of samples of the signal in time domain (the Sparse FFT problem) has received significant attention recently. It is known how to approximately compute the k-sparse Fourier transform in &#x2248; k log2 n time [Hassanieh et al'STOC'12], or using the optimal number O(k log n) of samples [Indyk et al'FOCS'14] in time domain, or come within (log log n)O(1) factors of both these bounds simultaneously, but no algorithm achieving the optimal O(k log n) bound in sublinear time is known. At a high level, sublinear time Sparse FFT algorithms operate by `hashing' the spectrum of the input signal into &#x2248; k `buckets', identifying frequencies that are `isolated' in their buckets, subtracting them from the signal and repeating until the entire signal is recovered. The notion of `isolation' in a `bucket', inspired by applications of hashing in sparse recovery with arbitrary linear measurements, has been the main tool in the analysis of Fourier hashing schemes in the literature. However, Fourier hashing schemes, which are implemented via filtering, tend to be `noisy' in the sense that a frequency that hashes into a bucket contributes a non-negligible amount to neighboring buckets. This leakage to neighboring buckets makes identification and estimation challenging, and the standard analysis based on isolation becomes difficult to use without losing &#x03C9;(1) factors in sample complexity. In this paper we propose a new technique for analysing noisy hashing schemes that arise in Sparse FFT, which we refer to as isolation on average. We apply this technique to two problems in Sparse FFT: estimating the values of a list of frequencies using few samples and computing Sparse FFT itself, achieving sample-optimal results in k logO(1) n time for both. We feel that our approach will likely be of interest in designing Fourier sampling schemes for more general settings (e.g. model based Sparse FFT).
[Algorithm design and analysis, fast Fourier transforms, Fourier sampling, Fourier transforms, time domain, sublinear time sparse FFT algorithms, signal processing, Estimation, Fourier sampling schemes, sparse recovery, noisy hashing schemes, Complexity theory, Time-domain analysis, Sparse Fourier Transform, input signal, Runtime, sublinear algorithms, Signal processing algorithms, neighboring buckets, k-sparse Fourier transform, computational complexity]
Fast Similarity Sketching
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We consider the Similarity Sketching problem: Given a universe [u] = {0, ... ,u - 1} we want a random function S mapping subsets A &#x2286; [u] into vectors S(A) of size t, such that similarity is preserved. More precisely: Given sets A, B &#x2286; [u], define X<sub>i</sub> = [S(A)[i] = S(B)[i]] and X = &#x03A3;<sub>i&#x2208;[t]</sub> X<sub>i</sub>. We want to have E[X] = t &#x00B7; J(A, B), where J(A, B) = |A &#x2229; B|/|A &#x222A; B| and furthermore to have strong concentration guarantees (i.e. Chernoff-style bounds) for X. This is a fundamental problem which has found numerous applications in data mining, large-scale classification, computer vision, similarity search, etc. via the classic MinHash algorithm. The vectors S(A) are also called sketches. The seminal t&#x00D7;MinHash algorithm uses t random hash functions h<sub>1</sub>, ... , h<sub>t</sub>, and stores (min<sub>a&#x2208;A</sub> h<sub>1</sub>(A), ... , min<sub>a&#x2208;A</sub> h<sub>t</sub>(A)) as the sketch of A. The main drawback of MinHash is, however, its O(t &#x00B7; |A|) running time, and finding a sketch with similar properties and faster running time has been the subject of several papers. Addressing this, Li et al. [NIPS'12] introduced one permutation hashing (OPH), which creates a sketch of size t in O(t+|A|) time, but with the drawback that possibly some of the t entries are &#x201C;empty&#x201D; when |A| = O(t). One could argue that sketching is not necessary in this case, however the desire in most applications is to have one sketching procedure that works for sets of all sizes. Therefore, filling out these empty entries is the subject of several follow-up papers initiated by Shrivastava and Li [ICML'14]. However, these &#x201C;densification&#x201D; schemes fail to provide good concentration bounds exactly in the case |A| = O(t), where they are needed. In this paper we present a new sketch which obtains essentially the best of both worlds. That is, a fast O(t log t+|A|) expected running time while getting the same strong concentration bounds as MinHash. Our new sketch can be seen as a mix between sampling with replacement and sampling without replacement. We demonstrate the power of our new sketch by considering popular applications in large-scale classification with linear SVM as introduced by Li et al. [NIPS'11] as well as approximate similarity search using the LSH framework of Indyk and Motwani [STOC'98]. In particular, for the j<sub>1</sub>, j<sub>2</sub>-approximate similarity search problem on a collection of n sets we obtain a data-structure with space usage O(n1+&#x03C1; + &#x03A3;<sub>A&#x2208;C</sub> |A|) and O(n&#x03C1; log n + |Q|) expected time for querying a set Q compared to a O(n&#x03C1; log n &#x00B7; |Q|) expected query time of the classic result of Indyk and Motwani.
[Similarity Sketching problem, graph theory, hashing, Search problems, set theory, linear SVM, random hash functions, fast similarity sketching, similarity sketch, sketches, one permutation hashing, Jaccard similarity, data structures, search problems, LSH, MinHash algorithm, approximate similarity search problem, support vector machines, Optimized production technology, Estimation, random processes, Data structures, cryptography, classification, Standards, randomised algorithms, random function mapping subsets, Support vector machines, Computer science, locality-sensitive hashing, sketching, computational complexity]
Sublinear Time Low-Rank Approximation of Positive Semidefinite Matrices
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show how to compute a relative-error low-rank approximation to any positive semidefinite (PSD) matrix in sublinear time, i.e., for any n x n PSD matrix A, in O&#x0303;(n &#x00B7; poly(k/&#x03B5;)) time we output a rank-k matrix B, in factored form, for which ||A - B|| 2 F &#x2264; (1 + &#x03B5;)||A - A<sub>k</sub>||<sub>F</sub>2, where Ak is the best rank-k approximation to A. When k and 1/&#x03B5; are not too large compared to the sparsity of A, our algorithm does not need to read all entries of the matrix. Hence, we significantly improve upon previous nnz(A) time algorithms based on oblivious subspace embeddings, and bypass an nnz(A) time lower bound for general matrices (where nnz(A) denotes the number of non-zero entries in the matrix). We prove time lower bounds for low-rank approximation of PSD matrices, showing that our algorithm is close to optimal. Finally, we extend our techniques to give sublinear time algorithms for lowrank approximation of A in the (often stronger) spectral norm metric ||A - B||<sub>2</sub>2 and for ridge regression on PSD matrices.
[general matrices, positive semidefinite matrices, PSD matrices, regression analysis, matrix decomposition, matrix sketching, Covariance matrices, Runtime, time lower bounds, rank-k approximation, Kernel, sublinear time low-rank approximation, relative-error low-rank approximation, approximation theory, Symmetric matrices, sublinear time algorithms, Matrix decomposition, low-rank approximation, factored form, Standards, O&#x0303;(n &#x00B7; poly(k/&#x03B5;)) time, nonzero entries, leverage score sampling, Approximation algorithms, positive semidefinite matrix, sparse matrices, computational complexity]
Hardness Results for Structured Linear Systems
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show that if the nearly-linear time solvers for Laplacian matrices and their generalizations can be extended to solve just slightly larger families of linear systems, then they can be used to quickly solve all systems of linear equations over the reals. This result can be viewed either positively or negatively: either we will develop nearly-linear time algorithms for solving all systems of linear equations over the reals, or progress on the families we can solve in nearly-linear time will soon halt.
[Linear systems, Algorithm design and analysis, Total Variation Matrices, Transmission line matrix methods, Laplace equations, Laplacian Solvers, Fine-grained Complexity, structured linear systems, nearly-linear time algorithms, Multi-commodity Flow Problems, Two dimensional displays, Numerical Linear Algebra, Linear System Solvers, linear systems, eigenvalues and eigenfunctions, matrix algebra, Complexity Theory, Truss Stiffness Matrices, Approximation algorithms, Iterative methods, Laplacian matrices, computational complexity, linear equations]
The Matching Problem in General Graphs Is in Quasi-NC
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show that the perfect matching problem in general graphs is in Quasi-NC. That is, we give a deterministic parallel algorithm which runs in O(log3 n) time on nO(log2 n) processors. The result is obtained by a derandomization of the Isolation Lemma for perfect matchings, which was introduced in the classic paper by Mulmuley, Vazirani and Vazirani [1987] to obtain a Randomized NC algorithm.Our proof extends the framework of Fenner, Gurjar and Thierauf [2016], who proved the analogous result in the special case of bipartite graphs. Compared to that setting, several new ingredients are needed due to the significantly more complex structure of perfect matchings in general graphs. In particular, our proof heavily relies on the laminar structure of the faces of the perfect matching polytope.
[Algorithm design and analysis, general graphs, pattern matching, graph theory, deterministic parallel algorithm, Randomized NC algorithm, Electronic mail, Complexity theory, Parallel algorithms, perfect matching problem, Program processors, bipartite graphs, perfect matching polytope, laminar structure, perfect matching, nO(log2 n) processors, Bipartite graph, parallel algorithms, parallel complexity, QuasiNC, Isolation Lemma, derandomization, deterministic algorithms, O(log3 n) time, randomised algorithms, Computer science, computational complexity]
On the Power of Statistical Zero Knowledge
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We examine the power of statistical zero knowledge proofs (captured by the complexity class SZK) and their variants. First, we give the strongest known relativized evidence that SZK contains hard problems, by exhibiting an oracle relative to which SZK (indeed, even NISZK) is not contained in the class UPP, containing those problems solvable by randomized algorithms with unbounded error. This answers an open question of Watrous from 2002. Second, we &#x201C;lift&#x201D; this oracle separation to the setting of communication complexity, thereby answering a question of Goos et al. (ICALP 2016). Third, we give relativized evidence that perfect zero knowledge proofs (captured by the class PZK) are weaker than general zero knowledge proofs. Specifically, we exhibit oracles which separate SZK from PZK, NISZK from NIPZK and PZK from coPZK. The first of these results answers a question raised in 1991 by Aiello and Hastad (Information and Computation), and the second answers a question of Lovett and Zhang (2016). We also describe additional applications of these results outside of structural complexity. The technical core of our results is a stronger hardness amplification theorem for approximate degree, which roughly says that composing the gapped-majority function with any function of high approximate degree yields a function with high threshold degree.
[hardness amplification theorem, Perfect Zero Knowledge Proof, Protocols, NISZK, Hardness Amplification, perfect zero knowledge proofs, Oracle Separation, class PZK, Complexity theory, communication complexity, statistical zero knowledge proofs, Quantum computing, theorem proving, Cryptography, hard problems, oracle separation, Computational modeling, general zero knowledge proofs, Statistical Zero Knowledge proof, cryptography, randomised algorithms, Computer science, randomized algorithms, complexity class SZK, Games, structural complexity, class UPP, statistics]
The Power of Sum-of-Squares for Detecting Hidden Structures
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study planted problems-finding hidden structures in random noisy inputs-through the lens of the sum-of-squares semidefinite programming hierarchy (SoS). This family of powerful semidefinite programs has recently yielded many new algorithms for planted problems, often achieving the best known polynomial-time guarantees in terms of accuracy of recovered solutions and robustness to noise. One theme in recent work is the design of spectral algorithms which match the guarantees of SoS algorithms for planted problems. Classical spectral algorithms are often unable to accomplish this: the twist in these new spectral algorithms is the use of spectral structure of matrices whose entries are low-degree polynomials of the input variables. We prove that for a wide class of planted problems, including refuting random constraint satisfaction problems, tensor and sparse PCA, densest-ksubgraph, community detection in stochastic block models, planted clique, and others, eigenvalues of degree-d matrix polynomials are as powerful as SoS semidefinite programs of degree d. For such problems it is therefore always possible to match the guarantees of SoS without solving a large semidefinite program. Using related ideas on SoS algorithms and lowdegree matrix polynomials (and inspired by recent work on SoS and the planted clique problem [BHK+16]), we prove a new SoS lower bound for the tensor PCA problem.
[Algorithm design and analysis, SoS algorithms, planted clique problem, degree-d matrix polynomials, eigenvalues, densest-ksubgraph, graph theory, Programming, average-case hardness, tensors, random noisy inputs, power of sum-of-squares, sparse PCA, eigenvalues and eigenfunctions, low-degree polynomials, semidefinite programming, classical spectral algorithms, Robustness, sum-of-squares semidefinite programming hierarchy, Mathematical model, stochastic processes, hidden structures detection, community detection, polynomials, constraint theory, problems-finding hidden structures, spectral algorithms, mathematical programming, matrix algebra, average-case algorithms, powerful semidefinite programs, Tensile stress, constraint satisfaction problems, lowdegree matrix polynomials, polynomial-time guarantees, random constraint satisfaction problems, semidefinite program, large semidefinite program, Inference algorithms, stochastic block models, principal component analysis, tensor PCA problem, Principal component analysis, sum of squares]
A Time-Space Lower Bound for a Large Class of Learning Problems
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We prove a general memory-samples lower bound that applies for a large class of learning problems and shows that for every problem in that class, any learning algorithm requires either a memory of quadratic size or an exponential number of samples. Our result is stated in terms of the norm of the matrix that corresponds to the learning problem. Let X, A be two finite sets. A matrix M : A &#x00D7; X &#x2192; {-1, 1} corresponds to the following learning problem: An unknown element x &#x2208; X was chosen uniformly at random. A learner tries to learn x from a stream of samples, (a<sub>1</sub>, b<sub>1</sub>), (a<sub>2</sub>, b<sub>2</sub>) ..., where for every i, a<sub>i</sub> &#x2208; A is chosen uniformly at random and b<sub>i</sub> = M(ai, x). Let &#x03C3;<sub>max</sub> be the largest singular value of M and note that always &#x03C3;<sub>max</sub> &#x2264; |A|1/2 &#x00B7; |X|1/2. We show that if &#x03C3;<sub>max</sub> &#x2264; |A|1/2 &#x00B7; |X|1/2-&#x03B5;, then any learning algorithm for the corresponding learning problem requires either a memory of size at least &#x03A9; ((&#x03B5;n)2) or at least 2&#x03A9;(&#x03B5;n) samples, where n = log<sub>2</sub> |X|. As a special case, this gives a new proof for the memory-samples lower bound for parity learning [14].
[learning algorithm, Computational modeling, memory-samples lower bound, parity learning, quadratic size, exponential number, set theory, Computational complexity, matrix algebra, Computer science, Memory management, time-space lower bound, Random variables, Cryptography, learning (artificial intelligence), computational complexity, general memory-samples]
From Gap-ETH to FPT-Inapproximability: Clique, Dominating Set, and More
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We consider questions that arise from the intersection between the areas of approximation algorithms, subexponential-time algorithms, and fixed-parameter tractable algorithms. The questions, which have been asked several times (e.g., [1], [2], [3]) are whether there is a non-trivial FPT-approximation algorithm for the Maximum Clique (Clique) and Minimum Dominating Set (DomSet) problems parameterized by the size of the optimal solution. In particular, letting OPT be the optimum and N be the size of the input, is there an algorithm that runs in t(OPT) poly(N) time and outputs a solution of size f(OPT), for any functions t and f that are independent of N (for Clique, we want f(OPT) = &#x03C9;(1))? In this paper, we show that both Clique and DomSet admit no non-trivial FPT-approximation algorithm, i.e., there is no o(OPT)-FPT-approximation algorithm for Clique and no f(OPT)-FPT-approximation algorithm for DomSet, for any function f (e.g., this holds even if f is an exponential or the Ackermann function). In fact, our results imply something even stronger: The best way to solve Clique and DomSet, even approximately, is to essentially enumerate all possibilities. Our results hold under the Gap Exponential Time Hypothesis (GapETH) [4], [5], which states that no 2o(n)-time algorithm can distinguish between a satisfiable 3SAT formula and one which is not even (1 - &#x03B5;)-satisfiable for some constant &#x03B5; &gt; 0. Besides Clique and DomSet, we also rule out non-trivial FPT-approximation for Maximum Balanced Biclique, the problem of finding maximum subgraphs with hereditary properties (e.g., Maximum Induced Planar Subgraph), and Maximum Induced Matching in bipartite graphs. Previously only exact versions of these problems were known to be W[1]-hard [6], [7], [8]. Additionally, we rule out ko(1)-FPT-approximation algorithm for Densest k-Subgraph although this ratio does not yet match the trivial O(k)-approximation algorithm. To the best of our knowledge, prior results only rule out constant factor approximation for Clique [9], [10] and log1/4+&#x03B5;(OPT) approximation for DomSet for any constant &#x03B5; &gt; 0 [11]. Our result on Clique significantly improves on [9], [10]. However, our result on DomSet is incomparable to [11] since their results hold under ETH while our results hold under Gap-ETH, which is a stronger assumption.
[FPT-inapproximability, graph theory, Dominating Set, Electronic mail, set theory, fixed-parameter tractable algorithms, constant factor approximation, maximum clique, t(OPT) poly(N) time algorithm, gap exponential time hypothesis, subexponential-time algorithms, Bipartite graph, trivial O(k)-approximation algorithm, Set Cover, approximation theory, Fixed Parameter Tractability, gap-ETH, Optimized production technology, Minimization, f(OPT)-FPT-approximation algorithm, 1/4+&#x03B5; approximation, nontrivial FPT-approximation algorithm, Standards, Computer science, GapETH, minimum dominating set problems, Hardness of Approximation, DomSet, Approximation algorithms, Clique, computational complexity]
Faster (and Still Pretty Simple) Unbiased Estimators for Network (Un)reliability
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Consider the problem of estimating the (un)reliability of an n-vertex graph when edges fail with probability p. We show that the Recursive Contraction Algorithms for minimum cuts, essentially unchanged and running in n2+o(1) time, yields an unbiased estimator of constant relative variance (and thus an FPRAS with the same time bound) whenever pc &lt;; n-2. For larger p, we show that reliable graphs-where failures are rare so seemingly harder to find-effectively act like small graphs and can thus be analyzed quickly. Combining these ideas gives us an unbiased estimator for unreliability running in O&#x0303;(n2.78) time, an improvement on the previous O&#x0303;(n3) time bound.
[Algorithm design and analysis, recursive contraction algorithms, unbiased estimator, graph theory, Estimation, probability, networkreliability, Runtime, Monte Carlo methods, Computer network reliability, n-vertex graph, Approximation algorithms, unreliability running, Reliability, computational complexity, constant relative variance]
Minor-Free Graphs Have Light Spanners
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show that every H-minor-free graph has a light (1 + &#x03B5;)-spanner, resolving an open problem of Grigni and Sissokho [13] and proving a conjecture of Grigni and Hung [12]. Our lightness bound is (&#x03C3;<sub>H</sub>/&#x03F5;3 log 1/&#x03F5;) where &#x03C3;<sub>H</sub> = |V (H)|&#x221A; log |V (H)| is the sparsity coefficient of H-minor-free graphs. That is, it has a practical dependency on the size of the minor H. Our result also implies that the polynomial time approximation scheme (PTAS) for the Travelling Salesperson Problem (TSP) in H-minor-free graphs by Demaine, Hajiaghayi and Kawarabayashi [7] is an efficient PTAS whose running time is 2O<sub>H</sub> (1/&#x03F5;4 log 1/&#x03F5;) nO(1) where O<sub>H</sub> ignores dependencies on the size of H. Our techniques significantly deviate from existing lines of research on spanners for H-minor-free graphs, but build upon the work of Chechik and Wulff-Nilsen for spanners of general graphs [6].
[Steiner trees, Electrical engineering, Greedy algorithms, general graphs, approximation theory, light spanners, Terminology, polynomials, graph theory, lightness, Standards, Traveling Salesperson Problem (TSP), Computer science, TSP, graphs, PTAS, minor-free graphs, Poynomial Time Approximation Scheme (PTAS), polynomial time approximation scheme, Approximation algorithms, H-minor-free graphs, Travelling Salesperson Problem]
Polylogarithmic Approximation for Minimum Planarization (Almost)
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In the minimum planarization problem, given some n-vertex graph, the goal is to find a set of vertices of minimum cardinality whose removal leaves a planar graph. This is a fundamental problem in topological graph theory. We present a logO(1) n-approximation algorithm for this problem on general graphs with running time nO(log n/log log n). We also obtain a O(n&#x03B5;)-approximation with running time nO(1/&#x03B5;) for any arbitrarily small constant &#x03B5; &gt; 0. Prior to our work, no non-trivial algorithm was known for this problem on general graphs, and the best known result even on graphs of bounded degree was a n&#x03A9;(1)-approximation [1]. As an immediate corollary, we also obtain improved approximation algorithms for the crossing number problem on graphs of bounded degree. Specifically, we obtain O(n1/2+&#x03B5;)approximation and n1/2 logO(1) n-approximation algorithms in time nO(1/&#x03B5;) and nO(log n/log log n) respectively. The previously best-known result was a polynomial-time n9/10 logO(1) n-approximation algorithm [2]. Our algorithm introduces several new tools including an efficient grid-minor construction for apex graphs, and a new method for computing irrelevant vertices. Analogues of these tools were previously available only for exact algorithms. Our work gives efficient implementations of these ideas in the setting of approximation algorithms, which could be of independent interest.
[approximation theory, n-approximation algorithm, Particle separators, graph theory, computational geometry, Electronic mail, planar graph, Computer science, Planarization, minimum planarization, apex graphs, n-vertex graph, approximation algorithm, Approximation algorithms, quasi-polynomial time, Skeleton, Face, minimum planarization problem, polynomial-time, computational complexity, polylogarithmic approximation]
Approximating the Held-Karp Bound for Metric TSP in Nearly-Linear Time
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We give a nearly linear-time randomized approximation scheme for the Held-Karp bound [22] for Metric-TSP. Formally, given an undirected edge-weighted graph G = (V, &#x03B5;) on m edges and &#x03B5; &gt; 0, the algorithm outputs in O(m log4 n/&#x03B5;2) time, with high probability, a (1 + &#x03B5;)-approximation to the Held-Karp bound on the Metric-TSP instance induced by the shortest path metric on G. The algorithm can also be used to output a corresponding solution to the Subtour Elimination LP. We substantially improve upon the O(m2 log2(m)/&#x03B5;2) running time achieved previously by Garg and Khandekar.
[Measurement, Algorithm design and analysis, approximation theory, metric TSP, Subtour Elimination LP, graph theory, Programming, Data structures, undirected edge-weighted graph, Optimization, nearly-linear time, Metric-TSP instance, travelling salesman problems, Held-Karp bound, linear-time randomized approximation, Approximation algorithms, computational complexity]
Derandomization Beyond Connectivity: Undirected Laplacian Systems in Nearly Logarithmic Space
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We give a deterministic O&#x0303;(log n)-space algorithm for approximately solving linear systems given by Laplacians of undirected graphs, and consequently also approximating hitting times, commute times, and escape probabilities for undirected graphs. Previously, such systems were known to be solvable by randomized algorithms using O(log n) space (Doron, Le Gall, and Ta-Shma, 2017) and hence by deterministic algorithms using O(log3/2 n) space (Saks and Zhou, FOCS 1995 and JCSS 1999). Our algorithm combines ideas from time-efficient Laplacian solvers (Spielman and Teng, STOC `04; Peng and Spielman, STOC `14) with ideas used to show that UNDIRECTED S-T CONNECTIVITY is in deterministic logspace (Reingold, STOC `05 and JACM `08; Rozenman and Vadhan, RANDOM `05).
[Linear systems, graph theory, Complexity theory, Electronic mail, spectral sparsification, STOC, undirected graphs, time-efficient Laplacian solvers, approximation theory, random walks, Laplace equations, Symmetric matrices, probability, linear systems, derandomization, deterministic logspace, deterministic algorithms, randomised algorithms, Computer science, undirected Laplacian systems, randomized algorithms, logarithmic space, Approximation algorithms, UNDIRECTED S-T CONNECTIVITY, expander graphs, space complexity, computational complexity]
Deterministic Search for CNF Satisfying Assignments in Almost Polynomial Time
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We consider the fundamental derandomization problem of deterministically finding a satisfying assignment to a CNF formula that has many satisfying assignments. We give a deterministic algorithm which, given an n-variable poly(n)-clause CNF formula F that has at least &#x03B5;2n satisfying assignments, runs in time n(O&#x0303;(log log n)2) for &#x03B5; &#x2265; 1/polylog(n) and outputs a satisfying assignment of F. Prior to our work the fastest known algorithm for this problem was simply to enumerate over all seeds of a pseudorandom generator for CNFs; using the best known PRGs for CNFs [DETT10], this takes time n&#x03A9;&#x0303;(log n) even for constant &#x03B5;. Our approach is based on a new general framework relating deterministic search and deterministic approximate counting, which we believe may find further applications.
[n-variable poly(n)-clause CNF formula F, Unconditional derandomization, polynomials, computability, Search problems, Generators, deterministic search, Complexity theory, deterministic approximate counting, deterministic algorithm, deterministic algorithms, Standards, Computer science, Runtime, CNF satisfying assignments, Approximation algorithms, fundamental derandomization problem, almost polynomial time, search problems, CNF satisfiability, computational complexity]
Fooling Intersections of Low-Weight Halfspaces
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
A weight-t halfspace is a Boolean function f(x) = sign(w<sub>1</sub>x<sub>1</sub> + &#x22EF; + w<sub>n</sub>x<sub>n</sub> - &#x03B8;) where each w<sub>i</sub> is an integer in {-t, . . . , t}. We give an explicit pseudorandom generator that &#x03B4;-fools any intersection of k weight-t halfspaces with seed length poly(log n, log k, t, 1/&#x03B4;). In particular, our result gives an explicit PRG that fools any intersection of any quasipoly(n) number of halfspaces of any polylog(n) weight to any 1/polylog(n) accuracy using seed length polylog(n). Prior to this work no explicit PRG with non-trivial seed length was known even for fooling intersections of n weight-1 halfspaces to constant accuracy. The analysis of our PRG fuses techniques from two different lines of work on unconditional pseudorandomness for different kinds of Boolean functions. We extend the approach of Harsha, Klivans and Meka [HKM12] for fooling intersections of regular halfspaces, and combine this approach with results of Bazzi [Baz07] and Razborov [Raz09] on bounded independence fooling CNF formulas. Our analysis introduces new couplingbased ingredients into the standard Lindeberg method for establishing quantitative central limit theorems and associated pseudorandomness results.
[intersection of halfspaces, PRG fuses techniques, Complexity theory, regular halfspaces, quantitative central limit theorems, random number generation, Boolean functions, explicit PRG, Lindeberg method, n weight-1 halfspaces, k weight-t halfspaces, pseudorandomness, bounded independence fooling CNF formulas, nontrivial seed length, Unconditional derandomization, low-weight halfspaces, explicit pseudorandom generator, seed length poly, Generators, fooling intersections, Standards, Computer science, Approximation algorithms, Random variables, weight-t halfspace, computational complexity]
Exponentially-Hard Gap-CSP and Local PRG via Local Hardcore Functions
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
The gap-ETH assumption (Dinur 2016; Manurangsi and Raghavendra 2016) asserts that it is exponentially-hard to distinguish between a satisfiable 3-CNF formula and a 3-CNF formula which is at most 0.99-satisfiable. We show that this assumption follows from the exponential hardness of finding a satisfying assignment for smooth 3-CNFs. Here smoothness means that the number of satisfying assignments is not much smaller than the number of almost-satisfying assignments. We further show that the latter (smooth-ETH) assumption follows from the exponential hardness of solving constraint satisfaction problems over well-studied distributions, and, more generally, from the existence of any exponentially-hard locally-computable one-way function. This confirms a conjecture of Dinur (ECCC 2016). We also prove an analogous result in the cryptographic setting. Namely, we show that the existence of exponentially-hard locally-computable pseudorandom generator with linear stretch (el-PRG) follows from the existence of an exponentially-hard locally-computable almost regular one-way functions.None of the above assumptions (gap-ETH and el-PRG) was previously known to follow from the hardness of a search problem. Our results are based on a new construction of general (GL-type) hardcore functions that, for any exponentially-hard one-way function, output linearly many hardcore bits, can be locally computed, and consume only a linear amount of random bits. We also show that such hardcore functions have several other useful applications in cryptography and complexity theory.
[exponential hardness, local PRG, Tools, computability, Exponentially-hard Gap-CSP, satisfiable 3-CNF formula, Search problems, Generators, Complexity theory, random number generation, gap ETH, Computer science, local hardcore functions, constraint satisfaction problems, local cryptography, Cryptography, computational complexity, locally-computable pseudorandom generator, gap-ETH assumption]
Testing Hereditary Properties of Ordered Graphs and Matrices
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We consider properties of edge-colored vertex-ordered graphs - graphs with a totally ordered vertex set and a finite set of possible edge colors - showing that any hereditary property of such graphs is strongly testable, i.e., testable with a constant number of queries. We also explain how the proof can be adapted to show that any hereditary property of two-dimensional matrices over a finite alphabet (where row and column order is not ignored) is strongly testable. The first result generalizes the result of Alon and Shapira [FOCS'05; SICOMP'08], who showed that any hereditary graph property (without vertex order) is strongly testable. The second result answers and generalizes a conjecture of Alon, Fischer and Newman [SICOMP'07] concerning testing of matrix properties. The testability is proved by establishing a removal lemma for vertex-ordered graphs. It states that if such a graph is far enough from satisfying a certain hereditary property, then most of its induced vertex-ordered subgraphs on a certain (large enough) constant number of vertices do not satisfy the property as well. The proof bridges the gap between techniques related to the regularity lemma, used in the long chain of papers investigating graph testing, and string testing techniques. Along the way we develop a Ramsey-type lemma for multipartite graphs with &#x201C;undesirable&#x201D; edges, stating that one can find a Ramsey-type structure in such a graph, in which the density of the undesirable edges is not much higher than the density of those edges in the graph.
[row order, hereditary properties, induced vertex, Electronic mail, Complexity theory, set theory, multipartite graphs, graph colouring, removal lemma, two-dimensional matrices, possible edge colors, column order, matrix properties, Ramsey, Testing, approximation theory, Image edge detection, finite set, hereditary graph property, totally ordered vertex set, finite alphabet, property testing, matrix algebra, Computer science, ordered graphs, graph testing, hereditary property testing, edge-colored vertex-ordered graphs, Periodic structures, computational complexity]
A Characterization of Testable Hypergraph Properties
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We provide a combinatorial characterization of all testable properties of k-graphs (i.e. k-uniform hypergraphs). Here, a k-graph property P is testable if there is a randomized algorithm which makes a bounded number of edge queries and distinguishes with probability 2/3 between k-graphs that satisfy P and those that are far from satisfying P. For the 2-graph case, such a combinatorial characterization was obtained by Alon, Fischer, Newman and Shapira. Our results for the k-graph setting are in contrast to those of Austin and Tao, who showed that for the somewhat stronger concept of local repairability, the testability results for graphs do not extend to the 3-graph setting.
[k-uniform hypergraphs, regularity lemma, testable hypergraph properties, k-graph property, graph theory, probability, edge queries, Complexity theory, Electronic mail, randomized algorithm, property testing, randomised algorithms, Computer science, Systematics, testable properties, hypergraphs, combinatorial characterization, bounded number, Testing, computational complexity]
Boolean Unateness Testing with &#xd5;(n^{3/4}) Adaptive Queries
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We give an adaptive algorithm that tests whether an unknown Boolean function f : {0, 1}n &#x2192; {0, 1} is unate (i.e. every variable of f is either non-decreasing or non-increasing) or &#x03B5;-far from unate with one-sided error and O&#x0303;(n3/4/&#x03F5;2) many queries. This improves on the best adaptive O(n/&#x03F5;)-query algorithm from Baleshzar, Chakrabarty, Pallavoor, Raskhodnikova and Seshadhri [1] when 1/&#x03F5; &lt;;&lt;; 4 n1/4. Combined with the &#x03A9;&#x0303;(n)query lower bound for non-adaptive algorithms with one-sided error of [2], [3], we conclude that adaptivity helps for the testing of unateness with one-sided error. A crucial component of our algorithm is a new subroutine for finding bi-chromatic edges in the Boolean hypercube called adaptive edge search.
[Algorithm design and analysis, approximation theory, adaptive O(n/&#x03F5;)-query algorithm, &#x03A9;&#x0303;(n)query lower bound, bichromatic edges, Adaptive algorithms, nonadaptive algorithms, O&#x0303;(n3/4) adaptive queries, unateness, property testing, unknown Boolean function, Computer science, query processing, Boolean functions, one-sided error, Hypercubes, adaptive edge search, Boolean unateness testing, Testing, computational complexity, Boolean hypercube]
Generalized Uniformity Testing
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In this work, we revisit the problem of uniformity testing of discrete probability distributions. A fundamental problem in distribution testing, testing uniformity over a known domain has been addressed over a significant line of works, and is by now fully understood. The complexity of deciding whether an unknown distribution is uniform over its unknown (and arbitrary) support, however, is much less clear. Yet, this task arises as soon as no prior knowledge on the domain is available, or whenever the samples originate from an unknown and unstructured universe.In this work, we introduce and study this generalized uniformity testing question, and establish nearly tight upper and lower bound showing that - quite surprisingly - its sample complexity significantly differs from the known-domain case. Moreover, our algorithm is intrinsically adaptive, in contrast to the overwhelming majority of known distribution testing algorithms.
[Algorithm design and analysis, uniformity, Probability distribution, Complexity theory, property testing, statistical distributions, unknown universe, known distribution testing algorithms, sample complexity, Computer science, discrete probability distributions, adaptivity, distribution testing, probability distributions, unknown distribution testing algorithms, generalized uniformity testing question, unstructured universe, statistical testing, Testing, Digital TV, computational complexity]
Much Faster Algorithms for Matrix Scaling
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We develop several efficient algorithms for the classical Matrix Scaling problem, which is used in many diverse areas, from preconditioning linear systems to approximation of the permanent. On an input n&#x00D7;n matrix A, this problem asks to find diagonal (scaling) matrices X and Y (if they exist), so that XAY &#x03B5;-approximates a doubly stochastic matrix, or more generally a matrix with prescribed row and column sums. We address the general scaling problem as well as some important special cases. In particular, if A has m nonzero entries, and if there exist X and Y with polynomially large entries such that XAY is doubly stochastic, then we can solve the problem in total complexity O&#x0303;(m + n4/3). This greatly improves on the best known previous results, which were either O&#x0303;(n4) or O(mn1/2/&#x03B5;). Our algorithms are based on tailor-made first and second order techniques, combined with other recent advances in continuous optimization, which may be of independent interest for solving similar problems.
[Linear systems, matrix scaling, doubly stochastic, first-order method, Complexity theory, Optimization, Ellipsoids, Convergence, diagonal matrices, column sums, general scaling problem, classical Matrix Scaling problem, stochastic processes, iterative algorithms, approximation theory, linear systems, matrix algebra, nonzero entries, Computer science, second-order method, doubly stochastic matrix, polynomially large entries, total complexity O&#x0303;, XAY &#x03B5;, Manganese, computational complexity]
Matrix Scaling and Balancing via Box Constrained Newton's Method and Interior Point Methods
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
In this paper1, we study matrix scaling and balancing, which are fundamental problems in scientific computing, with a long line of work on them that dates back to the 1960s. We provide algorithms for both these problems that, ignoring logarithmic factors involving the dimension of the input matrix and the size of its entries, both run in time O&#x0303;(m log &#x03BA; log2(1/&#x03B5;)) where &#x03B5; is the amount of error we are willing to tolerate. Here, &#x03BA; represents the ratio between the largest and the smallest entries of the optimal scalings. This implies that our algorithms run in nearly-linear time whenever &#x03BA; is quasi-polynomial, which includes, in particular, the case of strictly positive matrices. We complement our results by providing a separate algorithm that uses an interior-point method and runs in time O&#x0303;(m3/2 log(1/&#x03B5;)). In order to establish these results, we develop a new secondorder optimization framework that enables us to treat both problems in a unified and principled manner. This framework identifies a certain generalization of linear system solving that we can use to efficiently minimize a broad class of functions, which we call second-order robust. We then show that in the context of the specific functions capturing matrix scaling and balancing, we can leverage and generalize the work on Laplacian system solving to make the algorithms obtained via this framework very efficient.
[Linear systems, interior-point method, matrix scaling, optimal scalings, matrix balancing, Optimization, optimisation, second order optimization framework, specific functions capturing matrix scaling, Matrix balancing, Robustness, Iterative methods, Newton method, logarithmic factors, SDD solver, Laplace equations, polynomials, box constrained Newton's method, nearly-linear time, matrix algebra, interior point methods, linear system, Newton's method, quasipolynomial, strictly positive matrices, computational complexity]
Simply Exponential Approximation of the Permanent of Positive Semidefinite Matrices
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We design a deterministic polynomial time cn approximation algorithm for the permanent of positive semidefinite matrices where c = &#x03B3;+1 &#x2243; 4:84. We write a natural convex relaxation and show that its optimum solution gives a cn approximation of the permanent. We further show that this factor is asymptotically tight by constructing a family of positive semidefinite matrices. We also show that our result implies an approximate version of the permanent-ontop conjecture, which was recently refuted in its original form; we show that the permanent is within a cn factor of the top eigenvalue of the Schur power matrix.
[Algorithm design and analysis, positive semidefinite matrices, Linear matrix inequalities, eigenvalues and eigenfunctions, polynomial approximation, approximation algorithm, Eigenvalues and eigenfunctions, immanant, positive semidefinite, approximation theory, Symmetric matrices, permanent-ontop, convex programming, Schur power matrix, permanent-ontop conjecture, Standards, matrix algebra, Computer science, exponential approximation, permanent, cn factor, natural convex relaxation, semidefinite program, Approximation algorithms, deterministic polynomial time cn approximation, computational complexity]
Determinant-Preserving Sparsification of SDDM Matrices with Applications to Counting and Sampling Spanning Trees
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show variants of spectral sparsification routines can preserve the total spanning tree counts of graphs, which by Kirchhoff's matrix-tree theorem, is equivalent to determinant of a graph Laplacian minor, or equivalently, of any SDDM matrix. Our analyses utilizes this combinatorial connection to bridge between statistical leverage scores/effective resistances and the analysis of random graphs by [Janson, Combinatorics, Probability and Computing `94]. This leads to a routine that in quadratic time, sparsifies a graph down to about n1.5 edges in ways that preserve both the determinant and the distribution of spanning trees (provided the sparsified graph is viewed as a random object). Extending this algorithm to work with Schur complements and approximate Cholesky factorizations leads to algorithms for counting and sampling spanning trees which are nearly optimal for dense graphs. We give an algorithm that computes a (1&#x00B1;&#x03B4;) approximation to the determinant of any SDDM matrix with constant probability in about n2&#x03B4;-2 time. This is the first routine for graphs that outperforms general-purpose routines for computing determinants of arbitrary matrices. We also give an algorithm that generates in about n2&#x03B4;-2 time a spanning tree of a weighted undirected graph from a distribution with total variation distance of &#x03B4; from the w-uniform distribution.
[dense graphs, random graphs, Kirchhoff's matrix, weighted undirected graph, total spanning tree counts, Runtime, statistical leverage scores, sparsification, Eigenvalues and eigenfunctions, constant probability, Laplacian matrix, (1&#x00B1;&#x03B4;) approximation, graph algorithms, approximation theory, random spanning trees, Laplace equations, sampling methods, SDDM matrix, Estimation, probability, trees (mathematics), sampling, Graph theory, spectral sparsification routines, sparsified graph, spectral algorithms, matrix algebra, determinant-preserving sparsification, spectral graph theory, Computer science, SDDM matrices, Approximation algorithms, computational complexity, sampling spanning trees, determinant]
Optimal Las Vegas Locality Sensitive Data Structures
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show that approximate similarity (near neighbour) search can be solved in high dimensions with performance matching state of the art (data independent) Locality Sensitive Hashing, but with a guarantee of no false negatives. Specifically we give two data structures for common problems. For c-approximate near neighbour in Hamming space, for which we get query time dn1/c+o(1) and space dn1+1/c+o(1) matching that of [Indyk and Motwani, 1998] and answering a long standing open question from [Indyk, 2000a] and [Pagh, 2016] in the affirmative. For (s<sub>1</sub>, s<sub>2</sub>)-approximate Jaccard similarity we get query time d2n&#x03C1;+o(1) and space d2n1+&#x03C1;+o(1), &#x03C1; = [log (1+s1)/(2s<sub>1</sub>)]/[log (1+s<sub>2</sub>)/(2s<sub>2</sub>)], when sets have equal size, matching the performance of [Pagh and Christiani, 2017].We use space partitions as in classic LSH, but construct these using a combination of brute force, tensoring and splitter functions a&#x0300; la [Naor et al., 1995]. We also show two dimensionality reduction lemmas with 1-sided error.
[Measurement, Force, optimal las vegas locality sensitive data structures, query time, Hamming space, performance matching state, Data structures, Search problems, long standing open question, Partitioning algorithms, dimensionality reduction lemmas, space partitions, false negatives, near neighbour, 1-sided error, Locality Sensitive Hashing, Filtering algorithms, Approximation algorithms, Jaccard similarity, data structures, data handling, computational complexity, approximate similarity]
Dynamic Minimum Spanning Forest with Subpolynomial Worst-Case Update Time
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We present a Las Vegas algorithm for dynamically maintaining a minimum spanning forest of an nnode graph undergoing edge insertions and deletions. Our algorithm guarantees an O(no(1)) worst-case update time with high probability. This significantly improves the two recent Las Vegas algorithms by Wulff-Nilsen [2] with update time O(n0.5-&#x03B5;) for some constant &#x03B5; &gt; 0 and, independently, by Nanongkai and Saranurak [3] with update time O(n0.494) (the latter works only for maintaining a spanning forest). Our result is obtained by identifying the common framework that both two previous algorithms rely on, and then improve and combine the ideas from both works. There are two main algorithmic components of the framework that are newly improved and critical for obtaining our result. First, we improve the update time from O(n0.5-&#x03B5;) in [2] to O(no(1)) for decrementally removing all low-conductance cuts in an expander undergoing edge deletions. Second, by revisiting the &#x201C;contraction technique&#x201D; by Henzinger and King [4] and Holm et al. [5], we show a new approach for maintaining a minimum spanning forest in connected graphs with very few (at most (1 + o(1))n) edges. This significantly improves the previous approach in [2], [3] which is based on Frederickson's 2-dimensional topology tree [6] and illustrates a new application to this old technique.
[update time O, Heuristic algorithms, dynamic graph algorithms, graph decomposition, probability, trees (mathematics), Las Vegas algorithms, Graph theory, Topology, minimum spanning forests, Monte Carlo methods, subpolynomial worst-case update time, edge deletions, Vegetation, nnode graph, Approximation algorithms, computational complexity, dynamic minimum spanning forest, edge insertions]
Fast and Compact Exact Distance Oracle for Planar Graphs
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
For a given a graph, a distance oracle is a data structure that answers distance queries between pairs of vertices. We introduce an O(n5/3)-space distance oracle which answers exact distance queries in O(log n) time for n-vertex planar edge-weighted digraphs. All previous distance oracles for planar graphs with truly subquadratic space (i.e., space O(n2-&#x03F5;) for some constant &#x03F5; &gt; 0) either required query time polynomial in n or could only answer approximate distance queries. Furthermore, we show how to trade-off time and space: for any S &#x2265; n3/2, we show how to obtain an S-space distance oracle that answers queries in time O( n5/2/S3/2 logn). This is a polynomial improvement over the previous planar distance oracles with o(n1/4) query time.
[approximation theory, Transmission line matrix methods, query time polynomial, Particle separators, polynomials, graph theory, planar graphs, data structure, computational geometry, Routing, Table lookup, n-vertex planar edge-weighted digraphs, O-space distance oracle, truly subquadratic space, Computer science, query processing, S-space distance oracle, exact distance queries, directed graphs, approximate distance queries, data structures, Face, answer distance queries, computational complexity]
High Dimensional Expanders Imply Agreement Expanders
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show that high dimensional expanders imply derandomized direct product tests, with a number of subsets that is linear in the size of the universe. Direct product tests belong to a family of tests called agreement tests that are important components in PCP constructions and include, for example, low degree tests such as line vs. line and plane vs. plane. For a generic hypergraph, we introduce the notion of agreement expansion, which captures the usefulness of the hypergraph for an agreement test. We show that explicit bounded degree agreement expanders exist, based on Ramanujan complexes.
[agreement tests, Buildings, graph theory, Ramanujan complex, Graph theory, Electronic mail, set theory, agreement test, explicit bounded degree agreement expanders, direct product, direct sum, Convergence, Computer science, high dimensional expanders, agreement expansion, derandomized direct product tests, PCP constructions, Eigenvalues and eigenfunctions, Skeleton, generic hypergraph, low degree tests, computational complexity, subsets]
The Ising Partition Function: Zeros and Deterministic Approximation
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We study the problem of approximating the partition function of the ferromagnetic Ising model in graphs and hypergraphs. Our first result is a deterministic approximation scheme (an FPTAS) for the partition function in bounded degree graphs that is valid over the entire range of parameters &#x03B2; (the interaction) and &#x03BB; (the external field), except for the case |&#x03BB;| = 1 (the &#x201C;zero-field&#x201D; case). A randomized algorithm (FPRAS) for all graphs, and all &#x03B2;, &#x03BB;, has long been known. Unlike most other deterministic approximation algorithms for problems in statistical physics and counting, our algorithm does not rely on the &#x201C;decay of correlations&#x201D; property. Rather, we exploit and extend machinery developed recently by Barvinok, and Patel and Regts, based on the location of the complex zeros of the partition function, which can be seen as an algorithmic realization of the classical Lee-Yang approach to phase transitions. Our approach extends to the more general setting of the Ising model on hypergraphs of bounded degree and edge size, where no previous algorithms (even randomized) were known for a wide range of parameters. In order to achieve this extension, we establish a tight version of the Lee-Yang theorem for the Ising model on hypergraphs, improving a classical result of Suzuki and Fisher.
[Correlation, graph theory, bounded degree graphs, Electronic mail, complex zeros, ferromagnetic Ising model, Zeros of polynomials, Ising partition function, Ising model, zero-field case, Approximate counting, approximation theory, Lee-Yang theorem, Computational modeling, ferromagnetism, deterministic approximation algorithms, Taylor series, Partitioning algorithms, randomized algorithm, Physics, randomised algorithms, Stability theory, hypergraphs, external field, Approximation algorithms, computational complexity]
Eldan's Stochastic Localization and the KLS Hyperplane Conjecture: An Improved Lower Bound for Expansion
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We show that the KLS constant for n-dimensional isotropic logconcave measures is O(n1/4), improving on the current best bound of O(n1/3&#x221A;log n). As corollaries we obtain the same improved bound on the thin-shell estimate, Poincare&#x0301;constant and Lipschitz concentration constant and an alternative proof of this bound for the isotropic constant; it also follows that the ball walk for sampling from an isotropic logconcave density in &#x211D;n converges in O*(n2.5) steps from a warm start.
[Density measurement, isotropic constant, KLS hyperplane conjecture, thin-shell estimate, isotropic logconcave density, KLS constant, statistical distributions, Covariance matrices, Convergence, Isoperimetry, Convex Geometry, Needles, stochastic processes, stochastic localization, n-dimensional isotropic logconcave measures, Poincar\\'e constant, Logconcave distributions, High-dimensional Sampling, random processes, Concentration, Standards, Slicing conjecture, Geometry, Computer science, improved lower bound, Cheeger constant, Lipschitz concentration constant, Poincare&#x0301; constant, computational complexity, Thin-shell conjecture]
Weak Decoupling, Polynomial Folds and Approximate Optimization over the Sphere
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
We consider the following basic problem: given an n-variate degree-d homogeneous polynomial f with real coefficients, compute a unit vector x in R&#x0302;n that maximizes abs(f(x)). Besides its fundamental nature, this problem arises in diverse contexts ranging from tensor and operator norms to graph expansion to quantum information theory. The homogeneous degree-2 case is efficiently solvable as it corresponds to computing the spectral norm of an associated matrix, but the higher degree case is NP-hard. We give approximation algorithms for this problem that offer a trade-off between the approximation ratio and running time: in n&#x0302;O(q) time, we get an approximation within factor (O(n)/q)&#x0302;(d/2-1) for arbitrary polynomials, (O(n)/q)&#x0302;(d/4-1/2) for polynomials with non-negative coefficients, and (m /q)&#x0302;(1/2) for sparse polynomials with m monomials. The approximation guarantees are with respect to the optimum of the level-q sum-of-squares (SoS) SDP relaxation of the problem (though our algorithms do not rely on actually solving the SDP). Known polynomial time algorithms for this problem rely on &#x201C;decoupling lemmas.&#x201D; Such tools are not capable of offering a trade-off like our results as they blow up the number of variables by a factor equal to the degree. We develop new decoupling tools that are more efficient in the number of variables at the expense of less structure in the output polynomials. This enables us to harness the benefits of higher level SoS relaxations. Our decoupling methods also work with &#x201C;folded polynomials,&#x201D; which are polynomials with polynomials as coefficients. This allows us to exploit easy substructures (such as quadratics) by considering them as coefficients in our algorithms. We complement our algorithmic results with some polynomially large integrality gaps for d-levels of the SoS relaxation. For general polynomials this follows from known results for random polynomials, which yield a gap of Omega(n)&#x0302;(d/4-1/2). For polynomials with non-negative coefficients, we prove an Omega(n&#x0302;(1/6) /polylogs) gap for the degree-4 case, based on a novel distribution of 4-uniform hypergraphs. We establish an n&#x0302;Omega(d) gap for general degree-d, albeit for a slightly weaker (but still very natural) relaxation. Toward this, we give a method to lift a level-4 solution matrix M to a higher level solution, under a mild technical condition on M. From a structural perspective, our work yields worst-case convergence results on the performance of the sum-of-squareshierarchy for polynomial optimization. Despite the popularity of SoS in this context, such results were previously only known for the case of q = Omega(n).
[Polynomial Optimization, degree-4 case, graph theory, sparse polynomials, approximation algorithms, Optimization, worst-case convergence results, Convergence, level-4 solution matrix, optimisation, homogeneous degree-2 case, polynomial time algorithms, higher-level SoS relaxations, polynomial optimization, random polynomials, n-variate degree, approximation theory, quantum information theory, polynomials, polynomially large integrality gaps, Tools, polynomial folds, matrix algebra, algorithmic results, Computer science, Tensile stress, homogeneous polynomial, NP-hard problem, Decoupling, Spectral Algorithms, Quantum mechanics, Approximation algorithms, folded polynomials, arbitrary polynomials, decoupling methods, nonnegative coefficients, computational complexity, Sum of Squares]
Subdeterminant Maximization via Nonconvex Relaxations and Anti-Concentration
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Several fundamental problems that arise in optimization and computer science can be cast as follows: Given vectors v<sub>1</sub>, ..., v<sub>m</sub> &#x2208; &#x211D;d and a constraint family B &#x2286; 2[m], find a set S &#x2208; B that maximizes the squared volume of the simplex spanned by the vectors in S. A motivating example is the ubiquitous data-summarization problem in machine learning and information retrieval where one is given a collection of feature vectors that represent data such as documents or images. The volume of a collection of vectors is used as a measure of their diversity, and partition or matroid constraints over [m] are imposed in order to ensure resource or fairness constraints. Even with a simple cardinality constraint (B = (<sub>r</sub>[m])), the r problem becomes NP-hard and has received much attention starting with a result by Khachiyan [1] who gave an rO(r) approximation algorithm for this problem. Recently, Nikolov and Singh [2] presented a convex program and showed how it can be used to estimate the value of the most diverse set when there are multiple cardinality constraints (i.e., when B corresponds to a partition matroid). Their proof of the integrality gap of the convex program relied on an inequality by Gurvits [3], and was recently extended to regular matroids [4], [5]. The question of whether these estimation algorithms can be converted into the more useful approximation algorithms - that also output a set - remained open. The main contribution of this paper is to give the first approximation algorithms for both partition and regular matroids. We present novel formulations for the subdeterminant maximization problem for these matroids; this reduces them to the problem of finding a point that maximizes the absolute value of a nonconvex function over a Cartesian product of probability simplices. The technical core of our results is a new anti-concentration inequality for dependent random variables that arise from these functions which allows us to relate the optimal value of these nonconvex functions to their value at a random point. Unlike prior work on the constrained subdeterminant maximization problem, our proofs do not rely on real-stability or convexity and could be of independent interest both in algorithms and complexity where anti-concentration phenomena has recently been deployed.
[Algorithm design and analysis, Anti-concentration, constrained subdeterminant maximization problem, combinatorial mathematics, graph theory, set theory, computer science, approximation algorithm, Nonconvexity, Polynomials, learning (artificial intelligence), convex program, partition matroid, anti-concentration phenomena, ubiquitous data-summarization problem, approximation theory, Estimation, Optimized production technology, probability, information retrieval, convex programming, Partitioning algorithms, machine learning, matrix algebra, Computer science, anti-concentration inequality, fundamental problems, NP-hard problem, Subdeterminant Maximization, simple cardinality constraint, multiple cardinality constraints, attention starting, nonconvex function, Approximation algorithms, subdeterminant maximization problem, computational complexity]
Hashing-Based-Estimators for Kernel Density in High Dimensions
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Given a set of points P &#x2282; &#x211D;d and a kernel k, the Kernel Density Estimate at a point x &#x2208; &#x211D;d is defined as KDE<sub>P</sub> (x) = 1/|P| &#x03A3;y<sub>&#x2208;P</sub> k(x, y). We study the problem of designing a data structure that given a data set P and a kernel function, returns approximations to the kernel density of a query point in sublinear time. We introduce a class of unbiased estimators for kernel density implemented through locality-sensitive hashing, and give general theorems bounding the variance of such estimators. These estimators give rise to efficient data structures for estimating the kernel density in high dimensions for a variety of commonly used kernels. Our work is the first to provide data-structures with theoretical guarantees that improve upon simple random sampling in high dimensions.
[Algorithm design and analysis, approximation theory, Estimation, data set, data structure, Data structures, approximations, Kernel Density, Complexity theory, kernel function, high dimensions, hashing-based-estimators, Computer science, Kernel-Matrix Vector Multiplication, Kernel Density Estimate, Locality Sensitive Hashing, Cell-probe model, Approximation algorithms, file organisation, data structures, Kernel]
[Publisher's information]
2017 IEEE 58th Annual Symposium on Foundations of Computer Science
None
2017
Provides a listing of current committee members and society officers.
[]
