1047
Transaction management in a distributed database system for local area networks
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The design and implementation of an experimental fault-tolerant distributed database management system is described. The system provides a logically integrated view of data with distribution transparency and a controlled data replication. A commitment protocol used to guarantee atomicity of update operations is discussed. Efficient algorithms used to recover a site from a failure and restore data consistency are described. Recovery can be interleaved with the processing of regular database transactions and does not seriously limit the availability of data. The proposed solutions to the problems of fault recovery are designed to take advantage of the properties of a high-bandwidth local area network.<<ETX>>
[distribution transparency, transaction management, implementation, Concurrency control, local area networks, Transaction databases, Delay, data consistency, fault recovery, Intelligent networks, commitment protocol, Distributed databases, Prototypes, controlled data replication, Bandwidth, distributed databases, design, Database systems, fault tolerant computing, Computer network management, fault tolerant system, Local area networks, distributed database system]
Self-stabilization of the alternating-bit protocol
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
The alternating-bit protocol is a fundamental protocol for transmitting data across an unreliable transmission medium. The reliability of the protocol depends on its initial state. The authors present a self-stabilizing version of the alternating-bit protocol, i.e. the system converges to a state that guarantees reliable data transmission regardless of its initial state. Applications of the protocol and possible extensions are discussed.<<ETX>>
[self-stabilizing version, data communication systems, computer networks, Access protocols, reliability, Computer crashes, alternating-bit protocol, Delay, Convergence, Computer science, Transmitters, Automata, Safety, unreliable transmission medium, Data communication, protocols]
Reliable broadcast for fault-tolerance on local computer networks
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
The authors discuss the definition and design of a generic reliable communication architecture on a widely used host-independent platform, such as a local area network (LAN). Two relevant aspects are the use of nonreplicated LANs and self-checking components. The protocol is innovative, in the sense that, although clockless and running on a nonreplicated network, it displays bounded execution times. Thus the architecture is capable of reliably addressing realtime. Support of high-performance real-time applications with this architecture is being seriously considered in the present phase of project Delta-4, a CED Esprit II consortium designing an open, dependable distributed architecture. The authors' considerations regarding synchronism properties of clockless protocols are being applied in this context.<<ETX>>
[Protocols, fault-tolerance, Displays, local area networks, distributed architecture, Fault tolerance, protocol, Computer network reliability, Computer architecture, Broadcasting, protocols, self-checking components, Local area networks, generic reliable communication architecture, synchronism properties, project Delta-4, Synchronization, high-performance real-time applications, local area network, fault tolerant computing, reliable broadcast, Telecommunication network reliability, CED Esprit II consortium, local computer networks, Clocks]
A timestamp-based checkpointing protocol for long-lived distributed computations
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The authors present a timestamp-based protocol for checkpointing the global state of a long-lived distributed computation in an environment in which processor clocks are approximately synchronized. The protocol is based on periodic checkpointing of local process states and logging of incoming messages during a short bounded interval. It tolerates process crash and performance failures as well as network omission and performance failures. The proposed approach has the advantage of optimistic logging protocols in that it does not require synchronous logging of each message on stable storage. The approach also has the advantage of pessimistic logging protocols in that it avoids the domino effect by recovering to the most recent successful local checkpoint.<<ETX>>
[Checkpointing, timestamp-based checkpointing protocol, Protocols, process crash, domino effect, optimistic logging protocols, performance evaluation, processor clocks, Computer crashes, Synchronization, Distributed computing, Degradation, performance failures, Communication channels, System recovery, Hardware, long-lived distributed computations, protocols, Clocks]
A state machine approach to reliable distributed systems
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
In many distributed applications, processes synchronize with one another in a complex way and execute for a long period of time. Atomic transactions are inadequate for designing reliable applications with these characteristics, because transactions restrict the types of synchronization than can be specified. An alternative approach that exploits behavior specified in a hierarchical finite-state machine (FSM) model is proposed. A set of general conditions that ensures the correctness of recovery is identified. These general conditions permit combinations of different types of recovery methods to be used in a recovery. They also enable one to enhance recovery efficiency by exploiting permutation and substitution of operations allowed by the behavior specification. It is shown that existing recovery techniques, including those that exploit application semantics, satisfy these conditions for correctness of recovery.<<ETX>>
[software reliability, Materials handling, distributed processing, Concurrency control, recovery, Application software, finite state machines, Distributed computing, Robotics and automation, system recovery, behavior specification, reliable distributed systems, Software design, Robot kinematics, hierarchical finite-state machine, fault tolerant, Automatic control, synchronization, fault tolerant computing, Workstations, Manufacturing systems]
Some remarks on protecting weak keys and poorly-chosen secrets from guessing attacks
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
Authentication and key distribution protocols that utilize weak secrets (such as passwords and personal identification numbers) are traditionally susceptible to guessing attacks whereby an adversary iterates through a relatively small key space and verifies the correct guess. Such attacks can be defeated by the use of public key encryption and careful protocol construction. T. Lomas et al. (Proc. of ACM Symp. on Operating Syst. Principles, 1989) investigated this topic and developed a methodology for avoiding guessing attacks while incurring only moderate overhead. Several issues concerning the proposed solution are discussed here, and modifications that remove some of the constraints (such as synchronized time and state retention by the server) and result in simpler and more efficient protocols are suggested.<<ETX>>
[poorly-chosen secrets, Laboratories, public key encryption, access protocols, weak secrets, Cryptographic protocols, Computer science, state retention, security of data, Operating systems, public key cryptography, key distribution protocols, passwords, personal identification numbers, Authentication, Public key, synchronized time, authorisation, Pins, Cryptography, guessing attacks, Protection, protocol construction, Clocks]
Processing of read-only queries at a remote backup
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Remote backup systems are often used to provide high data availability. Updates are typically propagated to the backup via a log, which decouples the backup from the primary. We show that this decoupling can lead to efficient installation of updates in batches and efficient processing of read-only queries, by eliminating or reducing access conflicts between updates and queries. We present several methods for query processing at the backup and evaluate their performance analytically.<<ETX>>
[performance evaluation, Throughput, Transaction databases, Data mining, Application software, Computer science, query processing, Analytical models, performance, Query processing, remote backup, distributed databases, data availability, read-only queries]
On the design of systems of cooperating functional processes
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
This paper describes a design concept for systems of cooperating distributed processes based on a variant of coloured Petri-nets. It cleanly separates graphical specification of processes and their interaction (or communication) from the algorithmic specifications of the computations that need to be performed by the individual processes. Designing complex process systems is aided by abstractions similar to those that are available in programming languages. In conjunction with a small set of well-defined interaction schemes for process communication it ensures well-behaving systems largely by construction. Essential invariance properties of small subsystems which in incremental steps may either be verified by formal methods or validated by simulation are not corrupted when embedding them in the context of larger systems. The paper focuses particularly on the construction of large systems by recursive abstractions of small net templates which, at execution time, may be recursively expanded to distribute application problems evenly over several processing sites for concurrent processing.
[Process design, recursive abstractions, process communication, Stability, coloured Petri-nets, Petri nets, net templates, distributed processing, algorithmic specifications, graphical specification, Distributed computing, cooperating distributed processes, Computer science, Concurrent computing, concurrent processing, Computer languages, Analytical models, formal verification, complex process systems, execution time, System recovery, cooperating functional processes, Context modeling, Formal verification]
Locating more corruptions in a replicated file
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
When a data file is replicated at more than one site, we are interested in detecting corruption by comparing the multiple copies. In order to reduce the amount of messaging for large files, techniques based on page signatures and combined signatures have been explored. However, for 3 or more sites, the known methods assume that the number of corrupted page copies to be at most [M/2]-1, where M is the number of sites. We point out that this assumption is unrealistic and the corresponding methods are unnecessarily pessimistic. In this paper, we replace this assumption by another assumption which we show to be reasonable. Based on this assumption, we derived a distributed algorithm which in general achieves better performance than previously known results. Our system model is also more refined than previous work.
[file corruption, replicated databases, Data engineering, Mathematics, combined signatures, page signatures, Centralized control, Computer science, replicated file, distributed algorithm, Distributed control, corruption detection, Computer networks, Performance analysis, Samarium, Distributed algorithms]
Reliability analysis of disk array organizations by considering uncorrectable bit errors
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
We present an analytic model to study the reliability of some important disk array organizations that have been proposed by others in the literature. These organizations are based on the combination of two options for the data layout, regular RAID-5 and block designs, and three alternatives for sparing: hot sparing, distributed sparing and parity sparing. Uncorrectable bit errors have big effects on reliability but are ignored in traditional reliability analysis of disk arrays. We consider both disk failures and uncorrectable bit errors in the model. The reliability of disk arrays is measured in terms of MTTDL (Mean Time To Data Loss). A unified formula of MTTDL has been derived for these disk array organizations. The MTTDLs of these disk array organizations are also compared using the analytic model.
[data layout, hot sparing, Loss measurement, reliability theory, Information analysis, Degradation, storage management, parity sparing, System performance, Cities and towns, block designs, analytic model, disk failures, magnetic disc storage, Time measurement, Computer crashes, disk array organizations, MTTDL, distributed sparing, reliability analysis, uncorrectable bit errors, regular RAID-5, fault tolerant computing, Mean Time To Data Loss, Error correction codes, Error correction, errors]
Managing network security-a pragmatic approach
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
An efficient management is needed to make use of different security mechanisms in large networks. All mechanisms have to be configured consistently according to the security policy. To reduce complexity, the administrator should not have to cope with details not important for him. In the presented concept the network is divided into several administrative domains which are managed rather independently from each other. Each domain maintains its own network access control policy (NAP). The enterprise-wide policy is a combination of all NAPs. It is enforced by different security mechanisms and configuration can be derived from the global access policy automatically. Existing security mechanisms can be integrated by simply adding a policy transformation unit.
[Access control, telecommunication security, wide area networks, global access policy, Data security, policy transformation unit, Humans, Data processing, Information management, large networks, Filters, computer network management, security of data, enterprise-wide policy, Information security, Collaborative work, Workstations, network security management, security policy, Face, network access control policy]
Enforcing determinism for the consistent replication of multithreaded CORBA applications
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
In CORBA-based applications that depend on object replication for fault tolerance, inconsistencies in the states of the replicas of an object can arise when concurrent threads within those replicas perform updates in different orders. By imposing a single logical thread of control on every replicated multithreaded CORBA client or server object, and by providing deterministic scheduling of threads and operations a cross the replicas of each object, the Eternal system achieves consistent object replication. The Eternal system does this transparently, with no modification to the application, the ORB, or the concurrency model employed by the ORB.
[multi-threading, replicated databases, fault tolerance, object replication, File servers, Control systems, consistent replication, Application software, Proposals, Yarn, Programming profession, multithreaded CORBA applications, Concurrent computing, Fault tolerance, Multithreading, concurrency model, Fault tolerant systems, concurrency control, fault tolerant computing, distributed object management]
Implementing a reflective fault-tolerant CORBA system
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
The use of reflection is becoming popular today for the implementation of non-functional mechanisms such as fault tolerance. The main benefits of reflection are separation of concerns between the application and the mechanisms and transparency from the application programmer point of view. Unfortunately, metaobject protocols (MOPs) available today are not satisfactory with respect to necessary features needed for implementing fault tolerance mechanisms. Previously, we proposed a specialised MOP based on Corba, well adapted for such mechanisms (M.-O. Killijian and J.C. Fabre, 1998). We deliberately focus on the implementation of this metaobject protocol using compile-time reflection and its use for implementing distributed fault tolerance. We present the design and the implementation of a fault-tolerant Corba system using this metaobject together with some preliminary experimental results. From the lessons learnt from this work, we briefly address the benefits of reflection in other layers of a system for dependability issues.
[reflective fault-tolerant CORBA system, application programmer, Control systems, Machinery production industries, fault tolerance mechanisms, program compilers, Fault tolerance, Runtime, Fault tolerant systems, metaobject protocols, protocols, compile-time reflection, distributed object management, dependability issues, Java, MOPs, Access protocols, Reflection, Mechanical factors, Programming profession, non-functional mechanisms, fault tolerant computing, separation of concerns, distributed fault tolerance, specialised MOP]
Efficient recovery information management schemes for the fault tolerant mobile computing systems
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
This paper presents region-based storage management schemes, which support the efficient implementation of checkpointing and message logging for fault tolerant mobile computing systems. In the proposed schemes, a recovery manager assigned for a group of cells takes care of the recovery for the mobile hosts within the region. As a result, the recovery information of a mobile host, which may be dispersed over the network due to the mobility of the host, can efficiently be handled.
[Checkpointing, checkpointing, Costs, Mobile communication, Information management, Environmental management, system recovery, Space stations, Computer science, message logging, storage management, mobile computing, region-based storage management, protocol, recovery information management, Engineering management, Fault tolerant systems, fault tolerant computing, protocols, Mobile computing]
Broadcasting messages in fault-tolerant distributed systems: the benefit of handling input-triggered and output-triggered suspicions differently
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This paper investigates the two main and seemingly antagonistic approaches to broadcasting messages reliably in fault-tolerant distributed systems: the approach based on reliable broadcast, and that based on view synchronous communication (or VSC for short). While VSC does more than reliable broadcast, this has a cost. We show that this cost can be reduced by exploiting the difference between input-triggered and output-triggered suspicions, and by replacing the standard VSC broadcast primitive by two broadcast primitives, one sensitive to input-triggered suspicions, and the other sensitive to output-triggered suspicions.
[Context, output-triggered suspicions, Reliability theory, distributed processing, Computer crashes, view synchronous communication, broadcasting, input-triggered suspicions, broadcast primitives, Fault tolerant systems, Broadcasting, fault tolerant computing, reliable message broadcasting, reliable broadcast, fault-tolerant distributed systems]
Transparent runtime randomization for security
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
A large class of security attacks exploit software implementation vulnerabilities such as unchecked buffers. This paper proposes transparent runtime randomization (TRR), a generalized approach for protecting against a wide range of security attacks. TRR dynamically and randomly relocates a program's stack, heap, shared libraries, and parts of its runtime control data structures inside the application memory address space. Making a program's memory layout different each time it runs foils the attacker's assumptions about the memory layout of the vulnerable program and makes the determination of critical address values difficult if not impossible. TRR is implemented by changing the Linux dynamic program loader, hence it is transparent to applications. We demonstrate that TRR is effective in defeating real security attacks, including malloc-based heap overflow, integer overflow, and double-free attacks, for which effective prevention mechanisms are yet to emerge. Furthermore, TRR incurs less than 9% program startup overhead and no runtime overhead.
[Unix, security attacks, Data security, transparent runtime randomization, safety-critical software, Invasive software, Computer crime, Runtime, Operating systems, Linux, attack prevention, Computer bugs, application memory address space, TRR, Information security, Buffer overflow, software implementation vulnerabilities, dynamic program loader, Protection]
Model-based validation of an intrusion-tolerant information system
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
An increasing number of computer systems are designed to be distributed across both local and wide-area networks, performing a multitude of critical information-sharing and computational tasks. Malicious attacks on such systems are a growing concern, where attackers typically seek to degrade quality of service by intrusions that exploit vulnerabilities in networks, operating systems, and application software. Accordingly, designers are seeking improved techniques for validating such systems with respect to specified survivability requirements. In this regard, we describe a model-based validation effort that was undertaken as part of a unified approach to validating a networked intrusion-tolerant information system. Model-based results were used to guide the system's design as well as to determine whether a given survivability requirement was satisfied.
[application software, malicious attacks, survivability requirement, intrusion tolerant information system, Quality of service, Distributed computing, Information systems, Degradation, Software design, formal verification, Operating systems, Computer networks, information systems, model-based validation, operating system, computer networks, quality of service, Application software, networked intrusion-tolerant information system, security of data, critical information-sharing, critical computational task, Information security, computer systems, local area network, Software systems, fault tolerant computing, wide area network]
Self-* distributed query region covering in sensor networks
24th IEEE Symposium on Reliable Distributed Systems
None
2005
In this paper, we design self-* novel solutions to the minimal connected sensor cover problem. The concept of self-* is used to include fault-tolerant properties like self-configuring, self-reconfiguring/self-healing, etc. We present two self-stabilizing, fully distributed, strictly localized, and scalable solutions, and show that these solutions are both self-configuring and self-healing. The proposed solutions are space optimal in terms of the number of states used per node. Another feature of the proposed algorithms is that the faults are contained only within the neighborhood of the faulty nodes. This paper also includes a comparison of the performance of the two proposed solutions in terms of the stabilization time, cover size metrics, and ability to cope with transient and permanent faults.
[Pervasive computing, Computer vision, Military computing, wireless sensor networks, self-configuring property, minimal connected sensor cover problem, self-healing property, Ubiquitous computing, self-* distributed query region covering problem, Batteries, fault-tolerant property, Computer science, Intelligent networks, Wireless sensor networks, Fault tolerance, sensor network, distributed algorithms, fault tolerant computing, Personal digital assistants]
Reliably Executing Tasks in the Presence of Untrusted Entities
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
In this work we consider a distributed system formed by a master processor and a collection of n processors (workers) that can execute tasks; worker processors are untrusted and might act maliciously. The master assigns tasks to workers to be executed. Each task returns a binary value, and we want the master to accept only correct values with high probability. Furthermore, we assume that the service provided by the workers is not free; for each task that a worker is assigned, the master is charged with a work-unit. Therefore, considering a single task assigned to several workers, our goal is to have the master computer to accept the correct value of the task with high probability, with the smallest possible amount of work (number of workers the master assigns the task). We explore two ways of bounding the number of faulty processors: (a) we consider a fixed bound f &lt; n/2 on the maximum number of workers that may fail, and (b) a probability p &lt; 1/2 of any processor to be faulty (all processors are faulty with probability p, independently of the rest of processors). Our work demonstrates that it is possible to obtain high probability of correct acceptance with low work. In particular, by considering both mechanisms of bounding the number of malicious workers, we first show lower bounds on the minimum amount of (expected) work required, so that any algorithm accepts the correct value with probability of success 1 - epsiv, where epsiv Lt 1 (e.g., 1/n). Then we develop and analyze two algorithms, each using a different decision strategy, and show that both algorithms obtain the same probability of success 1 - epsiv, and in doing so, they require similar upper bounds on the (expected) work. Furthermore, under certain conditions, these upper bounds are asymptotically optimal with respect to our lower bounds
[Algorithm design and analysis, malicious workers, probability, distributed processing, distributed system, faulty processors, Distributed computing, Machine intelligence, reliable task execution, Computer science, decision strategy, Upper bound, security of data, Contracts, Computational intelligence, untrusted entities]
Building Trust in Storage Outsourcing: Secure Accounting of Utility Storage
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
We are witnessing a revival of storage service providers in the form of new vendors as well as traditional players. While storage outsourcing is cost-effective, many companies are hesitating to outsource their storage due to security concerns. The success of storage outsourcing is highly dependent on how well the providers can establish trust with their consumers. While significant work has been done to ensure confidentiality, integrity, and availability of data, a practical solution for accounting of outsourced storage is still at large missing. This paper presents Saksha, a secure accounting system that enables automated and verifiable metering of the resources utilized by the consumers. A provider that includes Saksha as a part of its storage service can prove to its customers the amount of resources utilized by them. As a result, Saksha will help to enhance trust by preventing any inflation or deflation of the service usage. Saksha is not restricted to any particular pricing model; it can be applied to the popular pay-per-use pricing model for utility storage as well as many of its variants. In addition, it can be used by the consumers to periodically evaluate their usage and reassess their outsourcing requirements. Saksha is developed such that it can be layered on the top of networked file systems. Our performance results demonstrate that Saksha is efficient and can be used in practice.
[Availability, trust, accounts data processing, networked file systems, Data security, Saksha, Space power stations, Financial management, pricing model, storage service providers, Secure storage, Computer science, storage management, File systems, security of data, outsourcing, Pricing, Bandwidth, storage outsourcing, Outsourcing, accounting system, pricing, utility storage]
Mitigating Distributed Denial of Service Attacks in Multiparty Applications in the Presence of Clock Drifts
2008 Symposium on Reliable Distributed Systems
None
2008
A weak point in network-based applications is that they commonly open some known communication port(s), making themselves targets for denial of service (DoS) attacks. Considering adversaries that can eavesdrop and launch directed DoS attacks to the applications' open ports, solutions based on pseudo-random port-hopping have been suggested. As port-hopping needs that the communicating parties hop in a synchronized manner, these solutions suggest acknowledgment-based protocols between a client-server pair or assume the presence of synchronized clocks. Acknowledgments, if lost, can cause a port to be open for a longer time and thus be vulnerable to DoS attacks; Time servers for synchronizing clocks can become targets to DoS attack themselves. Here we study the case where the communicating parties have clocks with rate drift, which is common in networking. We propose an algorithm, BigWheel, for servers to communicate with multiple clients in a port-hopping manner, thus enabling support to multi-party applications as well. The algorithm does not rely on the server having a fixed port open in the beginning, neither does it require from the client to get a "first-contact" port from a third party. We also present an adaptive algorithm, HoPerAA, for hopping in the presence of clock-drift, as well as the analysis and evaluation of the methods. The solutions are simple, based on each client interacting with the server independently of the other clients, without the need of acknowledgments or time server. Provided that one has an estimation of the time it takes for the adversary to detect that a port is open and launch an attack, the method we propose doesnot make it possible to the eavesdropping adversary to launch an attack directed to the application's open port(s).
[client-server synchronizing clocks, clock drifts, BigWheel, telecommunication services, Information filtering, Application software, Synchronization, Floods, distributed denial of service attacks, Computer crime, acknowledgment-based protocols, clocks, Network servers, security of data, Computer network reliability, time server, HoPerAA, DoS attacks, Information filters, Internet, protocols, Protection, pseudo-random port-hopping, Clocks]
Resource-Aware Migratory Services in Wide-Area Shared Computing Environments
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
In this paper we present the design and evaluation of a system for deploying highly available and migratable services in shared infrastructures, such as the PlanetLab, where the available resource capacities at a node can fluctuate significantly. A migratable service can monitor its operating conditions and autonomously relocate itself to another node when the available resource capacities at the current node fall below certain acceptable limits. We utilize the autonomous mobile agent paradigm for building such migratable services. Such agents can monitor their operating conditions and follow various migration policies. We investigate here the mechanisms for service relocation, and client-side protocols to access migratory services. The "blackout periods'', i.e. the time during which the clients are unable to access a migrating service, need to be minimized and kept within some tolerable limits for services required to be highly available. We first present the design of a migratable service implemented using a mobile agent, and evaluate its performance in terms of the blackout periods and the service agent's abilities to autonomously migrate in the network. We replicate service agents to reduce the blackout periods, and develop the coordination protocols for autonomous agent migration in a group of service agents. We also present here our work for monitoring PlanetLab nodes for their available resource capacities in order to assist a migratory service in selecting a target node for relocation.
[blackout period, wide area networks, PlanetLab node monitoring, service relocation, Distributed computing, Service Replication, client-side coordination protocol, Condition monitoring, Mobile Agents, resource allocation, USA Councils, Mobile agents, mobile agents, Robustness, Autonomous agents, computer network reliability, protocols, Availability, client-server systems, fault tolerance, Buildings, Access protocols, autonomous mobile agent paradigm, performance evaluation, Resilient Services, Computer science, wide-area network shared computing environment, resource-aware migratory service design, Service Migration, Service Availability, fault tolerant computing]
A Resource-Efficient Adaptive Caching Scheme for Mobile Ad Hoc Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
In a Mobile Ad-Hoc Network environment, wireless devices have finite resources such as memory, energy and they work within the wireless constraints such as limited bandwidth and unreliable communication. Therefore, storage space, bandwidth, and battery life must be managed effectively in order to extend the usefulness and lifespan of wireless devices and the network. Caching is one of those techniques which reduce the latency and tuning time for mobile devices in the wireless network. We propose a novel scheme that seeks to distribute the storage, bandwidth and energy burden through a resource efficient adaptive caching scheme for mobile ad-hoc networks. Our performance results show that our scheme reduces both response time and bandwidth utilization by, 36%, through a reduction in hop count, as well as both a 79% increase in energy efficiency and a 53% reduction in storage utilization when compared with a leading alternate methodology.
[Availability, mobile radio, resource-efficient adaptive caching scheme, Mobile communication, Ad hoc networks, cache storage, Wireless communication, mobile ad-hoc networks, wireless devices, storage utilization, Bandwidth, energy efficiency, Book reviews, bandwidth utilization, ad hoc networks, wireless network, Mobile computing]
Active Replication at (Almost) No Cost
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
MapReduce has become a popular programming paradigm in the domain of batch processing systems. Its simplicity allows applications to be highly scalable and to be easily deployed on large clusters. More recently, the MapReduce approach has been also applied to Event Stream Processing (ESP) systems. This approach, which we call StreamMapReduce, enabled many novel applications that require both scalability and low latency. Another recent trend is to move distributed applications to public clouds such as Amazon EC2 rather than running and maintaining private data centers. Most cloud providers charge their customers on an hourly basis rather than on CPU cycles consumed. However, many applications, especially those that process online data, need to limit their CPU utilization to conservative levels (often as low as 50%) to be able to accommodate natural and sudden load variations without causing unacceptable deterioration in responsiveness. In this paper, we present a new fault tolerance approach based on active replication for StreamMapReduce systems. This approach is cost effective for cloud consumers as well as cloud providers. Cost effectiveness is achieved by fully utilizing the acquired computational resources without performance degradation and by reducing the need for additional nodes dedicated to fault tolerance.
[Cloud computing, event stream processing, fault tolerance, Peer to peer computing, cloud consumers, distributed processing, active replication, Synchronization, StreamMapReduce, MapReduce, fault tolerance approach, Fault tolerance, Databases, mapreduce, Fault tolerant systems, energy efficiency, batch processing systems, Load management, fault tolerant computing, public clouds, cloud computing]
LogMaster: Mining Event Correlations in Logs of Large-Scale Cluster Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
This paper presents a set of innovative algorithms and a system, named Log Master, for mining correlations of events that have multiple attributions, i.e., node ID, application ID, event type, and event severity, in logs of large-scale cloud and HPC systems. Different from traditional transactional data, e.g., supermarket purchases, system logs have their unique characteristics, and hence we propose several innovative approaches to mining their correlations. We parse logs into an n-ary sequence where each event is identified by an informative nine-tuple. We propose a set of enhanced apriori-like algorithms for improving sequence mining efficiency, we propose an innovative abstraction-event correlation graphs (ECGs) to represent event correlations, and present an ECGs-based algorithm for fast predicting events. The experimental results on three logs of production cloud and HPC systems, varying from 433490 entries to 4747963 entries, show that our method can predict failures with a high precision and an acceptable recall rates.
[Algorithm design and analysis, Correlation, innovative algorithms, data mining, reliability, Data mining, transactional data, parallel processing, ECGs-based algorithm, Clustering algorithms, Electrocardiography, Prediction algorithms, large-scale cluster system logs, cloud computing, failure prediction, event correlation graphs, LogMaster, correlations mining, large-scale cloud systems, mining event correlations, correlation mining, large-scale systems, HPC systems, Timing, sequence mining]
Automated Multi-graceful Degradation: A Case Study
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
We focus on the problem of multi-graceful degradation. In multi-graceful degradation, the system provides successively reduced guarantees in the presence of increasingly severe faults. We present an automated technique for generation of a multi-graceful-degraded program from its original fault-intolerant/ideal version. In this algorithm, we begin with (1) an ideal program that satisfies all its specification in the absence of faults, (2) a set of faults that need to be tolerated and (3) reduced requirements in their presence. We subsequently generate several gracefullly degrading programs that only satisfy the reduced requirements. This step also identifies new states to which program needs to recover to satisfy the reduced specification. Subsequently, we utilize the original input program and the generated programs that ensures that (1) in the absence of faults, the entire specification is satisfied and (2) in the presence of faults, the program recovers to states from where the corresponding reduced specification is satisfied. We illustrate our technique with a case study of a system in the fuelcell lab of the Ohio Coal Research Center (OCRC). In this system, it is important to satisfy safety of lab personnel as well as safety of people in the building in which it is located. Moreover, in case of device failures, it is necessary to provide weaker guarantees that capture the best possible protection. In our example, we begin with an ideal model for this system and successively add multi-graceful degradation to obtain the same program (with some abstractions) as the one that was designed manually for this system.
[multigraceful-degraded program, automated multigraceful degradation, requirement reduction, device failure, formal specification, system recovery, fault-intolerant program, Degradation, Fault tolerance, Fault tolerant systems, severe fault, safety, fuelcell lab, program state identification, Polynomials, Safety, Model Repair, OCRC, fault tolerance, lab personnel, specification satisfaction, Graceful Degradation, program recovery, reduced guarantee, Formal Methods, System recovery, Ohio Coal Research Center, Ventilation, fault tolerant computing, Fault-tolerance, ideal program]
Make the Leader Work: Executive Deferred Update Replication
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
In this paper we propose executive deferred update replication (EDUR), a novel algorithm for multi-primary replication of transactional memory and databases. EDUR streamlines transaction certification (i.e., checking for conflicts between concurrent transactions) with the broadcast protocol, which improves overall performance and scalability compared to deferred update replication based on total order broadcast (TOB). EDUR uses executive order broadcast (EOB), a novel protocol that can be seen as a generalization of TOB. Compared to TOB, EOB features new primitives and properties that enable the application to delegate some work to a leader -- a process inherently present in many TOB algorithms that is responsible for coordination of message dissemination. The results of experimental evaluation show significant performance gains when using our approach.
[Algorithm design and analysis, transaction processing, Protocols, Scalability, Transforms, History, conflicts checking, broadcast protocol, executive order broadcast, Semantics, EOB, executive deferred update replication, protocols, EDUR, concurrency (computers), total order broadcast, databases, replicated databases, transaction certification, TOB, deferred update replication, Computer crashes, transactional memory, distributed transactional memory, multiprimary replication, concurrent transactions]
PSG-Codes: An Erasure Codes Family with High Fault Tolerance and Fast Recovery
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
As hard disk failure rates are rarely improved and the reconstruction time for TB-level disks typically amounts to days, multiple concurrent disk/storage node failures in datacenter storage systems become common and frequent. As a result, the erasure coding schemes used in datacenters must meet the critical requirements of high fault tolerance, high storage efficiency, and fast fault recovery. In this paper, we introduce a new XOR-based non-MDS erasure code family with an ability of tolerating up to 12-disk/node failures, called PSG-Codes. The basic idea behind PSG-Codes is to partition disks into groups, and exploit short parity chains to generate parity units. Then, the parity chain is further shortened by varying the number of parity elements for each strip. We conduct a simulation-based study to search configuration parameter space of PSG-Codes, and prove that PSG-Codes can tolerate up to 12 disk/node failures. Compared with a well-known XOR-based non-MDS code, WEAVER codes, PSG-Codes have higher storage efficiency and lower reconstruction cost. Moreover, the storage efficiency and performance of PSG-Codes are also competitive with another stat-of-the-art GF-based non-MDS codes, LRC codes.
[hard disk failure rates, Strips, fault tolerance, Storage systems, simulation-based study, reliability, parity elements, Encoding, XOR-based nonMDS erasure code family, Complexity theory, disk-node failures, fast recovery, hard discs, Fault tolerance, parity chain, PSG-codes, Fault tolerant systems, erasure codes family, erasure codes, Acceleration]
Norton Zone: Symantec's Secure Cloud Storage System
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Cloud storage services are the way of the future, if not the present, but broad adoption is limited by a stark trade-off between privacy and functionality. Many popular cloud services provide search capabilities, but make only nominal efforts to keep user data fully private. Alternatives that search private user data on an untrusted server sacrifice functionality and/or scalability. We describe Norton Zone, Symantec's secure and scalable public storage system based on our valet security model. Whereas most commercial cloud storage systems secure user data with access control and legal mechanisms, Zone's cryptographic techniques provide proven privacy guarantees. This gives users an extra layer of security without compromising functionality. Zone's performance is comparable to unencrypted cloud storage systems that support search and sharing. We report on the design of Zone and the lessons learned in developing and deploying it in commercial, distributed datacenters scalable to millions of users.
[Cloud computing, valet security model, Scalability, private user data, cryptography, Symantec secure cloud storage system, Encryption, cryptographic techniques, Servers, computer centres, Symantec scalable public storage system, Secure storage, storage management, Norton Zone, authorisation, distributed datacenters, data privacy, access control, cloud computing]
On Availability for Blockchain-Based Systems
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Blockchain has recently gained momentum. Startups, enterprises, banks, and government agencies around the world are exploring the use of blockchain for broad applications including public registries, supply chains, health records, and voting. Dependability properties, like availability, are critical for many of these applications, but the guarantees offered by the blockchain technology remain unclear, especially from an application perspective. In this paper, we identify the availability limitations of two mainstream blockchains, Ethereum and Bitcoin. We demonstrate that while read availability of blockchains is typically high, write availability - for transaction management - is actually low. For Ethereum, we collected 6 million transactions over a period of 97 days. First, we measured the time for transactions to commit as required by the applications. Second, we observed that some transactions never commit, due to the inherent blockchain design. Third and perhaps even more dramatically, we identify the consequences of the lack of built-in options for explicit abort or retry that can maintain the application in an uncertain state, where transactions remain pending (neither aborted nor committed) for an unknown duration. Finally we propose techniques to mitigate the availability limitations of existing blockchains, and experimentally test the efficacy of these techniques.
[transaction processing, Bitcoin, transaction management, read availability, inherent blockchain design, electronic money, blockchain-based systems, Ethereum, Software, write availability, Delays, financial data processing, Australia, blockchain technology, mainstream blockchains, Contracts]
Recovery in the Clouds kernel
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The Clouds kernel is a native-layer distributed kernel supporting the Clouds operating system. Clouds provides atomic actions to support reliable computation. The data-recovery mechanism supporting atomic actions in the Clouds kernel is the responsibility of a component called the storage manager. This recovery mechanism uses a pessimistic shadowing technique and is designed to be an efficient, low-level facility. Recovery is completely separate from the synchronization support for atomic actions. Clouds recovery is closely integrated with the virtual memory management of the system, making for an effective recovery system. The algorithms that control the update of objects are straightforward enough that a fairly rigorous proof of correctness is possible.<<ETX>>
[native-layer distributed kernel, atomic actions, Clouds kernel, operating system, distributed processing, data-recovery mechanism, Application software, virtual memory management, Shadow mapping, Computer science, Operating systems, Memory management, Fault tolerant systems, storage manager, operating systems (computers), Kernel]
A realistic evaluation of optimistic dynamic voting
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
When data are replicated an access protocol must be chosen to ensure the presentation of a consistent view of the data. Protocols based on quorum consensus provide good availability with the added benefit of mutual exclusion. Of the protocols based on quorum consensus, the dynamic voting protocols provide the highest known availability. A dynamic voting protocol that does not need the instantaneous state information required by the original dynamic voting proposal is described. It provides the same performance as the original dynamic voting in the asymptotic case and quickly converges to it for realistic access rates, at a cost in network traffic similar so that of static majority consensus voting. The availability afforded by dynamic voting protocols is analyzed, taking the access frequency into account. The analysis confirms the hypothesis that delaying state information does not appreciably affect availability. Discrete event simulation is used to confirm and to extend the analytical results.<<ETX>>
[Availability, static majority consensus voting, Costs, quorum consensus, Access protocols, Telecommunication traffic, access protocol, Proposals, Delay, Information analysis, optimistic dynamic voting, Voting, data replication, Traffic control, Frequency, protocols, discrete event simulation]
A fault-tolerant protocol for atomic broadcast
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
A novel general protocol for atomic broadcast in networks is presented. The protocol tolerates loss, duplication, reordering, delay of messages, and network partitioning in an arbitrary network of 'fail-stop' sites (i.e. no Byzantine site behavior is tolerated). The protocol is fully decentralized and is based on majority-consensus decisions to commit on unique ordering of received broadcast messages. Under normal operating conditions, the protocol requires three phases to complete and approximately 4N messages where N is the number of sites. If more than 4N broadcast messages are exchanged in each protocol execution, this protocol achieves better performance than any of the protocols published to date without assuming specific types of site connectivity, clock synchronization, or knowledge of failed sites and failed communication links. Under abnormal operating conditions, a decentralized termination protocol, also presented, is invoked. A performance analysis of this protocol shows that it commits with high probability under realistic operating conditions without invoking termination protocol if N is sufficiently large.<<ETX>>
[Disruption tolerant networking, reordering, Protocols, Costs, Satellite broadcasting, majority-consensus decisions, fault-tolerant protocol, delay of messages, Synchronization, Delay, atomic broadcast, duplication, Fault tolerance, loss, Fault tolerant systems, network partitioning, fault tolerant computing, decentralized termination protocol, Performance analysis, protocols, performance analysis, Clocks]
An implementation of reliable broadcast using an unreliable multicast facility
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The authors consider the problem of reliable broadcast in a point-to-point asynchronous network. Such a network consists of host computers and a communication subnetwork. The latter, in turn, is a collection of switches (special-purpose computers that have the ability to store and forward messages), interconnected by point-to-point bidirectional communication links. The subnetwork is unreliable, i.e. a message is never guaranteed to be delivered in a finite interval of time. Broadcast is to be implemented among the hosts of the network. The basic mode of operation for the proposed broadcast mechanism is the posting by the source of multiply addressed data messages to be delivered to all participating hosts, combined with a distributed redelivery algorithm that ensures that those messages that were lost in transit do eventually arrive at all intended destinations. At the heart of the algorithm is a set of structures called priority lists. The use of the priority lists to encode topological information and the performance of the algorithm are discussed.<<ETX>>
[Out of order, data communication systems, multiply addressed data messages, communication subnetwork, Switches, Partitioning algorithms, host computers, Computer science, Multicast algorithms, Computer network reliability, distributed redelivery algorithm, point-to-point asynchronous network, point-to-point bidirectional communication links, Broadcasting, Computer networks, reliable broadcast, unreliable multicast facility, Telecommunication network reliability, Communication networks, protocols, priority lists]
Interactive consistency with multiple failure modes
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The authors address the problem of reaching Byzantine agreement in a distributed system in the presence of different types of faults and show that significant improvements in reliability and performance are possible if faults can be partitioned into disjoint classes. They show that, in a distributed system, to guarantee Byzantine agreement requires N>2a+2s+b+r where N is the total number of processors, a is the number of malicious asymmetric faults (a<or=r), s the number of malicious symmetric faults, b the number of nonmalicious or intercepted faults, and r an algorithm-dependent term. The practical value of this unified model in designing ultrareliable systems is demonstrated by examples.<<ETX>>
[Costs, Byzantine agreement, interactive consistency, multiple failure modes, reliability, distributed processing, distributed system, algorithm-dependent term, Certification, Transmitters, performance, Fault tolerant systems, Authentication, Failure analysis, unified model, Hardware, fault tolerant computing]
Task allocation for optimized system reliability
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The authors deal with the task allocation problem in distributed software design, with the goal of maximizing the system reliability. A quantitative problem model, algorithms for optimal and suboptimal solutions, and simulation results are provided and discussed. Because the authors use a new allocation goal-to maximize system reliability-this paper complements the existing body of knowledge in task allocation.<<ETX>>
[software reliability, Maintenance, Application software, Distributed computing, distributed software design, quantitative problem model, task allocation, Software design, Message passing, simulation results, Computer applications, Cost function, Hardware, Reliability, Resource management, optimized system reliability]
An analysis of the performance impacts of lookahead execution in the conversation scheme
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The lookahead execution approach, which allows early finishing participant processes to exit from a conversation before other participants finish their conversation activities, is adopted as a fundamental approach to reducing the synchronization overhead. Queueing network models are developed for both the system operating under the basic conversation scheme and the system operating under the conversation scheme extended with the lookahead capability. Based on the models, various performance indicators such as the system throughput, the average number of processors idling inside a conversation due to the synchronization required, and the average time spent in a conversation are evaluated numerically for different application environments. The performances under the extended scheme are compared to those under the basic conversation scheme. The results provide insights into the extent of benefits that can be brought in by the lookahead execution approach.<<ETX>>
[queueing network models, Costs, queueing theory, lookahead execution, distributed processing, Throughput, Application software, Distributed computing, performance impacts, Concurrent computing, application environments, Fault tolerance, synchronization overhead, system throughput, conversation scheme, Hardware, fault tolerant computing, Performance analysis, Finishing, Testing]
Quorum consensus algorithms for secure and reliable data
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The authors address the issue of maintaining security in a fault-tolerant replicated database. They present a data-management protocol that integrates the information-dispersal algorithm (for security) and the quorum-consensus algorithm (for reliability). Although this protocol provides the desired level of security, it does not achieve the same level of availability for both read and write operations as the quorum-consensus algorithm. By integrating a log-based propagation mechanism with their protocol, the authors are able to achieve the same level of availability for both read and write operations as other quorum-consensus protocols, while maintaining the desired level of security.<<ETX>>
[Availability, Protocols, quorum consensus algorithms, Data security, Transaction databases, Maintenance, Communication system security, database management systems, fault-tolerant replicated database, Computer science, Fault tolerance, security, security of data, log-based propagation mechanism, Information security, information-dispersal algorithm, fault tolerant computing, data-management protocol, Gratings, protocols]
Recovering imprecise transactions with real-time constraints
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
In real-time database systems, a transaction may not have enough time to complete. In such cases, partial, or imprecise, results can still be produced. The authors have proposed an imprecise result mechanism for producing partial results, which is used to implement timing error recovery in real-time database systems. They also present a model of real-time systems that distinguishes the external data consistency from the internal data consistency maintained by non-real-time systems. Providing a timely response may require sacrificing internal consistency. The authors discuss three examples that have different requirements of data consistency and present algorithms for implementing them.<<ETX>>
[Real time systems, imprecise transaction recovery, real-time constraints, external data consistency, timing error recovery, Control systems, Scheduling, Transaction databases, Application software, database management systems, Computer science, Physics computing, Distributed databases, real-time systems, Database systems, fault tolerant computing, model, Timing, database systems]
Checkpointing and rollback recovery in a distributed system using common time base
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
An approach to checkpointing and rollback recovery in a distributed computing system using a common time base is proposed. First, a common time base is established in the system using a hardware clock synchronization algorithm. This common time base is coupled with a pseudorecovery block approach to develop a checkpointing algorithm that has the following advantages: (i) maximum process autonomy, (ii) no wait for commitment for establishing recovery lines, (iii) fewer messages to be exchanged, and (iv) less memory requirement.<<ETX>>
[Checkpointing, Real time systems, checkpointing, common time base, hardware clock synchronization algorithm, Laboratories, maximum process autonomy, distributed processing, distributed system, Synchronization, Distributed computing, pseudorecovery block approach, Fault tolerant systems, Broadcasting, Hardware, rollback recovery, Testing, Clocks]
An experimental investigation of software diversity in a fault-tolerant avionics application
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
Highly reliable and effective failure detection and isolation (FDI) software is crucial in modern avionics systems that tolerate hardware failures in real time. The FDI function is an excellent opportunity for applying the principal of software design diversity to the fullest, i.e., algorithm diversity, in order to provide gains in functional performance as well as potentially enhancing the reliability of the software. The authors examine algorithm diversity applied to the redundancy management software for a hardware fault-tolerant sensor array. Results of an experiment are presented that show the performance gains that can be provided by utilizing the consensus of three diverse algorithms for sensor FDI.<<ETX>>
[Real time systems, hardware failures, software design diversity, hardware fault-tolerant sensor array, Software algorithms, Redundancy, Software performance, Performance gain, Aerospace electronics, software diversity, aircraft instrumentation, algorithm diversity, Sensor arrays, fault-tolerant avionics application, Software design, Fault detection, aerospace computer control, failure detection and isolation, Hardware, fault tolerant computing, redundancy management software]
Pessimistic protocols for quasi-partitioned distributed database systems
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The authors propose two protocols for transaction processing in quasi-partitioned databases. The protocols are pessimistic in that they permit the execution of update transactions in exactly one partition. The first protocol is defined for a fully partition-replicated database in which every partition contains a copy of every data object. The second protocol is defined for a partially partition-replicated database in which some objects have no copies in some partitions. Both protocols improve their major performance measures linearly with the backup link speed but are not visibly affected by either duration of the partitioning or the database size. This is a desirable property, since the backup link speed is the only controllable parameter.<<ETX>>
[Availability, transaction processing, Protocols, pessimistic protocols, quasipartitioned distributed database systems, Size measurement, Computer crashes, Transaction databases, fully partition-replicated database, performance measures, Satellites, Voting, distributed databases, Telephony, Database systems, Velocity measurement, protocols]
Implementation of RAID
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
RAID is a robust and adaptable distributed system for transaction processing. It is a message-passing system, with server processes on each site. A high-level, layered communications package provides a clean, location independent interface between servers. RAID processes concurrent updates and retrievals on multiple sites. The servers manage concurrent processing, consistent replicated copies during site failures or network partitionings, and atomic distributed commitment. The latest version of the communications package is able to deliver messages in a high-performance configuration in which several servers are linked into a single process. RAID provides the infrastructure to investigate experimentally various methods for supporting reliable distributed-transaction processing. Experiments on handling site failure with partial replication, checkpointing, and alternative communications methods have been performed. Measurements on various aspects of RAID transaction processing performance are presented.<<ETX>>
[Checkpointing, transaction processing, checkpointing, distributed processing, site failures, Distributed computing, concurrent processing, Network servers, partial replication, Atomic layer deposition, atomic distributed commitment, layered communications package, Robustness, Software measurement, network partitionings, Testing, replicated copies, RAID, Sun, site failure, concurrency control, Packaging, message-passing system, Telecommunication network reliability, robust and adaptable distributed system]
Vote assignments in weighted voting mechanisms
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
Majority voting is commonly used in distributed computing systems to control mutual exclusion, and in fault-tolerant computing to achieve reliability. Different vote assignments may result in different reliabilities. The authors present vote assignment algorithms aimed at maximizing the reliability. In their approach the voting weight assigned to each node is readily determined if the link failure rate is negligible. For systems having imperfect links a heuristic algorithm is proposed. Simulation studies show that the algorithm can approximate optimal assignments quite well in small systems.<<ETX>>
[Availability, fault-tolerant computing, Heuristic algorithms, mutual exclusion, reliability, distributed processing, Control systems, majority rating, weighted voting mechanisms, Distributed computing, Delay, simulation studies, distributed computing systems, heuristic algorithm, Fault tolerance, Databases, heuristic programming, Voting, Fault tolerant systems, fault tolerant computing, vote assignments, Reliability]
Independent checkpointing and concurrent rollback for recovery in distributed systems-an optimistic approach
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
A checkpoint algorithm is presented that benefits from the research in concurrency control, commit, and site recovery algorithms in transaction processing. In the authors' approach a number of checkpointing processes, a number of rollback processes, and computations on operational processes can proceed concurrently while tolerating the failure of an arbitrary number of processes. Each process takes checkpoints independently. During recovery after a failure, a process invokes a two-phase rollback algorithm. It collects information about relevant message exchanges in the system in the first phase and uses it in the second phase to determine both the set of processes that must roll back and the set of checkpoints up to which rollback must occur. Concurrent rollbacks are completed in the order of the priorities of the recovering processes. The proposed solution is optimistic in the sense that it does well if failures are infrequent by minimizing overhead during normal processing.<<ETX>>
[Checkpointing, transaction processing, NASA, distributed processing, commit, concurrent rollback, Computer crashes, Virtual machining, recovery, Delay, Machine intelligence, Computer science, Concurrent computing, two-phase rollback algorithm, site recovery algorithms, independent checkpointing, concurrency control, distributed databases, distributed systems, Database systems]
A robust, distributed election protocol
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The authors present an election protocol that does not assume an underlying ring structure and that tolerates failures, including lost messages and network partitioning, during the execution of the protocol itself. The major problem to be solved is that when nodes cannot communicate with one another or messages are lost, a conflict in resolving the election will often arise. In the authors' approach, the conflict is detected by the cohorts (noncandidate participants in the election). Related election protocols are discussed, and the system model is described together with assumptions about the communication subsystem. The protocol and the lost-message situations are then examined.<<ETX>>
[Protocols, Merging, Nominations and elections, distributed processing, system model, Distributed computing, noncandidate participants, Computer science, Network topology, communication subsystem, Broadcasting, Robustness, Database systems, fault tolerant computing, distributed election protocol, protocols]
Distributed locking: a mechanism for constructing highly available objects
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
A description is given of the results of a study of methods of achieving fault tolerance in the Clouds system and, in particular, of achieving increased availability of objects. The problems explored in this work, the model of distributed computation in which the problems posed by the research were examined (the Clouds system), the tools that were used to address these problems (the Aeolus programming language), and some related research are briefly described. The authors present a methodology for achieving available services by conversion of resilient single-site implementations into replicated implementations. A mechanism with which they propose to support this methodology, called distributed locking (DL), is presented. A description is also given of a linguistic feature for the specification of the availability properties of an object replicated via DL. The language runtime support features (primitives) required for DL and the operating system support needed for these features are presented.<<ETX>>
[Availability, fault tolerance, Clouds, distributed locking, distributed processing, distributed computation, Proposals, Clouds system, Distributed computing, Aeolus programming language, linguistic feature, Computer science, Concurrent computing, Computer languages, Fault tolerant systems, Distributed databases, operating system support, operating systems (computers), fault tolerant computing, Internet]
A commit protocol for checkpointing transactions
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
A commit protocol is described for checkpointing distributed transactions. Commit protocols are used by distributed transaction management systems to ensure that the multiple nodes participating in a distributed transaction will commit or abort together. This commit protocol is different from others in that a process executing on behalf of a transaction can be interrupted and restarted at some previous snapshot of its state (a checkpoint). The commit protocol guarantees that processes working on behalf of a distributed transaction will be consistent, which implies that the work performed by a restarted process between the time of the checkpoint and the time of the interruption will be undone automatically. The undoing includes any local state changes during that period of time, and any state changes in other processes due to communication with the restarted process in that period. The use of a commit protocol for recovery purposes allows normal execution to be resumed before recovery is completed. Recovery will be carried out in parallel, and the commit protocol guarantees that it is performed eventually. A novel approach of reusing portions of a transaction reduced lost work.<<ETX>>
[Checkpointing, Protocols, Software algorithms, Computer crashes, Resilience, Computer science, Nonvolatile memory, checkpointing transactions, distributed databases, distributed transactions, management systems, protocols, commit protocol]
On the diagnosis of Byzantine faults
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
The class of evidence-based diagnosis algorithms is developed to identify Byzantine (and any other faulty) processors. Such algorithms are said to be fair if they identify no failure-free processor as faulty. This paper makes two significant contributions: (i) it introduces a very general and simple formal model of the evidence-based diagnosis algorithms; and (ii) it derives a simple fair diagnosis algorithm, which is proved optimal for a large class of algorithms. It is further demonstrated that no fair evidence-based diagnosis algorithm can guarantee the identification of all faulty processors (completeness). Several insights into the behavior of the algorithm are presented.<<ETX>>
[Fault diagnosis, Software testing, Computer science, faulty processors diagnosis, evidence-based diagnosis algorithms, formal model, Byzantine faults diagnosis, distributed processing, Hardware, Computer crashes, fault tolerant computing, Maintenance]
The commit/abort problem in type-specific locking
Proceedings [1988] Seventh Symposium on Reliable Distributed Systems
None
1988
Type-specific locking is designed to increase performance of distributed transactions by allowing competing transactions to concurrently alter shared objects concurrently, provided their changes are commutative. When one of the set of compatible transactions commits or aborts, a problem arises due to the indeterminacy of the alterations of the others. A tree of nineteen solutions to the problem has been investigated. Given the necessary support by the underlying architecture, this tree can be pruned severely. The problem and set of approaches came to the authors' attention during design for type-specific locking in PROFEMO, an operating system for a local area network that provides a location-independent object environment and built-in mechanisms for transaction control and recovery. A description is given of the problem, the possible solutions, and the arguments that allow the pruning. Testbed results are not yet available, since type-specific locking has not yet been implemented at the time of the initial tests of the PROFEMO system.<<ETX>>
[type-specific locking, System testing, operating system, distributed processing, Control systems, Programming environments, commit/abort problem, Concurrent computing, Operating systems, PROFEMO, local area network, Distributed control, distributed transactions, operating systems (computers), Computer networks, Hardware, Local area networks]
Ring network reliability-the probability that all operative nodes can communicate
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
The authors consider the reliability of fail-soft ring networks using as a measure the probability that all operative nodes can communicate P(C). Although computing P(C) is in general an NP-hard problem, it is shown that, for ring networks, closed-form expressions can be derived in terms of the failure probabilities of nodes, links, and switches (the configuration components used in attempts to achieve fail-soft operation of the rings). The authors derive closed-form expressions of P(C) for five simplex rings: (1) the basic unidirectional ring, (2) rings with nodal bypass switches, (3) rings with both bypass switches and a standby ring, (4) rings with self-heal switches, and (5) star-shaped rings. The expressions for these five rings are evaluated under varying failure probability assumptions, and reliability comparisons are made.<<ETX>>
[Switches, simplex rings, failure probability assumptions, star-shaped rings, reliability comparisons, Electric variables measurement, Bandwidth, Computer networks, Communication networks, unidirectional ring, Optical fibers, computer networks, probability, Educational institutions, closed-form expressions, Communication switching, nodal bypass switches, NP-hard problem, self-heal switches, standby ring, fail-soft ring networks, fault tolerant computing, configuration components, Telecommunication network reliability, operative nodes]
Distributed processing test simulator (DPTS)
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
DPTS (distributed processing test simulator) is a protocol and associated system-level software that result in a system in which the software can efficiently interact across processor boundaries, provide some measure of processor and/or communications fault tolerance, enhance cost effectiveness and reduce life-cycle cost, permit decreased dependency on system complexity, and eliminate concern over certain aspects of system design. Additionally, the protocol enhances the effectiveness of systems requiring rapid reconfigurability, such as simulators. The environment being targeted by the effort is described. The overall design concepts are described, followed by a discussion of the steps necessary to achieve the targeted design. Finally, the current state of the effort is presented.<<ETX>>
[Software testing, System testing, Protocols, Costs, program testing, software reliability, distributed processing, distributed processing test simulator, rapid reconfigurability, Distributed processing, protocol, Fault tolerant systems, software tools, Software measurement, design concepts, Life testing, processor boundaries, DPTS, Communication system software, system-level software, cost effectiveness, Software systems, fault tolerant computing, communications fault tolerance, life-cycle cost]
An efficient kernel-level dependable multicast protocol for distributed systems
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
Multicast communication in a distributed system connected by a local area network can increase parallelism, and it can also provide a greater functionality than one-to-one communication. In the authors' multicast protocol, the sender directs a message to a named group of receivers, which can be specified by function without requiring the sender to know the specific members of the group. Each host's kernel in the network can respond to every group message sent, providing various levels of reliability. It was found that the overhead of providing dependable multicast over a single local area network was very small, mainly because the protocol operates at the kernel level rather than the user level. Several forms of this multicast communication, expressed as simple message-passing communication primitives, are described, and the effectiveness of the protocol is evaluated using an example of a distributed algorithm. Performance analyses and actual performance data for the protocol are presented.<<ETX>>
[kernel-level dependable multicast protocol, parallelism, Multicast communication, Multicast protocols, local area networks, functionality, Application software, Distributed computing, Multicast algorithms, Computer network reliability, local area network, Parallel processing, distributed systems, message-passing communication primitives, Computer networks, performance data, protocols, Distributed algorithms, Local area networks]
Bounded approximate reliability models for distributed systems
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
A study is made of several methods for reducing complex fault tree models of fault-tolerant distributed systems. For each method the authors provide bounds on the estimate of unreliability that is obtained from the reduced model. They discuss methods for truncating the solution of a model expressed as a fault tree and then develop techniques that apply to the construction of the fault tree model. The emphasis is on producing approximate (but bounded) results applicable to realistic systems. The authors also discuss methods for incorporating dynamic system behavior (error handling and redundancy management) into fault tree models, and the corresponding truncated solution. The methods are presented as they are used in modeling two distributed systems, the Cm* system and AIPS (the Advanced Information Processing System).<<ETX>>
[approximate reliability models, distributed processing, dynamic system behavior, Distributed computing, Information analysis, Fault tolerance, Fault tolerant systems, Control system synthesis, distributed systems, redundancy, Cm* system, Fault trees, Redundancy, Advanced Information Processing System, trees (mathematics), Independent component analysis, complex fault tree models, redundancy management, Aerospace control, Computer science, realistic systems, AIPS, fault tolerant computing, fault-tolerant distributed systems, error handling]
Recovering from process failures in the time warp mechanism
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
A recovery procedure for distributed systems using the time warp control mechanism is described. Time warp is an optimistic execution technique in which synchronization is achieved using rollback. The recovery procedure is a protocol that exploits the redundancy already available to implement process rollback in the time warp mechanism. Thus, the recovery protocol has little additional bookkeeping overhead, unlike many other recovery procedures. An informal proof of the correctness of the recovery procedure for a single process failure is presented. The protocol is extended so that it becomes resilient to multiple process failures.<<ETX>>
[Checkpointing, Protocols, software reliability, time warp mechanism, recovery procedure, distributed processing, Control systems, Discrete event simulation, system recovery, protocol, Operating systems, network operating systems, distributed systems, Workstations, redundancy, bookkeeping overhead, rollback, correctness, Concurrency control, Transaction databases, synchronisation, Computer science, synchronization, fault tolerant computing, Time factors, optimistic execution technique, process failures]
Using checkpoints to localize the effects of faults in distributed systems
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
A checkpointing scheme can be used to ensure forward progress of a computation (program) even when failures occur. In a distributed system, many autonomous programs can execute concurrently and obtain services from a set of shared servers. In such a system, it is desirable to to restrict a checkpoint or rollback operation to a single program to localize the effects of failures, even when processes of different programs communicate with servers. This can be achieved by a scheme based on message logging and consistent checkpoints when the system is deterministic. When the system (communication network or programs) is nondeterministic, the semantics of the server functions should be exploited to reduce the additional synchronization that needs to be introduced to ensure locality. The authors illustrate this by presenting efficient algorithms for a file server that do not require the logging of messages on stable storage.<<ETX>>
[Checkpointing, Costs, checkpointing scheme, autonomous programs, server functions, distributed processing, File servers, semantics, Distributed computing, system recovery, forward progress, Concurrent computing, message logging, Network servers, network operating systems, file servers, distributed systems, file server, Communication networks, Resumes, synchronisation, Computer science, shared servers, synchronization, stable storage, Impedance, rollback operation]
Decentralized loopback reconfiguration of a bidirectional ring LAN
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
A decentralized reconfiguration method for local networks of bidirectional ring architecture is presented. The method relies on a suitably designed two-mode medium access control protocol that is used uniformly in all network configurations. The reconfiguration transitions are performed autonomously by individual nodes, based purely on the local status of medium access control; no specialized topology-testing frames or signals are used. Reconfiguration proceeds as a decentralized, stepwise process, in which the enabled network components grow together, leaving out the disabled parts. It is suggested that the proposed reconfiguration method is robust and suitable for embedded distributed systems operating in unsafe environments.<<ETX>>
[Nuclear and plasma sciences, local status, medium access control protocol, Laboratories, local area networks, decentralized reconfiguration method, bidirectional ring LAN, unsafe environments, network configurations, Network topology, Computer network reliability, local networks, Computer architecture, Media Access Protocol, distributed systems, Robustness, Computer networks, fault tolerant computing, protocols, Local area networks, Testing]
Workload analysis for performance study of distributed deadlock detection algorithms
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
The authors present an approach to distributed workload analysis which can be used as a basis for the performance study of distributed deadlock detection algorithms. In particular, the expected number of times a deadlock detection algorithm is locally initiated and the subsequent number of remote invocations are derived. Simulation work was done to validate the approach.<<ETX>>
[Algorithm design and analysis, transaction processing, distributed deadlock detection algorithms, Computational modeling, remote invocations, performance evaluation, Distributed computing, system recovery, Delay, Computer science, performance study, distributed databases, System recovery, Database systems, Performance analysis, Detection algorithms, Distributed algorithms]
Limits on scalability in gracefully degradable large-scale systems
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
The authors present an analysis of the scalability of large-scale degradable homogeneous multiprocessors by assessing the limitations imposed by reliability considerations on the number of processors. They demonstrate that graceful degradation in large-scale systems is not scalable. An increase in the number of processors must be matched by a significant increase in the coverage factor in order to maintain the same performance and reliability levels.<<ETX>>
[Availability, Performance evaluation, multiprocessing systems, reliability levels, Scalability, homogeneous multiprocessors, performance evaluation, Time measurement, scalability, Multiprocessing systems, Degradation, performance, Fault tolerant systems, gracefully degradable large-scale systems, Large-scale systems, Performance analysis, Power system reliability]
A low overhead checkpointing and rollback recovery scheme for distributed systems
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
A major obstacle in implementing a rollback recovery scheme for fault tolerance in a concurrent distributed system is the domino effect. A low overhead checkpointing scheme is proposed to prevent this effect. Each process saves its state periodically. The state-save synchronization among processes is implemented by bounding clock drifts. A communication protocol that assures that all saved states are consistent is developed.<<ETX>>
[Checkpointing, low overhead checkpointing, bounding clock drifts, Protocols, saved states, fault tolerance, domino effect, distributed processing, communication protocol, Synchronization, Distributed computing, system recovery, Radio access networks, Computer science, concurrent distributed system, Fault detection, Fault tolerant systems, network operating systems, distributed systems, fault tolerant computing, Power system reliability, protocols, rollback recovery scheme, Clocks]
Performance comparison of concurrency control protocols for transaction processing systems with regional locality
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
An examination is made of a system structure and protocols to improve the performance and availability of a distributed transaction processing (TP) system when there is some regional locality of data reference. Several TP applications, such as reservation systems, insurance, and banking, belong to this category. While maintaining a distributed system at each region, a central system is introduced with a replication of all databases at the distributed sites. Specialized protocols can be designed to keep the copies at the distributed and centralized systems consistent without incurring the overhead and delay of generalized protocols for fully replicated databases. The authors study the advantages of this system structure and the tradeoffs between protocols for concurrency and coherency control of the duplicate copies of the databases. An approximate analytic model is used to estimate the system performance and the method is validated through simulations.<<ETX>>
[transaction processing, insurance, Protocols, Control systems, transaction processing systems, Delay, banking, Concurrent computing, reservation systems, Distributed databases, distributed databases, regional locality, duplicate copies, protocols, Availability, replication, databases, replicated databases, coherency control, Banking, performance evaluation, Concurrency control, Transaction databases, concurrency control protocols, data reference, Insurance, concurrency control, system performance]
Testing reliable distributed applications through simulated events
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
There are many distributed applications that incorporate application-specific reliability algorithms that operate on top of general-purpose networking, operating system, and programming language facilities. The authors present a framework for application-level reliability testing suitable for a wide range of distributed applications using low-level events and the automatic generation of series of these events. They also describe how they used this testing approach for a particular application.<<ETX>>
[programming language facilities, System testing, Protocols, reliable distributed applications, simulated events, software reliability, operating system, Debugging, distributed processing, general-purpose networking, Discrete event simulation, Application software, Sun, Computer languages, Automatic testing, Operating systems, telecommunications computing, application-specific reliability algorithms, virtual machines, Hardware]
Recovery-management in the RelaX distributed transaction layer
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
Transactions are especially valuable in distributed systems, since they isolate the programmer from the effects of both concurrency and failures. In implementing transactions at the system level, flexibility has to be introduced into the transaction concept. Specifically, the premature release of data objects has to be addressed. To assure recoverability, resulting dependencies between transactions are stored by the system in a distributed data structure called a recovery graph. The storing of redundant information in the recovery graphs at the different sites reduces the complexity of the commit protocol, and a chase protocol used to abort transactions can be derived which excludes infinite chasing. The redundant information can be distributed almost for free because it can be piggybacked on messages. The new commit/abort protocols will be used in the RelaX project (reliable distributed applications support on Unix), which carries on work done at GMD in the PROFEMO project on distributed transaction mechanisms.<<ETX>>
[transaction processing, Unix, Protocols, distributed processing, PROFEMO project, system recovery, Concurrent computing, Fault tolerant systems, network operating systems, Prototypes, chase protocol, distributed systems, GMD, failures, distributed data structure, Redundancy, data objects, recoverability, programmer, Data structures, distributed transaction mechanisms, Concurrency control, transactions, Programming profession, Phase detection, concurrency, fault tolerant computing, redundant information, Resource management, recovery graph, commit protocol]
A token-based protocol for reliable, ordered multicast communication
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
A description is given of the token-passing multicast (TPM) protocol, a token-based protocol that provides reliable, ordered multicast communication for distributed process groups in the presence of failures and network partitions. The TPM protocol combines several positive features of other reliable multicast schemes into a single protocol, yet maintains a relatively simple structure and requires that only a minimal amount of state information be kept by process group members. It is designed specifically for computing environments with relatively low error rates, such as local area networks, and for process groups in which communication is symmetric, that is, each group member can send messages to the group, and the source must be a member of the group.<<ETX>>
[Error analysis, failures, reliability, Multicast communication, Multicast protocols, local area networks, Maintenance, Computer science, Unicast, computing environments, Computer network reliability, token-based protocol, network partitions, multicast communication, low error rates, Computer networks, state information, Telecommunication network reliability, protocols, token networks, Local area networks]
Implementing atomic rendezvous within a transactional framework
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
The authors address the problem of implementing the CSP (communicating sequential processes) rendezvous within a transactional framework. Instead of implementing a fair nondeterministic choice and assuming the correct functioning of processors and communication media, the authors propose an efficient transactional implementation of the atomic rendezvous in the presence of processor failures in a multiprocessor machine. Both atomicity and efficiency are obtained by using high-speed stable storage devices.<<ETX>>
[Out of order, transaction processing, CSP, multiprocessing systems, atomicity, communicating sequential processes, Resumes, processor failures, Transaction databases, Face detection, High level languages, Programming profession, synchronisation, high-speed stable storage, atomic rendezvous, Distributed databases, Production, Hardware, multiprocessor machine, Power system reliability, transactional framework]
Consistent replicated transactions: a highly reliable program execution environment
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
A highly reliable program execution environment which enables user programs to tolerate underlying hardware failures is presented. The approach is to run multiple copies of the user programs at the same time. As long as one copy survives, the user program can be completed successfully. In the meantime, the user interacts with the replicated program as if it were a normal program. The authors call this characteristic user transparent replication. In order to achieve user transparent replication, program replicas must behave consistently. Otherwise, users might get different queries or output from different running replicas. The authors identify the reasons why the inconsistent program execution problem occurs and propose algorithms to ensure that computation replicas behave consistently. With consistent running program replicas, a filter program can be easily constructed to delete duplicated I/O requests or duplicated output and thus achieve user transparency.<<ETX>>
[transaction processing, user transparency, hardware failures, user programs, software reliability, replicated program, Computer crashes, filter program, characteristic user transparent replication, Computer science, Condition monitoring, Concurrent computing, Fault tolerance, Filters, Content addressable storage, Intersymbol interference, concurrency control, I/O requests, operating systems (computers), Hardware, fault tolerant computing, highly reliable program execution environment, Detection algorithms, programming environments]
Availability analysis of the primary site approach for fault tolerance
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
The primary site approach is often used to support fault tolerance against node failures. The authors present an analytic model to evaluate the availability of a system using the primary site approach. The effect of the number of replicas and the checkpoint interval were studied using the model. The authors found that the optimal checkpoint interval is proportional to the square root of the checkpoint overhead and inversely proportional to the request arrival rate. For the degree of replication, the results depend on what kind of checkpointing scheme is used. In systems using the broadcasting scheme, it was found that there is no optimal degree of replication: increasing the degree of replication increases the availability. However, in systems using the point-to-point checkpointing scheme, an optimal degree of replication exists: increasing the degree of replication beyond this optimum decreases the availability. Although the authors only consider a single repair server in the system, the model can easily be extended to allow multiple repair servers.<<ETX>>
[Availability, Checkpointing, Out of order, primary site approach, fault tolerance, checkpoint interval, replicas, checkpoint overhead, distributed processing, performance evaluation, Educational institutions, Throughput, Computer science, Fault tolerance, Fault tolerant systems, network operating systems, broadcasting scheme, Frequency, Database systems, fault tolerant computing, single repair server, node failures, request arrival rate]
Implementing fault-tolerant replicated objects using Psync
Proceedings of the Eighth Symposium on Reliable Distributed Systems
None
1989
Psync is an IPC protocol that explicitly preserves the partial order of messages exchanged among a set of processes. A description is given of how Psync can be used to implement replicated objects in the presence of network and host failures. Unlike conventional algorithms that depend on an underlying mechanism that totally orders messages for implementing replicated objects, the authors' approach exploits the partial order provided by Psync to achieve additional concurrency.<<ETX>>
[fault-tolerant replicated objects, Protocols, History, IPC protocol, additional concurrency, Computer science, Concurrent computing, Fault tolerance, network operating systems, concurrency control, Psync, Communication system operations and management, Broadcasting, Concrete, fault tolerant computing, Communication networks, protocols, host failures, partial order]
Distributed lock management in a transaction processing environment
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
Distributed synchronization for data sharing is discussed, and the design of a distributed lock manager for the Camelot transaction facility is presented. The lock manager is a component of a proposed implementation of data sharing in the Camelot environment. A number of experiments that demonstrate the correct operation of the lock manager are reported and its performance is described. The performance metrics indicate that distributed lock management should not reduce the feasibility of data sharing in this environment. The similarity between the caching and synchronization strategies appropriate for locks and data suggests that protocols developed for distributed locks will be applicable to data sharing.<<ETX>>
[Measurement, transaction processing, Costs, Protocols, transaction processing environment, Cache storage, Application software, Camelot, Environmental management, distributed lock management, caching, Delay, Programming profession, Computer science, File systems, performance, network operating systems, file organisation, synchronization, protocols, data sharing]
A low-cost atomic commit protocol
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
The proposed coordinator log transaction execution protocol centralizes logging on a per-transaction basis and exploits piggybacking to provide the semantics of a distributed atomic commit without the associated costs. This protocol eliminates two rounds of messages (one phase) from the presumed commit protocol and dramatically reduces the number of log forces needed for distributed atomic commit. The authors compare the coordinator log transaction execution protocol with existing protocols, describe when it is desirable, and discuss how it affects the write-ahead log protocol and the database crash recovery algorithm.<<ETX>>
[transaction processing, Protocols, Costs, coordinator log transaction execution protocol, File servers, Computer crashes, Transaction databases, semantics, piggybacking, Multiprocessing systems, low-cost atomic commit protocol, Network servers, Distributed databases, Prototypes, distributed atomic commit, distributed databases, database crash recovery algorithm, Database systems, protocols]
Temporal uncertainties in interactions among real-time objects
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
A model of a distributed real-time system which supports reasoning about the consistency and accuracy of real-time data and about the performance of real-time communication protocols is presented. The conventional object model is extended into a model of a real-time (RT-) object which incorporates a real-time clock as a mechanism for initiating an object action as a function of real time. The notion of accuracy as referring to the time gap between a state variable in the external world and its representation in a real-time computer system is adopted. The effects of the temporal uncertainties of different classes of communication protocols on the consistency and the accuracy of RT-objects are analyzed. Finally, an approach to structuring fault-tolerant RT-objects in the form of active object replicas is discussed, and the effects of a failure of a task in a replica on the responsiveness of remote objects are analyzed.<<ETX>>
[Real time systems, Uncertainty, Protocols, real-time clock, reasoning, distributed processing, accuracy, Distributed computing, consistency, Delay, interactions, distributed real-time system, Failure analysis, state variable, model, protocols, Object oriented modeling, temporal uncertainties, communication protocols, real-time objects, fault-tolerant RT-objects, Synchronization, performance, real-time systems, fault tolerant computing, Timing, Clocks]
Voting as the optimal static pessimistic scheme for managing replicated data
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
The problem of finding an optimal static pessimistic replica control scheme is investigated. It has been widely accepted that coteries (proposed by Garcia-Molina and Barbara) provide the most general framework for such schemes. Under such as assumption, it is demonstrated that the voting scheme is an optimal static pessimistic scheme for fully connected networks with negligible link failure rates, as well as for Ethernet systems. It is also shown that voting is not optimal for somewhat more general systems. The authors propose a modification of the algorithm of Tong and Kain for the best voting in the operation-independent case so that it runs in linear (rather than exponential) time. They also propose a linear-time algorithm for computing the optimal vote assignment when relative frequencies of read and write operations are known.<<ETX>>
[Availability, Protocols, Ethernet networks, managing replicated data, performance evaluation, fully connected networks, optimal vote assignment, voting scheme, Delay, linear-time algorithm, Computer science, replica control scheme, operation-independent case, Voting, distributed databases, Ethernet, Frequency, Database systems, Communication system traffic control, fault tolerant computing, optimal static pessimistic scheme, Contracts]
Using stashing to increase node autonomy in distributed file systems
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
The authors present an enhancement to distributed file systems that allows the users of the system to keep local copies of important files, decreasing the dependency over file servers. Using the notions of stashing and quasi-copies, the system allows users to tune up the quality of the service they want to receive when the file server is not reachable. One of the key points of this work is the focus on the tradeoff between availability and degradation of service. The other main contribution is the design of a distributed file system which is ideally suited to very large distributed systems, in that it provides users with greater tolerance of network partitions and server failures. It is emphasized that the use of stashing does not preclude the use of other performance-enhancing or fault-tolerant techniques. The file system architecture has been implemented and FACE, a prototype of a file system service based on Sun's NFS, is described. Performance figures are reported. These figures show that the overhead of providing the service is negligible. Current plans also call for porting the FACE design to a number of other processors.<<ETX>>
[Measurement, fault-tolerant techniques, Costs, distributed file systems, node autonomy, distributed processing, stashing, Control systems, File servers, degradation, Computer science, Degradation, Network servers, File systems, Prototypes, network partitions, Computer architecture, quasi-copies, file organisation, fault tolerant computing, FACE, tolerance]
Fault tolerant distributed database system via data inference
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
A knowledge-gased approach for query processing during network partitioning is proposed. The approach uses available domain and summary knowledge to infer inaccessible data to answer a given query. A rule induction technique is used to extract correlated knowledge between attributes from the database contents. This knowledge is represented as rules for data inference. On the basis of a set of queries, simulation is used to evaluate the effectiveness of the proposed data inference technique for improving data availability under network partitioning. Object allocation has a significant impact on data availability. Allocating objects that increase remote redundancy and reduce local redundancy increases data Availability during network partitioning. A prototype distributed database system that uses the proposed inference technique with correlated knowledge from a ship database has been implemented. Experience indicates that the proposed inference technique can significantly improve the availability of a distributed database during network partitioning.<<ETX>>
[simulation, Data mining, query processing, local redundancy, Fault tolerant systems, Prototypes, Distributed databases, knowledge based systems, rule induction technique, distributed databases, data availability, data inference, Database systems, Marine vehicles, Contracts, Knowledge acquisition, information retrieval, inference mechanisms, remote redundancy, Computer science, ship database, Query processing, knowledge-gased approach, network partitioning, fault tolerant distributed database, fault tolerant computing, attributes]
RelaX-an extensible architecture supporting reliable distributed applications
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
The authors provide a description of RelaX (reliable distributed applications support on UniX), a portable and extensible system software layer on top of UNIX-like operating system kernels that supports reliable distributed applications by a generalized transaction mechanism. The transaction mechanism relieves each programmer of dealing explicitly with error recovery and concurrency control in every distributed application. In order to make transactions applicable as a general programming tool, flexibility has been introduced into the traditional transaction concept. The transaction mechanism is isolated in a server (Transaction Manager) that cooperates with an extensible set of resource managers, which provide different kinds of long-term storage entities accessible by RelaX transactions. Each resource manager provides a standard interface to the transaction kernel, and, if so desired, additional resource managers can be built. In order to ease the construction of new resource managers, RelaX provides generic software components as building blocks for any kind of resource manager. The RelaX architecture is described and the design of an examplary resource manager, the transactional object management system which provides access to persistent shared objects, is outlined.<<ETX>>
[transaction processing, resource managers, distributed processing, system software, Transaction Manager, Concurrent computing, reliable distributed applications support, error recovery, Operating systems, Fault tolerant systems, computer architecture, Computer architecture, generic software components, Kernel, UniX, reliable distributed applications, RelaX, Concurrency control, Application software, Programming profession, transactional object management system, extensible architecture, generalized transaction mechanism, concurrency control, fault tolerant computing, Error correction, Resource management, standard interface]
Adjudicators for diverse-redundant components
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
The authors define the adjudication problem, summarize the existing literature on the topic, and investigate the use of probabilistic knowledge about error/faults in the subcomponents of a fault-tolerant component to obtain good adjudication functions. They prove the existence of an optimal adjudication function, which is useful both as an upper bound on the probability of correctly adjudged obtainable output and as a guide for design decisions.<<ETX>>
[fault-tolerant component, adjudication problem, probabilistic knowledge, upper bound, Sensor systems, diverse-redundant components, error, faults, Design optimization, design decisions, Fault tolerance, Sufficient conditions, Content addressable storage, Voting, Computer errors, Hardware, fault tolerant computing, Error correction, Software measurement, optimal adjudication function]
The design and implementation of a reliable distributed operating system-ROSE
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
ROSE, a modular distributed operating system that provides support for building reliable applications, is designed and implemented. Failure detection capabilities are provided by a failure detection server. Configuration objects can be used to capture the relationship among multiple processes that cooperate to replicate certain resources. Replicated address space (RAS) objects, whose content is accessible with a high probability despite hardware failures, can be used to increase data availability. Finally, a resistant process (RP) abstraction allows user processes to survive hardware failures with minimal interruption. Two different implementations of RP are provided: one checkpoints the information about its state in an RAS object periodically; the other uses replicated execution by executing the same code in different nodes at the same time.<<ETX>>
[Protocols, Buildings, Software algorithms, implementation, distributed processing, ROSE, reliable distributed operating system, Application software, resistant process, Distributed computing, Computer science, replicated address space objects, Operating systems, network operating systems, Computer applications, design, Writing, Hardware, fault tolerant computing, failure detection]
An improved algorithm for the symbolic reliability analysis of networks
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
An efficient Boolean algebraic algorithm for the symbolic reliability and sensitivity analysis of coherent two-terminal networks with s independent components is described. The algorithm is also applicable to a fault tree model without NOT gates. The algorithm uses the concept originally proposed by A. Grnarov, L. Kleinrock, and M. Gerla (1979). After the algorithm is presented, the errors in the original technique are illustrated by two examples. The algorithm is extended t compute the reliability importance of a given component (sensitivity of system reliability to a given component's reliability). A computer program implementing the modified algorithm is used to solve and obtain measured time complexities for a large set of network and fault tree models.<<ETX>>
[Algorithm design and analysis, coherent two-terminal networks, NASA, sensitivity analysis, trees (mathematics), circuit analysis computing, Time measurement, Maintenance, Boolean algebra, time complexities, Computer science, symbolic reliability analysis, fault tree model, Computer network reliability, Computer errors, Boolean algebraic algorithm, computer program, Computer networks, fault tolerant computing, Fault trees]
Adaptability experiments in the RAID distributed database system
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
A series of experiments is being conducted on the RAID distributed database system to study the performance and reliability implications of providing static and dynamic adaptability. The authors' studies of the cost of their adaptable implementation were conducted in the context of the concurrency controller and the replication controller. It is shown that adaptable implementations can be provided at costs comparable to those of special-purpose implementations. The experimentation with dynamic adaptability focuses on concurrency control. It is shown that dynamic adaptability can result in performance benefits and that system reconfiguration can be accomplished dynamically with less cost than stopping the system, performing reconfiguration, and then restarting the system. The authors' examination of the costs of providing greater data availability includes studying the replication control and atomicity control subsystems of RAID. The cost associated with increasing availability in an adaptable scheme of replication control and commit protocols is demonstrated.<<ETX>>
[Availability, Algorithm design and analysis, Costs, Protocols, atomicity control, commit protocols, reliability, performance evaluation, Data structures, Control systems, Concurrency control, RAID distributed database system, Concurrent computing, Microwave integrated circuits, performance, replication control, distributed databases, static adaptability, dynamic adaptability, Database systems, concurrency controller, protocols, system reconfiguration]
A general methodology for the system state characterization of event recognitions
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
The authors present a methodology for the characterization of event recognitions based on the relationship between the system state at recognition time and its state when the event actually occurred. The methodology incorporates characteristics of the event and computation with a technique for collecting and organizing state information, thus providing an integrated approach to recognition characterization. Since the processes of a distributed computation are located at different processors in a system, the task of recognizing an event occurrence actually consists of two subtasks. First, the information pertaining to the activity described in an event definition must be collected and organized into a view of the computation's behavior which reflects the temporal relationships between the activity at the various processes. Second, this view must be analyzed in light of the user-provided event definitions to determine whether the behavior specified by the user occurred.<<ETX>>
[program debugging, system state characterization, Delay effects, distributed processing, distributed computation, Character recognition, Distributed computing, Software debugging, Organizing, Information analysis, Concurrent computing, Computer science, general methodology, integrated approach, Software tools, event recognitions, Monitoring, temporal relationships]
A comparison of voting strategies for fault-tolerant distributed systems
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
The problem of voting is studied for both the exact and inexact cases. Optimal solutions based on explicit computation of condition probabilities are given. The most commonly used strategies, i.e. majority, median, and plurality are compared quantitatively. The results show that plurality voting is the most powerful of these techniques and is, in fact, optimal for a certain class of probability distributions. An efficient method of implementing a generalized plurality voter when nonfaulty processes can produce differing answers is also given.<<ETX>>
[Performance evaluation, Redundancy, optimal solutions, voting strategies, Application software, Synchronization, explicit computation, Computer science, plurality, Fault tolerance, Voting, median, Fault tolerant systems, probability distributions, distributed databases, Software systems, fault tolerant computing, majority, fault-tolerant distributed systems, condition probabilities, Clocks]
Representation and execution support for reliable robot applications
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
Robot applications, represented as plans, are used to outline a viewpoint that robustness needs to be emphasized in two areas: in the plan representation and in the underlying system software. Robot applications are inherently distributed, since the hardware usually comprises a set of independent actuators and sensors, with the robot programs acting as links between them. A special model of distributed computation, the RS (Robot Schemas) model, has been designed to handle the issues of robot plan representation, and an overview of the model is presented. An initial implementation of the model with minimal execution support demonstrated that the domain-dependent aspect of robustness on its own was not sufficient for robust behavior. Consequently, the OS has been augmented with real-time scheduling, and monitoring facilities.<<ETX>>
[Real time systems, Laboratories, execution support, distributed processing, system software, Human robot interaction, actuators, scheduling, Robot sensing systems, Robustness, robots, real-time scheduling, Monitoring, robustness, distributed computation, Application software, monitoring, Robotics and automation, domain-dependent aspect, Robot programming, representation support, sensors, Robot Schemas, real-time systems, reliable robot applications, plans, fault tolerant computing, Timing]
A lower bound on the reliability of an n-dimensional hypercube
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
A recursive algorithm for computing a lower bound on the all-terminal reliability of an n-dimensional hypercube is presented. The recursive step decomposes an n-dimensional hypercube into lower dimension hypercubes that are linked together. As an illustration of the effectiveness and power of this method, a lower bound is computed on the all-terminal reliability of the 16-dimensional hypercube (Connection Machine architecture) whose links number 2/sup 19/. The notation and assumptions are defined, and background information on bounding the reliability polynomial is provided. Methods for tightening these bounds for the analysis of the hypercube architecture are discussed.<<ETX>>
[Multiprocessor interconnection networks, parallel architectures, all-terminal reliability, Connection Machine architecture, hypercube networks, lower bound, Multiprocessing systems, Information analysis, Computer science, n-dimensional hypercube, reliability polynomial, Failure analysis, Computer architecture, recursive algorithm, Hypercubes, Computer networks, fault tolerant computing, Power system reliability, Telecommunication network reliability, 16-dimensional hypercube]
A fault tolerant algorithm for distributed mutual exclusion
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
A fault-tolerant mutual exclusion algorithm for distributed systems is presented. The algorithm uses a distributed queue strategy and maintains alternative paths at each site to provide a high degree of fault tolerance. However, owing to these alternative paths, the algorithm must use reverse messages to avoid the occurrence of directed cycles, which may form when the direction of edges is reversed after the token passes through. If there is no alternative path, the total number of the messages exchanged is O (2*log N) in light traffic and two messages in heavy traffic; however, in this case the system cannot tolerate even a single communication link or site failure. If there are alternative paths between sites, the system can achieve a higher degree of fault tolerance at the expense of increased message traffic (owing to reverse messages). Thus, there is a tradeoff between efficiency and reliability, and a system can be designed to balance these two criteria properly. A recovery procedure for restoring a recovering site consistently into the system is also presented.<<ETX>>
[Tree data structures, Delay effects, distributed mutual exclusion, reliability, Computer crashes, Fault tolerance, Information science, Network topology, Fault tolerant systems, distributed databases, System recovery, distributed systems, queue strategy, fault tolerant computing, fault tolerant algorithm, Contracts, Propagation delay]
Preventing state divergence in replicated distributed programs
Proceedings Ninth Symposium on Reliable Distributed Systems
None
1990
Replicated execution of distributed programs, which provides a means of masking hardware (processor) failures in a distributed system, is discussed. Application-level entities (processes, objects) are replicated to execute on distinct processors. Such replica entities communicate by message passing. Nondeterminism within the replicas could cause messages to be processed in nonidentical order, producing a divergence of state. Possible sources of nondeterminism are identified, and a generic mechanism for ensuring that nonfaulty replicas process messages in identical order, thereby preventing state divergence among such replicate entities, is presented.<<ETX>>
[Protocols, message passing, Laboratories, Process control, state divergence preventing, distributed processing, distributed system, Distributed computing, Fault diagnosis, Fault tolerance, Computer languages, replica entities, Operating systems, Bidirectional control, generic mechanism, replicated distributed programs, Hardware, nondeterminism]
Performability evaluation of CSMA/CD and CSMA/DCR protocols under transient fault conditions
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The authors present the results of an evaluation for the CSMA/CD (carrier sense multiple access with collision detection) protocol and a deterministic protocol under workloads anticipated in an industrial environment. Stochastic activity networks are used as the model type, and simulation is used as the solution method. The results show that the preferred resolution scheme depends on the level of workload anticipated and whether transient faults occur. It is seen that stochastic activity networks permit the representation of a relatively complex fault model as well as normal protocol operations. It is shown that, when transient faults are considered, the deterministic collision resolution scheme performs better than the nondeterministic scheme.<<ETX>>
[Performance evaluation, collision detection, Protocols, Microcontrollers, simulation, performance evaluation, industrial environment, stochastic activity networks, deterministic collision resolution scheme, Multiaccess communication, Application software, Delay, deterministic protocol, Electrostatic interference, CSMA/DCR protocols, CSMA/CD, Working environment noise, Electromagnetic interference, fault tolerant computing, transient fault conditions, Electromagnetic transients, protocols, performability evaluation, carrier sense multiple access]
Keeping processes under surveillance
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
Two solutions for the surveillance problem that are based on an election algorithm which has to cope with process and communication failures are described. The election algorithm is presented in detail. The surveillance algorithms are simple and efficient: the central crash detection protocol requires n+1 messages for each surveillance period (assuming that n is the number of processes to keep under surveillance), and the distributed approach requires n messages. If the distributed crash detection approach is used, the election algorithm has to be executed after each crash detection to determine a new ring manager which generates a new token and establishes the virtual ring. In case of a crash detection with the central protocol, a new crash detection manager has to be determined only if the old manager has failed.<<ETX>>
[Protocols, Computational modeling, Nominations and elections, performance evaluation, Computer crashes, virtual ring, Distributed computing, election algorithm, token, Computer science, Fault tolerance, distributed approach, ring manager, Surveillance, central crash detection protocol, Fault tolerant systems, Broadcasting, communication failures, protocols, surveillance problem, process failures, distributed crash detection approach]
File system measurements and their application to the design of efficient operation logging algorithms
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
File system operation in a transparently fault-tolerant system that uses checkpointing and message logging is discussed. Logging messages to disk is one of the primary performance costs of such systems. The author has measured the file system operations performed on large timesharing systems running Unix in terms of the level of concurrency (number of consecutive operations that do not change the state of the file system). By performing much of the data analysis online within a modified Unix kernel, statistics were collected over a long period of time with a substantial variation in system load. Using this data, it is demonstrated that a technique called null logging can reduce the number of messages logged to disk by a factor of 10 to 25, depending on the workload. This reduces the overhead of the fault-tolerance mechanism and allows a large fraction of file system operations to commit instantaneously.<<ETX>>
[Checkpointing, Algorithm design and analysis, checkpointing, Unix, Costs, timesharing systems, time-sharing systems, transparently fault-tolerant system, file system measurements, null logging, Application software, operation logging algorithms, concurrency, Computer science, Concurrent computing, message logging, Fault tolerance, performance costs, File systems, Fault tolerant systems, file organisation, fault tolerant computing, Kernel]
Efficient transient simulation of failure/repair Markovian models
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
Simulation methods for the solution of the extremely large Markovian dependability models which result from complex fault-tolerant computer systems have recently been developed. The author presents efficient simulation methods for the estimation of transient reliability/availability metrics for repairable fault-tolerant computer systems which combine estimator decomposition techniques with an efficient importance sampling technique. Comparison with simulation methods previously proposed for the same type of metrics and models shows that the proposed methods are orders of magnitude faster.<<ETX>>
[Availability, failure/repair Markovian models, availability metrics, Computational modeling, Computer simulation, importance sampling technique, Steady-state, State-space methods, transient reliability, Monte Carlo methods, Fault tolerant systems, Markov processes, complex fault-tolerant computer systems, Sampling methods, estimator decomposition, fault tolerant computing, transient simulation, Context modeling]
Ordered broadcasts for large applications
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The problem of broadcasting messages to a large number of hosts in a network is considered. The work of propagating a message is distributed and minimized among the hosts by arranging them into a minimum-cost spanning tree. Only the cooperation of the hosts that are supposed to receive these messages is required. The proposed protocol guarantees eventual and ordered delivery despite transients, and at most k permanent failures. This is achieved without requiring a large amount of information to be stored and maintained by each host, even when many hosts may initiate broadcast. Propagation delays and message overhead are derived for the proposed protocol.<<ETX>>
[Protocols, propagation delays, performance evaluation, minimum-cost spanning tree, Routing, Application software, Computer science, Network servers, protocol, Databases, permanent failures, broadcasting messages, delays, Broadcasting, ordered broadcasts, Libraries, message overhead, protocols, Catalogs, Propagation delay]
Flexible handling of diverse dependability requirements in MARS
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The author analyzes variants of the MARS architecture for their handling of reliability, safety, and availability requirements. In order to provide these requirements, different strategies for tuning the MARS system have been applied: the installation of shadow components for reducing the probability of spare exhaustion failures, studies of the impact of the maintenance intervals on the system's dependability, and increasing the error detection coverage by time redundant execution of application tasks and by checking signatures of received messages. The results have shown that the fail-silent coverage of components is the most sensitive parameter, and a coverage value close to one is a necessary prerequisite for this type of architecture. The use of general purpose hardware does not provide a sufficient fail-silent behavior, even if sophisticated error detection mechanisms at the operating system level and application software level are used.<<ETX>>
[spare exhaustion failures, Costs, maintainable real-time systems, Humans, reliability, distributed processing, Control systems, operating system level, error detection, Mars, availability requirements, error detection coverage, fail-silent coverage, application tasks, checking signatures, computer architecture, safety, Computer architecture, Hardware, Safety, fault tolerant distributed real-time system architecture, Availability, probability, Application software, application software level, diverse dependability requirements, real-time systems, MARS, Computer applications, general purpose hardware, fault tolerant computing, flexible handling, maintenance intervals]
Optimistic failure recovery for very large networks
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
Optimistic failure recovery mechanisms are proposed as a way to provide transparent fault tolerance to distributed applications and systems. The authors identify problems that may arise when these mechanisms are applied to vast networks including many processors and spanning large geographical areas and many administrative domains. They present a technique-recovery unit gateways-that can be used to address many of these issues with existing failure recovery algorithms. This method can be applied with minimal disruption to existing transparent recovery systems, as well as to build large optimistic recovery systems while minimizing the dependency tracking overhead.<<ETX>>
[administrative domains, optimistic failure recovery, dependency tracking overhead, distributed processing, recovery unit gateways, Electronic mail, Application software, Distributed computing, distributed applications, Programming profession, Distributed processing, transparent fault tolerance, Fault tolerant systems, Memory management, very large networks, Computer applications, distributed systems, fault tolerant computing, System software, Local area networks]
Formalising replicated distributed processing
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The authors present a novel formal approach to proving the correctness of distributed systems of replicated processes that communicate by message passing. The notion of correctness introduced is based on the consistency of the replicated system with its nonreplicated counterpart. The formal framework of CSP (communicating sequential processes) allows the proof of partial correctness and deadlock-freedom properties of the systems of replicated processes. The authors also discuss how a replicated process may be implemented by N-base copies, a majority of which are non-faulty, and point out the necessity of coordinating the copies and the requirements they should satisfy.<<ETX>>
[correctness proving, replicated distributed processing formalising, N-base copies, programming theory, message passing, program verification, communicating sequential processes, Laboratories, Redundancy, distributed processing, consistency, deadlock-freedom properties, formal approach, Distributed processing, Fault tolerance, Voting, partial correctness, Communication channels, System recovery, distributed systems, fault tolerant computing, Nuclear magnetic resonance, Neck, Mathematical model]
A study of the reliability of Internet sites
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
Failure and repair rates of components are often assumed to be exponentially distributed. This hypothesis is testable for failure rates, though the process of gathering and reducing the data to a usable form can be difficult. By applying an appropriate test statistic, some samples were found to have a realistic change of being drawn from an exponential distribution, while others can be confidently classed as nonexponential. Data were collected from a large number of hosts via the Internet. Almost all of the visible Internet (over 350000 hosts) were considered, and more than 68000 of these that were judged likely to respond were queried. These hosts were sampled several times to obtain up-times, and finally to determine average host availability. Estimates of availability, mean-time-to-failure, and mean-time-to-repair were derived. The results reported correspond with those commonly seen in practice.<<ETX>>
[Availability, Costs, mean-time-to-repair, Internet sites, availability, Application software, Sun, Distributed computing, Fault tolerance, exponential distribution, mean-time-to-failure, Statistical distributions, reliability study, fault tolerant computing, Internet, Mathematical model, protocols, average host availability, Testing]
Checkpointing multicomputer applications
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The authors present a checkpointing scheme that is transparent, imposes overhead only during checkpoints, requires minimal message logging, and allows for quick resumption of execution from a checkpointed image. Since checkpointing multicomputer applications poses requirements different from those posed by checkpointing general distributed systems, existing distributed checkpointing schemes are inadequate for multicomputer checkpointing. The proposed checkpointing scheme makes use of special properties of multicomputer interconnection networks to satisfy this set of requirements. The proposed algorithm is efficient both when taking checkpoints and when recovering from checkpointed images.<<ETX>>
[Checkpointing, checkpointing scheme, Resumes, multiprocessor interconnection networks, minimal message logging, performance evaluation, Transaction databases, Application software, multicomputer interconnection networks, Computer science, Power system interconnection, Distributed databases, Computer applications, Time sharing computer systems, Hardware, fault tolerant computing, multicomputer applications]
A fault-tolerant, scalable, low-overhead distributed garbage detection protocol
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The author presents a protocol for the distributed detection of garbage in a distributed system subject to common failures such as lost and duplicated messages, network partition, dismounted disks, and process, site, and disk crashes. The protocol uses only information local to each site, or exchanged between pairs of sites; no global mechanism is necessary. Overhead is low. The protocol is parallel and should scale to extremely large systems.<<ETX>>
[Out of order, network partition, scalable protocol, Access protocols, distributed processing, distributed system, Computer crashes, low-overhead distributed garbage detection protocol, Programming profession, Counting circuits, Fault tolerance, storage management, Fault detection, fault tolerant protocol, dismounted disks, Object detection, Detectors, Database systems, fault tolerant computing, protocols, common failures]
Masking failures of multidimensional sensors
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
A methodology for transforming a process control program that cannot tolerate sensor failure into one that can is presented. In this methodology, a reliable abstract sensor is created by combining information from several real sensors that measure the same physical value. To be useful, an abstract sensor must deliver reasonably accurate information at reasonable computational cost. The authors consider sensors that deliver multidimensional values (e.g. location or velocity in three dimensions). Geometric techniques are used to derive upper bounds on abstract sensor accuracy and to develop efficient algorithms for implementing abstract sensors.<<ETX>>
[Chemical sensors, Actuators, geometric techniques, Multidimensional systems, Uncertainty, Redundancy, Process control, failures masking, upper bounds, Fault tolerance, real-time systems, process computer control, process control program, Thermal sensors, multidimensional sensors, Concrete, fault tolerant computing, reliable abstract sensor, electric sensing devices, Contracts]
Efficient communication of commitment-dependency information in the programmer-transparent coordination (PTC) scheme for cooperative recovery
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The programmer-transparent coordination (PTC) scheme is an approach that facilitates efficient cooperative backward recovery from errors caused by software faults and latent hardware faults in distributed computer systems. The efficiency of handling the information on commitment-dependency is a major factor that determines the overhead incurred by the PTC scheme. The authors establish a principle on efficient identification and internode communication of the dynamically changing commitment-dependency among the distributed processes. Previously the principle of representing and communicating the commitment-dependency information in the form called the direct potential recaller set (DPRS) descriptor was established. It is shown that it is sufficient to maintain and communicate a part of the DPRS descriptor called the primary direct potential recaller set descriptor. This could mean substantial reductions in time and storage overhead associated with the PTC scheme in many applications.<<ETX>>
[distributed computer systems, latent hardware faults, Software algorithms, Redundancy, distributed processing, programmer-transparent coordination, Application software, Distributed computing, software faults, commitment-dependency information, Fault detection, cooperative recovery, direct potential recaller set, backward recovery, Distributed control, Computer errors, Broadcasting, Hardware, fault tolerant computing, storage overhead, Local area networks, errors]
On tolerating faults in naturally redundant algorithms
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
A class of algorithms suitable for fault-tolerant execution in multiprocessor systems by exploiting the existing embedded redundancy in the problem variables is characterized. Because of this unique property, no extra computations need be superimposed on the algorithm in order to provide redundancy for fault recovery, as well as fault detection in some cases. A forward recovery scheme is thus used with very low time overhead. The method is applied to the implementation of two iterative algorithms: solution of Laplace equations by Jacobi's method and the calculation of the invariant distribution of a Markov chain. Experiments show less than 15% performance degradation for significant problem instances in fault-free situations, and as low as 2.43% in some cases. The extra computation time needed for locating and recovering from a detected fault does not exceed the time necessary to execute a single iteration. The fault-detection procedures provide fault coverage close to 100% for faults causing errors that affect the correctness of the computations.<<ETX>>
[iterative methods, Laplace equations, multiprocessing systems, Redundancy, multiprocessor systems, fault detection, embedded redundancy, Laplace transforms, Multiprocessing systems, Jacobian matrices, Degradation, fault recovery, Markov chain, Fault tolerance, Fault detection, naturally redundant algorithms, Fault tolerant systems, Markov processes, Hardware, Iterative algorithms, fault tolerant computing, forward recovery scheme, Jacobi's method, iterative algorithms]
Copying garbage collection for distributed object stores
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The authors describe a garbage collection scheme for managing a distributed object store in which accessible objects survive system failures. The scheme is based on copying garbage collection and has the following advantages: it is tolerant to node failures, a fail-stop behavior of the node is assumed; it is partial, a given execution of the distributed garbage collection does not need to involve all the nodes, that is, the garbage collector can collect garbage on a subset of the entire system nodes; it minimizes disk accesses; it can collect any kind of cycles both within a node and among different nodes; it uses the depth-first-search to increase locality; it is iterative and it does not assume any particular primitives of the operating system of a node. In addition this distributed garbage collector can be extended to be asynchronous, that is, every node can decide to begin the collection at any time independently of the others.<<ETX>>
[fail-stop behavior, operating system, distributed object stores, garbage collection, Distributed computing, depth-first-search, Fault tolerance, Computer languages, storage management, Parallel programming, File systems, copying, Database systems, fault tolerant computing]
A statistical clock synchronization algorithm for anisotropic networks
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
A method to estimate the value of remote clocks in distributed systems is proposed. The method is able to deal with isotropic and anisotropic networks and includes a way to detect performance failures on single exchanges. It uses a statistical approach to estimate the relative drift of clocks and a round trip clock reading protocol to compute the offset. A good precision can be attained and maintained without exchanging too many specific extra messages. Numerical results obtained from a discrete event simulation are presented.<<ETX>>
[isotropic networks, Protocols, Art, remote clocks, round trip clock reading protocol, statistical clock synchronization algorithm, Probability, distributed processing, Time measurement, Synchronization, Distributed computing, Delay, synchronisation, clocks, Anisotropic magnetoresistance, performance failures, Broadcasting, distributed systems, anisotropic networks, protocols, discrete event simulation, Clocks]
Flexible schemes for application-level fault tolerance
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
It is pointed out that the design of fault-tolerance provisions in the application level is normally necessary, but difficult and error-prone due to its ad-hoc nature. Structuring schemes have been proposed to reduce the difficulty of this task, but they appear too restrictive for the building of large, heterogeneous applications. The redundant structures that can be used in the individual components of a system depend on their requirements or inherent characteristics; it would be useful to combine components using different basic schemes. As an example, the authors propose a solution for interfacing components using conversations for backward recovery with components using atomic transactions. Constraints for the designers of the components to be interfaced and requirements on the virtual machine supporting their execution are defined. Ways a classification of components could be organized to allow the formulation of more general solutions are discussed.<<ETX>>
[atomic transactions, Buildings, Humans, Virtual machining, Application software, interfacing components, Delay, Fault tolerance, Runtime, virtual machine, Computer applications, virtual machines, backward recovery, Computer errors, Parallel processing, fault tolerant computing, application-level fault tolerance]
Dependability evaluation of bus and ring communication topologies for the Delta-4 distributed fault-tolerant architecture
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
The authors report a study of the dependability of the various communication topologies that can be used to construct a Delta-4 system. Single and dual bus and ring configurations are possible (based on 802.4, 802.5, and FDDI standards); the authors give closed-form expressions for the reliability and availability of each topology when repair is taken into account. It is shown that the dimensioning parameter in the dependability of the communication system is the coverage of the self-checking mechanisms built into the network attachment controllers.<<ETX>>
[ring communication topologies, reliability, distributed processing, Fault tolerance, Fault tolerant systems, bus communication topologies, network attachment controllers, Computer architecture, FDDI standards, Hardware, Computer networks, dimensioning parameter, Delta-4 distributed fault-tolerant architecture, Local area networks, Availability, self-checking mechanisms, LAN interconnection, 802.5, closed-form expressions, Topology, 802.4, standards, fault tolerant computing, Telecommunication network reliability, token networks, dependability evaluation]
On the testability of distributed real-time systems
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
Event-triggered and time-triggered systems are compared with respect to their testability. To facilitate the comparison, three aspects of testability are introduced: test coverage, observability, and controllability. After a brief review of the problems of testing concurrent programs, the author compares the two system architectures with respect to each of these three aspects. The results of the comparisons favor time-triggered systems in all three aspects, most notably with respect to controllability and test coverage. Thus, time-triggered systems are inherently more testable than event-triggered systems. A series of experiments is summarized that shows that it is easy to achieve reproducible testing of time-triggered systems.<<ETX>>
[Real time systems, Context, system architectures, System testing, Costs, program testing, distributed real-time systems, distributed processing, Explosions, testability, concurrent programs, Distributed computing, parallel programming, time-triggered systems, Sequential analysis, controllability, real-time systems, event triggered systems, Controllability, test coverage, reproducible testing, Timing, Observability, observability]
A model for interface groups
[1991] Proceedings Tenth Symposium on Reliable Distributed Systems
None
1991
How a model for interface groups can be integrated with the ANSA computational model is discussed. The result is a uniform model for one-to-one, one-to-many, many-to-one, and many-to-many communication. Whether a service is provided by a single server or distributed over a collection of servers cannot be inferred from the interface to the service. The proposed model thus provides full transparency of groups, and-if groups are used to support replication-full replication transparency. The interface group model is more general than those of ISIS and CIRCUS. In the prototype implementation of interface groups, the multi-endpoint communication protocol is implemented on top of a communication package with synchronous RPCs. The protocol ensures total order if a client uses RPC or no order if a client uses asynchronous calls.<<ETX>>
[Computer interfaces, Protocols, Computational modeling, Instruction sets, ANSA computational model, Europe, replication transparency, communication protocol, Distributed computing, computer communications software, uniform model, single server, interface groups, Prototypes, transparency, communication package, Packaging, Parallel processing, Robustness, model, protocols, synchronous RPCs, asynchronous calls]
Availability of coding based replication schemes
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
The availability of a coding-based replication scheme where simple voting is used to maintain correctness of replicated data is evaluated. It is shown that the storage requirement for maintaining the data with a given availability is reduced significantly. The ways that some of the extensions of the voting scheme can be modified to manage this coding-based replication are also described. The availability of these is evaluated, and the reduction in the storage space requirements achieved is studied.<<ETX>>
[Availability, storage space requirements, correctness, Educational institutions, data integrity, Computer science, simple voting, storage requirement, Voting, distributed databases, coding-based replication scheme, computational complexity, replicated data]
Managing replicated data in heterogeneous database systems
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
In a heterogeneous database system, supporting transaction semantics is expensive, and sometimes impossible. thus, in such systems, traditional methods such as quorum consensus cannot be used directly for managing replicated data. A method to manage replicated data in a heterogeneous database system is proposed. The method is based on the idea of quorum consensus but does not rely on transaction semantics. It is more robust than the other methods introduced in the literature, and can be used in systems where one copy serializability is the correctness criteria.<<ETX>>
[transaction processing, transaction semantics, correctness criteria, quorum consensus, Banking, Control systems, Concurrency control, Transaction databases, database theory, Computer science, copy serializability, Voting, distributed databases, Database systems, Robustness, Data models, heterogeneous database system]
xAMp: a multi-primitive group communications service
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
The xAMp is a highly versatile group communications service aimed at supporting the development of distributed applications with different dependability, functionality, and performance requirements. These range from unreliable and nonordered to atomic multicast, and are enhanced by efficient group addressing and management support. The basic protocols are synchronous, clockless and designed to be used over broadcast local area networks (LANs), and are portable to a number of them. The functionality provided yields a reasonably complete solution to the problem of reliable group communication. While other protocols offer similar services, the authors follow a novel engineering approach by deriving all qualities of services from a single basic procedure. Thus, their implementation shares data structures, procedures, failure-recovery algorithms, and group monitor services, resulting in a highly integrated package.<<ETX>>
[group addressing, Protocols, single basic procedure, highly versatile group communications service, Quality of service, reliability, group monitor services, Reliability engineering, local area networks, functionality, system recovery, distributed applications, management support, Condition monitoring, highly integrated package, groupware, Broadcasting, multi-primitive group communications service, data structures, Local area networks, performance requirements, failure-recovery algorithms, dependability, reliable group communication, Data structures, atomic multicast, novel engineering approach, broadcast local area networks, Packaging, LANs, fault tolerant computing, xAMp, Telecommunication network reliability, Clocks]
Dynamic management of highly replicated data
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
An efficient replication control protocol, called the dynamic group protocol, for managing replicated data objects that have more than five replicas is presented. Like the grid protocol, the dynamic group protocol requires only O( square root n) messages per access to enforce mutual consistency among n replicas. Unlike other protocols aimed at providing fast access, this protocol adapts itself to changes in site availability and network connectivity, which allows it to tolerate n-2 successive replica failures. The availability of a replicated object consisting of n replicas managed by the dynamic group protocol when the n replicas are on the same LAN segment, is evaluated under standard Markovian assumptions and found to equal that of an object with the same number of replicas, managed by the dynamic-linear voting protocol.<<ETX>>
[Availability, LAN segment, efficient replication control protocol, dynamic group protocol, Access protocols, dynamic-linear voting protocol, local area networks, standard Markovian assumptions, Computer science, Fault tolerance, Content addressable storage, Voting, site availability, mutual consistency, network connectivity, distributed databases, Markov processes, replicated object, Robustness, Large-scale systems, protocols, Local area networks, successive replica failures]
Reliable broadcasting in faulty hypercube computers
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
A nonredundant broadcasting algorithm for faulty hypercube computers is proposed. The concept of unsafe nodes is introduced to identify those nonfaulty nodes that will cause a detour or backtracking because of their proximity to faulty nodes. It is assumed that each healthy node, safe or unsafe, knows the status of all the neighboring nodes. The broadcasting is optimal, meaning that a message is sent to each node via a Hamming distance path if the broadcasting is initiated from a safe node. It is also shown that when the source node is unsafe and there is an adjacent safe node, then the broadcasting can be achieved with only one more time step than the fault-free case.<<ETX>>
[Tree data structures, nonfaulty nodes, Hamming distance, fault-free case, software reliability, Reliability engineering, hypercube networks, Hamming distance path, faulty hypercube computers, Computer science, Fault diagnosis, Fault tolerance, Casting, nonredundant broadcasting algorithm, adjacent safe node, Broadcasting, Hypercubes, fault tolerant computing, backtracking, unsafe nodes, source node]
The two-phase commit performance of the DECdtm services
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
The performance of the two-phase commit protocols implemented by Digital Equipment Corporation's (DEC)'s distributed transaction manager (DECdtm V1.1) is characterized. Throughput, response time, resource utilization and other performance aspects of reliable distributed transactions are analyzed. Efficient implementation and effective group-commit allowed DECdtm to reach up to 176 transactions per second on a single VAX8700 (6 VUPs) processor, and up to 76 distributed transactions per second between two VAX8700 nodes.<<ETX>>
[Wide area networks, transaction processing, Protocols, DECdtm services, Software performance, performance aspects, Distributed computing, two-phase commit protocols, Voice mail, two-phase commit performance, group-commit, Voting, response time, distributed transaction manager, Open systems, distributed databases, distributed transactions, reliable distributed transactions, VAX8700, Resource management, protocols, resource utilization, Local area networks, Testing]
Self-stabilizing real-time rule-based systems
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
The problem of automated recovery in distributed real-time rule-based systems where internal variables may be corrupted during computation as a result of transient faults is discussed. Given a distributed rule-based program p with bounded response time, the problem is to derive a self-stabilizing program q that implements p with the constraint that q must also have bounded response time. An approach for solving this problem for a class of rule-based programs with bounded response time is presented.<<ETX>>
[Real time systems, Algorithm design and analysis, Embedded computing, Knowledge based systems, distributed processing, automated recovery, distributed real-time rule-based systems, State-space methods, transient faults, system recovery, Delay, Space vehicles, self-stabilizing program, Embedded system, knowledge based systems, real-time systems, fault tolerant computing, Timing, Safety, bounded response time]
Neural networks for the design of distributed, fault-tolerant, computing environments
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
Binary optimization models for the design of distributed, fault-tolerant computing systems are considered, with a focus on the task allocation and file assignment modeling schema proposed by J. Bannister and K. Trivedi (Proc. Second Symp. on Reliability in Distributed Software and Database Systems, 1982). It is shown that R. Graham's (1969) partitioning algorithm, S, when applied to this schema can, in the case of finite resources, yield allocations that are arbitrarily poor with respect to the optimum allocation. This contrasts sharply with the case of ample resources, where S provides allocations that are provably close to the optimum. Two alternative allocation algorithms are suggested. Both are seen to deliver allocations preferable to those provided by S, but at some additional computational expense.<<ETX>>
[fault-tolerant computing, partitioning algorithm, optimum allocation, distributed processing, system design, binary optimisation models, Distributed computing, Design optimization, file assignment modeling, Fault tolerance, task allocation, Shape control, optimisation, Neural networks, Fault tolerant systems, Hopfield neural networks, Simulated annealing, distributed systems, Computer networks, fault tolerant computing, Resource management, neural nets]
Communication protocols for fault-tolerant clock synchronization in not-completely connected networks
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
Communications protocols for not-completely-connected networks are presented, and their cost is evaluated in terms of message exchanges. An efficient protocol tailored to convergence function clock synchronization is introduced. The number of messages used by this approach is equal to a proven lower bound on the number of messages and, hence, the approach is optimal. This protocol can be combined with a convergence function algorithm to achieve fault-tolerant clock synchronization in not-completely-connected networks at a far lower cost than previous approaches.<<ETX>>
[Protocols, Costs, message passing, cost, message exchanges, communications protocols, Synchronization, Relays, Convergence, clocks, Fault tolerance, Intelligent networks, Waste materials, not-completely-connected networks, Fault tolerant systems, fault tolerant computing, protocols, convergence function clock synchronization, fault-tolerant, Clocks]
A multi-paradigm programming language for constructing fault-tolerant, distributed systems
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
The design of FT-SR, a programming language based on SR and oriented toward constructing fault-tolerant distributed systems, is presented. The language, which is based on the existing SR language, is unique in that it has been designed to support equally well any of the programming paradigms that have been developed for this type of system, including the object/action model, the restartable action paradigm, and the state machine approach. To do this, the language is designed to support the implementation of systems modeled as collections of fail-stop atomic objects. Such objects execute operations as atomic actions except when a failure or series of failures cause underlying implementation assumptions to be violated; in this case, notification is provided. An example program consisting of a data manager and its associated stable storage is given.<<ETX>>
[Redundancy, high level languages, data manager, distributed processing, multi-paradigm programming language, fail-stop atomic objects, restartable action paradigm, Yarn, state machine approach, Strontium, Computer science, Computer languages, Fault tolerance, Fault tolerant systems, SR language, Genetic programming, Writing, Libraries, fault tolerant computing, stable storage, fault-tolerant distributed systems, FT-SR]
The performance of consistent checkpointing
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
Consistent checkpointing provides transparent fault tolerance for long-running distributed applications. Performance measurements of an implementation of consistent checkpointing are described. The measurements show that consistent checkpointing performs remarkably well. Eight computation-intensive distributed applications were executed on a network of 16 diskless Sun-3/60 workstations, and the performance without checkpointing was compared to the performance with consistent checkpoints taken at two-minute intervals. For six of the eight applications, the running time increased by less than 1% as a result of the checkpointing. The highest overhead measured was 5.8%. Incremental checkpointing and copy-on write checkpointing were the most effective techniques in lowering the running time overhead. It is argued that these measurements show that consistent checkpointing is an efficient way to provide fault tolerance for long-running distributed applications.<<ETX>>
[Checkpointing, Measurement, Performance evaluation, Costs, distributed processing, File servers, Magnetic heads, data integrity, Application software, Computer science, Fault tolerance, transparent fault tolerance, consistent checkpointing, diskless Sun-3/60 workstations, fault tolerant computing, Workstations, computation-intensive distributed applications, long-running distributed applications, copy-on write checkpointing]
Optimistic message logging for independent checkpointing in message-passing systems
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
Message-passing systems with a communication protocol transparent to the applications typically require message logging to ensure consistency between checkpoints. A periodic independent checkpointing scheme with optimistic logging to reduce performance degradation during normal execution while keeping the recovery cost acceptable is described. Both time and space overhead for message logging can be reduced by detecting messages that need not be logged. A checkpoint space reclamation algorithm is presented to reclaim all checkpoints which are not useful for any possible future recovery. Communication trace-driven simulation for several hypercube programs is used to evaluate the techniques.<<ETX>>
[Checkpointing, Costs, message passing, multiprocessing programs, communication trace-driven simulation, NASA, optimistic message logging, Access protocols, hypercube networks, communication protocol, performance degradation, message-passing systems, Degradation, Runtime, Message passing, recovery cost, hypercube programs, Hypercubes, Software systems, fault tolerant computing, checkpoint space reclamation algorithm, protocols]
Optimal replica control protocols for ring networks
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
Distributed computing environments are expected to offer highly available services. Replication of data is one of the main techniques used to achieve this goal. Protocols that achieve optimal performance in replicating data for ring networks are discussed. Coteries, proposed by H. Garcia-Molina and D. Barbara (1985), provide the most general framework for analyzing static pessimistic protocols. It is shown that a simple voting scheme, a very small class of the coterie scheme, provides optimal performance for several variations of the ring interconnection topology.<<ETX>>
[Protocols, ring interconnection topology, Poles and towers, computer networks, optimal performance, coterie scheme, Concurrency control, Transaction databases, ring networks, Distributed computing, Network topology, static pessimistic protocols, Voting, Optimal control, Distributed databases, distributed databases, Database systems, protocols, simple voting scheme]
Sensitivity and uncertainty analysis in performability modelling
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
The authors present an exact Taylor series model of steady state performability measures and discuss the validity region of this approximation. They also describe an uncertainty analysis approach based on Monte Carlo simulation, apply both approaches to a performability mode, and discuss their relative merits. They vary the dependencies between model parameters and discuss their influence. For the case studies done, the sensitivity analysis overestimated the mean performability and underestimated its variance.<<ETX>>
[Performance evaluation, Uncertainty, Sensitivity analysis, exact Taylor series model, steady state performability measures, series (mathematics), validity region, distributed processing, performance evaluation, uncertainty analysis approach, Taylor series, uncertainty handling, Steady-state, Distributed computing, mean performability, Monte Carlo methods, model parameters, Fault tolerant systems, Failure analysis, Open systems, Performance analysis, Monte Carlo simulation, performability mode]
A replicated object server for a distributed object-oriented system
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
The design and implementation of a replicated object server called Goofy, which is intended to provide a fault-tolerant storage system for distributed object-oriented applications, are described. Goofy supplies the object storage of the GUIDE (Grenoble Universities Integrated Distributed Environment) distributed object-oriented system with integrity, reliability and availability, while preserving object consistency despite network partition failures. Some preliminary experiments are described. Goofy is compared with related work on replicated servers, and the lessons drawn from this work and some conclusions on fault tolerance in GUIDE are given.<<ETX>>
[object storage, network partition failures, fault-tolerant storage system, Grenoble Universities Integrated Distributed Environment, replicated servers, software reliability, reliability, availability, GUIDE, Vehicles, Fault tolerance, Network servers, integrity, Operating systems, Fault tolerant systems, Prototypes, distributed databases, object consistency, object-oriented methods, Availability, fault tolerance, Object oriented modeling, Goofy, data integrity, Power system modeling, replicated object server, Computer languages, distributed object-oriented applications, fault tolerant computing]
Distributed problem solving in spite of processor failures
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
Processor failures not leading to a network partition are considered, and the issue of computing associative functions in spite of processor failures is addressed. An intuitive and fundamental result formally proved is that failure detection and computing associative functions are equivalent in faulty networks: one can be performed if and only if the other can be performed. Protocols and impossibility results in various system models are presented in the context of computing associative functions and solving topological problems.<<ETX>>
[network partition, Protocols, topological problems, processor failures, distributed processing, Paper technology, Distributed computing, distributed problem solving, associative functions, Computer science, Fault tolerance, Intelligent networks, system models, Fault detection, faulty networks, Computer networks, fault tolerant computing, theorem proving, Problem-solving, failure detection, Clocks]
Simulation of the Adapt on-line diagnosis algorithm for general topology networks
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
As dependence on wide-area and other point-to-point networks increases, the need for diagnosis of the distributed resources becomes critical. Continuous online distributed diagnosis at the system-level provides a desirable solution. The Adapt algorithm, which performs online adaptive distributed diagnosis in arbitrary networks in the presence of node failures, is examined. Simulation results depicting operation for various network topologies are given, including test and message counts and diagnosis latency. The results indicate that Adapt performs significantly better than derived worst-case bounds. The best- and worst-case performance bounds are analyzed.<<ETX>>
[Algorithm design and analysis, Adapt, wide area networks, Computational modeling, Computer simulation, on-line diagnosis algorithm, computer networks, simulation, distributed processing, online adaptive distributed diagnosis, Delay, Fault diagnosis, Sequential analysis, Analytical models, Network topology, distributed resources, fault tolerant computing, Performance analysis, point-to-point networks, arbitrary networks, node failures, Testing]
On the verification and validation of protocols with high fault coverage using UIO sequences
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
Various new classes of unique input/output (UIO) sequences for verification and validation (conformance testing) of protocols modeled as finite state machines (FSMs) are presented. The proposed sequences are referred to as adaptive because test sequence generation is not a mere concatenation of test subsequences for all edges of the FSM, but rather subsequences are concatenated using appropriate conditions in the UIO sequence for length minimization and no degradation of fault coverage.<<ETX>>
[protocol verification, Protocols, test sequence generation, Concatenated codes, fault coverage, conformance testing, finite state machines, Sun, Distributed computing, Certification, Degradation, Computer science, Sequential analysis, formal verification, Automata, fault tolerant computing, protocols, validation, Testing, input output sequence]
Dataflow-like languages for real-time systems: issues of computational models and notation
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
The use of dataflow-like models for the in-the-large design of real-time applications is discussed. In these models, modules can only communicate by (asynchronously) receiving messages when activated and transmitting result messages when terminating. This rather restrictive computational model allows the description of typical, cyclic control programs, with predictable, well-verifiable behavior. In particular, important timing properties can be dealt with in the in-the-large design. The case for the use of dataflow-like models is outlined, and the choice of appropriate notations, which implies a tradeoff between predictability of behavior and expressive power, and the potential for an advanced design support environment are discussed.<<ETX>>
[Real time systems, Computational modeling, parallel architectures, Redundancy, dataflow-like models, timing, Predictive models, diagrams, parallel architecture, Application software, Power system modeling, Computer languages, design support environment, real-time systems, specification languages, computational model, Software systems, Timing, Bonding]
K/sub I//sup T/L/sub G//sup O/: a generic logging service
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
A generic logging service should center to the variable and even antagonistic needs of clients, without imposing unnecessary overhead on clients that do not use all of its functions. A solution to this problem, called K/sub I//sup T/L/sub G//sup O/, is described in detail. It solves the problem by separating logging characteristics into five mechanisms: buffering policy, distribution of records, replication of records, sharing of logs, and management of physical media. Each characteristic is embodied in a class. For each class, multiple policy implementations can be provided. Instances of these classes are stackable in any appropriate number or order. A client customizes his log to a particular set of failure assumptions by selecting adequate classes, instantiating them, and connecting the instances together.<<ETX>>
[replication, Protocols, multiprocessing programs, Debugging, utility programs, buffering, KITLOG, reliability mechanisms, record distribution, generic logging service, Concurrent computing, Fault tolerance, physical media, File systems, Operating systems, Intersymbol interference, fault tolerant computing, Joining processes, Sprites (computer), Computer buffers, class]
The sentry system
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
A system that observes the execution of each loop in any given sequential program and issues a warning if some loop execution does not terminate as expected (due to a programming error or a failure) is proposed. At each iteration of a loop execution, the program writes the current values of some variables into shared storage; these values are read later by another program called the sentry. The sentry uses these values to compute the loop's termination function at the current iteration, and issues a warning if successive values of the termination function are not monotonically decreasing. The shared storage between the program and the sentry is finite, the program never waits for the sentry during execution, and some form of mutual exclusion is achieved between the program and the sentry. Extensions of the system and an implementation of a prototype are described.<<ETX>>
[Real time systems, loop execution, Costs, programming error, Debugging, mutual exclusion, system monitoring program, prototype, shared storage, supervisory programs, Fault detection, Prototypes, Computer errors, system monitoring, sentry system, Safety, sequential program, Monitoring, programming]
Error attenuation in distributed systems
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
A method for achieving fault-tolerance in distributed systems is proposed and compared with the known method of error masking based on distributed voting over the results of several variants. The method is called error attenuation, since the errors produced by nonpermanent faults attenuate eventually during the further execution of the variants. The term attenuation is used to represent the fact that error masking is obtained not in one step, but gradually, in several steps. The proposed method for error attenuation is studied for systems with distributed majority voting and with distributed plurality voting, and for systems implementing adaptive strategy.<<ETX>>
[distributed plurality voting, Adaptive systems, error attenuation, fault-tolerance, Redundancy, distributed processing, Throughput, distributed majority voting, Delay, error masking, distributed voting, Voting, nonpermanent faults, Computer errors, Attenuation, distributed systems, Hardware, fault tolerant computing, Error correction, Nuclear magnetic resonance, adaptive strategy]
Dependability analysis of distributed computing systems using stochastic Petri nets
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
Models based on stochastic Petri nets are developed to estimate the reliability/availability of distributed systems. The modeling approach discussed combines stochastic Petri net modeling and previous reliability algorithms so that not only can failures of nodes and links be modeled, but also the effect of global repairs can be considered. The models can be used to derive estimations on the successful execution of one or more programs distributed in the system. In a distributed computing environment successful execution means that all distributed programs can access all the required remote files even in the presence of faults. A global repair model for a small system is considered in detail. A generalization and the results reported for complex systems were obtained by using the Stochastic Petri Net Package (SPNP).<<ETX>>
[Petri nets, Stochastic processes, reliability, distributed processing, previous reliability algorithms, Distributed computing, distributed computing environment, remote files, complex systems, Fault tolerant systems, distributed systems, stochastic processes, Availability, dependability analysis, Stochastic Petri Net Package, stochastic Petri nets, global repairs, global repair model, Stochastic systems, Fault detection, Distributed control, Packaging, reliability/availability, SPNP, Joining processes, modeling approach, stochastic Petri net modeling]
Global checkpointing for distributed programs
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
A novel algorithm for checkpointing and rollback recovery in distributed systems is presented. Processes belonging to the same program must take periodically a nonblocking coordinated global checkpoint, but only a minimum overhead is imposed during normal computation. Messages can be delivered out of order, and the processes are not required to be deterministic. The nonblocking structure is an important characteristic for avoiding laying a heavy burden on the application programs. The method also includes the damage assessment phase, unlike previous schemes that either assume that an error is detected immediately after it occurs (fail-stop) or simply ignore the damage caused by imperfect detection mechanisms. A possible way to evaluate the error detection latency, which enables one to assess the damage made and avoid the propagation of errors, is presented.<<ETX>>
[Checkpointing, Out of order, message passing, multiprocessing programs, error detection latency, Application software, Proposals, system recovery, Programming profession, Phase detection, Delay, distributed programs, Software design, Operating systems, nonblocking coordinated global checkpoint, application programs, damage assessment phase, Hardware, checkpointing algorithm, rollback recovery]
An efficient decentralized approach to processor-group membership maintenance in real-time LAN systems: the PRHB/ED scheme
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
In constructing highly reliable LAN systems, a mechanism that enables every active node to maintain timely and consistent knowledge about the health status of all cooperating nodes can be used as a cornerstone. The authors consider the case where maintenance of such knowledge is achieved in a decentralized manner and timely and consistent recognition of newly joining nodes is also facilitated. The authors develop an optimal version of H. Kopetz et al.'s (1985) periodic reception history broadcast (PRHB) method. The authors' version enables detection of failures with minimum latency and is called the PRHB with earliest detection (PRHB/ED). This scheme has much shorter latency than the previous PRHB scheme and is yet equally practical in the sense that it does not increase the communication traffic at all, and the complexity of the algorithm for analyzing the observations exchanged among the active nodes is still bounded by a linear function of the number of nodes in the system.<<ETX>>
[Real time systems, complexity, software reliability, Telecommunication traffic, active node, Time division multiplexing, local area networks, History, efficient decentralized approach, Delay, cooperating nodes, Time division multiple access, processor-group membership maintenance, Broadcasting, Local area networks, highly reliable LAN systems, PRHB/ED, Maintenance, real-time LAN systems, health status, newly joining nodes, real-time systems, periodic reception history broadcast, Clocks, consistent knowledge]
The triangular lattice protocol: a highly fault tolerant and highly efficient protocol for replicated data
[1992] Proceedings 11th Symposium on Reliable Distributed Systems
None
1992
A protocol for managing replicated data in which data copies are organized as a triangular lattice is introduced. The smallest quorum size is O( square root N), where N is the number of data copies, which is currently considered optimal for a fully distributed environment. The protocol has the property of graceful degradation. The quorum sizes increase gradually as data copy failures increase. The protocol also has the property of asymptotically high availability, i.e., the availability approaches 1 as the number of data copies goes to infinity if the probability that a data copy available is greater than 0.5.<<ETX>>
[Availability, Costs, fully distributed environment, quorum sizes, data copy failures, quorum size, Lattices, Access protocols, triangular lattice protocol, data copies, Computer science, Degradation, Fault tolerance, Voting, distributed databases, highly fault tolerant, graceful degradation, Database systems, fault tolerant computing, asymptotically high availability, protocols, Distributed algorithms, replicated data]
Processor group membership protocols: specification, design and implementation
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
The specification, design and implementation of a set of protocols to solve the processor group membership problem in distributed systems are presented. These group membership protocols were developed as part of a toolkit for building distributed/parallel applications on a cluster of workstations. The group membership service forms the lowest layer in the toolkit, and is the glue which unifies all other layers. The membership service supports three distinct protocols: weak, strong, and hybrid. These protocols differ significantly in the level of consistency and the number of messages exchanged in reaching agreement. The modular implementation of these protocols and the optimization techniques used to enhance their performance are described.<<ETX>>
[Software prototyping, Protocols, Buildings, Process control, Pulp manufacturing, specification, formal specification, processor group membership problem, workstations, modular implementation, Manufacturing processes, computer communications software, Databases, group membership service, optimization techniques, Load management, distributed systems, Workstations, protocols, group membership protocols, Manufacturing automation, distributed/parallel applications]
APRICOTS a prototype implementation of a ConTract system: management of the control flow and the communication system
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
The principle of transactions has been proven in the field of database systems. However, there are many fields where classical transactions are not suitable to model the actions of the real world. An approach to extend the principle of transactions is the ConTract model, which weakens some demands of the ACID-principle but at the same time includes other features. An approach to implement a ConTract system is introduced. Especially, the implementation issues of a component for reliable control flow management and of a transaction-oriented communication system are discussed.<<ETX>>
[transaction processing, APRICOTS, software reliability, ACID-principle, Communication system control, ConTract model, Control systems, software fault tolerance, parallel programming, transaction-oriented communication system, real world, prototype implementation, reliable control flow management, Prototypes, distributed databases, Automatic control, remote procedure calls, Database systems, Error correction, Protection, Contracts, classical transactions, database systems]
An approach for combinatorial performance and availability analysis
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
The common approach to formulating and solving combined reliability/availability and performance problems is to use Markov reward models. However, the large size of state spaces is a problem that plagues Markovian models. Combinatorial models have been used for modeling reliability and availability of complex systems without paying the price of large Markov models. Yet, assumptions of two-state behavior of components (and that of the system), independence assumptions of component failure behavior, and restrictive repair assumptions decrease the potential of combinatorial models for realistic systems. A combinatorial approach is proposed for the combined performance and availability analysis of coherent repairable systems with multi-state components, allowing inter-dependent component state transitions. Examples showing the usefulness of the approach are presented.<<ETX>>
[Performance evaluation, combinatorial mathematics, Stochastic processes, distributed processing, two-state behavior, inter-dependent component state transitions, restrictive repair assumptions, Markov reward models, availability analysis, combinatorial performance, Markovian models, Performance analysis, independence assumptions, coherent repairable systems, Contracts, Fault trees, Availability, Independent component analysis, performance evaluation, performance problems, State-space methods, Aggregates, Markov processes, component failure behavior, multi-state components, Joining processes]
A secure two phase locking protocol
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
A secure concurrency control algorithm must, in addition to maintaining consistency of the database, be free from covert channels arising due to data conflicts between transactions. The existing secure concurrency control approaches are unfair to transactions at higher access classes. A secure two-phase locking protocol that is shown to be free from covert channels arising due to data conflicts between transactions and that provides reasonably fair execution of all transactions, regardless of their access class, is presented. A description of the protocol for a centralized database system is given, and the extensions that need to be provided in a distributed environment are discussed.<<ETX>>
[Access control, US Department of Energy, covert channels, Data security, secure two phase locking protocol, Access protocols, access class, secure concurrency control algorithm, Concurrency control, data integrity, Transaction databases, access protocols, centralized database system, Computer science, security of data, Information security, concurrency control, distributed databases, Database systems, Timing, data conflicts]
A compositional proof theory for fault tolerant real-time distributed systems
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
A compositional network proof theory for specifying and verifying fault tolerant real-time distributed systems is presented. Important in such systems is the failure hypothesis that stipulates the class of failures that must be tolerated. In the formalism presented, the failure hypothesis of a system is represented by a predicate which expresses how faults might transform the behavior of the system. The approach is illustrated by investigating a triple modular redundant system.<<ETX>>
[Real time systems, program verification, compositional proof theory, reliability, distributed processing, Control systems, Mathematics, Distributed computing, formal specification, software fault tolerance, Aerospace control, Condition monitoring, Patient monitoring, predicate, Hospitals, Fault tolerant systems, real-time systems, failure hypothesis, fault tolerant computing, theorem proving, fault tolerant real-time distributed systems, triple modular redundant system, Aircraft]
Efficient transparent optimistic rollback recovery for distributed application programs
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
A transparent rollback-recovery method that adds very little overhead to distributed application programs and efficiently supports the quick commit of all output to the outside world is introduced. Each process can independently choose at any time either to use checkpointing alone (as in consistent checkpointing) or to use optimistic message logging. The system is based on a new commit algorithm that requires communication with and information about the minimum number of other processes in the system, and supports the recovery of both deterministic and nondeterministic processes.<<ETX>>
[Checkpointing, checkpointing, transparent rollback-recovery method, optimistic message logging, Optimization methods, distributed application programs, commit algorithm, distributed processing, Application software, system recovery, Programming profession, software fault tolerance, Computer science, Concurrent computing, Fault tolerance, Information science, nondeterministic processes, fault tolerant computing, Contracts, Propagation delay]
Bayesian analysis for fault location in homogeneous distributed systems
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
A simple and practical probabilistic comparison-based model, employing multiple incomplete test concepts, for handling fault location in distributed systems using a Bayesian analysis procedure is proposed. This approach is more practical and complete than previous ones since it does not assume any conditions such as permanently faulty units, complete tests, perfect environments, or non-malicious environments. Fault-free systems are handled without overhead; hence, the test procedure may be used to monitor a functioning system. Given a system S with a specific test graph, the corresponding conditional distribution between the comparison test results (syndrome) and the fault patterns of S can be generated. To avoid the complex global Bayesian estimation process, a simple bitwise Bayesian algorithm is developed for fault location in S, which locates system failures with linear complexity, suitable for hard real-time systems.<<ETX>>
[Real time systems, test procedure, System testing, program testing, specific test graph, bitwise Bayesian algorithm, linear complexity, practical probabilistic comparison-based model, Fault location, distributed processing, homogeneous distributed systems, Table lookup, failure analysis, Fault diagnosis, hard real-time systems, multiple incomplete test concepts, functioning system, Bayesian analysis procedure, fault location, fault patterns, Test pattern generators, Computational complexity, Computer science, conditional distribution, Bayesian methods, comparison test results, global Bayesian estimation process, Inference algorithms, Bayes methods]
An O(1) quorum consensus protocol tailored for the client/server architecture
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
In a client/server architecture, replicated copies of an object are assigned to different servers (possibly client sites) in order to increase the availability of the database in case some but not all servers fail. A highly efficient, highly reliable quorum consensus protocol, the Quorum Rings protocol (QRP), specifically designed to take advantage of the client/server architecture, is introduced. The high resiliency of QRP is achieved at a very low cost in communication messages. More specifically, the number of communication messages required for a QRP read or write operation is O(1) in normal mode (when there are no failures in the system) and only O(/spl radic/N) in failure mode (when some but not all servers with replicated objects are faulty). Finally, QRP exhibits the property of graceful degradation: more communication cost is incurred only when the number of failures increases in the system.<<ETX>>
[Costs, communication messages, File servers, communication complexity, Databases, O(1) quorum consensus protocol, failure mode, Computer architecture, distributed databases, Hardware, Workstations, computer network reliability, client/server architecture, protocols, Local area networks, Quorum Rings protocol, client-server systems, replicated copies, Access protocols, resiliency, Petroleum, software fault tolerance, Computer science, client sites, graceful degradation]
Constructing secure distributed systems using components
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
Current hookup theories may impose overstrong security requirements on component systems. To overcome this problem, connectivities of the components may have to be considered in order to appropriately handle their composition. Such a consideration is used here to describe composable security properties. Security requirements are enforced only on some input and output entities of each component with regard to its connectivity, and communication constraints on its others so as to ensure that their entire system can satisfy its security requirement. This enables the system and its components to possess different security properties, i.e., the security property of the system can be logically stronger than security properties of its components.<<ETX>>
[Degradation, Identity-based encryption, security of data, connectivity, distributed processing, communication constraints, hookup theories, Communication system security, security requirements, composable security properties, secure distributed systems]
Using logging and asynchronous checkpointing to implement recoverable distributed shared memory
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
Distributed shared memory provides a useful paradigm for developing distributed applications. As the number of processors in the system and running time of distributed applications increase, the likelihood of processor failure increases. A method of recovering processes running in a distributed shared memory environment which minimizes lost work and the cost of recovery is desirable so that long-running applications are not adversely affected by processor failure. A technique for achieving recoverable distributed shared memory which utilizes asynchronous process checkpoints and logging of pages accessed via read operations on the shared address space is presented. The scheme supports independent process recovery without forcing rollback of operational processes during recovery. The method is particularly useful in environments where taking process checkpoints is expensive.<<ETX>>
[Checkpointing, Costs, shared address space, distributed shared memory environment, reliability, asynchronous checkpointing, Distributed computing, system recovery, distributed applications, Information science, running time, long-running applications, Trademarks, shared memory systems, processor failure, read operations, recoverable distributed shared memory, Data structures, Computer crashes, Application software, process checkpoints, independent process recovery, distributed memory systems, Load management, Software systems, asynchronous process checkpoints, fault tolerant computing, operational processes]
Using atomic broadcast to implement a posteriori agreement for clock synchronization
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
A clock synchronization algorithm was given by P. Verissimo et al. (1989), dubbed a posteriori agreement, a variant of the convergence nonaveraging technique. By exploiting the characteristics of broadcast networks, the effect of message delivery delay variance is largely reduced. In consequence, the precision achieved by the algorithm is drastically improved. Accuracy preservation is near to optimal. A particular materialization of this algorithm, implemented as a time service of the xAMp group communications system, is given here. The algorithm was implemented using some of the primitives offered by xAMp, which simplified the work and stressed its advantages. Performance results for this implementation obtained on two different infrastructures are presented. Timings validate the design choices and clearly show that the algorithm is able to provide improved precision without compromising accuracy and reliability.<<ETX>>
[xAMp group communications system, clock synchronization, Delay effects, Software algorithms, Access protocols, distributed processing, Multicast protocols, Synchronization, message delivery delay variance, Convergence, synchronisation, atomic broadcast, a posteriori agreement, Multicast algorithms, security of data, network operating systems, convergence nonaveraging technique, Broadcasting, time service, Timing, broadcast networks, Clocks]
Lazy checkpoint coordination for bounding rollback propagation
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
The technique of lazy checkpoint coordination, which preserves process autonomy while employing communication-induced checkpoint coordination for bounding rollback propagation is proposed. The notion of laziness is introduced to control the coordination frequency and allow a flexible tradeoff between the cost of checkpoint coordination and the average rollback distance. Worst-case overhead analysis provides a means for estimating the extra checkpoint overhead. Communication trace-driven simulation for several parallel programs is used to evaluate the benefits of the proposed scheme.<<ETX>>
[Checkpointing, Performance evaluation, Costs, Laboratories, NASA, average rollback distance, checkpoint overhead, process autonomy, Frequency measurement, communication-induced checkpoint coordination, History, system recovery, rollback propagation, parallel programming, parallel programs, coordination frequency, Runtime, Message passing, lazy checkpoint coordination, system monitoring, fault tolerant computing, Contracts]
An approach to constructing modular fault-tolerant protocols
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
Modularization is a well-known technique for simplifying complex software. An approach to modularizing fault-tolerant protocols such as reliable multicast and membership is described. The approach is based on implementing a protocol's individual properties as separate microprotocols and then combining selected microprotocols using an event-driven software framework. A system is constructed by composing these frameworks with traditional network protocols using standard hierarchical techniques. In addition to simplifying the software, this model helps clarify the dependencies among properties of fault-tolerant protocols and makes it possible to construct systems that are customized to the specifics of the application or underlying architecture. An example involving reliable group multicast is given, together with a description of a prototype implementation using the SR concurrent programming language.<<ETX>>
[microprotocols, Protocols, reliable multicast, distributed processing, group multicast, Computer crashes, membership, Application software, network protocols, Strontium, Computer science, Fault tolerance, Computer languages, hierarchical techniques, Computer network reliability, Fault tolerant systems, modular fault-tolerant protocols, Computer architecture, event-driven software framework, fault tolerant computing, SR concurrent programming language, protocols]
Rollback based on vector time
Proceedings of 1993 IEEE 12th Symposium on Reliable Distributed Systems
None
1993
Causality, as made concrete by Lamport's "happened before" relation, is the central underlying basis in the design of optimistic protocols for checkpoint and recovery. After the recovery of a formerly failed process, all events and messages which causally follow the events and messages discarded by restoration of a state checkpoint must be discarded also. The isomorphism between causality and vector time is well-known. That isomorphism is exploited here in order to develop and verify a simple and effective protocol which explicitly uses the causal partial order, through the use of vector time, in order to rollback a computation in response to a process failure.<<ETX>>
[checkpoint, Protocols, Optimization methods, Educational institutions, recovery, Synchronization, Distributed computing, system recovery, isomorphism, Design optimization, failed process, causality, Computer science, vector time, optimistic protocols, causal partial order, Concrete, fault tolerant computing, protocols, Clocks]
Transparent load sharing in distributed systems: decentralized design alternatives based on the Condor package
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
In recent years a number of load sharing (LS) mechanisms have been proposed or implemented to fully utilize system resources. We design and implement a decentralized LS mechanism based on the Condor package, and give in this paper a description of our design and implementation approaches. Two important features of the design are the use of region-change broadcasts in the information policy to provide each workstation with timely state information at the minimum communication cost, and the use of preferred list in the location policy to avoid task collisions. With these two features, we remove the central manager workstation in Condor, configure its functionalities into each participating workstation, and thus enhance the capability to tolerate single workstation failure and the reliability of Condor. We also discuss the experiments we conduct on the LS mechanism and the observations we obtained from empirical data.<<ETX>>
[Availability, Costs, load sharing, distributed processing, Condor package, Delay, Design engineering, task collisions, Software packages, Chaotic communication, resource allocation, Packaging, Broadcasting, distributed systems, Workstations, Kernel, decentralized design, region-change broadcasts]
Distributed reconfiguration of multiprocessor systems
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
We propose distributed algorithms for assigning tasks to processors in a distributed system, as well as algorithms for reassigning tasks from a failed processor to the remaining fault free processors in the system. The assignment and reassignment of the tasks should 1) make an efficient use of the capacity of the processors in the system, 2) distribute the load in a balanced way among the processors in the system, 3) assign tasks to processors for which they have a large preference (affinity), and 4) minimize the disturbance that is introduced by the migration of some tasks from one processor to another. We show that the distributed algorithms perform very closely to the centralized ones, even though they hold and maintain information only about processors in their local neighborhood.<<ETX>>
[Legged locomotion, Costs, multiprocessing systems, Decision making, multiprocessor systems, NP-complete problem, distributed reconfiguration, fault free processors, Multiprocessing systems, distributed algorithms, Bandwidth, Load management, Polynomials, fault tolerant computing, Resource management, Distributed algorithms]
Coordinated checkpointing-rollback error recovery for distributed shared memory multicomputers
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Most recovery schemes that have been proposed for Distributed Shared Memory (DSM) systems require unnecessarily high checkpointing frequency and checkpoint traffic, which are sensitive to the frequency of interprocess communication in the applications. For message-passing systems, low overhead error recovery based on coordinated checkpointing allows the frequency of checkpointing to be determined only by the reliability requirements of the application. Efficient adaptation of this approach to DSM multicomputers is complicated by the absence of explicit messages in DSM systems, the presence of a shared and partially replicated address space, and the presence of a distributed coherency directory. We present solutions to these issues, and propose an error recovery scheme based on coordinated checkpointing and rollback for DSM multicomputers. Our performance evaluation based on trace-driven simulations indicates that this scheme incurs less checkpoint traffic than recovery schemes previously proposed for DSM systems.<<ETX>>
[Checkpointing, Protocols, message passing, Very large scale integration, performance evaluation, distributed shared memory multicomputers, reliability requirements, Application software, message-passing systems, Computer science, trace-driven simulations, Fault tolerance, distributed memory systems, Computer errors, Traffic control, coordinated checkpointing-rollback error recovery, Frequency, Hardware, discrete event simulation]
A methodology for constructing a stabilizing crash-tolerant application
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
This paper is an exercise to construct a stabilizing mutual-exclusion protocol that withstands a single crash-failure. We begin with a collection of distributed processes arranged in a ring. The resulting protocol is stabilized by construction. Stabilizing protocols converge to a correct behavior regardless of their initial state. A faulty process is automatically removed from the system and, after repair, automatically integrated into the system. Our technique can be generalized to different systems by substituting appropriate protocols for various components.<<ETX>>
[Protocols, Channel capacity, reliability, Computer crashes, Topology, Application software, crash-tolerant application stabilisation, Delay, Computer science, Fault tolerance, distributed processes, Message passing, distributed algorithms, Cities and towns, mutual-exclusion protocol, fault tolerant computing, protocols]
Nested dynamic actions: how to solve the fault containment problem in a cooperative action model
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Elements of transaction processing become more and more accepted as a base for fault-tolerant distributed computing. We have developed an action concept with an extended functionality suited to better support the needs of Concurrent Engineering applications. This allows actions to access data produced by other, still active actions. In case of an action fault, however, fault containment may become a problem of major concern. Nesting seems to be an appropriate means to tackle this problem. Unfortunately, it is not possible to simply adopt the nested transactional concept as designed by Moss, because it does not consider dependencies between actions belonging to different parents. This paper is to present a nesting concept which provides an adequate means for fault containment in dynamic actions. Special emphasis is given to the problem of protecting the work of a parent action against the consequences of the abort of other actions.<<ETX>>
[transaction processing, Cooperative systems, distributed processing, nested transactional concept, fault containment problem, Transaction databases, Electronic mail, Application software, Distributed computing, concurrent engineering, nested dynamic actions, distributed computing, Computer science, Fault tolerance, Concurrent Engineering, parent action, cooperative action model, Concurrent engineering, fault tolerant computing, fault-tolerant distributed computing, Protection, Contracts, fault-tolerant]
A protocol description language for customizing failure semantics
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
To optimize performance in a fault-tolerant distributed system, it is often necessary to enforce different failure semantics for different components. By choosing a custom set of failure semantics for each component and then by enforcing the semantics with a minimal set of protocols for a particular architecture, performance may be maximized while ensuring the desired system behavior. We have developed DIL, a language for specifying, on a per-component basis, protocols that transparently enforce failure semantics. These protocols may be reused with arbitrary components, allowing the development of a library of protocols.<<ETX>>
[Protocols, DIL, distributed processing, Application software, Programming profession, Computer science, Fault tolerance, Fault tolerant systems, Whales, specification languages, failure semantics, fault-tolerant distributed system, Systems engineering and theory, fault tolerant computing, protocols, protocol description language, Contracts, fault-tolerant, Electronics packaging]
Analysis of checkpointing schemes for multiprocessor systems
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Parallel computing systems provide hardware redundancy that helps to achieve low cost fault-tolerance, by duplicating the task into more than a single processor, and comparing the states of the processors at checkpoints. This paper suggests a novel technique, based on a Markov reward model (MRM), for analyzing the performance of checkpointing schemes with task duplication. We show how this technique can be used to derive the average execution time of a task and other important parameters related to the performance of checkpointing schemes. Our analytical results match well the values we obtained using a simulation program. We compare the average task execution time and total work of four checkpointing schemes, and show that generally increasing the number of processors reduces the average execution time, but increases the total work done by the processors. However, in cases where there is a big difference between the time it takes to perform different operations, those results can change.<<ETX>>
[Checkpointing, Costs, multiprocessing systems, hardware redundancy, Laboratories, simulation program, multiprocessor systems, performance evaluation, Markov reward model, Multiprocessing systems, Information systems, Postal services, performance, Fault detection, Fault tolerant systems, parallel computing systems, low cost fault-tolerance, Parallel processing, Markov processes, checkpointing schemes, Hardware]
Reducing interprocessor dependence in recoverable distributed shared memory
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Checkpointing techniques in parallel systems use dependency tracking and/or message logging to ensure that a system rolls back to a consistent state. Traditional dependency tracking in distributed shared memory (DSM) systems is expensive because of high communication frequency. In this paper we show that, if designed correctly, a DSM system only needs to consider dependencies due to the transfer of blocks of data, resulting in reduced dependency tracking overhead and reduced potential for rollback propagation. We develop an ownership timestamp scheme to tolerate the loss of block state information and develop a passive server model of execution where interactions between processors are considered atomic. With our scheme, dependencies are significantly reduced compared to the traditional message-passing model.<<ETX>>
[Checkpointing, checkpointing techniques, recoverable distributed shared memory, interprocessor dependence, block state information, NASA, Laboratories, ownership timestamp scheme, Application software, rollback propagation, Concurrent computing, message logging, parallel systems, Message passing, dependency tracking, distributed memory systems, Software systems, Frequency, Hardware, fault tolerant computing, passive server model, Contracts]
Fault tolerance in a multisensor environment
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Replicating sensors is desirable not only to tolerate sensor failures but also to increase the expected accuracy of the ensemble of replicated sensors beyond that obtainable with a single one. Such replication is used in a multisensor environment or in a distributed sensor network. We model a continuous valued sensor as an interval of real numbers containing the physical value of interest. Given n sensors of which at most f of them can suffer arbitrary failures, we present an efficient O(n log n) fault tolerant algorithm whose output is reliable when f<n/2. The output of the algorithm could be a single interval or a set of intervals depending on the nature of the multisensor environment. This algorithm can be used not only to detect all the possibly-faulty sensors but to detect all sets (combinations) of possibly-faulty sensors. We derive a number of results pertaining to the possibly-faulty sensors pointed out by the algorithm which help narrow down the search to detect faulty sensors and to bound the number of intervals needed to construct an accurate and reliable "abstract sensor".<<ETX>>
[replicated sensors, fault tolerance, NASA, Process control, Communication system control, possibly-faulty sensors, distributed processing, Sensor systems, Multisensor systems, Fault tolerance, Information science, Fault detection, Working environment noise, Signal processing, multisensor environment, intelligent sensors, fault tolerant computing, fault tolerant algorithm]
PLinda 2.0: a transactional/checkpointing approach to fault tolerant Linda
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Robust parallel computation in Linda requires both tuple space and processes to be resilient to failure. In this paper, we present PLinda 2.0, set of extensions to Linda to support robust parallel computation on loosely coupled processors communicating over a network. The principal extensions of PLinda 2.0 to Linda are transaction mechanisms for reliable tuple space and process-private logging mechanisms for resilient processes. The transaction mechanisms support two kinds of tuple space: stable tuple space always guaranteed to reflect state as of last committed transaction, and unstable tuple space protected by a transaction-consistent checkpoint. The process-private logging mechanisms are provided as tools for a process checkpointing scheme. These mechanisms allow the customization of checkpointing and recovery operations in each process to achieve low runtime overhead.<<ETX>>
[Checkpointing, transaction processing, PLinda 2.0, high level languages, fault tolerant Linda, distributed processing, robust parallel computation, Programming profession, transaction mechanisms, Concurrent computing, Fault tolerance, Runtime, tuple space, transactional/checkpointing approach, process-private logging mechanisms, Robustness, fault tolerant computing, Workstations, Protection]
Simulating fail-stop in asynchronous distributed systems
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
The fail-stop failure model appears frequently in the distributed systems literature. However, in an asynchronous distributed system, the fail-stop model cannot be implemented. In particular, it is impossible to reliably detect crash failures in an asynchronous system. In this paper, we show that it is possible to specify and implement a failure model that is indistinguishable from the fail-stop model from the point of view of any process within an asynchronous system. We give necessary conditions for a failure model to be indistinguishable from the fail-stop model, and derive lower bounds on the amount of process replication needed to implement such a failure model. We present a simple one-round protocol for implementing one such failure model, which we call simulated fail-stop.<<ETX>>
[Protocols, Computational modeling, Scholarships, NASA, Nominations and elections, distributed processing, Computer crashes, asynchronous distributed systems, system recovery, Computer science, crash failures, Lead, fault tolerant computing, process replication, Distributed algorithms, Context modeling, fail-stop failure model]
Exploiting program semantics for efficient instrumentation of distributed event recognitions
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Event based monitors and debuggers for distributed systems automatically detect occurrences of user specified events that characterize the state of one or more processes in a distributed computation. The system monitors the occurrences of local events. Data pertaining to these occurrences is utilized to detect occurrences of distributed events which check the ordering or concurrency of local events that occur at different processes. The overhead of recognizing distributed events can be high, because of the need to capture, communicate, store and evaluate event information for analysis. This paper presents techniques that exploit program and event semantics to reduce the overhead of monitoring a distributed computation for ordered and concurrent events. Static analysis algorithms are presented that determine the minimal program instrumentation required to insure that adequate state and timing information are available to recognize every event occurrence.<<ETX>>
[Algorithm design and analysis, program debugging, event based monitors, Event detection, Instruments, debuggers, distributed processing, distributed computation, Distributed computing, Information analysis, Concurrent computing, Computer displays, program semantics, distributed event recognitions, distributed systems, distributed events, Timing, Monitoring, Testing]
Probabilistic internal clock synchronization
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
We propose an improved probabilistic method for reading remote clocks in systems subject to unbounded communication delays and use this method to design a fault-tolerant probabilistic internal clock synchronization protocol. This protocol masks clock reading failures and arbitrary failures of processes. Because of probabilistic reading, our protocol achieves better synchronization precisions than those achievable by previously known deterministic algorithms. Another advantage of the proposed protocol is that it uses a linear, instead of quadratic, number of messages, and that message exchanges are staggered in time instead of all happening in narrow synchronization intervals. The drift rate of the synchronized clocks is optimal.<<ETX>>
[Protocols, remote clocks, probabilistic internal clock synchronization, Design methodology, reliability, fault-tolerant protocol, Synchronization, Delay, Convergence, synchronisation, Computer science, Fault tolerance, Design engineering, Fault tolerant systems, delays, unbounded communication delays, synchronization, fault tolerant computing, protocols, Clocks]
Probabilistic validation using worst event driven and importance sampling simulation
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Probabilistic validation is an approach for the validation of highly dependable and complex systems. It relies on a partial analysis on a system model and tries to prove that the failed event occurrences has a sufficiently low probability. We define a probabilistic validation method using worst event driven and an importance sampling simulation. The system which must be validated is modeled by a stochastic Petri net. An efficient simulation of the net must be able to sample complex and improbable trajectories which eventually reach critical markings. Two problems have to be solved. The sequence of transition firings which may lead to critical markings must be characterized ad the Petri net level. The second problem is to sample these sequences and to build an accurate estimate of the incorrect behavior probability. We discuss several simulation algorithms in the Markovian and non-Markovian cases. We show the effectiveness of these techniques on the validation of several examples.<<ETX>>
[probabilistic validation, Petri nets, Stochastic processes, probability, performance evaluation, transition firings, Discrete event simulation, State-space methods, Aerospace industry, Monte Carlo methods, Defense industry, importance sampling simulation, Stochastic systems, stochastic Petri net, Failure analysis, Sampling methods, Aerospace safety, worst event driven simulation, discrete event simulation, Markovian processes]
Interaction of formal design systems in the development of a fault-tolerant clock synchronization circuit
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
We propose a design strategy that exploits the strengths of different formal approaches to establish a reliable path from a mechanically verified high-level description to a concrete gate-level realization. We demonstrate the use of this approach in the realization of a fault-tolerant clock synchronization circuit. We used the Digital Design Derivation system (DDD) to derive a major portion of the design leaving relatively small portions to be verified either by use of a mechanical theorem prover (PVS) or by demonstrating boolean equivalence using Ordered Binary Decision Diagrams. The interface between the different formal systems has not yet been completely formalized but we believe our approach will provide an effective formal design path from high-level specifications to concrete realizations.<<ETX>>
[fault-tolerant clock synchronization circuit, clock synchronization, Circuits, NASA, logic testing, Synchronization, formal specification, Aerospace control, Fault tolerance, high-level specifications, circuit reliability, Fault tolerant systems, mechanical theorem prover, Hardware, Concrete, formal design systems, Space exploration, gate-level realization, logic design, fault-tolerant, Clocks]
Efficient treatment of failures in RPC systems
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
This paper addresses extensions to be made to a basic remote procedure call system for the integration of primitive fault tolerance measures. Our main design goal is to not introduce performance penalty for remote procedure calls executing in the absence of failures, and to not impose significant overhead by the treatment of failures. Basically, extensions include a simple algorithm that finds and eliminates orphans, and a mechanism that detects abnormally terminated remote calls. Our solution for orphan detection as based on the extermination approach, its efficiency coming from a minor addition to the system architecture that allows the implementation of high speed stable storage. Performance measures given by the implementation of our reliability mechanisms on top of the Mach 3.0/BSD UX36 operating system show that the mechanisms are responsible for adding only 1% overhead on the operating system's base remote procedure call.<<ETX>>
[Out of order, Terminology, failures, fault tolerance, operating system, distributed processing, Computer crashes, orphan detection, Yarn, system recovery, primitive fault tolerance measures, Operating systems, Fault tolerant systems, Propagation losses, operating systems (computers), remote procedure call, RPC systems]
Reliability analysis of a hardware and software fault tolerant parallel processor
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Computer systems for critical applications must be designed to tolerate software faults as well as hardware faults. A unified approach to tolerating hardware and software faults is characterized by classifying faults in terms of duration (transient or permanent) rather than source (hardware or software). Errors arising from transient faults can be handled through masking or voting, but errors arising from permanent faults require system reconfiguration to bypass the failed component. Most errors which are caused by software faults can be considered transient, in that they are input dependent. Quantitative dependability analysis of systems which exhibit a unified approach to fault tolerance can be performed by a hierarchical combination of fault tree and Markov models. In this paper, a methodology for analyzing hardware and software fault tolerant systems is applied to the analysis of a hypothetical system, loosely based on the fault tolerant parallel processor (FTPP). The models considers both transient and permanent faults, hardware and software faults, unrelated and related software faults, automatic recovery and reconfiguration. The parameter values for the software part of the model are determined from an experimental implementation of an N-version programming application. The parameter values chosen for the hardware part of the model are considered fairly typical.<<ETX>>
[performance evaluation, quantitative dependability analysis, Application software, transient faults, parallel processing, fault tolerant parallel processor, N-version programming, Fault tolerance, Software design, Voting, reliability analysis, Fault tolerant systems, Computer errors, Markov processes, Software systems, Hardware, fault tolerant computing, Markov models, Performance analysis, Fault trees, system reconfiguration]
An efficient recovery scheme for locking-based distributed database systems
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
This paper presents a checkpointing recovery scheme which effectively copes with media failures in distributed database systems, under the two-phase locking policy. Our scheme utilizes both the current database area and the checkpoint area, so that the transactions whose effects are to be included in the checkpoint and the ones whose effects are to be excluded from the checkpoint can separately reflect their effects. Hence, the check pointing process under our scheme is executed concurrently with normal transactions, without aborting or delaying any of them. Unlike the existing schemes, each data item needs to be read at most once for checkpointing without the maintenance of the before-images of the data items.<<ETX>>
[Checkpointing, Availability, checkpointing, check pointing process, Computer crashes, Concurrency control, Transaction databases, recovery scheme, system recovery, Delay, Secure storage, two-phase locking, distributed database systems, Computer science, Distributed databases, distributed databases, Database systems, fault tolerant computing, media failures, locking-based]
A replication-transparent remote invocation protocol
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Although many algorithms and implementations of replicated services have been developed, most have embedded aspects of the replication management in the invocation protocol. This makes it extremely difficult to modify the replication protocol without changing the protocol used by the clients, and causes an undesirable violation of both transparency and modularity. The GRIP protocol supports the fault-tolerant remote invocation of replicated services, providing not only the usual location transparency but also transparency of replication semantics. Our approach is independent of the details of the replica control protocol used to maintain the consistency of server replicas. We use a lightweight remote invocation protocol in order to minimize the impact on the client of issues such as scale and replication consistency maintenance. Furthermore, unlike most previous systems we provide explicit support for weakly consistent replication protocols. GRIP is designed as a collection of modular services, which can be configured according to the needs of the application.<<ETX>>
[Availability, Costs, replication protocols, replication protocol, Access protocols, distributed processing, Fault tolerance, GRIP, Fault tolerant systems, remote invocation protocol, Broadcasting, Large-scale systems, protocols, remote invocation, replicated services, fault-tolerant]
An environment for importance sampling based on stochastic activity networks
Proceedings of IEEE 13th Symposium on Reliable Distributed Systems
None
1994
Model-based evaluation of reliable distributed and parallel systems is difficult due to the complexity of these systems and the nature of the dependability measures of interest. The complexity creates problems for analytical model solution techniques, and the fact that reliability and availability measures are based on rare events makes traditional simulation methods inefficient. Importance sampling is a well-known technique for improving the efficiency of rare event simulations. However, finding an importance sampling strategy that works well in general is a difficult problem. The best strategy for importance sampling depends on the characteristics of the system and the dependability measure of interest. This fact motivated the development of an environment for importance sampling that would support the wide variety of model characteristics and interesting measures. The environment is based on stochastic activity networks, and importance sampling strategies are specified using the new concept of the importance sampling governor. The governor supports dynamic importance sampling strategies by allowing the stochastic elements of the model to be redefined based on the evolution of the simulation. The utility of the new environment is demonstrated by evaluating the unreliability of a highly dependable fault-tolerant unit used in the well-known MARS architecture. The model is non-Markovian, with Weibull distributed failure times and uniformly distributed repair times.<<ETX>>
[MARS architecture, complexity, Stochastic processes, distributed processing, stochastic activity networks, Discrete event simulation, parallel processing, Concurrent computing, Analytical models, Monte Carlo methods, Computer network reliability, reliable parallel systems, rare event simulations, Computer networks, stochastic processes, uniformly distributed repair times, Availability, analytical model solution techniques, model-based evaluation, performance evaluation, importance sampling, reliable distributed system, Stochastic systems, Weibull distributed failure times, Sampling methods, computational complexity]
Non blocking atomic commitment with an unreliable failure detector
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
In a transactional system, an atomic commitment protocol ensures that for any transaction, all data manager processes agree on the same outcome (commit or abort). A non-blocking atomic commitment protocol enables an outcome to be decided at every correct process despite the failure of others. In this paper we apply, for the first time, the fundamental result of T. Chandra and S. Toueg (1991) on solving the abstract consensus problem, to non-blocking atomic commitment. More precisely, we present a non-blocking atomic commitment protocol in an asynchronous system augmented with an unreliable failure detector that can make an infinity of false failure suspicions. If no process is suspected to have failed, then our protocol is similar to a three phase commit protocol. In the case where processes are suspected, our protocol does not require any additional termination protocol: failure scenarios are handled within our regular protocol and are thus much simpler to manage.
[transaction processing, Protocols, Project management, H infinity control, distributed processing, unreliable failure detector, transactional system, Delay, Concurrent computing, abstract consensus problem, Upper bound, protocol, nonblocking atomic commitment, Detectors, Broadcasting, data manager processes, protocols, Contracts]
A hierarchy of totally ordered multicasts
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
The increased interest in protocols that provide a total order on message delivery has led to several different definitions of total order. In this paper we investigate these different definitions and propose a hierarchy that helps to better understand the implications of the different possibilities in terms of guarantees and communication cost. We identify two definitions: weak total order and strong total order, which are at the extremes of the proposed hierarchy, and incorporate them into a consistent design. Finally, we propose high-level algorithms based on a virtually synchronous communication environment that implement the given definitions.
[Algorithm design and analysis, Protocols, Costs, strong total order, totally ordered multicasts hierarchy, distributed processing, communication cost, Distributed computing, high-level algorithms, virtually synchronous communication environment, Delay, weak total order, Multicast algorithms, message delivery, Broadcasting, fault tolerant computing, protocols, Contracts]
Self diagnosis of processor arrays using a comparison model
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
This paper introduces a diagnosing algorithm for bidimensional processor arrays, where processors are interconnected in horizontal and vertical meshes. For the purpose of diagnosis, the array is considered to be partitioned in square clusters of processors. The algorithm is based on interprocessor tests, using a comparison model. The algorithm, which is divided in four steps, called intracluster diagnosis, interluster diagnosis, fault-free core identification and augmentation, identifies a set of non-faulty and a set of faulty units. The diagnosis is proved to be correct in the worst case, assuming that the actual number of faulty processors is no more that T(N), an increasing function of the number N of processors. It is shown that T(N) is O(N/sup 2/3/). Although correct, the diagnosis is generally incomplete. However, using probabilistic techniques, it is shown that the diagnosis is very likely to be complete under the same limitations which ensure correctness in the worst case.
[Performance evaluation, processor arrays, System testing, comparison model, Peer to peer computing, intracluster diagnosis, faulty processors, Partitioning algorithms, Decoding, interprocessor tests, augmentation, parallel processing, Fault diagnosis, interluster diagnosis, square clusters, Fault tolerant systems, Clustering algorithms, fault-free core identification, Bidirectional control, fault tolerant computing, bidimensional processor arrays]
Experimental evaluation of the impact of processor faults on parallel applications
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
This paper addresses the problem of processor faults in distributed memory parallel systems. It shows that transient faults injected at the processor pins of one node of a commercial parallel computer, without any particular fault-tolerant techniques, can cause erroneous application results for up to 43% of the injected faults (depending on the application). In addition to these very subtle faults, up to 19% of the injected faults (almost independent on the application) caused the system to hang up. These results show that fault-tolerant techniques are absolutely required in parallel systems, not only to ensure the completion of long-run applications but, and more important, to achieve confidence in the application results. The benefits of including some fairly simple behaviour based error detection mechanisms in the system were evaluated together with Algorithm Based Fault Tolerance (ABFT) techniques. The inclusion of such Mechanisms in parallel systems seems to be very important for detecting most of those subtle errors without greatly affecting the performance and the cost of these systems.
[Costs, Algorithm Based Fault Tolerance, Aerospace electronics, parallel applications, Application software, transient faults, error detection, Concurrent computing, Fault tolerance, parallel systems, ABFT, pin-level fault injection, High performance computing, Fault detection, Fault tolerant systems, distributed memory systems, Parallel processing, fault tolerant computing, Pins, processor faults, distributed memory parallel systems]
Membership and system diagnosis
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
A membership service is a service in a distributed system that maintains and provides information about which sites are functioning and which have failed at any given time. System diagnosis, on the other hand, is a method for detecting faulty processing elements and distributing this information to non-faulty elements. In spite of the apparent similarity of goals, these two fields have been considered separately from their beginnings. In this paper, we attempt to compare these fields and show the fundamental differences and the similarities. We demonstrate that the problems are closely related with the major differences being the assumptions made about the failure model, the testing methods, and the type of service guarantees provided to the application. Furthermore, we demonstrate that the fields are closely enough related that some algorithms utilized in one field can easily be transformed into algorithms in the other. As examples, we derive new membership algorithms from a distributed system diagnosis algorithm and new system diagnosis algorithms from membership algorithms.
[Collaborative software, reliability, distributed processing, distributed system, Distributed computing, membership service, Fault diagnosis, Computer science, distributed system diagnosis, Fault detection, Physics computing, Fault tolerant systems, Intersymbol interference, faulty processing elements, Software systems, system diagnosis, fault tolerant computing, Testing]
A longitudinal survey of Internet host reliability
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
An accurate estimate of host reliability is important for correct analysis of many fault-tolerance and replication mechanisms. In a previous study, we estimated host system reliability by querying a large number of hosts to find how long they had been functioning, estimating the mean time-to-failure (MTTF) and availability from those measures, and in turn deriving an estimate of the mean time-to-repair (MTTR). However, this approach had a bias towards more reliable hosts that could result in overestimating MTTR and underestimating availability. To address this bias we have conducted a second experiment using a fault-tolerant replicated monitoring tool. This tool directly measures TTF, TTR, and availability by polling many sites frequently from several locations. We find that these more accurate results generally confirm and improve our earlier estimates, particularly for TTR. We also find that failure and repair are unlikely to follow Poisson processes.
[Availability, replication, fault-tolerance, Access protocols, reliability, Information analysis, Condition monitoring, Fault tolerance, fault-tolerant replicated monitoring, Fault tolerant systems, host reliability, Failure analysis, Hardware, fault tolerant computing, Internet host reliability, host system reliability, Internet, Power system reliability]
A method for the construction and interpretation of high level models for distributed fault-tolerant systems
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
Traditional solutions for achieving fault-tolerance are intended for use at design time and they generally capture system information at a very low (hardware or machine instruction) level. Increasing reliability of complex information systems containing many (perhaps many thousands) of autonomous components requires different solutions. This article presents a new methodology for the implementation of large scale, distributed fault-tolerant systems. System models are formed of objects describing requirements, services and resources organized into high level top-down hierarchical decomposition structures. Since redundancy is a natural property of any large scale system, by using such models it is possible to achieve fault tolerant behaviour by finding multiple appropriate mappings between requirements and available services, and to support the required services by available resources. The distributed system is extended with dedicated components, called diagnostic centres, which manage distinct parts of the system model, continuously observe the operation of the distributed system, and find alternative requirement-service mappings, if some services fail to fulfil their associated requirements. The elements and the structure of the proposed system modelling method are presented, an appropriate fault model is defined, and the algorithms for model interpretation are described.
[system information, Instruments, Redundancy, distributed fault-tolerant systems, distributed processing, autonomous components, Reliability engineering, Distributed computing, Information systems, Fault tolerance, high level models, system modelling method, Fault tolerant systems, Computer architecture, design time, high level top-down hierarchical decomposition structures, Hardware, fault tolerant computing, Large-scale systems, complex information systems, diagnostic centres]
Performance analysis of a regeneration-based dynamic voting algorithm
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
RVC2 is a consistency control algorithm for replicated data objects in a distributed computing system. It is a dynamic voting algorithm which utilizes selective regeneration and recovery mechanisms for failed copies. Virtual copies which record information about the current state of a data object, but do not contain actual data, are used to reduce network and storage overhead. Experimental results for availability, storage cost, and message cost, obtained through simulation, are discussed. Our results show that the replacement of real copies with virtual copies has no significant impact on the availability of a data object. Neither does varying the generation threshold. We also show that high availability can be maintained without regeneration. We conclude that regeneration makes no significant contribution to the high availability of RVC2.
[Protocols, Costs, Heuristic algorithms, Control systems, availability, Distributed computing, distributed computing system, Voting, distributed databases, storage overhead, Performance analysis, failed copies, message cost, replicated data objects, Availability, replicated databases, consistency control algorithm, regeneration-based dynamic voting algorithm, recovery mechanisms, storage cost, software fault tolerance, Computer science, transport protocols, RVC2, concurrency control, Distributed control, selective regeneration]
A new deadlock detection algorithms for distributed real-time database systems
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
Recently the concurrency control issue of real-time transactions is gaining increasing attention of researchers in the database community. One of the major design issue in concurrency control of real-time transactions is the resolution of local as well as distributed deadlocks while at the same time meeting the timing requirements of the transactions. In this paper, a new deadlock detection algorithm specially designed for distributed real-time database systems is proposed. The performance of the proposed algorithm is evaluated through extensive simulation experiments. Studies have also been carried out to compare the performance of the real-time deadlock detection algorithm with a non real-time algorithm for both firm and soft real-time transactions. Results indicated that the real-time deadlock detection algorithm performs better than the non real-lime deadlock detection algorithm. Results also indicated that the performance of the new algorithm is substantially better for soft real-time than that of firm real-time systems.
[Real time systems, parallel algorithms, real-time deadlock detection algorithm, Protocols, distributed real-time database systems, deadlock detection algorithm, real-time transactions, Concurrency control, Transaction databases, system recovery, Scheduling algorithm, timing requirements, Computer science, deadlock detection algorithms, concurrency control, real-time systems, distributed databases, System recovery, concurrency control issue, Database systems, Timing, Detection algorithms, extensive simulation experiments]
MUSE: a message passing concurrent computer for on-board space systems
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
Satellite payloads of the near future will raise the need of very powerful and dependable computers. No embeddable monoprocessor will be able to satisfy such computing power need. Thus those computers will be multi-processor systems. Satellite payloads must meet high availability requirements rather then reliability ones. This allows the use of fail stop reconfigurable computers. This paper describes the MUSE architecture, a reconfigurable multi-processor computer designed to support signal processing applications in space systems.
[parallel architectures, reliability, processor scheduling, Concurrent computing, resource allocation, reconfigurable architectures, Computer architecture, reconfigurable multi-processor computer, aerospace computing, computer network reliability, Signal design, MUSE, Availability, Embedded computing, message passing, signal processing applications, embeddable monoprocessor, multi-processor systems, message passing concurrent computer, Satellites, Message passing, special purpose computers, fail stop reconfigurable computers, high availability requirements, Signal processing, fault tolerant computing, Power system reliability, digital signal processing chips, Payloads, on-board space systems]
System support for robust collaborative applications
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
Traditional transaction models ensure robustness for distributed applications through the properties of view and failure atomicity. It has generally been felt that such atomicity properties are restrictive for a wide range of application domains; this is particularly true for robust, collaborative applications because such applications have concurrent components that are inherently long-lived and that cooperate. Recent advances in extended transaction models can be exploited to structure long-lived and cooperative computations. Applications can use a combination of such models to achieve the desired degree of robustness; hence, we develop a system which can support a number of flexible transaction models, with correctness criteria that extend or relax serializability. We analyze two concrete CSCW applications-collaborative editor and meeting scheduler. We show how a combination of two extended transaction models, that promote split and cooperating actions, facilitates robust implementations of these collaborative applications. Thus, we conclude that a system that implements multiple transaction models provides flexible support for building robust collaborative applications.
[multiple transaction models, distributed processing, Distributed computing, Concurrent computing, robust collaborative applications, CSCW applications, groupware, serializability, Robustness, Libraries, computer supported cooperative work, correctness criteria, failure atomicity, view atomicity, concurrent components, Educational institutions, transaction models, Application software, system support, Power system modeling, collaborative editor, Collaboration, meeting scheduler, Collaborative work, Concrete]
TMR processing without explicit clock synchronisation
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
Replicated processing with majority voting is a well known method for achieving fault tolerance. Triple Modular Redundant (TMR) processing is the most commonly used version of that method. Replicated processing requires that the replicas reach agreement on the order in which messages are to be processed. Synchronous and deterministic ordering protocols published in the literature require that the replicas maintain an abstraction of clocks that are kept in known and bounded synchronism. We present a protocol for TMR systems that does not require this abstraction of synchronised clocks. We analyse the protocol performance and show that this protocol in practice can be at least as fast as any synchronised clock based ordering protocol. We also derive a faster protocol that has an improved performance in the absence of processor failures. We then build a TMR node and measure its performance to illustrate that the protocols developed here provide faster ordering and are easier to implement.
[protocol performance, Protocols, logical clocks, reliability, distributed processing, majority voting, deterministic ordering protocols, bounded synchronism, Fault tolerance, Voting, Fault tolerant systems, message ordering, Performance analysis, computer network reliability, redundancy, process replication, triple modular redundant processing, replicated processing, Redundancy, Byzantine failures, Synchronization, explicit clock synchronisation, Message passing, transport protocols, fault tolerant computing, Clocks]
A paradigm for user-defined security policies
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
One of today's major challenges in computer security is the ever-increasing multitude of individual, application-specific security requirements. As a positive consequence, a wide variety of security policies has been developed, each policy reflecting the specific needs of individual applications. As a negative consequence, the integration of the multitude of policies into today's system platforms made the limitations of traditional architectural foundations of secure computer systems quite obvious. Many of the traditional architectural foundations originally aimed at supporting only a single access control policy within a single trusted system environment. This paper discusses a new paradigm to support user-defined security policies in a distributed multi-policy system. The paradigm preserves the successful properties of the traditional architectural foundations while additionally providing strong concepts for user-defined security policies. Among these concepts are policy separation, encapsulation, persistency, cooperation, and reusability. We illustrate the application of our approach in a DCE environment.
[Access control, encapsulation, multi-policy environments, application-dependent user-defined security policies, confidentiality, inheritance, reusability, reference monitor, Communication system security, Condition monitoring, policy separation, integrity, persistency, application-specific security requirements, user-defined security policies, access control, object-oriented methods, Computer security, Protection, cooperation, Computerized monitoring, Data security, traditional architectural foundations, Application software, computer security, computer network management, security of data, Information security, Authentication, distributed multi-policy system, data encapsulation]
A correctness criterion for advanced transaction models
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
The transaction concept was originally applied to database applications. Serializability theory captured transaction correctness and database objects consistency properties in a single notion. Today, increasingly sophisticated information requires new correctness criteria due to the limitation of classical serialisability theory which allows only a limited cooperation between its components. Several models relaxing the ACID (Atomicity, Consistency, Isolation, Durability) properties in a controlled manner have been developed. These approaches exploit separately the semantics properties of operations (object semantic approach) and application semantics (transaction interleaving approach). The notion of correctness can be refined with the help of the two previous approaches whilst increasing concurrency. In this paper, we will the gap between transaction and object semantic correctness criteria. We define a new class of schedule called Multilevel Relative Serialisability (MLRS) to combine the two approaches. This class of schedule preserve correctness properties defined in terms of object and transaction semantics. We use ACTA formalism to express object consistency, transaction correctness and MLRS. This work merges existing /spl Lt/relaxed/spl Gt/ transaction models into a unified concept. This concept is useful for long-lived, cooperative and hierarchical transaction models.
[transaction processing, Interference, Scheduling, transaction models, object semantic approach, Transaction databases, formal specification, Programming profession, Concurrent computing, application semantics, correctness criterion, Interleaved codes, fault tolerant computing, Logic]
Maximum and minimum consistent global checkpoints and their applications
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
This paper considers the problem of constructing the maximum and the minimum consistent global checkpoints that contain a target set of checkpoints, and identify it as a generic issue in recovery-related applications. We formulate the problem as a reachability analysis problem on a directed rollback-dependency graph, and develop efficient algorithms to calculate the two consistent global checkpoints for both general nondeterministic executions and piecewise deterministic executions. We also demonstrate that the approach provides a generalization and unifying framework for many existing and potential applications including software error recovery, mobile computing recovery, parallel debugging and output commits.
[Checkpointing, program debugging, Protocols, parallel debugging, reachability analysis problem, output commits, consistent global checkpoints, unifying framework, Concurrent computing, generic issue, Nonvolatile memory, Hardware, recovery-related applications, reachability analysis, software error recovery, general nondeterministic executions, Application software, Power system modeling, Reachability analysis, Software debugging, software fault tolerance, piecewise deterministic executions, directed rollback-dependency graph, mobile computing recovery, Mobile computing]
A synchronization strategy for a time-triggered multicluster real-time system
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
The provision of a system-wide global time base with a good precision and sufficient accuracy is a fundamental prerequisite for the design of a multicluster distributed real-time system. We investigate the issues of clock synchronization in a multicluster system, where every node can have a different oscillator. Based on the parameter of a typical automotive distributed system we show that a precision and accuracy in the second range is achievable without undue effort.
[Real time systems, Art, clock synchronization, Communication system control, multiprocessor interconnection networks, reliability, global time, typical automotive distributed system, Automotive engineering, Fault tolerance, Design engineering, system-wide global time base, Safety, computer network reliability, client-server systems, fault tolerance, synchronization strategy, Oscillators, synchronisation, real-time systems, concurrency control, automotive electronics, time-triggered multicluster real-time system, fault tolerant computing, Frequency synchronization, Clocks]
Designing masking fault-tolerance via nonmasking fault-tolerance
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
Masking fault-tolerance guarantees that programs continually satisfy their specification in the presence of faults. By way of contrast, nonmasking fault-tolerance does not guarantee as much: it merely guarantees that when faults stop occurring, program executions converge to states from where programs continually (re)satisfy their specification. In this paper, we show that a practical method to design masking fault-tolerance is to first design nonmasking fault-tolerance and to then transform the nonmasking fault-tolerant program minimally so as to achieve masking fault-tolerance. We demonstrate this method by designing novel fully distributed programs for termination detection, mutual exclusion, and leader election, that are masking tolerant of any finite number of process fail-stops and/or repairs.
[process fail-stops, Costs, leader election, Design methodology, reliability, mutual exclusion, distributed processing, masking fault-tolerance, formal specification, Fault tolerance, Information science, fully distributed programs, Fault tolerant systems, distributed systems, computer network reliability, Feedforward systems, nonmasking fault-tolerance, Redundancy, Nominations and elections, specification, termination detection, Transaction databases, Circuit faults, formal methods, fault tolerant computing]
Configurable highly available distributed services
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
The paper addresses the problem of providing highly available services in distributed systems. In particular, we examine the situation where a service may be used by a large continuously changing set of clients. The requirements for providing services in this environment are analysed and an architecture and partial implementation for a replicated server group meeting a range of client requirements is presented. The architecture facilitates the dynamic configuration management of the replicated server group, while maintaining the service. Dynamic configuration management is required in order to replace failed replicas, upgrade the server implementation, or change the availability characteristics of the service. The paper reports on initial implementation results.
[Availability, replicated server group, replicas, Access protocols, client requirements, dynamic configuration management, distributed processing, Educational institutions, Maintenance, configuration management, Content addressable storage, Fault tolerant systems, Intersymbol interference, Open systems, Software systems, distributed systems, Hardware, fault tolerant computing, configurable highly available distributed services]
Supporting semantics-based transaction processing in mobile database applications
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
Advances in computer and telecommunication technologies have made mobile computing a reality. However, greater mobility implies a more tenuous network connection and a higher rate of disconnection. In order to tolerate disconnections as well as to reduce the delays and cost of wireless communication, it is necessary to support autonomous mobile operations on data shared by stationary hosts. This would allow the part of a computation executing on a mobile host to continue executing while the mobile host is not connected to the network. In this paper, we examine whether object semantics can be exploited to facilitate autonomous and disconnected operation in mobile database applications. We define the class of fragmentable objects which may be split among a number of sites, operated upon independently at each site, and then recombined in a semantically consistent fashion. A number of objects with such characteristics are presented and an implementation of fragmentable stacks is shown and discussed.
[transaction processing, stationary hosts, Costs, wide area networks, semantics-based transaction processing, Mobile communication, mobile database applications, Transaction databases, Telecommunication computing, Application software, Concurrent computing, Computer science, Intelligent networks, delays, distributed databases, Computer networks, Mobile computing, portable computers]
Failure detection algorithms for a reliable execution of parallel programs
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
We report on the design and simulation of novel algorithms which will ensure that application software runs correctly on a MIMD system in which processing units (PU) can fail. The effect of these algorithms is evaluated for random task graphs using simulation as failure rates increase. An example of a specific application is also examined (the Fast Fourier Transform) for which we construct the task graph and then simulate its execution under various values of the failure rates of processors.
[Algorithm design and analysis, task graph, failure rates, Computational modeling, Software algorithms, reliability, reliable execution, Application software, Surges, system recovery, parallel processing, random task graphs, Delay, parallel programs, Fast Fourier transforms, Databases, Parallel processing, MIMD system, fault tolerant computing, failure detection algorithms, Detection algorithms]
The performance of consistent checkpointing in distributed shared memory systems
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
This paper presents the design and implementation of a consistent checkpointing scheme for distributed shared memory (DSM) systems. Our approach relies on the integration of checkpoints within synchronization barriers already existing in applications; this avoids the need to introduce an additional synchronization mechanism. The main advantage of our checkpointing mechanism is that performance degradation arises only when a checkpoint is being taken; hence, the programmer can adjust the trade-off between the cost of checkpointing and the cost of longer rollbacks by adjusting the time between two successive checkpoints. The paper compares several implementations of the proposed consistent checkpointing mechanism (incremental, non-blocking, and pre-flushing) on the Intel Paragon multicomputer for several parallel scientific applications. Performance measures show that a careful optimization of the checkpointing protocol can reduce the time overhead of checkpointing from 8% to 0.04% of the application duration for a 6 mn checkpointing interval.
[Checkpointing, program debugging, Protocols, Costs, message passing, synchronization barriers, Random access memory, Intel Paragon multicomputer, Computer crashes, Time measurement, performance degradation, synchronisation, Degradation, performance, consistent checkpointing, Message passing, rollbacks, distributed memory systems, distributed shared memory systems, shared memory systems, Hardware, parallel scientific applications, Frequency synchronization, software performance evaluation]
An integer programmimg approach for assigning votes in a distributed system
Proceedings. 14th Symposium on Reliable Distributed Systems
None
1995
Voting is a general approach to maintain consistency of replicated data under node failures and network partitions. In voting, each node as assigned a particular number of votes, and any group with majority of votes can perform operations. Votes assigned to the nodes have a significant impact on the performance of a voting system. In this report, we propose an integer programming approach for determining the vote assignment for maximizing the throughput. We use Monte-Carlo simulation to find the most likely groups formed due to partition failures and use these groups to formulate vote assignment as an integer programming problem. We have developed a tool called vote assignment tool (VAT) that implements this approach. VAT takes as input the configuration of the network, and after formulating the problem as integer programming exercise, solves it to output a vote assignment. We have tried this approach for different networks and have found that in many cases this approach assigns votes equivalent to or better than the best vote assignment given by the various heuristics.
[Availability, replicated databases, integer programming, Maintenance engineering, distributed processing, distributed system, Linear programming, Data engineering, Throughput, partition failures, Distributed computing, Monte-Carlo simulation, Computer science, Voting, network partitions, Computer networks, fault tolerant computing, integer programmimg approach, votes assignment, Communication networks, node failures, replicated data]
Fail-aware failure detectors
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
In existing asynchronous distributed systems it is impossible to implement failure detectors which are perfect, i.e. they only suspect crashed processes and eventually suspect all crashed processes. Some recent research has however proposed that any "reasonable" failure detector for solving the election problem must be perfect. We address this problem by introducing two new classes of fail-aware failure detectors that are (1) implementable in existing asynchronous distributed systems, (2) not necessarily perfect, and (3) can be used to solve the election problem. In particular we show that there exists a fail-aware failure detector that allows to solve the election problem and which is strictly weaker than a perfect failure detector.
[Nominations and elections, failure detectors, Computer crashes, asynchronous distributed systems, fail-aware failure detectors, Computer science, Fault tolerant systems, Detectors, Broadcasting, Hardware, fault tolerant computing, Workstations, Safety, crashed processes, Clocks, election problem]
Developing reliable applications on cluster systems
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
A cluster is a group of computers which are loosely connected together to provide fast and reliable services. There have been many applications built on cluster systems such as distributed/parallel database applications, telecommunication systems and, recently, internet/intranet servers. Cluster systems can deliver similar or better performance and reliability than traditional mainframes, supercomputers and fault-tolerant systems with a much lower hardware cost.
[hardware cost, Computerized monitoring, reliability, File servers, Telecommunication computing, Image restoration, Maintenance, Application software, Distributed computing, Middleware, software fault tolerance, internet/intranet servers, Fault tolerant systems, telecommunication systems, distributed/parallel database, cluster systems, Resource management]
Exploiting data-flow for fault-tolerance in a wide-area parallel system
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Wide-area parallel processing systems will soon be available to researchers to solve a range of problems. In these systems, it is certain that host failures and other faults will be a common occurrence. Unfortunately, most parallel processing systems have not been designed with fault-tolerance in mind. Mentat is a high-performance object-oriented parallel processing system that is based on an extension of the data-flow model. The functional nature of data-flow enables both parallelism and fault-tolerance. In this paper, we exploit the data-flow underpinning of Mentat to provide easy-to-use and transparent fault-tolerance. We present results on both a small-scale network and a wide-area heterogeneous environment that consists of three sites: the National Center for Supercomputing Applications, the University of Virginia and the NASA Langley Research Center.
[Software prototyping, wide area networks, data-flow model, NASA Langley Research Center, small-scale network, high-performance object-oriented parallel processing system, National Center for Supercomputing Applications, World Wide Web, Application software, Distributed computing, Concurrent computing, Fault tolerance, wide-area heterogeneous environment, transparent fault tolerance, Space technology, Fault tolerant systems, Bandwidth, Parallel processing, Virginia University, Mentat, wide-area parallel processing systems, host failures]
Ongoing fault diagnosis
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
We consider a dynamic fault diagnosis problem: there are n processors, to be tested in a series of rounds. In every testing round we use a directed matching to have some processors report on the status (good or faulty) of other processors. Also, in each round up to t processors may break down, and we may direct that up to t processors are repaired. We show that it is possible to limit the number of faulty processors to O(t log2 t), even if the system is run indefinitely. We present an adversary which shows that this bound is optimal.
[Performance evaluation, System testing, Protocols, multiprocessing systems, Mathematics, directed matching, Fault diagnosis, Upper bound, dynamic fault diagnosis, optimisation, Automatic testing, Fault tolerant systems, multiprocessor system, Distributed algorithms, computational complexity]
Diagnosing crosstalk-faulty switches in photonic networks
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
A procedure for diagnosing crosstalk and crosstalk-faulty switches in photonic dilated Benes networks (DBNs) is presented. It obtains the crosstalk ratios of each and every switch in an N/spl times/N DBN in 4N tests, along with O(N/spl middot/log/sup 2/N) calculations. One of its applications is to identify single or multiple switches in the DBN which are generating excessive crosstalk, or crosstalk-faulty. A recursive algorithm is used to configure the DBN for each test such that the necessary power measurements of the signals can be taken accurately. An important feature of the proposed diagnostic procedure is its suitability for automated test generation.
[fault diagnosis, photonic dilated Benes networks, Crosstalk, Switches, Telecommunication switching, Optical fiber networks, Encoding, Communication switching, electrooptical switches, Intelligent networks, Power measurement, Automatic testing, Bandwidth, recursive algorithm, optical communication network, optical communication, crosstalk-faulty switches, crosstalk ratios, computational complexity]
A transparent light-weight group service
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
The virtual synchrony model for group communication has proven to be a powerful paradigm for building distributed applications. Implementations of virtual synchrony usually require the use of failure detectors and failure recovery protocols. In applications that require the use of a large number of groups, significant performance gains can be attained if these groups share the resources required to provide virtual synchrony. A service that maps user groups onto instances of a virtually synchronous implementation is called a light-weight group service. This paper proposes a new design for the light-weight group protocols that enables the usage of this service in a transparent manner as a test case, the new design was implemented in the Horus system, although the underlying principles can be applied to other architectures as well. The paper also presents performance results from this implementation.
[System testing, Protocols, Performance gain, transparent light-weight group service, File servers, virtual synchrony model, group communication, distributed applications, File systems, System performance, Intersymbol interference, Detectors, Bandwidth, protocols, Object oriented programming]
Observations from 16 years at a fault-tolerant computer company
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Observations acquired from 16 years of experience working for a vendor of fault-tolerant computer systems are presented, along with two "war stories" that illustrate some of the principles.
[Availability, Technological innovation, war stories, Telecommunication computing, fault-tolerant computer company, Fault tolerance, Engineering management, Fault tolerant systems, vendor, Computer industry, Hardware, fault tolerant computing, Reliability, Software engineering]
Minimizing timestamp size for completely asynchronous optimistic recovery with minimal rollback
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Basing rollback recovery on optimistic message logging and replay avoids the need for synchronization between processes during failure-free execution. Some previous research has also attempted to reduce the need for synchronization during recovery, but these protocols have suffered from three problems: not eliminating all synchronization during recovery, not minimizing rollback, or providing these properties but requiring large timestamps. This paper makes two contributions: we present a new rollback recovery protocol, based on our previous work, that provides these properties (asynchronous recovery, minimal rollback) while reducing the timestamp size; and we prove that no protocol can provide these properties and have asymptotically smaller timestamps.
[timestamp size, Protocols, optimistic message logging, replay, Distributed computing, Delay, synchronisation, Computer science, Fault tolerance, asynchronous optimistic recovery, rollback recovery protocol, Fault tolerant systems, asynchronous recovery, Cost function, failure-free execution, rollback recovery, minimal rollback, Contracts]
A fault-tolerant CORBA name server
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
OMG CORBA applications require a distributed naming service in order to install and to retrieve object references. High availability of the naming service is important since most CORBA applications need to access it at least once during their lifetime. Unfortunately, the OMG standards do not deal with availability issues; the naming services of many of the commercially available CORBA object request brokers introduce single points of failure. In this paper we describe the design and implementation of a replicated, highly-available CORBA name server that adheres to the OMG Common Object Services Specification. Our naming service can be replicated at run-time, while many applications are installing and retrieving object references. We compare our approach with the approaches taken by the ILU, NEO, Orbix, and DOME object request brokers. The performance of our name server is measured for various degrees of replication.
[Availability, Costs, Application software, object references, Computer science, Fault tolerance, Computer languages, Runtime, Management information systems, Computer architecture, object request, CORBA name server, System software, naming services, distributed naming service, replicated, fault-tolerant]
Analyzing dynamic voting using Petri nets
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Dynamic voting is considered a promising technique for achieving high availability in distributed systems with data replication. To date, stochastic analysis of dynamic voting algorithms is restricted to either site or link Markov models, but not both, possibly because of the difficulty in specifying the state-space which grows exponentially as the number of sites increases. Furthermore, to reduce the state-space, the assumption of "frequent updates" was normally made, which results in an overestimation of the availability. In this paper, we develop a Petri net model that considers both site and link failures and also relaxes the modeling assumption of frequent updates. We test our Petri net model on ring and star network topologies to analyze if availability under dynamic voting can be seriously degraded if updates are not frequent under various site and link failure/repair situations. Finally, We use the Petri net developed in the paper to determine the maximum achievable improvement in availability when null updates are introduced to augment regular updates to keep the status of availability up-to-date.
[Availability, Algorithm design and analysis, Heuristic algorithms, Petri nets, Stochastic processes, dynamic voting, maximum achievable improvement, availability, Degradation, high availability, Network topology, Voting, Petri net model, data replication, Failure analysis, distributed systems, Testing]
Specialized N-modular redundant processors in large-scale distributed systems
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Computers are being used to achieve increasingly sophisticated control for large and complex systems. Many of these systems require a large shared state-space or database. Thus, handling real-time concurrent accesses to a shared database is an essential feature for modern fault-tolerant systems. Many fault-tolerant systems have been implemented for uniformly tolerating various types of failures, such as MAFT (Multicomputer Architecture for Fault Tolerance), FTP (Fault-Tolerant Processor), FTPP (Fault-Tolerant Parallel Processors) and Delta-4. However, most of these either lack the notion of a shared state-space or do not efficiently support parallel tasks that concurrently access a shared state-space. We use a processor-specialization approach to increase the effectiveness of replication and, consequently, achieve cost-effective fault tolerance in such systems. The SNMR (specialized N-modular redundancy) protocol has been developed based on these concepts. Compared to many existing Byzantine-resilient systems, the SNMR approach incurs less overhead and can be easily parameterized to fit various fault models.
[shared state-space, fail-stop failures, large shared database, Control systems, Distributed computing, fault-tolerant systems, complex systems control, Concurrent computing, Fault tolerance, Fault tolerant systems, processor specialization, Large-scale systems, parallel tasks, specialized N-modular redundant processors, Byzantine-resilient systems, overhead, Access protocols, Electromagnetic radiation, Spatial databases, State-space methods, large-scale distributed systems, cost-effective fault tolerance, replication effectiveness, large-scale systems, malicious failures, real-time concurrent accesses, parameterizability, SNMR protocol]
Strong and weak virtual synchrony in Horus
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
This paper presents two variants of virtual synchrony, which are supported by Horus. The first variant, called strong virtual synchrony, includes the property that every message is delivered within the view in which it is sent. This property is very useful in developing applications, since it helps in minimizing the amount of context information that needs to be sent on messages, and the amount of computation which is required in order to process a message. However, it is shown that in order to support this property, the application program has to block messages during view changes. An alternative definition, called weak virtual synchrony, which can be implemented without blocking messages, is then presented. This definition still guarantees that messages will be delivered within the view in which they were sent, only that it uses a slightly weaker notion of what the view in which a message was sent is. An implementation of weak virtual synchrony that does not block messages during view changes as also developed in this paper.
[Computer science, application program, Horus, Intersymbol interference, context information, weak virtual synchrony, Computer crashes, Communication networks, Telecommunication network reliability, communication complexity, strong virtual synchrony]
The design of a CORBA group communication service
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
The common object request broker architecture (CORBA) is becoming a standard for distributed application middleware, and there are increasing needs for enriching the basic functionalities of CORBA. While mechanisms for persistence, transactions, event channels, etc. have been designed and specified for CORBA, no standard support is provided to handle object replication. In this paper we discuss the issue of augmenting CORBA with group communication, which is considered an adequate paradigm to handle replication. We distinguish two main approaches: the integration approach and the service approach. We argue that the service approach is more appropriate to CORBA as it preserves the modularity of the architecture. We describe a proposal for a group communication service and discuss some implementation issues.
[Availability, replicated databases, distributed application middleware, object replication, service approach, Computer crashes, Proposals, transactions, Middleware, common object request broker architecture, Programming profession, event channels, Fault tolerance, CORBA group communication service, Disaster management, persistence, Power system reliability, Telecommunication network reliability, Contracts, integration approach]
A proposal for ensuring high availability of distributed multimedia applications
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Recent advances in computing, like high-speed networks and data-compression, make extensible distributed multimedia applications a challenging application-domain of distributed systems. Such applications like VoD (Video on Demand) or real-time conferencing are characterized by QoS (quality of service) requirements which depend on the quality of video and sound transmitted to the client and on the respect of time constraints associated to video and audio data. Much work has been done in order to provide system support aimed at meeting these requirements. However, existing proposals do not integrate the consequence of failure occurrence on the guaranteed QoS. To deal with this issue, we propose a resource reservation model that integrates availability requirements of multimedia services in addition to the QoS constraints introduced above. Our paper details the resulting model together with its integration in a distributed system. In particular we show how the model implementation can be customized in the case of a VoD server.
[Availability, distributed multimedia, Video on demand, Multimedia systems, Quality of service, Multimedia computing, real-time conferencing, multimedia systems, distributed system, VoD server, Proposals, Videoconference, Distributed computing, Video on Demand, resource reservation model, high-speed networks, availability requirements, High-speed networks, data-compression, failure occurrence, Computer networks, model implementation]
Improving the performance of coordinated checkpointers on networks of workstations using RAID techniques
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Coordinated checkpointing systems are popular and general-purpose tools for implementing process migration, coarse-grained job swapping, and fault-tolerance on networks of workstations. Though simple in concept, there are several design decisions concerning the placement of checkpoint files that can impact the performance and functionality of coordinated checkpointers. Although several such checkpointers have been implemented for popular programming platforms like PVM and MPI, none have taken this issue into consideration. This paper addresses the issue of checkpoint placement and its impact on the performance and functionality of coordinated checkpointing systems. Several strategies, both old and new, are described and implemented on a network of SPARC-5 workstations running PVM. These strategies range from very simple to more complex borrowing heavily from ideas in RAID (Redundant Arrays of Inexpensive Disks) fault-tolerance. The results of this paper will serve as a guide so that future implementations of coordinated checkpointing can allow their users to achieve the combination of performance and functionality that is right for their applications.
[Checkpointing, fault-tolerance, Optimization methods, SPARC-5 workstations, performance evaluation, RAID techniques, coarse-grained job swapping, History, Computer science, workstations, process migration, Fault tolerance, Parallel programming, performance, Fault tolerant systems, Marketing and sales, Workstations, Clocks, coordinated checkpointers, networks of workstations]
A causal message ordering scheme for distributed embedded real-time systems
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
In any distributed system, messages must be ordered according to their cause-and-effect relation to ensure correct behavior of the system. Causal ordering is also essential for services like atomic multicast and replication. In distributed real-time systems, not only must proper causal ordering be ensured, but message deadlines must be met as well. Previous algorithms which ensure such behavior include the /spl Delta/-protocol family and the MARS approach. However, both these algorithms give large response times by delaying all messages for a fixed period of time. In this paper we show that for small- to medium-sized real-time systems (consisting of a few tens of nodes) as are commonly used for embedded applications, it becomes feasible to extend the h-protocol so that instead of delaying all messages for a fixed period, each message is delayed according to its deadline. Our algorithm requires certain message deadlines to be adjusted by the application designer and we show that for small-scale applications such as those used in embedded systems, this adjustment is feasible and can be automated by the use of proper CAD tools.
[Real time systems, Algorithm design and analysis, Design automation, distributed real-time systems, Delay effects, Laboratories, distributed, Distributed computing, Temperature sensors, Mars, Multicast algorithms, message deadlines, Embedded system, real-time systems, embedded applications, causal ordering, embedded, causal message ordering scheme]
On-line testing for application software of widely distributed system
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Widely distributed systems are constructed step-by-step over a long lime. These systems must permit on-line testing. On-line testing verifies newly added application software by receiving the real data in the real environment without disrupting the operating subsystems. To enable testing during system operation, an on-line test technique based on autonomous decentralized system structure was proposed. In this paper, the functions to verify various types of application software are proposed: (1) for non-real-time application software, checking whether it communicates with every operating application software. (2) for real-time application software, checking whether timing of output data is within the timing-deadline or not. (3) for new version software, verifying whether the version of all functions is newer than that off functions in the present software. An example of application software applied to the on-line test and the effectiveness of the technique is shown in a real system.
[Software testing, Real time systems, System testing, application software, program testing, autonomous decentralized system, Laboratories, Process control, Control systems, Software safety, Application software, on-line testing, timing-deadline, Automatic control, Timing, widely distributed system, on-line test]
Implementation and performance of a stable-storage service in Unix
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
This paper describes the design, implementation, and performance of a stable-storage service that has been implemented on top of the Unix operating system. This service allows servers to create, access, and delete persistent memory that survives server crashes. We describe its functionality and exported operations, discuss the experiences and performance of its implementation, and offer concrete examples of its use in implementing some real fault-tolerant distributed protocols.
[Unix, stable-storage service, Access protocols, File servers, Computer crashes, fault-tolerant distributed protocols, functionality, Programming profession, Computer science, Fault tolerance, Design engineering, servers, Operating systems, Memory management, Concrete, fault tolerant computing]
Dynamic fault tolerance in DCMA-a dynamically configurable multicomputer architecture
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
This paper introduces a new architecture for a fault-tolerant computer system which connects high-end PCs or workstations by a high-speed network. To achieve platform independence, coupling is based on the widely used PCI-bus. In contrast to commercially available fault-tolerant systems we strongly emphasize mechanisms for tolerating transient and intermittent faults. To keep hardware costs low the system is built with off-the-shelf computers and their extensions are kept as small as possible. To reduce the operational costs the system can be dynamically adapted to different demands on fault tolerance on a program-by-program basis. Adaptation is done transparently to the application software by the operating system. We use a commercially available real-time operating system with a POSIX-compliant UNIX-interface. The bandwidth of fault tolerance reaches from a non-redundant system of stand-alone computers, a master/checker configuration to a TMR-system. The high-performance network allows the system to operate as a parallel multicomputer, too.
[dynamically configurable multicomputer architecture, application software, Costs, PCI-bus, transient faults, Fault tolerance, High-speed networks, POSIX-compliant UNIX-interface, Operating systems, dynamic fault tolerance, Fault tolerant systems, Computer architecture, fault-tolerant computer system, Computer networks, Hardware, fault tolerant computing, Workstations, intermittent faults, Personal communication networks, real-time operating system]
Primary copy method and its modifications for database replication in distributed mobile computing environment
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Rapidly expanding cellular communication technology, wireless LANs and satellite services have made it possible for mobile users to access information anywhere and at any time. In a mobile computing environment replication might be considered as an essential technique providing reliability, throughput increase and data availability. This paper addresses the replica control protocols with an emphasis on workstation mobility issues. The modifications that have to be made to the primary copy method for replicated database management strategies in order to address the effect of mobility on the existing replica control protocols are analysed and proposed. A variation of the primary copy algorithm, called virtual primary copy method is proposed and it is shown that this method is well suited for the distributed mobile computing environment. The performance of virtual primary copy method comparative to traditional primary copy method using computer simulation is analysed.
[Availability, wireless LANs, Wireless LAN, virtual primary copy method, satellite services, Communication system control, Access protocols, Throughput, primary copy method, replicated database management, cellular communication, workstation mobility, Artificial satellites, mobile computing, replica control protocols, Databases, Communications technology, Workstations, mobile users, wireless LAN, Mobile computing]
Hierarchical adaptive distributed system-level diagnosis applied for SNMP-based network fault management
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
Fault management is a key functional area of network management systems, but currently deployed applications often implement rudimentary diagnosis mechanisms. This paper presents a new hierarchical adaptive distributed system-level diagnosis (Hi-ADSD) algorithm and its implementation based on SNMP (simple network management protocol). Hi-ADSD is a fully distributed algorithm that has diagnosis latency of at most (log/sub 2/N)/sup 2/ testing rounds for a network of N nodes. Nodes are mapped into progressively larger logical clusters, so that each node executes tests in a hierarchical fashion. The algorithm assumes no link faults, a fully-connected network and imposes no bounds on the number of faults. Both the worst-case diagnosis latency and correctness of the algorithm are formally proved. Experimental results are given through simulation of the algorithm for large networks. The algorithm was implemented on a small network using SNMP. We present details of the implementation, including device fault management, the role of the network management station, and the diagnosis management information base.
[Adaptive systems, Protocols, fault diagnosis, Delay, network nodes, Hi-ADSD algorithm, Fault diagnosis, distributed algorithm, Technology management, network fault management, worst-case diagnosis latency, Clustering algorithms, local area network, LAN, hierarchical adaptive distributed system-level diagnosis, Computer network management, Distributed algorithms, Monitoring, simple network management protocol, Testing]
Analysis of a multistage interconnection network using binary decision diagrams (BDD)
Proceedings 15th Symposium on Reliable Distributed Systems
None
1996
The authors use the BDD to help derive a closed-form solution for the reliability of a multistage interconnection network with n stages. The BDD reveals repeated structures, the reliability of which can be encoded in a recursive formula. An exact solution of a network with an arbitrary number of stages can be computed in time proportional to the number of stages. They also provide results which include the concept of imperfect coverage, in which two mutually-exclusive failure modes (with different effects) are possible for certain switching elements.
[recursive formula, Multiprocessor interconnection networks, Semiconductor device reliability, repeated structures, Binary decision diagrams, Switches, reliability, Data structures, Closed-form solution, mutually-exclusive failure modes, multistage interconnection networks, closed-form solution, Communication switching, Postal services, Fault tolerance, binary decision diagrams, Boolean functions, switching elements, exact solution, computation time, multistage interconnection network analysis, imperfect coverage]
Reliable software systems using reusable software components
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Summary form only given, as follows. J. Von Neumann (1956) and E.F. Moore and C.F. Shannon (1956) discussed building reliable automata and relay circuits (hardware) using less reliable components. They show that carefully designed replication of components in a hardware system can increase the probability of failure free operation of that system. There is a powerful trend in the industry now to build software systems using as many software components as possible. These components might be commercial off the shelf (COTS) or in-house software libraries and modules; we call all such components reusable software components. We argue that the reliability of such a software system can be improved not only by replicating the software components, but also by active monitoring, checkpointing and rejuvenation, and providing facilities for cold, warm and hot fail over/restart of those components. These capabilities themselves can be built as reusable software modules that can be linked to the actual system components. We present the architecture of such a software system and a preliminary analysis to show the feasibility of this approach for building reliable software systems using reusable software components. Research into analyzing the reliability of such systems is gaining attention. These facilities provide diversity in the execution environment of a software component leading to a higher level of reliability of the software system, much as replication provides diversity in the physical environment of a hardware component giving rise to improved reliability of the hardware system that Von Neumann and others have pioneered.
[checkpointing, Circuits, software reliability, component replication, software libraries, relay circuits, failure free operation, execution environment, commercial off the shelf, Power system relaying, Hardware, rejuvenation, Software reusability, less reliable components, hardware system reliability, in-house software libraries, Buildings, reusable software components, reliable software systems, Software libraries, reusable software modules, Automata, reliable automata, software reusability, Software systems, Computer industry, Power system reliability]
Probabilistic verification of a synchronous round-based consensus protocol
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Consensus protocols are used in a variety of reliable distributed systems, including both safety-critical and business-critical applications. The correctness of a consensus protocol is usually shown, by making assumptions about the environment in which it executes, and then proving properties about the protocol. But proofs about a protocol's behavior are only as good as the assumptions which were made to obtain them, and violation of these assumptions can lead to unpredicted and serious consequences. We present a new approach for the probabilistic verification of synchronous round based consensus protocols. In doing so, we make stochastic assumptions about the environment in which a protocol operates, and derive probabilities of proper and non proper behavior. We thus can account for the violation of assumptions made in traditional proof techniques. To obtain the desired probabilities, the approach enumerates possible states that can be reached during an execution of the protocol, and computes the probability of achieving the desired properties for a given fault and network environment. We illustrate the use of this approach via the evaluation of a simple consensus protocol operating under a realistic environment which includes performance, omission, and crash failures.
[Performance evaluation, Protocols, program verification, software reliability, Stochastic processes, Distributed computing, realistic environment, Network servers, formal verification, probabilistic verification, crash failures, Computer networks, Space exploration, protocols, business-critical applications, probabilities, Contracts, synchronous round based consensus protocols, synchronous round based consensus protocol, probability, simple consensus protocol, Computer crashes, network environment, safety-critical applications, reliable distributed systems, High performance computing, stochastic assumptions, protocol behavior, proper behavior, consensus protocol correctness, traditional proof techniques]
Tradeoffs when integrating multiple software components into a highly available application
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
We analyze nineteen months of field trouble reports for a particular highly available software application. We use this data to understand when it makes sense for the creator of a highly available software application to buy commercial software subcomponents rather than building them in-house. In particular we identify a number of potential negative impacts on system operability which can occur if too many or inappropriately chosen commercial software components are used. Our data indicates that the prudent application builder must carefully weigh these potential negative operability impacts against the well known potential cost savings that can accrue from using commercial software components.
[Costs, software reliability, Programming, Transaction databases, Application software, Middleware, system operability, parallel programming, highly available application, Communication system software, multiple software component integration, Operating systems, Software systems, field trouble reports, highly available software application, Robustness, commercial software subcomponents, commercial software components, potential negative operability impacts, Reliability, software performance evaluation, potential negative impacts, prudent application builder, potential cost savings]
A fail-aware membership service
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
We propose a new protocol that can be used to implement a partitionable membership service for timed asynchronous systems. The protocol is fail-aware in the sense that a process p knows at all times if its approximation of the set of processes in its partition is up-to-date or out-of-date. The protocol minimizes wrong suspicions of processes by giving processes a second chance to stay in the membership before they are removed. Our measurements show that the exclusion of live processes is rare and the crash detection times are good. The protocol guarantees that the memberships of two partitions never overlap.
[Protocols, timed asynchronous systems, software reliability, reliability, distributed processing, Computer crashes, partitionable membership service, Synchronization, asynchronous distributed systems, Delay, time free asynchronous system model, Computer science, crash detection times, live processes, protocol, fail aware membership service, wrong suspicions, second chance, Hardware, fault tolerant computing, Workstations, protocols, Marine vehicles, Clocks]
The art of creating reliable software-based systems using off-the-shelf software components
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Observations on creating reliable software systems using off-the-shelf software components, acquired from 17 years of experience working for a vendor of fault-tolerant computer systems are presented.
[Software testing, Software maintenance, software development, off-the-shelf software components, software reuse, Subspace constraints, software reliability, software development management, DP industry, software vendor, Application software, Operating systems, Fault tolerant systems, software packages, software reusability, fault-tolerant computer systems, Software systems, Cost function, Hardware, Software standards]
Fault detection using hints from the socket layer
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Describes a fault detection mechanism that uses the error codes returned by stream sockets to locate process failures. Since these errors are generated automatically when there is communication with a failed process, the mechanism does not incur in any failure-free overheads. However, for some types of faults, detection can only be attained if the surviving processes use certain communication operations. To assess the coverage and latency of the proposed mechanism, faults were injected during the execution of parallel applications. Our results show that in most cases, faults could be found using only the errors from the socket layer. Depending on the type of fault that was injected, detection occurred in an interval ranging from a few milliseconds to less than nine minutes.
[Protocols, fault diagnosis, parallel applications execution, latency, error detection, parallel processing, system recovery, Delay, Computer networks, Safety, fault injection, error codes, Contracts, Availability, coverage, Computer crashes, fault detection mechanism, stream sockets, process failure location, communication operations, Fault detection, Sockets, automatically generated errors, Computer errors, socket layer, surviving processes]
A supervisor-based semi-centralized network surveillance scheme and the fault detection latency bound
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Network surveillance (NS) schemes facilitate fast learning by each interested fault free node in the system of the faults or repair completion events occurring in other parts of the system. Currently concrete real time NS schemes effective in distributed computer systems based on point to point network architectures are scarce. We present a semi centralized real time NS scheme effective in a variety of point to point networks, called the supervisor based NS (SNS) scheme. This scheme is highly scalable and can be implemented entirely in software using commercial off the shelf (COTS) components without requiring any special purpose hardware support. An efficient execution support for the scheme has been designed as a new extension of the DREAM kernel, a timeliness guaranteed operating system kernel model developed in the authors' laboratory. This design can be viewed as an implementation model which can be easily adapted to various commercial operating system kernels. The paper also presents an analysis of the SNS scheme on the basis of the implementation model to obtain some tight bounds on the fault detection latency.
[Real time systems, distributed computer systems, fault detection latency bound, fault diagnosis, network surveillance schemes, execution support, semi centralized real time NS scheme, Distributed computing, telecommunication computing, concrete real time NS schemes, supervisory programs, Operating systems, commercial off the shelf, Computer architecture, Computer networks, Hardware, computer network reliability, Kernel, DREAM kernel, operating system kernels, timeliness guaranteed operating system kernel model, performance evaluation, supervisor based NS scheme, commercial operating system kernels, fault free node, Surveillance, Fault detection, real-time systems, supervisor based semi centralized network surveillance scheme, implementation model, Concrete, point to point network architectures, SNS scheme, repair completion events]
Reliability Of Distributed Applications With COTS Components
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
false
[System testing, Software libraries, USA Councils, Software systems, Reliability engineering, Software reliability, Application software, Distributed computing, Software engineering]
Chameleon: adaptive fault tolerance using reliable, mobile agents
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
In networked computing systems, a broad range of commercial and scientific applications that need varying degrees of availability must coexist. It is not cost-effective to develop a reliable platform in each case. It is more efficient to build an infrastructure that provides the required level of dependability for each application's needs. It is also essential that the proposed alternatives should leverage off-the-shelf components. There have been exhaustive studies on fault tolerance strategies capable of providing efficient mechanisms to deal with system operational failures. Most of this work has focused on specific application needs and thus provided only piecemeal solutions. Little work has been done in addressing how to build a reliable networked computing system out of unreliable computation nodes. As a result, there is no comprehensive solution for providing a wide range of fault-tolerant services in a single networked environment. The most feasible way of understanding how such a software environment would fit on top of existing layers (the operating system, the network interfaces, etc.) is to implement an infrastructure for providing a range of reliable services. Fundamental components of the envisioned infrastructure (Chameleon) have been designed so that none of them is a single point of failure. Each of the components is active for a certain period, e.g. during the setting up the system configuration. If a component fails during its active phase, there is a provision for recovery, either by switching to a backup or by regenerating the component.
[unreliable computation nodes, backup, adaptive fault tolerance, system configuration setup, scientific applications, availability, Environmental management, system recovery, Chameleon, reliable networked computing systems, Fault diagnosis, Fault tolerance, component failure, Mobile agents, system operational failures, Computer networks, application needs, computer network reliability, component active period, Availability, commercial applications, cost-effectiveness, dependability, Application software, software agents, Intelligent agent, adaptive systems, off-the-shelf components, component regeneration, Software libraries, software environment, fault tolerant computing, reliable mobile agents, fault-tolerant services, Remote monitoring]
Preventing useless checkpoints in distributed computations
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
A useless checkpoint is a local checkpoint that cannot be part of a consistent global checkpoint. The paper addresses the following important problem. Given a set of processes that take (basic) local checkpoints in an independent and unknown way, the problem is to design a communication induced checkpointing protocol that directs processes to take additional local (forced) checkpoints to ensure that no local checkpoint is useless. A general and efficient protocol answering this problem is proposed. It is shown that several existing protocols that solve the same problem are particular instances of it. The design of this general protocol is motivated by the use of communication induced checkpointing protocols in "consistent global checkpoint" based distributed applications. Detection of stable or unstable properties, rollback recovery and determination of distributed breakpoints are examples of such applications.
[Checkpointing, Protocols, Communication system control, reliability, distributed processing, Distributed computing, system recovery, Degradation, distributed breakpoints, useless checkpoints, consistent global checkpoint based distributed applications, protocols, rollback recovery, local forced checkpoints, efficient protocol, communication induced checkpointing protocol, Computational modeling, Process control, performance evaluation, basic local checkpoints, software fault tolerance, Computer science, communication induced checkpointing protocols, distributed computations, fault tolerant computing, local checkpoint, unstable properties]
Selecting a "primary partition" in partitionable asynchronous distributed systems
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
We consider network applications that are based on the process group paradigm. When such applications are deployed over networks that are subject to failures, they may partition across several disconnected clusters resulting in multiple views of the group's current composition to exist concurrently. Application semantics determine which operations, if any, can be performed in different partitions without compromising consistency. For certain application classes, most (possibly all) operations need to be confined to a single primary partition while other partitions are allowed to service only a (possibly empty) subset of the operations. We propose a mechanism for deciding when a view constitutes the primary partition for the group. Our solution is highly flexible and has the following novel features: each group member can establish if it belongs to the primary partition or not, based solely on local information; the group can be dynamic as processes voluntarily join and leave it; the selection rule for establishing the primary partition need not be universal but can be decided on a per-application basis and can be modified at run time; the primary partition can be re-established even after total failures. Layering our solution on top of a partitionable group membership service allows a wide range of applications with different and possibly conflicting notions of "primary partition" to be supported on a common computing base.
[disconnected clusters, process group paradigm, partitionable group membership service, common computing base, reliability, partitionable asynchronous distributed systems, Electronic mail, total failures, multiple views, single primary partition, Intelligent networks, Runtime, selection rule, group member, performance evaluation, local information, Computer crashes, data integrity, Partitioning algorithms, network applications, application classes, Computer science, computer network management, application semantics, primary partition, fault tolerant computing]
Controlled stochastic Petri nets
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
A new framework for the extension of stochastic Petri nets (SPNs) is introduced. SPNs are extended by elements providing means for a dynamic optimization of performability measures. A new type of transition is defined, offering a feature for specification of controlled switching, called reconfiguration, from one marking of a SPN to another marking. Optional reconfiguration transitions are evaluated in order to optimize a specified reward or cost function. The result of an analysis is provided in the output of a numerical computation, in the form of a graphical presentation of an optimal, marking dependent control strategy and the resulting performability measure when applying the optimal strategy. The extended SPNs are called COSTPNs (Controlled Stochastic Petri Nets). COSTPNs are mapped on EMRMs (Extended Markov Reward Models) for a numerical analysis. Computational analysis is possible with algorithms adopted from Markov decision theory, including transient and stationary optimization. The scope of the paper is to introduce the new control structure for SPNs and to present an algorithm for the mapping of COSTPNs on EMRMs.
[Performance evaluation, Algorithm design and analysis, EMRMs, COSTPNs, decision theory, Petri nets, Stochastic processes, graphical presentation, specified reward, controlled switching, optimal strategy, numerical computation, numerical analysis, cost function, Cost function, computational analysis, Performance analysis, Numerical models, extended SPNs, stochastic processes, optional reconfiguration transitions, dynamic programming, performance evaluation, performability measures, dynamic optimization, stationary optimization, performability measure, control structure, Numerical analysis, controlled stochastic Petri nets, Markov decision theory, Decision theory, Optimal control, marking dependent control strategy, Markov processes, Extended Markov Reward Models]
Applying simulation to the design and performance evaluation of fault-tolerant systems
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
The paper illustrates how the CESIUM simulation tool can be used for design and performance evaluation of fault tolerant and real time systems, in addition to testing the correctness of protocol implementations. We calibrate three increasingly accurate simulation models of a network of workstations using independently obtained data. For a sample group membership protocol, the predictions of the simulator are very close to the actual performance measured in the real system. We also apply CESIUM to the evaluation of two potential improvements for the protocol, performing experiments that would have been difficult to implement in the real system. The results of the simulations give us valuable insight on how to tune configuration parameters, as well as on the performance gains of the improved versions. Our experience shows that CESIUM can be used to develop best effort services which adapt their quality of service according to the failures that occur during operation.
[Software testing, Performance evaluation, System testing, Protocols, configuration parameter tuning, reliability, digital simulation, telecommunication computing, Engines, Fault tolerance, Fault tolerant systems, protocols, protocol implementations, Java, best effort services, Object oriented modeling, Debugging, performance evaluation, correctness testing, quality of service, accurate simulation models, software fault tolerance, performance gains, network of workstations, virtual machines, sample group membership protocol, fault tolerant computing, CESIUM simulation tool, real time systems, fault tolerant systems]
Load balancing schemes for high-throughput distributed fault-tolerant servers
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Clusters of workstations, connected by a fast network, are emerging as a viable architecture for building high-throughput fault-tolerant servers. This type of architecture is more scalable and more cost-effective than a tightly coupled multiprocessor and may achieve as good a throughput. We explore several combinations of fault tolerance (FT) and load-balancing (LB) schemes, and compare their impact on the maximum throughput achievable by the system, and on its survivability. In particular, we show that the FT scheme has an effect on the throughput of the system, while the LB scheme affects the ability of the system to override failures. We study the scalability of the different schemes under different loads and failure conditions. Our simulations take into consideration the overhead of each scheme, the network contention, and the resource loads.
[workstation clusters, Costs, cost-effective, distributed processing, network contention, Throughput, Control systems, Telecommunication control, local area networks, tightly coupled multiprocessor, resource load, Fault tolerance, Network servers, scalable system, resource allocation, Computer architecture, high-throughput, Hardware, software performance evaluation, client-server systems, load balancing schemes, distributed fault-tolerant servers, software fault tolerance, simulations, Computer science, Load management, survivability, maximum throughput]
A flexible security model for using Internet content
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Java applets, Netscape plug-ins and ActiveX controls have led to the popularization of a new paradigm: extensive downloading of executable code into applications to enhance the functionality of the desktop. One of the problems with this paradigm is the need to control the access rights of the downloaded content. In this paper, we describe a system for downloading content from the Internet and controlling its actions on a client machine. Our system generates a protection domain for the downloaded content dynamically rising the content's requested domain and a policy database that describes the user's trust in the content's manufacturer and type. Our system ensures that this protection domain is enforced throughout the execution of the content. We have modified the Java Virtual Machine to implement our security model. Our implementation, called Flexxguard, is freely available at http://www.alphaworks.ibm.com.
[flexible security model, executable code downloading, policy database, Control systems, desktop functionality, Databases, Permission, Manufacturing, ActiveX controls, Flexxguard, Java applets, Protection, user trust, protection domain, Java, client-server systems, access rights, Data security, Virtual machining, Java Virtual Machine, Internet content, security of data, virtual machines, client machine, Internet, Web sites, Netscape plug-ins]
Predicting dependability properties on-line
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Consider a system put into operation at time t/sub 0/. During the design phase, i.e. before time t/sub 0/, a model /spl Mscr/ is used to tune up certain system parameters in order to satisfy some dependability constraints. Once in operation, some aspects of the system's behavior are observed during the interval [t/sub 0/,t]. In this paper, we show how to integrate into model /spl Mscr/ the observations made during [t/sub 0/,t] in the operational phase, in order to improve the predictions on the behavior of the system after time t.
[Phase measurement, repairable systems, reliability, Predictive models, Reliability engineering, operational phase, reliability theory, dependability constraints, dependability properties, Fault tolerant systems, Software standards, system parameter tuning, Monitoring, Electric breakdown, system behaviour, Time measurement, fault-tolerant computing systems, State-space methods, Software reliability, modelling, online prediction, design phase, online operation, forecasting theory, fault tolerant computing]
Transaction reordering in replicated databases
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
The paper presents a fault tolerant lazy replication protocol that ensures 1-copy serializability at a relatively low cost. Unlike eager replication approaches, our protocol enables local transaction execution and does not lead to any deadlock situation. Compared to previous lazy replication approaches, we significantly reduce the abort rate of transactions and we do not require any reconciliation procedure. Our protocol first executes transactions locally, then broadcasts a transaction certification message to all replica managers, and finally employs a certification procedure to ensure 1-copy serializability. Certification messages are broadcast using a non blocking atomic broadcast primitive, which alleviates the need for a more expensive non blocking atomic commitment algorithm. The certification procedure uses a reordering technique to reduce the probability of transaction aborts.
[Atomic measurements, transaction processing, Costs, eager replication approaches, transaction certification message, abort rate, fault tolerant lazy replication protocol, non blocking atomic commitment algorithm, relatively low cost, Fault tolerance, 1-copy serializability, protocol, replica managers, Distributed databases, distributed databases, Broadcasting, deadlock situation, protocols, transaction aborts, certification procedure, replicated databases, Access protocols, reordering technique, Transaction databases, lazy replication approaches, Certification, software fault tolerance, certification, local transaction execution, non blocking atomic broadcast primitive, reconciliation procedure, System recovery, transaction reordering]
Fast replicated state machines over partitionable networks
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
The paper presents an implementation of replicated state machines in asynchronous distributed environments prone to node failures and network partitions. This implementation has several appealing properties: it guarantees that progress will be made whenever a majority of replicas can communicate with each other; it allows minority partitions to continue providing service for idempotent requests; it offers the application the choice between optimistic or safe message delivery. Performance measurements have shown that our implementation incurs low latency and achieves high throughput while providing globally consistent replicated state machine semantics.
[Costs, globally consistent replicated state machine semantics, reliability, distributed processing, Throughput, finite state machines, Delay, idempotent requests, minority partitions, Fault tolerant systems, Distributed databases, Broadcasting, Large-scale systems, redundancy, Availability, partitionable networks, fast replicated state machines, software fault tolerance, safe message delivery, Computer science, performance measurements, network partitions, fault tolerant computing, asynchronous distributed environments, node failures]
Comparing operating systems using robustness benchmarks
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
When creating mission-critical distributed systems using off-the-shelf components, it is important to assess the dependability of not only the hardware, but the software as well. This paper proposes a way to test operating system dependability. The concept of response regions is presented as a way to visualize erroneous system behavior and gain insight into failure mechanisms. A 5-point "CRASH" (catastrophic, restart, abort, silent, hindering) scale is defined for grading the severity of robustness vulnerabilities encountered. Test results from five operating systems are analyzed for robustness vulnerabilities, and exhibit a range of dependability. Robustness benchmarking comparisons of this type may provide important information to both users and designers of off-the-shelf software for dependable systems.
[robustness vulnerability severity grading, System testing, Visualization, erroneous system behavior visualization, Mission critical systems, software reliability, robustness benchmarks, response regions, operating systems, software dependability assessment, mission-critical distributed systems, off-the-shelf components, CRASH scale, Software design, Operating systems, network operating systems, Failure analysis, Benchmark testing, Software systems, Robustness, Hardware, failure mechanisms]
Availability analysis of transaction processing systems based on user-perceived performance
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Transaction processing systems are judged by users to be correctly functioning not only if their transactions are executed correctly, but also if most of them are completed within an acceptable time limit. Therefore, we propose a definition of availability for systems for whom there is a notion of system failure due to frequent violation of response time constraints. We define the system to be available at a certain time if at that time the fraction of transactions meeting a deadline is above a certain user requirement. This definition lends to very different estimates of availability measures such as system downtimes as compared with more traditional measures. We conclude that for transaction processing systems, where the user's perception is important, our definition more correctly quantifies the availability of the system.
[Availability, Real time systems, transaction processing, response time constraints, software reliability, system failure, user perceived performance, Time measurement, Transaction databases, user perception, transaction processing systems, Delay, software fault tolerance, systems availability, availability analysis, real-time systems, acceptable time limit, Libraries, Database systems, Performance analysis, user requirement, availability measures, system downtimes]
An index-based checkpointing algorithm for autonomous distributed systems
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
The paper presents an index based checkpointing algorithm for distributed systems with the aim of reducing the total number of checkpoints while ensuring that each checkpoint belongs to at least one consistent global checkpoint (or recovery line). The algorithm is based on an equivalence relation defined between pairs of successive checkpoints of a process which allows, in some cases, to advance the recovery line of the computation without forcing check points in other processes. This protocol shows good performance, especially in autonomous environments, where each process does not have any private information about other processes.
[Checkpointing, Process design, Algorithm design and analysis, Protocols, Communication system control, reliability, distributed processing, Remuneration, index based checkpointing algorithm, Distributed computing, system recovery, autonomous distributed systems, autonomous environments, software fault tolerance, recovery line, protocol, Fault tolerant systems, fault tolerant computing, equivalence relation, successive checkpoints, Force control, consistent global checkpoint, Contracts]
Data distribution algorithms for load balanced fault-tolerant Web access
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Describes the design and analysis of RobustWeb, a scalable and fault-tolerant World Wide Web server cluster that is based on HTTP redirection. The system consists of a set of N back-end document servers and one or more redirection servers which receive the HTTP requests and redirect them to the document servers. A load distribution algorithm is used for initial distribution of the documents on the servers. Given a specific degree of replication k, the distribution algorithm guarantees that at least k replicas of each document are present after document distribution is complete. The redirection servers redirect requests to one of the replicas with a pre-computed redirection probability. When a server fails, the redirection probabilities are recomputed using a novel algorithm based on network flow. Theis enables the load to be approximately balanced among the remaining servers, allowing for graceful degradation of the service in the event of failures. A preliminary prototype of RobustWeb has been implemented.
[server failure, document replicas, Throughput, scalable World Wide Web server cluster, replication degree, load balanced fault-tolerant access, data distribution algorithms, Delay, Degradation, Fault tolerance, Network servers, resource allocation, Clustering algorithms, Prototypes, file servers, distributed databases, load distribution algorithm, Robustness, Web server, Local area networks, client-server systems, RobustWeb, replicated databases, document distribution, probability, graceful service degradation, software fault tolerance, redirection servers, redirection probability, computer communications software, transport protocols, HTTP redirection, network flow, back-end document servers, Internet]
Software approach to hazard detection using on-line analysis of safety constraints
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Hazard situations in safety-critical systems are typically complex, so there is a need for means to detect complex hazards and react in a timely and meaningful way. This paper addresses the problem of hazard detection through the development of an online analysis tool. The approach allows the user to specify complex multi-source hazards using a query-like language, uses both synchronous and asynchronous online checking approaches to balance efficiency and expressiveness, accommodates dynamic applications through dynamic constraint addition, and supports distributed and parallel applications running in heterogeneous environments.
[online analysis, safety-critical software, hazards and race conditions, distributed processing, query languages, Software safety, dynamic applications, Distributed computing, distributed applications, Guidelines, Concurrent computing, synchronous online checking, dynamic constraint addition, expressiveness, safety constraints, query-like language, Injuries, efficiency, hazard situations, heterogeneous environments, Hazards, parallel applications, Application software, Linear accelerators, safety-critical systems, hazard detection, Radiation safety, online operation, complex multi-source hazards, asynchronous online checking, Accidents]
Incorporating code coverage in the reliability estimation for fault-tolerant software
Proceedings of SRDS'97: 16th IEEE Symposium on Reliable Distributed Systems
None
1997
Presents a technique that uses coverage measures in reliability estimation for fault-tolerant programs, particularly N-version software. This technique exploits both coverage and time measures collected during testing phases for the individual program versions and the N-version software system for reliability prediction. The application of this technique to single-version software was presented in our previous research (IEEE 3rd Int. Symp. on Software Metrics, Berlin, Germany, March 1996). In this paper, we extend this technique and apply it on the N-version programs. The results obtained from the experiment conducted on an industrial project demonstrate that our technique significantly reduces the hazard of reliability overestimation for both single-version and multi-version fault-tolerant software systems.
[Software testing, System testing, Phase measurement, reliability theory, N-version software, fault-tolerant programs, multi-version fault-tolerant software systems, Fault tolerance, Software metrics, industrial project, program versions, Software measurement, code coverage measures, time measures, single-version software, reliability prediction, Time measurement, Application software, software fault tolerance, configuration management, software testing phases, reliability overestimation, Software systems, Particle measurements, software reliability estimation]
Programming the grid: component systems for distributed applications
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Summary form only given. The traditional model of software design for large scale scientific problem solving is outdated. The emphasis is now on large teams that must build simulation software that integrates physical systems from multiple scientific disciplines. In addition to the problem of multi-disciplinary physics, the computational environment is now a grid of distributed resources consisting of large supercomputers, databases and networked instruments connected by high speed networks. Consequently, a new model for programming these applications is required. What is emerging is a programming style where the individual components of a large simulation can be located on remote systems and the application is built by composing these elements together to form a single distributed system. Building applications from software components is not a new idea. It has a rich history in the visualization and object oriented software design communities. More recently, the desktop top software industry has embraced this concept with systems like Java Beans, ActiveX and CORBA-Enterprise Beans. The author examines a new project to design a component architecture for scientific programming. He examines some of the design trade-offs that make this problem different from the conventional CA environment.
[object oriented software design, visualization, software design, large scale scientific problem solving, digital simulation, ActiveX, supercomputers, Distributed computing, distributed applications, Software design, networked instruments, programming model, Grid computing, Java Beans, Computer networks, Large-scale systems, software components, distributed object management, high speed networks, databases, Object oriented modeling, Computational modeling, software development management, physical systems, programming style, Application software, CORBA-Enterprise Beans, component systems, Physics, physics computing, large teams, distributed resources, software reusability, scientific programming, Problem-solving, simulation software]
Security in mobile systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Mobile computing has become popular over the last few years. Users need to have continuous access to information even when they are mobile, e.g., a doctor may need to constantly monitor a patient's health or a stock broker may need periodic information about the stock market, etc. Communication in such cases is typically over wireless links and it becomes critical to ensure secure message exchange. The traditional goals of secure computing have been to achieve confidentiality, integrity, availability, legitimacy and accountability. Messages exchanged between two hosts are usually coded by using symmetric or asymmetric ciphers, which makes it difficult for them to be seen by an outsider. This paper discusses issues concerned with mobility in a network and extensions of existing security schemes.
[Protocols, Mobile communication, Communication system security, wireless links, Network servers, mobile computing, secure message exchange, information access, data availability, Computer security, Stock markets, accountability, asymmetric ciphers, data integrity, data confidentiality, symmetric ciphers, Computer science, Patient monitoring, security of data, legitimacy, Authentication, data privacy, wireless LAN, Mobile computing, mobile systems security]
Legal reliability in large-scale distributed systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The "legal reliability" of an information system is the extent to which it is able to produce robust evidence upon which legal proceedings can be based. As such it is an essential feature of all information systems operating in the commercial and social domain. The legal test of reliability is not scientific proof or extent of assessment of quality of engineering but compliance with admissibility rules and demonstration of weight of evidence before a court. The gap between regular computer industry methods of achieving reliability and the approaches of the courts is explained. Adjustment in system development methodologies to encompass "legal reliability" are discussed. Finally it is suggested that "legal reliability" provides a new and useful determinant in the information security agenda.
[information security, Reliability engineering, admissibility rule compliance, courts, commercial domain, legal reliability, large-scale distributed systems, social domain, Information systems, Law enforcement, security of data, Information security, Computer industry, Robustness, Large-scale systems, information systems, robust evidence, system development methodologies, evidence weight, Computer security, Legal factors, Testing, legislation, legal proceedings]
Resource-usage prediction for demand-based network-computing
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
This paper reports on an application of artificial intelligence to achieve demand-based scheduling within the context of a network-computing infrastructure. The described AI system uses tool-specific, run-time input to predict the resource-usage characteristics of runs. Instance-based learning with locally weighted polynomial regression is employed because of the need to simultaneously learn multiple polynomial concepts and the fact that knowledge is acquired incrementally in this domain. An innovative use of a two-level knowledge base allows the system to account for short-term variations in compute-server and network performance and exploit temporal and spatial locality of runs. Instance editing allows the approach to be tolerant to noise and computationally feasible for extended use. The learning system was tested on three tools during normal use of the Purdue University Network Computing Hubs. Results indicate that the described instance-based learning technique using locally weighted regression with a locally linear model works well for this domain.
[System testing, resource-usage prediction, artificial intelligence, Learning systems, Intelligent networks, Runtime, resource allocation, network performance, locally weighted polynomial regression, knowledge based systems, scheduling, noise, Instance-based learning, Computer networks, Polynomials, demand-based scheduling, locally linear model, learning system, run-time input, Application software, Processor scheduling, statistical analysis, Artificial intelligence, Software tools, two-level knowledge base, learning by example, demand-based network-computing, Purdue University Network Computing Hubs]
Cyber-intrusion response
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Cyber response technologies are a relatively new area of research. This paper provides a high level description of the hard problems associated with responding to a cyber intrusion, identifies several questions that need to be addressed in order to solve these hard problems, provides a few examples of current response technologies, and describes an approach that can be taken to fill the gaps between the current response state of practice and the desired capabilities.
[Military communication, telecommunication security, cyber-intrusion response, Military computing, Local government, data confidentiality, high level description, Information systems, security of data, network intrusion detection, Intrusion detection, data privacy, Large-scale systems, Internet, Protection, Marine vehicles, National security, Telecommunication services]
Practical parallel algorithms for minimum spanning trees
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
We study parallel algorithms for computing the minimum spanning tree of a weighted undirected graph G with n vertices and m edges. We consider an input graph G with m/n/spl ges/p, where p is the number of processors. For this case, we show that simple algorithms with data-independent communication patterns are efficient both in theory and in practice. The algorithms are evaluated theoretically using Valiant's (1990) BSP model of parallel computation and empirically through implementation results.
[Algorithm design and analysis, parallel algorithms, parallel computation, Costs, Computational modeling, minimum spanning trees, User-generated content, graph theory, trees (mathematics), Parallel machines, weighted undirected graph, Parallel algorithms, Distributed computing, BSP model, Concurrent computing, Computer science, Tree graphs, input graph, data-independent communication patterns]
Off-line diagnosis of parallel systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
This paper presents an off-line diagnosis strategy for parallel message-passing systems. This strategy, called host-diagnosis, allows an external observer, i.e. the host system, to perform centralized diagnosis of the system state, given results of distributed tests performed among the system processors. Three algorithms that use the host-diagnosis strategy are proposed. The performance of the three algorithms are evaluated and compared to those of a classic distributed self-diagnosis algorithm. The obtained results show an interesting behaviour of the host-diagnosis algorithms in comparison with the self-diagnosis one.
[Performance evaluation, System testing, Assembly systems, message passing, distributed self-diagnosis algorithm, distributed tests, program diagnostics, Time to market, parallel message passing systems, performance evaluation, Electronic mail, Maintenance, host system, Fabrication, centralized diagnosis, Automatic testing, Fault detection, system processors, distributed algorithms, host-diagnosis, Manufacturing, parallel systems offline diagnosis, software performance evaluation]
Optimization of a real-time primary-backup replication service
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The primary-backup replication model is one of the commonly adopted approaches to providing fault tolerant data services. Its extension to the real time environment, however, imposes the additional constraint of timing predictability, which requires a bounded overhead for managing redundancy. The paper discusses the trade-off between reducing system overhead and increasing (temporal) consistency between the primary and backup, and explores ways to optimize such a system to minimize either the inconsistency or the system overhead while maintaining the temporal consistency guarantees of the system. An implementation built on top of the existing RTPB model (H. Zou and F. Jahanian, 1998) was developed within the x-kernel architecture on the Mach OSF platform running MK 7.2. Results of an experimental evaluation of the proposed optimization techniques are discussed.
[Real time systems, Protocols, bounded overhead, Laboratories, temporal logic, Environmental management, timing predictability, x-kernel architecture, Fault tolerance, temporal consistency, Mach OSF platform, Fault tolerant systems, primary-backup replication model, real time environment, system overhead, redundancy, replicated databases, Redundancy, Buildings, back-up procedures, real time primary-backup replication service optimization, data integrity, Application software, software fault tolerance, real-time systems, fault tolerant data services, Timing, RTPB model]
Requirements for a true enterprise-wide security infrastructure: the play's the thing
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Business has certain requirements that must be met if the enterprise is to thrive; some of these requirements are affected by security systems. The relationship of these requirements and supporting business has been well illustrated by the success of enterprise resource planning (ERP) systems and the initial failure and subsequent transformational impact of tools such as spreadsheets. If designed correctly, an enterprise-wide security infrastructure can, in fact, enable new business. However, the best security in the world will not be adequate if the business itself is not supported. This paper describes a set of minimum requirements that must be met if security systems are to meet their promise of enabling entirely new lines of business, and then describes an instantiation of these requirements in a currently available system.
[Costs, Data security, business, Enterprise resource planning, enterprise resource planning, Personnel, spreadsheets, Privacy, Computer hacking, security of data, Solids, enterprise-wide security infrastructure, security systems, Artificial intelligence, Computer security, Monitoring, business data processing]
Two branch predictor schemes for reduction of misprediction rate in conditions of frequent context switches
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Branch misprediction is one of the important causes of performance degradation in superpipelined and superscalar processors. Most of the existing branch predictors, based on the exploiting of branch history, suffer from prediction accuracy decrease caused by frequent context switches. The goal of this research is to reduce misprediction rate (MPR) when the context switches are frequent, and not to increase the MPR when the context switches are relatively rare. We propose two independent, but closely related modifications of global adaptive prediction mechanisms: first, to flush only the branch history register (BHR) at context switch, instead of reinitialization of the whole predictor, and second, to use two separated BHRs, one for user and one for kernel branches, instead of one global history register. We have evaluated the ideas by measurements on real traces from IBS (Instruction Benchmark Set), and have shown that both modifications reduce MPR at negligible hardware cost.
[branch history register, Costs, Switches, Registers, History, Tellurium, Accuracy, superpipelined processors, Hardware, Virtual manufacturing, Kernel, frequent context switches, distributed programming, branch misprediction, superscalar processors, misprediction rate, multiprocessing programs, branch predictor schemes, performance degradation, reinitialization, software fault tolerance, global adaptive prediction mechanisms, Instruction Benchmark Set, Influenza, branch history]
AQuA: an adaptive architecture that provides dependable distributed objects
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Dependable distributed systems are difficult to build. This is particularly true if they have dependability requirements that change during the execution of an application, and are built with commercial off-the-shelf hardware. In that case, fault tolerance must be achieved using middleware software, and mechanisms must be provided to communicate the dependability requirements of a distributed application to the system and to adapt the system's configuration to try to achieve the desired dependability. The AQuA architecture allows distributed applications to request a desired level of availability using the Quality Objects (QuO) framework and includes a dependability manager that attempts to meet requested availability levels by configuring the system in response to outside requests and changes in system resources due to faults. The AQuA architecture uses the QuO runtime to process and invoke availability requests, the Proteus dependability manager to configure the system in response to faults and availability requests, and the Ensemble protocol stack to provide group communication services. Furthermore, a CORBA interface is provided to application objects using the AQuA gateway. The gateway provides a mechanism to translate between process-level communication, as supported by Ensemble, and IIOP messages, understood by Object Request Brokers. Both active and passive replication are supported, and the replication type to use is chosen based on the performance and dependability requirements of particular distributed applications.
[AQuA, software quality, dependability manager, adaptive architecture, Proteus, group communication services, Runtime, Fault tolerant systems, Computer architecture, availability requests, Hardware, distributed object management, Quality management, Availability, replication, client-server systems, fault tolerance, dependable distributed objects, Object Request Brokers, commercial off-the-shelf hardware, process-level communication, dependability requirements, Application software, Middleware, software fault tolerance, CORBA, Communication system software, Quality Objects, middleware software, Ensemble protocol stack, Resource management]
Load balancing of dynamic and adaptive mesh-based computations
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
One ingredient which is viewed as vital to the successful conduct of many large-scale numerical simulations is the ability to dynamically repartition the underlying adaptive finite element mesh among the processors so that the computations are balanced and interprocessor communication is minimized. We present two new schemes for adaptive repartitioning: Locally-Matched Multilevel Scratch-Remap (or LMSR) and Wavefront Diffusion. The LMSR scheme performs purely local coarsening and partition remapping in a multilevel context. In Wavefront Diffusion, the flow of vertices move in a wavefront from overbalanced to underbalanced domains. We present experimental evaluations of our LMSR and Wavefront Diffusion algorithms on synthetically generated adaptive meshes as well as on some application meshes. We show that our LMSR algorithm decreases the amount of vertex migration required to balance the graph and produces repartitionings of similar quality compared to current scratch-remap schemes. Furthermore, we show that our LMSR algorithm is more scalable in terms of execution time compared to current scratch-remap schemes. We show that our Wavefront Diffusion algorithm obtains significantly lower vertex migration requirements, while maintaining similar edge-cut results compared to current multilevel diffusion algorithms, especially for highly imbalanced graphs.
[Kirk field collapse effect, Military computing, load balancing, adaptive repartitioning, graph theory, Locally-Matched Multilevel Scratch-Remap, vertex migration, Concurrent computing, Wavefront Diffusion, resource allocation, dynamic mesh based computation, LMSR algorithm, interprocessor communication, Large-scale systems, imbalanced graphs, Contracts, multiprocessing systems, large-scale numerical simulation, mesh generation, Partitioning algorithms, Computer science, High performance computing, distributed algorithms, execution time, adaptive finite element mesh, Load management, Numerical simulation, scratch-remap scheme, adaptive mesh based computation]
A taxonomy for describing matching and scheduling heuristics for mixed-machine heterogeneous computing systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The problem of mapping (defined as matching and scheduling) tasks and communications onto multiple machines and networks in a heterogeneous computing (HC) environment has been shown to be NP-complete, in general, requiring the development of heuristic techniques. Many different types of mapping heuristics have been developed in recent years. However, selecting the best heuristic to use in any given scenario remains a difficult problem. Factors making this selection difficult are discussed. Motivated by these difficulties, a new taxonomy for classifying mapping heuristics for HC environments is proposed (Purdue HC Taxonomy). The taxonomy is defined in three major parts: the models used for applications and communication requests; the models used for target hardware platforms; and the characteristics of mapping heuristics, Each part of the taxonomy is described, with examples given to help clarify the taxonomy. The benefits and uses of this taxonomy are also discussed.
[mapping heuristics, communication requests, multiple machines, matching heuristics, Taxonomy, mixed-machine heterogeneous computing, Purdue HC Taxonomy, Subcontracting, NP-complete, scheduling heuristics, Intelligent networks, Processor scheduling, resource allocation, High performance computing, distributed algorithms, Computer applications, Computer architecture, scheduling, Computer networks, Hardware, computational complexity]
The relative overhead of piggybacking in causal message logging protocols
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Message logging protocols ensure that crashed processes make the same choices when re-executing nondeterministic events during recovery. Causal message logging protocols achieve this by piggybacking the results of these choices (called determinants) on the ambient message traffic. By doing so, these protocols do not create orphan processes nor introduce blocking in failure-free executions. To survive f failures, they ensure that determinants are stored by at least f+1 processes. Causal logging protocols differ in the kind of information they piggyback to other processes. The more information they send, the better each process is able to estimate global properties of the determinants, which in turn results in fewer needless piggybacking of determinants. This paper quantifies the tradeoff between the cost of sending more information and the benefit of doing so.
[Protocols, Costs, determinants, Engineering profession, fault-tolerance, Computer crashes, Encoding, causal message logging protocols, piggybacking, system recovery, software fault tolerance, Computer science, Fault tolerance, message traffic, nondeterministic events, protocols, rollback recovery, distributed programming]
Architecture for group communication in mobile systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
In mobile computing systems the network configuration changes due to node mobility. The paper identifies the issues a group communication service has to take into account in order to handle node mobility. These include the need to identify the location of a node, and the ability to cope with inaccuracies in the determination of a group membership. A multi level architecture for group communication in mobile systems is presented. This architecture contains a synchronous proximity layer protocol to determine the set of mobile nodes in the proximity of a given node in the network. This information is used by a three round group membership protocol for construction of groups used by mobile applications. As an example, the architecture is specialized to solve the channel allocation problem.
[synchronous proximity layer protocol, three round group membership protocol, Protocols, group communication architecture, Mobile communication, node mobility, Intelligent networks, network configuration, mobile computing, Computer architecture, groupware, mobile applications, Computer networks, protocols, channel allocation, mobile nodes, multi level architecture, channel allocation problem, group communication service, Remuneration, mobile computing systems, Cellular networks, Computer science, Channel allocation, Mobile computing]
Optimizing join index based join processing: a graph partitioning approach
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The cost of join computation, which uses a join index in a sequential system with limited buffer space, depends primarily on the page access sequence used to fetch the pages of the base relations. We introduce a graph partitioning model that will minimize the length of the page access sequence thus minimizing the redundant I/O, given a fixed buffer. Experiments with Sequoia 2000 data sets show that the graph partitioning method outperforms the existing methods based on sorting and online clustering, particularly for a small number of buffers and high join selectivity.
[Costs, relational algebra, graph theory, join selectivity, Etching, query processing, optimisation, Intrusion detection, Clustering algorithms, sorting, data structures, sequential system, Bipartite graph, graph partitioning, page access sequence, join processing, join index, Data structures, buffer space, Partitioning algorithms, relational database, relational databases, Sorting, database theory, Computer science, Query processing, Sequoia 2000 data sets, redundant input output, join computation, online clustering]
An integration of the primary-shadow TMO replication scheme with a supervisor-based network surveillance scheme and its recovery time bound analysis
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The time-triggered message-triggered object (TMO) scheme was formulated a few years ago (K.H. Kim et al., 1994; K.H. Kim and C. Subbaraman, 1997), as a major extension of the conventional object structuring schemes with the idealistic goal of facilitating general form design and timeliness-guaranteed design of complex real time application systems. Recently, as a new scheme for realizing TMO-structured distributed and parallel computer systems capable of both hardware and software fault tolerance, we have formulated and demonstrated the primary-shadow TMO replication (PSTR) scheme. An important new extension of the PSTR scheme is an integration of the PSTR scheme and a network surveillance (NS) scheme. This extension results in a significant improvement in the fault coverage and recovery time bound achieved. The NS scheme adopted is a recently developed scheme, effective in a wide range of point-to-point networks and it is called the supervisor based NS (SNS) scheme. The integration of the PSTR scheme and the SNS scheme is called the PSTR/SNS scheme. The recovery time bound of the PSTR/SNS scheme is analyzed on the basis of an implementation model that can be easily adapted to various commercial operating system kernels.
[Real time systems, general form design, complex real time application systems, Software performance, distributed processing, Electrical capacitance tomography, Distributed computing, system recovery, time-triggered message-triggered object scheme, Concurrent computing, Design engineering, PSTR/SNS scheme, Fault tolerant systems, object structuring schemes, Hardware, point-to-point networks, PSTR scheme, operating system kernels, object-oriented programming, message passing, NS scheme, parallel computer systems, fault coverage, commercial operating system kernels, Application software, software fault tolerance, supervisor based network surveillance scheme, supervisor based NS, timeliness-guaranteed design, primary-shadow TMO replication scheme, Surveillance, real-time systems, implementation model, fault tolerant computing, recovery time bound analysis, SNS scheme]
Safe and efficient active network programming
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Active networks are aimed at incorporating programmability into the network to achieve extensibility. One approach to obtaining extensibility is to download router programs into network nodes. This programmability is critical to allow multipoint distributed systems to adapt to network conditions and individual clients' needs. Although promising, this approach raises critical issues such as safety to achieve reliability despite the complexity of a distributed system, security to protect shared resources, and efficiency to maximize usage of bandwidth. This paper proposes the use of a domain-specific language, PLAN-P, to address all of the above issues. To address safety and security, we give examples of properties of PLAN-P programs that can be automatically checked due to the use of a restricted language. For efficiency, we show that an automatically generated run-time compiler for PLAN-P produces code which outperforms an equivalent compiled Java program. Additionally, we present performance results on a real application (learning bridge) where we obtain 100% of the maximum possible throughput.
[Java, client-server systems, equivalent compiled Java program, multipoint distributed systems, Throughput, Security, programmability, extensibility, network nodes, Domain specific languages, Bridges, active network programming, Runtime, Program processors, domain-specific language, router programs, Bandwidth, shared resources, learning bridge, Safety, automatically generated run-time compiler, Protection, distributed programming]
Tolerating client and communication failures in distributed groupware systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
If a groupware system is to be effectively used, especially over a wide area network such as the Internet, where the quality of networking and computing resources are unpredictable, it should allow clients to tolerate client, link, and server failures. In particular, clients should be able to join groups and transfer groups' current state in the presence of most client and link failures. In order to reduce usage overhead, disconnected clients should also be able to rejoin groups without having to restart from scratch. Furthermore, lock management and group membership should tolerate transient failures in the system. We introduce the notion of stateful group communication, which frees clients of administrative management of shared application state and allows fault tolerant group join, state transfer, and rejoin. Stateful group communication is incorporated in Corona, a general purpose, group communication service provider. In order to allow groups to tolerate transient failures, Corona also provides locks with grace period and group membership notification services that are based on client connection status. We present and discuss Corona's fault tolerant services.
[administrative management, client connection status, group membership, state transfer, Postal services, Network servers, Fault tolerant systems, groupware, stateful group communication, communication failures, Computer networks, transient failures, Corona, computer network reliability, IP networks, Web server, group membership notification services, client-server systems, fault tolerance, Collaborative software, Computer crashes, lock management, Hip, distributed groupware systems, fault tolerant group join, server failures, usage overhead, Collaborative work, group communication service provider, fault tolerant services, fault tolerant computing, Internet, wide area network]
Evaluation of filtering mechanisms for MPEG video communications
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
In this paper, we evaluate two video filtering mechanisms for MPEG-1 video, namely low-pass filtering and selective frame dropping. The evaluation provides tradeoffs between the reduction achieved in bandwidth requirements and the perceptual quality of the video sequences delivered to the client. Extensive experiments revealed that these filtering mechanisms result in a significant reduction in bandwidth requirements while maintaining acceptable perceptual quality.
[data compression, Low pass filters, tradeoffs, Video sequences, visual communication, Information filtering, video coding, bandwidth compression, Videoconference, bandwidth requirements, Image coding, Transform coding, video filtering mechanisms, Video compression, Streaming media, video sequences, Information filters, selective frame dropping, low-pass filters, Internet, low-pass filtering, perceptual quality, image sequences, MPEG-1 video communications]
Design and analysis of a hardware-assisted checkpointing and recovery scheme for distributed applications
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
A checkpointing and recovery scheme which exploits the low latency and high coverage characteristics of a hardware error detection scheme is presented. Message dependency which is the main source of multi-step rollback in distributed systems is minimized by using a new message validation technique derived from hardware-assisted error detection. The main contribution of this paper is the development of an analytical model to establish the completeness and correctness of the new scheme. A novel concept of global state matrix is defined to keep track of the global state in a distributed system and assist in recovery. An illustration is given to show the distinction between conventional and the new recovery schemes.
[Checkpointing, Real time systems, distributed system, hardware-assisted error detection, recovery scheme, system recovery, distributed applications, Delay, Analytical models, message dependency, global state matrix, Hardware, multi-step rollback, hardware-assisted checkpointing, high coverage, message passing, Redundancy, message validation technique, Application software, hardware error detection scheme, Computer science, low latency, Computer errors, fault tolerant computing, Error correction]
Transformation-based reconstruction for audio transmissions over the Internet
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The paper studies the design of data transformation algorithms for audio data transmitted over the Internet, with a goal of reconstructing the original signals by the receiver with little distortions in the presence of bursty loss of packets. It assumes that a single audio stream is interleaved into multiple packets, and a lost sample at the receiver is reconstructed as the interpolation of adjacent samples received. We propose a non redundant transformation based reconstruction algorithm that can minimize the reconstruction error for any fixed, interpolation based reconstruction algorithm. Its basic idea is that the sender transforms the input audio stream optimally, based on the reconstruction method used at the receiver before sending the data packets. Consequently, the receiver is able to recover much better from losses of packets than without any knowledge of what the signals should be. In particular, we study our transformation algorithm based on one popular linear interpolation based reconstruction algorithm. We found that our scheme can improve the signal to noise ratio (SNR) by 1 to 2 dB with very little extra computation efforts as compared to the scheme without transformation.
[input audio stream, audio transmissions, Laboratories, packet switching, Data engineering, Electronic mail, Delay, Read only memory, Degradation, interpolation based reconstruction algorithm, signal reconstruction, Data communication, multimedia communication, multiple packets, bursty packet loss, reconstruction method, Redundancy, data transformation algorithms, audio data, reconstruction error, linear interpolation based reconstruction algorithm, audio signal processing, single audio stream, Streaming media, data packets, Internet, signal to noise ratio, non redundant transformation based reconstruction algorithm]
A fragmentation scheme for multimedia traffic in active networks
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Multimedia data are usually very large. Fragmentation of multimedia data units is inevitable when they are transmitted through networks. Active networks are becoming popular, and active technologies are being applied to various interesting problems. When applying active technologies to multimedia data, however, the problem of fragmenting large packets still exists. Furthermore, new issues emerge when active capsules are fragmented. In this paper, we propose a new fragmentation scheme which addresses the unique needs of active networks and which utilizes the special properties of active networks. We propose an algorithm to fragment the data at the transport layer, which can minimize the overhead. Preliminary experimental results show that the scheme works well under realistic scenarios, with an overhead of less than 5%.
[Transport protocols, active networks, Ethernet networks, packet switching, Telecommunication traffic, Electronic mail, multimedia data fragmentation scheme, multimedia traffic, active capsules, Intelligent networks, overhead minimization, transport layer, Sockets, Bandwidth, Motion pictures, data packets, IP networks, minimisation, multimedia communication, telecommunication traffic, Payloads]
Enforcing security policies in large scale communication networks
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Due to unexpected network interconnection growth, the security of technological and information infrastructures is becoming difficult to be managed and controlled. In addition, security is becoming more and more crucial for an organisation's information systems operation. The management of an organisation has to establish rules and regulations in order to face the threats that its information systems face. The network manager is obliged to enforce the regulations that senior management addresses. We propose a framework that a network manager could use in order to effectively enforce security policies. In addition, we present a scalable security management architecture suitable for TCP/IP networks. The communication of systems' logical components is based on the use of the SNMP protocol. Finally, the system includes facilities for collecting and efficiently storing raw and aggregate historical security management information in a temporal database for off-line analysis.
[telecommunication security, historical security management information, Communication system control, temporal database, organisation, SNMP protocol, Communication system security, TCP/IP networks, offline analysis, Technology management, Management information systems, TCPIP, Large-scale systems, information systems, Communication networks, IP networks, Data security, computer networks, security of data, large scale communication networks, transport protocols, temporal databases, Information security, security policies, regulations, network interconnection growth, scalable security management architecture, business data processing]
Survivable consensus objects
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Reaching consensus among multiple processes in a distributed system is fundamental to coordinating distributed actions. We present a new approach to building survivable consensus objects in a system consisting of a (possibly large) collection of persistent object servers and a transient population of clients. Our consensus object implementation requires minimal support from servers, but at the same time enables clients to reach coordinated decisions despite the arbitrary (Byzantine) failure of any number of clients and up to a threshold number of servers.
[client-server systems, Protocols, clients, distributed action coordination, Byzantine failure, distributed system, Electrical capacitance tomography, Read only memory, software fault tolerance, survivable consensus objects, persistent object servers, Emulation, concurrency control, Polynomials, distributed object management, persistent objects]
Distributed center location algorithm for fault-tolerant multicast in wide-area networks
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Group shared trees form a major component of most multicast routing protocols (e.g. PIM-SMv2, CBTv3). The shared trees are built by choosing one node as the center of the tree. The optimal location of a center under the constraints of minimal tree cost and delay for a particular group is an NP-complete problem. Current implementations of protocols decide on the location of these centers administratively, an attractive choice given that the solution is obviously sub-optimal and does not lend itself to dynamic reconfiguration of centers. We present a scalable heuristic to find a near-optimal solution to the center location problem. Our solution is easily amenable to distributed implementation and provides the protocol with a list of possible centers ranked in the order of their optimality, therefore providing fault tolerance and reducing the chances of a single point of failure at the center.
[distributed center location algorithm, scalable heuristic, minimal tree cost, wide area networks, Electronic mail, Parallel algorithms, Delay, Fault tolerance, Intelligent networks, multicast routing protocols, Tree graphs, wide-area networks, multicast communication, group shared trees, Routing protocols, dynamic reconfiguration, computer network reliability, fault-tolerant multicast, protocols, trees (mathematics), Multicast protocols, NP-complete problem, Multicast algorithms, delay, distributed algorithms, telecommunication network routing, Internet]
Issues in multimedia synchronization over broadband networks
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Distributed, networked multimedia information systems will be a critical component of technology-based information infrastructures in the future. Several ground-breaking applications have already appeared, and more are many are expected to follow. Innovations in hardware and software are feeding this revolution. In this paper, the notion of "quality of presentation" (QOP) for multimedia data transferred over the networks is developed. Multimedia data synchronization requirements are studied, and several synchronization protocols are presented. We discuss the management of pre-orchestrated as well as live multimedia data. It is anticipated that, with proper research breakthroughs, broadband multimedia networking technologies will bring about spectacular changes in the ways that we store, process and use information.
[Real time systems, information processing, information use, Multimedia databases, broadband networks, Multimedia communication, Information systems, technological forecasting, synchronization protocols, Intelligent networks, data synchronization requirements, broadband multimedia networking technologies, Image storage, protocols, multimedia communication, pre-orchestrated data management, multimedia data transfer, B-ISDN, Multimedia systems, live multimedia data management, Broadband communication, quality of service, Application software, multimedia synchronization, information storage, synchronisation, distributed multimedia information systems, Streaming media, presentation quality, technology-based information infrastructures]
A distributed version of the SequenceL language
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
This paper introduces a new computational model for SequenceL. The model is extended through the addition of a notion of the VRAM model. The computational model introduced together with the SequenceL language provides for a good expression of distributed behaviour.
[Data analysis, random-access storage, VRAM model, Computational modeling, SequenceL language, Application software, Appropriate technology, Distributed computing, Computer languages, distributed algorithms, Computer applications, US Government, computational model, Hardware, distributed behaviour, Software tools, parallel languages, distributed programming]
Local area detection of incoming war dial activity
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Two techniques for functional detection of local area incoming war dial are described. One technique employs a dedicated workstation looking for evidence of incoming war dial. We describe our experimental implementation of this technique, including alarm generation to a Cisco NetRanger(R) intrusion detection system. The second technique involves simple parsing of private branch exchange (PBX) call records for characteristic patterns of war dial. Baseline heuristics driving our algorithms are discussed. The non-terminated call detection limitations of this parsing technique for our Lucent Definity G3 PBX are discussed.
[telecommunication security, functional detection, Military computing, Heuristic algorithms, private telephone exchanges, Laboratories, business communication, parsing, intranet, heuristics, Intrusion detection, Telephony, Modems, Workstations, local area detection, PBX call records, Computer security, Data security, intranets, alarm generation, Cisco NetRanger, intrusion detection system, security of data, Lucent Definity G3 PBX, Authentication, Internet, private branch exchange call records, incoming war dial activity, dedicated workstation]
A non-blocking recovery algorithm for causal message logging
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
In the recovery of failed processes in a distributed program, causal logging schemes offer several benefits. These benefits include no rollback of unfailed processes and simple approaches to output commit. Unfortunately, previous approaches to the recovery of multiple simultaneous failures require that the distributed execution be blocked or that recovering processes coordinate. The latter requires assumptions which are not satisfactory. In this paper we present a solution that has neither of these drawbacks.
[rollback, message passing, causal message logging, Laboratories, Optimization methods, Computer crashes, nonblocking recovery algorithm, History, system recovery, software fault tolerance, output commit, failed process recovery, distributed algorithms, distributed program, distributed programming]
View consistency for optimistic replication
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Optimistically replicated systems provide highly available data even when communication between data replicas is unreliable or unavailable. The high availability comes at the cost of allowing inconsistent accesses, since users can read and write old copies of data. Session guarantees have been used to reduce such inconsistencies. They preserve most of the availability benefits of optimistic systems. We generalize session guarantees to apply to persistent as well as distributed entities. We implement these guarantees, called view consistency, on Ficus an optimistically replicated file system. Our implementation enforces consistency on a per-file basis and does not require changes to individual applications. View consistency is enforced by clients accessing the data and thus requires minimal changes to the replicated data servers. We show that view consistency allows access to available and high performing data replicas and can be implemented efficiently. Experimental results show that the consistency overhead for clients ranges from 1% to 8% of application runtime for the benchmarks studied in the prototype system. The benefits of the system are an improvement in access times due to better replica selection and improved consistency guarantees over a purely optimistic system.
[Availability, client-server systems, Costs, inconsistent access, replicated databases, data replica, Calendars, optimistic replication, Data engineering, data integrity, Read only memory, Computer science, view consistency, Runtime, File systems, optimistically replicated file system, replicated data servers, Ficus, Prototypes, session guarantees, consistency overhead, benchmarks, Contracts, software performance evaluation]
System-level versus user-defined checkpointing
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Checkpointing and rollback recovery is a very effective technique to tolerate transient faults and preventive shutdowns. In the past, most of the checkpointing schemes published in the literature were supposed to be transparent to the application programmer and implemented at the operating-system level. In recent years, there has been some work on higher-level forms of checkpointing. In this second approach, the user is responsible for the checkpoint placement and is required to specify the checkpoint contents. We compare the two approaches: system-level and user-defined checkpointing. We discuss the pros and cons of both approaches and we present an experimental study that was conducted on a commercial parallel machine.
[Checkpointing, user-defined checkpointing, preventive shutdowns, commercial parallel machine, Runtime library, system-level checkpointing, application programmer, experimental study, transient fault tolerance, system recovery, parallel machines, Programming profession, software fault tolerance, Fault tolerance, Program processors, Operating systems, Fault tolerant systems, Communication channels, rollback recovery, operating-system level]
Design and verification of a secure electronic auction protocol
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Auctions are an important and common form of commerce today. A difficult aspect of auctions is that the bidder must be present at the site of the auction. This reduces the appeal of auction and restricts the number of people who would otherwise participate in it. An auction over an electronic network is therefore an attractive way of conducting business. The author proposes a protocol for electronic auctions. This protocol ensures: (a) anonymity of the customer, (b) security from passive attacks, active attacks, message corruption, and loss of messages, (c) customer privacy, and (d) atomicity (i.e., under all circumstances, the transaction is either completed or aborted). A logic is developed based on the semantics of BAN-style logic (M. Burrows et al., 1990). Using this logic, the properties of anonymity, security, privacy, and atomicity are proved for the proposed protocol.
[Protocols, Costs, secure electronic auction protocol, bidder, Electronic mail, Electronic commerce, commerce, passive attacks, Information science, Privacy, security, electronic network, message corruption, Logic, protocols, atomicity, customer anonymity, BAN-style logic, information networks, active attacks, security of data, Information security, customer privacy, data privacy, Web sites, Consumer electronics]
A metaobject protocol for fault-tolerant CORBA applications
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The use of metalevel architectures for the implementation of fault-tolerant systems is today very appealing. Nevertheless, all such fault-tolerant systems have used a general-purpose metaobject protocol (MOP) or are based on restricted reflective features of some object-oriented language. According to our past experience, we define in this paper a suitable metaobject protocol, called FT-MOP for building fault-tolerant systems. We explain how to realize a specialized runtime MOP using compile-time reflection. This MOP is CORBA compliant: it enables the execution and the state evolution of CORBA objects to be controlled and enables the fault tolerance metalevel to be developed as CORBA software.
[Protocols, fault-tolerant CORBA applications, FT-MOP, Reflection, Telecommunications, Application software, software fault tolerance, fault-tolerant systems, Automatic speech recognition, Fault tolerance, software architecture, Runtime, fault tolerance metalevel, Operating systems, transport protocols, Fault tolerant systems, compile-time reflection, metaobject protocol, Contracts, distributed object management]
End to end reliable multicast transport protocol requirements for collaborative multimedia systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Multi-party collaborative multimedia applications require data to be transmitted reliably and efficiently in order to provide a guaranteed quality of service (QoS). The multimedia applications can vary from distributed games and shared whiteboards to interactive video conferencing. These applications often involve a large number of participants and are interactive in nature, with participants dynamically joining and leaving the applications. In order to provide many-to-many interaction when the number of participants is large, IP multicasting is a very good option for communication. IP multicasting provides scalability and efficient routing but does not provide the reliability that these multimedia applications may require. Though a lot of research has been done on reliable multicast transport protocols, it really seems that the only way of doing a reliable multicast is to build it for a given purpose like conference control in multimedia conferencing. This paper compares some of the available multicast transport protocols and analyses the most suitable features and functionalities provided by these protocols for a facet of conference control: floor control. The goal is to find or design a reliable multicast transport protocol which would scale to tens or hundreds of participants scattered across the Internet and which would deliver the control messages reliably.
[Transport protocols, Scalability, Quality of service, end-to-end reliable multicast transport protocol, distributed games, Videoconference, teleconferencing, shared whiteboard, scalability, routing, groupware, multicast communication, computer network reliability, multimedia communication, IP multicasting, many-to-many interaction, service quality, Video sharing, telecommunication control, control message delivery, Scattering, Multicast protocols, Routing, multimedia conferencing, transport protocols, participant numbers, Collaboration, Games, floor control, interactive video conferencing, conference control, Internet, multi-party collaborative multimedia applications, reliable data transmission, interactive applications]
An efficient MPEG video encryption algorithm
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Multimedia data security is important for multimedia commerce. Previous cryptography studies have focused on text data. The encryption algorithms developed to secure text data may not be suitable to multimedia applications because of large data sizes and real time constraints. For multimedia applications, light weight encryption algorithms are attractive. We present an efficient MPEG video encryption algorithm. This algorithm uses a secret key randomly changing the sign bits of encoded differential values of DC coefficients of I pictures and the sign bits of encoded differential values of motion vectors of B and P pictures. The encryption effects are achieved by the IDCT during MPEG video decompression processing. This algorithm adds very small overhead to MPEG codec. A software implementation is fast enough to meet the real time requirement of MPEG video applications. Experimental results show that this algorithm achieves satisfying results. We believe that it can be used to secure video-on-demand, video conferencing and video email applications.
[Video on demand, multimedia systems, multimedia data security, secret key, Multimedia communication, Videoconference, Digital multimedia broadcasting, real time constraints, multimedia commerce, video-on-demand, Cryptography, Business, MPEG, video decompression, Codecs, Data security, cryptography, video coding, code standards, Application software, video encryption algorithm, telecommunication standards, codec, motion vectors, Transform coding, real-time systems, video email, video conferencing]
A multiprocessor scheduling algorithm for low overhead fault-tolerance
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
We propose a new scheduling algorithm for achieving fault tolerance in multiprocessor systems. The new algorithm partitions a parallel program into subsets of tasks based on some characteristics of a task graph. Then for each subset, the algorithm duplicates and schedules its tasks successively. Applying the proposed algorithm to three kinds of practical task graphs (Gaussian elimination, Laplace equation solver and LU decomposition), we conduct simulations. Experimental results show that fault tolerance can be achieved at the cost of a small degree of time redundancy, and that performance in the case of a processor failure is improved compared to a previous algorithm.
[low overhead fault tolerance, parallel program partitioning, task graph, processor failure, Laplace equations, multiprocessing systems, Laplace equation solver, task subsets, multiprocessor systems, Scheduling algorithm, processor scheduling, parallel programming, software fault tolerance, time redundancy, Fault tolerance, Processor scheduling, Gaussian elimination, multiprocessor scheduling algorithm, LU decomposition]
Dependability analysis of a cache-based RAID system via fast distributed simulation
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
We propose a new speculation-based, distributed simulation method for dependability analysis of complex systems in which a detailed functional simulation of a system component is essential to obtain an accurate overall result. Our target example is a networked cluster with compute nodes and a single I/O node. Accurate system dependability characterization is achieved via a combination of detailed simulation of the I/O subsystem behavior in the presence of faults and more abstract simulation of the compute nodes and the switching network. Dependability measures like error coverage, error detection latency and performance measures such as delivery time in the presence of faults are obtained. The approach is implemented on a network of workstations, and experimental results show significant improvements over a Time Warp simulator for the same model.
[workstation clusters, networked cluster, delivery time, Predictive models, I/O subsystem behavior, Cache storage, cache storage, performance measures, Delay, Time Warp simulator, Analytical models, complex systems, Computer network reliability, Computer networks, switching network, Computational modeling, Computer simulation, dependability analysis, error detection latency, speculation-based distributed simulation, performance evaluation, error coverage, distributed simulation, Application software, RAID, functional simulation, cache-based RAID system, network of workstations, virtual machines, Computer errors, fault tolerant computing]
High throughput networks for petaflops computing
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The smallest networks that can connect eight thousand processing elements and memory interfaces in a petaflops cryocomputer contain hundreds of thousands of 2/spl times/2 switching nodes. We have determined circuit costs, maximal throughput and average latency for feasible multistage banyan and multidimensional pruned ring mesh networks. Each can deliver 20000 single-word packets every 30 picoseconds, more than eight million gigabytes per second. Switching delays one-way through each network total 1 to 2 nanoseconds. Banyans have 2/3 the switching delays of the smallest meshes. However, banyan signal propagation delays are larger. The only candidate network needing less than 100 square meters in four connection layers is a pruned mesh of shape 18/spl times/18/spl times/55/spl times/55 with nearly one million nodes. The smallest banyan has one quarter as many nodes, but needs nearly twice the wiring area.
[switching nodes, Circuits, average latency, Random access memory, multiprocessor interconnection networks, Switches, switching delays, Throughput, memory interfaces, Delay, maximal throughput, Prototypes, Computer networks, multiprocessing systems, high throughput networks, single-word packets, Superconductivity, performance evaluation, multistage interconnection networks, multistage banyan networks, Computer science, circuit costs, multidimensional pruned ring mesh networks, petaflops computing, delays, Josephson junctions]
Extending the Coda File System to handle cache misses on isolated clients
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
For mobile information access, client caching is widely used for coping with server unavailability due to temporary disconnection of the network. Cache-miss is the most visible phenomenon to users caused by the disconnection. When multiple clients are isolated from servers, cache-miss happening on a client can be fulfilled using cache contents on other clients. Using a mechanism called disconnected operation, clients of the Coda File System can continue to work without contacting servers; however, clients cannot pass files between them by themselves. This paper presents the design and implementation of two mechanisms: import/export and session server. These mechanisms add read-sharing capability for clean objects to clients disconnected from Coda servers with reasonable response time.
[Portable computers, Laboratories, cache content, mobile information access, File servers, cache storage, Electronic mail, Distributed computing, Delay, read-sharing, Coda File System, Network servers, File systems, network operating systems, distributed databases, Computer networks, client-server systems, cache misses, client caching, isolated clients, response time, import export, Mobile computing, temporary network disconnection, session server, server unavailability]
On patterns for practical fault tolerant software in Java
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Fault tolerance is important for both sequential and distributed software, and particularly so for long-running applications. The ability to stop an application and restart it, with minimal lost work, is especially useful. If components of the application can be restarted on arbitrary hosts, so much the better. In this paper, we explore Java's potential to support fault tolerant software design. We note that while there are "deficiencies" in these facilities, a little creativity can still yield solutions that would be quite difficult in other programming environments. The main contributions of the paper are several preliminary fault tolerant programming design patterns, aimed at easing the burden of application programmers who must write completely portable fault tolerant applications. This class of applications is growing, as Internet-domain software becomes increasingly prevalent. We expect that with further development the proposed patterns will be applicable to a wide range of sequential and parallel applications.
[Decision support systems, Fault tolerance, Java, long-running applications, fault tolerant programming design patterns, completely portable fault tolerant applications, Virtual reality, Fiber reinforced plastics, practical fault tolerant software, Internet-domain software, software fault tolerance, fault tolerant software design]
Pixel level interleaving schemes for robust image communication
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Interleaving schemes have proven to be an important mechanism in reducing the effect of network errors on image transmission. Current interleaving schemes spatially decorrelate neighboring image blocks by putting them into packets that are a long way apart from each other in the transmission sequence. Most of the existing schemes, while achieving good performance on random packet losses, do not work well in the case of bursty packet losses. In this paper, we propose two interleaving schemes where the decorrelation process is applied not only at the block level but also at the pixel or coefficient level (i.e. on the information within the blocks) in the compressed domain. The decorrelation is achieved via a k-way shuffle of pixels among the blocks and by using the spatial properties of different space-filling curves, where k depends on the total number of blocks in the image. Our results show that in spite of the decorrelation of coefficients in the triangular interleaving scheme, the compression ratio is within 92% to -98% of JPEG standard compression. We also show that, when compared with existing interleaving schemes, our techniques provide improved image quality and a low mean square error in the transmitted images in the presence of random as well as bursty packet losses in networks.
[network errors, packet switching, image transmission, distant packets, block-level decorrelation, visual communication, JPEG, neighboring image blocks, losses, random packet losses, Image coding, space-filling curves, robust image communication, Robustness, decorrelation, Decorrelation, k-way pixel shuffle, data compression, spatial decorrelation, Image communication, transmission sequence, coefficient-level decorrelation, triangular interleaving scheme, mean square error, compression ratio, Image quality, bursty packet losses, image quality, performance, Transform coding, Mean square error methods, pixel-level interleaving schemes, Interleaved codes, Performance loss, image coding, Pixel]
Checkpoint-recovery protocol for reliable mobile systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Information systems consist of mobile stations and fixed stations. Mission critical applications are required to be executed fault-tolerantly in these systems. However, mobile stations support neither enough volume of storage and processing power nor enough capacity of battery to do reliable, long-term communications. Moreover, wireless channels are less reliable. Hence, the channels with the mobile stations are often disconnected. Therefore, it is difficult for multiple mobile stations to synchronously take checkpoints since the communication channels with the mobile stations may be disconnected even during taking the checkpoints. In this paper, we propose a novel hybrid checkpointing protocol where the mobile stations take asynchronously and the fixed ones take synchronously checkpoints. Reliable information systems including mobile stations can be realized by the hybrid checkpointing protocol.
[Checkpointing, hybrid checkpointing protocol, Protocols, checkpoint-recovery protocol, Mission critical systems, fixed stations, Mobile communication, Batteries, system recovery, radio access networks, mobile stations, Information systems, Wireless communication, reliable mobile systems, mobile computing, transport protocols, Fault tolerant systems, Communication channels, mission critical applications, Power system reliability, computer network reliability, wireless channels]
CORBA evaluation of video streaming w.r.t. QoS provisioning
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Describes the design, implementation and evaluation of CORBA- and socket-based continuous media (CM) systems. TCP/IP is not suitable for distributed applications which require high network bandwidth and timing criticality. UDP/IP is one of the alternatives. However, due to the fact that UDP is a lossy protocol, many issues arise when implementing distributed CM applications. Most of the QoS (quality of service) metrics known so far assume that the communication channel is lossless. In this paper, since we use UDP for CM data transmission, we adopt a new QoS metric that is applicable to lossy streams to evaluate the performance of our CM server. To reduce QoS loss factors and drift factors, we adopt a new strategy, called the QoS-driven dropping mechanism, for the CM server. Besides the traditional C-socket (TCP-UDP/IP) based CM server mechanisms, we implemented our CM server on CORBA. It turns out that the CORBA-based implementation runs considerably slower than the UDP-version, but faster than the TCP version.
[Protocols, telecommunication channels, Quality of service, communication channel, visual communication, QoS loss factors, QoS drift factors, losses, video servers, distributed applications, QoS-driven dropping mechanism, C-socket, lossy streams, TCPIP, Bandwidth, Propagation losses, TCP/IP, timing criticality, video streaming, Data communication, distributed object management, multimedia communication, performance evaluation, quality of service, socket-based continuous media systems, network bandwidth, QoS metric, CORBA, server performance evaluation, service quality provisioning, transport protocols, Communication channels, data transmission, Streaming media, data communication, Performance loss, UDP/IP, Timing, lossy protocol]
Consensus in asynchronous systems where processes can crash and recover
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The consensus problem is now well identified as being one of the most important problems encountered in the design and the construction of fault-tolerant distributed systems. This problem is defined as follows: processes have to reach a common decision, which depends on their inputs, despite failures. We consider the consensus problem in asynchronous distributed systems augmented with unreliable failure detectors. Several protocols have been proposed for these systems, when process crashes are assumed to be definitive. This paper addresses the consensus problem in a more practical asynchronous system model, namely in a context where processes can crash and recover. As a process crash entails the loss of its volatile memory, each process is equipped with a stable storage. So, to be efficient a consensus protocol has to log as few critical data as possible. The proposed protocol uses a new class of failure detectors suited to the crash/recovery model. It is particularly efficient when, whether there are crashes or not, the underlying failure detector makes few mistakes. Additionally, the proposed protocol tolerates message duplication and copes with some message losses.
[Protocols, open systems, unreliable failure detectors, process crash, message duplication, volatile memory, consensus protocol, Computer crashes, Electronic switching systems, Electrical capacitance tomography, consensus problem, system recovery, software fault tolerance, crash/recovery model, Fault diagnosis, critical data, Detectors, Ash, Broadcasting, stable storage, protocols, fault-tolerant distributed systems, asynchronous systems, Context modeling]
A data allocation algorithm for distributed hypermedia documents
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
A major cost in executing queries in a distributed database system is the data transfer cost incurred in transferring relations (fragments) accessed by a query from different sites to the site where the query is initiated. The objective of a data allocation algorithm is to locate the fragments at different sites so as to minimize the total data transfer cost incurred in executing a set of queries. This is equivalent to minimizing the average query execution time, which is of primary importance in a wide class of distributed systems. The data allocation problem, however, is NP-complete, and thus requires fast heuristics to generate efficient solutions. The problem becomes more complex in the context of hypermedia documents (Web pages), within which the multimedia data objects (MDOs) need to be synchronized during presentation to the end users. Since the basic problem of data allocation in distributed database systems is NP-complete, we need heuristics which can generate near-optimal MDO allocations. In this paper, we propose a navigational model to represent hypermedia documents and their access behaviour from end users. We also formulate the problem by developing a base-case cost model for the response time, and we design an algorithm to find near-optimal solutions for allocating MDOs of the hypermedia documents while adhering to the synchronization requirements. We compare the algorithmic solution with an exhaustive solution over a set of experiments.
[storage allocation, Costs, query execution, multimedia presentation, Petri nets, hypermedia, fast heuristics, query processing, document access behaviour, near-optimal solutions, distributed databases, average query execution time minimization, World Wide Web pages, data transfer cost minimization, distributed hypermedia documents, information resources, Navigation, query fragment transfer, multimedia data object synchronization, Probability, NP-complete problem, Statistics, synchronisation, Computer science, response time, data allocation algorithm, Joining processes, navigational model, computational complexity, distributed database system]
The cost of recovery in message logging protocols
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Past research in message logging has focused on studying the relative overhead imposed by pessimistic, optimistic, and causal protocols during failure-free executions. We give the first experimental evaluation of the performance of these protocols during recovery. We discover that, if a single failure is to be tolerated, pessimistic and causal protocols perform best, because they avoid rollbacks of correct processes. For multiple failures, however, the dominant factor in determining performance becomes where the recovery information is logged (i.e. at the sender, at the receiver, or replicated at a subset of the processes in the system) rather than when this information is logged (i.e. if logging is synchronous or asynchronous).
[Protocols, message passing, Engineering profession, NASA, Laboratories, pessimistic protocols, causal protocols, message logging protocols, Computer crashes, Electronic mail, system recovery, Cost accounting, software fault tolerance, experimental evaluation, Uniform resource locators, optimistic protocols, performance, rollbacks, failure-free execution, protocols, software performance evaluation]
Semi-passive replication
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
This paper presents the semi-passive replication technique, a variant of passive replication, that can be implemented in the asynchronous system model without requiring a membership service to agree on a primary. Passive replication is a popular replication technique since it can tolerate non-deterministic servers (e.g., multi-threaded servers) and uses little processing power when compared to other replication techniques. However, passive replication suffers from a high reconfiguration cost in case of the failure of the primary. The semi-passive replication technique presented in the paper benefits from the same advantages as passive replication. However, since it does not require a group membership service, semi-passive replication has a considerably lower cost in case of failure. As explained in the paper, this technique can benefit from an aggressive time-out value significantly lower than what a group membership allows. As a result, the reaction to crashes is greatly improved. The semi-passive replication algorithm uses failure detectors. The algorithm given in the paper is analysed in the failure free case and in the case of one server crash. The response time (for the client) of these two scenarios is analysed through simulation.
[client-server systems, Costs, Protocols, nondeterministic servers, Redundancy, system failure, failure detectors, simulation, client server system, Computer crashes, high reconfiguration cost, system recovery, server crash, Read only memory, Hip, Delay, software fault tolerance, membership service, Fault tolerant systems, response time, asynchronous system model, Marine vehicles, semi-passive replication technique, Contracts]
Checkpoints-on-demand with active replication
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Checkpointing and roll-back recovery is a well known technique for recovering from software process failures. Analytical models have been developed for computing the completion time of processes that use various checkpointing strategies such as periodic checkpointing, random checkpointing etc. In this paper, we show that with active replication of processes, a strategy that uses a mechanism we call checkpoints-on-demand will result in an expected completion time smaller than that can be achieved with traditional schemes that use periodic checkpoints. With checkpoints-on-demand, when a process fails, it is recovered from an induced checkpoint taken of a replica of the process. Recovery of persistent server processes through state-transfer from a replica has been proposed in the context of group communication systems and in the process cloning approach of the Delta-4 architecture. But it has not been previously proposed and analyzed as a mechanism for reducing the expected completion time of a long running process.
[Checkpointing, Context, client-server systems, roll-back recovery, checkpoints-on-demand, Cloning, software process failures, active replication, completion time, persistent server processes, system recovery, software fault tolerance, Analytical models, random checkpointing, Failure analysis, group communication systems, periodic checkpointing, process cloning approach, Delta-4 architecture]
An efficient scheme to reduce handoff dropping in LEO satellite systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The problem of handoffs in cellular networks is compounded in a LEO (low Earth orbit) satellite-based cellular network due to the relative motion of the satellites themselves with respect to a stationary observer on Earth. Typically, the velocity of motion of mobile telephones can be ignored when compared to the very high velocity of the footprints of satellites. We exploit this property of the LEO satellite systems and propose a handoff scheme that results in a significant decrease in handoff dropping. For the same handoff dropping probability, our scheme has a significantly lower new call blocking probability than the conventional reservation scheme. We present an analytical approximation that is in very good accord with simulation results.
[stationary observer, radio networks, mobile telephones, mobile satellite communication, Wireless communication, Intelligent networks, Analytical models, Wireless networks, reservation scheme, Satellite ground stations, relative satellite motion, satellite footprints, probability, new call blocking probability, handoff dropping reduction, LEO satellite systems, velocity, Artificial satellites, Land mobile radio cellular systems, Low earth orbit satellites, Channel allocation, telephone networks, analytical approximation, low Earth orbit satellite-based cellular network, Propagation delay, cellular radio]
Security in the large: is Java's sandbox scalable?
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Using Java security as an example, this paper tries to draw attention to the various issues of security in large scale distributed systems, some of which are often ignored when the security mechanisms are designed. Even though a lot of work has been done on Java security, we argue in this paper that due to weaknesses inherent in the Java approach to building sandboxes, Java security is not suitable when applied to large-scale distributed systems. In addition, the paper also explains the impact of these issues on the security mechanisms and introduces some of our efforts to find the security mechanisms that address the issues of building large scale secure systems.
[sandbox model, Java, object-oriented programming, Laboratories, large scale distributed systems, Electronic switching systems, large scale secure systems, Electrical capacitance tomography, Security, Read only memory, Java security, Fault tolerance, security of data, Computer bugs, Large-scale systems, Internet]
The Chameleon infrastructure for adaptive, software implemented fault tolerance
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
This paper presents Chameleon, an adaptive software infrastructure for supporting different levels of availability requirements in a heterogeneous networked environment. Chameleon provides dependability through the use of ARMORs-Adaptive, Reconfigurable, and Mobile Objects for Reliability. Three broad classes of ARMORs are defined: Managers, Daemons, and Common ARMORs. Key concepts that support adaptive fault tolerance include the construction of fault tolerance execution strategies from a comprehensive set of ARMORs, the creation of ARMORs from a library of reusable basic building blocks, the dynamic adaptation to changing fault tolerance requirements, and the ability to detect and recover from errors in applications and in ARMORs.
[heterogeneous networked environment, open systems, NASA, Buildings, Redundancy, adaptive software implemented fault tolerance, ARMORs, reliability, Electrical capacitance tomography, Application software, Chameleon infrastructure, Read only memory, software fault tolerance, Fault tolerance, availability requirements, reusable basic building blocks, High performance computing, mobile objects, fault tolerance execution strategies, Computer architecture, Computer networks, adaptive objects, reconfigurable objects, distributed object management]
Look-ahead traffic distribution in wormhole-routed networks
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
A new approach, look-ahead traffic distribution, is proposed in this paper to improve the performance of adaptive routing algorithms in wormhole routed networks. In most adaptive routing algorithms, a packet changes its forwarding direction only if its requesting channel is not available, i.e., the routing decision is based only on the traffic information (buffer availability) in adjacent nodes. The performance of these adaptive routing algorithms usually is far from optimal due to such a crude routing decision-making process without taking into consideration traffic information in non-adjacent nodes which a packet potentially is to be forwarded to at a later time. By adding minimal hardware to the router, the proposed approach allows neighboring routers to exchange integrated traffic information and each router to re-integrate information gathered before further propagation. It is intended to use a small amount of such integrated information to give each router a sketch of semi-global traffic information and degree of congestion in an extended local area. Such information is then used for a more intelligent routing decision-making process in each router. Experimental results show that the communication latency and the saturation point have been significantly improved by applying such an add-on feature to existing adaptive algorithms.
[Adaptive algorithm, multiprocessor interconnection networks, Telecommunication traffic, intelligent routing decision-making, Electronic mail, Delay, Concurrent computing, network routing decision, semi-global traffic information, Intelligent networks, Hardware, software performance evaluation, multiprocessing systems, network routing, Decision making, network congestion, adaptive routing algorithms, Routing, wormhole routed networks, communication latency, neighboring routers, look-ahead traffic distribution, performance, High performance computing, distributed algorithms]
Cache injection on bus based multiprocessors
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Software-controlled cache prefetching and data forwarding are widely used techniques for tolerating memory latency in shared memory multiprocessors. However, some previous studies show that cache prefetching is not so effective on bus-based multiprocessors, while the effectiveness of data forwarding has not been explored in this environment, yet. In this paper, a novel technique called cache injection is proposed. Cache injection, tuned to the properties of bus-based architectures, combines advantages of both cache prefetching and data forwarding. Some preliminary experiments show that the proposed solution can significantly help in reducing the overall miss ratio and bus traffic in applications where write-shared data prevails.
[Costs, Prefetching, Multiprocessor interconnection networks, bus based multiprocessors, memory latency tolerance, Communication system control, system buses, experiments, performance evaluation, bus-based architectures, data forwarding, shared memory multiprocessors, Electronic switching systems, cache storage, Delay, Postal services, Multiprocessing systems, write-shared data, Program processors, software-controlled cache prefetching, cache injection, shared memory systems, Virtual manufacturing, bus traffic]
Evolving distributed software engineering environments
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
For a software engineering environment to be useful for the development process, it must provide a complete set of tools to assist the software development tasks. The tools focus on separate issues of a highly integrated problem and, in general, must be capable of assisting one another in the midst of intelligently pursuing their own goals for improving the software products. These tools must address the truly significant portion of the software-engineering task: evolution of existing software. The requirements of the tool set change as we learn more about the development process; therefore, the tools need the ability to be added, removed, substituted, and modified. More than that, the tools' requirements evolve as the software products that they operate on evolve. We meet these needs by providing the tool set as a distributed community of autonomous learning agents. The SWEEP system is part of the ongoing Software Service Bay project at the Artificial Intelligence Laboratory of Wayne State University that addresses these issues.
[Laboratories, Programming, Data structures, Displays, autonomous learning agents, distributed software engineering environments evolution, Software debugging, Database languages, software agents, Learning, SWEEP system, distributed community, computer aided software engineering, software tools, Software tools, Artificial intelligence, distributed programming, Software engineering, software-engineering task]
Fault-tolerant Total Order Multicast to asynchronous groups
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
While Total Order Broadcast (or Atomic Broadcast) primitives have received a lot of attention, the paper concentrates on Total Order Multicast to Multiple Groups in the context of asynchronous distributed systems in which processes may suffer crash failures. "Multicast to Multiple Groups" means that each message is sent to a subset of the process groups composing the system, distinct messages possibly having distinct destination groups. "Total Order" means that all message deliveries must be totally ordered. The paper proposes a protocol for such a multicast primitive. This protocol is based on two underlying building blocks, namely, Uniform Reliable Multicast and Uniform Consensus. Its design characteristics lie in the two following properties. The first one is a minimality property, more precisely, only the sender of a message and processes of its destination groups have to participate in the multicast of the message. The second property is a locality property: no execution of a consensus has to involve processes belonging to distinct groups (i.e., consensus are executed on a "per group" basis). This locality property is particularly useful when one is interested in using the Total Order Multicast primitive in large scale distributed systems. An improvement that reduces the cost of the protocol is also suggested.
[Protocols, Costs, asynchronous distributed system, distributed processing, distinct destination groups, Fault tolerance, locality property, Fault tolerant systems, Atomic Broadcast, crash failures, Detectors, Ash, multicast communication, Uniform Reliable Multicast, minimality property, Broadcasting, process groups, destination groups, multicast primitive, asynchronous groups, Total Order Multicast primitive, fault tolerant Total Order Multicast, large scale distributed systems, Uniform Consensus, Computer crashes, broadcasting, message deliveries, fault tolerant computing]
Interactive image retrieval over the Internet
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
An efficient image database system is developed. The most important features of the proposed system include compressed domain indexing, searching by using scalable features, and progressive image transmission. User interaction is involved both at the search refinement stage and in the display of the query results. The most important query types include query by color layout and query by wavelet-coefficient clustering information. The indexing and searching algorithms are tightly coupled with the underlying image compression algorithm by which means the images are stored in the database, reducing both the complexity and the storage requirements of the database management system. In this research, we utilize our previously developed (B.-B. Chai et al., 1997, 1998) high-performance wavelet image coding algorithm, termed "significance-linked connected component analysis\
[progressive image transmission, compressed domain indexing algorithm, search refinement, image database system, wavelet transforms, JPEG standard, visual databases, query by color layout, Wavelet analysis, Displays, visual communication, user interaction, image compression algorithm, computer experiments, Image coding, database indexing, Clustering algorithms, interactive systems, searching algorithm, high-performance wavelet image coding algorithm, query by wavelet-coefficient clustering information, Image storage, software performance evaluation, data compression, significance-linked connected component analysis, efficiency, Image communication, Image retrieval, interactive image retrieval, scalable features, Image databases, DBMS complexity, image retrieval, Internet, storage requirements, image coding, query results display, Indexing, compression performance]
A fault-tolerant protocol for providing the exactly-once property of mobile agents
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Mobile agent technology has been proposed for various fault-sensitive application areas, including electronic commerce, systems management and active messaging. Due to the autonomy of mobile agents, there is no natural instance that monitors the progress of an agent's execution. As a result, agents may be lost or blocked due to node crashes or network partitioning even if there are other nodes available that could continue processing. In this paper, we describe a protocol that provides exactly-once semantics of agent execution and additionally reduces the blocking probability of agents by introducing observer nodes for monitoring the progress of agents. This protocol is based on conventional transactional technology such as defined by X/Open DTP or CORBA OTS. It is being implemented in Mole, a mobile agent system developed at Stuttgart University.
[blocking probability, Protocols, node crashes, fault-tolerant protocol, Electronic switching systems, Electronic commerce, Security, Fault tolerance, Technology management, Mobile agents, mobile agents, systems management, Desktop publishing, computer network reliability, X/Open DTP, exactly-once property, Monitoring, distributed object management, electronic commerce, Mole, fault-sensitive application areas, Computer crashes, transactional technology, observer nodes, transport protocols, network partitioning, active messaging, fault tolerant computing, CORBA OTS]
A randomized algorithm for distributed consensus
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
We describe a randomized, fully distributed algorithm for distributed consensus and evaluate its performance assuming probabilistically bounded message delay. Each node randomly contacts a few other nodes and incorporates their values into its own value. All the nodes are able to reach consensus in this manner after a few rounds. The results show that the randomized algorithm is flexible, efficient and robust, and offers several advantages over a deterministic algorithm.
[distributed consensus, Identity-based encryption, Protocols, Costs, performance evaluation, message delay, randomized algorithm, deterministic algorithm, deterministic algorithms, Delay, randomised algorithms, distributed algorithm, Aggregates, distributed algorithms, software performance evaluation]
An object relational database for brain aging research
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
BioCompose is a next-generation data management system currently being developed at the University of California at Irvine. The system integrates two databases (a clinical patient database and a cellular-level neuropathological database) into a uniform framework with advanced data/knowledge management facilities and a simple, friendly user interface for all classes of users in the Internet environment. Using any standard Web browser, users can access data from any Internet-capable computer. This paper briefly discusses the design, architecture, and some implementation details of BioCompose.
[data management system, Relational databases, BioCompose, user interfaces, knowledge management, Engineering management, friendly user interface, Computer architecture, Aging, brain aging research, object-relational database, object-oriented databases, Information retrieval, Knowledge management, medical information systems, brain, relational databases, World Wide Web browser, Internet-capable computer, Pathology, geriatrics, clinical patient database, online front-ends, data access, User interfaces, Internet environment, Internet, neurophysiology, cellular-level neuropathological database, fifth generation systems, Dementia]
An efficient algorithm for causal message logging
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Causal message logging has many good properties such as nonblocking message logging and no rollback propagation. However, it requires a large amount of information to be piggybacked on each message, which may incur severe performance degradation. This paper presents an efficient causal logging algorithm based on the new message log structure, LogOn, which represents the causal interprocess dependency relation with much smaller overhead compared to the existing algorithms. The proposed algorithm is efficient in the sense that it requires no additional information other than LogOn to be carried in each message, while the other algorithms require extra information other than the message log, to eliminate the duplicates in log entries. Moreover, in those algorithms, as more extra information is added into the message, more duplicates can be detected. However, the proposed algorithm achieves the same degree of efficiency using only the message log carried in each message, without any extra information.
[Checkpointing, LogOn, Costs, message passing, Access protocols, performance degradation, system recovery, rollback propagation, software fault tolerance, Computer science, nonblocking message logging, causal message logging algorithm, Fault tolerant systems, distributed algorithms, software performance evaluation, message log structure, fault tolerant systems, causal interprocess dependency relation]
Secure and scalable replication in Phalanx
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
Phalanx is a software system for building a persistent, survivable data repository that supports shared data abstractions (e.g., variables, mutual exclusion) for clients. Phalanx implements data abstraction that ensures useful properties without trusting the servers supporting these abstractions or the clients accessing them, i.e., Phalanx can survive even the arbitrarily malicious corruption of clients and (some number of) servers. At the core of the system are survivable replication techniques that enable efficient scaling to hundreds of Phalanx servers. In this paper we describe the implementation of some of the data abstractions provided by Phalanx, discuss their ability to scale to large systems, and describe an example application.
[client-server systems, persistent survivable data repository, Identity-based encryption, replication techniques, replicated databases, Buildings, client server system, Application software, Online services, Reactive power, Publishing, security of data, Voting, Public key, shared data abstractions, secure scalable replication, Software systems, data structures, Large-scale systems, Phalanx]
A secure auction-like negotiation protocol for agent-based Internet trading
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
We propose a secure auction-like negotiation protocol for agent based Internet trading, which not only retains the agent's mobility and flexibility, but also takes secure measures to prevent attacks from malicious hosts during the negotiation process. The particular features of the proposed protocol are: (1) negotiation for agent based trading is performed through a novel pattern of electronic auction; (2) negotiation results between two hosts are ensured to be valid with their signatures; (3) malicious actions can be detected and the breeder can be dug out by the help of sociological factors; (4) information gathering and negotiation processes are combined together while few communications are needed.
[Protocols, breeder, Electronic commerce, secure measures, sociological factors, Information science, Network servers, negotiation process, electronic auction, malicious actions, Mobile agents, Web and internet services, malicious hosts, negotiation processes, cooperative systems, protocols, agent based Internet trading, Protection, Web server, electronic commerce, Data security, secure auction-like negotiation protocol, software agents, Intelligent agent, computer network management, security of data, negotiation results, Internet, information gathering]
Tolerating Visitor Location Register failures in mobile environments
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
For mobile users who move frequently but receive relatively rare calls, a forwarding scheme has been shown to outperform the normal IS-41 location management scheme. But the forwarding scheme is more vulnerable to failure of intermediate Visitor Location Registers (VLRs) than the IS-41 scheme. We propose two simple variations to the forwarding scheme to address the fault tolerance weakness. One is based on the idea of maintaining two paths from the home location server to the last VLR. The second scheme is based on the knowledge of the neighbors of the faulty VLR. We evaluate and compare the performance of these location management schemes.
[IS-41 location management scheme, personal communication networks, fault tolerance weakness, Scholarships, Telecommunication traffic, home location server, Electronic switching systems, Registers, intermediate Visitor Location Registers, Read only memory, Computer science, Reactive power, mobile computing, Databases, forwarding scheme, fault tolerant computing, Performance analysis, computer network reliability, mobile users, Personal communication networks, location management schemes]
ROI: an invocation mechanism for replicated objects
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
The reliable object invocation mechanism provided by HIDRA for the coordinator-cohort and the passive replication models offers support to ensure that all the replicas of the object being invoked are correctly updated before such an invocation is terminated. This mechanism also ensures that if a primary or coordinator replica crashes, the client is able to reconnect to the previously initiated invocations, collecting their results without requiring their reexecution. All this support is provided transparently to the client of the replicated objects, which does not notice any difference in respect to the invocations made to non-replicated objects. Moreover, the protocols described in the paper deal also with the failure of any of the objects involved in this kind of invocations.
[Checkpointing, object-oriented programming, reliable object invocation mechanism, Multicast protocols, Concurrency control, Computer crashes, HIDRA, Electrical capacitance tomography, fault tolerant objects, ROI, Delay, software fault tolerance, coordinator cohort model, replicated objects, passive replication model, Operating systems, Fault tolerant systems, System recovery, remote procedure calls, protocols, distributed object management]
A VP-accordant checkpointing protocol preventing useless checkpoints
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
A useless checkpoint corresponds to the occurrence of a checkpoint and communication pattern called Z-cycle. A recent result shows that ensuring a computation without Z-cycles is a particular application of a property, namely Virtual Precedence (VP), defined on an interval-based abstraction of a computation. We first propose a taxonomy of communication-induced checkpointing protocols based on the way they ensure the VP property. Then we derive a sufficient condition ensuring no Z-cycles in a distributed computation. This condition defines a checkpoint and communication pattern, namely suspect Z-cycle, such that if no suspect Z-cycle exists in a distributed computation then no Z-cycle exists. We present finally a communication-induced checkpointing protocol that avoids useless checkpoints by preventing on-the-fly the formation of suspect Z-cycles and discuss its performance with respect to other protocols.
[Checkpointing, Protocols, interval-based abstraction, Taxonomy, distributed processing, useless checkpoint, distributed computation, Electronic switching systems, Electrical capacitance tomography, Remuneration, Distributed computing, system recovery, software fault tolerance, Virtual Precedence, performance, Z-cycle, communication pattern, protocols, VP-accordant checkpointing protocol, software performance evaluation]
Failure handling in an optimized two-safe approach to maintaining primary-backup systems
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
In a primary backup database system, transaction processing takes place at the primary and the log records generated are propagated to the backup which uses them to reconstruct the database state at the primary. If the primary fails, the backup takes over to provide continued service. Most existing designs of primary backup database systems have concentrated on techniques to tolerate complete failures in which the entire primary fails, say due to a disaster. In multiprocessor environments, where the primary and the backup databases are partitioned across multiple computers, a more common case is a partial failure in which some database partitions fail but the system as a whole survives. Existing approaches either ignore partial failures, or require the failed database partition to be unavailable. We explore a design of the primary backup database system that uses the backup not only for disaster protection, but also for continued availability during partial failures. The approach is developed in the context of the improved optimized 2-safe strategy to transmitting logs from the primary to the backup, introduced by K. Hu et al. (1997), which combines the best features of the previously developed 1-safe and 2-safe strategies.
[transaction processing, database partitions, complete failures, disaster protection, Throughput, Standby generators, backup databases, failure handling, continued availability, Earthquakes, optimized 2-safe strategy, Propagation losses, Database systems, primary backup systems maintenance, Protection, multiprocessing systems, Delay effects, multiprocessor environments, disasters, back-up procedures, primary backup database system, multiple computers, partial failure, Transaction databases, Information technology, software fault tolerance, log records, Computer science, database state, concurrency control, optimized two-safe approach]
An active transcoding proxy to support mobile web access
Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems
None
1998
In this paper, we present a proxy based system (MOWSER) to support web browsing from mobile clients over wireless networks. Mowser is a proxy agent between the mobile host and the web server, which performs active transcoding of data on both upstream and downstream traffic to present web information to the mobile user according to the QoS parameters set by the user. Active transcoding is defined as modifying the HTTP stream in situ, and it is entirely transparent to the user. Further, our system does not pose any additional requirements on the mobile user. This is an improvement over other proxy based systems, which only transcode images on the downstream and are mostly not configurable. While developed for mobile users, such a system can actually be useful in any low bandwidth scenario.
[HTTP stream, low bandwidth scenario, web browsing, mobile computing, mobile host, web information, Wireless networks, Bandwidth, Hardware, Computer networks, active transcoding proxy, Web server, client-server systems, Transcoding, downstream, wireless networks, mobile web access, Middleware, web server, Computer science, Mowser, mobile clients, transport protocols, Web pages, Internet, proxy based systems, Mobile computing]
Workshop on Reliable Middleware (Wremi) Foreward
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Presents the introductory welcome message from the conference proceedings.
[]
Workshop on Electronic Commerce (Welcom) Foreward
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Presents the introductory welcome message from the conference proceedings.
[]
Tolerating transient faults in statically scheduled safety-critical embedded systems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Static off-line scheduling ensures predictability of worst-case behavior and high resource utilization for safety-critical applications but lacks the flexibility needed to deal with run-time fault-tolerance. We present a temporal redundancy-based recovery technique that tolerates transient task failures in statically scheduled distributed embedded systems where tasks have timing, resource, and precedence constraints. Task failures are handled using precomputed contingency schedules that introduce adaptive fault tolerance into table-driven dispatchers. Failures are masked using the spare capacity on the affected processor and the recovery scheme requires no hardware overhead. Our approach combines the benefits of static scheduling with the run-time flexibility needed for fault tolerance in low-cost embedded systems. We present a method to obtain contingency schedules and prove its correctness. We also evaluate the effectiveness of the proposed method through simulation.
[run-time fault-tolerance, redundancy-based recovery, Job shop scheduling, Redundancy, safety-critical, safety-critical software, Application software, system recovery, Automotive engineering, Mars, Runtime, Embedded system, Hardware, fault tolerant computing, statically scheduled, Timing, Resource management, transient task failures]
Logging and recovery in adaptive software distributed shared memory systems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Software distributed shared memory (DSM) improves the programmability of message-passing machines and workstation clusters by providing a shared memory abstract (i.e., a coherent global address space) to programmers. As in any distributed system, however; the probability of software DSM failures increases as the system size grows. This paper presents a new efficient logging protocol for adaptive software DSM (ADSM), called adaptive logging (AL). It is suitable for both coordinated and independent checkpointing since it speeds up the recovery process and eliminates the unbounded rollback problem associated with independent checkpointing. By leveraging the existing coherence data maintained by ADSM, our AL protocol adapts to log only unrecoverable data (which cannot be recreated or retrieved after a failure) necessary for correct recovery, reducing both the number of messages logged and the amount of logged data. We have performed experiments on a cluster of eight Sun Ultra-5 workstations, comparing our AL protocol against the previous message logging (ML) protocol by implementing both protocols in TreadMarks-based ADSM. The experimental results show that our AL protocol consistently outperforms the ML protocol: Our protocol increases the execution time slightly by 2% to 10% during failure-free execution, while the ML protocol lengthens the execution time by many folds due to its larger log size and higher number of messages logged. Our AL-based recovery also outperforms ML-based recovery by 9% to 17% under parallel application examined.
[workstation clusters, adaptive software DSM, adaptive software, Access protocols, Software performance, coherent global address space, Electronic switching systems, Application software, Distributed computing, Sun, system recovery, Programming profession, message-passing machines, Parallel programming, shared memory abstract, Coherence, distributed shared memory systems, Workstations, adaptive logging]
Exactly-once end-to-end semantics in CORBA invocations across heterogeneous fault-tolerant ORBs
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We propose a mechanism, based on fault tolerant request IDs piggybacked on GIOP messages, that can be used to enable fault tolerant CORBA invocations spanning multiple heterogeneous ORBs. The proposed mechanism supports replication at both server and client sides.
[Context-aware services, client-server systems, GIOP messages, Protocols, message passing, client server system, Electrical capacitance tomography, piggybacking, Standards publication, Read only memory, software fault tolerance, Fault diagnosis, CORBA, Fault tolerance, Network servers, fault tolerant request ID, fault tolerant CORBA invocations, Intrusion detection, end-to-end semantics, heterogeneous fault-tolerant ORB, Standards development, distributed object management]
HAMFS file system
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Unix's lack of a robust and expandable file system has become a significant problem with the growth of Unix in large commercial environments. The HAMFS (highly available multi-server file system) is a cluster file system designed to address this need. HAMFS offers disk-pooling, supports off-the-shelf disks, and automatically balances file load across disks dynamically. Data residing in a disk pool is directly accessible from every node in a HAMFS cluster. As user's capacity requirements grow, HAMFS provides easy disk pool expansion. Finally, HAMFS provides uniform scaling of file system performance from a single node configuration to large multi-node clusters, offering significant performance advantage over traditional file systems. For example, in short file access situations, HAMFS provide a factor of five performance improvement over NFS, and a factor of two improvement over conventional local file systems. Technologies developed for HAMFS are applied to Fujitsu's file system product SafeFILE.
[Availability, Unix, Costs, Laboratories, cluster file system, disk-pooling, Electronic switching systems, file access, HAMFS file system, storage management, File systems, Aggregates, Investments, SafeFILE, multiple server file system, file organisation, Robustness, Hardware, multinode clusters, Protection]
Reliability of future telephone networks
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
false
[Packet switching, Bandwidth, Quality of service, Switches, Telephony, Signal processing, Computer networks, IP networks, Resource management, Switching circuits]
An agent platform for reliable asynchronous distributed programming
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Production of reliable and flexible distributed applications is a growing area of interest and research. Various middleware technologies are often used as the communication infrastructure and as a practical ease to the network programming problem. Among them, message-oriented middleware (MOM) are known to provide reliable and flexible communication through asynchronous message passing. This kind of middleware is of particular interest when coordinating components that are not designed for simultaneous execution. Usually MOM focus on the communication layer with a programming interface, charge to the external components to adapt to the MOM communication model. We introduce a distributed programming model based on autonomous software entities called agents. Agents act as the glue software components and they offer reliable and flexible properties like atomic execution or migration from node to node. The combination of both a MOM and agents has been implemented in the AAA platform presented in the second part of the paper.
[agent platform, Protocols, Scalability, software reliability, Electrical capacitance tomography, reliable asynchronous distributed programming, Message-oriented middleware, asynchronous message passing, message-oriented middleware, network programming, Production, Hardware, software components, distributed programming, distributed object management, client-server systems, message passing, communication infrastructure, Data structures, MOM, Application software, software agents, Programming profession, Message passing, autonomous software entities, programming interface]
Service provision and composition in virtual business communities
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Service provisioning is at the base of the business economy and is a major driver for business-to-business interaction. The strong competition induced by market globalisation is forcing businesses to concentrate on their core competencies and to rely on specialised third parties for the provisioning of corollary service infrastructure. New expectations and opportunities are emerging for business uses of the Internet, and value chain optimisation is a priority. The role that the Internet is playing in the business service market at the moment revolves around a one-to-one model. Once two businesses have established a relationship, the Internet mainly acts as communication channel. Efficiency is an important benefit offered by the electronic transactions format, but very little value is added to the transaction itself. The decision about the best provider for a service and the trust in its capability to deliver the quality expected are instead high-value components for business transactions. The aim of our research is to explore electronic service provision in business-to-business scenarios, focusing on multi-party service composition. A composition-oriented service model has been designed, and it is presented in this paper, together with a prototype of its support infrastructure.
[support infrastructure, Supply chain management, value chain optimisation, core competencies, Laboratories, Supply chains, electronic business transactions, multi-party service composition, communication channel, electronic business, competition, Information management, Internet business uses, service provisioning, specialised third parties, Business communication, market globalisation, Economic forecasting, business economy, electronic commerce, Software prototyping, virtual business communities, efficiency, business-to-business interaction, Globalization, corollary service infrastructure, Hip, composition-oriented service model, quality, economics, business service market, business-to-business scenarios, Internet]
Failure data analysis of a LAN of Windows NT based computers
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
This paper presents results of a failure data analysis of a LAN of Windows NT machines. Data for the study was obtained from event logs collected over a six-month period from the mail routing network of a commercial organization. The study focuses on characterizing causes of machine reboots. The key observations from this study are: 1) most of the problems that lead to reboots are software related; 2) rebooting the machine does not always solve the problem; 3) there are indications of propagated or correlated failures; and 4) though the average availability evaluates to over 99%, the machine downtime lasts (on average) two hours. Since the machines are dedicated mail servers, bringing down one or more of them can potentially disrupt storage, forwarding, reception and delivery of mail. This suggests that the average availability is not a good measure to characterize this type of network service.
[Availability, Data analysis, Windows NT machines, Routing, local area networks, Electronic mail, availability, failure analysis, machine downtime, Postal services, Network servers, rebooting, Operating systems, Failure analysis, failure data analysis, LAN, Computer networks, computer network reliability, Local area networks, network service]
A practical guideline to the implementation of online shops
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Implementing an online shop can be a risky project, since there's no widespread and profound knowledge or experiences. This guideline is intended to support the management of organizational, technical and human challenges in online shop implementation projects. An overview of the implementation process is given. Subsequently, the development of a concept, using the services offered by a company to its customers, is discussed. Prototyping, realization and operation of the online shop are further topics. The last part introduces some practical "helper" tools for the implementation of online shops.
[overview, software prototyping, organizational challenges, prototyping, Companies, home shopping, helper tools, Electrical capacitance tomography, Electronic commerce, online shop implementation projects, Guidelines, customer services, online operation, Web and internet services, Prototypes, Isolation technology, Software standards, human challenges, Consumer electronics, technical challenges, Business, electronic commerce]
Trust and electronic commerce-more than a technical problem
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
It is argued that the building of trust in electronic commerce depends only partly on technical security, and the knowledge of security gaps and ways of closing them. It is not only a technical system which is trusted but rather a socio-technical system, including users, business practices and related institutions. We take a closer look at the concept of trust and its relation to knowledge, describe the current situation in electronic commerce, and analyse different technical approaches, that aim at providing security, and nontechnical possibilities to enhance security and trust through institutions.
[transaction processing, trust, Identity-based encryption, Paper technology, Electronic commerce, Cultural differences, Environmental economics, information networks, technical security, socio-technical system, technical approaches, institutions, Sociotechnical systems, technical system, security of data, security gaps, business practices, Information security, Authentication, data privacy, Acceleration, nontechnical possibilities, Business, electronic commerce]
On the provision of replicated Internet auction services
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The paper presents the design of a software infrastructure that can support negotiation and competition among buyers and sellers of goods (auctioning) over the Internet. The goals of data integrity, responsiveness, and scalability have been achieved by replicating the auction service across a number of auction servers.
[auctioning, Scalability, Data security, software infrastructure, software design, Quality of service, replicated Internet auction services, data integrity, competition, Electronic commerce, Distributed computing, Delay, scalability, Painting, responsiveness, Software design, auction servers, Web and internet services, file servers, negotiation, Internet, Web server, electronic commerce, retail data processing]
Security mechanisms for using mobile agents in electronic commerce
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
In order for mobile agents to be accepted as a basic technology for enabling electronic commerce, proper security mechanisms must be developed. Hosts must be protected from malicious agents, agents must be protected from other agents and also agents must be protected from malicious hosts. For solving the first three problems, existing technology from operating systems and distributed systems research can be used. The last problem is new and specific to the mobile agent paradigm and it is much harder to solve. Due to this problem, many say that mobile agents are not ready for the e-commerce systems. We discuss the security requirements of mobile agents in the context of electronic commerce and analyze how these requirements can be met. We show that, because of the characteristics of e-commerce systems, the security requirements of the agents and their users can be assured in real and open environments such as the Internet.
[malicious agents, operating systems, security mechanisms, Ice, Electronic commerce, Security, open environments, Read only memory, mobile computing, security of data, distributed systems research, Operating systems, Mobile agents, mobile agents, operating systems (computers), mobile agent paradigm, Internet, security requirements, Protection, Web server, e-commerce systems, Business, electronic commerce]
The mainframe as a high-available, highly scalable CORBA platform
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The increasing demand for interoperable systems requires the integration of traditional mainframe-based transaction processing (TP) monitors with CORBA-based middleware infrastructures. We discuss various integration approaches. The integration of IBM's IMS-TM (Information Management System Transaction Manager) and CORBA is discussed in detail, and up-to-date performance and scalability results are presented. The results show that the combination of CORBA and mainframe-based TP monitors is significant step towards highly available, highly scalable object transaction monitors (OTMs).
[Computer interfaces, transaction processing, middleware infrastructures, mainframe-based transaction processing monitors, open systems, software reliability, CORBA platform, Containers, Electrical capacitance tomography, availability, scalability, Space technology, Computer architecture, Desktop publishing, distributed object management, software performance evaluation, transaction manager, Graphical user interfaces, mainframes, client-server systems, IMS-TM, IBM Information Management System, Banking, Middleware, performance, object transaction monitors, system monitoring, integration approaches, Resource management, interoperable systems]
CosNamingFT-a fault-tolerant CORBA naming service
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
This paper describes the design and implementation of a fault-tolerant CORBA naming service-CosNaming FT. Every CORBA object is accessed through its Interoperable Object Reference (IOR), which is registered with the CORBA name service. The name service therefore is a critical gateway to all objects in a distributed system; to avoid having a single point of failure, the name service should be made fault-tolerant. CosNamingFT uses the GroupPac package, a CORBA-compliant suite of protocols, to replicate the name server. GroupPac services are built from Common Object Services that function as building blocks to implement fault-tolerant applications. This paper aims to demonstrate the usefulness of some of these object services and to demonstrate the importance of open solutions issues in replicating distributed objects.
[Availability, Identity-based encryption, GroupPac package, Electronic switching systems, Electrical capacitance tomography, Registers, Interoperable Object Reference, replicating distributed objects, Fault tolerance, Information science, Lungs, Fault tolerant systems, Open systems, CORBA naming service, CosNamingFT, fault tolerant computing, protocols, naming services, distributed object management, fault-tolerant]
Database replication: if you must be lazy, be consistent
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Due to the severe performance penalties associated with isochronous replication, there is significant interest in asynchronous replica management protocols. Lazy protocols currently in use either do not guarantee consistency and serializability, as needed by transactional semantics, or they impose restrictions on the placement of data and on which data objects can be updated. In this paper, we consider an alternative update protocol, based on epidemic communication, that guarantees consistency and serializability in spite of a write-anywhere capability.
[isochronous replication, Protocols, lazy protocols, transactional semantics, replicated databases, asynchronous replica management protocols, write-anywhere capability, memory protocols, Transaction databases, Electrical capacitance tomography, Maintenance, database replication, consistency, epidemic communication, Computer science, Network servers, data object update protocol, Operating systems, data placement, performance penalties, serializability, Communication networks, Human resource management, Clocks]
An approach for fault-tolerance in hard real-time distributed systems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The presence of hard timing constraints makes the design of fault tolerant systems difficult because when tasks are replicated to treat errors, both the task replicas and the fault tolerance building blocks (e.g., consensus) must be taken into account in the feasibility tests. This paper is devoted to the description of an approach for managing failures in hard real time distributed systems. Our approach is based on the use of a task replication tool named Hydra which makes tasks fault-tolerant off-line through the replication of parts to their code. The contribution of our work is not to provide new replication strategies but rather to provide replication strategies that are simultaneously suited to real time constraints, transparent to application designers and flexible (i.e., adaptable to application requirements and with low dependence with the underlying run-time support and hardware). Further details on Hydra can be found in (Chevochot and Puaut, 1999).
[Real time systems, Actuators, System testing, Dissolved gas analysis, distributed processing, Hydra, software fault tolerance, Postal services, task replication tool, Fault tolerant systems, timing constraints, real-time systems, Computer architecture, Abstracts, Hardware, software fault-tolerance, Timing, real-time distributed systems, errors]
XML documents production for an electronic platform of requests for proposals
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Addresses the problem of document production in the context of an electronic market application, namely a platform dealing with requests for proposals (RFP). Our project relies on XML as the leading technology, in accordance with a general trend in electronic commerce. The consequences of the RFP workflow on the document publishing requirements are motivated. Various aspects of the XML technology are discussed, with respect to both the present tools and developments which are expected soon. We describe our structuring approach and the two environments that we used as testbeds. The most challenging issues concern collaborative authoring, WYSIWYG software, the document life-cycle, and linking expressiveness.
[Proposals, Electronic commerce, authoring systems, electronic market application, electronic platform, Publishing, Production, groupware, document publishing requirements, Testing, hypermedia markup languages, electronic commerce, requests for proposals, Collaborative software, workflow, XML document production, WYSIWYG software, document life-cycle, structuring approach, XML, collaborative authoring, linking expressiveness, Collaborative work, Joining processes, Consumer electronics, desktop publishing]
A general framework to solve agreement problems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Agreement problems are among the most important problems designers of distributed systems have to cope with. A way to solve them is to first provide a solution to the Consensus problem and then to reduce each agreement problem to Consensus. This "run-time customizing" approach is particularly relevant when upper layer applications have to solve several distinct agreement problems. We investigate a "compile-time customizing" approach to automatically generate ad hoc agreement protocols. A general agreement framework, characterized by six "versatility" parameters, is defined. Appropriate instantiations of these parameters provide particular agreement protocols. This approach is particularly suited to generate efficient agreement protocols.
[Protocols, distributed processing, consensus problem, run-time customizing, compile-time customizing, software fault tolerance, agreement problem solving, Filters, Runtime, Voting, Fault tolerant systems, Broadcasting, distributed systems, protocols, ad hoc agreement protocols]
Fault injection based on a partial view of the global state of a distributed system
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
This paper describes the basis for and preliminary implementation of a new fault injector, called Loki, developed specifically for distributed systems. Loki addresses issues related to injecting correlated faults in distributed systems. In Loki, fault injection is performed based on a partial view of the global state of an application. In particular, facilities are provided to pass user-specified state information between nodes to provide a partial view of the global state in order to try to inject complex faults successfully. A post-runtime analysis, using an off-line clock synchronization and a bounding technique, is used to place events and injections on a single global time-line and determine whether the intended faults were properly injected. Finally, observations containing successful fault injections are used to estimate specified dependability measures. In addition to describing the details of our new approach, we present experimental results obtained from a preliminary implementation in order to illustrate Loki's ability to inject complex faults predictably.
[Performance evaluation, Visualization, Protocols, program testing, program verification, clock synchronization, software reliability, Loki, post-runtime analysis, bounding technique, Control systems, distributed software systems, Distributed computing, synchronisation, Writing, fault injection, Contracts, distributed programming]
Issues in the design of a reflective library for checkpointing C++ objects
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Object Persistence is an important feature of Object-oriented languages. The C++ language specification does not include or discuss any method of providing persistence for C++ objects. Several schemes have been developed for adding persistence to C++. Some of them require persistent objects to be allocated and treated differently than non-persistent objects, while some others require the programmer to provide vital parts of the persistence mechanism. It is desirable to make the persistence feature transparent, but the nature of C++ makes it difficult. This paper discusses in detail the various interesting language issues to be considered for adding persistence to C++ and how they lead to the design of the reflective object-checkpointing library, MemberAnalyzer.
[Checkpointing, checkpointing, Java, object-oriented programming, C++, reflective object-checkpointing library, Reflection, Electrical capacitance tomography, C++ language, Programming profession, Computer languages, Runtime, Program processors, Databases, MemberAnalyzer, persistence, Libraries, reflective library, persistent objects, C++ objects]
A distributed algorithm for deadlock detection under OR-request model
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
This paper presents a distributed algorithm for detecting deadlocks in the OR request model in distributed systems. The initiator of the algorithm builds a reduced local wait-for graph to determine a deadlock by employing two phases for probe propagation and receiving replies. The proposed algorithm exhibits faster deadlock detection and shorter blocked time of processes than the current algorithms. Furthermore, the deadlock resolution is simplified without any additional message transmission.
[message transmission, Computational modeling, local wait-for graph, probe propagation, Telecommunication computing, system recovery, OR-request model, software fault tolerance, deadlock detection, distributed algorithm, distributed algorithms, directed graphs, concurrency control, System recovery, distributed systems, Computer networks, receiving replies, Computer science education, Distributed algorithms, Probes, Gas detectors, Classification tree analysis]
Information dissemination in partitionable mobile ad hoc networks
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Ad-hoc wireless networks have no wired component, and may have unpredictable mobility pattern. Such networks can get partitioned and reconnected several times. One possible approach for information dissemination in such networks is to replicate information at multiple nodes acting as repositories, and employ quorum based strategies to update and query information. We propose three such strategies that also use local knowledge about the reachability of repositories to judiciously select quorums. The primary goal is high availability of information in the face of network partitioning. We also consider four policies to determine the appropriate time to perform updates. Experimental results indicate that a hybrid information management strategy and an absolute connectivity-based update trigger policy are most suited for partitionable ad-hoc networks.
[absolute connectivity-based update trigger policy, Electronic mail, Information management, high information availability, Intelligent networks, Network servers, partitionable mobile ad hoc networks, mobile computing, Network topology, Wireless networks, repositories, Spread spectrum communication, mobility pattern, reachability, Availability, client-server systems, reachability analysis, replicated databases, information dissemination, quorum based strategies, Information retrieval, Ad hoc networks, ad-hoc wireless networks, mobile communication, network partitioning, local knowledge a, hybrid information management strategy]
The "QoS query service" for improved quality-of-service decision making in CORBA
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We present the quality-of-service query service (QQS) a CORBA service which provides quality-of-service-related "decisions" for the configuration and parameterization of CORBA applications. Applications and services query QQS for those QoS-related decisions, and input data into QQS, so that evaluators have the necessary information to execute their decision-making algorithms. QQS consists of a database, a registration, naming and dramatic-loading facility for evaluators, and a registration and dynamic-loading facility for data collectors. The data model underlying the database is a QoS-oriented data model specially designed for QQS, such that a maximum amount of relevant queries and evaluators fit into QQS. This paper describes the motivation, design, implementation and use of the QoS query service.
[QoS query service, data model, Terminology, configuration, Quality of service, Engines, query processing, database, distributed databases, distributed object management, quality-of-service, Identity-based encryption, Decision making, parameterization, Linear programming, quality of service, Visual databases, Programming profession, data models, CORBA, dramatic-loading facility, QQS, Data visualization, decision making, naming, Data models]
Quality of service in business-to-business e-commerce applications
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Electronic commerce has attracted a great deal of attention recently. Among the different types of electronic commerce, business-to-business is the one most widespread in terms of turnover. An important aspect in business-to-business e-commerce scenarios is how to meet the response time and throughput requirements of applications in spite of execution taking place across corporate boundaries and, in the future, via the Internet instead of using leased lines. Given the unpredictable variations of available bandwidth in today's Internet, providing application quality-of-service guarantees for these requirements is a complex task. Due to the current trend to run multimedia applications over the Internet, we assume that the future Internet will not only support best-effort service but also guaranteed service, e.g. via network resource reservation. This allows us to also take advantage of this service for e-commerce applications. The problem remains non-trivial due to the wide variations among application scenarios. As a first step towards implementing a generic solution, this paper proposes a scheme which enables virtual business processes to meet response time and throughput requirements through network resource reservation. The paper describes the application scenarios we have in mind, the WISE (Workflow-based Internet SErvices) system where these ideas are being implemented, and the mechanisms involved in providing quality-of-service guarantees at the application level.
[response time requirements, business-to-business electronic commerce applications, multimedia applications, Quality of service, Companies, Throughput, Electronic commerce, WISE, Delay, throughput requirements, resource allocation, workflow-based Internet services, Web and internet services, Bandwidth, IP networks, turnover, multimedia communication, Business, electronic commerce, application scenarios, quality of service, guaranteed service quality, virtual business processes, application-level service quality guarantees, network resource reservation, Intserv networks, Internet, corporate boundaries, bandwidth variations]
Approaching a formal definition of fairness in electronic commerce
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The notion of fairness is a very general concept and can be used to coin terms in many different application areas. Recently the term fairness has appeared in the context of electronic commerce. Here, the term fair exchange refers to the problem that two parties want to swap some distinct items in a way which ensures that no participant can gain advantage over the other. Many protocols for fair exchange have been proposed but comparing or formally verifying them has remained rather difficult. The reason for this is that the notion of fairness they use is often different, and exact (i.e., formal) fairness definitions do not exist. We make a first attempt to approach a formal definition of fairness in electronic commerce. We do this by reviewing the established terminology regarding the notion of fairness in concurrency theory and adapting the formal apparatus to derive three precisely separable definitions of fairness in electronic commerce which we call strong, eventually strong and weak fairness.
[Protocols, Dictionaries, weak fairness, Terminology, fairness definition, precisely separable definitions, concurrency theory, eventually strong fairness, Electronic commerce, Application software, formal definition, strong fairness, Computer science, Concurrent computing, fair exchange, terminology, Computer networks, IP networks, protocols, formal apparatus, Business, electronic commerce]
Authorization methods for e-commerce applications
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
In the recent past, a lot of work has been done in establishing public key infrastructures (PKIs) for electronic commerce (e-commerce) applications Unfortunately, most of these PKIs can only be used to authenticate the participants of e-commerce applications; they can't be used to properly authorize the participants and to control access to system resources accordingly. Consequently, these PKIs address only half of the problem with regard to e-commerce applications and some complementary technologies are required to address the authorization problem as well. We elaborate on such technologies and corresponding authorization methods for e-commerce applications. In particular we address certificate based authorization, the use of attribute and SDSI/SPKI certificates, as well as the use of databases. We conclude with the insight that there is no single best authorization method, and that different e-commerce applications may require different authorization methods.
[Decision support systems, e-commerce applications, PKIs, databases, public key infrastructures, complementary technologies, authorization problem, certification, Authorization, public key cryptography, SDSI/SPKI certificates, message authentication, authorisation, authorization methods, system resources, Internet, electronic commerce, certificate based authorization]
The usability risk
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The article has two basic premises: why good user interface design matters more for e-commerce than for most online endeavors; and why-despite lots of corporate head-nodding and murmurs of assent-it's still very hard to achieve. The author is working on an e-commerce "wallet" project and struggling with two UI issues. One is minor, the type of wrangle designers and engineers bicker over daily. The second is both more serious and discouraging because it involves institutional change; in this case, the relinquishing of an historical artifact by a large Internet browser.
[Boats, human factors, user interfaces, UI issues, Electronic commerce, online endeavors, Design engineering, e-commerce wallet project, large Internet browser, Information security, Insurance, online front-ends, usability risk, historical artifact, User interfaces, interactive systems, institutional change, user interface design, e-commerce, Internet, Cryptography, Logic, Usability, electronic commerce]
Accountable anonymous access to services in mobile communication systems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We introduce a model that allows anonymous yet accountable access to services in mobile communication systems. This model is based on the introduction of a new business role, called the customer care agency and a ticket based mechanism for service access. We introduce the general idea of ticket based service access, and present a categorisation of ticket types and ticket acquisition models. We analyse the role of customer care agencies and emphasise their advantages.
[Wireless LAN, business role, Europe, business communication, ticket types, Mobile communication, Communication system security, Application software, mobile communication systems, Cellular networks, ticket based mechanism, Privacy, mobile computing, mobile communication, security of data, ticket acquisition models, Computer applications, Telephony, ticket based service access, service access, Protection, accountable anonymous access, business data processing, customer care agency]
A method for combining replication with cacheing
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Object replication and cacheing have been used individually in distributed systems for many years. There are benefits from being able to support both: replication for availability, and cacheing for performance. Although both involve handling multiple copies of objects, there are sufficient differences between the two to make the design of an integrated approach a challenging exercise. In this paper, we describe a design that provides the benefits of both replication and cacheing by allowing both types of protocols to co-exist within the same application. We do not propose a new replication or cacheing protocol, but rather a way in which existing implementations can be combined.
[Availability, Protocols, Identity-based encryption, replicated databases, object caching, replication protocols, object-oriented databases, Switches, memory protocols, object replication, cache storage, Electrical capacitance tomography, availability, implementation integration, caching protocols, performance, multiple object copies, distributed systems, distributed object management]
A software multilevel fault injection mechanism: case study evaluating the Virtual Interface Architecture
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The characteristics of failures occurring in networked computing systems are still poorly understood. As a consequence, this is a rich area for exploration, especially with the arrival of new network interface standards, such as the Virtual Interface Architecture (VIA) adopted by Microsoft, Intel and Compaq. The goal of VIA is to improve the performance of distributed applications by reducing the latency associated with the exchange of critical message between processes in Windows NT-based systems. In this paper, we propose the SMiFI (Software Multilevel Fault Injection) mechanism to evaluate the failure characteristics of networked systems, specifically VIA. The mechanism covers all software protocol layers of the host interface and corrupts both the messages and the computation engines that manipulate the messages.
[Computer interfaces, network interface standard, Computer aided software engineering, distributed processing, failure analysis, Engines, network interfaces, distributed applications performance, Virtual Interface Architecture, Computer architecture, computation engine corruption, message corruption, Computer networks, Hardware, critical message exchange latency, Kernel, software performance evaluation, Logic programming, program diagnostics, SMiFI, Microsoft Windows NT-based systems, networked computing system failures, software protocol layers, software multilevel fault injection mechanism, Application software, case study, VIA, message manipulation, Telecommunication network reliability]
Formal hazard analysis of hybrid systems in cTLA
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Hybrid systems like computer-controlled chemical plants are typical safety critical distributed systems. In present practice, the safety of hybrid systems is guaranteed by hazard analysis which is performed according to procedures (e.g., Ha/sub 2/Op) where experts discuss a series of informal argumentations. Each argumentation considers a specific required system property. Formal property proofs can increase the reliability. They, however have often to deal with very complex hybrid systems. Therefore, methods are needed which structure and decompose formal verification tasks into manageable subtasks. With respect to this, our approach achieves a relatively direct translation of informal argumentations into formal proofs. Since the informal argumentations mostly do not refer to the system as a whole but do only address specific parts and aspects, the formal proofs also can deal with partial, less complex system models. In result even very complex systems can be verified in well-manageable subtasks. The direct translation is supported by the characteristics of the specification technique applied. The temporal logic based technique cTLA supports the modular description of hybrid process systems. In particular one can model a system as a composition of behavior constraints. Properties which are implied by a subsystem of constraints also are properties of the system as a whole. Therefore a subsystem can correspond to the parts and aspects addressed by an informal argumentation. We outline cTLA and introduce the formalization of hazard analysis argumentations by means of a hybrid example system. Additionally, we sketch a framework of specification modules and theorems which supports the formal hazard analysis of hybrid systems.
[Petri nets, formal proofs, safety-critical software, distributed processing, temporal logic, cTLA, informal argumentation, Hazards, Electronic switching systems, Electrical capacitance tomography, State-space methods, formal specification, Equations, formal hazard analysis, formal verification, hybrid systems, Fault detection, safety critical distributed systems, Space exploration, Logic, Expert systems, hybrid process systems, computer-controlled chemical plants]
Optimistic recovery in multi-threaded distributed systems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The problem of recovering distributed systems from crash failures has been widely studied in the context of traditional non-threaded processes. However, extending those solutions to the multi-threaded scenario presents new problems. We identify and address these problems for optimistic logging protocols. There are two natural extension to optimistic logging protocols in the multi-threaded scenario. The first extension is process-centric, where the points of internal non-determinism caused by threads are logged. The second extension is thread-centric, where each thread is treated as a separate process. The process-centric approach suffers from false causality while the thread-centric approach suffers from high causality tracking overhead. By observing that the granularity of failures can be different from the granularity of rollbacks, we design a new balanced approach which incurs low causality tracking overhead and also eliminates false causality.
[Checkpointing, recovering distributed systems, Protocols, multi-threading, optimistic logging protocols, Computer crashes, Electronic switching systems, Yarn, system recovery, Read only memory, Concurrent computing, process-centric, crash failures, distributed systems, thread-centric, multi-threaded]
Design and validation of a distributed industrial control system's nodes
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The Fault Tolerant Systems Group (GSTF) of the Technical University of Valencia has developed the DICOS (Distributed Industrial COntrol System) system. The architecture of DICOS nodes and the error detection mechanisms used are presented. These mechanisms are based on the built-in capabilities of the microcontroller used, control flow checking with the aid of a second microcontroller and double execution of tasks. In order to validate the error detection mechanisms, a software fault injector (SOFI-SOftware Fault Injector) has been developed to obtain the error coverage and latency times. In this paper SOFI is presented, showing its primary features and results of different fault injection campaigns.
[program verification, Industrial control, Communication system control, Control systems, latency times, error detection, software architecture, Fault tolerant systems, Computer architecture, Hardware, double task execution, software fault injector, microcontrollers, DICOS, Microcontrollers, distributed control, distributed industrial control, control flow checking, Read-write memory, error coverage, software fault tolerance, microcontroller, Computer errors, industrial control, SOFI, Telecommunication network reliability]
Group multicast in distributed mobile systems with unreliable wireless network
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We propose a multicast protocol for a distributed system that includes mobile hosts. The protocol guarantees reliable delivery, i.e. delivery of every multicast and absence of duplicates. The sender of each multicast may select among three increasingly strong delivery ordering guarantees: FIFO, causal, total. We make loose assumptions on the underlying computing system: (i) we consider an unreliable wireless network i.e. one that provides only incomplete spatial coverage and such that messages could be lost even within cells (e.g., due to physical obstructions); (ii) movements are unpredictable, i.e. a user that leaves a cell may enter any other cell, perhaps after a potentially long disconnection. Our solution does not store any sensible state information at mobile support stations, thus movements do not trigger the transmission query message in the wired network and no notion of hand-off is used. Furthermore, movements at inopportune times can cause only occasional performance penalty but do not affect correctness. Weak assumptions on the underlying computing system, absence of state information at mobile support stations and loose mobility assumptions, contribute to improve the reliability of applications deployed over the proposed protocol.
[multicast protocol, Mobile communication, group multicast, mobile support stations, Proposals, Wireless communication, unreliable wireless network, Intelligent networks, mobile computing, Wireless networks, mobile hosts, multicast communication, Broadcasting, computer network reliability, state information, protocols, loose mobility assumptions, message passing, performance penalty, distributed mobile systems, Multicast protocols, Routing, Computer crashes, delivery ordering guarantees, incomplete spatial coverage, Telecommunication network reliability]
Resolving distributed deadlocks in the OR request model
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
In this work a new distributed deadlock resolution algorithm for the OR model is proposed. The algorithm verifies the correctness criteria: safety-false deadlocks are not resolved; and liveness-deadlocks are resolved in finite time.
[correctness criteria verification, distributed deadlock resolution algorithm, liveness, Proposals, false deadlocks, system recovery, software fault tolerance, System performance, concurrency control, safety, System recovery, Safety, OR request model, Detection algorithms]
Gateways to overcome incompatibilities of security mechanisms
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We describe the use of security gateways to overcome incompatibilities and configuration differences between acting parties participating in distributed applications. We present different variants and discuss their relative advantages in different scenarios considering possible attacks, performance and ease of use.
[Java, Data security, internetworking, distributed processing, distributed applications, Programming profession, acting parties, configuration management, ease of use, Databases, security of data, security gateways, performance, Sections, Operating systems, attacks, security mechanism incompatibilities, configuration differences, Computer networks, Cryptography, Protection, Computer security]
Fault tolerance in three-tier applications: focusing on the database tier
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Discusses the issues involved in providing an integrated fault tolerance solution to enterprise applications with a distributed three-tier architecture. We examine end-to-end fault tolerance requirements for interactions between the tiers and describe available solutions. We perform a detailed assessment of fault tolerance technologies that are available for the database tier, which poses most challenging reliability and high-availability problems. In particular, we discuss and compare the tradeoffs made in commercial database systems between different aspects of fault tolerance.
[Identity-based encryption, tradeoffs, Access protocols, reliability, enterprise applications, Electrical capacitance tomography, Paper technology, integrated fault tolerance solution, availability, software fault tolerance, distributed three-tier architecture, tier interactions, end-to-end fault tolerance requirements, Fault tolerance, Network servers, software architecture, database tier, Databases, distributed databases, User interfaces, commercial database systems, Logic, Web server, business data processing]
Safecharts for specifying and designing safety critical systems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
This paper proposes a novel variant of Statecharts, called Safecharts, especially for use in the specification and the design of safety critical systems. The objective is to provide a sharper focus on safety issues and a systematic approach to deal with them. This is achieved by making a clear separation between functional and safety requirements. A novel feature of Safecharts is the safety annotation, which proposes an explicit ordering of states according to risk level. Transitions are classified according to their risk nature and given a new priority scheme for their execution in the event of any non-determinism. A railway signalling system, a well-known case study, is used as an example to demonstrate some features and semantics of Safecharts.
[Safecharts, safety annotation, safety requirements, Railway safety, safety-critical software, system design, diagrams, Proposals, formal specification, Signal resolution, railways, Computer science, railway signalling system, Statecharts, risk level, Rail transportation, functional requirements, Signal analysis, Signal design, signalling, safety critical systems]
An adaptive checkpointing protocol to bound recovery time with message logging
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Numerous mathematical approaches have been proposed to determine the optimal checkpoint interval for minimizing total execution time of an application in the presence of failures. These solutions are often not applicable due to the lack of accurate data on the probability distribution of failures. Most current checkpoint libraries require application users to define a fixed time interval for checkpointing. The checkpoint interval usually implies the approximate maximum recovery time for single process applications. However, actual recovery time can be much smaller when message logging is used. Due to this faster recovery, checkpointing may be more frequent than needed and thus unnecessary execution overhead is introduced. In this paper, an adaptive checkpointing protocol is developed to accurately enforce the user-defined recovery time and to reduce excessive checkpoints. An adaptive protocol has been implemented and evaluated using a receiver-based message logging algorithm on wired and wireless mobile networks. The results show that the protocol precisely maintains the user-defined maximum recovery times for several traces with varying message exchange rates. The mechanism incurs lour overhead, avoids unnecessary checkpointing, and reduces failure free execution time.
[Checkpointing, adaptive checkpointing, failure free execution, Wireless application protocol, Probability distribution, Random processes, Application software, recovery time, system recovery, message logging, Exchange rates, Failure analysis, optimal checkpoint interval, Libraries, fault tolerant computing, Mathematical model, Contracts]
Performance evaluation of the circulating multisequencer and the consensus algorithms in a real-time distributed transactional system
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
In a real-time distributed transactional system, customers generate transactions, which should be scheduled to be executed on different servers. The transactions must be executed before their deadlines. To schedule these transactions the circulating multisequencer and the consensus algorithms have been considered to obtain a global view of the system. Mathematical models are developed to obtain the average stay time of a transaction within the system. The response time distribution is also computed. This allowed us to determine the minimum relative deadline, to affect to a generated transaction, to guarantee a given probability p that the transaction does not miss its deadline. This study shows that the circulating multisequencer algorithm presents better results.
[IEEE news, Real time systems, transaction processing, distributed transactional system, response time distribution, probability, mathematical models, distributed processing, performance evaluation, Mathematics, Electrical capacitance tomography, deadlines, Delay, Scheduling algorithm, circulating multisequencer, Network servers, real-time systems, scheduling, consensus algorithms, Mathematical model, Communication networks, Labeling, real-time system, software performance evaluation]
Improving level of service for mobile users using context-awareness
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The development of mobile computing combined, with the exponential growth of the Internet leads to new challenges in the development of large-scale information systems. As they move, users may experience dramatic variations in their environment, in terms of latency, network bandwidth, available services around etc. As a consequence, the level of service of the provided information may strongly depend on the context from which a user issues a query. To handle such variations while providing the best level of service to a user, information systems must be adaptive to change their behavior, preferably without the user intervention, depending on the current context of the user. We propose a general infrastructure based on contextual objects to design and develop adaptive distributed information systems in order to keep, even to improve, the level of the delivered service despite environment variations. As a first step to a complete implementation of our framework, we have implemented a location-aware Web service based on mobile-IP.
[Adaptive systems, contextual objects, Distributed information systems, latency, location-aware Web service, mobile-IP, large-scale information systems, Delay, context-awareness, Information systems, mobile computing, available services, Bandwidth, Large-scale systems, information systems, mobile users, level of service, Context-aware services, information resources, network bandwidth, information networks, Web services, transport protocols, Internet, Mobile computing, adaptive distributed information systems]
An efficient checkpointing algorithm for distributed systems implementing reliable communication channels
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
This paper presents a new checkpointing algorithm that guarantees the semantics of reliable communication channels despite the crash and recovery of processes. This algorithm requires O(n+m) communication messages, where n is the number of participating processes, and m is the number of "late" messages. The algorithm is nonblocking, requires minimal message logging, and has minimal stable storage requirements. This algorithm is also scalable, simple transparent to the user, and facilitates fast recovery. By introducing suitable delay in the checkpointing process, the parameter m can be made small. We also describe a variant of the algorithm that requires only O(n) messages, at a cost of O(n) additional storage for each process.
[Checkpointing, Protocols, Identity-based encryption, Costs, communication messages, distributed processing, reliable communication channels, semantics, Synchronization, system recovery, software fault tolerance, nonblocking algorithm, Computer science, message logging, system crash, delay, Message passing, delays, Communication channels, TCPIP, distributed systems, checkpointing algorithm, stable storage requirements, Clocks]
Implementing a semi-active replication strategy in CHORUS/ClassiX, a distributed real-time executive
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
The paper reports a practical implementation of a strategy to support semi-active replication of real-time software components (i.e. sets of tasks) running on the Chorus/ClassiX distributed operating system. The main property of the replication strategy developed in this paper is to solve the major difficulty of replica determinism. The semi-active replication scheme consists of a leader software component and identical follower replicas. Only the leader component sends out application messages as well as notifications indicating the order the messages have been consumed and produced. Dynamic non-deterministic scheduling of tasks within the different replicas may cause the follower tasks to lag in their execution regarding the leader ones.
[Real time systems, semi-active replication strategy, Navigation, Communication system control, Control systems, Dynamic scheduling, Application software, distributed real-time executive, software fault tolerance, software component, Fault tolerance, Patient monitoring, Operating systems, network operating systems, distributed operating system, real-time systems, CHORUS, scheduling, replica determinism, dynamic nondeterministic scheduling, application messages, Power generation, ClassiX, real-time software components]
An algorithm for fault-tolerant clock state &amp; rate synchronization
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We propose a fault-tolerant algorithm for synchronizing both state and rate of clocks in a distributed system. This algorithm is based on rounds, uses our fault-tolerant optimal precision (OF) convergence function as the means of synchronization, and maintains a collection of intervals to keep track of real-time, internal global time, and clock rates. The analysis shows that the interlocking between state and rate synchronization can be easily solved, and that oscillator stabilities together with the transmission delay uncertainties of packets predominate the internal synchronization. In addition, average case results gathered from simulation experiments with our SimUTC toolkit prove to be about one order of magnitude better than the worst case ones from the analysis of our state & rate algorithm.
[Uncertainty, distributed processing, distributed system, Delay, Convergence, internal global time, Fault tolerance, Analytical models, Fault tolerant systems, real-time system, oscillator stabilities, optimal precision convergence function, SimUTC toolkit, Stability analysis, clock state synchronization, Synchronization, clock rate synchronization, Oscillators, software fault tolerance, synchronisation, clocks, fault-tolerant algorithm, real-time systems, delays, clock rates, transmission delay uncertainties, Clocks]
View divergence control of replicated data using update delay estimation
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We propose a method to control the view divergence of replicated data when copies of sites in a replicated database are asynchronously updated. The view divergence of the replicated data is the difference in the lateness of the updates reflected in the data acquired by clients. Our method accesses multiple sites and provides a client with data that reflects all the updates received by the sites. We first define the probabilistic lateness of updates reflected in acquired data as read data freshness (RDF). The degrees of RDF of data acquired by clients is the range of the view divergence. Second, we propose a way to select sites in a replicated database by using the probability distribution of the update delays so that the data acquired by a client satisfies its required RDF. This way calculates the minimum number of sites in order to reduce the overhead of read transactions. Our method continues to adaptively and reliably provide data that meet the client's requirements in an environment where the delay of update propagation varies and applications' requirements change depending on situations. Finally, we evaluated the view divergence we can feasibly control using our method. The evaluation is done by means of simulations. The evaluation shows that our method can feasibly control the view divergence to about 1/4 that of a normal read transaction.
[transaction processing, read data freshness, simulation, Resource description framework, Electrical capacitance tomography, Distributed computing, update delays, Cost accounting, read transactions, Telegraphy, Telephony, probability distribution, Computer networks, replicated data, replicated databases, probability, Delay estimation, database update, view divergence control, probabilistic lateness, data integrity, Application software, replicated database, update delay estimation, Propagation delay]
Real-time fault-tolerant atomic broadcast
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We present algorithms for real-time fault-tolerant uniform atomic broadcast. We first design a distributed execution model for asynchronous systems with crash failure (called synchronized phase system (SPS)), then we give an algorithm for atomic broadcast in SPS. In an SPS, the processes try to run in synchronized sounds like in synchronous systems. SPSs can be implemented in asynchronous systems, but the liveness properties follow the properties of the knowledge of processes concerning the failures of other processes. In timed partially synchronous systems, we can give explicit feasibility conditions to solve real-time uniform atomic broadcast. At present, these algorithms are being implemented in the French project ATR.
[Real time systems, Computational modeling, Subspace constraints, distributed processing, distributed execution model, Computer crashes, crash failure, Electrical capacitance tomography, synchronized phase system, Delay, software fault tolerance, synchronisation, Fault tolerance, real-time uniform atomic broadcast, fault-tolerant uniform atomic broadcast, ATR project, liveness properties, real-time systems, timed partially synchronous systems, Broadcasting, synchronized sounds, asynchronous systems]
Highly available process support systems: implementing backup mechanisms
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Availability in process support systems (PSS) can be achieved by using standby mechanisms that allow a backup server to take over in case a primary server fails. These mechanisms, resembling the process pair approach used in operating systems, require the primary to send information about state changes to the backup on a regular basis. In PSS where all relevant state information is stored in a database, there are two principal strategies for synchronizing a primary-backup pair. One is to use the replication mechanisms provided by the DBMS. Another is to implement a message mechanism to exchange state information between servers above the database level. For both approaches, several variants exist that allow to trade run-time performance for failover time. This paper discusses the possible strategies and evaluates their performance based on an implementation within the OPERA process support kernel.
[standby mechanisms, Scientific computing, replication mechanism, process support systems, Programming, distributed processing, process pair approach, OPERA, backup server, Information systems, Condition monitoring, Runtime, database, Databases, Operating systems, Kernel, software performance evaluation, message mechanism, failover time, replicated databases, Interconnected systems, back-up procedures, operating systems, backup mechanisms, Application software, software fault tolerance, run-time performance, synchronization]
Scalable stability detection using logical hypercube
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
This paper proposes to use a logical hypercube structure for detecting message stability in distributed systems. In particular, a stability detection protocol that uses such a superimposed logical structure is presented, and its scalability is compared with other known stability detection protocols. The main benefits of the logical hypercube approach are scalability, fault-tolerance, and refraining from overloading a single node or link in the system. These benefits become evident both by an analytical comparison and by simulations. Another important feature of the logical hypercube approach is that the performance of the protocol is in general not sensitive to the topology of the underlying physical network.
[message stability, Scalability, fault-tolerance, simulation, distributed processing, hypercube networks, scalability, Communication standards, Analytical models, protocol, Network topology, Hypercubes, distributed systems, logical hypercube, protocols, scalable stability detection, stability detection protocol, software performance evaluation, Stability, Multicast protocols, network topology, Middleware, software fault tolerance, Computer science, performance, Telecommunication network reliability]
On diffusing updates in a Byzantine environment
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
We study how to efficiently diffuse updates to a large distributed system of data replicas, some of which may exhibit arbitrary (Byzantine) failures. We assume that strictly fewer than t replicas fail, and that each update is initially received by at least t correct replicas. The goal is to diffuse each update to all correct replicas while ensuring that correct replicas accept no updates generated spuriously by faulty replicas. To achieve reliable diffusion, each correct replica accepts an update only after receiving it from at least t others. We provide the first analysis of epidemic-style protocols for such environments. This analysis is fundamentally different from known analyses for the benign case due to our treatment of fully Byzantine failure-which, among other things, precludes the use of digital signatures for authenticating forwarded updates. We propose two epidemic-style diffusion algorithms and two measures that characterize the efficiency of diffusion algorithms in general. We characterize both of our algorithms according to these measures, and also prove lower bounds with regards to these measures that show that our algorithms are close to optimal.
[Byzantine environment, Protocols, replicated databases, large distributed system, Pipelines, distributed processing, faulty replicas, epidemic-style diffusion algorithms, Read only memory, Delay, software fault tolerance, update diffusion, lower bounds, data replicas, Computer science, Content addressable storage, arbitrary failures, protocols, Digital signatures, epidemic-style protocols]
Fault-tolerant replication management in large-scale distributed storage systems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
Failures of all forms happen: from losing single network packets to site-wide disasters. Since businesses rely heavily on their data, it is imperative that failures require minimal time and effort to repair and that the service interruption during the failure or repair period should be as short as possible. To this end, the ideal system should repair itself relying on humans only when absolutely necessary in the repair process. This paper describes one component of a self-healing storage system: the component that allows for automatic recovery of access to data when the power comes back on after a large-scale outage. Our failure recovery, protocol is part of a suite of modular protocols that make up the Palladio distributed storage system. This protocol guarantees that service will be repaired quickly and automatically when enough failures are repaired.
[self-repairing storage system, Access protocols, distributed processing, memory protocols, Educational institutions, system recovery, modular protocols, Read only memory, large-scale systems, protocol, Storms, automatic failure recovery, Fault detection, Fault tolerant systems, Earthquakes, Storage automation, Hardware, fault tolerant computing, Large-scale systems, Palladio distributed storage systems]
A component-based approach to reliability analysis of distributed systems
Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems
None
1999
This paper proposes a reliability analysis technique for distributed software systems. The technique is based on scenarios that are modeled as sequence diagrams. Using scenarios, we construct component-dependency graphs (CDG). CDGs have been introduced for reliability analysis of component-based systems. They are extended to serve the complex nature of distributed systems by applying nesting and hierarchy. CDGs include component and link reliabilities, which are treated as first class elements of the model. Based on CDGs, we present an algorithm to analyze the sensitivity of system reliability to reliabilities of its components, subsystems, and links. The proposed analysis technique is useful in identifying critical components and critical component links. An example based on medical informatics standard is presented to illustrate our methodology.
[Algorithm design and analysis, Protocols, sequence diagrams, component reliability, software reliability, graph theory, Biomedical informatics, component-dependency graphs, scenario based method, distributed software systems, Middleware, Computer science, Databases, medical informatics, reliability analysis, User interfaces, Software systems, Reliability, medical computing, Assembly, distributed programming, link reliability]
Deterministic scheduling for transactional multithreaded replicas
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
One way to implement a fault-tolerant service is by replicating it at sites that fail independently. One of the replication techniques is active replication where each request is executed by all the replicas. Thus, the effects of failures can be completely masked, resulting in an increase of service availability. In order to preserve consistency among replicas, replicas must exhibit a deterministic behavior, which has traditionally been achieved by restricting replicas to being single-threaded. However, this approach cannot be applied in some setups like transactional systems, where it is not admissible to process transactions sequentially. The authors present a deterministic scheduling algorithm for multithreaded replicas in a transactional framework. To ensure replica determinism, requests to replicated servers are submitted by means of reliable and totally ordered multicast. Internally, a deterministic scheduler ensures that all threads are scheduled in the same way at all replicas which guarantees replica consistency.
[transaction processing, CADCAM, replicated servers, scheduling algorithm, deterministic behavior, replica consistency, Yarn, processor scheduling, Network servers, Fault tolerance, transactional systems, totally ordered multicast, transactional multithreaded replicas, replica determinism, Contracts, Availability, replication techniques, multi-threading, Computer aided manufacturing, deterministic scheduling, deterministic scheduler, service availability, active replication, deterministic algorithms, fault-tolerant service replication, Scheduling algorithm, Multicast algorithms, Councils, fault tolerant computing, transactional framework]
Dynamic node management and measure estimation in a state-driven fault injector
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
false
[Algorithm design and analysis, Runtime, USA Councils, Computer crashes, Performance analysis, Synchronization, State estimation, Distributed computing, Contracts, Clocks]
Improvement of the QoS via an adaptive and dynamic distribution of applications in a mobile environment
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Mobile computing is a domain in great expansion. Wireless networks and Portable Information Appliances (PIAs) are developing very rapidly. More and more mobile users would like to perform their multimedia applications with the same facility as on their desktop station. Use of such applications in a mobile environment raises new challenges. These applications are interactive and extremely costly in system and network resources, whereas PIA resources are poor and wireless networks offer a very variable quality of connection. The authors propose an adaptive and dynamic distribution of applications on the local environment to overcome the poorness of available resources on PIAs, and to reduce and regulate variability effects.
[Wireless LAN, radio networks, Portable computers, Portable Information Appliances, multimedia applications, Optical computing, multimedia systems, Batteries, PIA resources, Home appliances, dynamic distribution, mobile computing, Wireless networks, QoS, Computer networks, mobile users, GSM, mobile environment, wireless networks, quality of service, adaptive systems, variability effects, Satellites, local environment, network resources, desktop station, Mobile computing, portable computers]
Modeling fault-tolerant mobile agent execution as a sequence of agreement problems
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Fault tolerance is fundamental to the further development of mobile agent applications. In the context of mobile agents, fault tolerance prevents a partial or complete loss of the agent, i.e. ensures that the agent arrives at its destination. Simple approaches such as checkpointing are prone to blocking. Replication can in principle improve solutions based on checkpointing. However existing solutions in this context either assume a perfect failure detection mechanism (which is not realistic in an environment such as the Internet), or rely on complex solutions based on leader election and distributed transactions, where only a subset of solutions prevents blocking. The paper proposes a novel approach to fault tolerant mobile agent execution, which is based on modeling agent execution as a sequence of agreement problems. Each agreement problem is one instance of the well understood consensus problem. Our solution does not require a perfect failure detection mechanism, while preventing blocking and ensuring that the agent is executed exactly once.
[Checkpointing, agreement problems, checkpointing, Uncertainty, fault tolerance, Laboratories, Nominations and elections, failure detection mechanism, Mechanical factors, consensus problem, system recovery, mobile agent applications, Delay, Fault tolerance, mobile computing, Operating systems, Mobile agents, Fault tolerant systems, fault-tolerant mobile agent execution modeling, fault tolerant computing, fault tolerant mobile agent execution]
Performance analysis of the CORBA event service using stochastic reward nets
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
The event service is the earliest CORBA solution to the message queue model of communication in distributed systems. Typical implementations, however, suffer from the lack of event delivery guarantees. The loss of messages is aggravated by the presence of burstiness in the input to the event service, and occurrences of isolated bursts of traffic could also have serious effects. In this paper, we develop stochastic reward net (SRN) models that can aid in the study and configuration of the event service to conform to design specifications. To capture burstiness in the input, a Markov-modulated Poisson process (MMPP) is used as the input source. Erlang distributed event consumption times are used in the models to accommodate more general distributions and a wider range of variances. The models also take into consideration the FIFO discard policy adopted in many event service implementations. The SRN models are solved using the tool SPNP (Stochastic Petri Net Package). The applicability of the models to the CORBA notification service is also briefly discussed.
[message loss, Portable computers, Petri nets, Stochastic processes, Quality of service, message queue model, CORBA notification service, Distributed computing, isolated traffic bursts, Fault tolerance, Computer architecture, Performance analysis, Markov-modulated Poisson process, distributed object management, software performance evaluation, Availability, event delivery guarantees, stochastic reward nets, queueing theory, FIFO discard policy, Object oriented modeling, input burstiness, Stochastic Petri Net Package, CORBA event service, distributed systems communication, variance, Erlang distributed event consumption times, Markov processes, Computer industry, SPNP, design specifications, telecommunication traffic, performance analysis]
Optimistic Virtual Synchrony
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
We present Optimistic Virtual Synchrony (OVS), a new form of group communication which provides the same capabilities as Virtual Synchrony with better performance. It does so by allowing applications to send messages during periods in which services implementing Virtual Synchrony block. OVS also allows applications to determine the policy as to when messages sent optimistically should be delivered and when they should be discarded. Thus, OVS gives applications fine grain control over the specific semantics they require, and does not impose costs for enforcing any semantics that they do not require. At the same time, OVS provides a single easy-to-use interface for all applications.
[Costs, message passing, Communication system control, Drives, Optimistic Virtual Synchrony, Subcontracting, Maintenance, Rivers, semantics, Application software, OVS, group communication, Milling machines, synchronisation, Computer science, fine grain control, optimisation, message sending, easy-to-use interface, fault tolerant distributed applications, groupware, multicast communication, Power system reliability, multicast groups]
Proxy-based recovery for applications on wireless hand-held devices
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
The low communication bandwidth, slow processor and limited memory of hand-held devices make it undesirable for them to store their own checkpoints or send process state information over a wireless network. The paper describes an approach to failure recovery for three-tier client and server application environments where the client applications execute on wireless handheld devices. The key idea is to have the middle-tier proxy transparently monitor the client's interaction with the back-end server and continuously maintain a copy of the client's state based on messages exchanged between the client and the server. The proxy also sustains the client's connection to the back-end server when a client unexpectedly disconnects. The client does not participate in checkpointing nor message logging, thereby saving power, processor cycles and bandwidth. The proxy is scalable and enhances backend server performance. Experimental results are provided for recovery time and runtime overhead.
[message exchange, runtime overhead, failure recovery, Random access memory, process state information, back-end server, proxy based recovery, Wireless communication, Network servers, Runtime, mobile computing, wireless hand-held device applications, Wireless networks, backend server performance, Bandwidth, notebook computers, wireless network, Contracts, client-server systems, communication bandwidth, bandwidth, middle-tier proxy, checkpoints, Computer crashes, Application software, recovery time, Handheld computers, processor cycles, radiocommunication, client interaction, fault tolerant computing, client state, three-tier client/server application environments]
On the use of model checking techniques for dependability evaluation
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Over the last two decades, many techniques have been developed to specify and evaluate Markovian dependability models. Most often, these Markovian models are automatically derived from stochastic Petri nets, stochastic process algebras or stochastic activity networks. However, whereas the model specification has become very comfortable, the specification of the dependability measures of interest most often has remained fairly cumbersome. In this paper, we show that our recently introduced logic CSL (continuous stochastic logic) provides ample means to specify state- as well as path-based dependability measures in a compact and flexible way. Moreover, due to the formal syntax and semantics of CSL, we can exploit the structure of CSL-specified dependability measures in the dependability evaluation process. Typically, the underlying Markov chains that need to be evaluated can be reduced considerably in size by this structure exploitation.
[Linear systems, Petri nets, Stochastic processes, structure exploitation, continuous stochastic logic, Markov chains, stochastic activity networks, Steady-state, reliability theory, semantics, formal syntax, Algebra, formal verification, dependability measures specification, Logic, algebraic specification, state-based dependability measures, fault tolerance, model checking techniques, model specification, Markovian dependability models, Power system modeling, path-based dependability measures, Equations, stochastic Petri nets, Computer science, stochastic process algebras, process algebra, Markov processes, Particle measurements, dependability evaluation]
High availability of the memory hierarchy in a cluster
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
A single-level store (SLS) integrating a shared virtual memory and a parallel file system with file mapping as its interface is attractive for the execution of high-performance applications in a cluster. However, the probability of a node reboot or failure is quite high. In this paper, we present the design of a highly available SLS system. Our approach combines checkpointing in memory and permanent checkpointing on disk in a cluster using all cluster memory and disk resources. Preliminary performance results show the applicability of the proposed approach for parallel applications with huge input/output requirements.
[Checkpointing, workstation clusters, parallel memories, single-level store, permanent on-disk checkpointing, Bit error rate, cluster disk resources, input/output requirements, cluster computing, system recovery, Fault tolerance, File systems, Microprocessors, memory hierarchy availability, Bandwidth, memory checkpointing, shared memory systems, cluster memory resources, file mapping interface, Availability, parallel file system, Laser sintering, virtual storage, performance evaluation, parallel applications, highly available system, Support vector machines, performance, Memory management, shared virtual memory, node failure, high-performance applications, node reboot]
Using multicast communication to reduce deadlock in replicated databases
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Obtaining good performance from a distributed replicated database that allows update transactions to originate at any site while ensuring one-copy serializability is a challenge. A popular analysis of deadlock probabilities in replicated databases shows that the deadlock rate for the system is high and increases as the third power of the number of replicas. We show how a replica management protocol that uses atomic broadcast for replica update reduces the occurrence of deadlocks and the dependency on the number of replicas. The analysis is confirmed by simulation experiments.
[deadlock reduction, replicated databases, probability, Multicast communication, Predictive models, replica update, Multicast protocols, Transaction databases, deadlock probabilities, replica management protocol, Distributed computing, broadcasting, one-copy serializability, atomic broadcast, Computer science, Analytical models, update transactions, Distributed databases, concurrency control, multicast communication, distributed replicated database, System recovery, Broadcasting]
Issues insufficiently resolved in Century 20 in the fault-tolerant distributed computing field
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
As the 21st Century has just opened up, it is a fitting time to reflect on the evolution of the fault-tolerant distributed computing technology that occurred in the last century. The author's view of that evolution is sketched in this paper, with emphasis on the major issues that were insufficiently resolved in the 20th Century. Such issues are naturally among what the author believes to be the prime subjects that need to be addressed in this decade by the research community. A substantial part of this paper deals with the issues that need to be resolved to advance the real-time fault-tolerant distributed computing branch into a mature practicing field.
[Availability, real-time computing, distributed processing, Paper technology, Application software, Distributed computing, Research and development, Fault tolerance, reviews, real-time systems, Computer applications, insufficiently resolved issues, Computer industry, Hardware, fault tolerant computing, System software, technology evolution, fault-tolerant distributed computing]
Reliable broadcast in the crash-recovery model
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
The paper addresses the problem of broadcasting messages in a reliable manner within a practical asynchronous system where processes and channels may crash and recover. In this crash-recovery model, we present meaningful specifications of reliable broadcast and we describe algorithms that implement those specifications. Our approach is modular and incremental. It is modular in the sense that we give the properties of reliable broadcast separately, and then consider their composition. It is incremental in the sense that we show how to automatically transform any reliable broadcast algorithm that implements a given specification into one that implements a stronger specification. In particular we show how to reuse, in a crash-recovery model, reliable broadcast algorithms that were initially designed in a simpler crash-stop model.
[Algorithm design and analysis, Communication systems, practical asynchronous system, software reliability, reliable broadcast algorithm reuse, Broadcast technology, Reliability theory, Computer crashes, meaningful specifications, Distributed computing, system recovery, formal specification, message broadcasting, broadcasting, Intersymbol interference, distributed algorithms, crash-recovery model, Broadcasting, reliable broadcast, crash-stop model, Distributed algorithms]
Optimal implementation of the weakest failure detector for solving consensus
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
The concept of unreliable failure detector was introduced by T.D. Chandra and S. Toueg (1996) as a mechanism that provides information about process failures. Depending on the properties which the failure detectors guarantee, they proposed a taxonomy of failure detectors. It has been shown that one of the classes of this taxonomy, namely Eventually Strong (/spl nabla/S), is the weakest class allowing a solution of the Consensus problem. The authors present a new algorithm implementing /spl nabla/S. Our algorithm guarantees that eventually all the correct processes agree on a common correct process. This property trivially allows us to provide the accuracy and completeness properties required by /spl nabla/S. We show then that our algorithm is better than any other proposed implementation of /spl nabla/S in terms of the number of messages and the total amount of information periodically sent. In particular, previous algorithms require periodic exchange of at least a quadratic amount of information, while ours only requires O(n log n) (where n is the number of processes). However, we also propose a new measure to evaluate the efficiency of this kind of algorithm, the eventual monitoring degree, which does not rely on a periodic behavior and expresses the degree of processing required by the algorithms better. We show that the runs of our algorithm have optimal eventual monitoring degree.
[Algorithm design and analysis, common correct process, Taxonomy, weakest failure detector, unreliable failure detector, Condition monitoring, Fault tolerance, periodic exchange, Detectors, Contracts, Eventually Strong, completeness properties, message passing, Computer aided manufacturing, Computer crashes, Fault detection, Councils, weakest class, distributed algorithms, optimal implementation, Consensus problem, periodic behavior, fault tolerant computing, optimal eventual monitoring degree, process failures, computational complexity]
Abstractions for devising Byzantine-resilient state machine replication
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
State machine replication is a common approach for making a distributed service highly available and resilient to failures, by replicating it on different processes. It is well known, however that the difficulty of ensuring the safety and liveness of a replicated service increases significantly when no synchrony assumptions are made, and when processes can exhibit Byzantine behaviors. The contribution of the work is to break the complexity of devising a Byzantine-resilient state machine replication protocol, by decomposing it into key modular abstractions. In addition to being modular, the protocol we propose always preserves safety in the presence of less than one third of Byzantine processes, independently of any synchrony assumptions. As for the liveness of our protocol, it relies on a Byzantine failure detector that encapsulates a sufficient amount of synchrony.
[Heart, Byzantine-resilient state machine replication, distributed processing, liveness, Byzantine-resilient state machine replication protocol, finite state machines, Fault tolerance, protocol, Byzantine failure detector, Detectors, Safety, protocols, Byzantine behaviors, Context-aware services, Byzantine processes, Change detection algorithms, key modular abstractions, distributed service, Multicast protocols, Computer crashes, abstractions, Multicast algorithms, Fault detection, synchrony assumptions, fault tolerant computing, replicated service]
Database replication techniques: a three parameter classification
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Data replication is an increasingly important topic as databases are more and more deployed over clusters of workstations. One of the challenges in database replication is to introduce replication without severely affecting performance. Because of this difficulty, current database products use lazy replication, which is very efficient but can compromise consistency. As an alternative, eager replication guarantees consistency but most existing protocols have a prohibitive cost. In order to clarify the current state of the art and open up new avenues for research, this paper analyses existing eager techniques using three key parameters (server architecture, server interaction and transaction termination). In our analysis, we distinguish eight classes of eager replication protocols and, for each category, discuss its requirements, capabilities and cost. The contribution lies in showing when eager replication is feasible and in spelling out the different aspects a database replication protocol must account for.
[Availability, workstation clusters, Protocols, Costs, replicated databases, cost, Laboratories, database replication techniques, 3-parameter classification, server interaction, Transaction databases, server architecture, transaction termination, consistency, eager replication protocols, Information systems, performance, Distributed databases, file servers, Software systems, Space exploration, Workstations, lazy replication protocols]
An investigation of membership and clique avoidance in TTP/C
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Avoiding the partitioning of a cluster into cliques that are not able to communicate with each other is an important issue in the time-triggered communication protocol TTP/C. This is achieved by a mechanism called clique avoidance. The clique avoidance algorithm always selects one partition (clique) to win and causes all nodes of other partitions to shut down. In this paper, we investigate the properties of this algorithm by analyzing its performance, elaborating the properties and showing how the clique avoidance algorithm interacts with the implicit acknowledgement algorithm of TTP/C.
[Algorithm design and analysis, Real time systems, TTP/C protocol, Protocols, TDMA, Jitter, distributed processing, Scheduling, Partitioning algorithms, membership, Time division multiple access, time-triggered communication protocol, clique avoidance algorithm, time-division multiple access scheme, transport protocols, Fault tolerant systems, time division multiple access, algorithm performance, Clustering algorithms, Frequency synchronization, implicit acknowledgement algorithm, software performance evaluation, cluster partitioning]
Semantically reliable multicast protocols
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Reliable multicast protocols can strongly simplify the design of distributed applications. However it is hard to sustain a high multicast throughput when groups are large and heterogeneous. In an attempt to overcome this limitation, previous work has focused on weakening reliability properties. The authors introduce a novel reliability model that exploits semantic knowledge to decide in which specific conditions messages can be purged without compromising application correctness. This model is based on the concept of message obsolescence: a message becomes obsolete when its content or purpose is overwritten by a subsequent message. We show that message obsolescence can be expressed in a generic way and can be used to configure the system to achieve higher multicast throughput.
[message passing, Stability, semantically reliable multicast protocols, Pipelines, reliability, reliability model, Multicast protocols, Throughput, Control systems, reliability properties, message obsolescence, communication complexity, distributed applications, Degradation, semantic knowledge, Tiles, Bandwidth, multicast communication, multicast throughput, Traffic control, Performance analysis, protocols, application correctness, high multicast throughput]
Pronto: a fast failover protocol for off-the-shelf commercial databases
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Enterprise applications typically store their state in databases. If a database fails, the application is unavailable while the database recovers. Database recovery is time consuming because it involves replaying the persistent transaction log. To isolate end users from database failures, we introduce Pronto, a protocol to orchestrate the transaction processing by multiple, standard databases so that they collectively implement the illusion of a single, highly available database. The key challenge in implementing this illusion is to enable fast failover from one database to another so that database failures do not interrupt the transaction processing. We solve this problem with a novel replication protocol that handles non-determinism without relying on perfect failure detection.
[transaction processing, Protocols, Pronto, Laboratories, Mission critical systems, database management systems, system recovery, non-determinism, perfect failure detection, highly available database, Broadcasting, Database systems, protocols, Web server, Business, fast failover protocol, Availability, replication protocol, multiple standard databases, enterprise applications, off-the-shelf commercial databases, Transaction databases, software fault tolerance, database failure, end users, persistent transaction log, database recovery, Internet]
Performance of mobile, single-object, replication protocols
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Discusses the implementation and performance of bounded voting, which is a new object replication protocol designed for use in mobile and weakly-connected environments. We show that the protocol eliminates several restrictions of previous work, such as the need for (1) strong or complete connectivity, (2) complete knowledge of system membership, and (3) low update rates. The protocol implements an asynchronous, weighted-voting scheme via epidemic information flow, and commits updates in an entirely decentralized fashion. A proxy mechanism is used to enable transparent handling of planned disconnections. We use a detailed simulation study to characterize the performance of bounded voting under a variety of loads and environments, and to compare it to another decentralized epidemic protocol. We further investigate the performance impact of the proxy mechanism.
[Protocols, Portable computers, proxy mechanism, planned disconnections, simulation, weakly-connected environments, bounded voting, decentralized update commitment, mobile single-object replication protocols, decentralized epidemic protocol, mobile computing, Voting, connectivity, Bandwidth, Hardware, protocols, transparent handling, Personal digital assistants, software performance evaluation, Availability, replicated databases, object-oriented databases, asynchronous weighted-voting scheme, loads, Topology, Computer science, performance, update rates, epidemic information flow, Mobile computing, system membership]
Continuous clock synchronization in wireless real-time applications
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Continuous clock synchronization avoids unpredictable instantaneous corrections of clock values. This is usually achieved by spreading the clock correction over the synchronization interval. In the context of wireless real time applications, a protocol achieving continuous clock synchronization must tolerate message losses and should have a low overhead in terms of the number of messages. The paper presents a clock synchronization protocol for continuous clock synchronization in wireless real time applications. It extends the IEEE 802.11 standard for wireless local area networks. It provides continuous clock synchronization, improves the precision by exploiting the tightness of the communication medium, and tolerates message losses. Continuous clock synchronization is achieved with an advanced algorithm adjusting the clock rates. We present the design of the protocol, its mathematical analysis, and measurements of a driver level implementation of the protocol on Windows NT.
[Real time systems, Windows NT, Wireless LAN, message loss tolerance, driver level implementation, Wireless application protocol, wireless local area networks, message losses, synchronization interval, Wireless communication, Mathematical analysis, Bandwidth, IEEE 802 11 standard, IEEE standards, protocols, unpredictable instantaneous corrections, overhead, continuous clock synchronization, Time measurement, mathematical analysis, Synchronization, Information technology, synchronisation, clocks, advanced algorithm, real-time systems, clock rates, fault tolerant computing, wireless LAN, clock values, Clocks, wireless real time applications]
An evolutionary algorithm for identifying faults in t-diagnosable systems
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
The paper describes a novel approach to the problem of system-level fault diagnosis using genetic algorithms. Consider a system composed of n independent units, each of which tests a subset of the others. It is assumed that at most t of these units are permanently faulty. Such a system is said to be t-diagnosable if, given any complete collection of test results, the set of faulty units can be uniquely identified. Genetic algorithms have recently received much attention as a class of robust stochastic search algorithms for various optimization problems. An efficient method based on evolutionary algorithms is developed to solve the diagnosis problem. The representation of the search space used is in the form of a binary vector of length n. Each bit indicates the status (faulty or fault-free) of its corresponding unit. Genetic operators are adapted to the context of system-level diagnosis. The genetic algorithm was implemented and tested on random test graphs. The simulation results demonstrate the efficiency of the proposed diagnosis algorithm.
[System testing, fault diagnosis, graph theory, Stochastic processes, Evolutionary computation, evolutionary algorithms, independent units, Mathematics, Genetic algorithms, fault identification, Fault diagnosis, Robustness, Large-scale systems, search space, diagnosis problem, search problems, faulty units, robust stochastic search algorithms, multiprocessing systems, test results, Computational modeling, binary vector, random test graphs, genetic algorithms, genetic operators, system-level diagnosis, evolutionary algorithm, Computer science, diagnosis algorithm, fault tolerant computing, t-diagnosable systems, system-level fault diagnosis]
A pragmatic implementation of e-transactions
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
Three-tier applications have nice properties, which make them scalable and manageable: clients are thin and servers are stateless. However, it is challenging to implement or even define end-to-end reliability for such applications. Furthermore, it is especially hard to make these applications reliable without violating their nice properties. In previous work, we identified e-transactions as a desirable and practical end-to-end reliability guarantee for three-tier applications (S. Frolund and R. Guerraoui, 1999). Essentially, an e-transaction guarantees that the server-side transactional side-effect happens exactly once, and that the client receives the result of the server-side computation. Thus, e-transactions mask server and database failures relative to the client. We present a pragmatic implementation of e-transactions that maintains the nice properties of three-tier applications in the special, but very common case of a single back-end database.
[transaction processing, client-server systems, Protocols, network computers, clients, pragmatic implementation, Laboratories, single back-end database, Computer crashes, server-side computation, Transaction databases, Technology management, three-tier applications, servers, database failures, server-side transactional side-effect, fault tolerant computing, Internet, end-to-end reliability guarantee, Logic, Web server, e-transactions, end-to-end reliability]
Detection of livelocks in communication protocols by means of a polygon time structure
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
As has been shown, the polygon time structure overcomes the main limitations of the interval time structure, and allows one to verify communication protocols, in which the explicit concurrency of both competing and supporting events is considered. The authors apply the polygon time structure to the verification of dynamic properties (like livelocks) of a simple protocol. As a result, they show that an application of the previously used interval time structure may lead to the evaluation of a protocol as incorrect, while it is livelock free.
[Real time systems, Protocols, Computational modeling, communication protocols, State-space methods, Application software, communication complexity, explicit concurrency, Concurrent computing, formal verification, interval time structure, Automata, Computer errors, livelock detection, Computer networks, polygon time structure, protocols, livelock free protocol, supporting events, simple protocol, Testing, dynamic property verification]
Consistent detection of global predicates under a weak fault assumption
Proceedings 19th IEEE Symposium on Reliable Distributed Systems SRDS-2000
None
2000
We study the problem of detecting general global predicates in distributed systems where all application processes and at most t<m monitor processes may be subject to crash faults, where m is the total number of monitor processes in the system. We introduce two new observation modalities called negotiably and discernibly (which correspond to possibly and definitely in fault-free systems) and present detection algorithms for them which work under increasingly weak fault assumptions.
[Uncertainty, consistent detection, detection algorithms, distributed processing, Computer crashes, application processes, monitor processes, fault-free systems, crash faults, Fault detection, weak fault assumption, general global predicates, Detectors, system monitoring, distributed systems, fault tolerant computing, observation modalities, Detection algorithms, Monitoring, global predicate detection]
Using the timely computing base for dependable QoS adaptation
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
In open and heterogeneous environments, where an unpredictable number of applications compete for a limited amount of resources, executions can be affected by also unpredictable delays, which may not even be bounded. Since many of these applications have timeliness requirements, they can only be implemented if they are able to adapt to the existing conditions. We present a novel approach, called dependable QoS adaptation, which can only be achieved if the environment is accurately and reliably observed. Dependable QoS adaptation is based on the timely computing base (TCB) model. The TCB model is a partial quality of service synchrony model that adequately characterizes environments of uncertain synchrony and allows, at the same time, the specification and verification of timeliness requirements. We introduce the coverage stability property and show that adaptive applications can use the TCB to dependably adapt and enjoy this property. We describe the characteristics and the interface of a QoS coverage service and discuss its implementation details.
[Real time systems, Stability, Navigation, Multimedia systems, Process control, specification, timely computing base, quality of service, dependable QoS adaptation, formal specification, Delay, synchrony model, synchronisation, formal verification, Web and internet services, fault tolerant computing, Timing, Large-scale systems, Informatics, verification]
Efficient update diffusion in byzantine environments
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
We present a protocol for diffusion of updates among replicas in a distributed system where up to b replicas may suffer Byzantine failures. Our algorithm ensures that no correct replica accepts spurious updates introduced by faulty replicas, by requiring that a replica accepts an update only after receiving it from at least b+1 distinct replicas (or directly from the update source). Our algorithm diffuses updates more efficiently than previous such algorithms and, by exploiting additional information available in some practical settings, sometimes more efficiently than known lower bounds predict.
[Protocols, trees (mathematics), update source, faulty replicas, distinct replicas, Communication system security, FT algorithms, Delay, update diffusion, Computer science, Upper bound, protocol, security of data, spurious updates, distributed algorithms, Information security, Broadcasting, Prediction algorithms, fault tolerant computing, Byzantine environments, protocols]
Reliable real-time cooperation of mobile autonomous systems
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Autonomous systems are expected to provide increasingly complex and safety-critical services that will, sooner or later, require the cooperation of several autonomous systems for their fulfillment. In particular, coordinating the access to shared physical and information technological resources will become a general problem. Scheduling these resources is subject to strong real-time and reliability requirements. In this paper, we present an architecture that allows autonomous mobile systems to schedule shared resources in real-time using their own wireless distributed infrastructure. In our architecture, there is a clear separation between the application-specific scheduling part that is modeled as a function of the global state and the communication part that is used to provide the global state. By isolating the more error-prone communication part within a communication hardcore, the reliability of the overall system is increased and the locally executed scheduling function can be designed with primary focus on the application-specific real-time requirements.
[Real time systems, autonomous systems, Roads, parallel architectures, Quality of service, Mobile communication, Control systems, Electrical capacitance tomography, application-specific scheduling, Information technology, processor scheduling, mobile computing, Processor scheduling, resource allocation, real-time, real-time systems, Computer architecture, autonomous mobile systems, safety-critical services, shared resources, distributed infrastructure, Safety, cooperation]
Research in high-confidence distributed information systems
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
A high-confidence system is one in which the designers, implementers, and users have a high degree of assurance that the system will not fail or misbehave due to errors in the system, faults in the environment, or hostile attempts to compromise the system. Consequences of such system behavior are well understood and are predictable under an operational context envisioned by its creators. High-confidence systems (HCS) are highly secure and robust: they can withstand various threats, malicious attacks, and hardware/software component failures. Correctness, predictability, reliability, availability, security, and survivability are the key properties that constitute the basis of high confidence. We expect to have confidence in networked embedded software applications in our homes, workplaces, and vehicles. We need to understand well the implications of software design and structuring for vulnerability in these systems to failures, extreme environmental events, and security attacks. This will require both fundamental and empirical research to establish, test, validate, and improve secure networked and software-enabled system construction principles.
[Software testing, malicious attacks, software design, Distributed information systems, hardware/software component failures, distributed processing, operational context, Embedded software, Vehicles, Software design, security, Employment, embedded systems, Robustness, Hardware, Availability, security attacks, software-enabled system construction principles, Application software, software fault tolerance, high-confidence distributed information systems, hostile attempts, systems analysis, extreme environmental events, HCS, networked embedded software applications, survivability, system behavior]
Primary-backup replication: from a time-free protocol to a time-based implementation
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Fault-tolerant control systems can be built by replicating critical components. However replication raises the issue of inconsistency. Multiple protocols for ensuring consistency have been described in the literature. PADRE (Protocol for Asymmetric Duplex REdundancy) is such a protocol, and an interesting case study of a complex and sensitive problem: the management of replicated traffic controllers in a railway system. However, the low level at which the protocol has been developed embodies system details, namely timeliness assumptions, that make it difficult to understand and may narrow its applicability. We argue that, when designing a protocol, it is preferable to consider first a general solution that does not include any timeliness assumptions; then, by taking into account an additional hypothesis, one can easily design a time-based solution tailored to a specific environment. This paper illustrates the benefit of a top-down protocol design approach and shows that PADRE can be seen as an instance of a standard primary-backup replication protocol based on view-synchronous communication (VSC).
[time-free protocol, view-synchronous communication, Protocols, top-down protocol design approach, Communication system control, system details, Control systems, fault-tolerant control systems, Communication standards, Fault tolerance, asymmetric duplex redundancy, Fault tolerant systems, traffic control, Rail transportation, primary-backup replication protocol, redundancy, computerised control, replicated critical components, replicated databases, PADRE, Delay effects, Redundancy, rail traffic, back-up procedures, time-based implementation, memory protocols, timeliness assumptions, fault tolerant computing, applicability, inconsistency, Time factors, replicated traffic controller management]
A consensus protocol based on a weak failure detector and a sliding round window
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
The paper revisits the "sliding window" notion commonly encountered in communication protocols and applies it to the round numbers of round-based asynchronous protocols. This approach is novel. To illustrate its benefits, the paper presents an original weak failure detector-based consensus protocol that allows each process to be simultaneously involved in several rounds. The rounds in which a process is simultaneously involved defines "sliding round window". The proposed approach has several advantages. It fits better to the uncertainty created by the asynchrony and failures, and consequently permits one to design efficient round-based asynchronous protocols. Maybe more important, it also provides a better understanding of the global synchronization that manages the protocol progress from round to round. This appears clearly in the proposed failure detector-based consensus protocol, where the "sliding round window" allows one to dynamically define the message exchange pattern for each round separately.
[Heart, Protocols, Uncertainty, electronic messaging, weak failure detector, uncertainty, Distributed computing, Detectors, Broadcasting, protocols, agreement problems, global synchronization, distributed computing problems, Process control, consensus protocol, communication protocols, Computer crashes, Telecommunications, synchronisation, distributed algorithms, failure detector-based consensus protocol, concurrency control, sliding round window, fault tolerant computing, round-based asynchronous protocols, message exchange pattern]
Performance analysis of the CORBA notification service
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
As CORBA (Common Object Request Broker Architecture) gains popularity as a standard for portable, distributed, object-oriented computing, the need for a CORBA messaging solution is being increasingly felt. This led the Object Management Group (OMQ) to specify a Notification Service that aims to provide a more flexible and robust messaging solution than the earlier Event Service. The Notification Service provides several configurable quality of service (QoS) and administrative settings that deal with issues such as reliability, event (message) delivery order and discard policies. Unlike in conventional queuing systems, some Notification Service QoS configurations can lead to discards from within the internal queues, requiring careful analysis and configuration if such discards are to be avoided or minimized. This paper presents stochastic models (based on continuous time Markov chains and queuing theory) to analyze the Notification Service delivery and discard policies in detail.
[Portable computers, message passing, Object oriented modeling, CORBA messaging, Stochastic processes, Quality of service, queuing systems, performance evaluation, QoS configurations, Distributed computing, distributed computing, CORBA, Common Object Request Broker Architecture, stochastic models, delivery and discard policies, Computer architecture, Markov processes, Computer industry, The Notification Service, Message service, object-oriented computing, Performance analysis, Queueing analysis, distributed object management]
Can reliability and security be joined reliably and securely?
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
The combined topics of reliability and security are briefly traced in relation to the past and present endeavors of the Air Force Research Laboratory's Information Directorate. It is concluded that in the realm of information assurance, system features created to tolerate benign failures and to respond to attack must be stressed and tested beforehand and their effectiveness predicted, otherwise they might inadvertently magnify the attacker's power. With the explosive growth of distributed and mobile systems and the need for information assurance to address the accompanying vulnerabilities, one history lesson comes to mind: although ancient Rome was not built in a day, it did not take very long for it to fall once the barbarians took hold.
[Military standards, Roads, Laboratories, Wheels, mobile systems, aircraft, reliability, distributed processing, Information technology, Rails, security, security of data, system features, benign failure tolerance, Information security, Failure analysis, Air Force Research Laboratory, distributed systems, vulnerabilities, fault tolerant computing, US Department of Defense, information assurance, Reliability, military computing]
Looking ahead in atomic actions with exception handling
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
An approach to introducing exception handling into object-oriented N is presented. A novel atomic action scheme is developed that does not impose any participant synchronisation on action exit. In order to use cooperative exception handling at the action level as the main fault tolerance mechanism, we develop a distributed protocol that finds, for any exception raised, an action containing all potentially erroneous information, aborts all of its nested actions, resolves multiple concurrent exceptions and involves all the action participants into cooperative handling of the resolved exception. In the scheme, no service messages are sent and no service synchronisation is introduced if there are no exceptions raised. This flexible scheme can be applied in a number of emerging areas in which entities of a different nature (including software tasks, people, plants, documents, organisations, etc.) participate in cooperative activities.
[Protocols, fault tolerance mechanism, exception handling, multiple concurrent exceptions, Added delay, Fault tolerance, Tree graphs, Fault tolerant systems, groupware, action participants, service messages, object-oriented N, participant synchronisation, object-oriented programming, flexible scheme, software tasks, service synchronisation, atomic action scheme, cooperative activities, Application software, distributed protocol, cooperative exception handling, synchronisation, action exit, Computer errors, fault tolerant computing, cooperative handling, action level]
Incorporation of security and fault tolerance mechanisms into real-time component-based distributed computing systems
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
The volume and size of real-time (RT) distributed computing (DC) applications are now growing faster than in the last century. The mixture of application tasks running on such systems is growing as well as the shared use of computing and communication resources for multiple applications including RT and non-RT applications. The increase in use of shared resources accompanies with it the need for effective security enforcement. More specifically, the needs are to prevent unauthorized users: (1) from accessing protected information; and (2) from disturbing bona-fide users in getting services from server components. Such disturbances are also called denial-of-service attacks.
[Real time systems, shared use, bona-fide users, RT applications, fault tolerance mechanisms, Distributed computing, Computer crime, unauthorized users, server components, Fault tolerance, Network servers, security enforcement, authorisation, computer crime, shared resources, real-time component-based distributed computing systems, real-time distributed computing applications, Protection, Local area networks, distributed object management, non-RT applications, multiple applications, protected information, Information security, real-time systems, fault tolerant computing, denial-of-service attacks, Resource management, Computer network management]
Quantifying rollback propagation in distributed checkpointing
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Proposes a new classification of executions with checkpoints that is based on the notion of k-rollback, indicating the maximal number of checkpoints that may need to be rolled back during recovery. The relation between known execution classes is explored, and it is shown that coordinated checkpointing, SZPF (strictly Z-path free) and ZPF (Z-path free) are 1-rollback mechanisms, while ZCF (Z-cycle free) is (n-1)-rollback, where n is the number of participants in an execution. A new class of executions, called d-BC (d-bounded cycles), is introduced, and is shown to be an [(n-1)/spl middot/d]-rollback mechanism (ZCF is a special case of d-BC for d=1). Finally, a d-BC protocol is presented. This protocol has the nice property that it does not impose any control information overhead on an application's messages, yet it only sends a few control messages of its own. Moreover, the protocol maintains information about recovery lines, which enables very efficient discovery of the most recent recovery line that existed a short time before the failure.
[Checkpointing, Z-path free class, d-BC protocol, Protocols, distributed checkpointing, Electronic mail, Distributed computing, system recovery, rollback propagation, ZPF, control information overhead, d-bounded cycles, control messages, Fault tolerant systems, application messages, protocols, system failure, Information retrieval, Application software, Software debugging, recovery line information, Computer science, execution classes, coordinated checkpointing, Z-cycle free class, distributed algorithms, k-rollback, fault tolerant computing, SZPF, ZCF, execution classification]
Efficient TDMA synchronization for distributed embedded systems
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
A desired attribute in safety critical embedded real-time systems is a system time/event synchronization capability on which predictable communication can be established. Focusing on bus-based communication protocols in TDMA environments, we present a novel, efficient, and low-cost synchronization approach with bounded start-up time. This approach utilizes information about each node's unique message lengths to achieve synchronization. The protocol avoids start-up collisions by postponing retries after a collision. We also present a re-synchronization strategy that incorporates recovering nodes into synchronization.
[Costs, message passing, time event synchronization, Communication system control, Access protocols, communication protocols, distributed processing, Control systems, Synchronization, TDMA communication, synchronisation, Time division multiple access, Embedded system, time division multiple access, embedded systems, real-time systems, Broadcasting, distributed systems, Safety, protocols, Clocks]
Optimizing file availability in a secure serverless distributed file system
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Farsite is a secure, scalable, distributed file system that logically functions as a centralized file server but that is physically realized on a set of client desktop computers. Farsite provides security, reliability and availability by storing replicas of each file on multiple machines. It continuously monitors machine availability and relocates replicas as necessary to maximize the effective availability of the system. We evaluate several replica placement methods using large-scale simulation with machine availability data from over 50,000 desktop computers. We find that initially placing replicas in an availability-sensitive fashion yields pathological results, whereas very good results are obtained by random initial placement followed by incremental improvement using a scalable, distributed, fault-tolerant and attack-resistant hill-climbing algorithm. The algorithm is resilient to severe restrictions on communication and replica placement, and it does not excessively co-locate replicas of different files on the same set of machines.
[reliability, desktop computers, File servers, continuously machine availability monitoring, Distributed computing, secure serverless distributed file system, scalable system, optimisation, File systems, Physics computing, replica placement methods, centralized file server, Large-scale systems, Availability, communication restrictions, replicated databases, Data security, Computational modeling, Computer simulation, large-scale simulation, incremental improvement, file replicas, Computer displays, security of data, system availability maximization, client desktop computers, fault tolerant computing, replica relocation, fault-tolerant attack-resistant hill-climbing algorithm, file availability optimization, random initial placement]
Chasing the FLP impossibility result in a LAN: or, How robust can a fault tolerant server be?
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Fault tolerance can be achieved in distributed systems by replication. However Fischer, Lynch and Paterson (1985) have proven an impossibility result about consensus in the asynchronous system model, and similar impossibility results exist for atomic broadcast and group membership. We investigate, with the aid of an experiment conducted in a LAN, whether these impossibility results set limits to the robustness of a replicated server exposed to extremely high loads. The experiment consists of client processes that send requests to a replicated server (three replicas) using an atomic broadcast primitive. It has parameters that allow us to control the load on the hosts and the network, as well as the timeout value used by our heartbeat failure detection mechanism. Our main observation is that the atomic broadcast algorithm never stops delivering messages, not even under arbitrarily high load and very small timeout values (1 ms). So, by trying to illustrate the practical impact of impossibility results, we discovered that we had implemented a very robust replicated service.
[Protocols, network servers, group membership, local area networks, FLP impossibility result, Fault tolerance, Network servers, Heart beat, heartbeat failure detection mechanism, Fault tolerant systems, impossibility result, Detectors, Broadcasting, asynchronous system model, distributed systems, Robustness, Local area networks, replication, client-server systems, replicated databases, fault tolerant server, atomic broadcast, timeout value, LAN, fault tolerant computing, Frequency synchronization]
Compiler-assisted heterogeneous checkpointing
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
We consider the problem of heterogeneous checkpointing in distributed systems. We propose a new solution to the problem that is truly heterogeneous in that it can support new architectures without any information about the architecture. The ability to support new architectures without additional knowledge or custom configuration is an important contribution of this work. This ability is particularly useful in mobile settings in which there is no a priori knowledge of the potential machines on which the application might execute. Our solution supports execution in unknown settings as long as there is compiler support for the high-level language in which the application is written. We precisely define what it means for a particular solution to be heterogeneous and discuss the heterogeneity of our solution and other solutions. We use code instrumentation at the source code level to provide heterogeneous checkpointing and recovery.
[Checkpointing, System testing, Instruments, Random access memory, distributed processing, High level languages, Sun, Yarn, system recovery, program compilers, Computer science, Operating systems, Linux, transparency, heterogeneous checkpointing, distributed systems, compiler support]
Detecting heap smashing attacks through fault containment wrappers
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Buffer overflow attacks are a major cause of security breaches in modern operating systems. Not only are overflows of buffers on the stack a security threat, overflows of buffers kept on the heap can be too. A malicious user might be able to hijack the control flow of a root-privileged program if the user can initiate an overflow of a buffer on the heap when this overflow overwrites a function pointer stored on the heap. The paper presents a fault-containment wrapper which provides effective and efficient protection against heap buffer overflows caused by C library functions. The wrapper intercepts every function call to the C library that can write to the heap and performs careful boundary checks before it calls the original function. This method is transparent to existing programs and does not require source code modification or recompilation. Experimental results on Linux machines indicate that the performance overhead is small.
[Buffer storage, fault-containment wrapper, security threat, heap buffer overflows, heap smashing attack detection, Security, C language, software libraries, function call, fault containment wrappers, buffer overflow attacks, Operating systems, control flow, Fault tolerant systems, computer crime, security breaches, Libraries, function pointer, root-privileged program, Protection, buffer storage, C library functions, program diagnostics, Debugging, malicious user, boundary checks, performance overhead, Fault detection, Linux, modern operating systems, operating systems (computers), Linux machines, Buffer overflow]
An analytical framework for reasoning about intrusions
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Local and wide area network information assurance analysts need current and precise knowledge about their system activities in order to address the challenges of critical infrastructure protection. In particular, the analyst needs to know in real-time that an intrusion has occurred so that an active response and recovery thread can be created rapidly. Existing intrusion detection solutions are basically after-the-fact, thereby offering very little in terms of damage confinement and restoration of service. Quick recovery is only possible if the assessment scheme has low latency and it occurs in real-time. The objective of the paper is to develop a reasoning framework to aid in the real-time detection and assessment task that is based on a novel idea of encapsulation of owner's intent. The theoretical framework developed here will help resolve dubious circumstances that may arise while inferring the premises of operations (encapsulated from owner's intent) by way of examining the observed conclusions resulting from the actual operations of the owner. This reasoning is significant in view of the fact that intrusion signaling is not a binary decision unlike error detection in traditional fault tolerance. Our reasoning framework has been developed by leveraging the concepts of cost analysis and pricing under uncertainty found in economics and finance. Our main result is the modeling of user activity on a computing system as a martingale and the subsequent quantification of the cost of performing a job to enable decision making.
[Encapsulation, user activity modeling, critical infrastructure protection, Costs, error detection, Yarn, system recovery, Delay, Signal resolution, Information analysis, intrusion detection solutions, Intrusion detection, safety systems, analytical framework, active response, intrusion signaling, Protection, Wide area networks, network information assurance analysts, system activities, recovery thread, reasoning framework, computer network management, Fault detection, real-time systems, decision making, real-time assessment scheme, martingale, fault tolerant computing, cost analysis]
A microkernel middleware architecture for distributed embedded real-time systems
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Today more and more embedded real-time systems are implemented in a distributed way. These distributed embedded systems consist of a few controllers up to several hundreds. Distribution and parallelism in the design of embedded real-time systems increase the engineering challenges and require new methodological framework based on middleware. Our research work focuses on the development of a middleware that supports the design of heterogeneous distributed real-time systems and allows the use of small microcontrollers as computation nodes. Our study is aimed to a new approach that led to the development of OSA+-a scalable service-oriented real-time middleware architecture. This middleware has been used as the basic platform for different domain applications: (i) conception of an autonomous guided vehicle system based on multithreaded Java microcontrollers and (ii) development of a permanent monitoring distributed system for an oil drilling application. This paper presents the basic architecture of OSA+ and its implementation for the distributed real-time embedded systems design.
[Real time systems, distributed embedded systems, Microcontrollers, Control systems, computation nodes, OSA+, Middleware, Distributed computing, autonomous guided vehicle system, Design engineering, andparallelism, Embedded system, real-time systems, embedded systems, Computer architecture, heterogeneous distributed real-time systems, Parallel processing, Systems engineering and theory, automatic guided vehicles, embedded real-time systems, distributed object management, middleware, microcontrollers]
On the effectiveness of a counter-based cache invalidation scheme and its resiliency to failures in mobile environments
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Caching frequently accessed data items on the client side is an effective technique to improve the performance of data dissemination in mobile environments. Classical cache invalidation strategies are not suitable for mobile environments due to the disconnection and mobility of the mobile clients. One attractive cache invalidation technique is based on invalidation reports (IRs). However, IR-based approach suffers from long query latency and it cannot efficiently utilize the broadcast bandwidth. In this paper, we propose techniques to address these problems. We first extend the UIR-based approach to reduce the query latency. Then, we propose techniques to efficiently utilize the broadcast bandwidth based on counters associated with each data item. Novel techniques are designed to maintain the accuracy of the counter in case of server failures, client failures, and disconnections. Extensive simulations are provided and used to evaluate the proposed methodology. Compared to previous IR-based algorithms, the proposed solution can significantly reduce the query latency, improve the bandwidth utilization, and effectively deal with disconnections and failures.
[mobile environments, cache invalidation, Portable computers, cache, counter-based cache invalidation, performance evaluation, Data engineering, cache storage, Batteries, Delay, Counting circuits, Computer science, mobile computing, server failures, Bandwidth, Broadcasting, Explosives, fault tolerant computing, client failures, Mobile computing, disconnections, broadcast bandwidth]
The challenge of creating productive collaborating information assurance communities via Internet research and standards
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Overviews the challenging 5-year process leading to the design, specification, and implementation of the Internet, Engineering Task Force (IETF) Intrusion Detection Working Group (IDWQ) Intrusion Exchange Protocol (IDXP). IDXP seeks to facilitate the ubiquitous interoperability of intrusion detection components across Internet enterprises. This capability is a critical enabler of successful intrusion detection for large networks. The IETF IDWG was inspired by the DARPA CIDF activity. IDXP was developed and demonstrated in recent IETF meetings and in the IEEE DISCEX (DARPA Information Survivability Conference and EXposition). In the future, we intend to incorporate event correlation into IDXP. The process of achieving technical and organizational consensus among the segmented communities that comprise the information assurance community has been exceedingly challenging. The paper addresses the driving factors for this situation, and analyses the reasons for the ultimate community success in getting the process on the road. It is hoped that this experience would be useful in other technical disciplines facing large collaborative challenges within large secure distributed environments.
[Protocols, Intrusion Detection Working Group, open systems, Communities, Meetings, productive collaborating information assurance communities, Standardization, Internet research, large collaborative challenges, Intrusion Exchange Protocol, interoperability, Research initiatives, large secure distributed environments, large networks, standards, Design engineering, security of data, event correlation, Collaboration, Intrusion detection, Information security, groupware, Internet enterprises, Internet]
Polynomial time synthesis of Byzantine agreement
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
We present a polynomial time algorithm for automatic synthesis of fault-tolerant distributed programs, starting from fault-intolerant versions of those programs. Since this synthesis problem is known to be NP-hard, our algorithm relies on heuristics to reduce the complexity. We demonstrate that our algorithm is able to synthesize an agreement program that tolerates a Byzantine fault.
[Algorithm design and analysis, fault-tolerant distributed programs, complexity, NP-hard, polynomial time synthesis, Distributed computing, Fault tolerance, Information science, heuristic programming, heuristics, Polynomials, Contracts, distributed programming, Java, automatic programming, Engineering profession, Byzantine agreement, automatic synthesis, State-space methods, polynomial time algorithm, software fault tolerance, Computer science, agreement program, formal methods, fault-intolerant versions, computational complexity, Byzantine fault]
Assessing inter-modular error propagation in distributed software
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
With the functionality of most embedded systems based on software (SW), interactions amongst SW modules arise, resulting in error propagation across them. During SW development, it would be helpful to have a framework that clearly demonstrates the error propagation and containment capabilities of the different SW components. In this paper, we assess the impact of inter-modular error propagation. Adopting a white-box SW approach, we make the following contributions: (a) we study and characterize the error propagation process and derive a set of metrics that quantitatively represents the inter-modular SW interactions, (b) we use a real embedded target system used in an aircraft arrestment system to perform fault-injection experiments to obtain experimental values for the metrics proposed, (c) we show how the set of metrics can be used to obtain the required analytical framework for error propagation analysis. We find that the derived analytical framework establishes a very close correlation between the analytical and experimental values obtained. The intent is to use this framework to be able to systematically develop SW such that inter-modular error propagation is reduced by design.
[fault-injection experiments, Error analysis, white-box approach, aircraft arrestment system, distributed processing, real embedded target system, error propagation analysis, Embedded software, Embedded system, embedded systems, inter-modular error propagation assessment, distributed software, Software systems, fault tolerant computing, Performance analysis, Aircraft, error handling]
Consensus with written messages under link faults
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
This paper shows that deterministic consensus with written messages is possible in presence of link faults and compromised signatures. Relying upon a suitable perception-based hybrid fault model that provides different categories for both node and link faults, we prove that the authenticated Byzantine agreement algorithms OMHA and ZA of Gong, Lincoln and Rushby (1995) can be made resilient to f/sub l/ link faults per node by adding 3f/sub l/ and 2f/sub l/ nodes, respectively. Both algorithms can also cope with compromised signatures if the affected nodes are considered as arbitrary faulty. Authenticated algorithms for consensus are therefore reasonably applicable even in wireless systems, where link faults and intrusions are the dominating source of errors.
[Automation, Byzantine agreement, Field buses, distributed processing, Network interfaces, Multiaccess communication, fault-tolerant systems, consensus, written messages, fault models, Wireless networks, inter-computer links, link faults, Fault tolerant systems, Authentication, message authentication, Spread spectrum communication, Broadcasting, distributed systems, fault tolerant computing, Distributed algorithms, authentication]
Comparison-based system-level fault diagnosis in ad hoc networks
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
The problem of identifying faulty mobiles in ad-hoc networks is considered. Current diagnostic models were designed for wired networks, thus they do not take advantage of the shared nature of communication typical of ad-hoc networks. In this paper we introduce a new comparison-based diagnostic model based on the one-to-many communication paradigm. Two implementations of the model are presented. In the first implementation, we assume that the network topology does not change during diagnosis, and we show that both hard and soft faults can be easily, detected Based on this implementation, a diagnosis protocol is presented The evaluation of the communication and time complexity of the protocol indicates that efficient diagnosis protocols for ad-hoc networks based on our model can be designed In the second implementation we allow the system topology to change during diagnosis. As expected, the ability of diagnosing faults under this scenario is significantly reduced with respect to the stationary case.
[complexity, Protocols, fault diagnosis, diagnosis protocol, Mobile communication, Ad hoc networks, network topology, wireless communication, Fault diagnosis, Intelligent networks, mobile computing, ad-hoc networks, Network topology, Wireless networks, faulty mobiles, Computer networks, protocols, Mobile computing, diagnostic models, mobile units, Testing]
Continental Pronto
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Continental Pronto unifies high availability and disaster resilience at the specification and implementation levels. At the specification level, Continental Pronto formalizes the client's view of a system addressing local-area and wide-area data replication within a single framework. At the implementation level, Continental Pronto makes data highly available and disaster resilient. The algorithm provides disaster resilience with a cost similar to traditional 1-safe and 2-safe algorithms and provides highly-available data with a cost similar to algorithms tailored for that purpose.
[Availability, client-server systems, databases, Costs, wide area networks, Laboratories, Humans, Computer crashes, Spatial databases, local area networks, Transaction databases, Resilience, local-area network, Network servers, Earthquakes, Continental Pronto, wide-area networks, distributed databases, Internet, protocols, disaster-recovery protocol]
How to select a replication protocol according to scalability, availability and communication overhead
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Data replication is playing an increasingly important role in the design of parallel information systems. In particular, the widespread use of cluster architectures in high-performance computing has created many opportunities for applying data replication techniques in new areas. For instance, as part of work related to cluster computing in bioinformatics, we have been confronted with the problem of having to choose an optimal replication strategy in terms of scalability, availability and communication overhead. Thus, we have evaluated several representative replication protocols in order to better understand their behavior in practice. The results obtained are surprising in that they challenge many of the assumptions behind existing protocols. Our evaluation indicates that the conventional read-one/write-all approach is the best choice for a large range of applications requiring data replication. We believe this is an important result for anybody developing code for computing clusters as the read-one/write-all strategy is much simpler to implement and more flexible than quorum-based approaches. In this paper we show that, in addition, it is also the best choice using a number of other selection criteria.
[workstation clusters, transaction processing, Protocols, Scalability, Distributed information systems, cluster computing, availability, communication overhead, scalability, Information systems, Concurrent computing, Databases, parallel information systems, cluster architectures, read-one/write-all approach, optimal replication strategy, quorum-based approaches, Computer architecture, Bioinformatics, high-performance computing, Availability, data replication protocol selection, replicated databases, parallel databases, memory protocols, transactions, Computer science, bioinformatics]
Applying fault-tolerance principles to security research
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
There has been much focus on building secure distributed systems. The CERIAS center has been established at Purdue along with 14 other such centers in USA. We note that many of the ideas, concepts, algorithms being proposed in security have many common threads with reliability. We need to apply the science and engineering of reliability research to the research in security and vice versa. We briefly give some examples to illustrate the ideas. To increase reliability in distributed systems, the use of quorums allows the transactions to read and write replicas even if some replicas have failed or are unavailable. So the systems manage the replicas so that a forum can be formed in the presence of failures. To make systems secure against unauthorized access, one can use the reverse strategy of making it difficult to form quorums. All accesses require permission from a group of authorities who could coordinate to deny a yes majority vote.
[Checkpointing, operational failures, distributed processing, Reliability engineering, intrusion detection, functional failure, availability, Distributed computing, Yarn, consistency, Fault tolerance, Voting, Fault tolerant systems, Permission, Availability, fault-tolerance principles, rollback, atomicity, Data security, service attacks, replicas, check points, durability, unauthorized access, adaptability, quorums, secure distributed systems, authorities, reliable distributed systems, security of data, security research, fault tolerant computing]
Designing a robust namespace for distributed file services
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
A number of ongoing research projects follow a partition-based approach to provide highly scalable distributed storage services. These systems maintain namespaces that reference objects distributed across multiple locations in the system. Typically, atomic commitment protocols, such as 2-phase commit, are used for updating the namespace, in order to guarantee its consistency even in the presence of failures. Atomic commitment protocols are known to impose a high overhead to failure-free execution. Furthermore, they use conservative recovery procedures and may considerably restrict the concurrency of overlapping operations in the system. This paper proposes a set of new protocols implementing the fundamental operations in a distributed namespace. The protocols impose a minimal overhead to failure-free execution. They are robust against both communication and host failures, and use aggressive recovery procedures to re-execute incomplete operations. The proposed protocols are compared with their 2-phase commit counterparts and are shown to outperform them in all critical performance factors: communication round-trips, synchronous I/O, operation concurrency.
[Protocols, Scalability, robust namespace design, Laboratories, distributed file services, partition-based approach, memory protocols, atomic commitment protocols, Control systems, Electrical capacitance tomography, Milling machines, Concurrent computing, File systems, Lapping, communication round-trips, operation concurrency, distributed databases, aggressive recovery procedures, Robustness, fault tolerant computing, synchronous I/O, naming services, 2-phase commit]
High-quality customizable embedded software from COTS components
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Dramatic advances in computer and communication technologies have greatly promoted the growth of embedded telecommunication systems. More and more critical applications, such as banking and financial services, remote patient monitoring systems, transportation, etc., are being developed. The software for these applications is becoming increasingly sophisticated and complex and this trend will accelerate over the next few years with the development of "software-defined telephony". To support these critical applications, it is necessary to achieve high quality and rigorously demonstrate that high quality has in fact been achieved. In today's highly competitive environment; it is also essential to have accelerated development schedules and the capability to quickly customize and adapt products for niche markets and to satisfy diverse regional standards and procedures. To meet all these challenges, software development technology is rapidly shifting away from low-level programming issues to automated code generation and integration of systems from components, either Commercial-Off-The-Shelf (COTS) components or specially developed in-house components. This is made possible by numerous recent breakthroughs in software technology, including web-based cooperative software development, in-process; monitoring, agents, Java, scripting languages, and, especially, industry-driven standardization efforts, such as CORBA, TINA, TL 9000, and XDAIS. The use of COTS components can significantly reduce software development time and cost.
[Embedded computing, component customization, Transportation, infrastructure for Advanced Programming for Embedded Computer Systems, Banking, Programming, Telecommunication computing, Application software, iAPEX, Embedded software, Patient monitoring, UT-Dallas Embedded Software Center, embedded software, COTS components, COTS, embedded systems, Communications technology, software engineering, Acceleration]
Optimistic validation of electronic tickets
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Electronic tickets, or e-tickets, give evidence that their holders have permission to enter a place of entertainment, use a means of transportation, or have access to some Internet services. E-tickets can be stored in desktop computers or personal digital assistants for future use. Before being used, e-tickets have to be validated to prevent duplication, and ensure authenticity and integrity. The paper discusses e-ticket validation in contexts in which users cannot be trusted and validation servers may fail by crashing. The paper considers formal definitions for the e-ticket problem and proposes an optimistic protocol for validation of e-tickets. The protocol is optimistic in the sense that its best performance is achieved when e-tickets are validated only once.
[electronic tickets, Internet services, e-ticket validation, optimistic protocol, formal definitions, desktop computers, authenticity, validation servers, data integrity, optimisation, integrity, formal verification, message authentication, Bismuth, Internet, protocols, optimistic validation, electronic commerce, personal digital assistants]
Application of commercial-grade digital equipment in nuclear power plant safety systems
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
Due to obsolescence, increasing maintenance costs, and the lack of qualified spare parts for the equipment and components of the analog instrumentation and control (I&C) systems in operating domestic nuclear power plants, nuclear utilities are replacing equipment and upgrading certain I&C systems. These activities generally involve changing from analog to digital technology. In many cases commercial products offer practical solutions. Digital I&C systems have the potential to enhance safety, reliability, and availability of the plant systems and improve plant operation. However, the use of digital software-based equipment presents challenges and concerns to the U.S. nuclear industry and the Nuclear Regulatory Commission (NRC). The NRC's approach to the review and acceptance of design qualification for digital systems largely focuses on confirming that the applicant or licensee has employed a high-quality development process that incorporated disciplined specification and implementation of design requirements. Inspection and testing is used to verify correct implementation and to validate the desired functionality of the final product.
[Costs, Digital systems, nuclear engineering computing, safety-critical software, reliability, Control systems, digital software-based equipment, Nuclear Power Plant Safety Systems, instrumentation and control, safety, Safety, Power generation, Availability, high-quality, Instruments, safety system, U.S. nuclear industry, testing, specification, nuclear power stations, nuclear safety systems, quality assurance, Computer industry, Power system reliability, Qualifications]
Why is it so hard to predict software system trustworthiness from software component trustworthiness?
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
When software is built from components, nonfunctional properties such as security, reliability, fault-tolerance, performance, availability, safety, etc. are not necessarily composed. The problem stems from our inability to know a priori, for example, that the security of a system composed of two components can be determined from knowledge about the security of each. This is because the security of the composite is based on more than just the security of the individual components. There are numerous reasons for this. The article considers only the factors of component performance and calendar time. It is concluded that no properties are easy to compose and some are much harder than others.
[nonfunctional properties, System testing, software system trustworthiness prediction, calendar time, software reliability, software safety, Interconnected systems, Calendars, Software safety, Security, Operating systems, software availability, Intrusion detection, Authentication, component performance, Software systems, software component trustworthiness, software fault-tolerance, software security, software performance, Software engineering]
Message logging optimization for wireless networks
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
This paper describes a message logging optimization that improves performance for failure recovery protocols where messages exchanged between mobile hosts are logged at base stations. The algorithm described and evaluated in this paper does not generate orphan processes in spite of base station failures and achieves run-time performance similar to that of asynchronous logging.
[Base stations, Wireless application protocol, Size measurement, wireless networks, radio access networks, Intelligent networks, Runtime, optimisation, mobile computing, Wireless networks, Measurement standards, failure recovery protocols, Bandwidth, Writing, telecommunication traffic recording, message logging optimization, protocols, Mobile computing]
Reducing noise in gossip-based reliable broadcast
Proceedings 20th IEEE Symposium on Reliable Distributed Systems
None
2001
We present in this paper a general garbage collection scheme that reduces the "noise" in gossip-based broadcast algorithms. In short, our garbage collection scheme uses a simple heuristic to trade "useless" messages with "useful" ones. Used with a given gossip-based broadcast algorithm, a given size of buffers, and a given number of disseminated messages (e.g., per gossip round), our garbage collection scheme provides higher overall reliability than more conventional schemes. We illustrate our approach through two algorithms: bimodal multicast (pbcast) and lightweight probabilistic broadcast (lpbcast). Our scheme is based on the intuitive idea of discarding messages according to their "age". The "age" of a message represents the number of times the message has been retransmitted.
[lightweight probabilistic broadcast, pbcast, Scalability, Noise reduction, Laboratories, Redundancy, buffers, Broadcast technology, computer networks, heuristic, lpbcast, Degradation, storage management, gossip-based reliable broadcast, bimodal multicast, Multicast algorithms, heuristic programming, Broadcasting, noise reduction, disseminated messages, Large-scale systems, garbage collection scheme, Local area networks]
Self-stabilizing distributed file systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
A self-stabilizing distributed file system is presented. The system constructs and maintains a spanning tree for each file volume. The spanning tree consists of the servers that have volume replicas and caches for the specific file volume. The spanning trees are constructed and maintained by self-stabilizing distributed algorithms. File system updates use the tree to implement file read and write operations.
[caches, volume replicas, File servers, Control systems, Cache storage, cache storage, File systems, servers, Operating systems, network operating systems, file servers, file volume, Cost function, Computer networks, tree data structures, file system updates, Maintenance, self-stabilizing distributed algorithms, file read operations, Secure storage, self-stabilizing distributed file system, Computer science, distributed algorithms, file write operations, spanning tree]
Power-aware epidemics
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Epidemic protocols have been heralded as appropriate for wireless sensor networks. The nodes in such networks have limited battery resources. In this paper we investigate the use of power in three styles of epidemic protocols: basic epidemics, neighborhood flooding epidemics, and hierarchical epidemics. Basic epidemics turn out to be highly power hungry, and are not appropriate for power-aware applications. Both neighborhood and hierarchical epidemics can be made to use power judiciously, but a trade-off exists between scalability and latency.
[power-aware epidemics, basic epidemics, hierarchical epidemics, wireless sensor networks, Peer to peer computing, Scalability, Wireless application protocol, latency, Batteries, Floods, neighborhood flooding epidemics, Delay, scalability, Computer science, Wireless sensor networks, epidemic protocols, Robustness, Database systems, protocols, wireless LAN]
OBIGrid: towards a new distributed platform for bioinformatics
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This paper describes the design philosophy for the grid system being developed by the Japan Committee on High-Performance Computing for Bioinformatics and Initiative for Parallel Bioinformatics (IPAB). The grid is an attractive solution to achieve a distributed bioinformatics environment with high performance parallel computers, large genomic databases, computation intensive applications such as homology search and molecular simulation. However, there is a lot of work to do in grid system design, especially in the wide area network environment. OBIGrid emphasizes the virtual organization aspect of the grid system and gives more priority to security and scalability than performance.
[homology search, wide area networks, open systems, molecular simulation, Genomics, distributed bioinformatics environment, large genomic databases, wide area network environment, Distributed computing, scalability, high performance parallel computers, Concurrent computing, security, biology computing, Distributed databases, Grid computing, Bioinformatics, Computational modeling, grid system, Application software, OBIGrid, security of data, High performance computing, computation intensive applications, Computer applications]
Fault-local stabilization : the shortest path tree
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
We present a fault-local solution to the shortest path tree problem in a rooted network. We consider the case where a transient fault corrupts f nodes (f is unknown, but inferior to half the size of the network) after the tree has been constructed. Our solution allows to recover in less than O (f) time units. If an upper bound k on the number of corrupted nodes is known, the memory space needed depends only on k.
[Algorithm design and analysis, Protocols, Costs, Design methodology, fault-local solution, stabilization, computer networks, transient fault, shortest path tree, voting based protocol, Upper bound, Tree graphs, Voting, fault-local error handling, Communication channels, rooted network, fault tolerant computing]
Asynchronous Byzantine group communication
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This paper summarizes our work on group communication in a fully asynchronous Byzantine environment. Instead of failure detectors or timing information, our protocols use randomization to circumvent the impossibility result by Fischer, Lynch and Paterson. This is the first time this technique is used for a real system; thanks to modern cryptography, our protocols are practical and fast enough to be used in practice. To cleanly combine cryptography with fault tolerance, a new model had to be developed that might be of independent interest.
[fault tolerance, Laboratories, cryptography, Computer crashes, asynchronous Byzantine group communication, group communication, Cryptographic protocols, Fault tolerance, randomization, Authentication, Prototypes, Detectors, Timing, Safety, Cryptography, protocols]
From Byzantine agreement to practical survivability: a position paper
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Only a decade ago, issues of replication, high availability and load balancing were the focus of small, closely coupled cluster projects. Consequently, techniques for cluster management and small replication systems are abundant. However, the advent of the Internet led to wide spread and highly decentralized access of services and content that bring issues of scale and ubiquitous deployment. In particular, the need to maintain copies of replicated data consistent grows beyond the limits of any local cluster. Consequently, researchers have been looking at ways to improve scalability, survivability and dynamism of replication technology. Additionally, there are a number of recent application domains that exhibit new and challenging models for information replication. For example, advances in storage technology permit processes to share information by directly accessing data on disks that are connected to a storage area network (SAN), thereby avoiding going through a file system service. This form of direct data sharing necessitates coordination among processes contending for access to data, and presents new building blocks for doing it. New needs are also re-shaped by novel services such as Jini, a global resource discovery and location tool that allows anonymous and transient clients to be serviced; by Java-spaces, a universal shared data space; by Oceanstore, an eternal storage archive that is built of peers that have an economical incentive to cooperate; by Publius, an anonymous and survivable publishing archive; and others. Many other peer-to-peer (P2P) systems offer the potential of a truly survivable settings, but on the other hand, pose challenges of scale, dynamism and trust issues.
[Java-spaces, load balancing, anonymous survivable publishing archive, Scalability, peers, Publius, scalability, eternal storage archive, File systems, Publishing, Web and internet services, storage technology, replicated data, Availability, replication, Java, replicated databases, Peer to peer computing, decentralized access, data integrity, global resource discovery and location tool, Computer science, Storage area networks, high availability, storage area network, universal shared data space, Oceanstore, dynamism, Load management, Jini, fault tolerant computing, Internet, survivability, data sharing]
Collaborative networking in an uncooperative Internet
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Collaborative applications often require peer-to-peer interaction and peer discovery mechanisms. In today's Internet, Firewall and NAT technology, and a lack of support of IP multicast, have made it very difficult to support such applications. Application Level Gateways and Directory Services can solve these problems to some extent, but have scalability problems and should be used as a last resort. This paper describes our experience with implementing a service called Astrolabe which uses a peer-to-peer epidemic protocol. We show how we solved peer-to-peer communication, auto-configuration, and peer discovery. The resulting Astrolabe service can be used to support the development of other peer-to-peer protocols and applications.
[Protocols, Peer to peer computing, Astrolabe, collaborative networking, Application software, auto-configuration, scalability, peer discovery, Intelligent networks, Collaboration, Directory Services, groupware, Collaborative work, Application Level Gateways, peer-to-peer protocols, Internet, IP networks, protocols, peer-to-peer epidemic protocol, peer-to-peer communication, Web server, Network address translation]
Probabilistic atomic broadcast
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Reliable distributed protocols, such as consensus and atomic broadcast, are known to scale poorly with large number of processes. Recent research has shown that algorithms providing probabilistic guarantees are a promising alternative for such environments. In this paper, we propose a specification of atomic broadcast with probabilistic liveness and safety guarantees. We present an algorithm that implements this specification in a truly asynchronous system (i.e., without assumptions about process speeds and message transmission times).
[probabilistic protocols, Protocols, Scalability, distributed protocols, Laboratories, Broadcast technology, distributed processing, group communication protocols, atomic broadcast, probabilistic liveness, Degradation, Analytical models, reliable distributed systems, Voting, Message passing, Broadcasting, safety guarantees, Safety, protocols]
Implementing IPv6 as a peer-to-peer overlay network
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This paper proposes to implement an IPv6 routing infrastructure as a self-organizing overlay network on top of the current IPv4 infrastructure. The overlay network builds upon a distributed IPv6 edge router with a master/slave architecture. We show how different slaves can be constructed to tunnel through NATs and firewalls, as well as to improve robustness of the routing infrastructure and to provide efficient and resilient implementations for features such as multicast, anycast, and mobile IP using currently available peer-to-peer (P2P) protocols. The resulting IPv6 overlay network would restore the end-to-end property of the original Internet, support evolution and dynamic updating of the protocols running on the overlay network, make available IPv6 and the associated features to network applications immediately, and provide an ideal underlying infrastructure for P2P applications, without changing networking hardware and software in the core Internet.
[Switches, firewalls, protocol evolution, master/slave architecture, distributed IPv6 edge router, mobile IP, self-organizing overlay network, multicast communication, Hardware, Routing protocols, Robustness, peer-to-peer protocols, Large-scale systems, peer-to-peer overlay network, dynamic updating, IP networks, protocols, Network address translation, IPv4 infrastructure, anycast, Peer to peer computing, robustness, Multicast protocols, multicast, Computer science, IPv6 routing infrastructure, telecommunication network routing, end-to-end property, Internet]
A self-stabilizing algorithm for the Steiner tree problem
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Self-stabilization is a theoretical framework of non-masking fault-tolerant distributed algorithms. In this paper, we investigate the Steiner tree problem in distributed systems, and propose a self-stabilizing solution to the problem. Our solution is based on the pruned-MST technique, a heuristic technique to find a minimal cost Steiner tree by pruning unnecessary nodes and edges in a minimum cost spanning tree, provided that a minimum spanning tree is available. Finally we propose an algorithm to reduce the cost of the solution.
[edge pruning, pruned-MST technique, minimum cost spanning tree, Heuristic algorithms, trees (mathematics), Routing, nonmasking fault-tolerant distributed algorithms, Steiner tree problem, heuristic technique, software fault tolerance, self-stabilizing algorithm, minimal cost Steiner tree, Fault tolerance, Multicast algorithms, Tree graphs, heuristic programming, node pruning, Fault tolerant systems, distributed algorithms, Cost function, Approximation algorithms, Safety, Distributed algorithms, computational complexity]
International workshop on self-repairing and self-configurable distributed systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
false
[]
The performance of checkpointing and replication schemes for fault tolerant mobile agent systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
We evaluate the performance of checkpointing and replication schemes for the fault tolerant mobile agent system. For the quantitative comparison, we have implemented an experimental system on top of the Mole mobile agent system and also built a simulation system to include various failure cases. Our experiment aims to have the insight into the behavior of agents under two schemes and provide a guideline for the fault tolerant system design. The experimental results show that the checkpointing scheme shows a very stable performance; and for the replication scheme, some controllable system parameter values should be chosen carefully to achieve the desirable performance.
[Checkpointing, checkpointing, Costs, Computational modeling, experimental system, simulation, Software performance, performance evaluation, Distributed computing, system recovery, software fault tolerance, Guidelines, Fault tolerance, Mole mobile agent system, replication schemes, Fault tolerant systems, Mobile agents, mobile agents, virtual machines, fault tolerant mobile agent systems, controllable system parameter values, Mobile computing, software performance evaluation]
Modeling communication delays in distributed systems using time series
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
The design of dependable distributed applications is a hard task, mainly because of the indefinable statistic behavior of the communication delays. Despite this feature, in practice, most system monitors make use of timeouts (a maximum waiting time) to ensure some termination properties in their protocols. To have better results, some monitors dynamically predict new timeout values based on observed communication delays to improve the performance and accuracy of the protocols. In the last years, time series theory emerged in the computer science area as a good tool to increase the prediction accuracy. The time series mathematical model is used to describe a sequence of observations of a stochastic process taken periodically in time. This paper shows how to model the round trip communication delay observed by a pull monitor (a periodic requester source), despite its non-periodic answers. As a result, this paper shows that the round trip communication delay pattern can be properly represented as a time series. This time series is an alternative model to explore new timeout predictors in a distributed system.
[Protocols, round trip communication delay, Delay systems, software reliability, Stochastic processes, Predictive models, distributed processing, timeouts, Accuracy, Statistical distributions, distributed systems, maximum waiting time, Mathematical model, protocols, prediction accuracy, system monitors, termination, Delay effects, communication delay modeling, time series, stochastic process, Computer science, dependable distributed applications, Computer displays, delays, system monitoring]
Analysis of inspection-based preventive maintenance in operational software systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Recently, the phenomenon of "software aging\
[Software maintenance, Costs, software reliability, Software performance, Steady-state, Degradation, Markov Regenerative Process, Analytical models, software rejuvenation, preemptive-resume type transitions, cost-benefit analysis, Aging, inspection-based preventive maintenance, operational software systems, crash hang failure, inspection, steady state conditions, cost, overhead, Computer crashes, software aging, transient conditions, performance degradation, software maintenance, Preventive maintenance, downtime, subordinated semi-Markov reward process, Markov processes, Software systems]
The guardian model for exception handling in distributed systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
We present an abstraction called guardian for exception handling in distributed systems. The guardian can solve several limitations with existing distributed exception handling techniques. To understand these limitations, we analyze distributed exception handling with respect to sequential exception handling and identify the significant differences between them. This leads to the fundamental problem with distributed exception handling, which is invoking the semantically correct exception handlers in all the distributed processes that are required to participate in the recovery. The guardian model addresses this problem. It introduces a set of programming primitives and a global exception handler. Finally, using a primary-backup example we illustrate how the guardian model is used for global exception handling in a distributed system.
[fault tolerance, exception handling, distributed processing, distributed exception handling, global exception handler, sequential exception handling, system recovery, Signal resolution, software fault tolerance, guardian model, Computer science, Computer languages, Runtime, Fault tolerant systems, Signal processing, distributed systems, programming, primary-backup]
Fault-tolerant virtual private networks within an autonomous system
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This paper proposes the concept of a fault-tolerant virtual private network (FVPN) within an autonomous system-a framework for supporting seamless network fail-over by leveraging the inherent redundancy of the underlying Internet infrastructure. The proposed architecture includes an application-level module, which is integrated into gateways at VPN end-points. This module enables fail-over to a redundant path without waiting for the underlying routing protocol converging to a new route. The paper introduces two algorithms for establishing redundant backup paths while minimizing overlapping network links. The proposed schemes are evaluated using topology information and sample routing logs from a regional Internet service provider network.
[algorithms, Costs, network servers, VPN end-points, regional Internet service provider network, overlapping network links, Convergence, fault-tolerant virtual private network, redundant backup paths, Fault tolerance, Fault tolerant systems, Routing protocols, computer network reliability, redundancy, IP networks, application-level module, Availability, gateways, autonomous system, Redundancy, virtual private networks, topology information, Internet infrastructure, telecommunication network routing, seamless network fail-over, fault tolerant computing, Virtual private networks, Internet, sample routing logs]
An analysis of fault detection latency bounds of the SNS scheme incorporated into an Ethernet based middleware system
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
The supervisor-based network surveillance (SNS) scheme is a semi-centralized network surveillance scheme for detecting the health status of computing components in a distributed real-time (RT) system. An implementation of the SNS scheme in a middleware architecture, named ROAFTS (real-time object-oriented adaptive fault-tolerance support), has been underway in the authors' lab. ROAFTS is a middleware subsystem which is layered above a COTS (commercial-off-the-shelf) operating system (OS), such as Windows XP or UNIX, and functions as the core of a reliable RT execution engine for fault-tolerant (FT) distributed RT applications. The applications supported by ROAFTS are structured as a network of RT objects, named time-triggered message-triggered objects (TMOs). The structure of the prototype implementation of the SNS scheme is discussed first, then a rigorous analysis of the time bounds for fault detection and recovery is provided.
[Real time systems, Ethernet networks, fault detection, local area networks, reliable real-time execution engine, Distributed computing, Delay, Fault tolerance, distributed real-time system, network operating systems, Computer architecture, semi-centralized network surveillance scheme, Computer networks, fault detection latency bounds, real-time object-oriented adaptive fault-tolerance support, surveillance, middleware, object-oriented programming, computing components, time bounds, time-triggered message-triggered objects, Middleware, health status detection, software fault tolerance, UNIX, fault recovery, Fault detection, Surveillance, real-time systems, COTS operating system, Windows XP, Ethernet based middleware system, supervisor-based network surveillance scheme]
On node state reconstruction for fault tolerant distributed algorithms
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
One of the main methods for achieving fault tolerance in distributed systems is recovery of the state of failed components. Though generic recovery methods like checkpointing and message logging exist, in many cases the recovery has to be application specific. In this paper we propose a general model for a node state reconstruction after crash failures. In our model the reconstruction operation is defined only by the requirements it fulfills, without referring to the specific application dependent way it is performed. The model provides a framework for formal treatment of algorithm-specific and system-specific recovery procedures. It is used to specify node state reconstruction procedures for several widely used distributed algorithms and systems, as well as to prove their correctness.
[Checkpointing, Algorithm design and analysis, checkpointing, fault tolerance, Software algorithms, Switches, Computer crashes, recovery, system recovery, state reconstruction, software fault tolerance, Computer science, message logging, Fault tolerance, Fault tolerant systems, distributed algorithms, distributed systems, Hardware, node state reconstruction, Distributed algorithms]
Tolerance to unbounded Byzantine faults
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
An ideal approach to deal with faults in large-scale distributed systems is to contain the effects of faults as locally as possible and, additionally, to ensure some type of tolerance within each fault-affected locality. Existing results using this approach accommodate only limited faults (such as crashes) or assume that fault occurrence is bounded in space and/or time. In this paper, we define and explore possibility/impossibility of local tolerance with respect to arbitrary faults (such as Byzantine faults) whose occurrence may be unbounded in space and in time. Our positive results include programs for graph coloring and dining philosophers, with proofs that the size of their tolerance locality is optimal. The type of tolerance achieved within fault-affected localities is self-stabilization. That is, starting from an arbitrary state of the distributed system, each non-faulty process eventually reaches a state from where it behaves correctly as long as the only faults that occur henceforth (regardless of their number) are outside the locality of this process.
[Art, dining philosophers programs, self-stabilization, distributed processing, Topology, local tolerance, arbitrary faults, large-scale distributed systems, Distributed computing, software fault tolerance, graph colouring, Computer science, Information science, resource allocation, graph coloring programs, Lungs, unbounded Byzantine fault tolerance, nonfaulty process, concurrency control, Large-scale systems, Safety, Contracts]
A lower bound on dynamic k-stabilization in asynchronous systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
It is desirable that the smaller the number of faults hitting a network, the faster a network protocol recovers. We study the scenario where up to k (for a given k) faults hit processors of a synchronous distributed system by corrupting their state undetectably. In this context, we show that the well known step complexity model is not appropriate to study time complexity of time-adaptive protocols (i.e. protocols that recover from memory corruption in a time that depends only on the number of faults and not on the network size). In more detail, we prove that for nontrivial dynamic problems (such as token passing), there exists a lower bound of /spl Omega/(D) (where D is the network diameter) steps on the stabilization time even when as few as 1 corruption can hit the system. This implies that there exists no time adaptive protocol for those problems in the asynchronous step model, even if we assume that the number of faults is bounded by 1 and that the scheduling of the processors is almost synchronous (between two actions of an enabled processor any other processor may execute at most one action).
[Protocols, Taxonomy, time-adaptive protocols, token passing, distributed processing, faults, processor scheduling, network, Iris, Modems, Robustness, protocols, asynchronous systems, Hamming distance, dynamic k-stabilization, time complexity, lower bound, Resilience, Upper bound, Processor scheduling, Fault detection, nontrivial dynamic problems, fault tolerant computing, asynchronous step model, network protocol recovery, stabilization time, computational complexity]
A search for routing strategies in a peer-to-peer network using genetic programming
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Results taken from a simulated peer-to-peer network are described, in which genetic programming is utilized to evolve routing strategies that optimize resource location in various traffic flow scenarios. In all cases the evolved strategies result in more numerous resource locations than a pure, non-adaptive peer-to-peer protocol such as the Gnutella protocol. The resulting evolved strategies are described, and empirical validation of the Gnutella protocol is given via both its creation through machine-learning techniques, and through the analysis of real-world constants used in the protocol.
[Content based retrieval, genetic programming, Law, Stability, Peer to peer computing, resource location optimization, computer networks, genetic algorithms, traffic flow scenarios, machine learning techniques, Gnutella protocol, Intelligent networks, Genetic programming, telecommunication network routing, routing strategies, Broadcasting, Routing protocols, Performance analysis, protocols, learning (artificial intelligence), discrete event simulation, Legal factors, simulated peer-to-peer network]
Heterogeneous checkpointing for multithreaded applications
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
We present the first heterogeneous checkpointing scheme for applications using POSIX threads. The scheme relies on source code instrumentation to achieve heterogeneity. It supports various types of synchronization primitives, such as locks, semaphores, condition variables, and join operations. Unlike other non-heterogeneous checkpointing schemes proposed in the literature, our scheme supports both kernel-level and application-level threads executing as part of the same application under various scheduling policies. Also, unlike other non-heterogeneous checkpointing mechanisms proposed in the literature, our solution does not interfere with the semantics of the application and does not use signals. Test results on various hardware platforms running Solaris, Linux, and Windows NT show that the overhead of our scheme is low.
[Checkpointing, Unix, Windows NT, source code instrumentation, semaphores, semantics, kernel-level threads, Yarn, system recovery, locks, processor scheduling, join operations, condition variables, hardware platforms, Hardware, application-level threads, Testing, operating system kernels, synchronization primitives, multi-threading, Instruments, heterogeneous checkpointing scheme, Solaris, Knowledge management, POSIX threads, Application software, synchronisation, Computer science, Multithreading, Linux, multithreaded applications]
A self-stabilizing algorithm for finding cliques in distributed systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Self-stabilization is a theoretical framework of non-masking fault-tolerant algorithms in distributed systems. In this paper, we consider a problem to find fully connected subgraphs (cliques) in a network. In our problem setting, each process P in a network G is given a set of its neighbor processes as input, and must find a set of neighbors that are fully connected together with P. As constraints on solutions which make the problem non-trivial, each process must compute larger cliques as possible, and a process P/sub j/ in a clique that a process P/sub i/ computes must agree on the result, i.e., the same clique must be obtained by P/sub j/. We present a self-stabilizing algorithm to find cliques, and show its correctness and performance.
[Algorithm design and analysis, Communication systems, Graph theory, fully connected subgraphs, software fault tolerance, self-stabilizing algorithm, Intelligent networks, Fault tolerance, Network topology, Tree graphs, nonmasking fault-tolerant algorithms, Fault tolerant systems, distributed algorithms, neighbor process input, algorithm performance, distributed systems, Computer networks, algorithm correctness, Distributed algorithms, clique finding]
Introspective failure analysis: avoiding correlated failures in peer-to-peer systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Failure independence is an important assumption for many fault tolerance techniques. Unfortunately, real systems exhibit correlated failures. In this paper, we present a framework for online discovery of groups of server nodes that are maximally independent in their failure characteristics. We discuss the framework in detail and provide a preliminary evaluation.
[Algorithm design and analysis, Availability, Protocols, fault tolerance techniques, correlated failures, Peer to peer computing, Redundancy, Computer science, Fault tolerance, peer-to-peer systems, Fault tolerant systems, Failure analysis, file servers, introspective failure analysis, fault tolerant computing, online server node group discovery, Internet, computer network reliability, Web server]
Random walk for self-stabilizing group communication in ad-hoc networks
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
We introduce a self-stabilizing group communication system for ad-hoc networks. The system design is based on random walks of mobile agents. Three possible settings for modeling the location of the processors in the ad-hoc network are presented; slow location change, complete random change, and neighbors with probability. The group membership algorithm is based on collecting and distributing information by a mobile agent. The new techniques support group membership and multicast, and also support resource allocation.
[Algorithm design and analysis, random walks, ad-hoc communication networks, self-stabilizing group communication, group membership, Computer crashes, Ad hoc networks, processors, Computer science, Intelligent networks, complete random change, Multicast algorithms, mobile computing, ad-hoc networks, neighbors with probability, Mobile agents, mobile agents, slow location change, wireless distributed systems, Computer networks, Communication networks, ad hoc networks, wireless networking, Mobile computing]
Loose synchronization of multithreaded replicas
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Although multithreading can improve performance, it is a source of nondeterminism in application behavior. Existing approaches to replicating multithreaded applications either synchronize replicas at the interrupt level, at the expense of performance, or use a nonpreemptive deterministic scheduler at the expense of concurrency. This paper presents a loose synchronization algorithm for ensuring deterministic replica behavior while preserving concurrency. The algorithm synchronizes replica threads only on state updates by enforcing an equivalent order of mutex acquisitions across replicas.
[multithreaded replicas, parallel algorithms, multi-threading, Transaction databases, Yarn, Middleware, state updates, Scheduling algorithm, synchronisation, concurrency, Concurrent computing, Fault tolerance, Multithreading, Processor scheduling, Operating systems, High performance computing, loose synchronization algorithm, concurrency control, multithreading, replica thread synchronization, mutex acquisitions, deterministic replica behavior, nondeterminism]
Efficient epidemic-style protocols for reliable and scalable multicast
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Epidemic-style (gossip-based) techniques have recently emerged as a scalable class of protocols for peer-to-peer reliable multicast dissemination in large process groups. These protocols provide probabilistic guarantees on reliability and scalability. However, popular implementations of epidemic-style dissemination are reputed to suffer from two major drawbacks: (a) (Network Overhead) when deployed on a WAN-wide or VPN-wide scale they generate a large number of packets that transit across the boundaries of multiple network domains (e.g., LANs, subnets, ASs), causing an overload on core network elements such as bridges, routers, and associated links; (b) (Lack of Adaptivity) they impose the same load on process group members and the network even under reduced failure rates (viz., packet losses, process failures). lit this paper we report on the (first) comprehensive set of solutions to these problems. The solution is comprised of two protocols: (1) a hierarchical gossiping protocol, and (2) an adaptive multicast dissemination framework that allows use of any gossiping primitive within it. These protocols work within a virtual peer-to-peer hierarchy called the Leaf Box hierarchy. Processes can be allocated in a topologically aware manner to the leaf boxes of this structure, so that (1) and (2) produce low traffic across domain boundaries in the network. In the interests of space, this paper focuses on a detailed discussion and evaluation (through simulations) of only the hierarchical gossiping protocol. We present an overview of the adaptive dissemination protocol and its properties.
[virtual peer-to-peer hierarchy, gossip-based techniques, reliable multicast, reliability, multiple network domains, Distributed computing, Computer network reliability, Distributed databases, hierarchical gossiping protocol, process groups, Large-scale systems, computer network reliability, scalable multicast, Leaf Box hierarchy, adaptive multicast dissemination framework, adaptive dissemination protocol, Peer to peer computing, Multicast protocols, Computer science, Bridges, Tiles, multicast protocols, peer-to-peer reliable multicast dissemination, probabilistic guarantees, Internet, epidemic-style protocols]
Failure detectors for large-scale distributed systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This paper discusses the problem of implementing a scalable failure detection service for grid systems. More specifically, traditional implementations of failure detectors are often tuned for running over local networks and fail to address important problems found in wide-area distributed systems, such as grid systems. We identify some of the most important problems raised in the context of grids. We then survey recent propositions that can help in solving some of these problems.
[Protocols, wide area networks, Computerized monitoring, failure detectors, grid systems, Computer crashes, large-scale distributed systems, Distributed computing, wide-area distributed systems, Condition monitoring, Information science, Detectors, Grid computing, Computer networks, fault tolerant computing, Large-scale systems, computer network reliability, scalable failure detection service]
Optimistic Byzantine agreement
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
The paper considers the Byzantine agreement problem in a fully asynchronous network, where some participants may be actively malicious. This is an important building block for fault-tolerant applications in a hostile environment, and a non-trivial problem: An early result by Fischer et al. (1985) shows that there is no deterministic solution in a fully asynchronous network subject to even a single crash failure. The paper introduces an optimistic protocol that combines the two best known techniques to solve agreement, randomization and timing. The timing information is used only to increase performance; safety and liveness of the protocol are guaranteed independently of timing. Under certain "normal" conditions, the protocol decides quickly and deterministically without using public-key cryptography, approximately as fast as a timed protocol subject to crash failures does. Otherwise, a randomized fallback protocol ensures safety and liveness. For this, we present an optimized version of the randomized Byzantine agreement protocol of Cachin et al. (2000), which is computationally less expensive and not only tolerates malicious parties, but also some loss of messages; it might therefore be of independent interest.
[Uncertainty, Laboratories, crash failure, randomized fallback protocol, Fault tolerance, randomization, safety, Detectors, malicious parties, Robustness, hostile environment, Safety, computer network reliability, Cryptography, protocols, optimistic protocol, timing, cryptography, Computer crashes, software fault tolerance, Cryptographic protocols, deadlock freeness, concurrency control, fully asynchronous network, Timing, fault-tolerant applications, optimistic Byzantine agreement]
Dependability of CORBA systems: service characterization by fault injection
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
The dependability of CORBA systems is a crucial issue for the development of today's distributed platforms and applications. This paper analyzes various techniques that can be applied to the dependability evaluation of CORBA systems. Due to the complexity of a middleware platform like CORBA and its various types of software components, experiments using several fault injection techniques are required to obtain comprehensive dependability benchmarks. To illustrate one of these techniques, we have applied fault injection at the communication level, targeting requests to major CORBA services, such as naming and events. Experiments have been carried out on a number of off-the-shelf implementations of CORBA. We present and discuss some of the results that we have obtained. They provide objective insights into the system's behaviour in the presence of faults, and are significant inputs for the selection of a candidate for a given application domain.
[software reliability, experiments, Application software, Middleware, service characterization, Fault diagnosis, CORBA systems dependability, Operating systems, Fault tolerant systems, Failure analysis, naming, off-the-shelf implementations, Robustness, fault injection, dependability benchmarks, software components, distributed object management, software performance evaluation, Testing, middleware]
Implementation of threshold-based diagnostic mechanisms for COTS-based applications
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This work investigates feasibility issues that must be addressed when threshold-based mechanisms are to be used for diagnostic purposes in COTS-based distributed systems. Threshold based mechanisms have typically been used for such purposes in embedded systems. A variety of solutions exist, with different characteristics of completeness, accuracy, and induced overhead. We first discuss the challenges related to applying such mechanisms to COTS-based distributed applications. We then identify alternative strategies for diagnosis, which use run-time data on COTS component service failures to trigger alarms to reconfiguration and fault treatment mechanisms. We implement those strategies in a system prototype, which is based on a substantial application, i.e. a real world (as opposed to a toy) application. We discuss the relationships between the sensitivity of the quality of service (QoS) provided by the diagnostic mechanisms and the accuracy of the available failure data. Our considerations and preliminary experiments on the prototype suggest that a careful evaluation of tradeoffs must be conducted, in order to achieve the best compromise between accuracy and cost, which depends on application characteristics, and service deployment requirements.
[Costs, Quality of service, experiments, reconfiguration, fault treatment mechanisms, Information systems, Fault diagnosis, Runtime, Embedded system, Prototypes, embedded systems, Computer architecture, software packages, COTS-based distributed systems, Bonding, distributed object management, program diagnostics, COTS component service failures, alarms, quality of service, Application software, service deployment requirements, software fault tolerance, threshold-based diagnostic mechanisms, CORBA, distributed architectures, run-time data]
Workshop on reliable peer-to-peer distributed systems
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
false
[]
Implementing quality of service in Web servers
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This paper analyzes the importance of differentiated service implementation in current Web servers, within the context of new Internet applications. An innovative approach of resource management is presented, which is carried out using the proportional cumulative value attribution (PCV) heuristic. This heuristic acts through two policies which, in response to requests, simultaneously sort the requests according to dynamically assigned priorities; and select different versions of Web pages.
[Context-aware services, resource management, dynamically assigned priorities, Web page selection, Scalability, Quality of service, Internet applications, Web servers, quality of service, Electronic commerce, processor scheduling, Aggregates, Web and internet services, Web pages, file servers, Communications technology, proportional cumulative value attribution heuristic, Internet, Resource management, request sorting, Web server, electronic commerce]
Asynchronous resource discovery in peer to peer networks
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
The resource discovery problem arises in the context of peer to peer (P2P) networks, where at any point of time a peer may be placed at or removed from any location over a general purpose network (e.g., an Internet site). A vertex (peer) can communicate with another vertex directly if and only if it knows a certain routing information to that other vertex. Hence, it is critical for peers to convey this routing information to each other. The problem was formalized by Harchol-Balter et al. (1999). The routing information needed for a vertex to reach another peer is that peer's identifier (e.g., IP address). A logical directed edge represents the fact that the peer at the tail of the edge knows the IP address of the one at its head. A number of algorithms were developed by Harchol-Balter et al. for this problem in the model of a synchronous network over a weakly connected directed graph. The best of these algorithms was randomized. Subsequently, a deterministic algorithm for the problem on synchronous networks with improved complexity was presented by Kutten et al. (2001). The current paper extends this deterministic algorithm to the environment of asynchronous networks, maintaining similar complexities (translated to the asynchronous model). These are lower than the complexities that would be needed to synchronize the system. The main technical difficulty in a directed, weakly connected system is to ensure that vertices take consistent steps, even if their knowledge about each other is not symmetric, and even if there is no timeout mechanism (which does exist in synchronous systems) to assist in that.
[complexity, vertex, peer to peer networks, Intelligent networks, Tail, IP networks, Web server, Context, logical directed edge, Peer to peer computing, Routing, asynchronous resource discovery, IP address, deterministic algorithm, deterministic algorithms, directed graphs, telecommunication network routing, weakly connected directed graph, randomized algorithms, Computer industry, synchronous network, Internet, routing information, Computer network management, computational complexity]
Non-intrusive, parallel recovery of replicated data
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
The increasingly widespread use of cluster architectures has resulted in many new application scenarios for data replication. While data replication is, in principle, a well understood problem. recovery of replicated systems has not yet received enough attention. In the case of clusters, recovery procedures are particularly important since they have to keep a high level of availability even during recovery. In fact, recovery is part of the normal operations of any cluster as the cluster is expected to continue working while sites leave or join the system. However, traditional recovery techniques usually require stopping processing. Once a quiescent state has been reached, the system proceeds to synchronize the state of failed or new replicas. In this paper. we concentrate on how to perform recovery in a replication middleware without having to stop processing. The proposed protocol focuses on how to minimize the redundancies that take place during concurrent recovery of several sites.
[Availability, Protocols, replicated databases, Scalability, Service oriented architecture, nonintrusive recovery, Transaction databases, quiescent state, replication middleware, Application software, Middleware, Computer science, protocol, cluster architectures, Fault tolerant systems, Computer architecture, protocols, parallel recovery, middleware, replicated data]
Service time optimal self-stabilizing token circulation protocol on anonymous undirectional rings
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
We present a self-stabilizing token circulation protocol on unidirectional anonymous rings. This protocol requires no processor identifiers or distinguished processor (i.e. all processors perform the same algorithm). The protocol is randomized and self-stabilizing, meaning that starting from an arbitrary configuration (in response to an arbitrary perturbation modifying the memory state), it reaches (with probability 1) a legitimate configuration (i.e. a configuration with only one token in the network). All previous randomized self-stabilizing token circulation protocols designed to work under unfair distributed schedulers have the same drawback: once stabilized, service time is slow (in the best case, it is bounded by 2N where N is the ring size). Once stabilized, our protocol provides an optimal service: after N computation steps, each processor has obtained the token once. The protocol can be used to implement fair distributed mutual exclusion in any ring topology network.
[randomized self-stabilizing protocol, Nominations and elections, service time, Access protocols, legitimate configuration, unidirectional anonymous rings, processor scheduling, software fault tolerance, unfair distributed schedulers, Processor scheduling, Network topology, distributed algorithms, computation steps, Modems, Robustness, protocols, service time optimal self-stabilizing token circulation protocol, computational complexity]
Efficient Byzantine-resilient reliable multicast on a hybrid failure model
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
The paper presents a new reliable multicast protocol that tolerates arbitrary faults, including Byzantine faults. This protocol is developed using a novel way of designing secure protocols which is based on a well-founded hybrid failure model. Despite our claim of arbitrary failure resilience, the protocol need not necessarily incur the cost of "Byzantine agreement\
[Protocols, Costs, crash failure model, Laboratories, efficient Byzantine-resilient reliable multicast, Trusted Timely Computing Base, Computer crashes, hybrid failure model, Byzantine faults, arbitrary fault tolerance, Delay, software fault tolerance, Resilience, Lungs, multicast protocols, TTCB, distributed security kernel, Kernel, Protection, Informatics, timed protocols, reliable multicast protocol]
Optimistic total order in wide area networks
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Total order multicast greatly simplifies the implementation of fault-tolerant services using the replicated state machine approach. The additional latency of total ordering can be masked by taking advantage of spontaneous ordering observed in LANs: A tentative delivery allows the application to proceed in parallel with the ordering protocol. The effectiveness of the technique rests on the optimistic assumption that a large share of correctly ordered tentative deliveries offsets the cost of undoing the effect of mistakes. This paper proposes a simple technique which enables the usage of optimistic delivery also in WANs with much larger transmission delays where the optimistic assumption does not normally hold. Our proposal exploits local clocks and the stability of network delays to reduce the mistakes in the ordering of tentative deliveries. An experimental evaluation of a modified sequencer-based protocol is presented, illustrating the usefulness of the approach in fault-tolerant database management.
[Wide area networks, modified sequencer-based protocol, Protocols, Stability, wide area networks, optimistic assumption, latency, Transaction databases, network delays, Delay, multicast, Intelligent networks, Fault tolerance, optimistic delivery, Computer hacking, Cost function, LANs, fault tolerant computing, protocols, fault-tolerant services, fault-tolerant database management, Clocks]
Management of mobile agent systems using social insect metaphors
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
The management of mobile agent systems that solve problems in a network is an issue that must be addressed if mobile agents are to be deployed industrially. It is clear that insufficient or excessive numbers of agents can cause the problem solving capabilities of an agent-based system to be impaired. Also, agents being software entities are almost always flawed therefore requiring the upgrade problem to be solved. This paper presents distributed algorithms based upon ant social behaviour that solve the problems of agent density maintenance and the agent upgrade problem.
[mobile agent systems management, Drives, Control systems, problem solving, Distributed computing, software agents, agent density maintenance, Computer science, Insects, Engineering management, Mobile agents, distributed algorithms, mobile agents, upgrade problem, Systems engineering and theory, Computer networks, agent upgrade problem, self-organizing agents, Computer network management, agent-based system, software entities]
Using file-grain connectivity to implement a peer-to-peer file system
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Recent work has demonstrated a peer-to-peer storage system that locates data objects using O(logN) messages by placing objects on nodes according to pseudo-randomly chosen IDs. While elegant, this approach constrains system functionality and flexibility: files are immutable, directories and symbolic names are not supported, data location is fixed, and access locality is not exploited. This paper presents Mammoth, a peer-to-peer hierarchical file system that, unlike alternative approaches, supports a traditional file-system API, allows files and directories to be stored on any node, and adapts storage location to exploit locality, balance load, and ensure availability. Our approach handles all coordination at the granularity of files instead of nodes. In effect, the nodes that store a particular file act as its server independently of other nodes in the system. The resulting system is highly available and robust to failure. Our experiments with our prototype have yielded good results, but an important question remains: how the system will perform on a massive scale. We discuss the key issues, some of which we have addressed and others that remain open.
[metadata, application program interfaces, wide area networks, load balancing, Scalability, experiments, storage management, File systems, Network topology, resource allocation, Intrusion detection, Prototypes, distributed databases, distributed systems, directories, Robustness, Availability, meta data, fine-grain connectivity, access locality, fault tolerance, Peer to peer computing, system failure, peer-to-peer hierarchical file system, software fault tolerance, Computer science, system functionality, Mammoth, Load management, messages, API, data location]
An indulgent uniform total order algorithm with optimistic delivery
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
A total order algorithm is a fundamental building block in the construction of distributed fault-tolerant applications. Unfortunately, the implementation of such a primitive can be expensive both in terms of communication steps and of number of messages exchanged. This problem is exacerbated in large-scale systems, where the performance of the algorithm may be limited by the presence of high-latency links. Typically, the most efficient total order algorithms do not provide uniform delivery and assume the availability of a perfect failure detector. Such algorithms may provide inconsistent results if the system assumptions do not hold. On the other hand, algorithms that assume an unreliable failure detector always provide consistent results but exhibit higher costs. This paper presents a new algorithm that combines the advantages of both approaches. On good periods, when the system is stable and processes are not suspected, the algorithm operates as if a perfect failure detector is assumed. Yet, the algorithm is indulgent, since it never violates consistency, even in runs where processes are suspected.
[Availability, Algorithm design and analysis, Costs, Object oriented modeling, distributed processing, performance evaluation, Computer crashes, Total Order Broadcast problem, large-scale systems, Fault tolerance, failure detector, Detectors, Broadcasting, fault tolerant computing, dynamic reconfiguration, Large-scale systems, Safety, total order algorithm, distributed fault-tolerant]
A fault-tolerant approach to secure information retrieval
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Several private information retrieval (PIR) schemes were proposed to protect users' privacy when sensitive information stored in database servers is retrieved. However, existing PIR schemes assume that any attack to the servers does not change the information stored and any computational results. We present a novel fault-tolerant PIR scheme (called FT-PIR) that protects users' privacy and at the same time ensures service availability in the presence of malicious server faults. Our scheme neither relies on any unproven cryptographic assumptions nor the availability of tamper-proof hardware. A probabilistic verification function is introduced into the scheme to detect corrupted results. Unlike previous PIR research that attempted mainly to demonstrate the theoretical feasibility of PIR, we have actually implemented both a PIR scheme and our FT-PIR scheme in a distributed database environment. The experimental and analytical results show that only modest performance overhead is introduced by FT-PIR while comparing with PIR in the fault-free cases. The FT-PIR scheme tolerates a variety of server faults effectively. In certain fail-stop fault scenarios, FT-PIR performs even better than PIR. It was observed that 35.82% less processing time was actually needed for FT-PIR to tolerate one server fault.
[Data privacy, query processing, Fault tolerance, Distributed databases, file servers, Management information systems, distributed databases, probabilistic verification function, Hardware, Protection, user privacy protection, client-server systems, fault-tolerant private information retrieval scheme, Data security, fail-stop fault scenarios, service availability, Information retrieval, sensitive information, software fault tolerance, Computer science, malicious server faults, performance overhead, security of data, secure information retrieval, Information security, corrupted results detection, database servers, data privacy, processing time, distributed database environment]
Self-stabilizing local mutual exclusion on networks in which process identifiers are not distinct
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
A self-stabilizing system is a system such that it autonomously converges to a legitimate system state, regardless of the initial system state. The local mutual exclusion problem is the problem of guaranteeing that no two processes neighboring each other execute their critical sections at a time. The process identifiers are said to be chromatic if no two processes neighboring each other have the same identifiers. Under the assumption that the process identifiers are chromatic, this paper proposes two self-stabilizing local mutual exclusion algorithms; one assumes a tree as the topology of communication network and requires 3 states per process, while the other which works on any communication network, requires n + 1 states per process, where n is the number of processes in the system. We also show that the process identifiers being chromatic is close to necessary for a system to have a self-stabilizing local mutual exclusion algorithm. We adopt the shared memory model for communication and the unfair distributed daemon for process scheduling.
[message passing, Resumes, self-stabilizing system, Control systems, Distributed computing, shared memory model, Intelligent networks, Fault tolerance, Network topology, Message passing, distributed algorithms, concurrency control, process identifiers, process scheduling, local mutual exclusion, fault tolerant computing, Communication networks, communication, fault tolerant distributed algorithms, Distributed algorithms, unfair distributed daemon]
Self-organizing formation algorithm for active elements
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
We propose a novel method of self-organizing formation. It is assumed that elements are not connected to each other and they can move in continuous space. The objective is to arrange elements in a certain spatial pattern like a crystal, and to make the outline of the group in the desired shape. For this purpose, we propose a method by using virtual springs among the elements. In this algorithm, an element generates virtual springs between the neighbor element based on information of how many other elements exist in the neighborhood with a certain radius. Although the elements interact locally, only by virtual springs, and they do not have global information at all, they form a shape much larger than the sensory radius. By a simulation study, we confirmed convergence to a target shape from a random state in very high probability. This kind of algorithm gives a new principle of self-organizing formation, and its simplicity will be useful for the design of self-assembling nano machines in future.
[Algorithm design and analysis, self-organizing formation algorithm, virtual springs, Shape, Extraterrestrial phenomena, Lattices, probability, simulation, active elements, Birds, self-adjusting systems, digital simulation, mobile robots, Mobile robots, spatial pattern, Proteins, Self-assembly, self-assembling nano machines, Springs, Assembly]
Building a reliable mutable file system on peer-to-peer storage
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
This paper sketches the design of the Eliot File System (Eliot), a mutable file system that maintains the pure immutability of its peer-to-peer (P2P) substrate by isolating mutation in an auxiliary metadata service. The immutability of address-to-content bindings has several advantages in P2P systems. However mutable file systems are desirable because they allow clients to update existing files; a necessary property for many applications. In order to facilitate modifications, the file system must provide some atom of mutability. Since this atom of mutability is a fundamental characteristic of the file system and not the underlying storage substrate, it is a mistake to violate the integrity of the substrate with special cases for mutability. Instead, Eliot employs a separate, generalized metadata service that isolates all mutation and client state in an auxiliary replicated database. Eliot provides fine-granularity file updates with either AFS open-close or NFS-like consistency semantics. Eliot builds a mutable filesystem on a global resource bed of purely immutable P2P block storage.
[wide area networks, Genetic mutations, software reliability, Computer crime, storage management, File systems, Databases, peer-to-peer storage, fine-granularity file updates, generalized metadata service, mutability, Protection, Wide area networks, meta data, replicated databases, Peer to peer computing, Buildings, reliable mutable file system, data integrity, Maintenance, immutable P2P block storage, Eliot File System, replicated database, address-to-content bindings, metadata service, P2P systems, consistency semantics]
A unified proof of minimum time complexity for reaching consensus and uniform consensus - an oracle-based approach
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
In this paper, we offer new proofs to two lower bound results in distributed computing: a minimum of f+1 and f+2 rounds for reaching consensus and uniform consensus respectively when at most f fail-stop faults can happen. Here the computation model is synchronous message passing. Both proofs are based on a novel oracle argument. These two induction proofs are unified in the following sense: the induction steps are the same and only the initial step (f=0) needs to be proved separately. The techniques used in the proof offer new insights into the lower bound results in distributed computing.
[Context, Algorithm design and analysis, message passing, fault tolerance, Computational modeling, Delay systems, uniform consensus, time complexity, Educational institutions, Computer crashes, Distributed computing, distributed computing, lower bounds, induction proofs, consensus, Fault tolerance, Message passing, concurrency control, fault tolerant computing, Mobile computing, synchronous message passing, oracle argument, computational complexity]
Reperasure: replication protocol using erasure-code in peer-to-peer storage network
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Peer-to-peer overlay networks offer a convenient way to host an infrastructure that can scale to the size of the Internet and yet stay manageable. These overlays are essentially self-organizing distributed hash tables (DHT). The dynamic nature of the system, however, poses serious challenges of data reliability. Furthermore, in order to see wider adoption, it is time to design support for generic replication mechanisms capable of handling arbitrary update requests - most of the existing proposals are deep archival systems in nature. Utilizing the fact that DHT can function as a super-reliable and high performance disk when data stored inside are erasure coded, we believe practical and simple protocols can be designed. In this paper, we introduce the reperasure protocol, a layer on top of the basic DHT, which efficiently supports strong consistency semantic with high availability guarantee. By relieving the DHT layer out of replication duo, inside, a cleaner overall architecture can be derived because of the clear division of responsibility.
[Availability, erasure-code, Protocols, generic replication mechanisms, replicated databases, self-organizing distributed hash tables, Peer to peer computing, replication protocol, peer-to-peer storage network, Proposals, reperasure protocol, Computer science, data reliability, Intelligent networks, Space technology, Asia, consistency semantic, peer-to-peer overlay networks, Internet, IP networks, Computer network management, protocols]
Active software replication through a three-tier approach
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
A replication logic is the set of protocols and mechanisms implementing a software replication technique. A three-tier approach to replication consists in separating the replication logic from both clients and replicated servers by embedding such logic in a middle-tier In this paper we first introduce the fundamental concepts underlying three-tier replication. This approach has two main practical advantages: (i) it allows to maintain consistency among the state of server replicas deployed within an asynchronous distributed system and (ii) it supports very thin clients. Then we present the Interoperable Replication Logic (IRL) architecture, which is a Fault Tolerant CORBA compliant infrastructure exploiting a three-tier approach to replicate stateful deterministic CORBA objects. In this context, we illustrate the three-tier replication protocol currently implemented in our IRL prototype and a performance analysis that shows the feasibility of the three-tier approach to software replication.
[Availability, software replication, client-server systems, Protocols, replicated servers, fault tolerant CORBA, asynchronous distributed system, replication logic, Computer crashes, Remuneration, Network servers, Fault tolerance, clients and servers, Computer architecture, Hardware, fault tolerant computing, Logic, Communication networks, protocols, distributed object management, server replicas]
Improving object search using hints, gossip, and supernodes
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Gnutella is a highly popular protocol for locating objects. It uses a non-scalable approach which results in either high loads or small yields. In this paper we present PALOCATE, an evolving protocol focusing on efficiency without sacrificing quality of recall. We present simulation studies showing the effectiveness of hint-based caching, gossip (epidemics), and incorporating supernodes into the basic protocol.
[Protocols, Peer to peer computing, simulation, hint-based caching, Wire, Computer science, query processing, nonscalable approach, epidemics, object search, peer-to-peer protocol, recall, Gnutella, gossip, protocols, supernodes, evolving protocol, PALOCATE]
Efficient distributed precision control in symmetric replication environments
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
Maintaining strict consistency of replicated data can be prohibitively expensive for many distributed applications and environments. In order to alleviate this problem, some systems allow applications to access stale, imprecise data. Due to relaxed correctness requirements, many applications can tolerate stale data but require that the imprecision be properly bounded. This paper describes ReBound, a system that provides an adaptive framework for supporting and exploiting data precision vs. efficiency tradeoffs in symmetric replication environments via distributed precision control. Previous work proposed efficient precision control algorithms that support continuous read requests tagged with custom numerical precision ranges. ReBound generalizes and extends previous work with a new algorithm for continuous reads, support for ad-hoc reads, and light-weight adaptation mechanisms for coping with dynamically changing update patterns. This paper also presents preliminary experimental results, based on a prototype implementation, that demonstrate the performance advantages of exploiting precision vs. efficiency tradeoffs.
[strict replicated data consistency, Base stations, client-server systems, symmetric replication environments, Costs, Computerized monitoring, ReBound, data precision/efficiency tradeoffs, data integrity, efficient distributed precision control, Application software, adaptive framework, Computer science, Temperature measurement, Temperature sensors, continuous reads, dynamically changing update patterns, distributed algorithms, Distributed control, ad-hoc reads, lightweight adaptation mechanisms, Marketing and sales, Personal digital assistants]
Availability models with age-dependent checkpointing
21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.
None
2002
In this paper, we consider a new stochastic model for file recovery action with checkpointing when a system failure occurs according to a homogeneous Poisson process. The present checkpoint model strongly depends on the system age and is quite different from the models by Gelenbe (1979) and Goes and Sumita (1995). We propose three kinds of approximation schemes to determine the optimal checkpoint interval which maximizes system availability, taking account of queueing effect due to idle periods in the transaction processing system. In numerical examples, the checkpoint model based on three approximation schemes is compared with earlier models quantitatively, and it is shown that it can reduce system overhead which may occur in unplanned system downtime.
[Checkpointing, transaction processing, queueing effect, unplanned system downtime, system recovery, approximation schemes, File systems, Databases, Fault tolerant systems, homogeneous Poisson process, optimal checkpoint interval, idle periods, Dynamic programming, stochastic processes, Informatics, Availability, transaction processing system, Educational programs, age-dependent checkpointing, Data processing, Maintenance, software fault tolerance, file recovery action, availability models, stochastic model]
Pesto flavored security
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
We demonstrate that symmetric-key cryptography can be used for both read and write access control. One-time write access can be granted by handing over an encryption key, and our encryption framework allows the revocation of previously granted rights. The number of keys to be managed explicitly grows linearly with the number of access control policies a user defines, making security manageable. The framework is used in the Pesto distributed storage system. In Pesto, policies can be stored the same as other data and the same mechanism can be used to control access to them. Delegation of authority over policies concerning different tasks can then be performed. Separating the different tasks of the system, allows for different tasks to be assigned to different sets of nodes. Nodes need then only be trusted with respect to the specific task(s) they have been assigned with.
[Access control, Availability, Data security, read access control, distributed processing, symmetric-key cryptography, Pesto flavored security, Secure storage, Computer science, Authorization, storage management, encryption framework, public key cryptography, Authentication, message authentication, Pesto distributed storage system, authorisation, Safety, Cryptography, Contracts, write access control, encryption key]
Hazard analysis of complex distributed railway systems
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
To operate real-time, distributed, safety critical systems, their logical and temporal correctness must be validated against strict safety requirements. International committees, like CENELEC, produced standards that define appropriate life cycle and techniques to be used in all the phases of development and V&V process. However the guidelines given by the norms are quite general: a more detailed methodology is needed to exhaustibly cover all the aspects of complex system. This paper describes the hazard analysis methodology defined and used in ASF (Ansaldo Segnalamento Ferroviario) and the results obtained by its application to the ERTMS/ETCS system. This methodology is divided in several phases: first, all the functional and architectural components and their interfaces are identified, then all possible hazard scenarios are identified. These scenarios are then analyzed in a series of hazard workshops and traced in a log, the hazard log, which records also measures needed to mitigate them. Mitigations become new requirements for the systems: only providing evidence of their correct implementation the system can be certified to be safe.
[Real time systems, safety-critical software, distributed processing, formal specification, Guidelines, hazard scenarios, formal verification, complex systems, Microprocessors, ASF, hazard analysis, Rail transportation, temporal correctness, Safety, Performance analysis, Standards development, Logic, safety critical systems, hazard log, International Committee, Hazards, strict safety requirements, Ansaldo Segnalamento Ferroviario, ERTMS/ETCS system, railway safety, correctness validation, logical correctness, correct implementation]
Fault tolerance technology for autonomous decentralized database systems
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
The Autonomous Decentralized Database System (ADDS) has been proposed in the background of e-business in respect to the dynamic and heterogeneous requirements of the users. With the rapid development of information technology, different companies in the field of e-business are supposed to cooperate in order to cope with the continuous changing demands of services in a dynamic market. In a diversified environment of service provision and service access, the ADDS provides flexibility to integrate heterogeneous and autonomous systems while assuring timeliness and high availability. A loosely-consistency management technology confers autonomy to each site for updating while maintaining the consistency of the whole system. Moreover, a background coordination technology, by utilizing a mobile agent, has been devised to permit the sites to coordinate and cooperate with each other while conferring the online property. The use of mobile agent, however, is critical and requires reliability with regard to mobile agent failures that may lead to bad response times and hence the availability of the system may lost. A fault tolerance technology is proposed in order that the system autonomously detect and recover the fault of the mobile agent due to a failure in a transmission link, site or bug in the software. The effectiveness of the proposition is shown by simulation.
[service provision, information technology, fault detection, ADDS, system recovery, e-business, software fault tolerance, loosely-consistency management, fault recovery, Fault tolerant systems, fault tolerance technology, mobile agent, transmission failure, mobile agents, distributed databases, Autonomous Decentralized Database Systems, Database systems, service access, electronic commerce]
A reliable key authentication schema for secure multicast communications
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
The paper analyzes the Logical Key Hierarchy (LKH) secure multicast protocol focusing on the reliability of the re-keying authentication process. We show that the key management in the LKH model is subject to some attacks. In particular, these attacks can be performed by entities external to the multicast group, as well as from internal users of the multicast group. The spectrum of these attacks is spread from the denial of service (DoS) to the session hijack that is the attacker is able to have legitimate users to commit on a session key that is provided by the attacker. The contributions of this paper are: (1) the definition of the threats the LKH key management is subject to; and (2) a reliable key authentication scheme that solves the weaknesses previously identified. This objective is achieved without resorting to public key signatures.
[TV, reliable key authentication, Multicast communication, LKH, session hijack, Multicast protocols, DoS, Security, Remuneration, key management, Computer crime, secure multicast communications, multicast group, public key cryptography, logical key hierarchy, Web and internet services, multicast protocols, Authentication, Public key, message authentication, Public key cryptography, computer network reliability, Internet, denial of service]
An experimental evaluation of correlated network partitions in the Coda distributed file system
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Experimental evaluation is an important way to assess distributed systems, and fault injection is the dominant technique in this area for the evaluation of a system's dependability. For distributed systems, network failure is an important fault model. Physical network failures often have far-reaching effects, giving rise to multiple correlated failures as seen by higher-level protocols. This paper presents an experimental evaluation, using the Loki fault injector, which provides insight into the impact that correlated network partitions have on the Coda distributed file system. In this evaluation, Loki created a network partition between two Coda file servers, during which updates were made at each server to the same replicated data volume. Upon repair of the partition, a client requested directory resolution to converge the diverging replicas. At various stages of the resolution, Loki invoked a second correlated network partition, thus allowing us to evaluate its impact on the system's correctness, performance, and availability.
[distributed file systems, Telecommunication traffic, File servers, Mechanical engineering, network failure, experimental evaluation, multiple correlated failures, Intelligent networks, Network servers, Runtime, File systems, replicated data, Availability, client-server systems, replicated databases, performance evaluation, Educational institutions, network topology, Coda distributed file system, software fault tolerance, fault model, Loki fault injector, system performance evaluation, correlated network partitions, Mobile computing]
Transparent fault-tolerant Java virtual machine
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Replication is one of the prominent approaches for obtaining fault tolerance. Implementing replication on commodity hardware and in a transparent fashion, i.e., without changing the programming model, has many challenges. Deciding at what level to implement the replication has ramifications on development costs and portability of the programs. Other difficulties lie in the coordination of the copies in the face of non-determinism. We report on an implementation of transparent fault tolerance at the virtual machine level of Java. We describe the design of the system and present performance results that in certain cases are equivalent to those of non-replicated executions. We also discuss design decisions stemming from implementing replication at the virtual machine level, and the special considerations necessary in order to support symmetric multi-processors (SMP).
[Java, Costs, multiprocessing systems, symmetric multiprocessors, system design, active replication, Virtual machining, Application software, transparent fault-tolerant, software fault tolerance, design decisions, Fault tolerance, Virtual machine monitors, Operating systems, Fault tolerant systems, virtual machines, SMP, Hardware, Java virtual machine, Logic]
Federation Web: a scheme to compound authorization chains on large-scale distributed systems
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Traditional security systems are not easily scalable and can become single points of failure or performance bottlenecks when used on a large-scale distributed system such as the Internet. This problem occurs also when using a public key infrastructure (PKI) with a hierarchical thrust model. SDSI/SPKI is a PKI that adopts a more scalable trust paradigm, which is focused on the client and based on authorization chains. However, the task of locating the chain that links a client to a server is not completely addressed by SDSI/SPKI. Aiming to overcome this limitation, the paper proposes extensions to the SDSI/SPKI authorization and authentication model. The proposed approach introduces the concept of Federation Webs, which allows the client to build new authorization chains linking it to a server when a direct path does not exist. A prototype implementation of this proposal has shown promising results.
[telecommunication security, SDSI, Scalability, public key infrastructure, Authorization, scalable trust paradigm, public key cryptography, authorisation, compound authorization chains, security systems, Large-scale systems, Computer security, SPKI, authorization model, Federation Web, Automation, large-scale distributed systems, PKI, Computer science, authentication model, Authentication, Public key, message authentication, Internet, Joining processes]
Sharing memory with semi-Byzantine clients and faulty storage servers
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
This paper presents several fault-tolerant simulations of a single-writer multi-reader regular register in storage systems. One simulation tolerates fail-stop failures of storage servers and require a majority of nonfaulty servers, while the other simulation tolerates Byzantine failures and requires that two-thirds of the servers to be nonfaulty. A construction of Afek et al.(1995) is used to mask semi-Byzantine failures of clients that result in erroneous write operations. The simulations are used to derive Paxos algorithms that tolerate semi-Byzantine failures of clients as well as failstop or Byzantine failures of storage servers.
[fault-tolerant simulations, fail-stop failures, Laboratories, Byzantine failure, File servers, Registers, Delay, Fault tolerance, Fault tolerant systems, semiByzantine clients, shared memory systems, redundancy, client-server systems, Computational modeling, Computer simulation, Redundancy, faulty storage servers, memory sharing, Computer science, Paxos algorithms, storage area networks, erroneous write operations, fault tolerant computing, fault simulation, single-writer multireader register]
A reconfigurable Byzantine quorum approach for the Agile Store
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Quorum-based protocols can be used to manage data when it is replicated at multiple server nodes to improve availability and performance. If some server nodes can be compromised by a malicious adversary, Byzantine quorums must be used to ensure correct access to replicated data. This paper introduces reconfigurable Byzantine quorums, which allow various quorum protocol parameters to be adapted based on the behavior of compromised nodes and the performance needs of the system. We present a protocol that generalizes dynamic Byzantine quorums by allowing the system size to change as faulty servers are removed from the system, in addition to adapting the fault threshold. A new architecture and algorithm that provide the capability to detect and remove faulty servers are also described. Finally, simulation results are presented that demonstrate the benefits offered by our approach.
[Availability, Pervasive computing, multiple server nodes, client-server systems, reconfigurable Byzantine quorum, Change detection algorithms, quorum-based protocols, simulation, Access protocols, Educational institutions, Data engineering, Application software, software fault tolerance, malicious adversary, Technology management, Fault detection, Engineering management, Agile Store, protocols, faulty servers]
Trustworthiness of open information systems: how should it be achieved?
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
false
[Computer architecture, Security, Distributed computing, Information systems]
AMSD: a dependability roadmap for the information society in europe
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
false
[Costs, Europe, Information security, Software quality, Economic forecasting, Control systems, Safety, Computer security, Aircraft, Embedded software]
Reliable peer-to-peer information monitoring through replication
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
A key challenge in peer-to-peer computing systems is to provide a decentralized and yet reliable service on top of a network of loosely coupled, weakly connected and possibly unreliable peers. This paper presents an effective dynamic passive replication scheme designed to provide reliable service in PeerCQ, a decentralized and self-configurable peer-to-peer Internet information monitoring system. We first describe the design of a distributed replication scheme, which enables reliable processing of long-running information monitoring requests in an environment of inherently unreliable peers. Then we present an analytical model to discuss its fault tolerance properties. A set of initial experiments is reported, showing the feasibility and the effectiveness of the proposed approach.
[Costs, fault tolerance, Peer to peer computing, information monitoring system, Educational institutions, loosely coupled peers, Application software, analytical model, Distributed computing, peer-to-peer computing systems, Condition monitoring, Analytical models, Fault tolerance, Web and internet services, multicast communication, peer-to-peer information monitoring, Computer networks, fault tolerant computing, Internet, replication scheme]
Autonomous replication for high availability in unstructured P2P systems
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
We consider the problem of increasing the availability of shared data in peer-to-peer systems. In particular, we conservatively estimate the amount of excess storage required to achieve a practical availability of 99.9% by studying a decentralized algorithm that only depends on a modest amount of loosely synchronized global state. Our algorithm uses randomized decisions extensively together with a novel application of an erasure code to tolerate autonomous peer actions as well as staleness in the loosely synchronized global state. We study the behavior of this algorithm in three distinct environments modeled on previously reported measurements. We show that while peers act autonomously, the community as a whole will reach a stable configuration. We also show that space is used fairly and efficiently, delivering three times availability at a cost of six times the storage footprint of the data collection when the average peer availability is only 24%.
[Availability, Costs, Peer to peer computing, Video sharing, autonomous replication, Extraterrestrial measurements, autonomous peer actions, stable configuration, data collection, Computer science, storage management, File systems, unstructured P2P systems, peer-to-peer systems, distributed algorithms, storage footprint, Time sharing computer systems, shared data availability, peer availability, Internet, computer network reliability, State estimation, loosely synchronized global state, erasure code]
Performance and effectiveness analysis of checkpointing in mobile environments
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Many mathematical models have been proposed to evaluate the execution performance of an application with and without checkpointing in the presence of failures. They assume that the total program execution time without failure is known in advance, under which condition the optimal checkpointing interval can be determined. In mobile environments, application components are distributed and tasks are computed by sending and receiving computational and control messages. The total execution time includes communication time and depends on multiple factor, such as heterogeneous processing speeds, link bandwidth, etc., making it unpredictable during different executions. However, the number of total computational messages received is usually unchanged within an application. Another special factor that should be considered for checkpointing purpose is handoff, which often happens in mobile networks. With these observations, we analyze application execution performance and average effectiveness, and introduce an equi-number checkpointing strategy. We show how checkpointing and handoff affect performance and effectiveness metrics, determine the conditions when checkpointing is beneficial, and calculate the optimal checkpointing interval for minimizing the total execution time and maximizing the average effectiveness in mobile environments.
[Checkpointing, effectiveness analysis, checkpointing, mobile environments, Portable computers, application components, performance evaluation, mobile networks, Application software, Distributed computing, system recovery, computational messages, Computer science, mobile computing, mobile communication, communication time, control messages, Bandwidth, Computer networks, Performance analysis, Mathematical model, Mobile computing]
Randomized asynchronous consensus with imperfect communications
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
We introduce a novel hybrid failure model, which facilitates an accurate and detailed analysis of round-based synchronous, partially synchronous and asynchronous distributed algorithms under both process and link failures. Granting every process in the system up to f/sub /spl lscr// send and receive link failures (with f/sub /spl lscr///sup a/ arbitrary faulty ones among those) in every round, without being considered faulty, we show that the well-known randomized Byzantine agreement algorithm of (Srikanth & Toueg 1987) needs just n /spl ges/ 4f/sub /spl lscr// + 2ff/sub /spl lscr///sup a/+ 3f/sub a/ + 1 processes for coping with f/sub a/ Byzantine faulty processes. The probability of disagreement after R iterations is only 2/sup -R/, which is the same as in the FLP model and thus much smaller than the lower bound 0(1/R) known for synchronous systems with lossy links. Moreover, we show that 2-stubborn links are sufficient for this algorithm. Hence, contrasting widespread belief, a perfect communications subsystem is not required for efficiently solving randomized Byzantine agreement.
[Algorithm design and analysis, Process design, Embedded computing, Protocols, message passing, FLP model, process failure, Byzantine agreement algorithm, hybrid failure model, Discrete event simulation, Distributed computing, link failure, Byzantine faulty processes, software fault tolerance, imperfect communications, Fault tolerance, randomized asynchronous consensus, distributed algorithms, Failure analysis, concurrency control, Distributed algorithms, fault simulation, Context modeling]
Group communication protocols under errors
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Group communication protocols constitute a basic building block for highly dependable distributed applications. Designing and correctly implementing a group communication system (GCS) is a difficult task. While many theoretical algorithms have been formalized and proved for correctness, only few research projects have experimentally assessed the dependability of GCS implementations under complex error scenarios. This paper describes a thorough error-injection experimental campaign conducted on Ensemble, a popular GCS. By employing synthetic benchmark applications, we stress selected components of the GCS $the group membership service, the FIFO-ordered reliable multicast - under various error models, including errors in the memory (text and heap segments) and in the network messages. The data show that about 5-6% of the failures are due to an error escaping Ensemble's error-containment mechanism and manifesting as a fail silence violation. This constitutes an impediment to achieving high dependability, the natural objective of GCSs. Our results are derived for a particular system (Ensemble), and more investigation involving other GCSs is required to generalize the conclusions. Nevertheless, through an accurate analysis of the failure causes and the error propagation patterns, this paper offers insights into the design and the implementation of robust GCSs.
[Protocols, Communication systems, error models, Computer crashes, error detection, Distributed computing, group communication protocols, Stress, fail silence violation, synthetic benchmark applications, group communication system, formal verification, error-injection experiment, error propagation patterns, Failure analysis, GCS, multicast communication, Ensemble, fault tolerant computing, Error correction, Impedance, protocols, Pattern analysis, Formal verification]
TCP-friendly many-to-many end-to-end congestion control
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
The paper addresses the issue of TCP-friendly congestion control mechanism for many-to-many communication environment. Lack of congestion control inhibits deployment of WAN applications that involve collaboration of groups of processes in the Internet environment. Recent efforts targeted unicast WAN congestion control (TFRC). We extend that approach to multicast many-to-many applications that operate using a middleware framework. Our congestion control mechanism was implemented within a group communication middleware and tested in a multi-continent environment. The measurements have proved the proposed approach to be robust, efficient and TCP-friendly, as well as to provide fairness among processes that compete for shared resources.
[Wide area networks, end-to-end congestion control, telecommunication congestion control, wide area networks, Communication system control, Middleware, Equations, Computer science, WAN application, Unicast, transport protocols, Feedback, Collaboration, group communication middleware, groupware, TCP-friendly congestion control, many-to-many communication, Internet, middleware framework, Testing, middleware]
The design and implementation of a JCA-compliant capture protection infrastructure
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
A capture protection server protects a cryptographic key on a device that may be captured by authenticating the user of the device (e.g., by password) before permitting the key to be used. Delegation from one capture protection server to another enables the new server to perform this capture protection function for the device. Delegation, however, opens the system to new vulnerabilities, including difficulties in limiting online password-guessing attacks and in disabling a device that has been stolen by an attacker who knows the password. Here we propose a lightweight protocol for coordinating capture protection servers that eliminates these vulnerabilities. We also report on the implementation of our protocol in a JCA-compliant cryptographic service provider, and ramifications of the JCA interfaces for our approach.
[JCA interfaces, capture protection server, public key cryptography, capture protection infrastructure, online password-guessing attacks, lightweight protocol, access protocols, JCA-compliant, cryptographic key, Protection, cryptographic service provider, middleware]
SNARE: a strong security scheme for network-attached storage
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
This paper presents a strong security scheme for network-attached storage (NAS) that is based on capability and uses a key distribution scheme to keep network-attached storage from performing key management. Our system uses strong cryptography to protect data from spoofing, tampering, eavesdropping and replay attacks, and it also guarantees that the data stored on the storage is copy-resistant. In spite of this level of security, our system does not impose much performance penalty. Our experimental results shows that, using a relatively inexpensive CPU in the storage device, there are little performance penalty for random disk accesses and about 9-25% performance degradation for large sequential disk accesses (/spl ges/4 KB).
[Data privacy, NAS, Scalability, replay attacks, key management, eavesdropping, SNARE, Degradation, Network servers, data tampering, public key cryptography, Cryptography, Protection, storage device, storage media, Data security, strong cryptography, copy-resistant storage, key distribution scheme, Secure storage, Disk drives, storage area networks, network-attached storage, Authentication, data spoofing, security scheme]
Assessing the dependability of OGSA middleware by fault injection
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
This paper presents our research on devising a dependability assessment method for the upcoming OGSA 3.0 middleware using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing OGSA middleware and derive a new method and fault model. From this we have implemented an extendable fault injector framework and undertaken some proof of concept experiments with a simulated OGSA middleware system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy with our simulated OGSA system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard Web service to the stateful environment of an OGSA service.
[dependability testing, System testing, program testing, grid computing, Apache Tomcat, Simple object access protocol, SWIFI, Grid computing, Large-scale systems, fault injection, Cryptography, software performance evaluation, middleware, Computational modeling, fault injector framework, Middleware, dependability assessment, GRID computing, Programming profession, software fault tolerance, Computer science, fault model, Web services, GRID middleware, Internet, Apache SOAP, OGSA middleware]
Reliably networking a multicast repository
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
In this paper we consider the design of a reliable multicast facility over an unreliable multicast network. Our multicast facility has several interesting properties: it has different numbers of clients interested in each data packet, allowing us to tune our strategy for each data transmission; it has recurring data items, so that missed data items can be rescheduled for later transmission; and it allows the server to adjust the scheduler according to loss information. We exploit the properties of our system to extend traditional reliability techniques for our case, and use performance evaluation to highlight the resulting differences. We find that our reliability techniques can reduce the average client wait time by over thirty percent.
[client-server systems, Costs, information dissemination, performance evaluation, multicast repository, multicast network, Computer science, Network servers, multicast facility, reliability techniques, Computer network reliability, multicast protocols, multicast communication, telecommunication network reliability, data packet, Broadcasting, Propagation losses, data transmission strategy, Data communication, IP networks, recurring data items, Web server, Teletext, reliably networking]
Modeling and detecting failures in next-generation distributed multimedia applications
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
In this paper we investigate dependability issues of next-generation distributed multimedia applications. Examples of such applications are autonomous vehicle control, tele-medicine, and audio/video control. For these applications the quality of the delivered multimedia data is a critical factor. According to the ITU-T (working group SG 12), the quality of a multimedia service as perceived by end-users is defined by three parameters: delay, delay variation, and information loss. It is paramount to formalize the concept of a failure from the user's perspective. This paper defines the correctness of a multimedia service as a function of temporal distributions of the user-related parameters. It proposes a strategy for modeling and detecting failures of the considered applications. In particular, the detection process is based on error filtering functions. We show that the combination of threshold-based mechanisms is suitable for implementing an efficient detection strategy. We also evaluate the effectiveness of the proposed mechanism both by simulations and by experiments performed on a prototype. Such a prototype is tested with respect to a case study application, consisting of distributed remote-control based on RTP/RTCP standard streaming protocols.
[Protocols, failure modeling, Communication system control, delay variation, user-related parameters, Quality of service, distributed processing, multimedia systems, Mobile robots, Delay, threshold-based mechanisms, Remotely operated vehicles, error filtering functions, Network servers, autonomous vehicle control, protocols, audio/video control, information loss, distributed multimedia applications, Multimedia systems, delivered data quality, Decoding, quality of service, temporal distributions, Streaming media, multimedia service, fault tolerant computing, data handling, fault simulation, failure detection, tele-medicine]
Improving the multiple errors detection coverage in distributed embedded systems
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Currently, a lot of critical applications in automobile and aircraft avionics are built on fault-tolerant real-time distributed embedded systems. Fault injection techniques have been used extensively in the experimental validation of these systems and it is a challenge to adapt them to the demands of new technologies. This paper deals with the effect of physical faults at pin level on the Communication Network Interface in a prototype based on time-triggered architecture. Due to the essential necessity of observing system behavior during injection experiments, a suitable monitor for distributed embedded systems is proposed. The monitor is used to detect failures in the value domain that could lead a system to violate its main concern of fail-silence. With the encouragement to improve detection coverage in the value domain, an error detection code is presented, which is useful for dealing with both unidirectional multiple errors as well as random multiple errors. In order to understand how much the code can increase the coverage, it is tested with a realistic brake-by-wire control application.
[Real time systems, distributed embedded systems, error detection codes, failure detection monitor, error detection code, Aerospace electronics, time-triggered architecture, communication network interface, Condition monitoring, Embedded system, Fault tolerant systems, Prototypes, embedded systems, aerospace computing, brake-by-wire control application, fault injection, Communication networks, Testing, physical faults, automobiles, avionics, Automobiles, distributed memory systems, system monitoring, fault tolerant computing, multiple error detection coverage, Aircraft]
Easy: engineering high availability QoS in wServices
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Developing and administrating distributed applications is complex. Frameworks, hiding the distribution hurdles through encapsulation were proposed, but their acceptance by the industry has been limited. The main reason is the difficulty to provide a simple interface to meet a wide range of application types. In this paper we address the functional services provided over the Web (henceforth, wServices). Examples of wServices are grid services. These typically small software components are often developed under tight budget and timeframe constraints. A wService may be deployed on different platforms and provide different QoS guarantees. With the advent of e-business, wServices become an important type of distributed applications. We claim that narrowing the view to this type of applications allows providing a simple interface. Furthermore, we show that good performance can be achieved if wService developers provide simple tuning parameters as part of a wService package. In our Easy model, platform and QoS specifics are decoupled from wService development costs. In addition we aim at increasing automation of wService deployment on various platforms and for different QoS. The focus of this paper is on performance aware high availability, achieved through wService cloning and replication of its state. In our philosophy, wService developers are aware of potential cloning and replication but not of the mechanisms that provide it. We demonstrate the feasibility of Easy through a prototype with an automatically deployed TomCat Web Container. Easy clones TomCat and replicates its states. We show that this automated process imposes only slight performance degradation compared to a manual one.
[Encapsulation, Costs, Containers, distributed applications, e-business, Degradation, Prototypes, state cloning, software engineering, software components, high availability QoS engineering, electronic commerce, Availability, state replication, Automation, object-oriented programming, tuning parameters, Cloning, TomCat Web container, quality of service, grid services, Application software, Web services, QoS guarantees, Packaging, Internet]
Performance virtualization for large-scale storage systems
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Current data centers require storage capacities of hundreds of terabytes to petabytes. Time-critical applications such as online transactions processing depend on getting adequate performance from the storage subsystem: otherwise, they fail. It is difficult to provide predictable quality of service at this level of complexity, because I/O workloads are extremely variable and device behavior is poorly understood. Ensuring that unrelated but competing workloads do not affect each other's performance is still more difficult, and equally necessary. We present SLEDS (Service Level Enforcement Discipline for Storage), a distributed controller that provides statistical performance guarantees on a storage system built from commodity components. SLEDS can adaptively handle unpredictable workload variations so that each client continues to get the performance it needs even in the presence of misbehaving, competing peers. After evaluating the SLEDS on a heterogeneous mid-range storage system, we found that it is vastly superior to the raw system in its ability to provide performance guarantees, while only introducing a negligible overhead.
[storage allocation, transaction processing, storage system, Costs, Service Level Enforcement Discipline for Storage, time-critical applications, data centers, Quality of service, Switches, distributed processing, statistical performance, Control systems, Delay, Network servers, resource allocation, Large-scale systems, unpredictable workload variations, virtual storage, performance evaluation, quality of service, SLEDS, online transaction processing, Storage area networks, distributed controller, peripheral interfaces, performance virtualization, Distributed control, large-scale storage systems, storage capacities, Time factors, commodity components]
Raptor: integrating checkpoints and thread migration for cluster management
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Software distributed shared-memory (SDSM) provides the abstraction necessary to run shared-memory applications on cost-effective parallel platforms such as clusters of workstations. However, problems such as cluster component reliability and cluster management, which are not directly related to performance, need to be addressed before SDSM solutions can be widely adopted. This paper presents Raptor, an SDSM cluster management system based on checkpoint/recovery and thread migration. Raptor checkpoints decouple the runtime system and application data from application threads, allowing efficient load balancing, resource allocation, and rollback recovery. There are two important features of the system. First, it reduces checkpoint overhead by only saving application-specific data that cannot be recreated at recovery time. Second, by integrating thread migration capability both at running and recovery, it allows the addition or removal of computing resources from a running application, while adding little or no additional burden on the SDSM application programmer.
[workstation clusters, cluster management, thread migration, load balancing, Windows, Registers, Yarn, system recovery, SDSM, Raptor, checkpoint integration, resource allocation, rollback recovery, distributed programming, software distributed shared-memory, workstations clusters, Programming profession, Programming environments, shared-memory applications, Waste management, cluster component reliability, Coherence, Packaging, distributed shared memory systems, Personal communication networks, Resource management]
Network awareness and failure resilience in self-organizing overlay networks
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
The growth of peer-to-peer applications on the Internet motivates interest in general purpose overlay networks. The construction of overlays connecting a large population of transient nodes poses several challenges. First, connections in the overlays should reflect the underlying network topology, in order to avoid overloading the network and to allow god application performance. Second, connectivity among active nodes of the overlay should be maintained, even in the presence of high failure rates or when a large proportion of nodes are not active. Finally, the cost of using the overlay should be spread evenly among peer nodes for fairness reasons as well as for the sake of application performance. To preserve scalability, we seek solutions to these issues that can be implemented in a fully decentralized manner and rely on local knowledge from each node. In this paper, we propose an algorithm called the localizer which addresses these three key challenges. The localizer refines the overlay in a way that reflects geographic locality so as to reduce network overload. Simultaneously, it helps to evenly balance the number of neighbors of each node in the overlay, thereby sharing the load evenly as well as improving the resilience to random node failures or disconnections. The proposed algorithm is presented and evaluated in the context of an unstructured peer-to-peer overlay network produced using the Scamp protocol. We provide a theoretical analysis of the various aspects of the algorithm. Simulation results based on a realistic network topology model confirm the analysis and demonstrate the localizer efficiency.
[Algorithm design and analysis, network balancing, Costs, Protocols, Scalability, transient nodes connectivity, self-organizing overlay networks, application performance, peer-to-peer applications, Analytical models, Network topology, resource allocation, Scamp protocol, computer network reliability, peer-to-peer overlay network, IP networks, network topology model, network awareness, load sharing, Peer to peer computing, theoretical analysis, Resilience, reduce network overload, multicast protocols, distributed algorithms, telecommunication network routing, failure resilience, Internet, Joining processes, node failures, telecommunication traffic]
NEEM: network-friendly epidemic multicast
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
Epidemic, or probabilistic, multicast protocols have emerged as a variable mechanism to circumvent the scalability problems of reliable multicast protocols. However, most existing epidemic approaches use connectionless transport protocols to exchange messages and rely on the intrinsic robustness of the epidemic dissemination to mask network omissions. Unfortunately, such an approach is not network-friendly, since the epidemic protocol makes no effort to reduce the load imposed on the network when the system is congested. In this paper, we propose a novel epidemic protocol whose main characteristic is to be network-friendly. This property is achieved by relying on connection-oriented transport connections, such as TCP/IP, to support the communication among peers. Since during congestion messages accumulate in the border of the network, the protocol uses an innovative buffer management scheme, which combines different selection techniques to discard messages upon overflow. This technique improves the quality of the information delivered to the application during periods of network congestion. The protocol has been implemented and the benefits of the approach are illustrated using a combination of experimental and simulation results.
[Transport protocols, telecommunication congestion control, Scalability, Throughput, Proposals, TCP/IP protocols, TCPIP, telecommunication network reliability, epidemic dissemination, epidemic approaches, network omissions, Communication system traffic control, Robustness, peer-to-peer communication, buffer management scheme, buffer storage, network-friendly epidemic multicast, connection-oriented transport connections, Access protocols, network congestion, epidemic protocol, Multicast protocols, quality of service, scalability problems, transport protocols, multicast protocols, reliable multicast protocols, Innovation management, NEEM, telecommunication traffic]
Component replication in distributed systems: a case study using Enterprise Java Beans
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
A recent trend has seen the extension of object-oriented middleware. A major advantage components offer over objects is that only the business logic of an application needs to be addressed by a programmer with support services required incorporated into the application at deployment time. This is achieved via components (business logic of an application), containers that host components and are responsible for providing the underlying middleware services required by components and application servers that host containers. Well-known examples of component middleware architectures are Enterprise Java Beans (EJBs) and the CORBA Component model (CCM). Two of the many services available at deployment time in most component architectures are component persistence and atomic transactions. This paper examines, using EJBs, how replication for availability can be supported by containers so that components that are transparently using persistence and transactions can also be made highly available.
[transaction processing, Computer aided software engineering, Containers, component replication, Enterprise Java Beans, Fault tolerance, component middleware architectures, Computer architecture, distributed systems, application servers, Logic, Web server, distributed object management, middleware, atomic transactions, Java, component persistence, Object oriented modeling, deployment time, CORBA component model, CCM, Middleware, Programming profession, business logic, object-oriented middleware, EJB, persistent objects, middleware services]
Coordinated forward error recovery for composite Web services
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
This paper proposes a solution based on forward error recovery, oriented towards providing dependability of composite Web services. While exploiting their possible support for fault tolerance (e.g., transactional support at the level of each service), the proposed solution has no impact on the autonomy of the individual Web services, our solution lies in system structuring in terms of co-operative atomic actions that have a well-defined behavior, both in the absence and in the presence of service failures. More specifically, we define the notion of Web Service Composition Action (WSCA), based on the Coordinated Atomic Action concept, which allows structuring composite Web services in terms of dependable actions. Fault tolerance can then be obtained as an emergent property of the aggregation of several potentially non-dependable services. We further introduce a framework enabling the development of composite Web services based on WSCAs, consisting of an XML-based language for the specification of WSCAs.
[Transport protocols, Simple object access protocol, system recovery, Fault tolerance, Web service composition action, WSCA, Web and internet services, Fault tolerant systems, forward error recovery, Standards development, Assembly, electronic commerce, hypermedia markup languages, fault tolerance, composite Web services, Service oriented architecture, nondependable services aggregation, service failures, Middleware, co-operative atomic actions, software fault tolerance, coordinated forward error recovery, Web services, system structuring, emergent property, XML-based language, Internet, error handling]
Preventing orphan requests in the context of replicated invocation
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
In today's systems, applications are composed from various components that may be located on different machines. The components (acting as servers) may have to collaborate in order to service a client request. More specifically, a client request to one component may trigger a request to another component. To ensure fault-tolerance, components are generally replicated. Replicated components lead to the problem of a replicated server invoking another replicated server. We call this the problem of replicated invocation. Replicated invocation has been considered in the context of deterministic servers. However, the problem is more difficult to address when servers are non-deterministic. In this context, work has been done to enforce deterministic execution. In this paper, we consider a different approach: instead of preventing non-deterministic execution of servers we discuss how to handle it. We present the problem of non-deterministic replicated invocation. Our solution then consists of sharing sufficient undo information among the replicas of the server invoking another server.
[client-server systems, orphan requests, Laboratories, replicated invocation, nondeterministic execution, fault-tolerance components, Fault tolerance, Filters, client request, multicast protocols, Collaboration, file servers, replicated server, fault tolerant computing, Contracts, replicated components, deterministic execution]
Distributed programming for dummies: a shifting transformation technique
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
The perfectly synchronized round model provides the powerful abstraction of crash-stop failures with atomic message delivery. This abstraction makes distributed programming very easy. We present an implementation of this abstraction in a distributed system with general message omissions. Protocols devised using our abstraction (i.e., in the perfectly synchronized round model) are automatically transformed into protocols for the omission model. The transformation is achieved using a round shifting technique with a constant time complexity overhead. This transformation is in a precise sense optimal. Furthermore, and rather surprisingly, no automatic transformation from a weaker model, say the traditional crash-stop model (with no atomic message delivery), onto an even stronger model than the general-omission one, say the send-omission model, can provide better time complexity performance.
[Protocols, Costs, atomic message delivery, Laboratories, general message omissions, distributed system, communication complexity, Distributed computing, round shifting technique, Computer networks, protocols, perfectly synchronized round model, constant time complexity, distributed programming, message passing, powerful abstraction, Computer crashes, Power system modeling, Programming profession, telecommunication network routing, precise sense optimal, send-omission model, shifting transformation technique, crash-stop failures, Buffer overflow, Integrated circuit modeling, omission model]
Epidemic spreading in real networks: an eigenvalue viewpoint
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
How will a virus propagate in a real network? Does an epidemic threshold exist for a finite graph? How long does it take to disinfect a network given particular values of infection rate and virus death rate? We answer the first question by providing equations that accurately model virus propagation in any network including real and synthesized network graphs. We propose a general epidemic threshold condition that applies to arbitrary graphs: we prove that, under reasonable approximations, the epidemic threshold for a network is closely related to the largest eigenvalue of its adjacency matrix. Finally, for the last question, we show that infections tend to zero exponentially below the epidemic threshold. We show that our epidemic threshold model subsumes many known thresholds for special-case graphs (e.g., Erdos-Renyi, BA power-law, homogeneous); we show that the threshold tends to zero for infinite power-law graphs. We show that our threshold condition holds for arbitrary graphs.
[telecommunication security, epidemic spreading, epidemic threshold conditions, model virus propagation, graph theory, infinite power-law graphs, computer networks, eigenvalue viewpoint, computer viruses, network graphs, real networks, eigenvalues and eigenfunctions, Intelligent networks, finite graph, computer virus, Eigenvalues and eigenfunctions]
Service continuations: an operating system mechanism for dynamic migration of Internet service sessions
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
We propose service continuations (SC), an operating system mechanism that supports seamless dynamic migration of Internet service sessions between cooperating multi-process servers. Service continuations provide a server application with a simple and easy to use abstraction, and a means to migrate the service state along with the serviced connection. SC supports transparent resumption of service to the client of another server, and guaranteed integrity and consistency of communication channels used by server processes. SC is a generic, application independent mechanism that can be used to provide service continuity and availability for today's complex Internet services. We have implemented SC in FreeBSD and used them successfully in three real servers: the Apache Web server, the PostgreSQL transactional database server, and the Icecast streaming server. We present results of an experimental evaluation showing that using SC adds negligible run-time overhead to existing servers and that SC enables efficient dynamic migration of client sessions.
[Availability, client-server systems, service continuations, serviced connection, multiprocess servers, Application software, Computer crime, Computer science, Network servers, operating system mechanism, resource allocation, Operating systems, dynamic migration, Web and internet services, network operating systems, TCPIP, Communication channels, Internet, application independent mechanism, Web server, FreeBSD, Internet service sessions, service resumption, communication channels]
Appia vs. Cactus: comparing protocol composition frameworks
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
The paper presents and compares Appia and Cactus, two frameworks for protocol composition. The comparison is based on the experience gained in implementing a fault-tolerant atomic broadcast service. The paper also gives preliminary performance results, and concludes with a discussion of the most interesting features of the two frameworks, and suggestions for an improved framework.
[Java, Protocols, performance evaluation, module composition, Middleware, Programming profession, Computer science, Concurrent computing, Computer languages, Appia, protocol modules, Intersymbol interference, Prototypes, protocol composition frameworks, groupware, Cactus, Broadcasting, Atomic Broadcast service, fault tolerant service, protocols, middleware]
A systematic approach to the development of event based applications
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
We propose a novel framework logic of event consumption and publication (LECAP) for the development of event-based applications. Our approach offers the following advantages over existing approaches: 1) it supports a while-parallel language, 2) the reasoning allows a dynamic (instead of static) binding of programs to events, 3) it is oriented towards stepwise development of systems, and 4) the underlying logic supports the composition of specifications. The event based architectural style has been recognized as fostering the development of large-scale and complex systems by loosely coupling their components. It is therefore increasingly deployed in various environments such as middleware for mobile computing, message oriented middleware, integration frameworks, communication standards, and commercial toolkits. Current approaches to the development of event-based applications are ad hoc and do not support reasoning about their correctness. The LECAP approach is intended to solve this problem through a compositional and stepwise approach to specification and verification of event-based applications.
[program verification, Mobile communication, formal specification, integration frameworks, large-scale system, Communication standards, message oriented middleware, loosely coupled components, mobile computing, Message-oriented middleware, while-parallel language, LECAP, logic of event consumption and publication, software engineering, communication standards, Large-scale systems, Logic, systematic development, middleware, message passing, program-event dynamic binding, Application software, complex system, Communication system software, event based applications, mobile computing middleware, Teamwork, Software tools, Mobile computing, commercial toolkit]
Proceedings 22nd International Symposium on Reliable Distributed Systems
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
The following topics are dealt with: dependable multicasting; peer-to-peer and beyond; replication; checkpointing; making services robust; frameworks and protocols; security; experimental evaluation; system engineering; and coping with failure models.
[Computer fault tolerance, mobile environment checkpointing, Data security, distributed processing, component replication, Distributed computing, system recovery, software fault tolerance, experimental evaluation, failure models, Software fault tolerance, robust services, security, security of data, Systems engineering, multicast protocols, System recovery, dependable multicasting, systems engineering, fault tolerant computing, peer-to-peer communication, protocols, system engineering]
Buffer management in probabilistic peer-to-peer communication protocols
22nd International Symposium on Reliable Distributed Systems, 2003. Proceedings.
None
2003
In multipeer communication decentralized probabilistic protocols have received a lot of attention because of their robustness against faults in the communication traffic and their potential to provide scalability for large groups. These protocols provide a probabilistic guarantee for a propagated event to reach every group member. Recent work aims to improve the scalability of such protocols by reducing memory requirements. In saving memory resources, the history buffer, which is used to "remember" received events and to prevent multiple deliveries of events to the application, plays a very significant role. We examine how the buffer size should be chosen to challenge the multiple delivery problems. Further, we propose and evaluate several methods of optimizing the dissemination of events in order to provide high reliability and reduce the number of multiple deliveries at the same time.
[probabilistic protocols, Protocols, Scalability, memory resources saving, multipeer communication, Frequency estimation, History, buffer size, Fault tolerance, Robustness, computer network reliability, peer-to-peer communication, protocol scalability, buffer storage, Peer to peer computing, information dissemination, history buffer, probability, Maintenance, dissemination optimization, multicast protocols, buffer management, Collaboration, routing protocols, communication traffic, multiple delivery problem, Telecommunication network reliability]
Slow advances in fault-tolerant real-time distributed computing
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Is fault-tolerant (FT) real-time computing a specialized branch of FT computing? The key issue in real-time (RT) computing is to economically produce systems that yield temporal behavior which is relatively easily analyzable and acceptable in given application environments. Fault-tolerant (FT) RT computing has been treated by the predominant segment of the FT computing research community as a highly specialized branch of FT computing. This author believes that the situation should be changed. It seems safe to say that FT techniques for which useful characterizations of temporal behavior have not been or cannot be developed, are at best immature, if not entirely useless. This means that FT RT computing is at the core of FT computing.
[Real time systems, Process design, fault-tolerant computing, FT RT computing, Redundancy, real-time computing, Humans, automated recovery, Distributed computing, Environmental economics, distributed computing, Fault tolerance, Fault tolerant systems, network operating systems, real-time systems, Computer industry, fault tolerant computing, Fuel processing industries]
A signal processing approach to global predicate monitoring
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Global predicate evaluation is a fundamental problem in distributed systems. This paper views it from a different perspective, namely that of the signals and systems area of electrical engineering. It adapts a signal processing approach to address this problem in the context of monitoring of 'health' of a software system. The global state of the system is viewed as a 'state' signal which evolves over time. The distributed processes are assumed to possess roughly synchronized clocks. The states of individual processes are periodically sampled and reported to a global monitor. The observed system state constructed by the global monitor is viewed as being composed of two components - the consistent global states and an error signal due to the messages in transit and differences in the local clocks. The global monitor removes the error signal by processing the observed global signal through a low-pass filter. It evaluates the predicates on the filtered signal. The approach presented is applicable to distributed systems which are semi-stationary, i.e. whose internal states of interest remain stable over comparatively long intervals of time. The paper presents the relevant signal processing concepts (p-spectrum and p-filtering), outlines an architecture for global predicate monitoring and describes the signal processing done in the global monitor. The paper then summarizes an evaluation of the approach presented on a small computer aided vehicle dispatch system. The evaluation experiments are described and the results are presented and analyzed.
[software system health, computer aided vehicle dispatch system, Costs, systems software, p-spectrum, Laboratories, signal processing, software reliability, global predicate monitoring, distributed processing, electrical engineering, Optical signal processing, error signal, global predicate evaluation, distributed systems, synchronized clock, low-pass filter, p-filtering, Low pass filters, Computerized monitoring, Software reliability, Signal processing algorithms, Signal processing, Software systems, condition monitoring, Clocks]
Dependability in web services
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
false
[Web services, USA Councils]
Proactive hot spot avoidance for Web server dependability
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Flash crowds, which result from the sudden increase in popularity of some online content, are among the most important problems that plague today's Internet. Affected servers are overloaded with requests and quickly become "hot spots." They usually suffer from severe performance failures or stop providing service altogether, as there are scarcely any effective techniques to scalably deliver content under hot spot conditions to all requesting clients. In this paper, we propose and evaluate collaborative techniques to detect and proactively avoid the occurrence of hot spots. Using our mechanisms, groups of small- to medium-sized Web servers can team up to withstand unexpected surges of requests in a cost-effective manner. Once a Web server detects a sudden increase in request traffic, it replicates on-the-fly the affected content on other Web servers; subsequent requests are transparently redirected to the copies to offload the primary server. Each server acts both as a primary source for its own content, and as a secondary source for other servers' content in the event of a flash-crowd; scalability and dependability are therefore achieved in a peer-to-peer fashion, with each peer contributing to, and benefiting from, the service. Our proactive hot spot avoidance techniques are implemented as a module for the popular Apache Web server. We have conducted a comprehensive experimental evaluation, which demonstrates that our techniques are effective at dealing with flash crowds and scaling to very high request loads.
[collaborative techniques, Costs, telecommunication congestion control, peer-to-peer computing, Web server dependability, Scalability, Peer to peer computing, Computer crashes, proactive hot spot avoidance, Surges, online content, Apache Web server, request traffic, Collaboration, Bandwidth, groupware, flash crowds, Collaborative work, Internet, Web server, telecommunication traffic]
Performance comparison of a rotating coordinator and a leader based consensus algorithm
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Protocols that solve agreement problems are essential building blocks for fault tolerant distributed systems. While many protocols have been published, little has been done to analyze their performance, especially the performance of their fault tolerance mechanisms. In this paper, we compare two well-known asynchronous consensus algorithms. In both algorithms, a leader process tries to impose a decision, and another leader retries if the leader fails doing so. The algorithms elect leaders differently: the Chandra-Toueg algorithm has a rotating leader, whereas processes in the Paxos algorithm elect leaders directly. We investigate the performance implications of this difference. In the system under study, processes send atomic broadcasts to each other. Consensus is used to decide the delivery order of messages. We evaluate the steady state latency in (1) runs with neither crashes nor suspicions, (2) runs with crashes and (3) runs with no crashes in which correct processes are wrongly suspected to have crashed, as well as the transient latency after (4) one crash and (5) multiple correlated crashes. The results show that the Paxos algorithm tolerates frequent wrong suspicions (3) and correlated crashes (5) better, while the performance is comparable in all other scenarios.
[Algorithm design and analysis, protocol performance, Protocols, steady state latency, fault tolerant distributed system, distributed processing, message delivery order, Steady-state, Delay, Fault tolerance, agreement problem solving, asynchronous algorithm, Fault tolerant systems, leader based consensus algorithm, failure detector, Broadcasting, failure simulation, Performance analysis, Safety, protocols, Paxos algorithm, rotating coordinator, Chandra-Toueg algorithm, performance evaluation, transient latency, Computer crashes, atomic broadcast, correlated crash, fault tolerant computing, performance comparison]
On the progress in fault-tolerant real-time computing
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Measuring progress in terms of industrial take-up, the paper takes a view that the progress in FT RT computing has been significant, and that the new dominant application domains do not allow the 'ingredients' attributable to past success to be re-used at the same level they were once used. Consequently, FT RT computing is acquiring new faces in the form of adaptive and autonomic computing.
[Real time systems, Fault tolerance, FT RT computing, Fault tolerant systems, real-time systems, industrial take-up, adaptive computing, fault tolerant computing, fault-tolerant real-time computing, autonomic computing, Distributed computing, adaptive systems]
An efficient checkpointing protocol for the minimal characterization of operational rollback-dependency trackability
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
A checkpointing protocol that enforces rollback-dependency trackability (RDT) during the progress of a distributed computation must induce processes to take forced checkpoints to avoid the formation of nontrackable rollback dependencies. A protocol based on the minimal characterization of RDT tests only the smallest set of nontrackable dependencies. The literature indicated that this approach would require the processes to maintain and propagate O(n/sup 2/) control information, where n is the number of processes in the computation. In this paper, we present a protocol that implements this approach using only O(n) control information.
[Checkpointing, checkpointing, Protocols, minimal characterization, Computational modeling, operational rollback-dependency trackability, Software algorithms, control information, Communication system control, distributed processing, distributed computation, Application software, checkpointing protocol, Distributed computing, Software debugging, protocols, Force control, Testing, computational complexity]
Hardware support for high performance, intrusion- and fault-tolerant systems
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
The paper proposes a combined hardware/software approach for realizing high performance, intrusion- and fault-tolerant services. The approach is demonstrated for (yet not limited to) an attribute authority server, which provides a compelling application due to its stringent performance and security requirements. The key element of the proposed architecture is an FPGA-based, parallel crypto-engine providing (1) optimally dimensioned RSA Processors for efficient execution of computationally intensive RSA signatures and (2) a KeyStore facility used as tamper-resistant storage for preserving secret keys. To achieve linear speed-up (with the number of RSA Processors) and deadlock-free execution in spite of resource-sharing and scheduling/synchronization issues, we have resorted to a number of performance enhancing techniques (e.g., use of different clock domains, optimal balance between internal and external parallelism) and have formally modeled and mechanically proved our crypto-engine with the Spin model checker. At the software level, the architecture combines active replication and threshold cryptography, but in contrast with previous work, the code of our replicas is multithreaded so it can efficiently use an attached parallel crypto-engine to compute an attribute authority partial signature (as required by threshold cryptography). Resulting replicated systems that exhibit nondeterministic behavior, which cannot be handled with conventional replication approaches. Our architecture is based on a preemptive deterministic scheduling algorithm to govern scheduling of replica threads and guarantee strong replica consistency.
[program verification, attached parallel crypto-engine, replica threads, optimally dimensioned RSA Processor, hardware-software codesign, Software performance, computationally intensive RSA signature, replica consistency, Concurrent computing, Fault tolerance, software architecture, high performance fault-tolerant service, security requirement, tamper-resistant storage, Fault tolerant systems, Spin model checker, high performance intrusion-tolerant service, Computer architecture, authorisation, scheduling, high performance fault-tolerant system, Hardware, preemptive deterministic scheduling, Cryptography, FPGA-based parallel crypto-engine, attribute authority server, high performance intrusion-tolerant system, threshold cryptography, cryptography, active replication, Application software, replicated system, KeyStore facility, attribute authority partial signature, Processor scheduling, formal modeling, hardware-software approach, System recovery, fault tolerant computing, digital signatures]
Low latency probabilistic broadcast in wide area networks
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
In this paper we propose a novel probabilistic broadcast protocol that reduces the average end-to-end latency by dynamically adapting to network topology and traffic conditions. It does so by using an unique strategy that consists in adjusting the fanout and preferred targets for different gossip rounds as a function of the properties of each node. Node classification is light-weight and integrated in the protocol membership management. Furthermore, each node is not required to have full knowledge of the group membership or of the network topology. The paper shows how the protocol can be configured and evaluates its performance with a detailed simulation model.
[Wide area networks, traffic conditions, Protocols, wide area networks, Scalability, low latency probabilistic broadcast, Telecommunication traffic, telecommunication network topology, group membership, average end-to-end latency, network topology, Relays, Delay, broadcasting, Intelligent networks, Network topology, multicast protocols, telecommunication network routing, Broadcasting, Traffic control, probabilistic broadcast protocol, protocol membership management]
Why have progresses in real-time fault tolerant computing been slow?
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
false
[Fault tolerance]
Skewed checkpointing for tolerating multi-node failures
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Large cluster systems have become widely utilized because they achieve a good performance/cost ratio especially in high performance computing. Although these cluster systems are distributed memory systems, coordinated checkpointing is a promising way to maintain high availability because the computing nodes are tightly connected to one another. However, as the number of computing nodes gets larger, the probability of multi-node failures increases. To tolerate multi-node failures, a large degree of redundancy is required in checkpointing, but this leads to performance degradation. Thus, we propose a new coordinated checkpointing called skewed checkpointing. In this method, checkpointing is skewed every time. Although each checkpointing itself contains only one degree of redundancy, this skewed checkpointing ensures /spl lfloor/log/sub 2/N/spl rfloor/ degrees of redundancy when the number of nodes is N. In this paper, we present the proposed method and an analysis of the performance overhead. Then, this method is applied to a cluster system and compared with other conventional checkpointing schemes. The results reveal the superiority of our method, especially for large cluster systems.
[Checkpointing, Availability, checkpointing, workstation clusters, Costs, multinode failure tolerance, Redundancy, skewed checkpointing, Reliability engineering, distributed memory system, Distributed computing, Delay, Degradation, computing node, coordinated checkpointing, High performance computing, distributed memory systems, fault tolerant computing, high performance computing, Performance analysis, redundancy, large cluster system]
Balancing the tradeoffs between data accessibility and query delay in ad hoc networks
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
In mobile ad hoc networks, nodes move freely and link/node failures are common. This leads to frequent network partitions, which may significantly degrade the performance of data access in ad hoc networks. When the network partition occurs, mobile nodes in one network are not able to access data hosted by nodes in other networks. In this paper, we deal with this problem by applying data replication techniques. Existing data replication solutions in both wired or wireless networks aim at either reducing the query delay or improving the data accessibility. As both metrics are important for mobile nodes, we propose schemes to balance the tradeoffs between data accessibility and query delay under different system settings and requirements. Simulation results show that the proposed schemes can achieve a balance between these two metrics and provide satisfying system performance.
[network partition, data accessibility, Mobile communication, wireless networks, Ad hoc networks, Electronic mail, query delay, Delay, Mobile ad hoc networks, Computer science, Degradation, query processing, Intelligent networks, mobile computing, wired networks, Wireless networks, data replication, mobile ad hoc networks, data access, IP networks, ad hoc networks]
Using program analysis to identify and compensate for nondeterminism in fault-tolerant, replicated systems
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Fault-tolerant replicated applications are typically assumed to be deterministic, in order to ensure reproducible, consistent behavior and state across a distributed system. Real applications often contain nondeterministic features that cannot be eliminated. Through the novel application of program analysis to distributed CORBA applications, we decompose an application into its constituent structures, and discover the kinds of nondeterminism present within the application. We target the instances of nondeterminism that can be compensated for automatically, and highlight to the application programmer those instances of nondeterminism that need to be manually rectified. We demonstrate our approach by compensating for specific forms of nondeterminism and by quantifying the associated performance overheads. The resulting code growth is typically limited to one extra line for every instance of nondeterminism, and the runtime overhead is minimal, compared to a fault-tolerant application with no compensation for nondeterminism.
[distributed CORBA applications, fault-tolerant application, object-oriented programming, Engineering profession, program diagnostics, consistent behavior, distributed system, Computer crashes, Application software, Distributed computing, Programming profession, fault-tolerant replicated applications, Fault diagnosis, nondeterministic features, Fault tolerance, Runtime, fault-tolerant replicated systems, Operating systems, Fault tolerant systems, program analysis, fault tolerant computing, distributed object management]
Nested objects in a Byzantine quorum-replicated system
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Modern distributed, object-based systems support nested method invocations, whereby one object can invoke methods on another. In this paper we present a framework that supports nested method invocations among Byzantine fault-tolerant, replicated objects that are accessed via quorum systems. A challenge in this context is that client object replicas can induce unwanted method invocations on server object replicas, due either to redundant invocations by client replicas or Byzantine failures within the client replicas. At the core of our framework are a new quorum-based authorization technique and a novel method invocation protocol that ensure the linearizability and failure atomicity of nested method invocations despite Byzantine client and server replica failures. We detail the implementation of these techniques in a system called Fleet, and give preliminary performance results for them.
[client-server systems, Protocols, Fleet, client object replica, invocation protocol, failure atomicity, object-based systems, linearizability, server object replica, Authorization, Byzantine fault tolerant replicated objects, authorization, nested objects, Byzantine quorum replicated system, Fault tolerant systems, object nesting, authorisation, remote procedure calls, distributed systems, fault tolerant computing, protocols, distributed object management]
Progress in real-time fault tolerance
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
This paper discusses progress in the field of real-time fault tolerance. In particular, it considers synchronous vs. asynchronous fault tolerance designs, maintaining replica consistency, alternative fault tolerance strategies, including checkpoint restoration, transactions, and consistent replay, and custom vs. generic fault tolerance.
[Real time systems, checkpointing, Costs, checkpoint restoration, distributed processing, synchronous fault tolerance, Probability distribution, Application software, replica consistency, Delay, Fault tolerance, real-time fault tolerance, asynchronous fault tolerance, Microprocessors, Fault detection, Hardware, fault tolerant computing, Timing]
State maintenance and its impact on the performability of multi-tiered Internet services
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
In this paper, we evaluate the performance, availability, and combined performability of four soft state maintenance strategies in two multitier Internet services, an online book store and an auction service. To take soft state and service latency into account, we propose an extension of our previous quantification methodology, and novel availability and performability metrics. Our results demonstrate that storing the soft state in a database can achieve better performability than storing it in main memory, even when the state is efficiently replicated. Strategies that offload the handling of soft state from the database increase the load on other tiers and, consequently, increase the impact of faults in these tiers on service availability. Based on these results, we conclude that service designers need to provision the cluster and balance the load with availability and cost, as well as performance, in mind.
[Performance evaluation, multitiered Internet services, Costs, Scalability, availability evaluation, service latency, Delay, Databases, Web and internet services, performability metrics, Books, Cryptography, electronic commerce, Availability, availability metrics, soft state latency, performance evaluation, service availability, software maintenance, Computer science, online book store, auction service, Internet, quantification methodology, soft state maintenance]
Crash-resilient time-free eventual leadership
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Leader-based protocols rest on a primitive able to provide the processes with the same unique leader. Such protocols are very common in distributed computing to solve synchronization or coordination problems. Unfortunately, providing such a primitive is far from being trivial in asynchronous distributed systems prone to process crashes. (It is even impossible in fault-prone purely asynchronous systems.) To circumvent this difficulty, several protocols have been proposed that build a leader facility on top of an asynchronous distributed system enriched with synchrony assumptions. This paper consider another approach to build a leader facility, namely, it considers a behavioral property on the flow of messages that are exchanged. This property has the noteworthy feature not to involve timing assumptions. Two protocols based on this time-free property that implement a leader primitive are described. The first one uses potentially unbounded counters, while the second one (which is a little more involved) requires only finite memory. These protocols rely on simple design principles that make them attractive, easy to understand and provably correct.
[Algorithm design and analysis, time-free protocol, leader primitive, Protocols, time-free property, finite memory, asynchronous distributed systems, Distributed computing, system recovery, distributed computing, Counting circuits, Fault tolerant systems, Detectors, Broadcasting, protocols, Distributed algorithms, fault tolerance, process crash, crash-resilient time-free eventual leadership, Computer crashes, leader-based protocols, leader facility, synchronisation, distributed algorithm, distributed algorithms, fault tolerant computing, Timing]
Self checking network protocols: a monitor based approach
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
The wide deployment of high-speed computer networks has made distributed systems ubiquitous in today's connected world. The machines on which the distributed applications are hosted are heterogeneous in nature, the applications often run legacy code without the availability of their source code, the systems are of very large scales, and often have soft real-time guarantees. In this paper, we target the problem of online detection of disruptions through a generic external entity called Monitor that is able to observe the exchanged messages between the protocol participants and deduce any ongoing disruption by matching against a rule base composed of combinatorial and temporal rules. The Monitor architecture is application neutral, with the rule base making it specific to a protocol. To make the detection infrastructure scalable and dependable, we extend it to a hierarchical Monitor structure. The infrastructure is applied to a streaming video application running on a reliable multicast protocol called TRAM installed on the campus wide network. The evaluation brings out the scalability of the monitor infrastructure and detection coverage under different kinds of faults for the single level and the hierarchical arrangements.
[checkpointing, legacy code, Protocols, message passing, ubiquitous distributed systems, message exchange, multicast protocol, computer networks, temporal rules, source code, TRAM, online disruption detection, monitor based approach, supervisory programs, self checking network protocols, combinatorial rules, streaming video, hierarchical monitor structure, video streaming, protocols, Monitoring]
The design and evaluation of a defense system for Internet worms
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Many areas of society have become heavily dependent on services such as transportation facilities, utilities and so on that are implemented in part by large numbers of computers and communications links. Both past incidents and research studies show that a well-engineered Internet worm can disable such systems in a fairly simple way and, most notably, in a matter of a few minutes. This indicates the need for defenses against worms but their speed rules out the possibility of manually countering worm outbreaks. We present a platform that emulates the epidemic behavior of Internet active worms in very large networks. A reactive control system operates on top of the platform and provides a monitor/analyze/respond approach to deal with infections automatically. Details of our highly configurable platform and various experimental performance results are presented.
[invasive software, Computer worms, Computerized monitoring, Transportation, Internet worms, Control systems, Sensor systems, highly configurable platform, Information systems, Counting circuits, defense system, very large networks, monitor-analyze-respond approach, Automatic control, Internet, reactive control system, IP networks]
XNET: a reliable content-based publish/subscribe system
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Content-based publish/subscribe systems are usually implemented as a network of brokers that collaboratively route messages from information providers to consumers. A major challenge of such middleware infrastructures is their reliability and their ability to cope with failures in the system. In this paper, we present the architecture of the XNET XML content network and we detail the mechanisms that we implemented to gracefully handle failures and maintain the system state consistent with the consumer population at all times. In particular, we propose several approaches to fault tolerance so that our system can recover from various types of router and link failures. We analyze the efficiency of our techniques in a large scale experimental deployment on the PlanetLab testbed. We show that XNET does not only offer good performance and scalability with large consumer populations under normal operation, but can also quickly recover from system failures.
[message passing, fault tolerance, PlanetLab testbed, Subscriptions, system failure, Routing, router failure, message routing, Maintenance, Middleware, system recovery, link failure, content management, Network servers, middleware infrastructure, content-based publish-subscribe system, Fault tolerant systems, XML, Collaboration, Filtering algorithms, XNET XML content network, fault tolerant computing, Large-scale systems, middleware]
A stability-oriented approach to improving BGP convergence
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
This paper shows that the elimination of fault-agnostic instability, the instability caused by fault-agnostic distributed control, substantially improves BGP convergence speed. To this end, we first classify BGP convergence instability into two categories: fault-agnostic instability and distribution-inherent instability; secondly, we prove the impossibility of eliminating all distribution-inherent instability in distributed routing protocols; thirdly, we design the grapevine border gateway protocol (G-BGP) to show that all fault-agnostic instability can be eliminated. G-BGP eliminates all fault-agnostic instability under different fault and routing policy scenarios by (i) piggybacking onto BGP UPDATE messages fine-grained information about faults to the nodes affected by the faults, (ii) quickly resolving the uncertainty between link and node failure as well as the uncertainty of whether a node has changed route, and (iii) rejecting obsolete fault information. We have evaluated G-BGP by both analysis and simulation. Analytically, we prove that, by eliminating fault-agnostic instability, G-BGP achieves optimal convergence speed in several scenarios where BGP convergence is severely delayed (e.g., when a node or a link fail-stops), and when the shortest-path-first policy is used, G-BGP asymptotically improves BGP convergence speed except in scenarios where BGP convergence speed is already optimal (e.g., when a node or a link joins). By simulating networks with up to 115 autonomous systems, we observe that G-BGP improves BGP convergence stability and speed by an order of magnitude.
[Uncertainty, Pipelines, convergence, distributed processing, path vector routing, Delay, Convergence, stability-oriented approach, Analytical models, BGP UPDATE messages, BGP convergence, G-BGP, fault information, Routing protocols, stability, distributed routing protocol, Stability, fault-agnostic instability, fault-agnostic distributed control, grapevine border gateway protocol, shortest-path-first policy, Computer science, computer communications software, routing protocols, Distributed control, fault tolerant computing, Internet, distribution inherent instability]
Panel statement: why progress in (composite) fault tolerant real-time systems has been slow (-er than expected... &amp; what can we do about it?)
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
The pervasiveness of computers in our current IT driven society (transportation, e-commerce, e-transactions, communication, process control), also implies our growing dependency on their "correct" functionality. In many a case, the real value of the systems and also our usage of these systems comes, in part, based on the dependency (real or perceived) we are consequently willing to put into the provisioning of the services i.e., the implicit or explicit assurance of trust we put for sustained delivery of desired services. Some systems are considered as safety-critical (flight/reactor control etc), though others are accorded varied degrees of criticality. Nevertheless, our expectancy extends to obtaining the proper services when the system is fault-free and especially when it encounters perturbations (design or operational), e.g., electromagnetic interference or a lightning strike for an aircraft. Consequently, it is important to qualitatively and quantitatively associate some measures of trust in the system's ability to "actually" deliver us the desired services in the presence of faults. This is often termed as "dependability" measures for a system with a plethora of fault-tolerance (FT) strategies to help achieve desired levels of dependability. As before, dependability entails the sustained delivery of services, be they service-critical or cost-critical, regardless of the perturbations encountered during their operation.
[Real time systems, Pervasive computing, trust, fault tolerance, Transportation, Process control, computer pervasiveness, dependability measures, safety-critical software, Control systems, ubiquitous computing, Inductors, safety-critical systems, Electromagnetic measurements, Fault tolerant systems, Electromagnetic interference, service delivery, Lightning, real-time systems, panel statement, fault tolerant computing, fault tolerant real-time systems, perturbations]
The mutable consensus protocol
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
In this paper we propose the mutable consensus protocol, a pragmatic and theoretically appealing approach to enhance the performance of distributed consensus. First, an apparently inefficient protocol is developed using the simple stubborn channel abstraction for unreliable message passing. Then, performance is improved by introducing judiciously chosen finite delays in the implementation of channels. Although this does not compromise correctness, which rests on an asynchronous system model, it makes it likely that the transmission of some messages is avoided and thus the message exchange pattern at the network level changes noticeably. By choosing different delays in the underlying stubborn channels, the mutable consensus protocol can actually be made to resemble several different protocols. Besides presenting the mutable consensus protocol and four different mutations, we evaluate in detail the particularly interesting permutation gossip mutation, which allows the protocol to scale gracefully to a large number of processes by balancing the number of messages to be handled by each process with the number of communication steps required to decide. The evaluation is performed using a realistic simulation model which accurately reproduces resource consumption in real systems.
[Performance evaluation, distributed consensus, Protocols, message passing, stubborn channel abstraction, Genetic mutations, Computer crashes, Relays, Delay, Voting, Message passing, Detectors, Broadcasting, asynchronous system model, mutable consensus protocol, permutation gossip mutation, protocols, finite delays, distributed programming, message exchange pattern, resource consumption]
Design and evaluation of a QoS-adaptive system for reliable multicasting
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
This paper presents and studies a reliable multicast protocol whose objective is to deliver a message to all intended destinations, despite possible crashes of the sender and other processes, and communication failures. The protocol enables QoS metrics such as absolute and relative latencies and the probability of reliable delivery, to be negotiated prior to service provisioning. Moreover, it adapts certain parameters dynamically in order to minimize the message traffic required to achieve the negotiated QoS metrics. The performance of the protocol is analyzed mathematically under simplifying assumptions. The accuracy of the approximations is evaluated by simulations.
[multicast protocol, Quality of service, latency, Delay, service provisioning, Web and internet services, delivery reliability, Traffic control, Performance analysis, Availability, message passing, communication failure, Multicast protocols, Computer crashes, quality of service, QoS adaptive system, adaptive systems, reliable multicasting, Intersymbol interference, multicast protocols, message traffic, message delivery, fault tolerant computing, QoS metrics, Resource management]
How to tolerate half less one Byzantine nodes in practical distributed systems
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
The application of dependability concepts and techniques to the design of secure distributed systems is raising a considerable amount of interest in both communities under the designation of intrusion tolerance. However, practical intrusion-tolerant replicated systems based on the state machine approach (SMA) can handle at most f Byzantine components out of a total of n = 3f + 1, which is the maximum resilience in asynchronous systems. This paper extends the normal asynchronous system with a special distributed oracle called TTCB. Using this extended system we manage to implement an intrusion-tolerant service based on the SMA with only 2f + 1 replicas. Albeit a few other papers in the literature present intrusion-tolerant services with this approach, this is the first time the number of replicas is reduced from 3f + 1 to 2f + 1. Another interesting characteristic of the described service is a low time complexity.
[Protocols, Laboratories, Quality of service, distributed processing, intrusion-tolerant replicated system, low time complexity, Security, Fault tolerance, Large-scale systems, Informatics, asynchronous systems, distributed oracle, Context-aware services, Context, intrusion tolerance, intrusion-tolerant service, secure distributed systems, Resilience, state machine approach, security of data, TTCB, Byzantine components, fault tolerant computing, Byzantine nodes, maximum resilience]
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
false
[]
An hoarding approach for supporting disconnected write operations in mobile environments
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Caching is one technique that reduces costs and improves performance in mobile environments. It also increases availability during temporary, involuntary disconnections. However, our focus is on voluntary, client initiated disconnections, where hoarding can be used to predict data requirements. Existing hoarding approaches ignore conflicts arising out of write sharing and are thus unable to deal with them. However, since conflicts are detrimental to bandwidth utilisation, for scenarios with high write sharing, hoarding techniques need to provide support for sharing in a manner that reduces or avoids conflicts. We propose a hoarding approach for disconnected write operations that focuses on reducing the likelihood of conflicts, arising from write sharing, in a highly concurrent environment. Data that clients might need when disconnected is predicted based on the notion of semantic similarity. To avoid/reduce conflicts, data are first clustered based on their update probabilities. The hoard tree is then created based on the clusters and semantic similarity between data. Simulations show an increase in the cache hit-rate along with an reduction in the total number of conflicts.
[storage allocation, Costs, semantic similarity, cache storage, bandwidth utilisation, Delay, mobile computing, Wireless networks, Bandwidth, Computer networks, disconnected write operations, Availability, mobile environments, client-server systems, Computational modeling, concurrent environment, caching technique, hoarding approach, hoard tree, Maintenance, client initiated disconnections, conflict reduction, write sharing, distributed shared memory systems, Australia, Mobile computing]
Simple and efficient oracle-based consensus protocols for asynchronous Byzantine systems
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
This paper is on the consensus problem in asynchronous distributed systems where (up to f) processes (among n) can exhibit a Byzantine behavior, i.e., can deviate arbitrarily from their specification. A way to solve the consensus problem in such a context consists of enriching the system with additional oracles that are powerful enough to cope with the uncertainty and unpredictability created by the combined effect of Byzantine behavior and asynchrony. Considering two types of such oracles, namely, an oracle that provides processes with random values, and a failure detector oracle, the paper presents two families of Byzantine asynchronous consensus protocols. Two of these protocols are particularly noteworthy: they allow the processes to decide in one communication step in favorable circumstances. The first is a randomized protocol that assumes n > 5f. The second one is a failure detector-based protocol that assumes n > 6f. These protocols are designed to be particularly simple and efficient in terms of communication steps, the number of messages they generate in each step, and the size of messages. So, although they are not optimal in the number of Byzantine processes that can be tolerated, they are particularly efficient when we consider the number of communication steps they require to decide, and the number and size of the messages they use. In that sense, they are practically appealing.
[failure detector oracle, Protocols, Uncertainty, asynchronous Byzantine systems, Byzantine behavior, unreliable failure detector, asynchronous distributed systems, Distributed computing, Fault tolerance, oracle-based consensus protocols, Fault tolerant systems, Detectors, Broadcasting, protocols, randomized protocol, Distributed algorithms, distributed programming, fault tolerance, consensus problem, Byzantine asynchronous consensus protocols, Computer science, distributed algorithm, Fault detection, random oracle, distributed algorithms, fault tolerant computing, Byzantine process, failure detector-based protocol]
Run-time monitoring for dependable systems: an approach and a case study
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
This paper describes a run-time monitoring system designed for same functionality systems installed in different places that use equivalent hardware configurations, but with slightly different implementations. These systems exhibit common characteristics. They are large software systems, they depend on hardware to execute their functions, and they are usually adjusted to meet new user needs. In this scenario it is unreasonable to assume that software testing will uncover all latent errors. Besides gathering information about a target program as it executes the run-time monitoring system proposed provides information about the target operating system and the target hardware in order to improve availability by reducing time to diagnose failures and provide a system with the reactive capability of reconfiguring and reinitializing after the occurrence of a failure. A case study for an automatic teller machine system is discussed as an application of the run-time monitoring system and the results from this application are presented.
[Software testing, System testing, Computer aided software engineering, Costs, automatic teller machines, program testing, Computerized monitoring, software testing, software reliability, operating system, large software systems, dependable systems, Application software, system recovery, Condition monitoring, Runtime, network operating systems, run-time monitoring, hardware dependent system, Software systems, system monitoring, failure diagnosis, Hardware, automatic teller machine system]
Self-managing federated services
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
We consider the problem of deploying and managing federated services that run on federated systems spanning multiple collaborative organizations. In particular, we present a peer-to-peer framework targeted to the construction of self-managing services that automatically adjust the number of service components and their placements in response to changes in the system or client loads. Our framework is completely decentralized, depending only on a modest amount of loosely synchronized global state. More specifically, our framework is comprised of a set of per-node monitoring agents and per-service-component management agents that periodically exchange information about the state of the system and of the service with each other using a gossiping protocol. Each management agent then periodically searches for configurations that are better than the current one according to an application model and explicit performance and availability targets. On finding a better configuration, an agent will enact the new configuration after a random delay to avoid possible collisions. We evaluate our framework by studying a prototype UDDI service. We show that while agents act autonomously, the service rapidly reaches a stable and appropriate configuration in response to system dynamics.
[Protocols, multi-agent systems, peer-to-peer framework, gossiping protocol, Distributed computing, Environmental management, federated systems, prototype UDDI service, Web and internet services, Prototypes, distributed databases, client loads, multiple collaborative organization, loosely synchronized global state, distributed object management, Availability, client-server systems, per-node monitoring agents, object-oriented programming, peer-to-peer computing, Peer to peer computing, random delay, service components, self-managing federated services, Computer science, system dynamics, Web services, information exchange, Collaboration, Internet, per-service-component management agents]
Token-based atomic broadcast using unreliable failure detectors
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Many atomic broadcast algorithms have been published in the last twenty years. Token-based algorithms represent a large class of these algorithms. Interestingly, all the token-based atomic broadcast algorithms rely on a group membership service, i.e., none of them uses unreliable failure detectors directly. The paper presents the first token-based atomic broadcast algorithm that uses an unreliable failure detector - the new failure detector denoted by /spl Rscr/ - instead of a group membership service. The failure detector /spl Rscr/ is compared with <>V and <>S. In order to make it easier to understand the atomic broadcast algorithm, the paper derives the atomic broadcast algorithm from a token-based consensus algorithm that also uses the failure detector /spl Rscr/.
[Costs, fault diagnosis, Broadcast technology, Educational technology, consensus algorithm, Computer crashes, unreliable failure detector, Distributed computing, Fault tolerance, distributed algorithms, multicast protocols, group membership service, Detectors, algorithm theory, Broadcasting, Contracts, token-based algorithm, token-based atomic broadcast]
The /spl phi/ accrual failure detector
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
The detection of failures is a fundamental issue for fault-tolerance in distributed systems. Recently, many people have come to realize that failure detection ought to be provided as some form of generic service, similar to IP address lookup or time synchronization. However, this has not been successful so far; one of the reasons being the fact that classical failure detectors were not designed to satisfy several application requirements simultaneously. We present a novel abstraction, called accrual failure detectors, that emphasizes flexibility and expressiveness and can serve as a basic building block to implementing failure detectors in distributed systems. Instead of providing information of a binary nature (trust vs. suspect), accrual failure detectors output a suspicion level on a continuous scale. The principal merit of this approach is that it favors a nearly complete decoupling between application requirements and the monitoring of the environment. In this paper, we describe an implementation of such an accrual failure detector, that we call the /spl phi/ failure detector. The particularity of the /spl phi/ failure detector is that it dynamically adjusts to current network conditions the scale on which the suspicion level is expressed. We analyzed the behavior of our /spl phi/ failure detector over an intercontinental communication link over a week. Our experimental results show that if performs equally well as other known adaptive failure detection mechanisms, with an improved flexibility.
[fault diagnosis, Event detection, Quality of service, distributed processing, /spl phi/ accrual failure detector, system recovery, application requirements, Condition monitoring, Information science, Fault tolerant systems, Failure analysis, Detectors, distributed systems, adaptive mechanism, continuous information provision, fault tolerance, H infinity control, suspicion level, Computer crashes, intercontinental communication link, monitoring, Fault detection, data communication, fault tolerant computing, failure detection]
Message from the Symposium Chair
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Presents the welcome message from the conference proceedings.
[]
Message from the technical program co-chairs
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Conference committees
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Provides a listing of current committee members.
[]
Dependable pervasive systems
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Summary form only given. Present trends indicate that huge networked computer systems are likely to become pervasive, as information technology is embedded into virtually everything, and to be required to function essentially continuously. I believe that even today's (underused) "best practice" regarding the achievement of high dependability - reliability, availability, security, safety, etc. - from large networked computer systems will not suffice for future pervasive systems. I will give my perspective on the current state of research into the four basic dependability technologies: (i) fault prevention (to avoid the occurrence or introduction of faults), (ii) fault removal (through validation and verification), (iii) fault tolerance (so that failures do not necessarily occur even if faults remain), and (iv) fault forecasting (the means of assessing progress towards achieving adequate dependability). I will then argue that much further research is required on all four dependability technologies in order to cope with pervasive systems, identify some priorities, and discuss how this research could best be aimed at making system dependability into a "commodity" that industry can value and from which it can profit.
[reliability, fault prevention, fault forecasting, availability, ubiquitous computing, Best practices, technological forecasting, security, Computer network reliability, safety, networked computer systems, Computer networks, Safety, computer network reliability, Computer security, Pervasive computing, Availability, Embedded computing, fault tolerance, dependability, Information technology, fault removal, dependable pervasive systems, Information security, fault tolerant computing]
An integrated architecture for dependable embedded systems
Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.
None
2004
Summary form only given. A federated architecture is characterized in that every major function of an embedded system is allocated to a dedicated hardware unit. In a distributed control system this implies that adding a new function is tantamount to adding a new node. This has led to a order to achieve some functional coordination. Adding fault-tolerance to a federated architecture, e.g., by the provision of triple modular redundancy (TMR) leads to a further significant increase in the number of nodes and networks. The major advantages of a dedicated architecture are the physical encapsulation of the nearly autonomous subsystems, their outstanding fault containment and their clear-cut complexity management (independent development) in case the subsystems are nearly autonomous. An integrated distributed architecture for mixed-criticality applications must be based on a core design that supports the safety requirements of the highest considered criticality class. This is of particular importance in safety-critical applications, where the physical structure of the integrated system is determined to a significant extent by the independence requirement of fault-containment regions. The central part of an integrated distributed architecture for time-critical systems must provide the following core services: deterministic and timely transport of messages; fault tolerant clock synchronization; strong fault isolation with respect to arbitrary node failures; and consistent diagnosis of failing nodes. Any architecture that provides these core services can be used as a base architecture for an integrated distributed embedded system architecture. An example of such an integrated architecture is the time-triggered architecture (TTA). In this contribution we describe the structure and the services of the TTA that has been developed during the last twenty years and is deployed in a number of safety-critical applications in the transport sector.
[Encapsulation, safety requirements, arbitrary node failure, safety-critical software, distributed processing, dependable embedded systems, time-triggered architecture, message transport, fault tolerant clock synchronization, consistent failing node diagnosis, Fault tolerance, software architecture, time-critical systems, clear-cut complexity management, strong fault isolation, Embedded system, Fault tolerant systems, embedded systems, independent development, Hardware, Safety, federated architecture fault-tolerance, integrated distributed embedded system architecture, safety-critical application, transport sector, triple modular redundancy, Redundancy, mixed-criticality application, fault containment, Distributed control, fault tolerant computing, Time factors, autonomous subsystem, distributed control system, Clocks]
Message from the Symposium Chairs
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Presents the welcome message from the conference proceedings.
[]
Message from the Program Committee Co-Chairs
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Presents the introductory welcome message from the conference proceedings. May include the conference officer(s) offer(s) congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Provides a listing of current committee members.
[]
Consistency management among replicas in peer-to-peer mobile ad hoc networks
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Recent advances in wireless communication along with peer-to-peer (P2P) paradigm have led to increasing interest in P2P mobile ad hoc networks. In this paper, we assume an environment where each mobile peer accesses data items held by other peers which are connected by a mobile ad hoc network. Since peers' mobility causes frequent network partitions, replicas of a data item may be inconsistent due to write operations performed by mobile peers. In such an environment, the global consistency of data items is not desirable by many applications. Thus, new consistency maintenance based on local conditions such as location and time need to be investigated. This paper attempts to classify different consistency levels according to requirements from applications and provides protocols to realize them. We report simulation results to investigate the characteristics of these consistency protocols in a P2P wireless ad hoc network environment and their relationship with the quorum sizes.
[peer-to-peer computing, replicated databases, Peer to peer computing, data replica, data integrity, P2P wireless mobile ad hoc network, Application software, consistency management, Mobile ad hoc networks, wireless communication, Wireless communication, Computer science, Intelligent networks, mobile computing, peer-to-peer wireless mobile ad hoc network, Computer networks, Routing protocols, Computer network management, ad hoc networks, Mobile computing, consistency protocol]
Architecture-based autonomous repair management: an application to J2EE clusters
24th IEEE Symposium on Reliable Distributed Systems
None
2005
This paper presents a component-based architecture for autonomous repair management in distributed systems, and a prototype implementation of this architecture, called JADE, which provides repair management for J2EE application server clusters. The JADE architecture features three major elements, which we believe to be of wide relevance for the construction of autonomic distributed systems: (1) a dynamically configurable, component-based structure that exploits the reflective features of the FRACTAL component model; (2) an explicit and configurable feedback control loop structure, that manifests the relationship between the managed system and repair management functions; (3) an original replication structure for the management subsystem itself which makes it fault-tolerant and self-healing.
[workstation clusters, J2EE cluster, Feedback control, Distributed computing, system recovery, software architecture, Engineering management, Fault tolerant systems, Prototypes, Computer architecture, Computer networks, JADE architecture, autonomic distributed system, Pervasive computing, J2EE application server cluster, configurable feedback control loop structure, fault-tolerant computing, Java, object-oriented programming, FRACTAL component model, software maintenance, software fault tolerance, autonomous repair management, component-based architecture, Computer network management, Software engineering]
Automatic model-driven recovery in distributed systems
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Automatic system monitoring and recovery has the potential to provide a low-cost solution for high availability. However, automating recovery is difficult in practice because of the challenge of accurate fault diagnosis in the presence of low coverage, poor localization ability, and false positives that are inherent in many widely used monitoring techniques. In this paper, we present a holistic model-based approach that overcomes these challenges and enables automatic recovery in distributed systems. To do so, it uses theoretically sound techniques including Bayesian estimation and Markov decision theory to provide controllers that choose good, if not optimal, recovery actions according to a user-defined optimization criteria. By combining monitoring and recovery, the approach realizes benefits that could not have been obtained by using them in isolation. In this paper, we present two recovery algorithms with complementary properties and trade-offs, and validate our algorithms (through simulation) by fault injection on a realistic e-commerce system.
[fault diagnosis, decision theory, distributed processing, distributed system, system recovery, Condition monitoring, Fault diagnosis, optimisation, optimization, e-commerce system, fault injection, Testing, Availability, Bayesian estimation, Computerized monitoring, Redundancy, Application software, automatic system monitoring, Bayesian methods, Markov decision theory, Decision theory, Optimal control, Markov processes, system monitoring, automatic model-driven recovery, fault tolerant computing, Bayes methods, automatic system recovery]
Octopus: a fault-tolerant and efficient ad-hoc routing protocol
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Mobile ad-hoc networks (MANETs) are failure-prone environments; it is common for mobile wireless nodes to intermittently disconnect from the network, e.g., due to signal blockage. This paper focuses on withstanding such failures in large MANETs: we present Octopus, a fault-tolerant and efficient position-based routing protocol. Fault-tolerance is achieved by employing redundancy, i.e., storing the location of each node at many other nodes, and by keeping frequently refreshed soft state. At the same time, Octopus achieves a low location update overhead by employing a novel aggregation technique, whereby a single packet updates the location of many nodes at many other nodes. Octopus is highly scalable: for a fixed node density, the number of location update packets sent does not grow with the network size. And when the density increases, the overhead drops. Thorough empirical evaluation using the ns2 simulator with up to 675 mobile nodes shows that Octopus achieves excellent fault-tolerance at a modest overhead: when all nodes intermittently disconnect and reconnect, Octopus achieves the same high reliability as when all nodes are constantly up.
[mobile radio, fault tolerance, Redundancy, Ad hoc networks, ns2 simulator, Relays, position-based routing protocol, Octopus fault-tolerant ad-hoc routing protocol, Wireless communication, Degradation, Fault tolerance, Network servers, mobile ad-hoc network, MANET, routing protocols, Position measurement, failure-prone environment, Routing protocols, Telecommunication network reliability, ad hoc networks]
Reliable estimation of influence fields for classification and tracking in unreliable sensor networks
24th IEEE Symposium on Reliable Distributed Systems
None
2005
The influence field of an object, a commonly exploited feature in science and engineering applications, is the region where the object is detectable by a given sensing modality. Being spatially distributed, this feature allows us to tradeoff nodal computation with network communication. By the same token, not only is its calculation subject to nodal failures and false detections, but also to channel fading and channel contention. In this paper, we study how to accurately and efficiently estimate the influence fields of objects in such an unreliable setting and how this reliable estimation of influence fields can be used to classify and track different types of objects. We derive, for node and network fault models, the necessary nodal density for reliably estimating the influence fields so that objects can be classified and tracked. We present four algorithmic techniques: temporal aggregation, probabilistic reporting, temporal segregation and spatial reconstruction, to deal with cases where the effective network density differs from this minimum. We provide corroboration of our analysis through field experiments with Mica2 sensor nodes wherever appropriate. Finally, we demonstrate how these results and techniques were applied to achieve reliable and efficient classification and tracking in a fielded system of 90 Mica2 sensor nodes that we called "A Line In The Sand'.
[Shape, wireless sensor networks, fault-tolerance, reliable estimation, object classification, wireless sensor network, reliability, Sensor phenomena and characterization, network communication, Reliability engineering, Sensor systems, intrusion detection, channel fading, spatial reconstruction, Vehicles, Intelligent networks, false detection, Mica2 sensor node, Computer network reliability, telecommunication network reliability, object tracking, surveillance, faulttolerance, unreliable sensor network, fault tolerance, nodal failure, temporal segregation, channel contention, Computer science, Wireless sensor networks, probabilistic reporting, Object detection, target tracking, temporal aggregation, network fault model, object influence field]
Database replication using generalized snapshot isolation
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Generalized snapshot isolation extends snapshot isolation as used in Oracle and other databases in a manner suitable for replicated databases. While (conventional) snapshot isolation requires that transactions observe the "latest" snapshot of the database, generalized snapshot isolation allows the use of "older" snapshots, facilitating a replicated implementation. We show that many of the desirable properties of snapshot isolation remain. In particular, read-only transactions never block or abort and they do not cause update transactions to block or abort. Moreover, under certain assumptions on the transaction workload the execution is serializable. An implementation of generalized snapshot isolation can choose which past snapshot it uses. An interesting choice for a replicated database is prefix-consistent snapshot isolation, in which the snapshot contains at least all the writes of locally committed transactions. We present two implementations of prefix-consistent snapshot isolation. We conclude with an analytical performance model of one implementation, demonstrating the benefits, in particular reduced latency for read-only transactions, and showing that the potential downsides, in particular change in abort rate of update transactions, are limited.
[Availability, transaction processing, prefix-consistent snapshot isolation, replicated databases, update transaction, Transaction databases, database replication, History, generalized snapshot isolation, Delay, Programming profession, Analytical models, Fault tolerance, Web services, read-only transaction, Performance analysis, Oracle, Informatics]
Consistent main-memory database federations under deferred disk writes
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Current cluster architectures provide the ideal environment to run federations of main-memory database systems (FMMDBs). In FMMDBs, data resides in the main memory of the federation servers, significantly improving performance by avoiding I/O during the execution of read operations. To maximize the performance of update transactions as well, some applications recur to deferred disk writes. This means that update transactions commit before their modifications are written on stable storage and durability must be ensured outside the database. While deferred disk writes in centralized MMDBs relax the durability property of transactions only, in FMMDBs transaction atomicity may be also violated in case of failures. We address this issue from the perspective of log-based rollback-recovery in distributed systems and provide an efficient solution to the problem.
[transaction processing, consistent main-memory database system federation, rollbackrecovery, Costs, log-based rollback-recovery, distributed system, Telecommunication computing, disc storage, consistency, Delay, Concurrent computing, federation server, storage management, disk I/O, Computer architecture, distributed databases, distributed transactions, Database systems, Target tracking, transaction atomicity, Computer crashes, data integrity, Transaction databases, Application software, database update transaction, cluster architecture, disk write deferring, dependency tracking, MMDBs., distributed transaction, centralized MMDB]
Fault-tolerance for stateful application servers in the presence of advanced transactions patterns
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Replication is widely used in application server products to tolerate faults. An important challenge is to correctly coordinate replication and transaction execution for stateful application servers. Many current solutions assume that a single client request generates exactly one transaction at the server. However, it is quite common that several client requests are encapsulated within one server transaction or that a single client request can initiate several server transactions. In this paper, we propose a replication tool that is able to handle these variations in request/transaction association. We have integrated our approach into the J2EE application server JBoss. Our evaluation using the ECPerf benchmark shows a low overhead of the approach.
[transaction processing, client-server systems, Java, fault-tolerance, JBoss J2EE application server, Computer crashes, Transaction databases, transaction pattern, Application software, database management systems, Programming profession, software fault tolerance, request/transaction association, Computer science, application server, Fault tolerance, Management information systems, Database systems, replication tool, Internet, Web server]
From static distributed systems to dynamic systems
24th IEEE Symposium on Reliable Distributed Systems
None
2005
A noteworthy advance in distributed computing is due to the recent development of peer-to-peer systems. These systems are essentially dynamic in the sense that no process can get a global knowledge on the system structure. They mainly allow processes to look up for data that can be dynamically added/suppressed in a permanently evolving set of nodes. Although protocols have been developed for such dynamic systems, to our knowledge, up to date no computation model for dynamic systems has been proposed. Nevertheless, there is a strong demand for the definition of such models as soon as one wants to develop provably correct protocols suited to dynamic systems. This paper proposes a model for (a class of) dynamic systems. That dynamic model is defined by (1) a parameter (an integer denoted a) and (2) two basic communication abstractions (query-response and persistent reliable broadcast). The new parameter is a threshold value introduced to capture the liveness part of the system (it is the counterpart of the minimal number of processes that do not crash in a static system). To show the relevance of the model, the paper adapts an eventual leader protocol designed for the static model, and proves that the resulting protocol is correct within the proposed dynamic model. In that sense, the paper has also a methodological flavor, as it shows that simple modifications to existing protocols can allow them to work in dynamic systems.
[Protocols, Peer-to-Peer system., Communication abstraction, persistent reliable broadcast, Distributed computing, distributed computing, static distributed system, communication abstraction, Broadcasting, Grid computing, query-response broadcast, protocols, Persistent reliable broadcast, message passing, Stability, peer-to-peer computing, Peer to peer computing, Computational modeling, Computer crashes, Power system modeling, Computer science, Dynamic system, Query-response pattern, dynamic systems, peer-to-peer system, eventual stability condition]
Enforcing enterprise-wide policies over standard client-server interactions
24th IEEE Symposium on Reliable Distributed Systems
None
2005
We propose and evaluate a novel framework for enforcing global coordination and control policies over interacting software components in enterprise computing environments. This framework combines a per-node reference monitor with two existing coordination and control systems to enforce policies that, among other properties, are stateful and communal. Each reference monitor filters messages exchanged between the interacting software components similar to a firewall, passing only messages that are allowed by the policies in effect. This filtering approach decouples coordination and control from application implementation, allowing the coordination and control mechanism and application implementations to evolve independently of each other. We demonstrate the power of our framework by using it to specify and enforce an RBAC policy with delegation, revocation, and separation-of-duty over accesses to a cluster of NFS and SMB file servers without changing any client or server implementations. Measurements show that our framework imposes acceptable overheads when enforcing this policy.
[Access control, client-server systems, Protocols, message passing, object-oriented programming, enterprise-wide policy enforcement, NFS file servers, Filtering, Computerized monitoring, per-node reference monitor, Control systems, File servers, client-server interactions, Application software, Helium, Computer science, Filters, enterprise computing environments, RBAC policy, file servers, SMB file servers, authorisation, software component interaction]
Thema: Byzantine-fault-tolerant middleware for Web-service applications
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Distributed applications composed of collections of Web services may call for diverse levels of reliability in different parts of the system. Byzantine fault tolerance (BFT) is a general strategy that has recently been shown to be practical for the development of certain classes of survivable, client-server, distributed applications; however, little research has been done on incorporating it into selective parts of multi-tier, distributed applications like Web services that have heterogeneous reliability requirements. To understand the impacts of combining BFT and Web services, we have created Thema, a new BFT middleware system that extends the BFT and Web services technologies to provide a structured way to build Byzantine-fault-tolerant, survivable Web services that application developers can use like other Web services. From a reliability perspective, our enhancements are also novel in that they allow Byzantine-fault-tolerant services: (1) to support the multi-tiered requirements of Web services, and (2) to provide standardized Web services support for their own clients (through WSDL interfaces and SOAP communication). In this paper we study key architectural implications of combining BFT with Web services and provide a performance evaluation of Thema using the TPC-W benchmark.
[Availability, Byzantine-fault-tolerant middleware, TPC-W benchmark, WSDL interfaces, performance evaluation, Computer crashes, Application software, Security, Simple object access protocol, multitiered requirements, Middleware, Web-service applications, Computer science, Fault tolerance, Web services, Diversity reception, Thema, SOAP communication, fault tolerant computing, Internet, middleware]
Agile Store: experience with quorum-based data replication techniques for adaptive Byzantine fault tolerance
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Quorum protocols offer several benefits when used to maintain replicated data but techniques for reducing overheads associated with them have not been explored in detail. It is desirable that a system be able to adapt its operation so that fault tolerance related overheads are only incurred when the protocol execution actually encounters faults. There are a number of issues that need to be carefully examined to achieve such agility of quorum based systems. We make use of a file system prototype, developed in our Agile Store project, to experimentally evaluate several techniques that are important for efficient implementation of Byzantine fault-tolerant quorum protocols. We present an optimistic quorum collection scheme and a probabilistic hashing scheme for determining the response to a quorum request, and show that they lead to significant performance improvements. The Agile Store also makes use of reconfigurable quorum techniques to allow system size and fault threshold to be dynamically varied when, for example, faulty servers are removed, new servers are added, or the threat level is changed. We quantify the performance gains made possible by such reconfiguration of quorum parameters. We also show how performance scales with different system parameters and how it is affected by design choices such as whether to use proxies. We believe that the results in the paper provide important insights into how to implement quorum protocols to provide good performance while achieving Byzantine fault tolerance.
[Protocols, Maintenance engineering, Educational institutions, Data engineering, cryptography, quorum protocols, Sun, quorum-based data replication, Home computing, Fault tolerance, optimistic quorum collection scheme, File systems, Agile Store project, Fault tolerant systems, distributed algorithms, Prototypes, file system prototype, probabilistic hashing scheme, fault tolerant computing, adaptive Byzantine fault tolerance, protocols]
Distributed construction of a fault-tolerant network from a tree
24th IEEE Symposium on Reliable Distributed Systems
None
2005
We present an algorithm by which nodes arranged in a tree, with each node initially knowing only its parent and children, can construct a fault-tolerant communication structure (an expander graph) among themselves in a distributed and scalable way. The tree overlayed with this logical expander is a useful structure for distributed applications that require the intrinsic "treeness" from the topology but cannot afford any obstruction in communication due to failures. At the core of our construction is a novel distributed mechanism that samples nodes uniformly at random from the tree. In the event of node joins, node departures or node failures, the expander maintains its own fault tolerance and permits the reformation of the tree. We present simulation results to quantify the convergence of our algorithm to a fault tolerant network having both good vertex connectivity and expansion properties.
[Tree data structures, Peer to peer computing, computer networks, fault-tolerant network, logical expander, Multicast protocols, Graph theory, Topology, Application software, trees, node departure, Convergence, Fault tolerance, Tree graphs, vertex connectivity, distributed algorithms, fault tolerant computing, tree data structures, fault-tolerant communication structure, Distributed algorithms, node failures]
Design and performance-study of crash-tolerant protocols for broadcasting and reaching consensus in MANETs
24th IEEE Symposium on Reliable Distributed Systems
None
2005
The mobile ad-hoc networking (MANET) technology offers an ideal medium for hosting self-organized collaborative applications in terrains with no infrastructure support for untethered communication. Collaboration involves users with potentially different initial opinions deciding identically, i.e., reaching consensus. Efficient consensus solutions require efficient broadcast support. This paper presents four crash-tolerant broadcast protocols which are designed (i) to provide the maximum broadcast coverage that can ever be guaranteed, and (ii) to suit a wide range of MANET types: from a connected MANET (no partitions) to intermittently disconnected one (partitions occurring rarely and healing swiftly) to an intermittently connected one (partitions taking longer to heal and re-appearing swiftly). The resulting design challenges are addressed systematically, presenting two foundational results that would guide the protocol design. The protocols' performance is then studied through simulations for a range of node speeds and network densities. The best-performing one is used to host a consensus protocol as its 'application'. The overhead and the latency for reaching consensus are measured; surprisingly, they are hardly affected as the number of nodes with distinct initial opinions increases beyond one.
[mobile ad-hoc networking technology, Protocols, Broadcast technology, performance evaluation, Mobile communication, Computer crashes, TemporaryPartitions, self-organized collaborative applications, Delay, Consensus, Mobile ad hoc networks, broadcasting, Crash-tolerance, Asynchronous communication, Intelligent networks, mobile computing, Collaboration, crash-tolerant broadcast protocols, Broadcasting, Simulations., fault tolerant computing, ad hoc networks, protocols, Ad-hoc networking]
Lazy verification in fault-tolerant distributed storage systems
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Verification of write operations is a crucial component of Byzantine fault-tolerant consistency protocols for storage. Lazy verification shifts this work out of the critical path of client operations. This shift enables the system to amortize verification effort over multiple operations, to perform verification during otherwise idle time, and to have only a subset of storage-nodes perform verification. This paper introduces lazy verification and describes implementation techniques for exploiting its potential. Measurements of lazy verification in a Byzantine fault-tolerant distributed storage system show that the cost of verification can be hidden from both the client read and write operation in workloads with idle periods. Furthermore, in workloads without idle periods, lazy verification amortizes the cost of verification over many versions and so provides a factor of four higher write bandwidth when compared to performing verification during each write operation.
[client-server systems, Protocols, Costs, Cache storage, Computer crashes, Degradation, Fault tolerance, Home appliances, storage management, write operation verification, formal verification, Fault tolerant systems, Bandwidth, distributed databases, fault-tolerant distributed storage systems, fault tolerant computing, Digital signatures, Byzantine fault-tolerant consistency protocols]
Asynchronous verifiable information dispersal
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Information dispersal addresses the question of storing a file by distributing it among a set of servers in a storage-efficient way. We introduce the problem of verifiable information dispersal in an asynchronous network, where up to one third of the servers as well as an arbitrary number of clients might exhibit Byzantine faults. Verifiability ensures that the stored information is consistent despite such faults. We present a storage and communication-efficient scheme for asynchronous verifiable information dispersal that achieves an asymptotically optimal storage blow-up. Additionally, we show how to guarantee the secrecy of the stored data with respect to an adversary that may mount adaptive attacks. Our technique also yields a new protocol for asynchronous reliable broadcast that improves the communication complexity by an order of magnitude on large inputs.
[client-server systems, Protocols, Fingerprint recognition, File servers, Time measurement, Byzantine faults, Complexity theory, communication complexity, asynchronous verifiable information dispersal problem, broadcasting, Network servers, storage management, optimal storage blow-up, security of data, distributed databases, Broadcasting, fault tolerant computing, Error correction codes, Cryptography, protocols, Rotation measurement]
A new look at atomic broadcast in the asynchronous crash-recovery model
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Atomic broadcast in particular, and group communication in general, have mainly been specified and implemented in a system model where processes do not recover after a crash. The model is called crash-stop. The drawback of this model is its inability to express algorithms that tolerate the crash of a majority of processes. This has led to extend the crash-stop model to the so-called crash-recovery model, in which processes have access to stable storage, to log their state periodically. This allows them to recover a previous state after a crash. However, the existing specifications of atomic broadcast in the crash-recovery model are not satisfactory, and the paper explains why. The paper also proposes a new specification of atomic broadcast in the crash-recovery model that addresses these issues. Specifically, our new specification allows to distinguish between a uniform and a non-uniform version of atomic broadcast. The non-uniform version logs less information, and is thus more efficient. The uniform and non-uniform atomic broadcast have been implemented and compared with a published atomic broadcast algorithm. Performance results are presented.
[message passing, multiprocessing systems, Computer crashes, Electronic mail, atomic broadcast specification, Distributed computing, system recovery, group communication, broadcasting, Fault tolerance, Detectors, Broadcasting, asynchronous crash-recovery model, Database systems, fault tolerant computing, crash-stop model, Context modeling]
LRRM: a randomized reliable multicast protocol for optimizing recovery latency and buffer utilization
24th IEEE Symposium on Reliable Distributed Systems
None
2005
An efficient recovery protocol for lost messages is crucial for supporting reliable multicasting. The tree-based recovery protocols group nodes into recovery regions and designate a recovery node per region for buffering and retransmitting lost messages. In these protocols, the recovery host may get overloaded during periods of large message losses and costly remote recovery may be initiated even though a peer node has the lost message. To address these drawbacks, the randomized reliable multicast protocol (RRMP) was proposed which distributes the responsibility of error recovery among all members in a group. The pressure on the buffer and computational resources on the intermediate nodes is increasing due to the wide distribution of multicast participants with widely varying reception rates and periodic disconnections. In this paper, we propose the lightweight randomized reliable multicast (LRRM) protocol that optimizes the amount of buffer space by providing an efficient mechanism based on best-effort multicast for retrieving a lost message. A theoretical analysis and a simulation based study of two realistic topologies indicate that LRRM provides comparable recovery latency to RRMP for lower buffer space usage. While presented in the context of RRMP, LRRM can also benefit other tree-based reliable multicast protocols.
[Laboratories, Reliability engineering, Randomized protocols, Distributed computing, system recovery, Delay, Postal services, Analytical models, optimisation, recovery protocol, multicast communication, randomized reliable multicast protocol, protocols, buffer utilization, Tree-based multicast protocols., buffer storage, message passing, Peer to peer computing, Probability, Multicast protocols, Topology, Reliable multicast, buffer space usage, recovery latency optimization, Recovery latency, Buffer utilization]
A distributed algorithm for path restoration in circuit switched communication networks
24th IEEE Symposium on Reliable Distributed Systems
None
2005
Path restoration is an important approach for building survivable telecommunication backbone networks. Path restoration is known for high restoration efficiency and its ability to protect against single link, multiple link and node failures. Path restoration can be formulated as the well-known multi-commodity network flow (MCNF) problem. While many centralized algorithms have been proposed for solving the MCNF problem, distributed algorithms have received very little attention. This paper presents an online distributed multi-commodity flow approximation algorithm specifically tailored for path restoration. Our algorithm uses O(|E|diam/sup 2/) messages and O(diam/sup 2/) time in the worst case, and substantially fewer messages and less time in practical networks. When simulated on a sample real-life backbone network similar to those used by the telecommunication service providers, our algorithm finds a solution significantly faster than many published algorithms.
[path restoration, Spine, computer networks, Telecommunication traffic, Telecommunication switching, circuit switched communication networks, circuit switching, online distributed multicommodity flow approximation algorithm, Communication switching, Switching circuits, telecommunication switching, Computer science, Intelligent networks, distributed algorithm, multicommodity network flow problem, centralized algorithms, distributed algorithms, telecommunication network routing, Communication networks, Distributed algorithms, Protection, real-life backbone network, computational complexity]
Message from the Symposium Chair
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Presents the welcome message from the conference proceedings.
[]
Message from the Technical Program Co-chairs
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Presents the welcome message from the conference proceedings.
[]
Conference Committees
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Provides a listing of current committee members.
[]
Reducing the Availability Management Overheads of Federated Content Sharing Systems
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
We consider the problem of ensuring high data availability in federated content sharing systems. Ideally, such a system would provide high data availability in a device transparent manner so that users are not faced with the time-consuming and error-prone task of managing data replicas across the constituent devices of the system. We propose a novel unified availability model and a decentralized replication algorithm to approximate this ideal. Our availability model addresses three different concerns: availability during connected operation (online), availability during disconnected operation (offline), and availability after permanent disconnection from the federated system (ownership). Our replication algorithm centers around the intuition that devices should selfishly use their local storage to ensure offline and ownership availability for their individual owners. Excess storage, however, is used communally to ensure high online availability for all shared content. Evaluation of an implementation shows that our algorithm rapidly reaches stable and communally desirable configurations when there is sufficient space. Consistent with the fact that devices in a federated system are owned by different users, however, as space becomes highly constrained, the system approaches a non-cooperative configuration where devices only hoard content to serve their individual owners' needs
[Availability, Content management, Portable computers, federated content sharing systems, content management, data availability management overhead reduction, decentralized replication algorithm, Computer science, unified availability model, File systems, data replica management, Computer errors, Collaborative work, data handling, Personal communication networks, Personal digital assistants, Online Communities/Technical Collaboration]
Topology Sensitive Replica Selection
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
As the disks typically found in personal computers grow larger, protecting data by replicating it on a collection of "peer" systems rather than on dedicated high performance storage systems can provide comparable reliability and availability guarantees but at reduced cost and complexity. In order to be adopted, peer-to-peer storage systems must be able to replicate data on hosts that are trusted, secure, and available. However, recent research has shown that the traditional model, where nodes are assumed to have identical levels of trust, to behave independently, and to have similar failure modes, is over simplified. Thus, there is a need for a mechanism that automatically and efficiently selects replica nodes from a large number of available hosts with varying capabilities and trust levels. In this paper we present an algorithm to handle replica node selection either for new replica groups or to replace failed replicas in a peer-to-peer storage system. We show through simulation that our algorithm maintains the node inter-connection topology minimizing the cost of recovery from a failed replica, measured by the number of nodes affected by the failure and the number of inter-node messages
[Availability, peer-to-peer storage systems, Costs, peer-to-peer computing, Peer to peer computing, replica node selection, topology sensitive replica selection, Microcomputers, Topology, personal computers, Secure storage, Computer science, inter-node messages, data replication, Bandwidth, digital storage, node inter-connection topology, data protection, Safety, data handling, Protection]
Deleting Files in the Celeste Peer-to-Peer Storage System
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Celeste is a robust peer-to-peer object store built on top of a distributed hash table (DHT). Celeste is a working system, developed by Sun Microsystems Laboratories. During the development of Celeste, we faced the challenge of complete object deletion, and moreover, of deleting "files" composed of several different objects. This important problem is not solved by merely deleting meta-data, as there are scenarios in which all file contents must be deleted, e.g., due to a court order. Complete file deletion in a realistic peer-to-peer storage system has not been previously dealt with due to the intricacy of the problem - the system may experience high churn rates, nodes may crash or have intermittent connectivity, and the overlay network may become partitioned at times. We present an algorithm that eventually deletes all file content, data and meta-data, in the aforementioned complex scenarios. The algorithm is fully functional and has been successfully integrated into Celeste
[file deletion, peer-to-peer computing, Peer to peer computing, Data security, Laboratories, Computer crashes, meta-data deletion, Partitioning algorithms, object deletion, Sun, Secure storage, Read only memory, Sun Microsystems Laboratories, Storage area networks, Celeste peer-to-peer storage system, distributed hash table, file organisation, Robustness]
WRAPS: Denial-of-Service Defense through Web Referrals
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
The Web is a complicated graph, with millions of Web sites interlinked together. In this paper, we propose to use this Web sitegraph structure to mitigate flooding attacks on a Web site, using a new Web referral architecture for privileged service ("WRAPS"). WRAPS allows a legitimate client to obtain a privilege URL through a click on a referral hypher-link, from a Web site trusted by the target Web site. Using that URL, the client can get privileged access to the target Web site in a manner that is far less vulnerable to a DDoS flooding attack. WRAPS does not require changes to Web client software and is extremely lightweight for referrer Web sites, which eases its deployment. The massive scale of the Web sitegraph could deter attempts to isolate a Web site through blocking all referrers. We present the design of WRAPS, and the implementation of a prototype system used to evaluate our proposal. Our empirical study demonstrates that WRAPS enables legitimate clients to connect to a Web site smoothly in spite of an intensive flooding attack, at the cost of small overheads on the Web site's ISP's edge routers
[telecommunication security, ISP edge routers, Costs, distributed denial-of-service, World Wide Web, Floods, Proposals, Computer crime, Uniform resource locators, Prototypes, DDoS flooding attack, referral hypher-link, Web referral architecture for privileged service, Software prototyping, Service oriented architecture, Web sitegraph structure, denial-of-service defense, Computer science, Web referrals, flooding attacks, WRAPS, Internet, Web sites, Joining processes, privilege URL]
A Client-Transparent Approach to Defend Against Denial of Service Attacks
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Denial of service (DoS) attacks attempt to consume a server's resources (network bandwidth, computing power, main memory, disk bandwidth etc.) to near exhaustion so that there are no resources left to handle requests from legitimate clients. An effective solution to defend against DoS attacks is to filter DoS attack requests at the earliest point (say, the Web site's firewall), before they consume much of the server's resources. Most defenses against DoS attacks attempt to filter requests from inauthentic clients before they consume much of the server's resources. Client authentication using techniques like IPSec or SSL may often require changes to the client-side software and may additionally require superuser privileges at the client for deployment. Further, using digital signatures (as in SSL) makes verification very expensive, thereby making the verification process itself a viable DoS target for the adversary. In this paper, we propose a light-weight client transparent technique to defend against DoS attacks with two unique features: (i) Our technique can be implemented entirely using JavaScript support provided by a standard client-side browser like Mozilla FireFox or Microsoft Internet Explorer. Client transparency follows from the fact that: (i) no changes to client-side software are required, (ii) no client-side superuser privileges are required, and (iii) clients (human beings or automated clients) can browse a DoS protected Web site in the same manner that they browse other Web sites, (ii) Although we operate using the client-side browser (HTTP layer), our technique enables fast IP level packet filtering at the server's firewall and requires no changes to the application(s) hosted by the Web server. In this paper we present a detailed design of our technique along with a detailed security analysis. We also describe a concrete implementation of our proposal on the Linux kernel and present an evaluation using two applications: bandwidth intensive Apache HTTPD and database intensive TPCW. Our experiments show that our approach incurs a low performance overhead and is resilient to DoS attacks
[IPSec, light-weight client transparent technique, Web Servers, server firewall, Humans, Linux kernel, client-side browser, Information filtering, Denial, Microsoft Internet Explorer, Computer crime, client-transparent approach, Mozilla FireFox, of Service (DoS) attacks, HTTP layer, Bandwidth, authorisation, JavaScript, Information filters, Computer networks, Web server, Digital signatures, Availability, Java, client authentication, database intensive TPCW, denial of service attacks, IP level packet filtering, SSL, DoS protected Web sites, Client Transparency, Authentication, Apache HTTPD, online front-ends, digital signatures, Internet]
Proactive Resilience Revisited: The Delicate Balance Between Resisting Intrusions and Remaining Available
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
In a recent paper, we presented proactive resilience as a new approach to proactive recovery, based on architectural hybridization. We showed that, with appropriate assumptions about fault rate, proactive resilience makes it possible to build distributed intrusion-tolerant systems guaranteed not to suffer more than the assumed number of faults during their lifetime. In this paper, we explore the impact of these assumptions in asynchronous systems, and derive conditions that should be met by practical systems in order to guarantee long-lived, i.e., available, intrusion-tolerant operation. Our conclusions are based on analytical and simulation results as implemented in Mobius, and we use the same modeling environment to show that our approach offers higher resilience in comparison with other proactive intrusion-tolerant system models
[architectural hybridization, Laboratories, distributed processing, Mobius, proactive recovery, proactive resilience, distributed intrusion-tolerant systems, Online services, Computer crime, system recovery, Resilience, Analytical models, security of data, Web and internet services, Resists, intrusion resilience, Large-scale systems, Informatics, asynchronous systems, Electronic government]
Call Availability Prediction in a Telecommunication System: A Data Driven Empirical Approach
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Availability prediction in a telecommunication system plays a crucial role in its management, either by alerting the operator to potential failures or by proactively initiating preventive measures. In this paper, we apply linear (ARMA, multivariate, random walk) and nonlinear (Radial and Universal Basis Functions) regression techniques to recognize system failures and to predict the system's call availability up to 15 minutes in advance. Secondly we introduce a novel nonlinear modeling technique for call availability prediction. We benchmark all five techniques against each other. The applied modeling methods are data driven rather than analytical and can handle large amounts of data. We apply the modeling techniques to real data of a commercial telecommunication platform. The data used for modeling includes: a) time stamped event-based log files; and b) continuously measured system states. Results are given in terms of a) receiver operator characteristics (AUC) for classification into classes of failure and non-failure states and b) as a cost-benefit analysis. Our findings suggest: a) high degree of nonlinearity in the data; b) statistically significant improved forecasting performance and cost-benefit ratio of nonlinear modeling techniques; and finally finding that c) log file data does not contribute to improve model performance with any modeling technique
[system call availability prediction, System testing, nonlinear regression, telecommunication system, universal basis functions regression, Humans, regression analysis, Predictive models, linear regression, continuously measured system states, telecommunication computing, data driven empirical approach, cost-benefit analysis, Economic forecasting, data driven modeling, random walk regression, radial regression, Availability, time stamped event-based log files, receiver operator characteristics, Linear regression, forecasting, Time measurement, Software debugging, ARMA regression, nonlinear modeling, commercial telecommunication platform, Software systems, Computer industry, data handling, multivariate regression, system failure recognition]
FT-PPTC: An Efficient and Fault-Tolerant Commit Protocol for Mobile Environments
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Transactions are required not only for wired networks but also for the emerging wireless environments where mobile and fixed hosts participate side by side in the execution of the transaction. This heterogenous environment is characterized by constraints in mobile host capabilities, network connectivity and also an increasing number of possible failure modes. Classical atomic commit protocols used in wired networks are therefore not directly suitable for this heterogenous environment. Furthermore, the few commit protocols designed for mobile transactions either consider mobile hosts only as initiators though not as active participants, or show a high resource blocking time. We present the Fault-Tolerant Pre-Phase Transaction Commit (FT-PPTC) protocol for mobile environments. FT-PPTC decouples the commit of mobile participants from that of fixed participants. Consequently, the commit set can be reduced to a set of entities in the fixed network. Thus, the commit can easily be supported by any traditional atomic commit protocol, such as the established 2PC protocol. We integrate fault-tolerance as a key feature of FT-PPTC. Performance evaluations confirm the efficiency, scalability and low resource blocking time of our approach
[transaction processing, mobile environments, mobile transactions, Fault-Tolerant Pre-Phase Transaction Commit protocol, Wireless application protocol, Scalability, FT-PPTC, Spatial databases, Electronic mail, Transaction databases, Fault tolerance, mobile computing, Wireless networks, wireless environments, Distributed databases, Resists, heterogenous environment, atomic commit protocols, fault tolerant computing, protocols, Business]
Modeling Distributed Computing System Reliability with DRBD
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Nowadays the great part of devices or systems we commonly use are often driven or managed by microchips and computers: cars, music players, phones, trains, planes, .... A consolidated trend of technology is to substitute mechanical with electronic parts, analogical with digital devices or controls, and so on. In this context, features like security, availability and reliability, usually summarized under the concept of dependability, are receiving higher attention. The dependability analysis, especially for what regards critical parts as computing systems or subsystems, is becoming more strategic: specific requirements and explicit or tighter constraints have to be satisfied. Even though this fact, there is a lack of suitable tools to properly model and analyze these aspects, with particular reference to reliability. To fill this gap, we propose the dynamic reliability block diagram (DRBD) modeling tool derived from the reliability block diagram (RBD) formalism. The DRBD permits to model the dynamic reliability behavior of a system through dependence models, exploited to represent dynamics behaviors as redundancy, load sharing, multiple, probabilistic and common failure mode. In this paper, the DRBD expressiveness and other capabilities, are illustrated through the analysis of a complex distributed computing system taken as example
[Availability, Neodymium, dynamic reliability system behavior, Redundancy, software reliability, DRBD modeling tool, distributed processing, Reliability engineering, distributed computing system reliability modeling, Roentgenium, Distributed computing, Digital control, dynamic reliability block diagram, dependence models, Engineering management, system dependability analysis, reliability block diagram formalism, Resource management, Fault trees]
DRIFT: Efficient Message Ordering in Ad Hoc Networks Using Virtual Flooding
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
We present DRIFT - a total order multicast algorithm for ad hoc networks with mobile or static nodes. Due to the ad hoc nature of the network, DRIFT uses flooding for message propagation. The key idea of DRIFT is virtual flooding - a way of using unrelated message streams to propagate message causality information in order to accelerate message delivery. We describe DRIFT in detail. We evaluate its performance in a simulator and in a wireless sensor network. In both cases our results demonstrate that the performance of DRIFT exceeds that of the simple total order multicast algorithm designed for wired networks, on which it is based. In simulation at scale, for certain experiment settings, DRIFT achieved speedup of several orders of magnitude
[Algorithm design and analysis, message stream, electronic messaging, message propagation, wireless sensor network, efficient message ordering, Floods, causality information, mobile computing, virtual flooding, multicast communication, message passing, simulator, ad hoc network, performance evaluation, Routing, Ad hoc networks, Radio broadcasting, DRIFT, Computer science, Wireless sensor networks, Multicast algorithms, total order multicast algorithm, message delivery, mobile node, static node, Acceleration, ad hoc networks, Mobile computing]
Generalised Repair for Overlay Networks
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
We present and evaluate a generic approach to the repair of overlay networks which identifies general principles of overlay repair and embodies these as a reusable service. At the heart of our approach is an algorithm that discovers the extent of a failed section of any type of overlay, and assigns responsibility to carry out the repair. The repair strategy itself is 'pluggable' and can be tailored to the requirements of a specific overlay type or instance. Our approach is efficient in terms of the number of repair-related message exchanges it incurs; scalable in that it involves only nodes in the locality of the failed section of the overlay; and resilient in that it correctly handles cases in which multiple adjacent nodes fail simultaneously, and it tolerates new failures that occur while a repair is underway. The benefits of our approach are that: (i) it extracts and encapsulates best practice in repair for overlays; (ii) it simplifies the design and implementation of new overlays (because repair issues can be treated orthogonally to basic functionality); and (iii) it supports tailorable levels of dependability for overlays, including pluggable repair strategies
[Heart, Tree data structures, Protocols, generalised overlay network repair, pluggable repair strategy, Wheels, computer networks, Routing, Computer crashes, repair-related message exchange, Best practices, overlay failed section, multiple adjacent node, Network topology, Computer networks, fault tolerant computing, reusable service]
Decentralized Local Failure Detection in Dynamic Distributed Systems
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
A failure detector is an important building block when constructing fault-tolerant distributed systems. In asynchronous distributed systems, failed processes are often indistinguishable from slow processes. A failure detector is an oracle that can intelligently suspect processes to have failed. Different classes of failure detectors have been proposed to solve different kinds of problems. Almost all of this work is focused on global failure detection, and moreover, in systems that do not contain mobile nodes or include dynamic topologies. In this paper, we present diamPm <sub>l</sub> - a local failure detector that can tolerate mobility and topology changes. This means that diamPm <sub>l</sub> can distinguish between a failed process and a process that has moved away from its original location. We also establish an upper bound on the duration for which a process wrongly suspects a node that has moved away from its neighborhood. We support our theoretical results with experimental findings from an implementation of this algorithm for sensor networks
[local failure detector, asynchronous distributed system, distributed processing, Electrical fault detection, Sensor systems, global failure detection, Topology, Distributed computing, system recovery, decentralized local failure detection, dynamic distributed system, failed process, Upper bound, slow process, Fault detection, sensor network, Fault tolerant systems, Detectors, Broadcasting, fault-tolerant distributed system, fault tolerant computing, Resource management]
Improvements and Reconsideration of Distributed Snapshot Protocols
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Distributed snapshots are an important building block for distributed systems, and, among other applications, are useful for constructing efficient checkpointing protocols. In addition to the imposed overhead of the existing distributed snapshot protocols, those protocols are not trivially applicable (if at all) in many of today's distributed systems, e.g., grid, mobile, and sensors systems. After presenting the shortages and the inapplicability of the most popular existing distributed snapshot protocols, this paper discusses improvement directions for the protocols. In addition, it presents a new and an important improvement for the most popular distributed snapshot protocol, which was presented by Chandy and Lamport in 1985. Although the proposed improvement is simple and easy to implement, it has significant benefits in reducing the software and hardware overheads of distributed snapshots. Then, the paper presents proofs for the safety and progress of the new protocol. Lastly, it presents a performance analysis of the protocol using stochastic models
[Checkpointing, checkpointing, Wireless application protocol, protocol performance analysis, Access protocols, Software performance, distributed processing, performance evaluation, distributed system, Control systems, Sensor systems, checkpointing protocol, distributed snapshot protocol, Grid computing, Hardware, Safety, Performance analysis, protocols, stochastic model]
Weakly-Persistent Causal Objects in Dynamic Distributed Systems
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
In the context of clients accessing a read/write shared object, persistency of a written value is a property stating that a value written into the object is always available unless overwritten by a successive write operation. This property can be easily guaranteed in a static distributed system provided that either a subset of processes implementing the object does not crash or processes can crash and then recover being able to retrieve their last state. Unfortunately the enforcing of this property in a potentially large scale and dynamic distributed system (e.g. a P2P system) is far from being trivial when considering the case in which processes implementing the object may fail or leave at any time without notifying any other process (i.e., the last state might not be retrievable). The paper introduces the notion of weak persistency that guarantees persistency of values when a system becomes quiescent (arrivals and departures subside). An implementation of a weakly-persistent object ensuring causal consistency is provided along with its correctness proof. The interest of causal consistency lies in the fact that, contrarily to atomic consistency, it can be maintained even during non-quiescent periods of the distributed system (i.e., when persistency is not guaranteed)
[Protocols, weakly-persistent causal object, causal consistency, Peer to peer computing, distributed processing, Computer crashes, weak persistency, Distributed computing, correctness proof, dynamic distributed system, quiescent, Message passing, Abstracts, Resists, Computer networks, Large-scale systems, atomic consistency, Clocks]
Non-Blocking Synchronous Checkpointing Based on Rollback-Dependency Trackability
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
This article proposes an original approach that applies the rollback-dependency trackability (RDT) property to implement a new non-blocking synchronous checkpointing protocol, called RDT-NBS, that takes mutable checkpoints and efficiently supports concurrent initiators. Mutable checkpoints can be saved in non-stable storage and make it possible for non-blocking synchronous checkpointing protocols to save a minimal number of checkpoints in stable storage during the construction of a consistent global checkpoint. We prove that this minimality property does not hold in presence of concurrent checkpointing initiations. Even though, RDT-NBS uses mutable checkpoints to reduce the use of stable memory assuring the existence of a consistent global checkpoint in stable storage. We also present simulation results that compare RDT-NBS to quasi-synchronous RDT
[Checkpointing, checkpointing, storage allocation, Protocols, nonblocking synchronous checkpointing, Fault tolerant systems, concurrency control, global checkpoint, RDT-NBS, rollback-dependency trackability, mutable checkpoints, concurrent initiators]
PLATO: Predictive Latency-Aware Total Ordering
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
PLATO is a predictive total ordering protocol designed for low-latency multicast in datacenters. It predicts out-of-order arrival of multicast packets by observing their inter-arrival times, and delays packets before passing them up to the application only if it believes the packets to have arrived in the wrong order. We show through experimentation on real datacenter-style networks that the inter-arrival time of consecutive packet pairs is an excellent predictor of out-of-order delivery. We evaluate an implementation of PLATO on the Emulab testbed, and show that it drives down delivery latencies by more than a factor of 2 compared to the fixed-sequencer protocol
[Out of order, Context, System testing, Costs, Military computing, inter-arrival time, packet switching, Drives, Multicast protocols, Application software, Delay, out-of-order delivery, datacenter-style network, Emulab testbed, Computer science, multicast protocols, multicast packet out-of-order arrival, packet delay, low-latency multicast, predictive latency-aware total ordering protocol]
Cryptree: A Folder Tree Structure for Cryptographic File Systems
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
We present Cryptree, a cryptographic tree structure which facilitates access control in file systems operating on untrusted storage. Cryptree leverages the file system's folder hierarchy to achieve efficient and intuitive, yet simple, access control. The highlights are its ability to recursively grant access to a folder and all its subfolders in constant time, the dynamic inheritance of access rights which inherently prevents scattering of access rights, and the possibility to grant someone access to a file or folder without revealing the identities of other accessors. To reason about and to visualize Cryptree, we introduce the notion of cryptographic links. We describe the Cryptrees we have used to enforce read and write access in our own file system. Finally, we measure the performance of the Cryptree and compare it to other approaches
[Tree data structures, Access control, Visualization, cryptographic link, untrusted storage, Laboratories, Scattering, cryptographic file system, cryptographic tree structure, cryptography, file system folder hierarchy, File systems, Cryptree, folder tree structure, authorisation, Permission, Public key cryptography, Computer networks, access control, tree data structures, Testing]
How To Safeguard Your Sensitive Data
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
In order to safeguard a sensitive database, we must ensure both its privacy and its longevity. However, privacy and longevity tend to be competing objectives. We show how to design a system that provides both good privacy and good longevity simultaneously. Systems are modelled as compositions of two basic operators, copy and split. We propose metrics with which to evaluate the privacy, longevity and performance offered by such systems. The search for the "best" system under these metrics is then formulated as a constrained optimization problem. Solving the optimization problem exactly turns out to be intractable, so we propose techniques for efficiently finding an approximate solution
[Data privacy, copy operator, Error analysis, Credit cards, database management systems, Constraint optimization, Computer science, Fault tolerance, optimisation, Databases, Hospitals, split operator, data privacy, Cryptography, sensitive database, Protection, constrained optimization problem, software metrics]
Solving Consensus Using Structural Failure Models
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Failure models characterise the expected component failures in fault-tolerant computing. In the context of distributed systems, a failure model usually consists of two parts: a functional part specifying in what way individual processing entities may fail and a structural part specifying the potential scope of failures within the system. Such models must be expressive enough to cover all relevant practical situations, but must also be simple enough to allow uncomplicated reasoning about fault-tolerant algorithms. Usually, an increase in expressiveness complicates formal reasoning, but enables more accurate models that allow to improve the assumption coverage and resilience of solutions. In this paper, we introduce the structural failure model class DiDep that allows to specify directed dependent failures, which, for example, occur in the area of intrusion tolerance and security. DiDep is a generalisation of previous classes for undirected dependent failures, namely the general adversary structures, the fail-prone systems, and the core and survivor sets, which we show to be equivalent. We show that the increase in expressiveness of DiDep does not significantly penalise the simplicity of corresponding models by giving an algorithm that transforms any consensus algorithm for undirected dependent failures into a consensus algorithm for a DiDep model. We characterise the improved resilience obtained with DiDep and show that certain models even allow to circumvent the famous FLP impossibility result
[fault-tolerant computing, intrusion tolerance, distributed processing, consensus algorithm, Computer crashes, Security, Distributed computing, software fault tolerance, Resilience, Computer science, Fault tolerance, component failures, security, security of data, undirected dependent failures, Fault tolerant systems, distributed systems, Hardware, structural failure model, Software engineering, Context modeling, DiDep model]
Performance evaluation of a fair fault-tolerant mutual exclusion algorithm
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
This paper presents an efficient and fair fault-tolerant token-based algorithm for achieving mutual exclusion. It is an extension of the Naimi-Trehel algorithm that uses a distributed queue of token requests and a dynamic tree. In case of failures, our algorithm tries to recover the requests' queue by gathering intact portions of the one which existed just before the failure. Thus, fairness of token requests is preserved despite failures. Furthermore, the use of broadcast is minimized when rebuilding the dynamic tree. Experiment results with different fault injection scenarios show that our approach presents a fast failure recovery and low message broadcast overhead
[Costs, Naimi-Trehel algorithm, program verification, Heuristic algorithms, Nominations and elections, trees (mathematics), fair fault-tolerant mutual exclusion algorithm, performance evaluation, Proposals, Delay, dynamic tree, Fault tolerance, distributed algorithms, distributed queue, Broadcasting, Permission, Traffic control, fault tolerant computing, Distributed algorithms, fair fault-tolerant token-based algorithm]
Experimental Comparison of Local and Shared Coin Randomized Consensus Protocols
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
The paper presents a comparative performance study of the two main classes of randomized binary consensus protocols: a local coin protocol, with an expected high communication complexity and cheap symmetric cryptography, and a shared coin protocol, with an expected low communication complexity and expensive asymmetric cryptography. The experimental evaluation was conducted on a LAN environment, by varying several system parameters, such as the fault types and number of processes. The analysis shows that there is a significant gap between the theoretical and the practical performance results of these protocols, and provides an important insight into what actually happens during their execution
[symmetric cryptography, Humans, distributed processing, cryptography, Computer crashes, shared coin randomized consensus protocol, randomized binary consensus protocol, Complexity theory, communication complexity, Delay, Cryptographic protocols, local coin randomized consensus protocol, Failure analysis, Bandwidth, Performance analysis, Cryptography, protocols, Local area networks]
Hidden Markov Models as a Support for Diagnosis: Formalization of the Problem and Synthesis of the Solution
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
In modern information infrastructures, diagnosis must be able to assess the status or the extent of the damage of individual components. Traditional one-shot diagnosis is not adequate, but streams of data on component behavior need to be collected and filtered over time as done by some existing heuristics. This paper proposes instead a general framework and a formalism to model such over-time diagnosis scenarios, and to find appropriate solutions. As such, it is very beneficial to system designers to support design choices. Taking advantage of the characteristics of the hidden Markov models formalism, widely used in pattern recognition, the paper proposes a formalization of the diagnosis process, addressing the complete chain constituted by monitored component, deviation detection and state diagnosis. Hidden Markov models are well suited to represent problems where the internal state of a certain entity is not known and can only be inferred from external observations of what this entity emits. Such over-time diagnosis is a first class representative of this category of problems. The accuracy of diagnosis carried out through the proposed formalization is then discussed, as well as how to concretely use it to perform state diagnosis and allow direct comparison of alternative solutions
[monitored component, fault diagnosis, Quality of service, diagnosis process formalization, Control systems, Pattern recognition, software fault tolerance, Fault diagnosis, Fault tolerance, hidden Markov models, hidden Markov models formalism, Filters, information infrastructure, formal verification, diagnosis support, Councils, Hidden Markov models, state diagnosis, deviation detection, system monitoring, Bonding, Monitoring]
Consistent Replication of Multithreaded Distributed Objects
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Determinism is mandatory for replicating distributed objects with strict consistency guarantees. Multithreaded execution of method invocations is a source of nondeterminism, but helps to improve performance and avoids deadlocks that nested invocations can cause in a single-threaded execution model. This paper contributes a novel algorithm for deterministic thread scheduling based on the interception of synchronisation statements. It assumes that shared data are protected by mutexes and client requests are sent to all replicas in total order; requests are executed concurrently as long as they do not issue potentially conflicting synchronisation operations. No additional communication is required for granting locks in a consistent order in all replicas. In addition to reentrant mutex locks, the algorithm supports condition variables and time-bounded wait operations. An experimental evaluation shows that, in some typical usage patterns of distributed objects, the algorithm is superior to other existing approaches
[object-oriented programming, multi-threading, Multicast protocols, consistent replication, Computer crashes, Yarn, Scheduling algorithm, Computer science, Concurrent computing, deterministic thread scheduling, Operating systems, multithreaded distributed objects, System recovery, scheduling, method invocation, Frequency synchronization, Protection, distributed object management]
Recovering from Distributable Thread Failures with Assured Timeliness in Real-Time Distributed Systems
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
We consider the problem of recovering from failures of distributable threads with assured timeliness. When a node hosting a portion of a distributable thread fails, it causes orphans - i.e., thread segments that are disconnected from the thread's root. We consider a termination model for recovering from such failures, where the orphans must be detected and aborted, and failure-exception notification must be delivered to the farthest, contiguous surviving thread segment for resuming thread execution. We present a realtime scheduling algorithm called AUA, and a distributable thread integrity protocol called TP-TR. We show that AUA and TP-TR bound the orphan cleanup and recovery time, thereby bounding thread starvation durations, and maximize the total thread accrued timeliness utility. We implement AUA and TP-TR in a real-time middleware that supports distributable threads. Our experimental studies with the implementation validate the algorithm/protocol's time-bounded recovery property and confirm their effectiveness
[Real time systems, Protocols, failure-exception notification, distributed processing, failures recovery, Yarn, Middleware, Sun, system recovery, Scheduling algorithm, software fault tolerance, Phased arrays, Concurrent computing, termination model, real time scheduling, Time factors, Resource management, distributable thread failures, real-time distributed systems, distributable thread integrity protocol]
SegmentShield: Exploiting Segmentation Hardware for Protecting against Buffer Overflow Attacks
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
This paper presents a strong and efficient scheme for protecting against buffer overflow attacks. The basic approach of this scheme is pointer copying: copies of code pointers are stored in a safe memory area to detect and prevent the manipulation of code pointers. In order to protect the copied code pointers from data-pointer modification attacks, this scheme exploits the segmentation hardware of IA-32 (Intel x86) processors. This scheme provides as strong protection as write-protecting the memory area via system calls. On the other hand, this scheme involves a modest overhead because copying a code pointer requires only a few user-level instructions and there is no penalty of entering the kernel. The experimental results show that the performance overhead in OpenSSL ranges from 0.9% to 4.3%
[pointer copying, buffer storage, SegmentShield, Security, buffer overflow attack, Information science, Runtime, security of data, Linux, data-pointer modification attack, Hardware, Agriculture, Buffer overflow, Cryptography, Protection, Kernel]
A Scalable Services Architecture
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Data centers constructed as clusters of inexpensive machines have compelling cost-performance benefits, but developing services to run on them can be challenging. This paper reports on a new framework, the scalable services architecture (SSA), which helps developers develop scalable clustered applications. The work is focused on non-transactional high-performance applications; these are poorly supported in existing platforms. A primary goal was to keep the SSA as small and simple as possible. Key elements include a TCP-based "chain replication" mechanism and a gossip-based subsystem for managing configuration data and repairing inconsistencies after faults. Our experimental results confirm the effectiveness of the approach
[Availability, Protocols, Fluctuations, TCP-based chain replication, scalable clustered application, Service oriented architecture, data centers, Quality of service, gossip-based subsystem, Computer crashes, Transaction databases, scalable services architecture, Computer science, software architecture, Web services, configuration data management, Computer architecture]
Fault-tolerant and scalable TCP splice and web server architecture
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
This paper describes three enhancements to the TCP splicing mechanism: (1) Enable a TCP connection to be simultaneously spliced through multiple machines for higher scalability; (2) Make a spliced connection fault-tolerant to proxy failures; and (3) Provide flexibility of splitting a TCP splice between a proxy and a backend server for further increasing the scalability of a Web server system. A Web server architecture based on this enhanced TCP splicing is proposed. This architecture provides a highly scalable, seamless service to the users with minimal disruption during server failures. In addition to the traditional Web services in which users download Web pages, multimedia files and other types of data from a Web server, the proposed architecture supports newly emerging Web services that are highly interactive, and involve relatively longer, stateful client-server sessions. A prototype of this architecture has been implemented as a Linux 2.6 kernel module, and the paper presents important performance results measured from this implementation
[spliced connection, Splicing, Scalability, Web server architecture, Service oriented architecture, Web server system, software fault tolerance, Fault tolerance, software architecture, Web services, Linux, transport protocols, Fault tolerant systems, proxy failure tolerance, Web pages, Prototypes, file servers, fault-tolerant TCP splice, scalable TCP splice, Web server]
Adaptive Batching for Replicated Servers
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
This paper presents two novel generic adaptive batching schemes for replicated servers. Both schemes are oblivious to the underlying communication protocols. Our novel schemes adapt their batching levels automatically and immediately according to the current communication load. This is done without any explicit monitoring or calibration of the system. Additionally, the paper includes a detailed performance evaluation
[Protocols, Costs, network servers, adaptive batching, replicated servers, performance evaluation, Throughput, Calibration, Distributed computing, Delay, Computer science, Bandwidth, Computational efficiency, Monitoring]
Satem: Trusted Service Code Execution across Transactions
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Web services and service oriented architectures are becoming the de facto standard for Internet computing. A main problem faced by users of such services is how to ensure that the service code is trusted. While methods that guarantee trusted service code execution before starting a client-service transaction exist, there is no solution for extending this assurance to the entire lifetime of the transaction. This paper presents Satem, a Service-aware trusted execution monitor that guarantees the trustworthiness of the service code across a whole transaction. The Satem architecture consists of an execution monitor residing in the operating system kernel on the service provider platform, a trust evaluator on the client platform, and a service commitment protocol. During this protocol, executed before every transaction, the client requests and verifies against its local policy a commitment from the service platform that promises trusted code execution. Subsequently, the monitor enforces this commitment for the duration of the transaction. To initialize the trust on the monitor, we use the Trusted Platform Module specified by the Trusted Computing Group. We implemented Satem under the Linux 2.6.12 kernel and tested it for a Web service and DNS. The experimental results demonstrate that Satem does not incur significant overhead to the protected services and does not impact the unprotected services
[transaction processing, Protocols, service commitment protocol, trusted service code execution, Service oriented architecture, Internet computing, Satem architecture, execution monitor, client-service transaction, software architecture, Web services, security of data, Operating systems, Linux, Web and internet services, trust evaluator, service oriented architecture, Kernel, Protection, Monitoring, Testing]
Coordination in Loosely Coupled Systems
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Internet-scale distributed applications are frequently built as loosely coupled compositions of services. We would like that despite loose coupling, each constituent service has a mutually consistent view of the state of the application not withstanding software, hardware and network related problems (e.g., clock skews, unpredictable transmission delays, message loss, node crashes etc.). Some observations on how messaging middleware should be structured to accomplish this are presented.
[Protocols, Web services, Operating systems, Linux, Web and internet services, Service oriented architecture, Kernel, Protection, Monitoring, Testing]
Systematic composition and analyzability of dependable networked embedded computing systems
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
In the past decade, the application field of networked embedded computing systems (NECSs) has been showing gradual acceleration in its growth. A great majority of NECSs are subject to some fault tolerance requirements. In other words, in their application environments, the fault rates of some components, e.g., communication links, batteries, some chips, etc., are not negligible. So, research interests in fault-tolerant (FT) NECSs have been showing growing trends in the past decade in good contrast to the relatively stagnant research interests in more traditional branches of FTC such as FT data servers, etc. In particular, object-/component-oriented (OO/CO) structuring techniques for RT distributed computing systems have been established in highly promising forms with convincing demonstrations although they are still very slow in spreading through industry. Such OO/CO structuring techniques not only lead to efficient design of RTC application systems with considerably reduced labor and improved maintainability but also enable design of complex systems yielding analyzable and yet tight response time bounds. On the basis of such foundation for designing RTC systems, research in FT NECS can now proceed with much improved effectiveness and confidence
[component-oriented structuring technique, Embedded computing, object-oriented programming, dependable networked embedded computing systems, RT distributed computing systems, Distributed computing, Middleware, Delay, software fault tolerance, Fault tolerance, Network servers, object-oriented structuring technique, Fault detection, Fault tolerant systems, Refining, fault tolerance requirements, embedded systems, National electric code]
"Open and challenging research issues in dependable distributed computing" A personal view from the Defence Industry
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Network enabled capability (NEC) is the UK MoD's response to the rapidly changing conflict environment in which its forces must operate. The armed forces must be flexible, ready and rapidly deployable, and must possess attributes that allow the application of controlled and precise force to achieve realisable effects as part of a wider scene that includes diplomatic and political aspects. The implications of this operational goal are immense and will stimulate significant organisational changes throughout the entire defence supply chain, with knock-on effects in other industrial sectors and civilian environments. Recognising that achievement of NEC requires co-evolution across many different development areas and demands innovation throughout the supply chain, it is apparent that multidisciplinary research that includes a clear understanding of the integration issues is required. This may be attempted through development and application of systems engineering approaches to the delivery of through-life capability for NEC
[Technological innovation, dependable distributed computing, Supply chains, software reliability, distributed processing, integration issues, Distributed computing, conflict environment, Defense industry, defence supply chain, defence industry, Force control, National security, network enabled capability, military computing]
AVCast : New Approaches For Implementing Availability-Dependent Reliability for Multicast Receivers
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Today's large-scale distributed systems consist of collections of nodes that have highly variable availability - a phenomenon sometimes called churn. This availability variation is often a hindrance to achieving reliability and performance for distributed applications such as multicast. This paper looks into utilizing and leveraging availability information in order to provide availability-dependent message reliability for multicast receivers. An application (e.g., a publish-sub scribe system) may want to scale the multicast message reliability at each receiver according to that receiver's availability (in terms of the fraction of time that receiver is online) <sub>i</sub>fferent options are that the reliability is independent of the availability, or proportional to it. We propose several gossip-based algorithms to support several such predicates. These techniques rely on each node's availability being monitored in a distributed manner by a small group of other nodes in such a way that the monitoring load is evenly distributed in the system. Our techniques are light-weight, scalable, and are space- and time-efficient. We analyze our algorithms and evaluate them experimentally by injecting availability traces collected from real peer-to-peer systems
[Availability, message passing, Peer to peer computing, monitoring load distribution, orig-research, Switches, Multicast protocols, multicast receivers, Computer science, Multicast algorithms, resource allocation, gossip-based algorithms, Publish-subscribe, multicast communication, telecommunication network reliability, availability-dependent message reliability, Streaming media, distributed systems, Large-scale systems, Monitoring]
MOve: Design of An Application-Malleable Overlay
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Peer-to-peer overlays allow distributed applications to work in a wide-area, scalable, and fault-tolerant manner. However, most structured and unstructured overlays present in literature today are inflexible from the application viewpoint. In other words, the application has no control over the structure of the overlay itself. This paper proposes the concept of an application-malleable overlay, and the design of the first malleable overlay which we call MOve. In MOve, the communication characteristics of the distributed application using the overlay can influence the overlay's structure itself, with the twin goals of (1) optimizing the application performance by adapting the overlay, while also (2) retaining the large scale and fault tolerance of the overlay approach. The influence could either be explicitly specified by the application or implicitly gleaned by our algorithms. Besides neighbor list membership management, MOve also contains algorithms for resource discovery, update propagation, and churn-resistance. The emergent behavior of the implicit mechanisms used in MOve manifests in the following way: when application communication is low, most overlay links keep their default configuration; however, as application communication characteristics become more evident, the overlay gracefully adapts itself to the application
[Engineering profession, peer-to-peer computing, resource discovery, Peer to peer computing, Scalability, Communication system control, neighbor list membership management, application communication, Maintenance, distributed applications, application performance, Fault tolerance, application-malleable overlay, MOve, update propagation, Collaboration, peer-to-peer overlays, Large-scale systems, Internet, Resource management]
An SNMP based failure detection service
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
In this paper, we present the SNMP-FD service, a novel failure detection service entirely based on the Simple Network Management Protocol (SNMP). This approach promises better interoperability with external tools and failure information sources, including network equipment and cluster management tools. We first show how the SNMP standard can be used to build a failure detection service. We describe the already standardized interfaces that can be reused and introduce the interfaces that need to be added. SNMP is used extensively in the service for messaging, process status description, configuration, services statistics and delivering failure detection information to applications. We then present our implementation and an evaluation of performance and quality of service
[Protocols, open systems, Quality of service, performance evaluation, Computer crashes, network equipment tools, Electronic mail, Simple Network Management Protocol, quality of service, Middleware, Statistics, system recovery, cluster management tools, Technology management, SNMP based failure detection service, Detectors, protocols, SNMP standard, Web server, Quality management]
Lightweight Reflection for Middleware-based Database Replication
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Middleware-based database replication approaches have emerged in the last few years as an alternative to traditional database replication implemented within the database kernel. A middleware approach enables third party vendors to provide high availability solutions, a growing practice nowadays in the software industry. However, middleware solutions often lack scalability and exhibit a number of consistency and performance issues. The reason is that in most cases the middleware has to handle the database as a black box, and hence, cannot take advantage of the many optimizations implemented in the database kernel. Thus, middleware solutions often reimplement key functionality but cannot achieve the same efficiency as a kernel implementation. Reflection has been proposed during the last decade as a fruitful paradigm to separate non-functional aspects from functional ones, simplifying software development and maintenance whilst fostering reuse. However, fully reflective databases are not feasible due to the high cost of reflection. Our claim is that by exposing some minimal database functionality through a lightweight reflective interface, efficient and scalable middleware database replication can be attained. In this paper we explore a wide variety of such lightweight reflective interfaces and discuss what kind of replication algorithms they enable. We also discuss implementation alternatives for some of these interfaces and evaluate their performance
[Availability, Software maintenance, Costs, replicated databases, software development, Scalability, reflection, Programming, reflective databases, lightweight reflective interface, database replication, software maintenance, Middleware, Databases, middleware-based database replication, Computer industry, database functionality, Optical reflection, fault-tolerant distributed systems, Kernel, fault-tolerant distributed systems., middleware]
Improving DBMS Performance through Diverse Redundancy
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Database replication is widely used to improve both fault tolerance and DBMS performance. Non-diverse database replication has a significant limitation - it is effective against crash failures only. Diverse redundancy is an effective mechanism of tolerating a wider range of failures, including many non-crash failures. However it has not been adopted in practice because many see DBMS performance as the main concern. In this paper we show experimental evidence that diverse redundancy (diverse replication) can bring benefits in terms of DBMS performance, too. We report on experimental results with an optimistic architecture built with two diverse DBMSs under a load derived from TPC-C benchmark, which show that a diverse pair performs faster not only than non-diverse pairs but also than the individual copies of the DBMSs used. This result is important because it shows potential for DBMS performance better than anything achievable with the available off-the-shelf servers
[replicated databases, fault tolerance, Redundancy, diverse redundancy, Performance gain, Computer crashes, Software reliability, database replication, DBMS performance, Middleware, system recovery, Fault tolerance, Databases, System performance, Computer bugs, crash failures, Protection, software performance evaluation]
Managing Transaction Conflicts in Middleware-based Database Replication Architectures
2006 25th IEEE Symposium on Reliable Distributed Systems
None
2006
Database replication protocols need to detect, block or abort part of conflicting transactions. A possible solution is to check their writesets (and also their readsets in case a serialisable isolation level is requested), which however burdens the consumption of CPU time. This gets even worse when the replication support is provided by a middleware, since there is no direct DBMS support in that layer. We propose and discuss the use of the concurrency control support of the local DBMS for detecting conflicts between local transactions and writesets of remote transactions. This allows to simplify many database replication protocols and to enhance their performance
[transaction processing, local transactions, replicated databases, middleware-based database replication architectures, Delay systems, database replication protocols, remote transactions, Multicast protocols, Data structures, Concurrency control, Transaction databases, Maintenance, transaction conflict management, Middleware, Abortion, concurrency control, Testing, middleware]
26th IEEE International Symposium on Reliable Distributed Systems - Introduction
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
false
[]
Message from the Symposium Chair
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Presents the welcome message from the conference proceedings.
[]
Welcome from the Technical Program Co-Chairs
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Presents the welcome message from the conference proceedings.
[]
Conference Committees
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Provides a listing of current committee members.
[]
ASFALT: A Simple Fault-Tolerant Signature-based Localization Technique for Emergency Sensor Networks
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
We consider the problem of robust node deployment and fault-tolerant localization in wireless sensor networks for emergency and first response applications. Signature-based localization algorithms are a popular choice for use in such applications due to the non-uniform nature of the sensor node deployment. But, random destruction/disablement of sensor nodes in such networks adversely affects the deployment strategy as well as the accuracy of the corresponding signature-based localization algorithm. In this paper, we first model the phenomenon of sensor node destruction as a non-homogeneous Poisson process and derive a robust and efficient strategy for sensor node deployment based on this model. Next, we outline a protocol, called Group Selection Protocol, that complements current signature-based algorithms by reducing localization errors even when some nodes in a group are destroyed. Finally, we propose a novel yet simple localization technique, ASFALT, that improves the efficiency of the localization process by combining the simplicity of range-based schemes with the robustness of signature-based ones. Simulation experiments are conducted to verify the performance of the proposed algorithms.
[group selection protocol, Protocols, fault tolerance, wireless sensor networks, wireless sensor network, Sensor phenomena and characterization, Sensor systems and applications, emergency application, ASFALT localization, Application software, first response application, Global Positioning System, Fault tolerance, Wireless sensor networks, Computer network reliability, Fault tolerant systems, sensor node destruction, Robustness, emergency sensor network, protocols, stochastic processes, fault-tolerant signature-based localization, nonhomogeneous Poisson process]
RAPID: Reliable Probabilistic Dissemination in Wireless Ad-Hoc Networks
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
In this paper, we propose a novel reliable probabilistic dissemination protocol, RAPID, for mobile wireless ad-hoc networks that tolerates message omissions, node crashes, and selfish behavior. The protocol employs a combination of probabilistic forwarding with deterministic corrective measures. The forwarding probability is set based on the observed number of nodes in each one-hop neighborhood, while the deterministic corrective measures include deterministic gossiping as well as timer based corrections of the probabilistic process. These aspects of the protocol are motivated by a theoretical analysis that is also presented in the paper, which explains why this unique protocol design is inherent to ad-hoc networks environments. Since the protocol only relies on local computations and probability, it is highly resilient to mobility and failures. The paper includes a detailed performance evaluation by simulation. We compare the performance and the overhead of RAPID with the performance of other probabilistic approaches. Our results show that RAPID achieves a significantly higher node coverage with a smaller overhead.
[Wireless application protocol, Computational modeling, Ad hoc networks, radio access networks, Computer science, mobile computing, Computer network reliability, Collaboration, probabilistic dissemination protocol, Broadcasting, Robustness, Telecommunication network reliability, ad hoc networks, protocols, mobile wireless ad-hoc networks, Mobile computing, RAPID]
The Eventual Clusterer Oracle and Its Application to Consensus in MANETs
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
This paper studies the design of hierarchical consensus protocols for mobile ad hoc networks. A two-layer hierarchy is imposed on the mobile hosts by grouping them into clusters, each with a clusterhead. The messages from and to the hosts in the same cluster are merged/unmerged by the clusterhead so as to reduce the message cost and improve the scalability. We adopt a modular method in the design, separating clustering from achieving consensus using the clusters. The clustering function, named eventual clusterer (denoted as diamC), is designed to construct a cluster-based hierarchy over the mobile hosts in the network. Since diamC provides the fault tolerant clustering function transparently, it can be used as a new oracle (i.e. an abstract tool to provide some kind of information about the state of the system) for the design of hierarchical consensus protocols. Based on diamC, we design a new consensus protocol, which can significantly reduce the message cost of achieving consensus. We also propose an implementation of the diamC oracle based on the failure detector diamS.
[Protocols, Costs, Scalability, Design methodology, eventual clusterer oracle, Switches, Mobile ad hoc networks, Fault tolerant systems, mobile hosts, fault tolerant clustering, mobile ad hoc networks, failure detector, Detectors, hierarchical consensus protocols, protocols, Random number generation, message passing, clusterhead, cluster-based hierarchy, Computer crashes, mobile clusters, MANET, mobile communication, message cost reduction, ad hoc networks]
Modeling and Assessing the Dependability ofWireless Sensor Networks
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
This paper proposes a flexible framework for dependability modeling and assessing of Wireless Sensor Networks (WSNs). The framework takes into account network related aspects (topology, routing, network traffic) as well as hardware/software characteristics of nodes (type of sensors, running applications, power consumption). It is composed of two basic elements: i) a parametric Stochastic Activity Networks (SAN) failure model, reproducing WSN failure behavior as inferred from a detailed Failure Mode Effect Analysis (FMEA), and ii) an external library reproducing network behavior on behalf of the SAN model. This library specializes the SAN model by feeding it with quantitative parameters obtained by simulation or by experimental campaigns; it is also in charge of updating the network state in response to failure events during the simulation (e.g., routing tree updated due to node failures). The framework is thus suited to evaluate the dependability of several WSNs, with different topologies, routing algorithms, hardware/software platforms, without requiring any changes to its structure. The use of the external library makes the model simpler, decoupling the network behavior from the failure behavior. Simulation experiments are discussed that provide a quantitative evaluation of WSN dependability for a sample scenario: results show how the proposed framework supports WSN developers to find proper cost-reliability trade-offs for the system being deployed.
[wireless sensor networks, failure mode effect analysis, failure events, network routing, hardware/software platforms, Telecommunication traffic, Sensor phenomena and characterization, telecommunication network topology, Routing, Discrete event simulation, network topology, network traffic, routing algorithms, Wireless sensor networks, Storage area networks, Software libraries, Network topology, stochastic activity networks failure model, telecommunication network routing, Traffic control, Hardware, telecommunication traffic]
Enhancing Edge Computing with Database Replication
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
As the use of the Internet continues to grow explosively, edge computing has emerged as an important technique for delivering Web content over the Internet. Edge computing moves data and computation closer to end-users for fast local access and better load distribution. Current approaches use caching, which does not work well with highly dynamic data. In this paper, we propose a different approach to enhance edge computing. Our approach lies in a wide area data replication protocol that enables the delivery of dynamic content with full consistency guarantees and with all the benefits of edge computing, such as low latency and high scalability. What is more, the proposed solution is fully transparent to the applications that are brought to the edge. Our extensive evaluations in a real wide area network using TPC-W show promising results.
[Wide area networks, Scalability, edge computing, Transaction databases, database replication, database management systems, Distributed computing, Middleware, Delay, content management, Web content delivery, Network servers, wide area data replication protocol, Computer networks, Internet, Web server]
Customizable Fault Tolerance forWide-Area Replication
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Constructing logical machines out of collections of physical machines is a well-known technique for improving the robustness and fault tolerance of distributed systems. We present a new, scalable replication architecture, built upon logical machines specifically designed to perform well in wide-area systems spanning multiple sites. The physical machines in each site implement a logical machine by running a local state machine replication protocol, and a wide-area replication protocol runs among the logical machines. Implementing logical machines via the state machine approach affords free substitution of the fault tolerance method used in each site and in the wide-area replication protocol, allowing one to balance performance and fault tolerance based on perceived risk. We present a new byzantine fault-tolerant protocol that establishes a reliable virtual communication link between logical machines. Our communication protocol is efficient (a necessity in wide-area environments), avoiding the need for redundant message sending during normal-case operation and allowing a logical machine to consume approximately the same wide-area bandwidth as a single physical machine. This dramatically improves the wide-area performance of our system compared to existing logical machine based approaches. We implemented a prototype system and compare its performance and fault tolerance to existing solutions.
[Protocols, replicated databases, wide area networks, wide-area system, wide-area replication protocol, distributed system, local state machine replication protocol, communication protocol, reliable virtual communication link, Information systems, Fault tolerance, Network servers, transport protocols, Fault tolerant systems, Prototypes, scalable replication architecture, Bandwidth, customizable fault tolerance, Robustness, fault tolerant computing, logical machine, byzantine fault-tolerant protocol, Large-scale systems, Local area networks]
Hypervisor-Based Efficient Proactive Recovery
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Proactive recovery is a promising approach for building fault and intrusion tolerant systems that tolerate an arbitrary number of faults during system lifetime. This paper investigates the benefits that a virtualization-based replication infrastructure can offer for implementing proactive recovery. Our approach uses the hypervisor to initialize a new replica in parallel to normal system execution and thus minimizes the time in which a proactive reboot interferes with system operation. As a consequence, the system maintains an equivalent degree of system availability without requiring more replicas than a traditional replication system. Furthermore, having the old replica available on the same physical host as the rejuvenated replica helps to optimize state transfer. The problem of remote transfer is reduced to remote validation of the state in the frequent case when the local replica has not been corrupted.
[Costs, hypervisor-based efficient proactive recovery, distributed processing, Application software, Communication system security, Distributed computing, Virtual machine monitors, security of data, Operating systems, Computer networks, fault tolerant computing, virtualization-based replication infrastructure, Large-scale systems, Communication networks, Computer security, fault tolerant systems, intrusion tolerant systems]
Test &#x00026; Set, Adaptive Renaming and Set Agreement: a Guided Visit to Asynchronous Computability
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
An important issue in fault-tolerant asynchronous computing is the respective power of an object type with respect to another object type. This question has received a lot of attention, mainly in the context of the consensus problem where a major advance has been the introduction of the consensus number notion that allows ranking the synchronization power of base object types (atomic registers, queues, test&amp;set objects, compare&amp;swap objects, etc.) with respect to the consensus problem. This has given rise to the well-known Herlihy's hierarchy. Due to its very definition, the consensus number notion is irrelevant for studying the respective power of object types that are too weak to solve consensus for an arbitrary number of processes (these objects are usually called subconsensus objects). Considering an asynchonous system made up of n processes prone to crash, this paper addresses the power of such object types, namely, the k-test&amp;set object type, the k-set agreement object type, and the adaptive M-renaming object type for M = 2p - [P/N] and M = min(2p - 1,p + k - 1), where p &lt; n is the number of processes that want to acquire a new name. It investigates their respective power stating the necessary and sufficient conditions to build objects of any of these types from objects of any of the other types. More precisely, the paper shows that (1) these object types define a strict hierarchy when k ne1,n - 1, (2) they all are equivalent when k = n - 1, and (3) they all are equivalent except k-set agreement that is stronger when k = 1 ne n - 1 (a side effect of these results is that that the consensus number of the renaming problem is 2.)
[Algorithm design and analysis, System testing, Adaptive systems, Herlihy hierarchy, set agreement, Vehicle crash testing, asynchronous computability, Computer crashes, consensus problem, Distributed computing, adaptive renaming, Computer science, Fault tolerance, Sufficient conditions, USA Councils, fault-tolerant asynchronous computing, power synchronization, test&amp;set objects, fault tolerant computing]
The Fail-Heterogeneous Architectural Model
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Fault tolerant distributed protocols typically utilize a homogeneous fault model, either fail-crash or fail-Byzantine, where all processors are assumed to fail in the same manner. In practice, due to complexity and evolvability reasons, only a subset of the nodes can actually be designed to have a restricted, fail-crash failure mode, provided that they are free of design faults. Based on this consideration, we propose a fail-heterogeneous architectural model for distributed systems which considers two classes of nodes: (a) full-fledged execution nodes, which can be fail-Byzantine, and (b) lightweight, validated coordination nodes, which can only be fail-crash. To illustrate the model we introduce HeterTrust as a practical trustworthy service replication protocol. It has a low latency overhead, requires few execution nodes with diversified design, and prevents intruded servers from disclosing confidential data. We also discuss applications of the model to DoS attacks mitigation and to group membership.
[trustworthy service replication protocol, Protocols, fail-heterogeneous architectural model, fault tolerant distributed protocol, distributed processing, distributed system, Computer crime, Delay, software fault tolerance, security of data, Fault tolerant systems, DoS attack, Resists, fail-Byzantine, HeterTrust, fail-crash, Hardware, Safety, Logic, Cryptography, Payloads]
The Paxos Register
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
We introduce the Paxos register to simplify and unify the presentation of Paxos-style consensus protocols. We use our register to show how Lamport's Classic Paxos and Castro and Liskov's Byzantine Paxos are the same consensus protocol, but for different failure models. We also use our register to compare and contrast Byzantine Paxos with Martin and Alvisi's fast Byzantine consensus. The Paxos register is a write-once register that exposes two important abstractions for reaching consensus: (i) read and write operations that capture how processes in Paxos protocols propose and decide values and (ii) tokens that capture how these protocols guarantee agreement despite partial failures. We encapsulate the differences of several Paxos-style protocols in the implementation details of these abstractions.
[write-once register, fault diagnosis, Paxos-style consensus protocol, register abstraction, Access protocols, distributed processing, classic Paxos, Computer crashes, Registers, Byzantine Paxos, Distributed computing, failure model, Asynchronous communication, Paxos register, read and write operation, Safety, protocols, Digital signatures, Testing]
Characterizing Aging Phenomena of the Java Virtual Machine
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
In this work we investigate software aging phenomena inside the Java Virtual Machine (JVM). Starting from an experimental campaign on real world testbeds, this work isolates the contribution of the JVM to the overall aging trend, and identifies, through statistical methods, which workload parameters are more relevant to aging dynamics. Experimental results show that the Sun Hotpost JVM experiences software aging phenomena. A consistent memory depletion trend (up to 50 KB/min) has been observed during periods of low garbage collector activity; the Just-In-Time compiler is also responsible for a lighter, but not negligible, memory depletion trend; finally, a consistent throughput loss (up to 24 KB/min) has been observed.
[Java, memory depletion trend, just-in-time compiler, Throughput, software aging phenomena, Virtual machining, Application software, software maintenance, Degradation, Operating systems, Computer bugs, Aging, consistent memory depletion, Safety, Java virtual machine, aging dynamics, Software measurement]
Model Checking of Consensus Algorit
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
We show for the first time that standard model checking allows one to completely verify asynchronous algorithms for solving consensus, a fundamental problem in fault-tolerant distributed computing. Model checking is a powerful verification methodology based on state exploration. However it has rarely been applied to consensus algorithms, because these algorithms induce huge, often infinite state spaces. Here we focus on consensus algorithms based on the Heard-Of model, a new computation model for distributed computing. By making use of the high abstraction level provided by this computation model and by devising a finite representation of unbounded timestamps, we develop a methodology for verifying consensus algorithms in every possible state by model checking.
[Algorithm design and analysis, heard-of model, Computational modeling, State-space methods, asynchronous algorithm verification, Distributed computing, Power system modeling, Fault tolerance, formal verification, Fault tolerant systems, distributed algorithms, consensus algorithm model checking, finite unbounded timestamp representation, Mathematical model, fault-tolerant distributed computing, Distributed algorithms, Formal verification]
A Language-Based Approach for Improving the Robustness of Network Application Protocol Implementations
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
The secure and robust functioning of a network relies on the defect-free implementation of network applications. As network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. In this paper, we present a domain-specific language, Zebu, for generating robust and efficient message processing layers. A Zebu specification, based on the notation used in RFCs, describes protocol message formats and related processing constraints. Zebu-based applications are efficient, since message fragments can be specified to be processed on demand. Zebu-based applications are also robust, as the Zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. Using a message torture suite in the context of SIP and RTSP, we show that Zebu-generated code is both complete and defect-free.
[domain-specific languages, Protocols, Scattering, Throughput, network application protocol implementations, Domain specific languages, Network servers, Computer bugs, Robustness, Standards development, IP networks, protocols, Web server, message processing]
Using Hidden Semi-Markov Models for Effective Online Failure Prediction
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
A proactive handling of faults requires that the risk of upcoming failures is continuously assessed. One of the promising approaches is online failure prediction, which means that the current state of the system is evaluated in order to predict the occurrence of failures in the near future. More specifically, we focus on methods that use event-driven sources such as errors. We use hidden semi-Markov models (HSMMs)for this purpose and demonstrate effectiveness based on field data of a commercial telecommunication system. For comparative analysis we selected three well-known failure prediction techniques: a straightforward method that is based on a reliability model, dispersion frame technique by Lin and Siewiorek and the eventset-based method introduced by Vilalta et al. We assess and compare the methods in terms of precision, recall, F-measure, false-positive rate, and computing time. The experiments suggest that our HSMM approach is very effective with respect to online failure prediction.
[fault diagnosis, faults proactive handling, Predictive models, Scheduling, Pattern recognition, commercial telecommunication system, failure analysis, event set-based method, Preventive maintenance, online failure prediction, Condition monitoring, hidden Markov models, event-driven sources, Runtime, Fault detection, Fault tolerant systems, Failure analysis, failure prediction techniques, hidden semiMarkov models]
Quantifying Temporal and Spatial Correlation of Failure Events for Proactive Management
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Networked computing systems continue to grow in scale and in the complexity of their components and interactions. Component failures become norms instead of exceptions in these environments. Moreover, failure events exhibit strong correlations in time and space domain. In this paper, we develop a spherical covariance model with an adjustable timescale parameter to quantify the temporal correlation and a stochastic model to characterize spatial correlation. The models are further extended to take into account the information of application allocation to discover more correlations among failure instances. We cluster failure events based on their correlations and predict their future occurrences. Experimental results on a production coalition system, the Wayne State Grid, show the offline and online predictions by our predicting system can forecast 72.7% to 85.3% of the failure occurrences and capture failure correlations in cluster coalition environment.
[Production systems, application allocation, Stochastic processes, grid computing, predicting system, component complexity, Distributed computing, failure instances, Computer network reliability, Engineering management, USA Councils, Failure analysis, timescale parameter, Grid computing, cluster failure events, Computer networks, stochastic processes, cluster coalition environment, failure correlations, object-oriented programming, production coalition system, temporal correlation, spatial correlation, proactive management, interaction complexity, component failures, networked computing systems, Wayne State Grid, Computer network management, stochastic model, computational complexity]
Distributed Diagnosis of Failures in a Three Tier E-Commerce System
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
For dependability outages in distributed Internet infrastructures, it is often not enough to detect a failure, but it is also required to diagnose it, i.e., to identify its source. Complex applications deployed in multi-tier environments make diagnosis challenging because of fast error propagation, black-box applications, high diagnosis delay, the amount of states that can be maintained, and imperfect diagnostic tests. Here, we propose a probabilistic diagnosis model for arbitrary failures in components of a distributed application. The monitoring system (the Monitor) passively observes the message exchanges between the components and, at runtime, performs a probabilistic diagnosis of the component that was the root cause of a failure. We demonstrate the approach by applying it to the Pet Store J2EE application, and we compare it with Pinpoint by quantifying latency and accuracy in both systems. The Monitor outperforms Pinpoint by achieving comparably accurate diagnosis with higher precision in shorter time.
[Protocols, Reliability engineering, distributed application, Distributed computing, Condition monitoring, Fault diagnosis, Runtime, distributed Internet infrastructures, Monitor, black-box applications, monitoring system, multitier environments, e-commerce system, arbitrary failures, high diagnosis delay, Testing, electronic commerce, complex applications, object-oriented programming, program diagnostics, Pinpoint, message exchanges, probabilistic diagnosis model, Application software, imperfect diagnostic tests, Pet Store J2EE application, failure distributed diagnosis, Fault detection, fast error propagation, Internet, Positron emission tomography, failure detection]
PEACE-VO: A Secure Policy-Enabled Collaboration Framework for Virtual Organizations
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
The increasing complexity and dynamics of grid environments have posed great challenges for secure and privacy-preserving collaboration in a virtual organization. In this paper, we propose PEACE-VO, a secure policy-enabled collaboration framework for virtual organizations. PEACE-VO employs role mapping to define trust relationships across autonomous domains. Nevertheless, a critical issue emerges when the system applies role mapping, which is potential policy conflict in a local domain. We first develop two concepts to depict such possible conflicts within the collaboration policy. Next, we propose a fully distributed evaluation algorithm to detect potential policy conflicts, which does not require domains to disclose their full local security policies and therefore preserves critical domain privacy. Finally, we design two dedicated protocols for virtual organization management and authorization services, respectively. We have successfully implemented the PEACE-VO framework with two fundamental protocols, i.e., VO management protocol and service authorization protocol, in the CROWN grid. Comprehensive experimental study shows our approach is scalable and efficient.
[Access control, Algorithm design and analysis, service authorization protocol, Protocols, CROWN grid, dedicated protocols, grid computing, Security, grid environments, Authorization, Privacy, authorisation, groupware, Permission, Protection, virtual organization management, privacy-preserving collaboration, policy-enabled collaboration framework security, International collaboration, Computer science, distributed evaluation algorithm, role mapping, data privacy, virtual enterprises, PEACE-VO]
RandSys: Thwarting Code Injection Attacks with System Service Interface Randomization
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Code injection attacks are a top threat to today's Internet. With zero-day attacks on the rise, randomization techniques have been introduced to diversify software and operation systems of networked hosts so that attacks that succeed on one host cannot succeed on others. Two most notable system-wide randomization techniques are instruction set randomization (ISR) and address space layout randomization (ASLR). The former randomizes instruction set for each process, while the latter randomizes the memory address space layout. Both suffer from a number of attacks. In this paper, we advocate and demonstrate that by combining ISR and ASLR effectively, we can offer much more robust protection than each of them individually. However, trivial combination of both schemes is not sufficient. To this end, we make the key observation that system call instructions matter the most to attackers for code injection. Our system, RandSys, uses system call instruction randomization and the general technique of ASLR along with a number of new enhancements to thwart code injection attacks. We have built a prototype for both Linux and Windows platforms. Our experiments show that RandSys can effectively thwart a wide variety of code injection attacks with a small overhead.
[instruction sets, Software prototyping, system service interface randomization, Spraying, RandSys, code injection attacks, call instruction randomization, address space layout randomization, system-wide randomization technique, security of data, Linux, Operating systems, Web and internet services, Prototypes, instruction set randomization, Software systems, Robustness, Libraries, Internet, Protection]
Distributed Software-based Attestation for Node Compromise Detection in Sensor Networks
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Sensors that operate in an unattended, harsh or hostile environment are vulnerable to compromises because their low costs preclude the use of expensive tamper-resistant hardware. Thus, an adversary may reprogram them with malicious code to launch various insider attacks. Based on verifying the genuineness of the running program, we propose two distributed software-based attestation schemes that are well tailored for sensor networks. These schemes are based on a pseudorandom noise generation mechanism and a lightweight block-based pseudorandom memory traversal algorithm. Each node is loaded with pseudorandom noise in its empty program memory before deployment, and later on multiple neighbors of a suspicious node collaborate to verify the integrity of the code running on this node in a distributed manner. Our analysis and simulation show that these schemes achieve high detection rate even when multiple compromised neighbors collude in an attestation process.
[Costs, distributed processing, Reliability engineering, Sensor systems, sensor networks, Noise generators, distributed sensors, node compromise detection, Delay, harsh environment, block-based pseudorandom memory traversal, Computer science, Computer network reliability, pseudorandom noise generation, Collaboration, distributed software-based attestation, Hardware, hostile environment, Cryptography]
Framework for Intrusion Tolerant Certification Authority System Evaluation
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Various intrusion tolerant certification authority (CA) systems have been recently proposed to provide attack resistant certificate update/query services. However, it is difficult to compare them against each other directly due to diversity in system organizations, threshold cryptography schemes, protocols and usage scenarios. We present a framework for intrusion tolerant CA system evaluation, which consists of three components, namely, an intrusion tolerant CA model, a threat model and a metric for comparative evaluation. The framework covers system organizations, protocols, usage scenarios, period of certificate validity, revocation rate and mean time to recovery (MTTR). Based on the framework, four representative CA systems are evaluated and compared in three typical usage scenarios, producing reasonable and insightful results. The inter-dependency between usage scenarios and system characteristics is investigated, providing a guideline to design better systems for different usage scenarios. The proposed framework provides an effective method to evaluate intrusion tolerant CA systems quantitatively. Moreover, the comparison results offer valuable insights to further improve the attack resilience of intrusion tolerant CA systems.
[Data security, revocation rate, intrusion tolerant certification authority system evaluation, mean time, Certification, system organization, Cryptographic protocols, Guidelines, Resilience, certification, Content addressable storage, security of data, Information security, Public key, usage scenario, Public key cryptography, Hardware, protocols]
An Analytical Framework and Its Applications for Studying Brick Storage Reliability
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
The reliability of a large-scale storage system is influenced by a complex set of inter-dependent factors. This paper presents a comprehensive and extensible analytical framework that offers quantitative answers to many design tradeoffs. We apply the framework to a number of important design strategies that a designer and/or administrator must face in reality, including topology-aware replica placement, proactive replication that uses small background network bandwidth and unused disk space to create additional copies. We also quantify the impact of slow (but potentially more accurate) failure detection and lazy replacement of failed disks. We use detailed simulation to verify and refine our analytical model. These results demonstrate the versatility of the framework and serve as a solid step towards more quantitative studies of fundamental system tradeoffs between reliability, performance, and cost in large-scale distributed storage systems.
[Costs, software reliability, proactive replication, Switches, Maintenance, interdependent factors, system recovery, topology-aware replica placement, Delay, Analytical models, storage area networks, Network topology, Bandwidth, background network bandwidth, large-scale distributed storage system reliability, Large-scale systems, Image storage, Reliability, brick storage reliability, failure detection]
Evaluating Byzantine Quorum Systems
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
Replication is a mechanism extensively used to guarantee the availability and good performance of data storage services. Byzantine Quorum Systems (BQS) have been proposed as a solution to guarantee the consistency of that kind of services, even if some of the replicas fail arbitrarily. Many BQS have been proposed recently, but comparing their performance is not simple. In fact, it has been shown that theoretical metrics like the number of steps or communication rounds say as much about the practical performance of distributed algorithms as they hide. This paper presents a comparative evaluation of several BQS algorithms in the literature. The evaluation is based both on experiments and simulations. For that purpose, a framework for evaluating BQS called BQSNeko was developed. The results of the evaluation allow a better understanding of the algorithms and the tradeoffs involved.
[Availability, Algorithm design and analysis, replication, replicated databases, Data security, Memory, data storage service, Delay, Cryptographic protocols, Byzantine quorum system, Fault tolerant systems, BQSNeko, Robustness, Cryptography, Distributed algorithms]
Hybrid Overloading and Stochastic Analysis for Redundant Real-time Multiprocessor Systems
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
In multiprocessor systems, redundant scheduling is a technique that trades processing power for increased reliability. One approach, called primary-backup task scheduling, is often used in real-time multiprocessor systems to ensure that deadlines are met in spite of faults. Briefly, it consists in scheduling a secondary task conditionally, in such a way that the secondary task actually gets executed only if the primary task (or the processor executing it) fails to terminate properly. Doing so avoids wasting CPU resources in the failure-free case, but primary and secondary tasks must then compete for resources in case of failure. To overcome this, overloading strategies, such as primary and backup overloading (PB) and backup-backup overloading (BB), aim at improving schedulability while retaining a certain level of reliability. In this paper, we propose a hybrid overloading technique based on extended PB overloading, which combines advantages of both PB and BB overloading. The three overloading strategies are then compared through a stochastic analysis, and by simulating them under diverse system conditions. The analysis shows that hybrid overloading provides an excellent tradeoff between schedulability and reliability.
[Real time systems, software reliability, system reliability, hybrid overloading, processor scheduling, Multiprocessing systems, Analytical models, Information science, Fault tolerance, primary-backup task scheduling, Fault tolerant systems, backup-backup overloading, stochastic processes, stochastic analysis, multiprocessing systems, redundant scheduling, primary overloading, Dynamic scheduling, system schedulability, real-time multiprocessor systems, failure-free case, Processor scheduling, Stochastic systems, CPU resources, Power system reliability]
Stateful Detection in High Throughput Distributed Systems
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
With the increasing speed of computers and the complexity of applications, many of today's distributed systems exchange data at a high rate. Significant work has been done in error detection achieved through external fault tolerance systems. However, the high data rate coupled with complex detection can cause the capacity of the fault tolerance system to be exhausted resulting in low detection accuracy. We present a new stateful detection mechanism which observes the exchanged application messages, deduces the application state, and matches against anomaly-based rules. We extend our previous framework (the monitor) to incorporate a sampling approach which adjusts the rate of verified messages. The sampling approach avoids the previously reported breakdown in the monitor capacity at high application message rates, reduces the overall detection cost and allows the monitor to provide accurate detection. We apply the approach to a reliable multicast protocol (TRAM) and demonstrate its performance by comparing it with our previous framework.
[telecommunication security, Costs, sampling methods, message exchange, Electric breakdown, electronic messaging, Throughput, Application software, error detection, Distributed computing, fault tolerance system, stateful error detection, sampling approach, Fault detection, high throughput distributed system, Fault tolerant systems, multicast protocols, Computer errors, Sampling methods, fault tolerant computing, computer network reliability, anomaly-based rule, Monitoring, reliable multicast protocol]
A Gambling Approach to Scalable Resource-Aware Streaming
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
In this paper, we propose a resource-aware solution to achieving reliable and scalable stream diffusion in a probabilistic model, i.e., where communication links and processes are subject to message losses and crashes, respectively. Our solution is resource-aware in the sense that it limits the memory consumption, by strictly scoping the knowledge each process has about the system, and the bandwidth available to each process, by assigning a fixed quota of messages to each process. We describe our approach as gambling in the sense that it consists in accepting to give up on a few processes sometimes, in the hope to better serve all processes most of the time. That is, our solution deliberately takes the risk not to reach some processes in some executions, in order to reach every process in most executions. The underlying stream diffusion algorithm is based on a tree-construction technique that dynamically distributes the load of forwarding stream packets among processes, based on their respective available bandwidths. Simulations show that this approach pays off when compared to traditional gossiping, when the latter faces identical bandwidth constraints.
[available bandwidths, memory consumption, scalable resource-aware streaming, forwarding stream packets, distributed processing, message losses, Electronic mail, statistical distributions, probabilistic model, Tree graphs, Bandwidth, Large-scale systems, gambling approach, Peer to peer computing, Computational modeling, Routing, Computer crashes, tree-construction, scalable stream diffusion, resource-aware solution, Inference algorithms, Internet, stream diffusion algorithm, bandwidth constraints, reliable stream diffusion]
Epidemic Broadcast Trees
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
There is an inherent trade-off between epidemic and deterministic tree-based broadcast primitives. Tree-based approaches have a small message complexity in steady-state but are very fragile in the presence of faults. Gossip, or epidemic, protocols have a higher message complexity but also offer much higher resilience. This paper proposes an integrated broadcast scheme that combines both approaches. We use a low cost scheme to build and maintain broadcast trees embedded on a gossip-based overlay. The protocol sends the message payload preferably via tree branches but uses the remaining links of the gossip overlay for fast recovery and expedite tree healing. Experimental evaluation presented in the paper shows that our new strategy has a low overhead and that is able to support large number of faults while maintaining a high reliability.
[Availability, Costs, gossip-based overlay, Peer to peer computing, trees (mathematics), Multicast protocols, tree branches, Steady-state, Maintenance, gossip protocol, communication complexity, Resilience, broadcasting, deterministic tree-based broadcast primitives, message complexity, epidemic broadcast trees, Broadcasting, tree healing, Large-scale systems, protocols, Payloads]
On the Latency Efficiency of Message-Parsimonious Asynchronous Atomic Broadcast
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
We address the problem of message-parsimonious asynchronous atomic broadcast when a subset t out of n parties may exhibit byzantine behavior. Message parsimony involves using only the optimal O(n) message exchanges per atomically delivered payload in the normal case. Message parsimony is desirable for Internet-like deployment environments in which message loss rates are non-negligible. Protocol PABC, the only previously-known message-parsimonious solution, suffered from two limitations vis-a-vis the solutions with O(n2) message complexity: more communication steps and the use of digital signatures. We present a protocol termed AMP that for the first time provides signature-free message parsimony while at the same time reducing the number of communication steps to the minimum necessary. In contrast to many previous atomic broadcast solutions, our protocol satisfies both safety and liveness in the asynchronous model.
[message passing, authenticator-based message-parsimonious protocol, latency efficiency, communication complexity, Computer crime, Delay, Cryptographic protocols, message complexity, signature-free message parsimony, Fault tolerant systems, message authentication, Broadcasting, byzantine behavior, Safety, Internet, protocols, Protection, message-parsimonious asynchronous atomic broadcast, Digital signatures, Payloads]
Activity Monitoring to Guarantee File Availability in Structured P2P File-sharing Systems
2007 26th IEEE International Symposium on Reliable Distributed Systems
None
2007
A cooperative structured peer-to-peer file-sharing system requires that the nodes participating in the system need to maintain the location mappings of other nodes. However, the selfish nodes have no incentive to donate their own resources to other nodes and they will refuse to take the responsibility for the maintenance of location mappings, if they can get the system's resources for free. Thus, this free-riding behavior of selfish nodes will make many files unavailable to the whole system. To address this problem, we propose a robust distributed system to monitor the activities of each node and evaluate the node's reputation, which influences the node's access to the system resources. We analyze the system and demonstrate that it is scalable and is secure under a strong attack model. Furthermore, simulation results show that the system can detect and prevent the free- riding behavior effectively and efficiently.
[Availability, peer-to-peer computing, Peer to peer computing, Computerized monitoring, Computational modeling, Maintenance, node reputation evaluation, Application software, monitoring, Computer science, Waste materials, distributed activity monitoring system, Bandwidth, Robustness, cooperative structured P2P file-sharing systems]
Message from the General Chair
2008 Symposium on Reliable Distributed Systems
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
Welcome from the Technical Program Co-Chairs
2008 Symposium on Reliable Distributed Systems
None
2008
Presents the introductory welcome message from the conference proceedings.
[]
Conference Committees
2008 Symposium on Reliable Distributed Systems
None
2008
Provides a listing of current committee members.
[]
Assuring Resilient Time Synchronization
2008 Symposium on Reliable Distributed Systems
None
2008
In many distributed and pervasive systems the clocks of nodes are required to be synchronized to a unique global time. Due to unpredictable system and environment characteristics, the distance of a local clock from global time is a variable factor very hard to predict. Systems usually adopt measures to guarantee an upper bound on such distance from global time that are very often quite far from typical execution scenarios and thus are of practical little use. As a consequence, while in many circumstances reliable information on the actual distance from global time would improve system behaviour, unfortunately such information is usually not available. In this paper we propose the Reliable and Self-Aware Clock (R&amp;SAClock), a low-intrusive software service that is able to compute a conservative estimation of distance from an external global time. R&amp;SAClock acts as a new clock that couples information gained from synchronization mechanisms with information collected from the local clock to provide both current time and a self-adaptive reliable estimation of distance from global time. This paper describes the R&amp;SAClock as a system component: we define its main functions, services and time-related mechanisms. Finally details of an implementation of the R&amp;SAClock for the NTP synchronization mechanism and Linux OS are shown.
[low-intrusive software service, Uncertainty, Protocols, clock synchronization, distributed processing, distributed system, resilient time synchronization, ubiquitous computing, synchronization uncertainty, pervasive system, Bonding, Monitoring, system behaviour, reliable-self-aware clock, conservative distance estimation, Time measurement, synchronisation, Wireless sensor networks, Pervasive systems, Linux, R&#x00026;SAClock, Linux operating system, system monitoring, Frequency synchronization, Clocks, Propagation delay]
Application-Level Recovery Mechanisms for Context-Aware Pervasive Computing
2008 Symposium on Reliable Distributed Systems
None
2008
We identify here various kinds of failure conditions and robustness issues that arise in context-aware pervasive computing applications. Such conditions are related to failures in an application's interactions with ambient services, failures in resource discovery and binding, and invalidation of context conditions during the execution of an application task. In this paper we present an exception handling model for integrating forward error recovery mechanisms in the designs of such applications. This model is integrated in a role-based framework and supported by a programming environment for construction of such applications.
[Pervasive computing, Context-aware services, application-level recovery mechanisms, resource discovery, forward error recovery mechanisms, Buildings, failure conditions, exception handling, Information management, Application software, ubiquitous computing, context-aware pervasive computing, Distributed Systems, system recovery, Information systems, Programming environments, exception handling model, Computer science, resource allocation, Application-Level Recovery, Exception Handling, Context-Aware Computing, Robustness, Context modeling]
SeNDORComm: An Energy-Efficient Priority-Driven Communication Layer for Reliable Wireless Sensor Networks
2008 Symposium on Reliable Distributed Systems
None
2008
In many reliable Wireless Sensor Network (WSN) applications, messages have different priorities depending on urgency or importance. For example, a message reporting the failure of all nodes in a region is more important than that for a single node. Moreover, traffic can be bursty in nature, such as when a correlated error is reported by multiple nodes running identical code. Current communication layers in WSNs lack efficient support for these two requirements. We present a priority-driven communication layer, called SeNDORComm, which schedules transmission of packets driven by application-specified priority, buffers and packs multiple messages in a packet, and honors the latency guarantee for a message. We show that SeNDORComm improves energy efficiency, message reliability, and network utilization and delays congestion in a network. We extensively evaluate SeNDORComm using analysis, simulation, and testbed experiments. We demonstrate the improvement in goodput of SeNDORComm over the default communication layer, GenericComm in TinyOS (134.78% for a network of 20 nodes).
[network utilization, wireless sensor networks, energy-efficient priority-driven communication layer, Telecommunication traffic, reliability, Throughput, priority-driven communication layer, Delay, message reliability, Computer network reliability, message aggregation, reliable message delivery, Traffic control, Computer networks, delays congestion, priority driven communication, wireless sensor networks reliability, Routing, application-specified priority, Wireless sensor network, Wireless sensor networks, SeNDORComm, congestion avoidance, Energy efficiency, Telecommunication network reliability, telecommunication traffic]
Analytical Assessment of the Precision Degradation Caused by Faults in a Fault-Tolerant Master/Slave Clock Synchronization Service for CAN
2008 Symposium on Reliable Distributed Systems
None
2008
The main goal of a clock synchronization service is to keep a consistent perception of time among the nodes of the system. In this context, consistency means that at any instant, the values of all the clocks in the system do not differ more than a given amount, which is called the precision. Moreover, clock synchronization is said to be fault tolerant if the intended precision is guaranteed despite the occurrence of the faults included in the fault model. In this paper, we consider a specific fault-tolerant master/slave clock synchronization service for the Controller Area Network (CAN) field bus, and analyze its precision under different fault assumptions. The equations obtained in our analysis show that, when using master redundancy, inconsistent channel faults may have a negative impact on the guaranteed precision.
[Clock synchronization, Master-slave, master redundancy, fault-tolerant master/slave clock synchronization service, Degradation, Fault tolerance, Fault tolerant systems, Controller Area Network, Hardware, redundancy, controller area networks, fault tolerance, Redundancy, fieldbus, analytical assessment, field buses, Synchronization, precision degradation, Equations, synchronisation, clocks, CAN, controller area network field bus, Clocks, Formal verification]
ALPS: Authenticating Live Peer-to-Peer Live Streams
2008 Symposium on Reliable Distributed Systems
None
2008
Live streaming is one of many applications where data is continuously created, and has to be quickly distributed among a large number of users. The peer-to-peer paradigm is thereby attracting interest with the prospect of overcoming scalability issues of more centralized approaches. Since data blocks travel along multiple (possibly malicious) peers, authenticating the origin of blocks becomes of prime importance to guarantee safety and reliability. The asymmetry of a single source and an arbitrary number of untrusted receivers requires the use of digital signatures and public key cryptography in general. This paper proposes a new signature scheme for broadcast authentication tailored towards peer-to-peer systems to overcome limitations of traditional approaches based on signature schemes like RSA and DSA, most notably in terms of delays, signature size, and computational complexity. It may further be of practical interest for other real-time applications such as massive multiplayer peer-to-peer gaming.
[peer-to-peer computing, Peer to peer computing, Scalability, reliability, Computational complexity, Distributed computing, Delay, peer-to-peer live streaming, security, public key cryptography, live streaming authentication, Authentication, Public key, peer-to-peer live streams, media streaming, Public key cryptography, digital signatures, Safety, multiplayer peer-to-peer gaming, Digital signatures, computational complexity, authentication]
The Search for Efficiency in Automated Intrusion Response for Distributed Applications
2008 Symposium on Reliable Distributed Systems
None
2008
Providing automated responses to security incidents in a distributed computing environment has been an important area of research. This is due to the inherent complexity of such systems that makes it difficult to eliminate all vulnerabilities before deployment and costly to rely on humans for responding to incidents in real time. Earlier work has investigated automated responses but failed to argue about the optimality of the response choices. Here we propose a new approach where the optimality of responses is considered from a global point of view, i.e., "What&#x02019;s the eventual outcome on the entire system due to a response?" We formalize the process of providing automated responses and the criterion for asserting global optimality of the set of deployed responses. We show that reaching the globally optimal solution is an NP-hard problem. Therefore we design a genetic algorithm framework for searching for good solutions. Our framework adapts itself to the changing environment based on history of attacks seen so far and effectiveness of responses. We demonstrate the solution on a distributed e-commerce application called PetStore with injection of real attacks and show that it improves the survivability of the system over the prior ADEPTS system.
[Real time systems, automated intrusion response, Humans, Reliability engineering, distributed e-commerce system, Application software, Distributed computing, Computer science, intrusion containment, NP-hard problem, Intrusion detection, Detectors, optimal response, survivability, Computer security]
Protecting BitTorrent: Design and Evaluation of Effective Countermeasures against DoS Attacks
2008 Symposium on Reliable Distributed Systems
None
2008
BitTorrent is a P2P file-sharing protocol that can be used to efficiently distribute files such as software updates and digital content to very large numbers of users. In a previous paper, we have shown that vulnerabilities can be exploited to launch Denial-of-Service attacks against BitTorrent swarms, which can substantially increase download times and network traffic. In this paper, we review the three most damaging attacks, and propose two algorithms as countermeasures to effectively tackle them. We implemented the attacks and countermeasures in a packet-level BitTorrent simulator. The results indicate that our proposed approach is effective when there is an ongoing attack while at the same time efficient when the countermeasure is active but there is no attack. To the best of our knowledge, this is the first proposal in the literature to make BitTorrent more robust against Denial-of-Service (DoS) attacks.
[Algorithm design and analysis, Internet-based systems and applications, Protocols, peer-to-peer computing, Peer to peer computing, Telecommunication traffic, Proposals, Computer crime, network traffic, Event-based and Peer-to-Peer infrastructures, security of data, P2P file-sharing protocol, Traffic control, DoS attacks, BitTorrent, Robustness, Internet, denial-of-service attacks, Protection]
Dynamically Quantifying and Improving the Reliability of Distributed Storage Systems
2008 Symposium on Reliable Distributed Systems
None
2008
In this paper, we argue that the reliability of large-scale storage systems can be significantly improved by using better reliability metrics and more efficient policies for recovering from hardware failures. Specifically, we make three main contributions. First, we introduce NDS (Normalcy Deviation Score), a new metric for dynamically quantifying the reliability status of a storage system. Second, we propose MinI (Minimum Intersection), a novel recovery scheduling policy that improves reliability by efficiently reconstructing data after a hardware failure. MinI uses NDS to tradeoff reliability and performance in making its scheduling decisions. Third, we evaluate NDS and MinI for three common data-allocation schemes and a number of different parameters. Our evaluation focuses on a distributed storage system based on erasure codes. We find that MinI improves reliability significantly, as compared to conventional policies.
[storage allocation, Costs, large-scale storage system, Computational modeling, Redundancy, data reconstruction, distributed processing, data-allocation scheme, system recovery, minimum intersection, distributed storage system reliability, Current measurement, normalcy deviation score, Bandwidth, National electric code, decision making, scheduling, Page description languages, Hardware, Large-scale systems, hardware failure, Reliability, recovery scheduling policy]
An Absolute-Relative Risk Assessment Methodology Approach to Current Safety Critical Systems and its Application to the ADS-B based Air Traffic Control System
2008 Symposium on Reliable Distributed Systems
None
2008
This work presents a risk assessment methodology, preliminary proposed in [1], which is the fusion of the "absolute" and the "relative" risk assessment methods adopted by the International Civil Aviation Organization. The proposed methodology uses the Fluid Stochastic Petri Net (FSPN) as modeling formalism, and compares the safety metrics estimated from the simulation of both the proposed and the legacy system models. It was applied to assess the safety properties of a new air traffic surveillance concept, named "automatic dependent surveillance - broadcasting" (ADS-B). As conclusions, the proposed methodology assured to assess the safety properties of systems based on the current safety critical system paradigm - especially concerning the air transportation system. Besides, the FSPN formalism provided important modeling capabilities and discrete event simulation allowing estimating the desired safety metrics. Finally, the ADS-B (proposed system) has significantly reduced the risks of separation losses between aircrafts if compared to the usual surveillance radar systems (legacy system) in air traffic control (ATC) environment.
[Air safety, surveillance radar systems, Petri nets, safety-critical software, legacy system models, Discrete event simulation, International Civil Aviation Organization, absolute-relative risk assessment methodology, air traffic control, automatic dependent surveillance - broadcasting, Fluid Stochastic Petri Nets, Traffic control, Broadcasting, discrete event simulation, risk management, Risk Assessment, air traffic surveillance concept, Air traffic control, software maintenance, current safety critical systems, CNS/ATM, ADS-B based air traffic control system, ADS-B, safety metrics, Surveillance, Stochastic systems, Air transportation, SPNP, fluid stochastic Petri Net, Risk management, Aircraft, software metrics]
Systematic Structural Testing of Firewall Policies
2008 Symposium on Reliable Distributed Systems
None
2008
Firewalls are the mainstay of enterprise security and the most widely adopted technology for protecting private networks. As the quality of protection provided by a firewall directly depends on the quality of its policy (i.e., configuration), ensuring the correctness of security policies is important and yet difficult.To help ensure the correctness of a firewall policy, we propose a systematic structural testing approach for firewall policies. We define structural coverage (based on coverage criteria of rules, predicates, and clauses) on the policy under test. Considering achieving higher structural coverage effectively, we develop three automated packet generation techniques: the random packet generation, the one based on local constraint solving (considering individual rules locally in a policy), and the most sophisticated one based on global constraint solving (considering multiple rules globally in a policy).We have conducted an experiment on a set of real policies and a set of faulty policies to detect faults with generated packet sets. Generally, our experimental results show that a packet set with higher structural coverage has higher fault detection capability (i.e., detecting more injected faults). Our experimental results show that a reduced packet set (maintaining the same level of structural coverage with the corresponding original packet set) maintains similar fault detection capability with the original set.
[Software testing, Firwall Policy, System testing, Home automation, program testing, Telecommunication traffic, Reliability engineering, enterprise security, Computer science, Business communication, Fault detection, Mutation Testing, the random packet generation, authorisation, systematic structural testing, Structural Coverage, Protection, Computer security, firewall policies, Test Generation]
An Empirical Study of Denial of Service Mitigation Techniques
2008 Symposium on Reliable Distributed Systems
None
2008
We present an empirical study of the resistance of several protocols to denial of service (DoS) attacks on client-server communication. We show that protocols that use authentication alone, e.g., IPSec, provide protection to some extent, but are still susceptible to DoS attacks, even when the network is not congested. In contrast, a protocol that uses a changing filtering identifier (FI) is usually immune to DoS attacks, as long as the network itself is not congested. This approach is called FI hopping. We build and experiment with two prototype implementations of FI hopping. One implementation is a modification of IPSec in a Linux kernel, and a second implementation comes as an NDIS hook driver on a Windows machine. We present results of experiments in which client-server communication is subject to a DoS-attack. Our measurements illustrate that FI hopping withstands severe DoS attacks without hampering the client-server communication. Moreover, our implementations show that FI hopping is simple, practical, and easy to deploy.
[telecommunication security, client-server systems, denial of service mitigation, Protocols, Filtering, Linux kernel, DoS, Computer crime, client-server communication, FI hopping, Network servers, Linux, Authentication, Prototypes, Windows machine, protocols, Web server, Protection, Kernel, denial of service mitigation techniques, filtering identifier]
An Autonomic Approach for Replication of Internet-based Services
2008 Symposium on Reliable Distributed Systems
None
2008
As more and more applications are deployed as Internet-based services, they have to be available anytime anywhere in a seamless manner. This requires the underlying infrastructure to provide scalability, fault tolerance and fast response times. While replicating the services and the data they access across sites that are located in different geographic regions is a promising means to achieve these requirements, data consistency is challenging if data continuously changes and queries are dynamic by nature, as is typical for e-commerce applications.Thus, current WAN replication solutions either trade performance for data consistency or are notable to scale in wide-area settings. In this paper, we present a novel approach to provide performance and consistency for Internet services. One of the main contributions is an autonomic replica placement module that places data copies only on servers close to clients that actually need them. The goal is to find the right trade-off between fast local access and the overhead of keeping data copies consistent. As data access patterns might change over time, reconfiguration is done periodically and online, i.e., allowing sites to receive new data copies or drop data copies while at the same time transaction processing continues in the system.
[Measurement, Scalability, Delay, scalability, Autonomic computing, Fault tolerance, fast response times, Web and internet services, geographic regions, replicated databases and transaction processing, data access patterns, Books, Web server, Local area networks, electronic commerce, middleware, Wide area networks, Availability, Internet-based systems and applications, fault tolerance, autonomic replica placement module, software fault tolerance, electronic commerce and enabling technologies, Internet-based service replication, Internet]
Gumshoe: Diagnosing Performance Problems in Replicated File-Systems
2008 Symposium on Reliable Distributed Systems
None
2008
Replicated file-systems can experience degraded performance that might not be adequately handled by the underlying fault-tolerant protocols. We describe the design and implementation of Gumshoe, a system that aims to diagnose performance problems in replicated file-systems. Gumshoe periodically gathers OS and protocol metrics and then analyzes these metrics to automatically localize the performance problem to the culprit node(s). We describe our results and experiences with problem diagnosis in two replicated file-systems (replicated-CoreFS and BFS) using two file-system benchmarks (Postmark and IOzone).
[replicated file-systems, Protocols, Reliability engineering, Postmark, replicated-CoreFS, Distributed computing, Degradation, Fault diagnosis, Fault tolerance, Fault tolerant systems, replicated databases, Peer to peer computing, program diagnostics, problem diagnosis, fault-tolerant protocols, protocol metrics, file-system benchmarks, performance problems, diagnosis, Computer crashes, software fault tolerance, Fault detection, IOzone, operating systems (computers), Gumshoe, software metrics]
Probabilistic Failure Detection for Efficient Distributed Storage Maintenance
2008 Symposium on Reliable Distributed Systems
None
2008
Distributed storage systems often use data replication to mask failures and guarantee high data availability. Node failures can be transient or permanent. While the system must generate new replicas to replace replica lost to permanent failures, it can save significant replication costs by not replicating following transient faults. Given the unpredictability of network dynamics, however, distinguishing permanent and transient failures is extremely difficult. Traditional timeout approaches are difficult to tune and can introduce unnecessary replication. In this paper, we propose Protector, an algorithm that addresses this problem using network-wide statistical prediction. Our algorithm drastically improves prediction accuracy by making predictions across aggregate replica groups instead of single nodes. These estimates of the number of "live replicas" can guide efficient data replication policies. We prove that given data on node down times and the probability of permanent failures, the estimate given by our algorithm is more accurate than all alternatives. We describe two ways to obtain the failure probability function driven by models or traces. We conduct extensive simulations based both on synthetic and real traces, and show that Protector closely approximates the performance of a perfect "oracle" failure detector, while significantly outperforming timeout-based detectors using a wide range of parameters.
[Availability, Costs, data analysis, Peer to peer computing, Optimized production technology, distributed storage maintenance, distributed processing, probabilistic failure detection, Failure Detection, Maintenance, failure analysis, storage management, Data Recovery, Aggregates, Computer network reliability, data replication, Detectors, Bandwidth, data availability, Distributed Storage, Protection, Protector]
An Incremental File System Consistency Checker for Block-Level CDP Systems
2008 Symposium on Reliable Distributed Systems
None
2008
A block-level continuous data protection (CDP) system logs every disk block update from an application server (e.g., a file or DBMS server) to a storage system so that any disk updates within a time window are undoable, and thus is able to provide a more flexible and efficient data protection service than conventional periodic data backup systems. Unfortunately, no existing block-level CDP systems can support arbitrary point-in-time snapshots that are guaranteed to be consistent with respect to the metadata of the application server. This deficiency seriously limits the flexibility in recovery point objective (RTO) of block-level CDP systems from the standpoint of the application servers whose data they protect. This paper describes an incremental file system check mechanism (iFSCK) that is designed to address this deficiency for file servers, and exploits file system-specific knowledge to quickly fix an arbitrary point-in-time block-level snapshot so that it is consistent with respect to file system metadata. Performance measurements taken from a fully operational iFSCK prototype show that iFSCK can turn a 10 GB point-in-time block-level snapshot to be file-system consistent in less than 1 second, and takes less than 25% of the time required by the Fsck utility for vanilla ext3 under relaxed metadata consistency requirements.
[Measurement, storage system, metadata consistency, recovery point objective, Access protocols, File servers, consistency checker, Application software, CDP, block-level continuous data protection system, Computer science, Network servers, File systems, security of data, Computer network reliability, FSCK, incremental file system check mechanism, file organisation, Image storage, Protection, incremental compilers]
Scalable Topology Discovery and Link State Detection Using Routing Events
2008 Symposium on Reliable Distributed Systems
None
2008
Discovering the topology of a network and detecting link state changes (e.g.: link failures) is an essential element for various network management and monitoring tasks. In this paper, we investigate scalable mechanisms to monitor the topology and link states of networks based on information available in network nodes' routing tables. We first present an algorithm that infers the network topology based on the full or partial information about network distances between nodes, based on which we obtain a scalable network topology discovery solution via a novel use of random walk in graphs. We then present scalable algorithms to detect the state changes of remote links by monitoring the routing tables of a small fraction of the routers, where the routers to be monitored are selected by a greedy approach to an NP-complete Tree Cover problem. We show the efficacy and scalability of our topology monitoring algorithms through experimental evaluation performed both on synthetic topologies and on a large topology data-set from a real enterprise network.
[Algorithm design and analysis, Event detection, Scalability, graph theory, Network Management, link state detection, Information analysis, Condition monitoring, random walk, NP-complete tree cover problem, Network topology, information available, network distances, Failure analysis, synthetic topologies, Performance analysis, topology discovery, Government, routing events, telecommunication network topology, real enterprise network, Discovery, Routing, network management, computer network management, monitoring tasks, telecommunication network routing, greedy approach, routing tables, Remote monitoring, computational complexity, Fault Tolerance]
Fault-Tolerant Coverage Planning in Wireless Networks
2008 Symposium on Reliable Distributed Systems
None
2008
Typically wireless networks coverage is planned with static redundancy to compensate temporal variations in the environment. As a result, the service still is delivered but the network coverage could have entered a critical state, meaning that further changes in the environment may lead to service failure. Service failures have to be explicitly notified by the applications. Therefore, in this paper we propose a methodology for fault-tolerant coverage planning. The idea is detecting the critical state and removing it by on-line system reconfiguration, and restoration of the original static redundancy. Even in case of a failure the system automatically generates a new configuration to restore the service, leading to shorter repair times. We describe how this approach can be applied to wireless mesh networks, often used in industrial applications like manufacturing, automation and logistics. The evaluation results show that the underlying model used for error detection and system recovery is accurate enough to correctly identify the system state.
[fault-tolerance, radio coverage, Redundancy, Spine, wireless networks, service failures, wireless mesh networks, software fault tolerance, Wireless communication, fault-tolerant coverage planning, Fault tolerance, Wireless networks, Wireless mesh networks, online system reconfiguration, System recovery, Automatic control, static redundancy, wireless networks coverage, wireless LAN, Manufacturing automation, Logistics]
POSH: Proactive co-Operative Self-Healing in Unattended Wireless Sensor Networks
2008 Symposium on Reliable Distributed Systems
None
2008
Unattended Wireless Sensor Networks (UWSNs) are composed of many small resource-constrained devices and operate autonomously, gathering data which is periodically collected by a visiting sink. Unattended mode of operation, deployment in hostile environments and value (or criticality) of collected data are some of the factors that complicate UWSN security. This paper makes two contributions. First, it explores a new threat model involving a mobile adversary who periodically compromises and releases sensors aiming to maximize its advantage and overall knowledge of collected data. Second, it constructs a self-healing protocol that allows sensors to continuously and collectively recover from compromise. The proposed protocol is both effective and efficient, as supported by analytical and simulation results.
[telecommunication security, mobile adversary, mobile radio, wireless sensor networks, Data security, self-healing protocol, resiliency, Batteries, unattended wireless sensor networks, Cryptographic protocols, Wireless sensor networks, Analytical models, Privacy, resource-constrained devices, sensor network, Authentication, proactive cooperative self-healing, Bandwidth, Open source hardware, data handling, Cryptography]
Towards Reliable Reputations for Dynamic Networked Systems
2008 Symposium on Reliable Distributed Systems
None
2008
A new generation of distributed systems and applications rely on the cooperation of diverse user populations motivated by self-interest. While they can utilize "reputation systems" to reduce selfish behaviors that disrupt or manipulate the network for personal gain, current reputations face a key challenge in large dynamic networks: vulnerability to peer collusion. In this paper, we propose to dramatically improve the accuracy of reputation systems with the use of a statistical metric that measures the "reliability" of a peer's reputation taking into account collusion-like behavior. Trace-driven simulations on P2P network traffic show that our reliability metric drastically improves system performance. We also apply our metric to 18,000 randomly selected eBay user reputation profiles, and surprisingly discover numerous users with collusion-like behaviors worthy of additional investigation.
[reputation systems, Protocols, peer collusion, peer-to-peer computing, software reliability, dynamic networked systems, Telecommunication traffic, Probability, Application software, Jamming, Computer science, P2P network traffic, security of data, eBay user reputation, Computer network reliability, Statistical distributions, Traffic control, distributed systems, reliability metric, Manipulator dynamics, software metrics]
Self-Stabilization in Tree-Structured Peer-to-Peer Service Discovery Systems
2008 Symposium on Reliable Distributed Systems
None
2008
The efficiency of service discovery is critical in the development of fully decentralized middleware intended to manage large scale computational grids. This demand influenced the design of many peer-to-peer based approaches. The ability to cope with the expressiveness of the service discovery was behind the design of a new kind of overlay structures that is based on tries, or prefix trees. Although these overlays are well designed, one of their weaknesses is the lack of any concrete fault tolerant mechanism, especially in dynamic platforms; the faults are handled by using preventive and costly mechanisms, \\eg using a high degree of replication. Moreover, those systems cannot handle any arbitrary transient failure. Self-stabilization, which is an efficient approach to designreliable solutions for dynamic systems, was recently suggested to be a good alternative to inject fault-tolerance in peer-to-peer systems. However, most of the previous research on self-stabilization in tree and/or P2P networks was designed in theoretical models, making these approaches hard to implement in practice. In this paper, we provide a self-stabilizing message passing protocol to maintain prefix trees over practical peer-to-peer networks. A complete correctness proof is provided, as well as simulation results to estimate the practical impact of our protocol.
[fault tolerant mechanism, Protocols, message passing, peer-to-peer computing, Peer to peer computing, self-stabilization, fault-tolerance, Laboratories, message passing protocol, Computer crashes, Middleware, P2P networks, service discovery, prefix trees, Fault tolerance, Web services, fully decentralized middleware, peer-to-peer systems, Fault tolerant systems, tree-structured peer-to-peer service discovery systems, Computer architecture, Grid computing, Large-scale systems, tree data structures]
Adaptive Internal Clock Synchronization
2008 Symposium on Reliable Distributed Systems
None
2008
Existing clock synchronization algorithms assume a bounded clock reading error. This, in turn, results in an inflexible design that typically requires node crashes whenever the given bound might be violated. We propose a novel, adaptive internal clock synchronization algorithm which allows to compute the deviation between the clocks during runtime. The computed deviation can be propagated to the application layer to allow it to adapt its behavior according to the current clock deviation. The contributions of this paper are: (1) a new specification of a relaxed clock synchronization problem, and (2) a new clock synchronization algorithm with a novel approach to dealing with crash failures.
[internal clock synchronization, Adaptive systems, timed asynchronous systems, Reliability engineering, Computer crashes, crash failure, formal specification, system recovery, Delay, Oscillators, synchronisation, clocks, Upper bound, Runtime, real-time, bounded clock reading error, Systems engineering and theory, distributed systems, relaxed clock synchronization problem, Frequency synchronization, Clocks, adaptive internal clock synchronization algorithm]
Extending Paxos/LastVoting with an Adequate Communication Layer for Wireless Ad Hoc Networks
2008 Symposium on Reliable Distributed Systems
None
2008
Most papers addressing consensus in wireless ad hoc networks adopt system models similar to those developed for wired networks. These models are focused towards node failures while ignoring link failures, and thus are poorly suited for wireless ad hoc networks. The recently proposed HO model does not have this drawback. The paper shows that an existing algorithm and the HO model can be used for multi-hop wireless ad hoc networks, if extended with an adequate communication layer. The description of the communication layer is augmented with simulation results that validate the feasibility of our approach and provide better understanding of the behavior of wireless environments.
[Performance evaluation, Ad hoc networks, Computer crashes, Discrete event simulation, Consensus, Mobile ad hoc networks, Computer science, Fault tolerance, Wireless networks, Computer network reliability, Distributed systems, Wireless ad hoc networks, Spread spectrum communication, Detectors, Telecommunication network reliability]
Investigating the Existence and the Regularity of Logarithmic Harary Graphs
2008 Symposium on Reliable Distributed Systems
None
2008
This paper studies the existence and the regularity of Logarithmic Harary Graphs (LHGs). This study is motivated by the fact that these graphs are employed for modeling the communication topology to support efficient flooding in presence of link and node failures when considering an initial arbitrary number of nodes n. Therefore, the capability to identify graph constraints that allow the construction of LHGs for the largest number of pairs (n, k) (where k is the desired degree of connectivity to be tolerant to failures) becomes of primary importance. The paper presents several results in that direction. We introduce a graph constraint, namely K-TREE, that allows the construction of a LHG for every pair (n, k) such that n ges 2k. Secondly we presents another graph constraint for LHG, namely KDIAMOND, which is equivalent to K-TREE in terms of capability to construct LHGs for any pair (n, k). The interest of K-DIAMOND lies in the fact that, for a given k, KDIAMOND allows to construct more regular graphs than K-TREE does. A k-regular graph shows the minimal number of links required by a k-connected graph, leading tominimal flooding cost. The paper formally shows, in particular, that there are an infinite number of pairs (n, k), such that there exists a k-regular LHG for the pair (n, k) that satisfies K-DIAMOND and does not satisfy K-TREE.
[k-regular graph, Logarithmic Harary Graphs, K-DIAMOND, flooding overlay networks., Costs, Peer to peer computing, Buildings, graph theory, graph constraints, Computer crashes, Topology, Floods, Delay, logarithmic Harary graphs, communication topology, Hypercubes, Robustness, k-connected graph, link failures, node failures, K-TREE]
A Probabilistic Analysis of Snapshot Isolation with Partial Replication
2008 Symposium on Reliable Distributed Systems
None
2008
Snapshot isolation has received a considerable amount of attention in the context of full database replication. Such popularity is mainly because read-only transactions executing under snapshot isolation are never blocked or aborted. In partial replication, where each replica holds only a part of the database, transactions may require access to remote databases. Each remote read operation of the transaction must execute in a consistent global database snapshot as the local operations; if such a snapshot is not available, the transaction must be aborted. In this paper we are interested in the effects of distributed transactions on the abort rate of partially replicated snapshot isolation systems. We present a simple probabilistic analysis of transaction abort rates for two different concurrency control mechanisms: lock- and version-based. The former models the behavior of a replication protocol providing one-copy-serializability; the latter models snapshot isolation. Our analysis reveals that in the version-based system the execution abort rate decreases exponentially as the number of data versions available increases. As a consequence, in all cases considered, two versions of each data item were sufficient to eliminate aborts due to distributed transactions.
[Context, transaction processing, replicated databases, Scalability, probability, replication protocol, Access protocols, concurrency control mechanisms, Concurrency control, probabilistic analysis, Transaction databases, database replication, Middleware, snapshot isolation, partial replication, Distributed databases, concurrency control, System recovery, snapshot isolation systems, distributed transactions, Database systems, remote databases, protocols]
Using Tractable and Realistic Churn Models to Analyze Quiescence Behavior of Distributed Protocols
2008 Symposium on Reliable Distributed Systems
None
2008
Large-scale distributed systems are subject to churn, i.e., continuous arrival, departure and failure of processes. Analysis of protocols under churn requires one to use churn models that are tractable (easy to apply), realistic (apply to deployment settings), and general (apply to many protocols and properties). In this paper, we propose two new churn models - called train and crowd - that together achieve these goals, for a broad class of stability properties called quiescent properties, and for arbitrary distributed protocols. We show (i) how analysis of protocol quiescence in the train model can be extended to the crowd model, (ii) how to apply the train and crowd model to several distributed membership protocols, (iii) how, even under real churn traces, the train and crowd models are reasonably good at predicting system-wide stability metrics for membership protocols.
[Algorithm design and analysis, stability properties, Protocols, Peer to peer computing, Predictive models, Stability analysis, Computer crashes, quiescence behavior, large-scale distributed systems, crowd model, Computer science, membership protocols, distributed algorithms, Failure analysis, distributed membership protocols, system-wide stability metrics, Large-scale systems, protocols, Pattern analysis, arbitrary distributed protocols]
Formalizing System Behavior for Evaluating a System Hang Detector
2008 Symposium on Reliable Distributed Systems
None
2008
This paper presents an approach to formally verify the detection capability of a system hang detector. To achieve this goal, an abstract formal model of a typical Linux system is created to thoroughly exercise all execution scenarios that may lead to hangs. The goal is to expose cases (i.e., hang scenarios) that escape detection. Our system model abstracts the basic hardware (e.g., timer, hardware counter) and software (e.g., processes/threads) components present in the Linux system. The model enables: (i) capturing behavior of these components so as to depict execution scenarios that lead to hangs, and (ii) evaluating hang detection coverage. Explicit-state model checking is applied to reason about system behavior and uncover hang scenarios that escape detection. The results indicate that the proposed framework allows identification of corner cases of hang scenarios that escape detection and provides valuable insight to developers for enhancing detection mechanisms.
[formal model of system, system hang, formal languages, Formal languages, hierarchical FSM, abstract formal model, Linux system, Yarn, Distributed computing, Delay, Counting circuits, system hang detector, detection capability, formal verification, model checking, Linux, Operating systems, Detectors, Abstracts, Hardware, explicit-state model checking, system behavior]
Availability of Globally Distributed Nodes: An Empirical Evaluation
2008 Symposium on Reliable Distributed Systems
None
2008
Dependability models of distributed systems are often parameterised by the failure characteristics of the nodes that form a system. For realistic results, these parameters must be estimated accurately, for example, based on evaluations of real-world systems. We empirically evaluate over 400 globally distributed nodes of the PlanetLab research cluster and estimate the popular parameters mean-time-to-failure, mean-time-to-repair, availability, and failure correlation coefficients. We fit the resulting empirical distributions by simple theoretical distributions and find that the mean-time-to-failure, the availability, and the failure correlation coefficient correlate with the geographical distance between nodes.
[Availability, failure characteristics, Parameter estimation, mean-time-to-repair, software reliability, PlanetLab research cluster, distributed processing, globally distributed nodes, availability, Empirical Evaluation of Dependable Distributed Systems, failure analysis, Condition monitoring, Computer science, empirical evaluation, Storms, mean-time-to-failure, Web pages, Internet, failure correlation coefficients, Wool, Web server, software performance evaluation, correlation methods, geographical distance]
[Roster]
2008 Symposium on Reliable Distributed Systems
None
2008
Provides a listing of current committee members and society officers.
[]
Message from General Chair
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Presents the welcome message from the conference proceedings.
[]
Message from Technical Program Co-chairs
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Presents the welcome message from the conference proceedings.
[]
Conference Committees
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Provides a listing of current committee members.
[]
Technical Program Committee
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Provides a listing of current committee members.
[]
Relaxed Atomic Broadcast: State-Machine Replication Using Bounded Memory
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Atomic broadcast is a useful abstraction for implementing fault-tolerant distributed applications such as state-machine replication. Although a number of algorithms solving atomic broadcast have been published, the problem of bounding the memory used by these algorithms has not been given the attention it deserves. It is indeed impossible to solve repeated atomic broadcast with bounded memory in a system (non-synchronous or not equipped with a perfect failure detector) in which consensus is solvable with bounded memory. The intuition behind this impossibility is the inability to safely garbage-collect unacknowledged messages, since a sender process cannot tell whether the destination process has crashed or is just slow.The usual technique to cope with this problem is to introduce a membership service, allowing the exclusion of a slow or silent process from the group and safely discarding unacknowledged messages sent to this process. In this paper,we present a novel solution that does not rely on a membership service. We relax the specification of atomic broadcast so that it can be implemented with bounded memory, while being strong enough to still be useful for applications that use atomic broadcast, e.g., state-machine replication.
[fault-tolerant distributed applications, abstraction, distributed processing, Computer crashes, finite state machines, relaxed atomic broadcast, state-machine replication, safely garbage-collect unacknowledged messages, Fault tolerance, State-Machine replication, Fault tolerant systems, Prototypes, Detectors, Broadcasting, bounded memory, fault tolerant computing]
When and How to Change Quorums on Wide Area Networks
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
In wide-area settings, unpredictable events, such as flash crowds caused by nearly instantaneous popularity of services, can cause servers that are expected to respond quickly to instead suddenly respond slowly. This presents a problem for achieving consistently good performance in quorum-based distributed systems, in which clients must choose which quorums (sets of servers) to access. Typically,clients are motivated to choose quorums containing the servers that respond fastest. Often, these may be the closest servers, but when the closest servers are particularly slow to respond, e.g., because of a changed workload, servers that are farther may actually respond faster. In this paper, we show how clients can locally change their quorum selections efficiently such that the overall system performance rapidly converges to that of the best global strategy for the current conditions. Moreover, we discuss how to benefit even when changes in quorums must be accompanied by expensive state-transfer operations.
[Wide area networks, Algorithm design and analysis, Protocols, network servers, wide area networks, load balancing, Switches, Telecommunication traffic, unpredictable events, Delay, Network servers, quorum systems, servers, USA Councils, System performance, Fault tolerant systems, quorum-based distributed systems, flash crowds, wide-area settings]
Multithreading-Enabled Active Replication for Event Stream Processing Operators
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Event stream processing (ESP) systems are very popular in monitoring applications. Algorithmic trading, network monitoring and sensor networks are good examples of applications that rely upon ESP systems. As these systems become larger and more widely deployed, they have to answer increasingly stronger requirements that are often difficult to satisfy. Fault-tolerance is a good example of such a non-trivial requirement. Making ESP operators fault-tolerant can add considerable performance overhead to the application. In this paper, we focus on active replication as an approach to provide fault-tolerance to ESP operators. More precisely, we address the performance costs of active replication for operators in distributed ESP applications.We use a speculation mechanism based on software transactional memory (STM) to achieve the following goals: (i) enable replicas to make progress using optimistic delivery; (ii) enable early forwarding of speculative computation results; (iii) enable active replication of multi-threaded operators using transactional executions. Experimental evaluation shows that, using this combination of mechanisms, one can implement highly efficient fault-tolerant ESP operators.
[transaction processing, speculation, Costs, event stream processing, multi-threading, fault-tolerance, event processing, Reliability engineering, Sensor systems and applications, active replication, Application software, Electrostatic precipitators, Delay, software fault tolerance, multithreading-enabled active replication, software transactional memory, Fault tolerance, Broadcasting, Systems engineering and theory, distributed systems, parallel computing, Monitoring]
Performance Evaluation of a Metaprotocol for Database Replication Adaptability
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Common solutions to database replication use a single replication protocol. This approach lacks flexibility for changing scenarios or when dealing with heterogeneous client application requirements. Our proposal is a metaprotocol that supports several replication protocols which may follow different replication techniques or provide different isolation levels. With our metaprotocol, replication protocols can either work concurrently with the same data or be sequenced for adapting to dynamic environments. Experimental results demonstrate its low overhead and measure the influence of protocol concurrency on system performance.
[replicated databases, single replication protocol, Scalability, replication protocols, Access protocols, database replication adaptability, database replication, adaptability, Proposals, Abortion, Delay, Concurrent computing, System performance, Distributed databases, distributed databases, Load management, metaprotocol performance evaluation, heterogeneous client application requirement, protocols, Testing]
Reliable and Highly Available Distributed Publish/Subscribe Service
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
This paper develops reliable distributed publish/subscriber algorithms with service availability in the face of concurrent crash failure of up to delta brokers. The reliability of service in our context refers to per-source in-order and exactly-once delivery of publications to matching subscribers. To handle failures, brokers maintain data structures that enable them to reconnect the topology and compute new forwarding paths on the fly. This enables fast reaction to failures and improves the system's availability. Moreover, we present a recovery procedure that recovering brokers execute in order to re-enter the system, and synchronize their routing information.
[Availability, Context-aware services, message passing, Scalability, Subscriptions, distributed publish/subscribe service, topology, data structure, service availability, Routing, Data structures, Computer crashes, Maintenance, per-source in-order, service reliability, Jacobian matrices, concurrent crash failure, Network topology, data structures, fault tolerant computing, routing information, middleware]
Model-Based Validation for Internet Services
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Operator mistakes are a significant source of unavailability in Internet services. In our previous work, we proposed operator action validation as an approach for detecting mistakes while hiding them from the service and its users. Previous validation strategies have limitations, however, including the need for instances of correct behavior for comparison. In this paper, we propose a novel model-based validation strategy that addresses these limitations and complements our previous techniques. Model-based validation calls for service engineers to define models of Internet services that can be used to differentiate between correct and incorrect configurations and behaviors. These models are then used to guide the specification of validation assertions that check the correctness of operator actions before they are exposed. We have implemented a prototype model-based validation system for two services, the Web crawler of a commercial search engine (Ask.com) and an academic yet realistic online auction service. Experimentation with model-based validation demonstrates that it is highly effective at detecting and hiding both activated and latent mistakes.
[model-based validation, Internet service, Software prototyping, System testing, search engines, Web crawler, search engine, internet service, Crawlers, online auction service, Electronic mail, Computer science, Design engineering, Web services, USA Councils, Web and internet services, Prototypes, Search engines, model, operator mistake]
The Effects of Threading, Infection Time, and Multiple-Attacker Collaboration on Malware Propagation
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Self-propagating malware spreads over the network quickly and automatically. Malware propagation should be modeled accurately for fast detection and defense. State-of-the-art malware propagation models fail to consider a number of issues. First, the malware can scan a host for multiple vulnerabilities on multiple ports. Second, the vulnerability scanning can be done by multiple threads concurrently. Third, the exploitation of vulnerabilities and the infection of vulnerable hosts cannot be done instantly. Fourth, the malware propagation can start from multiple places in the network rather than a single release point. Finally, the malware copies can collaborate with each other to cause much more damage. Little was done to understand the effects of multi-port scanning, multi-threading, infection time, multiple starting points, and collaboration (MMIMC) on malware propagation. This research quantitatively measures the effects of MMIMC on infected hosts. We employ the Fibonacci number sequence (FNS)to model the effects of infection time. We derive the shift property, which illustrates that different malware initialization scan be represented by shifting their propagations on the time axis. We prove the linear property, which shows that the effects of multiple-attacker collaboration can be represented by linear combinations of individual attacks. Experimental results show that the above issues significantly affect malware propagation and verify our analysis.
[invasive software, Computer worms, multiple-attacker collaboration, Fibonacci sequences, Propagation, Network Security, self-propagating malware, shift property, Yarn, Distributed computing, linear property, Weapons, Operating systems, USA Councils, malware propagation, Reconnaissance, Computer networks, Malware, multi-threading, International collaboration, Fibonacci number sequence, multi-port scanning, Thread, Collaboration, infection time, Payloads]
Designing System-Level Defenses against Cellphone Malware
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Cellphones are increasingly becoming attractive targets of various malware, which not only cause privacy leakage, extra charges, and depletion of battery power, but also introduce malicious traffic into networks. In this work, we seek system-level solutions to handle these security threats. Specifically, we propose a mandatory access control-based defense to blocking malware that launch attacks through creating new processes for execution. To combat more elaborated malware which redirect program flows of normal applications to execute malicious code within a legitimate security domain, we further propose using artificial intelligence (AI) techniques such as Graphic Turing test. Through extensive experiments based on both Symbian and Linux smartphones, we show that both our system-level countermeasures effectively detect and block cellphone malware with low false positives, and can be easily deployed on existing smartphone hardware.
[invasive software, Batteries, artificial intelligence, smartphone hardware, Privacy, Linux smartphones, Communication system traffic control, Malware, access control, Turing test, Testing, authentication, system-level defenses design, mandatory access control-based defense, Power system security, Graphics, cellphone malware, artificial intelligence techniques, Symbian smartphones, Cellular phones, Linux, graphic turing test, malicious traffic, Artificial intelligence, Smart phones, telecommunication traffic]
PolyVaccine: Protecting Web Servers against Zero-Day, Polymorphic and Metamorphic Exploits
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Today Web servers are ubiquitous having become critical infrastructures of many organizations. However, they are still one of the most vulnerable parts of organizations infrastructure. Exploits are many times used by worms to fast propagate across the full Internet being Web servers one of their main targets. New exploit techniques have arouse in the last few years that have rendered useless traditional IDS techniques based on signature identification. Exploits use polymorphism (code encryption) and metamorphism (code obfuscation) to evade detection from signature-based IDSs. In this paper, we address precisely the topic of how to protect Web servers against zero-day (new), polymorphic, and metamorphic malware embedded in data streams (requests) that target Web servers. We rely on a novel technique to detect harmful binary code injection (i.e., exploits) in HTTP requests that is more efficient than current techniques based on binary code emulation or instrumentation of virtual engines. The detection of exploits is done through sandbox processes. The technique is complemented by another set of techniques such as caching, and pooling, to reduce its cost to neglectable levels.Our technique has little assumptions regarding the exploit unlike previous approaches that assume the existence of sled or getPC code, loops, read of the payload, writes to different addresses, etc. The evaluation shows that caching is highly effective and that the average latency introduced by our system is neglectable.
[telecommunication security, invasive software, worms, harmful binary code injection detection, Computer worms, HTTP request, cache storage, virtual engine instrumentation, signature-based IDS, Emulation, Intrusion detection, file servers, Binary codes, Search engines, Cryptography, Protection, Web server, zero-day exploit, code encryption, sandbox process, binary code emulation, polymorphic malware, code obfuscation, malware, Instruments, Web server protection, metamorphic malware, cryptography, polymorphic exploit, vulnerability, zero-day malware, web server, metamorphic exploit, PolyVaccine, digital signatures, Internet, exploit]
Fault Localization for Firewall Policies
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Firewalls are the mainstay of enterprise security and the most widely adopted technology for protecting private networks. Ensuring the correctness of firewall policies through testing is important. In firewall policy testing, test inputs are packets and test outputs are decisions. Packets with unexpected (expected) evaluated decisions are classified as failed (passed) tests. Given failed tests together with passed tests, policy testers need to debug the policy to detect fault locations (such as faulty rules). Such a process is often time-consuming.To help reduce effort on detecting fault locations, we propose an approach to reduce the number of rules for inspection based on information collected during evaluating failed tests. Our approach ranks the reduced rules to decide which rules should be inspected first. We performed experiments on applying our approach. The empirical results show that our approach can reduce 56% of rules that are required for inspection in fault localization.
[firewall policy testing, fault localization detection, Telecommunication traffic, private network protection, Network Security, Inspection, Fault location, Reliability engineering, enterprise security, Computer science, Policy Testing, Firewall Policy, Fault detection, Failure analysis, Fault Localization, authorisation, Computer security, Protection, Testing, Firewalls]
DToken: A Lightweight and Traceable Delegation Architecture for Distributed Systems
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Several major techniques have been proposed to address delegation problems in distributed computing environments of various scales, ranging from LAN, WAN, to the Internet. One of the major characteristics of existing public key cryptography based delegation mechanisms is their use of a fresh key pair every step along the delegation chain. This has led to a range of open issues, including a non-negligible performance overhead imposed by using a fresh key pair in proxy credentials; the lack of traceability of the principals in a delegation chain; and the complexity of managing the dynamically created key pairs in the distributed environment. This paper focuses on the architectural issues of delegation. We propose a new delegation architecture, called DToken, that takes advantage of the PKI. DToken is lightweight as it eliminates the use of freshly generated key pairs in a distributed setting. DToken is also traceable because the identity of the principals in a delegation chain is preserved by cryptographically verifiable mechanisms. A preliminary evaluation demonstrates that DToken outperforms the popular delegation solution of proxy certificate. In a single-level delegation, the cost of creating a DToken, the major cost of delegation, is roughly 1/3, 1/5, and 1/10 of that of creating a proxy certificate when the certificate key size is 512, 1024, and 2048 bits, respectively.
[Decision support systems, Costs, traceable delegation, lightweight delegation, proxy certificate, WAN, distributed processing, Grid delegation, delegation architecture, Security, Distributed computing, Authorization, cryptography verifiable mechanisms, public key cryptography, Computer architecture, distributed systems, grid delegation, single-level delegation mechanism, Local area networks, Wide area networks, traceable delegation architecture, Grid key management, Councils, distributed computing environments, DToken architecture, LAN, Public key cryptography, Internet, grid key management]
Constraint Based Automated Synthesis of Nonmasking and Stabilizing Fault-Tolerance
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
We focus on constraint based automated addition of nonmasking and stabilizing fault-tolerance to hierarchical programs. We specify legitimate states of the program in terms of constraints that should be satisfied in those states. To deal with faults that may violate these constraints, we add recovery actions while ensuring interference freedom among the recovery actions added for satisfying different constraints. Since the constraint based manual design of fault tolerance is well known to be applicable in the manual design of nonmasking fault tolerance, we expect our approach to have a significant benefit in automation of fault tolerant programs. We illustrate our algorithms with three case studies: stabilizing mutual exclusion, stabilizing diffusing computation, and a data dissemination problem in sensor networks. With experimental results,we show that the complexity of synthesis is reasonable and that it can be reduced using the structure of the hierarchical systems. To our knowledge, this is the first instance where automated synthesis has been successfully used in synthesizing programs that are correct under fairness assumptions. Moreover, in two of the case studies considered in this paper, the structure of the recovery paths is too complex to permit existing heuristic based approaches for adding recovery.
[stabilizing fault tolerance, Design automation, constraint based, constraint based automated addition, stabilization, diffusing computation stabilisation, Reliability engineering, mutual exclusion stabilisation, Electronic mail, nonmasking fault tolerance, Fault tolerance, USA Councils, Fault tolerant systems, nonmasking, Safety, constraint handling, interference freedom, hierarchical system structure, program synthesis, constraint based automated synthesis, data dissemination, Interference constraints, hierarchical program, distributed programs, Computer science, sensor network, fault tolerant program, Network synthesis, fault tolerant computing, program legitimate state, computational complexity]
Communication-Based Prevention of Non-P-Pattern
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
An issue pertinent to the design of checkpointing protocols is how to improve the autonomy of checkpointing and keep computation loss under control. To address the problem, a time-based multi-cycle checkpointing protocol is proposed in this paper. In this protocol, processes are allowed to take checkpoints with desired checkpoint cycles. To enable recent checkpoints to be used to form a consistent global checkpoint, a communication-based checkpoint cycle adjustment approach is also proposed. In this approach, the checkpoint cycle adjustment of each process follows a P-pattern. Simulation results show that the rollback deviation of the proposed protocol can be well controlled under a low checkpointing overhead.
[Checkpointing, checkpointing, checkpoint, Protocols, non-P-pattern, fault-tolerance, Communication system control, communication-based prevention, Control systems, Distributed computing, Delay, Computer science, Fault tolerance, Fault tolerant systems, protocols, Frequency synchronization, multi-cycle checkpointing, pattern recognition]
Spin One's Wheels? Byzantine Fault Tolerance with a Spinning Primary
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Most Byzantine fault-tolerant state machine replication (BFT) algorithms have a primary replica that is in charge of ordering the clients requests. Recently it was shown that this dependence allows a faulty primary to degrade the performance of the system to a small fraction of what the environment allows. In this paper we present Spinning, a novel BFT algorithm that mitigates such performance attacks by changing the primary after every batch of pending requests is accepted for execution. This novel mode of operation deals with those attacks at a much lower cost than previous solutions, maintaining a throughput equal or better to the algorithm that is usually consider to be the baseline in the area, Castro and Liskov's PBFT.
[Byzantine fault tolerance, client-server systems, Costs, Delay effects, state machine replication, Wheels, performance degradation attacks, Throughput, Computer crashes, clients request, Security, Degradation, Fault tolerance, performance attack, File systems, security of data, rotating primary, fault tolerant computing, BFT algorithm, Spinning, Spinning algorithm]
A Self-Stabilizing O(n)-Round k-Clustering Algorithm
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Given an arbitrary network G of processes with unique IDs and no designated leader, and given a k-dominating set I C G, we propose a silent self-stabilizing distributed algorithm that computes a subset D of I which is a minimal k-dominating set of G. Using D as the set of cluster-heads, a partition of G into clusters, each of radius k, follows. The algorithm is comparison-based, requires O(log n) space per process, converges in O(n) rounds and O(n2) steps, where n is the size of the network, and works under an unfair scheduler.
[Algorithm design and analysis, self-stabilization, graph theory, network theory (graphs), minimal k-dominating set, silent self-stabilizing k-clustering distributed algorithm convergence, set theory, Distributed computing, USA Councils, Clustering algorithms, Intrusion detection, unfair scheduling, distributed system fault tolerance, scheduling, K-dominating set, Computer networks, cluster-head set, graph partitioning, unfair scheduler, Distributed algorithms, silent, Partitioning algorithms, arbitrary distributed network, Scheduling algorithm, Computer science, pattern clustering, distributed algorithms, fault tolerant computing, K-clustering, computational complexity]
A Partition-Tolerant Manycast Algorithm for Disaster Area Networks
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Information dissemination in disaster scenarios requires timely and energy-efficient communication in intermittently connected networks. When the existing infrastructure is damaged or overloaded, we suggest the use of a manycast algorithm that runs over a wireless mobile ad hoc network, and overcomes partitions using a store-and-forward mechanism. This paper presents a random walk gossip protocol that uses an efficient data structure to keep track of already informed nodes with minimal signaling. Avoiding unnecessary transmissions also makes it less prone to overloads. Experimental evaluation shows higher delivery ratio, lower latency, and lower overhead compared to a recently published algorithm.
[partition tolerance, Protocols, Costs, disaster management, data structure, energy-efficient communication, Delay, store-and-forward mechanism, Mobile ad hoc networks, Network topology, partition-tolerant manycast algorithm, energy efficiency, Broadcasting, random walk gossip protocol, disaster area network, signalling, GSM, mobile radio, information dissemination, dependability, disasters, telecommunication signalling, Partitioning algorithms, ad-hoc networks, manycast, Streaming media, intermittently connected network, Energy efficiency, wireless mobile ad hoc network, ad hoc networks]
Genuine versus Non-Genuine Atomic Multicast Protocols for Wide Area Networks: An Empirical Study
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
We study atomic multicast, a fundamental abstraction for building fault-tolerant systems. We suppose a system composed of data centers, or groups, that host many processes connected through high-end local links; a few groups exist, interconnected through high-latency communication links. A recent paper showed that no multicast protocol can deliver messages addressed to multiple groups in one inter-group delay and be genuine, i.e., to deliver a message m, only the addressees of m are involved in the protocol. We propose a non-genuine multicast protocol that may deliver messages addressed to multiple groups in one inter-group delay. Experimental comparisons against a latency-optimal genuine protocol show that the non-genuine protocol offers better performance in almost all considered scenarios. We also identify a convoy effect in multicast algorithms that may delay the delivery of local messages, i.e., messages addressed to a single group, by as much as the latency of global messages, i.e., messages addressed to multiple groups, and propose techniques to minimize this effect. To complete our study, we evaluate a latency-optimal protocol that tolerates disasters, i.e., group crashes.
[Wide area networks, wide area networks, high-latency communication links, Delay effects, Scalability, fault-tolerance, data centers, analytical and experimental evaluation, Multicast protocols, Mobile communication, Computer crashes, fault-tolerant systems, Multicast algorithms, atomic multicast, Fault tolerant systems, multicast protocols, non-genuine atomic multicast protocols, Bandwidth, Broadcasting, inter-group delay]
A Holistic Solution to Pursuer-Evader Tracking in Sensor Networks
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
In this paper we devise a holistic solution to the pursuer-evader tracking problem taking into account the limitations of the wireless sensor networks (WSNs) as well as the dynamics of both the pursuer and evader. More specifically, we present an optimal strategy for the pursuer to capture the evader despite the delayed and imprecise information available at the pursuer-side. In order to minimize the communication overhead while ensuring capture, we provide an optimal evader sampling scheme that adjusts the sampling frequency based on the strategies of the pursuer and evader, as well as the distance between the pursuer and evader. We support our adaptive sampling scheme with a just-in-time delivery protocol that publishes the evader's location updates directly to the pursuer, reducing the communication overhead of tracking even further. To further enhance the tracking reliability, we use a two-level design of fault tolerance: 1) a double position advertisement scheme to mask single message losses, and 2) a breadcrumbs-based backup scheme for stabilizing from desynchronization.Our simulation results show that the adaptive sampling scheme guides the pursuer to capture the evader effectively, and reduces the communication overhead significantly compared to fixed rate sampling. Our simulation results also show that our two-level fault-tolerance strategy ensures high capture rates even under consecutive message losses.
[adaptive sampling, pursuer-evader tracking, fault tolerance, wireless sensor networks, Sensor systems, Game theory, communication overhead, Delay, Computer science, Wireless sensor networks, Fault tolerance, adaptive signal processing, Computer network reliability, optimal evader sampling, just-in-time delivery protocol, target tracking, signal sampling, Sampling methods, Frequency, Positron emission tomography]
PEQ: A Privacy-Preserving Scheme for Exact Query Evaluation in Distributed Sensor Data Networks
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Evaluating queries in distributed sensor networks while preserving privacy of data is a challenging problem. In this paper, we propose a new scheme for evaluating almost all types of queries, including sum, min/max, mean, median and histogram, accurately while, at the same time, preserving privacy of individual data. Our scheme does not require sensor nodes to share secret keys with each other. Further, it does not use encryption and secure hashing, both of which can be expensive operations.
[telecommunication security, Data privacy, Base stations, wireless sensor networks, Data security, wireless sensor network, privacy-preserving scheme, query evaluation, Sensor systems, data aggregation, distributed sensor data network, query processing, Wireless sensor networks, Histograms, Query processing, sensor network, Cities and towns, privacy-preservation, data privacy, Cryptography, Monitoring]
On Consistent Neighborhood Views in Wireless Sensor Networks
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Wireless sensor networks (WSNs) are characterized by localized interactions. Indeed, several WSN algorithms and protocols work in a decentralized fashion by coordinating nodes within the wireless communication range, e.g., localization algorithms and MAC protocols. Nevertheless, most often these mechanisms do not address faults that may affect the way wireless neighborhoods are recognized by nodes, e.g., as in the case of data corruption. As the operation of these mechanisms is rooted in the use of topology information, these faults may be a significant detriment to correct and efficient system operation.In this paper, we argue that the above issues are particular instances of a general problem of consistent neighborhood view. We present three increasingly weaker specifications of the problem. Next, we prove the impossibility of solving the two stronger specifications, and provide an algorithm to solve the weakest specification. In addition, we implement our algorithm in a commonly used WSN network stack, and assess its performance both in simulation and in a real-world testbed. The results show that, when possible, our mechanisms efficiently solve the problem of consistent neighborhood view, providing higher-level mechanisms with a re-usable building block to leverage off.
[wireless sensor networks, Wireless application protocol, access protocols, mobility management (mobile radio), consistent neighborhood views, Wireless communication, Computer science, Wireless sensor networks, Time division multiple access, Network topology, Processor scheduling, Computer network reliability, Media Access Protocol, Propagation losses, localization algorithms, MAC protocols]
TASK: Template-Based Key Assignment for Confidential Communication in Wireless Networks
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Predistribution of cryptographic keys is a widely used approach for establishing secure communication between network nodes which are severely resource-constrained. Many proposed key predistribution schemes make the implicit assumption that message contents need not be kept private from nodes other than the intended recipient. Messages in such schemes are not guaranteed to be confidential--they may be read by nodes within the network other than the intended recipient. In this paper, we present TASK--a symmetric key predistribution scheme that enables secure and confidential communication within wireless networks. TASK distributes keys by generating and reinforcing a series of template key assignment instances. We show, through analysis and simulation, that TASK achieves a level of security superior to that of two recently proposed schemes that also provide confidentiality, while maintaining the same space complexity. TASK is also parameterized, which allows it to make use of key storage capacities that other recently proposed schemes cannot.
[telecommunication security, radio networks, Mobile communication, cryptography, wireless networks, confidential communication, Communication system security, secure communication, Computer science, Wireless communication, Wireless sensor networks, Analytical models, key storage capacity, Wireless networks, Computer network reliability, cryptographic key predistribution, symmetric key predistribution scheme, Telecommunication network reliability, Cryptography, template-based key assignment, space complexity]
ZoneTrust: Fast Zone-Based Node Compromise Detection and Revocation in Sensor Networks Using Sequential Analysis
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Due to the unattended nature of wireless sensor networks, an adversary can physically capture and compromise sensor nodes and then mount a variety of attacks with these compromised nodes. To minimize the damage incurred by compromised nodes, the system should detect and revoke them as soon as possible. To meet this need, we propose a zone-based node compromise detection and revocation scheme in sensor networks. The main idea of the proposed scheme is to use the sequential hypothesis testing to detect suspect regions in which compromised nodes are likely placed. In these suspect regions, the network operator performs software attestation against sensor nodes, leading to the detection and revocation of the compromised nodes. Through analysis and simulation, we show that the proposed scheme provides effective and robust node compromise detection and revocation capability with little overhead.
[ZoneTrust, wireless sensor networks, zone-based node compromise detection, sequential analysis, Software performance, Reliability engineering, Sensor systems, software attestation, Computer science, Sequential analysis, Wireless sensor networks, Analytical models, Computer network reliability, Robustness, zone-based node compromise revocation, Monitoring]
The Blocking Option in Routing Protocols
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Routing protocols are designed under the assumption that each node in a network should be able to reach (i. e. send or forward packets to) every other node in the network. Unfortunately, adopting this assumption in a routing protocol does allow adversary nodes to launch spam or DoS attacks against the other nodes in the network. In this paper, we introduce the "blocking option" in routing protocols; this option allows a node u to block a specified set of nodes {v,..,w} and prevent each of them from reaching node u. It turns out that if node u blocks a large number of nodes, then u may end up blocking other nodes as well. We refer to these unintentionally blocked nodes as blind_to u nodes. Clearly, a node u cannot communicate with its blind nodes in a regular manner. Thus, we extend the routing protocol to allow each node u to communicate with its blind nodes via some special node, called the joint node. To perform its intended function, the joint node needs to be neither blocked by any node nor blind to any node in the network. We give an algorithm for identifying the node that is best suited to be the joint node in a network. Finally, we show, through extensive simulation, that the average number of blind nodes is close to zero when the average number of blocked nodes is small (&lt; 20) and that the probability of a joint node being blind is quite small, on the order of 10-3. The path length of using a joint node for communication between a node u and any one of its blind nodes v is around 1.5 the shortest path between u and v.
[Filtering, Telecommunication traffic, Distance Vector, Routing Protocols, Security, Distributed computing, Computer crime, Sufficient conditions, Blocking Option, Information security, block codes, routing protocols, Bandwidth, Routing protocols, Computer networks, Internet, blocking option, joint node]
X-BOT: A Protocol for Resilient Optimization of Unstructured Overlays
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Gossip, or epidemic, protocols have emerged as a highly scalable and resilient approach to implement several application level services such as reliable multicast, data aggregation, publish-subscribe, among others. All these protocols organize nodes in an unstructured random overlay network. In many cases, it is interesting to bias the random overlay in order to optimize some efficiency criteria, for instance, to reduce the stretch of the overlay routing. In this paper we propose X-BOT, a new protocol that allows to bias the topology of an unstructured gossip overlay network. X-BOT is completely decentralized and, unlike previous approaches, preserves several key properties of the original (non-biased) overlay (most notably, the node degree and consequently, the overlay connectivity). Experimental results show that X-BOT can generate more efficient overlays than previous approaches.
[resilient unstructured random overlay network optimization, Peer to peer computing, Redundancy, computer networks, telecommunication network topology, unstructured gossip overlay network topology, Multicast protocols, Routing, Maintenance, overlay routing protocol, Peer-to-Peer, Unstructured Overlay Networks, Distributed Systems, optimisation, Network topology, Network Protocols, routing protocols, Publish-subscribe, Broadcasting, Sampling methods, Cost function, X-BOT]
A Framework for Distributed Monitoring and Root Cause Analysis for Large IP Networks
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
As the size of a centrally managed IP network increases, the cost of monitoring network devices and the number of reported events increase super-linearly. This in turn degrades the performance of the event correlation engine that is responsible for suppressing dependent events and escalating root cause events to a network administrator. To solve this scalability problem, we propose a distributed framework that partitions the network into smaller management domains and enables concurrent monitoring and event correlation in those domains. The gain in performance, however, comes with the challenge of correlating cross-domain events which occurs when failure in one domain induces events in other domain(s). In this paper, we investigate such situations and show in the worst case it would be impossible to determine the root cause. We propose a two step approach to solve this problem. First, we define a property called route-closure, which if satisfied by every partition not only minimizes the number of cross-domain events but also eliminates cases wherein root cause analysis may be inconclusive. We also describe a technology-centric partitioning mechanism that constructs partitions satisfying the route-closure property. Next, we propose a distributed architecture to efficiently identify and correlate cross-domain events. We use a commercial network management system to implement our distributed framework and run experiments by injecting synthetic events on large, real network topologies. Our experimental results show that our approach can manage over 200,000 managed entities and handle event bursts of size 15,000 in under five minutes without compromising the efficacy of event correlation.
[Costs, technology-centric partitioning mechanism, Fault Diagnosis, Network Management, cross-domain events, Event Correlation, root cause analysis, Engines, Condition monitoring, Degradation, route-closure, large IP networks, Network topology, Databases, USA Councils, routing protocols, distributed monitoring, Performance analysis, IP networks, Probes]
On the Cost of Database Clusters Reconfiguration
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Database clusters based on share-nothing replication techniques are currently widely accepted as a practical solution to scalability and availability of the data tier. A key issue when planning such systems is the ability to meet service level agreements when load spikes occur or cluster nodes fail. This translates into the ability to provision and deploy additional nodes. Many current research efforts focus on designing autonomic controllers to perform such reconfiguration, tuned to quickly react to system changes and spawn new replicas based on resource usage and performance measurements. In contrast, we are concerned about the inherent impact of deploying an additional node to an online cluster, considering both the time required to finish such an action as well as the impact on resource usage and performance of the cluster as a whole. If noticeable, such impact hinders the practicability of self-management techniques, since it adds an additional dimension that has to be accounted for. Our approach is to systematically benchmark a number of different reconfiguration scenarios to assess the cost of bringing a new replica online. We consider factors such as: workload characteristics, incremental and parallel recovery, flow control and outdatedness of the recovering replica. As a result, we show that research should be refocused from optimizing the capture and transmition of changes to applying them, which in a realistic setting dominates the cost of the recovery operation.
[Measurement, Availability, Costs, Protocols, Scalability, database clusters reconfiguration, share-nothing replication, Throughput, Control systems, database management systems, resource usage, Computer science, Databases, Distributed databases, Replication, Meeting planning, Group Communication, Performance and QoS, Testing]
Location-Aware Cache-Coherence Protocols for Distributed Transactional Contention Management in Metric-Space Networks
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
A transactional memory API utilizes contention managers to guarantee that whenever two transactions have a conflict on a resource, one of them is aborted. While they have been well studied in the context of multiprocessors, their properties for distributed transactional memory systems are still unknown. Compared with multiprocessor transactional memory systems, the design of distributed transactional memory systems is more challenging because of the need for distributed cache-coherence protocols and the underlying (higher) network latencies involved. The choice of the combination of the contention manager and the cache-coherence protocol is critical for the performance of distributed transactional memory systems. How does a designer go about deciding what contention manager and what cache-coherence protocol to use in a distributed transactional memory system? In this paper, we answer this question. We consider metric-space networks, where the communication delay between nodes forms a metric. We show that the performance of a distributed transactional memory system on metric-space networks is O(N<sub>i</sub> 2) for N<sub>i</sub> transactions requesting for a single object under the Greedy contention manager and an arbitrary cache-coherence protocol. To improve the performance, we propose a class of location-aware distributed cache-coherence protocols, called LAC protocols. We show that the combination of the greedy contention manager and an efficient LAC protocol yields an O(N log N middot s) competitive ratio, where N is the maximum number of nodes that request the same object, and s is the number of objects. This is the first such performance bound established for distributed transactional memory contention managers. Our results yield the following design strategy: select a distributed contention manager and determine its performance without considering distributed cache-coherence protocols; then find an appropriate cache-coherence protocol to improve performance.
[transaction processing, location-aware cache-coherence protocol, Content management, application program interfaces, metric-space network, Access protocols, distributed processing, memory protocols, cache storage, distributed transactional contention management, Delay, greedy contention manager, transactional memory API, Engineering management, Computer network reliability, USA Councils, Memory management, LAC protocol, Los Angeles Council, Resource management, Computer network management, distributed cache-coherence protocol]
Co-scheduling of Disk Head Time in Cluster-Based Storage
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
Disk time slicing is a promising technique for storage performance insulation. To work with cluster based storage, however, time slices associated with striped data must be co-scheduled on the corresponding servers. This paper describes algorithms for determining global time slice schedules and mechanisms for coordinating the independent server activities. Experiments with a prototype show that, combined, they can provide performance insulation for workloads sharing a storage cluster -- each workload realizes a configured minimum efficiency within its time slices regardless of the activities of the other workloads.
[independent server activity coordination, Laboratories, Throughput, File servers, approximation algorithms, disc storage, processor scheduling, shared storage, workloads sharing, striped data, Network servers, USA Councils, heuristics, Clustering algorithms, disk head time co-scheduling, shared memory systems, Argon, configured minimum efficiency, Insulation, Interference, disk time slicing, performance evaluation, Scheduling, quality of service, strip packing, cluster based storage, performance, storage performance insulation, clustering, performance isolation]
Assessment and Improvement of Hang Detection in the Linux Operating System
2009 28th IEEE International Symposium on Reliable Distributed Systems
None
2009
We propose a fault injection framework to assess hang detection facilities within the Linux operating system (OS). The novelty of the framework consists in the adoption of a more representative fault load than existing ones, and in the effectiveness in terms of number of hang failures produced; representativeness is supported by a field data study on the Linux OS. Using the proposed fault injection framework, along with realistic workloads, we find that the Linux OS is unable to detect hangs in several cases. We experience a relative coverage of 75%. To improve detection facilities, we propose a simple yet effective hang detector, which periodically tests OS liveness, as perceived by applications, by means of I/O system calls; it is shown that this approach can improve relative coverage up to 94%. The hang detector can be deployed on any Linux system, with an acceptable overhead.
[Software testing, System testing, operating system, Autonomic Systems, Application software, Face detection, Hang Detection, Fault tolerance, Linux, Operating systems, Fault detection, Linux OS, Detectors, hang detection, Hardware, Fault Injection]
Message from General Co-chairs
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Presents the welcome message from the conference proceedings.
[]
Message from Technical Program Co-chairs
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Presents the welcome message from the conference proceedings.
[]
Conference Committees
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Provides a listing of current committee members.
[]
Technical Program Committee
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Provides a listing of current committee members.
[]
PrEServD - Privacy Ensured Service Discovery in Mobile Peer-to-Peer Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
In mobile peer-to-peer networks, proposed service discovery protocols disregard the exposure of the participating peers' privacy details (privileged information). In these methods, the participating peers must provide their identities during the service discovery process to be authorized to utilize services. However, a peer may not be willing to reveal its privileged information until it identifies the service providing peer. So these peers face a problem, should the service requesting or the service providing peer reveal the identity first. The protocol presented in solves this problem to some extent and discover the services available in the service requester's vicinity in a single-hop time sync peers only. In this paper, we propose a privacy-preserving model based on challenged/response idea to discover the services available in the mobile peer-to-peer network even when the moving service requester and the service provider are at a multi-hop distance away. The performance studies shows that our protocol does preserve the privacy in a communication efficient way with reduced false positives in comparison to one other recently proposed protocol.
[privacy ensured service discovery, peer-to-peer computing, Peer to peer computing, Medical services, utilize services, Mobile communication, privileged information, computer network security, mobile peer-to-peer networks, PrEServD, service requester, mobile computing, single hop time sync peers, Games, service providing peer, Routing protocols, Cryptography]
Securing Mobile Unattended WSNs against a Mobile Adversary
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
One important factor complicating security in Wireless Sensor Networks (WSNs) is lack of inexpensive tamper-resistant hardware in commodity sensors. Once an adversary compromises a sensor, all memory and forms of storage become exposed, along with all secrets. Thereafter, any cryptographic remedy ceases to be effective. Regaining sensor security after compromise (i.e., intrusion-resilience) is a formidable challenge. Prior approaches rely on either (1) the presence of an on-line trusted third party (sink), or (2) the availability of a True Random Number Generator (TRNG) on each sensor. Neither assumption is realistic in large-scale Unattended Wireless Sensor Networks (UWSNs) composed of low-cost commodity sensors. periodic visits by the sink. Previous work has demonstrated that sensor collaboration is an effective, yet expensive, means of attaining intrusion-resilience in UWSNs. In this paper, we explore intrusion resilience in Mobile UWSNs in the presence of a powerful mobile adversary. We show how the choice of the sensor mobility model influences intrusion resilience with respect to this adversary. We also explore self healing protocols that require only local communication. Results indicate that sensor density and neighborhood variability are the two key parameters affecting intrusion resilience. Our findings are supported by extensive analyses and simulations.
[telecommunication security, mobile adversary, Protocols, wireless sensor networks, Mobile communication, cryptography, Encryption, mobility management (mobile radio), Security, Wireless Sensor Networks, sensor security, TRNG, mobile UWSN, Wireless sensor networks, sensor mobility, unattented WSN security, true random number generator, Collaboration, intrusion resilience]
Attack Injection to Support the Evaluation of Ad Hoc Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
The increasing emergence of mobile computing devices seamlessly providing wireless communication capabilities opens a wide range of new application domains for ad hoc networks. However, the sensitivity of ad hoc routing protocols to malicious faults (attacks) limits in practice their confident use in commercial products. This requires not only practical means to enforce the security of these protocols, but also approaches to evaluate their behaviour in the presence of attacks. Our previous contribution to the evaluation of ad hoc networks has been focused on the definition of an approach for injecting grey hole attacks in real (non-simulated) ad hoc networks. This paper relies on the use of this methodology to evaluate (i) three different implementations of a proactive ad hoc routing protocol, named OLSR, and (ii) two ad hoc routing protocols of different nature, one proactive (OLSR) and one reactive (AODV). Reported results have proven useful to extend the applicability of attack injection methodologies for evaluation beyond the mere assessment of the robustness of ad hoc routing protocols.
[Wireless communication, Availability, mobile computing, ad hoc routing protocols, routing protocols, attack injection, Routing, Routing protocols, Ad hoc networks, Communication system security, ad hoc networks, mobile computing devices]
Optimization Based Topology Control for Wireless Ad Hoc Networks to Meet QoS Requirements
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
This paper proposes a technique for topology control (TC) of wireless nodes to meet Quality of Service (QoS) requirements between source and destination node pairs. The nodes are assumed to use a TDMA (Time Division Multiple Access) based MAC (Medium Access Control) layer. Given a set of QoS requirements, a set of wireless nodes and their initial positions, the goal is to find a topology of the nodes by adjusting the transmitting power, which will meet the QoS requirements under the presence of interference and at the same time minimize the energy consumed. The problem of TC is treated like an optimization problem and techniques of Linear Programming (LP) and Genetic Algorithms (GA) are used to solve it. The solution obtained after solving the optimization problem is in the form of optimal routes to be followed between each source, destination node pair. This information is used to construct the optimal topology.
[Topology Control, Gallium, Quality of service, Genetic Algorithm, linear programming, access protocols, Wireless communication, genetic algorithm, Adhoc Wireless Networks, Network topology, QoS, Interference Feasibility, Bandwidth, topology control, MAC layer, Linear Programming, TDMA, Quality of Service Requirement, Interference, medium access control, Topology, genetic algorithms, quality of service, Quality of Service, wireless ad hoc network, time division multiple access, ad hoc networks, Conflict Graph]
Fault-Tolerant Aggregation for Dynamic Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Data aggregation is a fundamental building block of modern distributed systems. Averaging based approaches, commonly designated gossip-based, are an important class of aggregation algorithms as they allow all nodes to produce a result, converge to any required accuracy, and work independently from the network topology. However, existing approaches exhibit many dependability issues when used in faulty and dynamic environments. This paper extends our own technique, Flow Updating, which is immune to message loss, to operate in dynamic networks, improving its fault tolerance characteristics. Experimental results show that the novel version of Flow Updating vastly outperforms previous averaging algorithms, it self adapts to churn without requiring any periodic restart, supporting node crashes and high levels of message loss.
[Heuristic algorithms, fault-tolerance, distributed processing, distributed system, Computer crashes, data aggregation, Delay, dynamic network, flow updating, Convergence, Accuracy, Network topology, distributed algorithms, Detectors, dynamic networks, fault tolerant computing, fault-tolerant aggregation]
Crash-Tolerant Collision-Free Data Aggregation Scheduling for Wireless Sensor Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Data aggregation scheduling, or converge cast, is a fundamental pattern of communication in wireless sensor networks (WSNs), where sensor nodes aggregate and relay data to a sink node. For WSN applications that require fast response time, it is imperative that the data reaches the sink as fast as possible. For such timeliness guarantees, TDMA-based scheduling can be used to assign time slots to nodes in which they can transmit messages. However, any slot assignment approach needs to be cognisant of the fact that crash failures can occur (e.g., due to battery exhaustion, defective hardware). In this paper, we study the design of such data aggregation scheduling (converge cast) protocols. We make the following contributions: (i) we identify a necessary condition to solve the converge cast problem, (ii) we introduce two versions of the converge cast problem, namely (a) a strong version, and (b) a weak version , (iii) we show that the strong converge cast problem cannot be solved, (iv) we show that deterministic weak converge cast cannot be solved in presence of crash failures, (v) we show that there is no d-local algorithm that solves stabilising weak converge cast in presence of crash failures, (vi) we provide a modular d-local algorithm that solves stabilising weak converge cast in presence of crash failures where d is the network radius, and (vii) we show how specific instantiations of parameters can lead to an d-local algorithm that achieves more efficient stabilization. Our contributions are novel: (i) the first contribution (necessary condition) provides the theoretical basis which explains the structure of existing converge cast algorithms, and (ii) the study of converge cast in presence of crash failures has not previously been studied.
[Schedules, algorithms, Protocols, TDMA-based scheduling, wireless sensor networks, stabilization, battery exhaustion, convergecast protocols, Computer crashes, data aggregation, Wireless sensor networks, Time division multiple access, sensor nodes, time division multiple access, crash failures, Distributed databases, crash-tolerant collision-free data aggregation scheduling, Safety, defective hardware, modular d-local algorithm, protocols, sink node]
Fixed Cost Maintenance for Information Dissemination in Wireless Sensor Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Because of transient wireless link failures, incremental node deployment, and node mobility, existing information dissemination protocols used in wireless ad-hoc and sensor networks cause nodes to periodically broadcast "advertisement" containing the version of their current data item even in the "steady state" when no dissemination is being done. This is to ensure that all nodes in the network are up-to-date. This causes a continuous energy expenditure during the steady state, which is by far the dominant part of a network's lifetime. In this paper, we present a protocol called Varuna which incurs a constant energy cost, independent of the duration of the steady state. In Varuna, nodes monitor the traffic pattern of the neighboring nodes to decide when an advertisement is necessary. Using testbed experiments and simulations, we show that Varuna achieves several orders of magnitude energy savings compared to Trickle, the existing standard for dissemination in sensor networks, at the expense of a reasonable amount of memory for state maintenance.
[Protocols, wireless sensor networks, maintenance engineering, wireless ad hoc networks, Steady-state, Wireless communication, node mobility, continuous energy expenditure, Dissemination, protocols, Trickle, Maintenance engineering, Ad hoc networks, Topology, Steady state, fixed cost maintenance, Varuna protocol, Wireless sensor networks, neighboring nodes, information dissemination protocols, incremental node deployment, transient wireless link failures, ad hoc networks, traffic pattern, telecommunication traffic, network lifetime]
VMDriver: A Driver-Based Monitoring Mechanism for Virtualization
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Monitoring virtual machine (VM) is an essential function for virtualized platforms. Existing solutions are either coarse-grained - monitoring in granularity of VM level, or not general - only support specific monitoring functions for particular guest operating system (OS). Thus they do not satisfy the monitoring requirement in large-scale server cluster such as data center and public cloud platform, where each physical platform runs hundreds of VMs with different guest OSes. In this paper, we propose VMDriver, a general and fine-grained approach for virtualization monitoring. The novel design of VMDriver is the separation of event interception point in VMM level and rich guest OS semantic reconstructions in management domain. With this design, variant monitoring drivers in management domain can mask the differences of guest OSes. We implement VMDriver on Xen and our experimental study shows that it introduces very small performance overhead. We demonstrate its generality by inspecting four aspects information about the target virtual machines with different guest OSes. The unified interface of VMDriver brings convenience to develop complex monitoring tools for distributed virtualization environment.
[Driver-based Monitroing, Event Interception, distributed processing, guest OS semantic reconstruction, virtualization monitoring, guest operating system, VM Monitoring, Semantics, VMDriver, Malware, Kernel, Monitoring, Driver circuits, Generality, distributed virtualization environment, variant monitoring driver, driver-based monitoring mechanism, public cloud platform, performance overhead, virtual machine, large-scale server cluster, data center, Linux, event interception point, virtual machines, operating systems (computers), system monitoring, virtualized platform, Virtualization, Semantic Reconstruction]
DKSM: Subverting Virtual Machine Introspection for Fun and Profit
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Virtual machine (VM) introspection is a powerful technique for determining the specific aspects of guest VM execution from outside the VM. Unfortunately, existing introspection solutions share a common questionable assumption. This assumption is embodied in the expectation that original kernel data structures are respected by the untrusted guest and thus can be directly used to bridge the well-known semantic gap. In this paper, we assume the perspective of the attacker, and exploit this questionable assumption to subvert VM introspection. In particular, we present an attack called DKSM (Direct Kernel Structure Manipulation), and show that it can effectively foil existing VM introspection solutions into providing false information. By assuming this perspective, we hope to better understand the challenges and opportunities for the development of future reliable VM introspection solutions that are not vulnerable to the proposed attack.
[Switches, Data structures, Direct Kernel Structure Manipulation, virtual machine introspection, Introspection, security of data, Semantics, kernel data structures, virtual machines, Malware, data structures, direct kernel structure manipulation, Reliability, Kernel, Virtualization, Monitoring]
Real Distribution of Response Time Instability in Service-Oriented Architecture
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
This paper reports our practical experience of benchmarking a complex System Biology Web Service, and investigates the instability of its behaviour and the delays induced by the communication medium. We present the results of our statistical data analysis and distributions which fit and predict the response time instability typical of Service-Oriented Architectures (SOAs) built over the Internet. Our experiment has shown that the request processing time of the target e-science Web Service (WS) has a higher instability than the network round trip time. It has been found that by using a particular theoretical distribution, within short time intervals the request processing time can be represented better than the network round trip time. Moreover, certain characteristics of the probability distribution series of the round trip time make it particularly difficult to fit them theoretically. The experimental work reported in the paper supports our claim that dealing with the uncertainty inherent in the very nature of SOA and WSs is one of the main challenges in building dependable service-oriented systems. In particular, this uncertainty exhibits itself through very unstable web service response times and Internet data transfer delays that are hard to predict. Our findings indicate that the more experimental data is considered the less precise distributional approximations become. The paper concludes with a discussion of the lessons learnt about the analysis techniques to be used in such experiments, the validity of the data, the main causes of uncertainty and possible remedial actions.
[Uncertainty, distribution law, SOA, Service oriented architecture, real distribution, response time instability, Approximation methods, Delay, system biology web service, software architecture, service oriented systems, response time, bioinformatics, instability, service oriented architecture, statistical data analysis, Time factors, statistical analysis, network round trip time, service-oriented architecture]
Lightweight Task Graph Inference for Distributed Applications
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Recent paradigm shifts in distributed computing such as the advent of cloud computing pose new challenges to the analysis of distributed executions. One important new characteristic is that the management staff of computing platforms and the developers of applications are separated by corporate boundaries. The net result is that once applications go wrong, the most readily available debugging aids for developers are the visible output of the application and any log files collected during their execution. In this paper, we propose the concept of task graphs as a foundation to represent distributed executions, and present a low overhead algorithm to infer task graphs from event log files. Intuitively, a task represents an autonomous segment of computation inside a thread. Edges between tasks represent their interactions and preserve programmers' notion of data and control flows. Our technique leverages existing logging support where available or otherwise augments it with aspect-based instrumentation to collect events of a set of predefined types. We show how task graphs can improve the precision of anomaly detection in a request-oriented analysis of field software and help programmers understand the running of the Hadoop Distributed File System (HDFS).
[data flows, field software, Instruction sets, debugging aids, graph theory, Programming, anomaly detection, distributed computing, Distributed databases, aspect-oriented programming, cloud computing, log analysis, Java, control flows, data flow analysis, aspect based instrumentation, task graph inference, Synchronization, inference mechanisms, task graphs, Sockets, happens-before, Internet, Clocks, Hadoop distributed file system]
A Cooperative Sampling Approach to Discovering Optimal Configurations in Large Scale Computing Systems
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
With the growing scale of current computing systems, traditional configuration tuning methods become less effective because they usually assume a small number of parameters in the system. In order to handle the scalability issue of configuration tuning, this paper proposes a cooperative optimization framework, which mimics the behavior of team playing to discover the optimal configuration setting in computing systems. We follow a `best of the best' rule to decompose the tuning task into a number of small subtasks with manageable size and complexity. While each decomposed module is responsible for the optimization of its own configuration parameters, all the modules share the performance evaluations of new samples as common feedbacks to enhance their optimization objectives. As a result, the qualities of generated samples become improved during the search, and the cooperative sampling will eventually discover the optimal configurations in the system. Experimental results demonstrate that our proposed cooperative optimization can identify better solutions within limited time periods compared with other state of the art configuration search methods. Such advantage becomes more significant when the number of configuration parameters increases.
[sampling methods, cooperative optimization framework, data mining, probability, scalability issue, performance evaluation, Search problems, Covariance matrix, optimal configuration discovery, Servers, Tuning, Optimization, Equations, configuration management, large scale computing system, team playing behavior, System performance, configuration search, cooperative sampling, configuration tuning]
Uncertainty Propagation in Analytic Availability Models
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
In this paper, we discuss a Monte Carlo sampling based method for propagating the epistemic uncertainty in model parameters, through the system availability model. We also outline methods to compute the number of samples needed to obtain a desired confidence interval for various scenarios. We illustrate this method with a real system example and discuss the results obtained. While our example discusses confidence interval for system availability, this method can be directly applied to compute uncertainty for other dependability, performance and perform ability measures, computed by solving stochastic analytic models. We also emphasize the fact that no simulation is carried out in our method but a repeated sampling is performed over the parameter space followed by the execution of the analytic model with the final phase being the statistical analysis of the output vector.
[Availability, Uncertainty, sampling methods, epistemic distribution, Computational modeling, distributed processing, Markov model, Monte Carlo sampling, Equations, uncertainty propagation, Analytical models, Monte Carlo methods, analytic availability models, availability model, epistemic uncertainty modeling, confidence interval, hierarchical model, fault tree, Mathematical model, Aleatory uncertainty]
Monitoring Local Progress with Watchdog Timers Deduced from Global Properties
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Distributed systems are used in numerous applications where failures can be costly. Due to concerns that some of the nodes may become faulty, critical services are usually replicated across several nodes, which execute distributed algorithms to ensure correct service in spite of failures. To prevent replica-exhaustion, it is fundamental to detect errors and trigger appropriate recovery actions. In particular, it is important to detect situations in which nodes cease to execute the intended algorithm, e.g., when a replica is compromised by an attacker or when a hardware fault causes the node to behave erratically. This paper proposes a method for monitoring the local execution of nodes using watchdog timers. The approach consists in deducing, from the global system properties, local states that must be visited periodically by nodes that execute the intended algorithm correctly. When a node fails to trigger a watchdog before the time limit, an appropriate response can be initiated. The approach is applied to a well-known Byzantine consensus algorithm. The algorithm is modeled in the Promela language and the Spin model checker is used to identify local states that must be visited periodically by correct nodes. Such states are suitable for online monitoring using watchdog timers.
[Protocols, fault tolerance, Computational modeling, critical services, Process control, intrusion tolerance, watchdog timers, watchdogs, online monitoring, hardware fault causes, Fault tolerance, spin model checker, security of data, model checking, distributed algorithms, monitoring local progress, global properties, Lead, distributed systems, replica exhaustion, Promela language, Timing, Monitoring]
Adaptare-FD: A Dependability-Oriented Adaptive Failure Detector
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Unreliable failure detectors are a fundamental building block in the design of reliable distributed systems. But unreliability must be bounded, despite the uncertainties affecting the timeliness of communication. This is why it is important to reason in terms of the quality of service (QoS) of failure detectors, both in their specification and evaluation. We propose a novel dependability-oriented approach for specifying the QoS of failure detectors, and introduce Adapt are-FD, an autonomous and adaptive failure detector that executes according to this new specification. The main distinguishing features of Adapt are-FD with respect to existing adaptive failure detection approaches are discussed and explained in detail. A comparative evaluation of Adapt are-FD is presented. We highlight the practical differences between our approach and the well known Chen et al. approach for the specification of QoS requirements. We show that Adapt are-FD is easily configured, independently of the specific network environment. Furthermore, the results obtained using the Planet Lab platform indicate that Adapt are-FD outperforms other timeout-based solutions, combining versatility with improved QoS and dependability assurance.
[software reliability, dependability, Quality of service, PlanetLab platform, distributed processing, adaptation, quality of service, dependability oriented adaptive failure detector, Delay, Accuracy, Upper bound, Detectors, Adaptare-FD, distributed systems, Safety, Tunneling magnetoresistance, failure detection]
GAUL: Gestalt Analysis of Unstructured Logs for Diagnosing Recurring Problems in Large Enterprise Storage Systems
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
We present GAUL, a system to automate the whole log comparison between a new problem and the ones diagnosed in the past to identify recurring problems. GAUL uses a fuzzy match algorithm based on the contextual overlap between log lines and efficiently implements this using scalable index/search. The accuracy and efficiency of the comparison is further improved by leveraging problem set information and noise tolerance techniques. We evaluate GAUL using 4339 customer problems that occurred in all field deployments of an enterprise storage system over the course of a year. Our results show that with human-filtered logs, GAUL can identify the correct problem set 66% of the time among the top10 matches, which is 15% more accurate than the VSM system that uses cosine similarity and 19% more accurate than the ERRCMP system that uses error codes for log comparison. With unfiltered logs, the top10 match accuracy of GAUL is 40%, which is 22% more accurate than VSM and 26% more accurate than ERRCMP.
[pattern matching, log lines, Gestalt analysis, Noise, Humans, fuzzy set theory, Problem diagnosis, Search problems, fuzzy match algorithm, cosine similarity, scalable index, human-filtered log, Accuracy, search, fuzzy match, Hardware, whole log comparison, error codes, contextual overlap, leveraging problem set information, program diagnostics, ERRCMP system, large enterprise storage system, index, Indexes, recurring problem identification, recurring problem diagnosis, noise tolerance technique, GAUL system, scalable search, unstructured logs, business data processing, log comparison, Microprogramming]
Invariants Based Failure Diagnosis in Distributed Computing Systems
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
This paper presents an instance based approach to diagnosing failures in computing systems. Owing to the fact that a large portion of occurred failures are repeated ones, our method takes advantage of past experiences by storing historical failures in a database and retrieving similar instances in the occurrence of failure. We extract the system `invariants' by modeling consistent dependencies between system attributes during the operation, and construct a network graph based on the learned invariants. When a failure happens, the status of invariants network, i.e., whether each invariant link is broken or not, provides a view of failure characteristics. We use a high dimensional binary vector to store those failure evidences, and develop a novel algorithm to efficiently retrieve failure signatures from the database. Experimental results in a web based system have demonstrated the effectiveness of our method in diagnosing the injected failures.
[Measurement, Correlation, Web based system, Failure Diagnosis, Computational modeling, binary vector, graph theory, distributed processing, network graph, Distributed Systems, software fault tolerance, Invariants, distributed computing systems, vectors, Databases, Data models, Web server, invariant based failure diagnosis]
Shedding Light on Enterprise Network Failures Using Spotlight
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Fault localization in enterprise networks is extremely challenging. A recent approach called Sherlock makes some headway into this problem by using an inference algorithm over a multi-tier probabilistic dependency graph that relates fault symptoms with possible root causes (e.g., routers, servers). A key limitation of Sherlock is its scalability because of the use of complicated inference algorithms based on Bayesian networks. We present a fault localization system called Spotlight that essentially uses two basic ideas. First, it compresses a multi-tier dependency graph into a bipartite graph with direct probabilistic edges between root causes and symptoms. Second, it runs a novel weighted greedy minimum set cover algorithm to provide fast inference. Through extensive simulations with real service dependency graphs and enterprise network topologies reported previously in literature, we show that Spotlight is about 100&#x00D7; faster than Sherlock in typical settings, with comparable accuracy in diagnosis.
[greedy minimum set cover algorithm, graph theory, enterprise networks, Servers, Spotlight system, Accuracy, Network topology, enterprise network topology, enterprise network failure, Sherlock approach, belief networks, dependency graphs, Instruments, fault localization, probability, Probabilistic logic, inference mechanisms, bipartite graph, fault localization system, inference algorithm, Bayesian methods, multitier probabilistic dependency graph, service dependency graphs, Inference algorithms, fault tolerant computing, Bayesian networks, business data processing]
An Entity-Centric Approach for Privacy and Identity Management in Cloud Computing
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Entities (e.g., users, services) have to authenticate themselves to service providers (SPs) in order to use their services. An entity provides personally identifiable information (PII) that uniquely identifies it to an SP. In the traditional application-centric Identity Management (IDM) model, each application keeps trace of identities of the entities that use it. In cloud computing, entities may have multiple accounts associated with different SPs, or one SP. Sharing PIIs of the same entity across services along with associated attributes can lead to mapping of PIIs to the entity. We propose an entity-centric approach for IDM in the cloud. The approach is based on: (1) active bundles-each including a payload of PII, privacy policies and a virtual machine that enforces the policies and uses a set of protection mechanisms to protect themselves, (2) anonymous identification to mediate interactions between the entity and cloud services using entity's privacy policies. The main characteristics of the approach are: it is independent of third party, gives minimum information to the SP and provides ability to use identity data on untrusted hosts.
[Cloud computing, Protocols, zero-knowledge proofs (ZKP), Clouds, identity data, privacy, untrusted host, active bundles, Privacy, security, Prototypes, user authentication, protection mechanism, cloud computing, privacy policy, personally identifiable information (PII), cloud services, personally identifiable information, entity-centric approach, identity management, identity management (IDM), security of data, virtual machine, Authentication, service providers, virtual machines, privacy-enhancing technologies (PET), data privacy, anonymous identification, privacy management]
CloudRank: A QoS-Driven Component Ranking Framework for Cloud Computing
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
The rising popularity of cloud computing makes building high quality cloud applications a critical and urgently required research problem. Component quality ranking approaches are crucial for making optimal component selection from a set of functionally equivalent component candidates. Moreover, quality ranking of cloud components helps the application designers detect the poor performing components in the complex cloud applications, which usually include huge number of distributed components. To provide personalized cloud component ranking for different designers of cloud applications, this paper proposes a QoS-driven component ranking framework for cloud applications by taking advantage of the past component usage experiences of different component users. Our approach requires no additional invocations of the cloud components on behalf of the application designers. The extensive experimental results show that our approach outperforms the competing approaches.
[Measurement, Clouds, QoS, component ranking, Internet, quality of service, CloudRank, QoS driven component ranking framework, Indexes, cloud computing]
Secure, Dependable, and High Performance Cloud Storage
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
There have been works considering protocols for accessing partitioned data. Most of these works assume the local cluster based environment and their designs target atomic semantics. However, when considering widely distributed cloud storage systems, these existing protocols may not scale well. In this paper, we analyze the requirements of access protocols for storage systems based on data partitioning schemes in widely distributed cloud environments. We consider the regular semantics instead of atomic semantics to improve access efficiency. Then, we develop an access protocol following the requirements to achieve correct and efficient data accesses. Various protocols are compared experimentally and the results show that our protocol yields much better performance than the existing ones.
[access semantics, Clouds, distributed cloud environments, Access protocols, high performance cloud storage, access protocol, Servers, History, data partitioning schemes, distributed cloud storage systems, Dependable cloud storage, partitioned data access, security of data, pattern clustering, Semantics, distributed hash table, short secret sharing, Silicon, Internet, protocols, atomic semantics]
On-Demand Recovery in Middleware Storage Systems
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
This paper presents a recovery architecture for in-memory data management systems. Recovery in such systems boils down to solving two problems: retrieving and installing the last committed image of the crashed database on a new server and replaying the updates missing from the image. We improve recovery time with a novel technique called On-Demand Recovery, which removes the need to replay all missing updates before new transactions can be accepted. We have implemented and thoroughly evaluated the technique. We show in the paper that in some cases On-Demand Recovery can reduce recovery time by more than 50%.
[Protocols, optimistic techniques, Computer crashes, recovery, Servers, Indexes, database management systems, Engines, middleware storage systems, software architecture, on-demand recovery technique, database, recovery architecture, Hardware, in-memory data management systems, middleware]
P-Store: Genuine Partial Replication in Wide Area Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Partial replication is a way to increase the scalability of replicated systems: updates only need to be applied to a subset of the system's sites, thus allowing replicas to handle independent parts of the workload in parallel. In this paper, we propose P-Store, a partially replicated key-value store for wide area networks. In P-Store, each transaction T optimistically executes on one or more sites and is then certified to guarantee serializability of the execution. The certification protocol is genuine, it only involves sites that replicate data items read or written by T, and incorporates a mechanism to minimize a convoy effect. P-Store makes a thrifty use of an atomic multicast service to guarantee correctness: no messages need to be multicast during T's execution and a single message is multicast to certify T. In case T is global, that is, T's execution is distributed at different geographical locations, an extra vote phase is required. Our approach may offer better scalability than previously proposed solutions that either require multiple atomic multicast messages to execute T or are non-genuine. Experimental evaluations reveal that the convoy effect plays an important role even when one percent of the transactions are global. We also compare the scalability of our approach to a fully replicated solution when the proportion of global transactions and the number of sites vary.
[Wide area networks, P-Store, Protocols, replicated systems, wide area networks, certification protocol, Scalability, geographical locations, Computer crashes, key value store, History, Delay, Databases, serializability, genuine partial replication, multicast messages, Fault-tolerance, partial database replication]
FireSpam: Spam Resilient Gossiping in the BAR Model
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Gossip protocols are an efficient and reliable way to disseminate information. These protocols have nevertheless a drawback: they are unable to limit the dissemination of spam messages. Indeed, messages are redundantly disseminated in the network and it is enough that a small subset of nodes forward spam messages to have them received by a majority of nodes. In this paper, we present FireSpam, a gossiping protocol that is able to limit spam dissemination. FireSpam organizes nodes in a ladder topology, where nodes highly capable of filtering spam are at the top of the ladder, whereas nodes with a low spam filtering capability are at the bottom of the ladder. Messages are disseminated from the bottom of the ladder to its top. The ladder does thus act as a progressive spam filter. In order to make it usable in practice, we designed FireSpam in the BAR model. This model takes into account selfish and malicious behaviors. We evaluate FireSpam using simulations. We show that it drastically limits the dissemination of spam messages, while still ensuring reliable dissemination of good messages.
[FireSpam, Protocols, BAR model, malicious behaviors, Peer to peer computing, information dissemination, gossiping protocol, spam messages, unsolicited e-mail, information filtering, Topology, spam resilient gossiping, security of data, Fires, spam dissemination, Bandwidth, spam filtering, gossip protocols, Biomedical monitoring, Monitoring, ladder topology, byzantine and rational behaviours]
Semias: Self-Healing Active Replication on Top of a Structured Peer-to-Peer Overlay
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Active replication on top of a structured peer-to-peer overlay is an attractive solution for transparently providing high availability to distributed applications. However, self-healing is necessary to ensure the availability of the replicated application despite node arrivals, failures or departures in the overlay. Self-healing means to automatically reconfigure the replica groups when changes in the overlay occur. In the case of active replication, reconfigurations must be done carefully, to keep the replicas consistency. Moreover, as every reconfiguration could imply a state transfer between replicas, their number should be limited. In this paper we propose a self-healing solution that limits the number of group reconfigurations and ensures the availability of the replicated application in a dynamic environment. To evaluate the performance of our solution, we implemented it in a framework, called Semias, in the context of Vigne grid middleware. Experiments run on Grid'5000 and Planet Lab show the performance of the framework and the efficiency of our self-healing mechanisms in a dynamic environment.
[Availability, High Availability, Protocols, peer-to-peer computing, Peer to peer computing, Semias, structured peer-to-peer overlay, grid computing, Vigne grid middleware, Peer-to-Peer System, self healing active replication, state transfer, PlanetLab, Grid'5000, Fault tolerance, Self-Healing, group reconfigurations, Fault tolerant systems, Safety, Active Replication, Monitoring, middleware]
Experimental Validation of a Synchronization Uncertainty-Aware Software Clock
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
A software clock capable of self-evaluating its synchronization uncertainty is experimentally validated for a specific implementation on a node synchronized through NTP. The validation methodology takes advantage of an external node equipped with a GPS-synchronized clock acting as a reference, which is connected to the node hosting the system under test through a fast Ethernet connection. Experiments are carried out for different values of the software clock parameters and different types of workload, and address the possible occurrence of faults in the system under test and in the NTP synchronization mechanism. The validation methodology is designed to be as less intrusive as possible and to grant a resolution of the order of few hundreds of microseconds. The experimental results show very good performance of R&amp;SAClock, and their analysis gives precious hints for further improvements.
[Uncertainty, experimental validation, distributed processing, ethernet connection, Extraterrestrial measurements, Synchronization, ubiquitous computing, synchronisation, clocks, NTP synchronization mechanism, synchronization uncertainty, synchronization uncertainty aware software clock, R&amp;SAClock, Software, Safety, GPS synchronized clock, Monitoring, Clocks]
Applying Text Classification Algorithms in Web Services Robustness Testing
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Testing web services for robustness is an effective way of disclosing software bugs. However, when executing robustness tests, a very large amount of service responses has to be manually classified to distinguish regular responses from responses that indicate robustness problems. Besides requiring a large amount of time and effort, this complex classification process can easily lead to errors resulting from the human intervention in such a laborious task. Text classification algorithms have been applied successfully in many contexts (e.g., spam identification, text categorization, etc) and are considered a powerful tool for the successful automation of several classification-based tasks. In this paper we present a study on the applicability of five widely used text classification algorithms in the context of web services robustness testing. In practice, we assess the effectiveness of Support Vector Machines, Nai&#x0308;ve Bayes, Large Linear Classification, K-nearest neighbor (Ibk), and Hyperpipes in classifying web services responses. Results indicate that these algorithms can be effectively used to automate the identification of robustness issues while reducing human intervention. However, in all mechanisms there are cases of misclassified responses, which means that there is space for improvement.
[pattern classification, program debugging, text analysis, support vector machines, software bugs, Hyperpipes, web services, text classification algorithms, robustnes testing, Classification algorithms, k-nearest neighbor, Simple object access protocol, classification, Web services robustness testing, Training, Web services, Text categorization, naive Bayes, Robustness, Bayes methods, Testing]
Availability Assessment of HA Standby Redundant Clusters
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Computing systems are becoming the heart of modern technology, implementing critical tasks usually demanded to and implying human interactions. This highlights the problem of dependability in computer science contexts. High availability computing/clusters is a possible solution in such cases, implementing standby redundancy as a trade-off between dependability and costs. From the engineering perspective, this implies the use of specific techniques and tools for adequately evaluating the reliability/availability of high availability clusters, also taking into account dependencies among nodes (standby, repair, etc.) and the effect of wear and tear into such nodes, especially when failure and repair times are not exponentially distributed. The solution proposed in this paper is based on the use of phase type distributions and Kronecker algebra. In fact, we represent the reliability and maintainability of each component by specific phase type distributions, whose interactions describe the system availability. This latter is thus modeled by an expanded Markov chain expressed in terms of Kronecker algebra in order to face the state space explosion problem of expansion techniques and to represent the memory policies related to the aging process. More specifically, the paper firstly details the technique and then applies it to the evaluation of a standby redundant system representing a high availability cluster taken as example with the aim of demonstrating its effectiveness. Moreover, in order to show the potentiality of the technique, different maintenance strategies are evaluated and therefore compared.
[Availability, Kronecker algebra, Redundancy, software reliability, Maintenance engineering, algebra, state space explosion problem, HA standby redundant clusters, Markov chain, High Availability Clusters, Algebra, Standby Redundancy, availability assessment, Markov processes, Aging, Dynamic Reliability, high availability clusters]
Diskless Checkpointing with Rollback-Dependency Trackability
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
One way to implement fault tolerant applications is storing its current state in stable memory and, when a failure occurs, restart the application from the last global consistent state. If the number of simultaneous failures is expected to be small a diskless check pointing approach can be used, where a failed process's state can be determined only accessing non-faulty process's memory. In the literature diskless check pointing is usually based on synchronous protocols or properties of the application. In this paper we present a quasi-synchronous diskless check pointing algorithm, called RDT-Diskless, based on Rollback-Dependency Track ability. The proposed algorithm includes a garbage collection approach that limits the number of checkpoints that must be kept in memory. A framework, called Cheops, was developed and experimental results were obtained from a commercial cloud environment.
[Checkpointing, checkpointing, Protocols, Clouds, nonfaulty process memory, fault-tolerance, dependability, quasi-synchronous diskless check pointing algorithm, synchronous protocols, rollback-dependency trackability, availability, Synchronization, Servers, garbage collection approach, Cheops, Fault tolerance, storage management, Fault tolerant systems, distributed algorithms, fault tolerant applications, fault tolerant computing, protocols, RDT-diskless]
Swift Algorithms for Repeated Consensus
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
We introduce the notion of a swift algorithm. Informally, an algorithm that solves the repeated consensus is swift if, in a partial synchronous run of this algorithm, eventually no timeout expires, i.e., the algorithm execution proceeds with the actual speed of the system. This definition differs from other efficiency criteria for partial synchronous systems. Furthermore, we show that the notion of swiftness explains why failure detector based algorithms are typically more efficient than round-based algorithms, since the former are naturally swift while the latter are naturally non-swift. We show that this is not an inherent difference between the models, and provide a round implementation that is swift, therefore performing similarly to failure detector algorithms while maintaining the advantages of the round model.
[Algorithm design and analysis, Context, round-based algorithm, failure detector based algorithm, Computer crashes, partial synchronous system, Proposals, swift algorithm, failure analysis, system recovery, Delay, distributed algorithms, Detectors, repeated consensus, algorithm execution]
Thicket: A Protocol for Building and Maintaining Multiple Trees in a P2P Overlay
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
One way to efficiently disseminate information in a P2P overlay is to rely on a spanning tree. However, in a tree, interior nodes support a much higher load than leaf nodes. Also, the failure of a single node can break the tree, impairing the reliability of the dissemination protocol. These problems can be addressed by using multiple trees, such that each node is interior in just a few trees and a leaf node in the remaining, the multiple trees approach allows to achieve load distribution and also to send redundant information for fault-tolerance. This paper proposes Thicket, a decentralized algorithm to efficiently build and maintain such multiple trees over a single unstructured overlay network. The algorithm has been implemented and is extensively evaluated using simulation in a P2P overlay with 10.000 nodes.
[Protocols, Fasteners, Spanning Tree, leaf nodes, Load Balancing, Data Dissemination &amp; Streaming, Fault tolerance, P2P overlay, decentralized algorithm, Fault tolerant systems, Thicket, computer network reliability, protocols, dissemination protocol reliability, peer-to-peer computing, fault tolerance, Peer to peer computing, information dissemination, trees (mathematics), Peer-to-Peer System, Maintenance engineering, Topology, spanning tree, fault tolerant computing, multiple trees, Fault Tolerance]
A Tactical Information Management Middleware for Resource-Constrained Mobile P2P Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
In this paper we provide an architecture for Tactical Information Middleware for bandwidth constrained information management. We propose the ideas of rank-based data dissemination, and the use of a tactical information management query language. These ideas will deal with dynamic changes in bandwidth and explore opportunistic data dissemination. Thus, will lead to a cross layer design of a system capable of handling the dynamic data management issues relevant in many mission critical applications.
[mobile radio, peer-to-peer computing, Peer to peer computing, information management, Mobile communication, query languages, bandwidth constrained information management, Wireless communication, information management query language, Databases, Bandwidth, tactical information management middleware, resource-constrained mobile P2P networks, rank-based data dissemination, Mobile computing, Payloads, middleware]
On Optimizing Traffic Signal Phase Ordering in Road Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Traffic signals are an elementary component of all urban road networks and play a critical role in controlling the flow of vehicles. However, current road transportation systems and traffic signal implementations are very inefficient. The objective of this research is to evaluate optimal phase ordering within a signal cycles to minimize the average waiting delay and thus in turn minimizing fuel consumption and greenhouse gas (GHG) emissions. Through extensive simulation analysis, we show that by choosing optimal phase ordering, the stopped delay can be reduced by 40% per car at each signal resulting in a saving of up to 100 gallons of fuel per traffic signal each day.
[road traffic, traffic signal phase ordering, average waiting delay minimization, Roads, greenhouse gas emissions minimization, Vehicular Networks, urban road networks, Turning, Road networks, Fuels, Delay, Vehicles, transportation, optimal phase ordering, Traffic Signals, road transportation systems, fuel consumption minimization, vehicles flow control, Sensors]
Lightweight Fault-Tolerance for Peer-to-Peer Middleware
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
We address the problem of providing transparent, lightweight, fault-tolerance mechanisms for generic peer-to-peer middleware systems. The main idea is to use the peer-to-peer overlay to provide for fault-tolerance rather than support it higher up in the middleware architecture, e.g. in the form of services. To evaluate our approach we have implemented a fault-tolerant middleware prototype that uses a hierarchical peer-to-peer overlay in which the leaf peers connect to sensors that provide data streams. Clients connect to the root of the overlay and request streams that are routed upwards through intermediate peers in the overlay up to the client. We report encouraging preliminary results for latency, jitter and resource consumption for both the non-faulty and faulty cases.
[Real time systems, lightweight fault-tolerance mechanism, peer-to-peer overlay, peer-to-peer computing, Peer to peer computing, Jitter, Peer-to-Peer, Middleware, Fault tolerance, Fault tolerant systems, fault tolerant computing, Sensors, Fault-tolerance, middleware, peer-to-peer middleware systems]
Adaptive Routing Scheme for Emerging Wireless Ad Hoc Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Node mobility causes fading wireless channels, which in turn renders topology changes in emerging wireless ad hoc networks. In this paper, on-line estimators and Markov models are utilized to estimate fading channel conditions. Using the estimated channel conditions as well as queue occupancy, available energy and link delay, approximate dynamic programming (ADP) techniques are utilized to find dynamic routes, while solving discrete-time Hamilton-Jacobi-Bellman (HJB) equation forward-in-time for route cost in multichannel multi-interface networks. The performance of the proposed load balancing method in the presence of fading channels and the performance of the optimal route selection approach for multi-channel multi-interface wireless ad hoc network is evaluated by extensive simulations and comparing it to AODV.
[approximate dynamic programming techniques, Uncertainty, wireless ad hoc networks, Load Balancing, Vehicle dynamics, node mobility, Multi-channel Multi-interface Routing, Multi-path, topology changes, Bandwidth, Wireless Ad hoc Networks, wireless channels, AODV, Approximate Dynamic Programming, Fading, adaptive routing scheme, multichannel multi interface networks, dynamic programming, telecommunication network topology, Routing, Adaptive Dynamic Routing, radio access networks, discrete time Hamilton-Jacobi-Bellman equation, online estimators, telecommunication network routing, Markov processes, Load management, Markov models, ad hoc networks]
Towards Mobile Data Streaming in Service Oriented Architecture
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Service Oriented Architecture (SOA) is an architectural pattern providing agility to align technical solutions to modular business services that are decoupled from service consumers. Service capabilities such as interface options, quality of service (QoS), throughput, security and other constraints are described in the Service Level Agreement (SLA) that would typically be published in the service registry (UDDI) for use by consumers and/or mediation mechanisms. For mobile data streaming applications, problems arise when a service provider's SLA attributes cannot be mapped one-to-one to the service consumers (i.e. 150MB/sec video stream service provider to 5MB/sec data consumer). In this paper we present a generic framework prototype for managing and disseminating streaming data within a SOA environment as an alternative to custom service implementations based upon specific consumers or data types. Based on this framework, we implemented a set of services: Stream Discovery Service, Stream Multiplexor/Demultiplexor (routing) Service, Stream Brokering Service, Stream Repository Service and Stream Filtering Service to demonstrate the flexibility of such a streaming data framework within SOA environment.
[Multiplexing, Protocols, stream demultiplexer service, stream brokering service, stream filtering service, Quality of service, Mobile communication, Service Oriented Architecture (SOA), Service Level Agreement (SLA), architectural pattern, Streaming Architecture, software architecture, mobile computing, Bandwidth, media streaming, stream multiplexor service, modular business services, Service oriented architecture, mobile data streaming, stream repository service, Encoding, Mobile Data Streaming, custom services, service level agreement, Services, Web services, stream discovery service, service registry, service oriented architecture, business data processing]
Practical Aspects in Analyzing and Sharing the Results of Experimental Evaluation
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Dependability evaluation techniques such as the ones based on testing, or on the analysis of field data on computer faults, are a fundamental process in assessing complex and critical systems. Recently a new approach has been proposed consisting in collecting the row data produced in the experimental evaluation and store it in a multidimensional data structure. This paper reports the work in progress activities of the entire process of collecting, storing and analyzing the experimental data in order to perform a sound experimental evaluation. This is done through describing the various steps on a running example.
[data warehouse, dependability evaluation techniques, Instruments, testing, Data warehouses, performance evaluation, multidimensional data structure, computer faults, Synchronization, row data, experimental evaluation, star schema, Loading, Warehousing, olap, Software, data structures, fault tolerant computing, Monitoring]
A Study on Latent Vulnerabilities
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Software code reuse has long been touted as a reliable and efficient software development paradigm. Whilst this practice has numerous benefits, it is inherently susceptible to latent vulnerabilities. Source code which is re-used without being patched for various reasons may result in vulnerable binaries, despite the vulnerabilities being made publicly known. To aggravate matters, crackers have access to information on these vulnerabilities as well. Defenders need to ensure all loopholes are patched, while attackers need just one such loophole. In this work, we define latent vulnerabilities, and study the prevalence of the problem. This provides us the motivation, and an insight into the future work to be done in solving the problem. Our results show that unpatched source files which are more than one year old are commonly used in the latest operating systems. In fact, several of these files are more than ten years old. We explore the premises of using symbols in identifying binaries and conclude that they are insufficient in solving the problem. Additionally, we discuss two possible approaches to solve the problem.
[software protection, source coding, software reliability, software safety, operating systems, Security, software libraries, computer security, latent vulnerabilities, Software packages, Databases, security of data, Linux, software code reuse, Computer bugs, software development paradigm, software reusability, operating systems (computers), Libraries, vulnerable binaries]
CReW: Cloud Resilience for Windows Guests through Monitored Virtualization
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Clouds are complex systems subject to an increasing number of anomalies and threats. In this paper we briefly revisit the issues related to Windows guest cloud service resilience and later provide some preliminary results on the resilience of Windows cloud guests via virtualization. In particular, we propose an architecture, Cloud Resilience for Windows (CReW). CReW can transparently monitor guest Windows VMs and can also react to both security breaches and system integrity violation, improving the dependability of cloudified Windows systems. CReW can also improve resilience from software misconfiguration by restoring the guest latest safe state. Effectiveness and performance of a CReW prototype have been evaluated, obtained results show the feasibility of such a system.
[file and storage systems, Clouds, Resilience management and security issues in clouds, Windows guest cloud service resilience, Virtual machining, Resilience measurement studies, Servers, Security, Resilience, guest Windows VM, Secure and intrusion tolerant systems, security of data, monitored virtualization, Operating systems, virtual machines, software misconfiguration, Internet, Kernel, Monitoring, CReW]
Quantifying Resiliency of IaaS Cloud
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Cloud based services may experience changes - internal, external, large, small - at any time. Predicting and quantifying the effects on the quality-of-service during and after a change are important in the resiliency assessment of a cloud based service. In this paper, we quantify the resiliency of infrastructure-as-a-service (IaaS) cloud when subject to changes in demand and available capacity. Using a stochastic reward net based model for provisioning and servicing requests in a IaaS cloud, we quantify the resiliency of IaaS cloud w.r.t. two key performance measures - job rejection rate and provisioning response delay.
[Clouds, Computational modeling, IaaS cloud, Steady-state, Delay, stochastic reward net based model, resiliency quantification, Analytical models, software architecture, Markov processes, Internet, stochastic processes, cloud based services]
Benchmarking the Resilience of Self-Adaptive Systems: A New Research Challenge
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Self-adaptive systems are widely recognized as the future of computer systems. Due to their dynamic and evolving nature, the characterization of self-adaptation and resilience attributes is of upmost importance. The problem is that nowadays there is no practical way to characterize self-adaptation capabilities or to compare alternative solutions concerning resilience. In this paper we discuss the problem of resilience benchmarking of self-adaptive systems. We start by identifying a set of key challenges and then propose a research roadmap to tackle those challenges.
[Measurement, Computers, program testing, benchmarking, self-adaptive, Security, adaptive systems, Resilience, computer systems, Benchmark testing, Autonomic systems, self-adaptive systems, resilience attribute, metrics, benchmark testing, resilience, software performance evaluation]
Data-Mining-Based Link Failure Detection for Wireless Mesh Networks
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Mobile robot applications operating in wireless environments require fast detection of link failures in order to enable fast repair. In previous work, we have shown that cross-layer failure detection can reduce failure detection latency significantly. In particular, we monitor the behavior of the WLAN MAC layer to predict failures on the link layer. In this paper, we investigate data mining techniques to determine which parameters, i.e., the events, or combination and timing of events, occurring on the MAC layer most probably lead to link failures. Our results show, that the parameters revealed with the data mining approach produce similar or even more accurate failure predictions than achieved so far.
[data mining, reliability, Mobile communication, Ad hoc networks, mobile robots, cross-layer, Data mining, telecommunication computing, wireless mesh networks, Wireless communication, cross layer failure detection, Training data, Data models, Mobile robot applications, link failure detection, Transient analysis]
Data Validity and Dependable Perception in Networked Sensor-Based Systems
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Although the technology and applications of wireless sensor networks have greatly increased over the last years, ensuring a dependable real-time operation despite faults and temporal uncertainties is still an on-going research topic. The problems are particularly significant when considering that future applications will interact with their environment not only for supervision or monitoring, but also to directly control physical (real-time) entities, sometimes with safety-critical requirements. We believe that reasoning in terms of data validity might be a good way to approach the problem. The ability to know if sensor data flowing in the system is valid - data validity awareness -, is a first step to achieve a dependable operation. But more than that, it should be possible to ensure, given requirements for data validity throughout the operation, a dependable perception of the environment. In this paper we essentially discuss the problem, analyzing some of the issues that need to be addressed to achieve these goals. Particularly, we introduce fundamental concepts and relevant definitions, we elaborate on the main impediments to achieve data validity awareness and describe relevant means to deal with these impediments. Finally, we address the issue of ensuring a dependable perception and present some research ideas in this direction.
[Real time systems, Data validity, Protocols, wireless sensor networks, sensor data validity, Dependability, Quality of service, network sensor-based systems, Temporal consistency, Middleware, sensor arrays, Real-Time, Wireless sensor networks, Sensor networks, Robot sensing systems, Monitoring]
A Multi-step Simulation Approach toward Secure Fault Tolerant System Evaluation
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
As new techniques of fault tolerance and security emerge, so does the need for suitable tools to evaluate them. Generally, the security of a system can be estimated and verified via logical test cases, but the performance overhead of security algorithms on a system needs to be numerically analyzed. The diversity in security methods and design of fault tolerant systems make it impossible for researchers to come up with a standard, affordable and openly available simulation tool, evaluation framework or an experimental test-bed. Therefore, researchers choose from a wide range of available modeling-based, implementation-based or simulation-based approaches in order to evaluate their designs. All of these approaches have certain merits and several drawbacks. For instance, development of a system prototype provides a more accurate system analysis but unlike simulation, it is not highly scalable. This paper presents a multi-step, simulation-based performance evaluation methodology for secure fault tolerant systems. We use a divide-and-conquer approach to model the entire secure system in a way that allows the use of different analytical tools at different levels of granularity. This evaluation procedure tries to strike a balance between the efficiency, effort, cost and accuracy of a system's performance analysis. We demonstrate this approach in a step-by-step manner by analyzing the performance of a secure and fault tolerant system using a JAVA implementation in conjunction with the ARENA simulation.
[Java, modeling based approach, Architecture, Security, Modeling, Analytical models, Fault tolerance, Accuracy, Runtime, implementation based approach, Simulation, Fault tolerant systems, Prototypes, fault tolerant computing, security algorithm, multistep simulation approach, secure fault tolerant system evaluation, software performance evaluation, Fault Tolerance]
Protection of Identity Information in Cloud Computing without Trusted Third Party
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Cloud computing allows the use of Internet-based services to support business processes and rental of IT-services on a utility-like basis. It offers a concentration of resources but also poses risks for data privacy. A single breach can cause significant loss. The heterogeneity of &#x201C;users&#x201D; represents a danger of multiple, collaborative threats. In cloud computing, entities may have multiple accounts associated with a single or multiple service providers (SPs). Sharing sensitive identity information (that is, Personally Identifiable information or PII) along with associated attributes of the same entity across services can lead to mapping of the identities to the entity, tantamount to privacy loss. Identity management (IDM) is one of the core components in cloud privacy and security and can help alleviate some of the problems associated with cloud computing. Available solutions use trusted third party (TTP) in identifying entities to SPs. The solution providers do not recommend the usage of their solutions on untrusted hosts. We propose an approach for IDM, which is independent of TTP and has the ability to use identity data on untrusted hosts. The approach is based on the use of predicates over encrypted data and multi-party computing for negotiating a use of a cloud service. It uses active bundle-which is a middleware agent that includes PII data, privacy policies, a virtual machine that enforces the policies, and has a set of protection mechanisms to protect itself. An active bundle interacts on behalf of a user to authenticate to cloud services using user's privacy policies.
[Cloud computing, Data privacy, sensitive identity information sharing, Clouds, identity management system, privacy, identity information protection, multiparty computing, trusted third party, Privacy, security, multi-party computing, multiple service providers, Cryptography, cloud computing, middleware, IT services, cloud security, data encryption, cryptography, personally identifiable information, Internet based services, identity management, Authentication, computing predicates, data privacy, Internet, business processes support, active bundle]
[Publisher's information]
2010 29th IEEE Symposium on Reliable Distributed Systems
None
2010
Provides a listing of current committee members and society officers.
[]
Message from the General Chair
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Presents the welcome message from the conference proceedings.
[]
Message from the Technical Program Committee Co-Chairs
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Presents the welcome message from the conference proceedings.
[]
Conference Committees
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Provides a listing of current committee members.
[]
Technical Program Committee
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Provides a listing of current committee members.
[]
Exploring Latent Features for Memory-Based QoS Prediction in Cloud Computing
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
With the increasing popularity of cloud computing as a solution for building high-quality applications on distributed components, efficiently evaluating user-side quality of cloud components becomes an urgent and crucial research problem. However, invoking all the available cloud components from user-side for evaluation purpose is expensive and impractical. To address this critical challenge, we propose a neighborhood-based approach, called CloudPred, for collaborative and personalized quality prediction of cloud components. CloudPred is enhanced by feature modeling on both users and components. Our approach CloudPred requires no additional invocation of cloud components on behalf of the cloud application designers. The extensive experimental results show that CloudPred achieves higher QoS prediction accuracy than other competing methods. We also publicly release our large-scale QoS dataset for future related research in cloud computing.
[Cloud computing, Quality of service, latent features, Prediction, Vectors, quality of service, Sparse matrices, storage management, distributed components, Accuracy, Cloud Computing, QoS, Collaboration, neighborhood-based approach, personalized quality prediction, groupware, memory-based QoS prediction, cloud computing, collaborative quality prediction, Monitoring, CloudPred]
ELT: Efficient Log-based Troubleshooting System for Cloud Computing Infrastructures
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
We present an Efficient Log-based Troubleshooting(ELT) system for cloud computing infrastructures. ELT adopts a novel hybrid log mining approach that combines coarse-grained and fine-grained log features to achieve both high accuracy and low overhead. Moreover, ELT can automatically extract key log messages and perform invariant checking to greatly simplify the troubleshooting task for the system administrator. We have implemented a prototype of the ELT system and conducted an extensive experimental study using real management console logs of a production cloud system and a Hadoop cluster. Our experimental results show that ELT can achieve more efficient and powerful troubleshooting support than existing schemes. More importantly, ELT can find software bugs that cannot be detected by current cloud system management practice.
[Algorithm design and analysis, Cloud computing, Production systems, program debugging, efficient log-based troubleshooting system, invariant checking, Hadoop cluster, software bugs, cloud system management practice, coarse-grained log features, ELT, cloud computing infrastructures, hybrid log mining approach, Runtime, Clustering algorithms, production cloud system, Feature extraction, system monitoring, fine-grained log features, cloud computing]
Transaction Models for Massively Multiplayer Online Games
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Massively Multiplayer Online Games are considered large distributed systems where the game state is partially replicated across the server and thousands of clients. Given the scale, game engines typically offer only relaxed consistency without well-defined guarantees. In this paper, we leverage the concept of transactions to define consistency models that are suitable for gaming environments. We define game specific levels of consistency that differ in the degree of isolation and atomicity they provide, and demonstrate the costs associated with their execution. Each action type within a game can then be assigned the appropriate consistency level, choosing the right trade-off between consistency and performance. The issue of durability and fault-tolerance of game actions is also discussed.
[transaction processing, Protocols, Avatars, fault-tolerance, transaction models, Servers, transactions, massively multiplayer online games, consistency, game engines, gaming environments, Semantics, computer games, Games, online games, distributed systems, persistence, Software, Reliability]
Balancing the Communication Load of State Transfer in Replicated Systems
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
State transfer mechanisms are an essential building block in the design of many distribution applications that replicate the state, such as partially replicated databases or view-synchronous group communication. When a reconfiguration occurs, a need arises to ship a subset of objects that constitute the application state to a subset of nodes in the system. The most commonly employed solution is to elect a leader that collects state objects and transmits them to the nodes that need to receive them. In this paper, we present the problem of communication-balanced state transfer wherein the goal is to distribute the load of communication due to state transfer evenly across the participating nodes. We propose an algorithm that achieves the optimal balance, analyze it, and describe how it can be used in a variety of applications. We evaluate the algorithm on a typical setup of partially replicated databases and show that it attains a significant improvement compared with existing approaches.
[Context, distribution applications, Protocols, replicated systems, replicated databases, load balancing, distributed processing, communication balanced state transfer, state transfer, state transfer mechanisms, Optimization, partially replicated databases, Unicast, Databases, resource allocation, distributed systems, view synchronous group communication, Resource management, Matrix converters, communication load balancing]
Automated Discovery of Credit Card Data Flow for PCI DSS Compliance
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Credit cards are key instruments in personal financial transactions. Credit card payment systems used in these transactions and operated by merchants are often targeted by hackers to steal the card data. To address this threat, the payment card industry establishes a mandatory security compliance standard for businesses that process credit cards. A central pre-requisite for this compliance procedure is to identify the credit card data flow, specifically, the stages of the card transaction processing and the server nodes that touch credit card data as they travel through the organization. In practice, this pre-requisite poses a challenge to merchants. As the payment infrastructure is implemented and later maintained, it often deviates from the original documented design. Without consistent tracking and auditing of changes, such deviations in many cases remain undocumented. Therefore building the credit card data flow for a given payment card processing infrastructure is considered a daunting task that at this point requires significant manual efforts. This paper describes a tool that is designed to automate the task of identifying the credit card data flow in commercial payment systems running on virtualized servers hosted in private cloud environments. This tool leverages virtual machine introspection technology to keep track of credit card data flows across multiple machines in real time without requiring intrusive instrumentation of the hyper visor, virtual machines, middleware or application source code. Effectiveness of this tool is demonstrated through its successful discovery of the credit card data flow of several open and closed source payment applications.
[Decision support systems, security compliance, payment card industry, Security, PCI DSS compliance, credit card process, private cloud environments, payment infrastructure, financial data processing, virtual machine introspection technology, payment system, Kernel, middleware, source code application, virtualized servers, Credit cards, credit card data flow automated discovery, Virtual machining, personal financial transactions, Virtual machine monitors, security of data, virtual machine, Sockets, compliance, virtual machines, intrusive instrumentation, card data flow, private cloud, card transaction processing]
OSARE: Opportunistic Speculation in Actively REplicated Transactional Systems
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
In this work we present OSARE, an active replication protocol for transactional systems that combines the usage of Optimistic Atomic Broadcast with a speculative concurrency control mechanism in order to overlap transaction processing and replica synchronization. OSARE biases the speculative serialization of transactions towards an order aligned with the optimistic message delivery order. However, due to the lock-free nature of its concurrency control algorithm, at high concurrency levels, namely when the probability of mismatches between optimistic and final deliveries is higher, OSARE explores additional alternative transaction serialization orders in a lightweight and opportunistic fashion. A simulation study we carried out in the context of Software Transactional Memory systems shows that OSARE achieves robust performance also in scenarios characterized by non-minimal likelihood of reorder between optimistic and final deliveries, providing remarkable speed-up with respect to state of the art speculative replication protocols.
[transaction processing, speculation, Protocols, Optimized production technology, optimistic message delivery order, active replication, Concurrency control, software transactional memories, History, Proposals, transaction processing systems, OSARE, optimistic atomic broadcast, actively replicated transactional systems, replica synchronization, concurrency control, concurrency control mechanism, Benchmark testing, software transactional memory systems]
Modeling Medium Utilization for Admission Control in Industrial Wireless Mesh Networks
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Wireless Mesh Networks (WMNs) are a promising technology for industrial environment communication because of their high flexibility and low cost. To fulfill the thereby arising reliability demands, admission control can be used to prevent congestion and to provide end-to-end guarantees to applications. However, this requires a very precise modeling of the medium utilization for existing and requested data flows, which is challenging due to the dynamics of such networks. In this paper we propose a medium utilization model for small-scale multi-rate WMNs considering the end-to-end throughput. The model is integrated into a feedback control loop to allow an admission control mechanism to ensure reliable end-to-end communication. Our approach increases the available performance without sacrificing reliability. A real test-bed evaluation shows that our admission control manager allows to fully utilize the network capacity while keeping packet losses under 0.5% and three-hop latency below 5.5ms. This is a significant improvement towards reliable multi-rate WMNs, leveraging their applicability in industrial applications with high throughput demands.
[industrial environment communication, feedback control loop, congestion prevent, telecommunication congestion control, medium utilization model, business communication, Throughput, admission control, wireless mesh networks, Wireless communication, feedback, admission control mechanism, real-time, Admission control, Bit rate, network capacity, industrial wireless mesh networks, end-to-end throughput, Propagation losses, Reliability, small scale multirate WMN, reliable end-to-end communication]
Modeling the Fault Tolerance Consequences of Deduplication
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Modern storage systems are employing data deduplication with increasing frequency. Often the storage systems on which these techniques are deployed contain important data, and utilize fault-tolerant hardware and software to improve the reliability of the system and reduce data loss. We suggest that data deduplication introduces inter-file relationships that may have a negative impact on the fault tolerance of such systems by creating dependencies that can increase the severity of data loss events. We present a framework composed of data analysis methods and a model of data deduplication that is useful in studying the reliability impact of data deduplication. The framework is useful for determining a deduplication strategy that is estimated to satisfy a set of reliability constraints supplied by a user.
[Target tracking, data deduplication, fault tolerance, software reliability, reliability, system reliability, reliability constraints, deduplication, fault tolerant hardware, Fault tolerance, Analytical models, storage management, Fault tolerant systems, fault tolerance consequences modeling, UDE, storage systems, Hardware, Data models, fault tolerant computing, data handling, constraint handling, fault tolerant software]
Resilience-Driven Parameterisation of Ad Hoc Routing Protocols: olsrd as a Case Study
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Ad hoc routing protocols are threatened by a variety of accidental and malicious faults that limit their use. Although a number of well-known strategies exist to enhance the performance and resilience of such type of protocols, their final effectiveness strongly relies on the usage of appropriate protocol configuration parameters. This paper investigates how to parameterise ad hoc routing protocols to combine high performance, with acceptable levels of resilience and low consumption of resources. The research places the spotlight on olsrd, an ad hoc proactive routing protocol able to run on real devices and deploy challenge-response authentication, packet signature and fault tolerance strategies at runtime. The reported practical experience is carried out in different ad hoc networking contexts integrating different types of devices, thus checking the influence that mobility of nodes and device resource constraints have on the parameterisation of the different protocol features considered.
[telecommunication security, Portable computers, appropriate protocol configuration parameters, olsrd, Ad hoc networks, Security and fault tolerance, Resilience, Memory management, ad hoc routing protocols, routing protocols, fault tolerance strategies, resilience-driven parameterisation, Routing protocols, fault tolerant computing, challenge-response authentication, ad hoc networks, Mobile computing, packet signature]
DONUT: Building Shortcuts in Large-Scale Decentralized Systems with Heterogeneous Peer Distributions
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Large-scale distributed systems gather thousands of peers spread all over the world. Such systems need to offer good routing performances regardless of their size and despite high churn rates. To achieve that requirement, the system must add appropriate shortcuts to its logical graph (overlay). However, to choose efficient shortcuts, peers need to obtain information about the overlay topology. In case of heterogeneous peer distributions, retrieving such information is not straightforward. Moreover, due to churn, the topology rapidly evolves, making gathered information obsolete. State of- the-art systems either avoid the problem by enforcing peers to adopt a uniform distribution or only partially fulfill these requirements. To cope with this problem, we propose DONUT, a mechanism to build a local map that approximates the peer distribution, allowing the peer to accurately estimate graph distance to other peers with a local algorithm. The evaluation performed with real latency and churn traces shows that our map increases the routing process efficiency by at least 20% compared to the state-of-the-art techniques. It points out that each map is lightweight and can be efficiently propagated through the network by consuming less than 10 bps on each peer.
[DONUT, logical graph, peer-to-peer computing, Peer to peer computing, graph theory, heterogeneous peer distributions, large scale distributed systems, Small-World graphs, distributed processing, churn, range queries, Routing, Topology, Partitioning algorithms, Approximation methods, long-range links, routing, routing process efficiency, Semantics, overlays, large scale decentralized systems, Approximation algorithms, heterogeneous keyspaces, overlay topology]
Partition-Tolerant Distributed Publish/Subscribe Systems
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
In this paper, we develop reliable distributed publish/subscribe algorithms that can tolerate concurrent failure of up to d broker machines or communication links. In our approach, d is a configuration parameter which determines the level of fault-tolerance of the system and reliability refers to exactly-once and per-source, in-order delivery of publications to clients with matching subscriptions. We propose protocols to address three problems in presence of broker or link failures: (i) subscription propagation, (ii) publication forwarding, and (iii) broker recovery. Finally, we study the effectiveness of our approach when the number of concurrent failures exceeds d. Through large-scale experimental evaluations with up to 500 brokers, we demonstrate that a system configured with a modest value of d = 3 is able to reliably deliver 97% of publications in presence of failure of up to 17% of its brokers.
[message passing, partition tolerant distributed subscribe systems, Subscriptions, Routing, Fault-Tolerance, Computer crashes, Publish/Subscribe, Servers, subscription propagation, Videos, publication forwarding, broker recovery, broker machines, Detectors, Reliability, configuration parameter, partition tolerant distributed publish systems, middleware]
Exploiting Node Connection Regularity for DHT Replication
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Distributed Hash-Tables (DHTs) provide an efficient way to store objects in large-scale peer-to-peer systems. To guarantee that objects are reliably stored, DHTs rely on replication. Several replication strategies have been proposed in the last years. The most efficient ones use predictions about the availability of nodes to reduce the number of object migrations that need to be performed: objects are preferably stored on highly available nodes. This paper proposes an alternative replication strategy. Rather than exploiting highly available nodes, we propose to leverage nodes that exhibit regularity in their connection pattern. Roughly speaking, the strategy consists in replicating each object on a set of nodes that is built in such a way that, with high probability, at any time, there are always at least k nodes in the set that are available. We evaluate this replication strategy using traces of two real-world systems: eDonkey and Skype. The evaluation shows that our regularity-based replication strategy induces a systematically lower network usage than existing state of the art replication strategies.
[Availability, Protocols, Limiting, peer-to-peer computing, Peer to peer computing, Distributed Hash Tables (DHTs), object replication, Educational institutions, cryptography, large-scale peer-to-peer systems, node connection regularity, connection patterns, eDonkey, Skype, regularity-based replication strategy, Bandwidth, distributed hash-tables, file organisation, node availability, DHT replication]
An Approach Based on Swarm Intelligence for Event Dissemination in Dynamic Networks
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Dynamic networks require adaptive strategies for information dissemination, as the topology constantly changes. This work presents an event-based bio-inspired dissemination approach that employs ants, which correspond to mobile agents, to spread information throughout the network. An event is defined as a state transition of a node or link. A node which detects an event in its neighborhood triggers the dissemination. Pheromones are used to both control the ant population and help to define the paths that the agents take. An empirical study was performed, in which the proposed strategy was compared with flooding and gossip algorithms. Results show that the proposed strategy presents a good trade-off between the time required to disseminate information and the overhead in terms of the number of messages employed.
[Measurement, Peer to peer computing, Heuristic algorithms, information dissemination, flooding algorithms, topology, ant population, gossip algorithms, Topology, bio-inspired information dissemination, Floods, Particle swarm optimization, event based bio inspired dissemination approach, optimisation, Network topology, dynamic networks, pheromones, event dissemination, swarm intelligence]
Identifying Compromised Users in Shared Computing Infrastructures: A Data-Driven Bayesian Network Approach
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
The growing demand for processing and storage capabilities has led to the deployment of high-performance computing infrastructures. Users log into the computing infrastructure remotely, by providing their credentials (e.g., username and password), through the public network and using well-established authentication protocols, e.g., SSH. However, user credentials can be stolen and an attacker (using a stolen credential) can masquerade as the legitimate user and penetrate the system as an insider. This paper deals with security incidents initiated by using stolen credentials and occurred during the last three years at the National Center for Supercomputing Applications (NCSA) at the University of Illinois. We analyze the key characteristics of the security data produced by the monitoring tools during the incidents and use a Bayesian network approach to correlate (i) data provided by different security tools (e.g., IDS and Net Flows) and (ii) information related to the users' profiles to identify compromised users, i.e., the users whose credentials have been stolen. The technique is validated with the real incident data. The experimental results demonstrate that the proposed approach is effective in detecting compromised users, while allows eliminating around 80% of false positives (i.e., not compromised user being declared compromised).
[Protocols, NCSA, Vectors, intrusion detection, storage capabilities, authentication protocols, stolen credentials, security, security of data, Bayesian methods, correlation, data driven Bayesian network approach, Authentication, national center for supercomputing applications, IP networks, belief networks, security incidents, shared computing infrastructures, credential stealing, Bayesian network, Monitoring]
DiveInto: Supporting Diversity in Intrusion-Tolerant Systems
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Intrusion tolerant services are usually implemented as replicated systems. If replicas execute identical software, then they share the same vulnerabilities and the whole system can be easily compromised if a single flaw is found. One solution to this problem is to introduce diversity by using different server implementations, but this increases the chances of incompatibility between replicas. This paper studies various kinds incompatibilities and presents a new methodology to evaluate the compliance of diverse server replicas. The methodology collects network traces to identify syntax and semantic violations, and to assist in their resolution. A tool called DiveInto was developed based on the methodology and was applied to three replication scenarios. The experiments demonstrate that DiveInto is capable of discovering various sorts of violations, including problems related with nondeterministic execution.
[Protocols, Correlation, network servers, intrusion tolerance, semantic violation identification, nondeterministic execution, Generators, Servers, DiveInto tool, replicated system, service replication, syntax violation identification, diversity, security of data, diverse server replica, intrusion-tolerant system, network trace, Semantics, Syntactics, software tools, intrusion tolerant service, Testing]
Process Implanting: A New Active Introspection Framework for Virtualization
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Previous research on virtual machine introspection proposed "out-of-box" approach by moving out security tools from the guest operating system. However, compared to the traditional "in-the-box" approach, it remains a challenge to obtain a complete semantic view due to the semantic gap between the guest VM and the hyper visor. In this paper, we present Process Implanting, a new active VM introspection framework, to narrow the semantic gap by implanting a process from the host into the guest VM and executing it under the cover of an existing running process. With the protection and coordination from the hyper visor, the implanted process can run with a degree of stealthiest and exit gracefully without leaving negative impact on the guest operating system. We have designed and implemented a proof-of-concept prototype on KVM which leverages hardware virtualization. We also propose and demonstrate application scenarios for Process Implanting in the area of VM security.
[Context, KVM, Instruction sets, virtualization, security tools, Switches, virtualisation, virtual machine introspection, Security, guest operating system, process implanting, Active VM introspection, Virtual machine monitors, security of data, active introspection framework, virtual machines, hypervisor, operating systems (computers), Malware, Kernel, Virtualization]
Dangers and Joys of Stock Trading on the Web: Failure Characterization of a Three-Tier Web Service
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Characterizing latent software faults is crucial to address dependability issues of current three-tier systems. A client should not have a misconception that a transaction succeeded, when in reality, it failed due to a silent error. We present a fault injection-based evaluation to characterize silent and non-silent software failures in a representative three-tier web service, one that mimics a day trading application widely used for benchmarking application servers. For failure characterization, we quantify distribution of silent and non-silent failures, and recommend low cost application-generic and application-specific consistency checks, which improve the reliability of the application. We inject three variants of null-call, where a callee returns null to the caller without executing business logic. Additionally, we inject three types of unchecked exceptions and analyze the reaction of our application. Our results show that 49% of error injections from null-calls result in silent failures, while 34% of unchecked exceptions result in silent failures. Our generic-consistency check can detect silent failures in null-calls with an accuracy as high as 100%. Non-silent failures with unchecked exceptions can be detected with an accuracy of 42% with our application-specific checks.
[latent software faults, detection, failure characterization, Containers, multi-tier application, Browsers, Servers, commerce, software fault tolerance, Databases, Web services, stock trading, generic-consistency check, application-specific checks, three-tier Web service]
netCSI: A Generic Fault Diagnosis Algorithm for Large-Scale Failures in Computer Networks
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
In this paper we present a framework and a set of algorithms for determining faults in networks when large scale outages occur. The design principles of our algorithm, netCSI, are motivated by the fact that failures are geographically clustered in such cases. We address the challenge of determining faults with incomplete symptom information due to a limited number of reporting nodes in the network. netCSI consists of two parts: hypotheses generation algorithm, and ranking algorithm. When constructing the hypotheses list of potential causes, we make novel use of the positive and negative symptoms to improve the precision of the results. The ranking algorithm is based on conditional failure probability models that account for the geographic correlation of the network objects in clustered failures. We evaluate the performance of netCSI for networks with both random and realistic topologies. We compare the performance of netCSI with an existing fault diagnosis algorithm, MAX-COVERAGE, and achieve an average gain of 128% in accuracy for realistic topologies.
[Algorithm design and analysis, fault diagnosis, large scale failures, incomplete information, generic fault diagnosis algorithm, computer networks, netCSI, hypotheses ranking algorithm, Optimization, Equations, clustered failures, Fault diagnosis, hypotheses generation algorithm, Clustering algorithms, Mathematical model, Joints, large-scale failures]
Finding Almost-Invariants in Distributed Systems
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
It is notoriously hard to develop dependable distributed systems. This is partly due to the difficulties in foreseeing various corner cases and failure scenarios while implementing a system that will be deployed over an asynchronous network. In contrast, reasoning about the desired distributed system behavior and the corresponding invariants is easier than reasoning about the code itself. Further, the invariants can be used for testing, theorem proving, and runtime enforcement. In this paper, we propose an approach to observe the system behavior and automatically infer invariants which reveal implementation bugs. Using our tool, Avenger, we automatically generate a large number of potentially relevant properties, check them within the time and spatial domains using traces of system executions, and filter out all but a few properties before reporting them to the developer. Our key insight in filtering is that a good candidate for an invariant is the one that holds in all but a few cases, i.e., an "almost-invariant". Our experimental results with the XORP BGP implementation demonstrate Avenger's ability to identify the almost-invariants that lead the developer to programming errors.
[implementation bugs, program debugging, Avenger, reasoning, Containers, Inspection, distributed processing, Routing, Data structures, corner cases, Cognition, Complexity theory, Noise measurement, inference mechanisms, XORP BGP implementation, asynchronous network, distributed systems, programming errors, theorem proving, almost invariants, failure scenarios]
An Architecture for Reliable Encapsulation Endpoints Using Commodity Hardware
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Custom hardware is expensive and making software reliable is difficult to achieve as complexity increases. Recent trends towards cloud computing highlight the importance of operating continuously using both unreliable commodity hardware and, as services grow in complexity, failure-vulnerable software. We have developed an approach for building dependable networking software that exposes a reliable encapsulation service to clients although it executes on commodity hardware, we do so without substantially increasing the implementation complexity of the encapsulation software. Our approach demonstrates the viability of building reliable systems using unreliable components, including unreliable server software.
[Encapsulation, encapsulation, Protocols, software reliability, IP continuity, Routing, reliable encapsulation service, Servers, reliable encapsulation endpoints, software reliable, failure-vulnerable software, dependable networking software, commodity hardware, transparent failover, Software, Hardware, IP networks, cloud computing, data encapsulation, software routing, computational complexity]
Analyzing Performance of Lease-Based Schemes under Failures
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Leases have proved to be an effective concurrency control technique for distributed systems that are prone to failures. However, many benefits of leases are only realized when leases are granted for approximately the time of expected use. Correct assessment of lease duration has proven difficult for all but the simplest of resource allocation problems. In this paper, we present a model that captures a number of lease styles and semantics used in practice. We consider a few performance characteristics for lease-based systems and analytically derive how they are affected by lease duration. We confirm our analytical findings by running a set of experiments with the OO7 benchmark suite using a variety of workloads and fault loads.
[Measurement, failures, software leases, software reliability, 007 benchmark suite, distributed processing, lease based schemes, Servers, resource allocation problems, Analytical models, Databases, resource allocation, concurrency control technique, concurrency control, Benchmark testing, distributed systems, Resource management, Mathematical model, performance analysis]
A Characterization of Node Uptime Distributions in the PlanetLab Test Bed
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
In this paper, we study nodes from the PlanetLab test bed to form a model of their uptime behavior. By applying clustering techniques to over a year's worth of availability data for the nodes, we identify six uptime distributions, each exhibiting unique characteristics shared by the nodes within it. The behavioral patterns exhibited by these groups, combined with the behaviors exhibited by the aggregate across the system, provide useful information for researchers designing applications that are run or tested on PlanetLab.
[Availability, availability data, program testing, clustering techniques, modeling, Peer to peer computing, Redundancy, distributed processing, distributed system, Extraterrestrial measurements, node uptime distributions, uptime behavior, availability, classification, Tuning, Aggregates, pattern clustering, PlanetLab test bed, Weibull distribution]
Candy: Component-based Availability Modeling Framework for Cloud Service Management Using SysML
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
High-availability assurance of cloud service is a critical and challenging issue for cloud service providers. To quantify the availability of cloud services from both architectural and operational points of views, availability modeling and evaluation are essential. This paper presents a component-based availability modeling framework, named Candy, which constructs a comprehensive availability model semi-automatically from system specifications described by Systems Modeling Language (SysML). SysML diagrams are translated into components of availability model and the components are assembled together to form the entire availability model in Stochastic Reward Nets (SRNs). In order to incorporate the maintenance operations of cloud services in availability models, Candy defines the translation rules from Activity diagram to SRN and synchronizes the related SRNs according to SysML allocation notations. The feasibility of the proposed modeling and availability evaluation process is studied by an illustrative example of a web application service hosted on a cloud infrastructure having multiple failure isolation zones and automatic scale-up function.
[Availability, Cloud computing, stochastic reward nets, object-oriented programming, cloud service management, automatic scale-up, Unified modeling language, candy, SysML, Maintenance engineering, Web servers, systems modeling language (SysML), SRN, Web services, stochastic reward nets (SRNs), cloud service, component-based availability modeling framework, systems modeling language, availability assessment, Web application service, simulation languages, Resource management, cloud computing]
CloudInsight: Shedding Light on the Cloud
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Cloud computing provides a revolutionary new computing paradigm for deploying enterprise applications and Internet services. Rather than operating their own data centers, today cloud users run their applications on the remote cloud infrastructures that are owned and managed by cloud providers. However, the cloud computing paradigm also introduces some new challenges in system management. Cloud users create virtual machine instances to run their specific application logic without knowing the underlying physical infrastructure. On the other side, cloud providers manage and operate their cloud infrastructures without knowing their customers' applications. Due to the decoupled ownership of applications and infrastructures, if a problem occurs, there is no visibility for either cloud users or providers to understand the whole context of the incident and solve it quickly. To this end, we propose a software solution, Cloud Insight, to provide some visibility through the middle virtualization layer for both cloud users and providers to address their problems quickly. Cloud Insight automatically tracks each VM instance's configuration status and maintains their life-cycle configuration records in a configuration management database (CMDB). When a user reports a problem, our algorithms automatically analyze CMDB to probabilistically determine the root cause and invoke a recovery process by interacting with the cloud user. Experimental results over data from Amazon EC2 online support forum and NEC Labs' research cloud infrastructures demonstrate that our approach can effectively automate the problem troubleshooting process in cloud environments.
[Internet services, Cloud computing, problem troubleshooting process, virtualisation, History, Engines, Cloudlnsight, remote cloud infrastructures, Databases, cloud computing, NEC Labs research cloud infrastructures, Monitoring, virtual machine instances, data analysis, system management, configuration management database, enterprise applications, configuration management, application logic, Sensitivity, Virtual machine monitors, life cycle configuration, virtual machines, VM, troubleshooting, middle virtualization layer, Amazon EC2 online support forum]
A Scalable Cloud-based Queuing Service with Improved Consistency Levels
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Queuing is commonly used to connect loosely coupled components to form large-scale, highly-distributed, and fault-tolerant applications. As cloud computing continues to gain popularity, a number of vendors have started offering cloud-hosted, multi-tenant queuing service. They provide high availability at the cost of reduced consistency. Although they offer at-least-once delivery guarantee, that is, no message loss, they do not make any effort in maintaining FIFO order, which is an important aspect of the queuing semantics. Thus they are not adequate for some applications. This paper presents the design and implementation of a scalable cloud-based queuing service, called Blue Dove Queuing Service (BDQS). It provides improved queuing consistency - at-least-once and best-effort in-order message delivery - while preserving high availability and reliability. It also offers clients a flexible trade-off between duplication and message order. Comprehensive evaluation is carried out on an Infrastructure-as-a-Service cloud computing platform with up to 70 server nodes and 1000 queues. It shows that BDQS achieves excellent performance scalability. Meanwhile, it offers an order-of-magnitude improvement in out-of-order measurement compared to existing no-order systems. Results also indicate that BDQS is highly reliable and available.
[Out of order, Availability, best effort in order message delivery, queueing theory, order of magnitude improvement, large scale highly distributed fault tolerant application, scalable cloud based queuing service, Receivers, Generators, Indexes, cloud hosted multitenant queuing service, performance scalability, at least once message delivery, infrastructure as a service cloud computing, fault tolerant computing, BlueDove queuing service, queuing consistency, BDQS, cloud computing, Message systems, out of order measurement]
On the Reduction of Atomic Broadcast to Consensus with Byzantine Faults
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
We investigate the reduction of atomic broadcast to consensus in systems with Byzantine faults. Among the several definitions of Byzantine consensus that differ only by their validity property, we identify those equivalent to atomic broadcast. Finally, we give the first deterministic atomic broadcast reduction with a constant time complexity with respect to consensus.
[Context, message passing, time complexity, deterministic atomic broadcast reduction, Vectors, Byzantine faults, Complexity theory, Atomic broadcast, Indexes, Delay, Byzantine consensus, consensus, Reduction, Semantics, atomic broadcast reduction, fault tolerant computing, Reliability, protocols, computational complexity, Byzantine fault]
Scheduling of Dynamic Participants in Real-Time Distributed Systems
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Access to shared resources can be controlled by schedules or mutual exclusion. Such methods are not practical in an environment with dynamic participants, where nodes requiring access to shared resources can enter or leave the scene at any time. Current scheduling methods are usually centralized, demand that the system has a clear idea of when the resources are required and do not consider communication failures. Current implementations of distributed mutual exclusion use token- or permission-based methods. Dynamic participation amplifies the lost token problem in token-based approaches, while limited knowledge of the number of nodes makes obtaining quora and consensus in permission-based approaches impossible, rendering both mutual exclusion implementations impractical. This paper presents the CwoRIS protocol which enables short-term scheduling in real-time within an environment with dynamic participants. It motivates the need to support dynamic participants by means of a scenario for autonomous vehicle coordination in intersection crossing. The paper shows that the protocol is able to work in an environment with message loss and argues its correctness by showing mutual exclusion: there are no cases in which two nodes have access to the same resources at the same time.
[Real time systems, Schedules, Protocols, distributed mutual exclusion, distributed processing, token-based method, CwoRIS protocol, Vehicle dynamics, dynamic participants, Vehicles, autonomous vehicle coordination, permission-based methods, real-time systems, scheduling, real-time distributed systems, Junctions, Contracts]
Fast Genuine Generalized Consensus
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Consensus (agreeing on a sequence of commands) is central to the operation and performance of distributed systems. A well-known solution to consensus is Fast Paxos. In a recent paper, Lamport enhances Fast Paxos by lever aging the commutativity of concurrent commands. The new primitive, called Generalized Paxos, reduces the collision rate, and thus the latency of Fast Paxos. However if a collision occurs, Generalized Paxos needs four communication steps to recover, which is slower than Fast Paxos. This paper presents FGGC, a novel consensus algorithm that reduces recovery delay when a collision occurs to one. FGGC tolerates f &lt;; n/2 replicas crashes, and during failure-free runs, processes learn commands in two steps if all commands commute, and three steps otherwise; this is optimal. Moreover, as long as no fault occurs, FGGC needs only f + 1 replicas to progress.
[Wide area networks, algorithms, Generalized Paxos, fault-tolerance, reliability, distributed processing, replicas crashes, Computer crashes, Fast Paxos, system recovery, Lamport, Optimization, Delay, software fault tolerance, consensus, Fault tolerance, Upper bound, recovery delay, distributed systems, failure free runs, Local area networks, fast genuine generalized consensus]
A Theory of Fault Recovery for Component-Based Models
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
This paper introduces a theory of fault recovery for component-based models. In our framework, a model is specified in terms of a set of atomic components that are incrementally composed and synchronized by a set of glue operators. We define what it means for such models to provide a recovery mechanism, so that the model converges to its normal behavior in the presence of faults. We identify corrector (atomic or composite) components whose presence in a model is essential to guarantee recovery after the occurrence of faults. We also formalize component-based models that effectively separate recovery from functional concerns.
[recovery, component-based, formal specification, system recovery, Fault tolerance, glue operators, modularity, Fault tolerant systems, Semantics, Bismuth, Safety, object-oriented programming, Computational modeling, corrector component, composite component, BIP, fault recovery theory, synchronisation, corrector component identification, atomic component, fault tolerant computing, Fault-tolerance, separation of concerns, component based model formalization, Context modeling]
[Publishers information]
2011 IEEE 30th International Symposium on Reliable Distributed Systems
None
2011
Provides a listing of current committee members and society officers.
[]
Message from General Chairs
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Presents the welcome message from the conference proceedings.
[]
Message from Technical Program Co-chairs
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Presents the welcome message from the conference proceedings.
[]
Organizing Committee
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Provides a listing of current committee members.
[]
Technical Program Committee
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Provides a listing of current committee members.
[]
Detecting Anomalous User Behaviors in Workflow-Driven Web Applications
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Web applications are increasingly used as portals to interact with back-end database systems and support business processes. This type of data-centric workflow-driven web application is vulnerable to two types of security threats. The first is an request integrity attack, which stems from the vulnerabilities in the implementation of business logic within web applications. The second is guideline violation, which stems from privilege misuse in scenarios where business logic and policies are too complex to be accurately defined and enforced. Both threats can lead to sequences of web requests that deviate from typical user behaviors. The objective of this paper is to detect anomalous user behaviors based on the sequence of their requests within a web session. We first decompose web sessions into workflows based on their data objects. In doing so, the detection of anomalous sessions is reduced to detection of anomalous workflows. Next, we apply a hidden Markov model (HMM) to characterize workflows on a per-object basis. In this model, the implicit business logic involved in this object defines the unobserved states of the Markov process, where the web requests are observations. To derive more robust HMMs, we extend the object-specific approach to an object-cluster approach, where objects with similar workflows are clustered and HMM models are derived on a per-cluster basis. We evaluate our models using two real systems, including an open source web application and a large web-based electronic medical record system. The results show that our approach can detect anomalous web sessions and lend evidence to suggest that the clustering approach can achieve relatively low false positive rates while maintaining its detection accuracy.
[anomalous user behavior detection, Markov process, web security, public domain software, hidden Markov model, business processes, anomaly detection, sequence similarity, Web-based electronic medical record system, Security, Electronic medical records, Guidelines, Training, hidden Markov models, object-cluster approach, security threats, open source Web application, back-end database systems, Robustness, Hidden Markov Model, Business, Web requests, data-centric workflow-driven Web application, portals, business logic implementation, integrity attack, HMM models, medical information systems, Web session, security of data, Hidden Markov models, workflow-driven Web applications, workflow-driven web application, object-specific approach, clustering, Internet, Web sites]
Susceptibility Analysis of Structured P2P Systems to Localized Eclipse Attacks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Peer-to-Peer (P2P) protocols are susceptible to Localized Eclipse Attacks (LEA), i.e., attacks where a victim peer's environment is masked by malicious peers which are then able to instigate progressively insidious security attacks. To obtain effective placement of malicious peers, LEAs significantly benefit from overlay topology-awareness. Hence, we propose heuristics for Chord, Pastry and Kademlia to assess the protocols' LEA susceptibility based on their topology characteristics and overlay routing mechanisms. As a result, our method can be used for P2P protocol parameter tuning in order to substantially mitigate LEAs. We present evaluations highlighting LEA's impact on contemporary P2P protocols. Our proposed heuristics are abstract in nature, making them applicable plus customizable for many other structured P2P protocols. We validate our model's accuracy through a simulation case study.
[Protocols, malicious peers, Peer-to-Peer Security, structured P2P protocols, Overlay networks, Abstracts, peer-to-peer protocols, Mathematical model, Overlay Topology Analysis, Chord, security attacks, structured P2P systems, peer-to-peer computing, Peer to peer computing, topology-awareness, telecommunication network topology, overlay routing mechanisms, Routing, Pastry, Equations, computer network security, localized Eclipse attacks, Eclipse Attack, transport protocols, Kademlia, susceptibility analysis]
Benchmarking Dependability of MapReduce Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
MapReduce is a popular programming model for distributed data processing. Extensive research has been conducted on the reliability of MapReduce, ranging from adaptive and on-demand fault-tolerance to new fault-tolerance models. However, realistic benchmarks are still missing to analyze and compare the effectiveness of these proposals. To date, most MapReduce fault-tolerance solutions have been evaluated using micro benchmarks in an ad-hoc and overly simplified setting, which may not be representative of real-world applications. This paper presents MRBS, a comprehensive benchmark suite for evaluating the dependability of MapReduce systems. MRBS includes five benchmarks covering several application domains and a wide range of execution scenarios such as data-intensive vs. compute-intensive applications, or batch applications vs. online interactive applications. MRBS allows to inject various types of faults at different rates. It also considers different application workloads and data loads, and produces extensive reliability, availability and performance statistics. We illustrate the use of MRBS with Hadoop clusters running on Amazon EC2, and on a private cloud.
[Hadoop clusters, Cloud computing, data-intensive applications, ondemand fault-tolerance model, Amazon EC2, online interactive applications, benchmarking dependability, MapReduce, micro benchmarks, Fault tolerance, MRBS, programming model, MapReduce reliability, Fault tolerant systems, Benchmark testing, Benchmark, Motion pictures, cloud computing, compute-intensive applications, software performance evaluation, adaptive fault-tolerance model, Dependability, performance statistics, Hadoop, Computer crashes, software fault tolerance, application workloads, application data loads, batch applications, MapReduce systems, private cloud, distributed data processing]
Aggregating CVSS Base Scores for Semantics-Rich Network Security Metrics
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
A network security metric is desirable in evaluating the effectiveness of security solutions in distributed systems. Aggregating CVSS scores of individual vulnerabilities provides a practical approach to network security metric. However, existing approaches to aggregating CVSS scores usually cause useful semantics of individual scores to be lost in the aggregated result. In this paper, we address this issue through two novel approaches. First, instead of taking each base score as an input, our approach drills down to the underlying base metric level where dependency relationships have well-defined semantics. Second, our approach interprets and aggregates the base metrics from three different aspects in order to preserve corresponding semantics of the individual scores. Finally, we confirm the advantages of our approaches through simulation.
[Measurement, CVSS base score aggregation, semantics-rich network security metrics, common vulnerability scoring system, Vectors, dependency relationships, security solutions effectiveness evaluation, Equations, computer network security, individual score semantics, Semantics, Authentication, network vulnerabilities, Mathematical model]
First Step toward Cloud-Based Firewalling
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
With the explosive growth of network-based services and attacks, the complexity and cost of firewall deployment and management have been increasing rapidly. Yet, each private network, no matter big or small, has to deploy and manage its own firewall, which is the critical first line of defense. To reduce the complexity and cost in deploying and managing firewalls, businesses have started to outsource the firewall service to their Internet Service Providers (ISPs), such as AT&amp;T, which provide cloud-based firewal service. Such fire walling model saves businesses in managing, deploying, and upgrading firewalls. The current firewall service outsourcing model requires businesses fully trust their ISPs and give ISPs their firewall policies. However, businesses typically need to keep their firewall policies confidential. In this paper, we propose the first privacy preserving firewall outsourcing approach where businesses outsource their firewall services to ISPs without revealing their firewall policies to the ISPs. The basic idea is that businesses first anonymize their firewall policies and send the anonymized policies to their ISP, then the ISP performs packet filtering based on the anonymized firewall policies. For anonymizing firewall policies, we use Firewall Decision Diagrams to cope with the multi-dimensionality of policies and Bloom Filters for the anonymization purpose. This paper deals with a hard problem. By no means that we claim our scheme is perfect, however, this effort represents the first step towards privacy preserving outsourcing of firewall services. We implemented our scheme and conducted extensive experiments. Our experimental results show that our scheme is efficient in terms of both memory usage and packet lookup time. The firewall throughput of our scheme running at ISPs is comparable to that of software firewalls running at businesses themselves.
[telecommunication services, private network, firewalls, firewall policy anonymization, Complexity theory, packet filtering, packet lookup time, AT&amp;T, Privacy, network-based service, network-based attack, Cloud Computing, firewall management, IP networks, cloud computing, software firewall, cloud-based firewal service, firewall service outsourcing model, Internet Service Provider, ISP, Data structures, Bloom filter, memory usage, firewall decision diagram, Firewall, computer network management, outsourcing, privacy preserving firewall outsourcing approach, data privacy, firewall upgrade, Outsourcing, Virtual private networks, policy multidimensionality, firewall deployment]
Enabling Data Integrity Protection in Regenerating-Coding-Based Cloud Storage
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
To protect outsourced data in cloud storage against corruptions, enabling integrity protection, fault tolerance, and efficient recovery for cloud storage becomes critical. Regenerating codes provide fault tolerance by striping data across multiple servers, while using less repair traffic than traditional erasure codes during failure recovery. Therefore, we study the problem of remotely checking the integrity of regenerating-coded data against corruptions under a real-life cloud storage setting. We design and implement a practical data integrity protection (DIP) scheme for a specific regenerating code, while preserving the intrinsic properties of fault tolerance and repair traffic saving. Our DIP scheme is designed under a Byzantine adversarial model, and enables a client to feasibly verify the integrity of random subsets of outsourced data against general or malicious corruptions. It works under the simple assumption of thin-cloud storage and allows different parameters to be fine-tuned for the performance-security trade-off. We implement and evaluate the overhead of our DIP scheme in a real cloud storage test bed under different parameter choices. We demonstrate that remote integrity checking can be feasibly integrated into regenerating codes in practical deployment.
[secure and trusted storage systems, Cloud computing, thin-cloud storage, performance-security trade-off, real-life cloud storage setting, failure recovery, data integrity protection, set theory, Servers, program compilers, system recovery, Fault tolerance, storage management, Fault tolerant systems, outsourced data protection, experimentation, random subsets, Byzantine adversarial model, cloud computing, fault tolerance, regenerating-coded data integrity checking, random processes, implementation, Maintenance engineering, repair traffic saving, Encoding, data integrity, security of data, DIP scheme, specific regenerating code, fault tolerant computing, remote integrity checking, regenerating coding-based cloud storage testbed, remote data checking, malicious corruptions, Electronics packaging]
TailCon: Power-Minimizing Tail Percentile Control of Response Time in Server Clusters
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
To provide satisfactory customer experience, modern server clusters like Amazon usually set Service Level Agreement (SLA) as guaranteeing a certain percentile (i.e. 99%) of the customer requests to have a response time within a threshold (i.e. 1s). One way to meet the SLA constraint is to serve the customer requests with sufficient computing capacity based on the worst case workload estimation in the server cluster. However, this may cause unnecessary power consumption in the server cluster due to over-provision of the computing capacity especially when the workload is highly dynamic. In this paper, we propose an adaptive computing capacity allocation scheme referred to as TailCon. TailCon aims at minimizing the power consumption in the server cluster while satisfying the SLA constraint by adjusting the number of active servers and the CPU frequencies of the turn on machines online. In TailCon, we analyze the distribution of the request response time dynamically and leverage the measured request response time to estimate the workload intensity in the server cluster, which is used as a continuous feedback to find the proper provision of the computing capacity online based on optimization techniques. We conduct both the emulation using the real-word HTTP traces and the experiments to evaluate the performance of TailCon. The experimental results demonstrate the effectiveness of TailCon scheme in enforcing the SLA constraint while saving the power consumption.
[workstation clusters, optimization technique, Servers, power consumption, optimisation, SLA constraint, workload intensity, active server, adaptive computing capacity allocation, power-minimizing tail percentile control, Power demand, continuous feedback, Fitting, Educational institutions, real-word HTTP traces, service level agreement, server cluster, customer request, transport protocols, TailCon, request response time, Internet, CPU frequency, Time factors, Resource management, Amazon, workload estimation, Capacity planning, satisfactory customer experience]
RAM-DUR: In-Memory Deferred Update Replication
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Many database replication protocols are based on the deferred update replication technique. In deferred update replication, transactions are executed by a single server, and certified and possibly committed by every server. Thus, servers must store a full copy of the database. This assumption is detrimental to performance since servers may not be able to cache the entire database in main memory. This paper introduces RAM-DUR, a variation of deferred update replication whereby transaction execution is in-memory only. RAM-DUR's key insight is a sophisticated distributed cache mechanism that provides high performance and strong consistency without the limitations of existing solutions (e.g., no single server must have enough memory to cache the entire database). In addition to presenting RAM-DUR, we detail its implementation, and provide an extensive analysis of its performance.
[transaction processing, Protocols, database copy storage, replicated databases, fault tolerance, Random access memory, database replication protocols, Throughput, Computer crashes, cache storage, transaction execution, Servers, software fault tolerance, in-memory deferred update replication, RAM-DUR, transactional systems, Databases, distributed cache mechanism, Memory management, Database replication, distributed memory systems, high performance, software performance evaluation, performance analysis]
SPECULA: Speculative Replication of Software Transactional Memory
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
This paper introduces SPECULA, a novel replication protocol for Software Transactional Memory (STM) systems that seeks maximum overlap between transaction execution and replica synchronization phases via speculative processing techniques. By removing the replica synchronization phase from the critical path of execution of transactions, SPECULA allows threads to speculatively pipeline the execution of both transactional and/or non-transactional code. The core of SPECULA is a multi-version concurrency control algorithm that supports speculative transaction processing while ensuring the strong consistency criteria that are desirable in non-sand-boxed environments like STMs. Via an experimental study, based on a fully-fledged prototype and on both synthetic and standard STM benchmarks, we demonstrate that SPECULA can achieve speedups of up to one order of magnitude with respect to state-of-the-art non-speculative replication techniques.
[transaction processing, speculative processing techniques, Protocols, Instruction sets, replication protocols, transactional code, software transactional memories, History, synthetic STM benchmarks, transactional systems, standard STM benchmarks, speculative processing, Benchmark testing, multiversion concurrency control algorithm, replica synchronization phases, replication protocol, nonspeculative replication techniques, fully-fledged prototype, Concurrency control, transaction execution, Synchronization, speculative pipeline, concurrency control, non-sand-boxed environments, nontransactional code, STM systems, software transactional memory systems, speculative replication]
Model-Driven Comparison of State-Machine-Based and Deferred-Update Replication Schemes
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
In this paper, we analyze and experimentally compare state-machine-based and deferred-update (or transactional) replication, both relying on atomic broadcast. We define a model that describes the upper and lower bounds on the execution of concurrent requests by a service replicated using either scheme. The model is parametrized by the degree of parallelism in either scheme, the number of processor cores, and the type of requests. We analytically compared both schemes and a non-replicated service, considering a bcast- and request-execution-dominant workloads. To evaluate transactional replication experimentally, we developed Paxos STM---a novel fault-tolerant distributed software transactional memory with programming constructs for transaction creation, abort, and retry. For state-machine-based replication, we used JPaxos. Both systems share the same implementat ion of atomic broadcast based on the Paxos algorithm. We present the results of performance evaluation of both replication schemes, and a non-replicated (thus prone to failures) service, considering various workloads. The key result of our theoretical and experimental work is that neither system is superior in all cases. We discuss these results in the paper.
[transaction processing, Protocols, Servers, finite state machines, formal specification, parallel processing, abcast-execution-dominant workload, deferred-update replication scheme, request-execution-dominant workload, Concurrent computing, Analytical models, nonreplicated service, Parallel processing, request type, Paxos algorithm, software performance evaluation, programming construct, state machine replication, deferred update replication, processor core, performance evaluation, Computer crashes, model-driven comparison, state-machine-based replication, software fault tolerance, atomic broadcast, Upper bound, transactional replication, concurrency control, concurrent request execution, Paxos STM, JPaxos, model parametrization, fault-tolerant distributed software transactional memory]
S-Paxos: Offloading the Leader for High Throughput State Machine Replication
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Implementations of state machine replication are prevalently using variants of Paxos or other leader-based protocols. Typically these protocols are also leader-centric, in the sense that the leader performs more work than the non-leader replicas. Such protocols scale poorly, because as the number of replicas or the load on the system increases, the leader replica quickly reaches the limits of one of its resources. In this paper we show that much of the work performed by the leader in a leader-centric protocol can in fact be evenly distributed among all the replicas, thereby leaving the leader only with minimal additional workload. This is done (i) by distributing the work of handling client communication among all replicas, (ii) by disseminating client requests among replicas in a distributed fashion, and (iii) by executing the ordering protocol on ids. We derive a variant of Paxos incorporating these ideas. Compared to leader-centric protocols, our protocol not only achieves significantly higher throughput for any given number of replicas, but also increases its throughput with the number of replicas.
[ordering protocol execution, Protocols, High Throughput, Scalability, leader replicas, Throughput, finite state machines, Optimization, Fault tolerance, State Machine Replication, Fault tolerant systems, high-throughput state machine replication, Bandwidth, Lead, protocols, client-server systems, multi-threading, replicated databases, client request dissemination, client communication, work distribution, S-Paxos, leader-centric protocols, fault tolerant computing, nonleader replicas, Performance, Paxos]
FORTRESS: Adding Intrusion-Resilience to Primary-Backup Server Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Primary-backup replication enables arbitrary services, which need not be built as deterministic state machines, to be reliable against server crashes. Further, when the primary does not crash, the performance can be close to that of an un-replicated, 1-server system and is arguably far better than what state machine replication can offer. These advantages have made primary-backup replication a widely used technique in commercial provisioning of services, even though the technique assumes that residual software bugs in a server system can lead only to crashes and cannot result in state corruption. This assumption cannot hold against an attacker intent on exploiting vulnerabilities and corrupting the service state when attacks lead to intrusions. This paper presents a system, called FORTRESS, which can encapsulate a primary-backup system and safeguard it from being intruded. At its core, FORTRESS applies proactive obfuscation techniques in a manner appropriate to primary-backup replication and deploys proxy servers for additional defence. Gain in intrusion resilience is shown to be substantial when assessed through analytical evaluations and simulations for a range of attacker scenarios. Further, by implementing two web-based applications, the average performance drop is demonstrated to be in the order of tens of milliseconds even when obfuscation intervals are as small as tens of seconds.
[program debugging, Servers, FORTRESS, randomization, residual software bugs, proxy servers, primary-backup server systems, web-based service-provisioning, proactive obfuscation techniques, derandomization attacks, back-up procedures, unreplicated 1-server system, Educational institutions, Computer crashes, software fault tolerance, primary-backup replication, service replication, Resilience, performance measurement, simulations, security of data, arbitrary services, Software, intrusion resilience, UCIT vulnerabilities, Timing, Reliability, proactive obfuscation]
From Backup to Hot Standby: High Availability for HDFS
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Cluster-based distributed file systems generally have a single master to service clients and manage the namespace. Although simple and efficient, that design compromises availability, because the failure of the master takes the entire system down. Before version 2.0.0-alpha, the Hadoop Distributed File System (HDFS) -- an open-source storage, widely used by applications that operate over large datasets, such as MapReduce, and for which an uptime of 24x7 is becoming essential -- was an example of such systems. Given that scenario, this paper proposes a hot standby for the master of HDFS achieved by (i) extending the master's state replication performed by its check pointer helper, the Backup Node, and by (ii) introducing an automatic fail over mechanism. The step (i) took advantage of the message duplication technique developed by other high availability solution for HDFS named Avatar Nodes. The step (ii) employed another Hadoop software: ZooKeeper, a distributed coordination service. That approach resulted in small code changes, 1373 lines, not requiring external components to the Hadoop project. Thus, easing the maintenance and deployment of the file system. Compared to HDFS 0.21, tests showed that both in loads dominated by metadata operations or I/O operations, the reduction of data throughput is no more than 15% on average, and the time to switch the hot standby to active is less than 100 ms. Those results demonstrate the applicability of our solution to real systems. We also present related work on high availability for other file systems and HDFS, including the official solution, recently included in HDFS 2.0.0-alpha.
[workstation clusters, public domain software, distributed file systems, distributed processing, Throughput, Servers, MapReduce, Heart beat, master state replication, message duplication technique, IP networks, distributed coordination service, Testing, cluster-based distributed file system, Availability, replication, fault tolerance, Avatar Nodes, Hadoop, HDFS, Synchronization, namespace management, check pointer helper, high availability, ZooKeeper, open-source storage, failover, HDFS 2.0.0-alpha, naming services, Backup Node, Hadoop distributed file system]
Availability Modeling and Analysis for Data Backup and Restore Operations
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Data backup operation is an essential part of common IT system administration to protect against data loss caused by any storage failures, human errors, or disasters. Lost data can be recovered from the backed up data if it exists. Since the backup and restore operations accrue downtime overhead or performance degradation, they have to be designed to ensure the data reliability while minimizing the performance and availability overhead. In this paper, we study the impacts of different backup policies on availability measures such as storage availability, system availability, and user-perceived availability. Backup and restore operations are designed using SysML Activity diagrams that are automatically translated into Stochastic Reward Net (SRN) to compute the availability measures. Our numerical results show the effectiveness of the combination of full backup and partial backup in terms of user-perceived data availability and data loss rate. Furthermore, the sensitivity ranking can help improve the availability measures.
[partial data backup, storage system, availability modeling, Humans, SysML activity diagrams, data loss rate, data restore, availability, SRN, Degradation, Analytical models, storage management, availability analysis, IT system administration, data recovery, stochastic processes, data restore operation, user-perceived availability, software performance evaluation, downtime overhead, Availability, disasters, back-up procedures, stochastic reward net, Systems Modeling Language, storage availability, Synchronization, data backup operation, performance overhead minimization, sensitivity ranking, data reliability, Stochastic Reward Net (SRN), security of data, availability overhead minimization, storage failures, system availability, Data models, human errors, data backup]
Availability-Based Methods for Distributed Storage Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Distributed storage systems rely heavily on redundancy to ensure data availability as well as durability. In networked systems subject to intermittent node unavailability, the level of redundancy introduced in the system should be minimized and maintained upon failures. Repairs are well-known to be extremely bandwidth-consuming and it has been shown that, without care, they may significantly congest the system. In this paper, we propose an approach to redundancy management accounting for nodes heterogeneity with respect to availability. We show that by using the availability history of nodes, the performance of two important faces of distributed storage (replica placement and repair) can be significantly improved. Replica placement is achieved based on complementary nodes with respect to nodes availability, improving the overall data availability. Repairs can be scheduled thanks to an adaptive per-node timeout according to node availability, so as to decrease the number of repairs while reaching comparable availability. We propose practical heuristics for those two issues. We evaluate our approach through extensive simulations based on real and well-known availability traces. Results clearly show the benefits of our approach with regards to the critical trade-off between data availability, load-balancing and bandwidth consumption.
[load balancing, reliability theory, distributed storage systems, History, storage management, resource allocation, availability-based methods, Bandwidth, data availability, nodes heterogeneity, replica placement, redundancy, intermittent node unavailability, nodes availability, bandwidth consumption, Availability, replicated databases, Peer to peer computing, Redundancy, availability traces, redundancy minimization, durability, Maintenance engineering, performance evaluation, redundancy management, distributed storage performance improvement, distributed memory systems, adaptive per-node timeout, Load management, minimisation]
Robust and Speculative Byzantine Randomized Consensus with Constant Time Complexity in Normal Conditions
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Randomized Byzantine Consensus can be an interesting building block in the implementation of asynchronous distributed systems. Despite its exponential worst-case complexity, which would make it less appealing in practice, a few experimental works have argued quite the opposite. To bridge the gap between theory and practice, we analyze a well-known state-of-the-art algorithm in normal system conditions, in which crash failures may occur but no malicious attacks, proving that it is fast on average. We then leverage our analysis to improve its best-case complexity from three to two phases, by reducing the communication operations through speculative executions. Our findings are confirmed through an experimental validation.
[Algorithm design and analysis, message passing, asynchronous model, randomized byzantine consensus, speculative execution algorithm, asynchronous distributed system, worst case complexity, Computer crashes, crash failure, randomized consensus, Approximation methods, Proposals, speculative algorithm, parallel programming, randomised algorithms, normal system condition, normal situations, fault tolerant computing, Reliability, constant time complexity, Time complexity, computational complexity]
GRADE: Graceful Degradation in Byzantine Quorum Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Distributed storage systems are expected to provide correct services in the presence of Byzantine failures, which do not have any assumptions about the behavior of faulty servers and clients. In designing such systems, we often encounter the paradox of fault tolerance vs. performance (or efficiency), because better fault tolerance usually requires a tradeoff of system performance. In this paper, we present GRADE, a Byzantine-fault-tolerant (BFT) distributed storage system that enables graceful degradation. Two Byzantine quorum systems (BQSs) are supported on each GRADE server: a masking BQS storing generic data and a dissemination BQS storing self-verifying ones. Based on the system status and the environment, servers dynamically and seamlessly switch between two BQSs, without converting the stored data. Therefore, GRADE provides high performance in a normal running-state, and degrades performance to maintain high fault tolerance in emergency situations. The computation and communication costs of the running-state switch are very low, and the switch is completely transparent to clients. Our performance analysis and experimental results demonstrate that GRADE provides a balance between performance and fault tolerance.
[Protocols, Frequency modulation, Byzantine quorum systems, fault tolerance maintenance, Switches, BQS storing generic data masking, Registers, storage, Servers, failure analysis, Fault tolerance, data storage, Fault tolerant systems, file servers, distributed databases, communication costs, GRADE server, emergency services, Byzantine fault tolerance, fault tolerance, information dissemination, BQS, Byzantine failures, running-state switch, BFT distributed storage system, emergency situations, Byzantine quorum system, information retrieval systems, graceful degradation, Byzantine-fault-tolerant distributed storage system, dynamically servers, dissemination BQS storing self-verification, performance analysis, faulty servers]
Scalable and Secure Polling in Dynamic Distributed Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
We consider the problem of securely conducting a poll in synchronous dynamic networks equipped with a Public Key Infrastructure (PKI). Whereas previous distributed solutions had a communication cost of O(n2) in an n nodes system, we present SPP (Secure and Private Polling), the first distributed polling protocol requiring only a communication complexity of O(n log3 n), which we prove is near-optimal. Our protocol ensures perfect security against a computationally-bounded adversary, tolerates (1/2 - &#x03F5;)n Byzantine nodes for any constant 1/2 &gt;; &#x03F5; &gt;; 0 (not depending on n), and outputs the exact value of the poll with high probability. SPP is composed of two sub-protocols, which we believe to be interesting on their own: SPP-Overlay maintains a structured overlay when nodes leave or join the network, and SPP-Computation conducts the actual poll. We validate the practicality of our approach through experimental evaluations and describe briefly two possible applications of SPP: (1) an optimal Byzantine Agreement protocol whose communication complexity is &#x0398;(n log n) and (2) a protocol solving an open question of King and Saia in the context of aggregation functions, namely on the feasibility of performing multiparty secure aggregations with a communication complexity of o(n2).
[Protocols, high probability, privacy, Complexity theory, public key infrastructure, SPP-Overlay, polling, multiparty computation, communication complexity, scalable polling, Privacy, public key cryptography, distributed polling protocol, dynamic networks, byzantine agreement, secure and private polling, SPP-Computation, synchronous dynamic networks, distributed, PKI, Aggregates, secure polling, dynamic distributed networks, Public key, computationally-bounded adversary, Byzantine nodes, multiparty secure aggregations, optimal Byzantine agreement protocol]
Distributed and Private Group Management
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Group management is a fundamental building block of today's Internet applications. Mailing lists, chat systems, collaborative document editing, even well established online social networks such as Twitter and Facebook also use group management systems. In many cases, group security is required to restrict access and visibility of data in a group only to members of the group. Some applications also require privacy by keeping group members anonymous and unlinkable. Group management systems routinely rely on a central authority that manages and controls the infrastructure and data of the system. This can negatively impact the privacy and scalability properties of the system. In this paper, we propose a completely distributed approach for group management based on distributed hash tables. Enrollment to the system is not controlled by any central authority. Anyone can create groups and principals, and a various set of applications can share existing groups. In this paper, we describe a novel decentralized system for group management, address various security and privacy issues that arise by removing the central authority, and formally validate the security properties using AVISPA. We demonstrate the feasibility of this protocol by implementing a prototype running on top of Vuze's DHT.
[Protocols, Scalability, access restriction, unlinkable group members, Twitter, infrastructure management, security property, infrastructure control, Privacy, decentralized system, central authority, anonymous group members, chat system, Vuze DHT, mailing list, AVISPA, collaborative document editing, Facebook, online social network, group security, Radiation detectors, private group management, cryptography, distributed group management, group creation, privacy issues, distributed hash table, Public key, social networking (online), data privacy, Internet, group management system, data visibility restriction, Internet application]
Improving Security of Internet Services through Continuous and Transparent User Identity Verification
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Session management in distributed Internet services is traditionally based on username and password, and explicit logouts and timeouts that expire due to idle activity of the user. Emerging biometric solutions allow substituting username and password with biometric data, but still a single verification is deemed sufficient, and the identity of a user is considered immutable during the entire session. Additionally, the length of the timeout may impact on the usability of the service and consequent client satisfaction. This paper explores promising alternatives offered by biometrics for the management of sessions. A secure protocol is defined for perpetual authentication through continuous user verification. The protocol determines adaptive timeouts selected on the basis of the quality, frequency and type of biometric data acquired transparently from the user. Protocol behavior is shown through simulations.
[Internet services, session management, Protocols, Web service, CASHMA, Servers, web service, continuous authentication, biometric data, password, logout, security, Bioinformatics, authentication, user idle activity, service usability, adaptive timeout, user identity verification, client satisfaction, biometrics, Web services, Authentication, message authentication, secure protocol, protocol behavior, mobile devices, distributed Internet service, biometric solution, Biosensors, username]
Off the Wall: Lightweight Distributed Filtering to Mitigate Distributed Denial of Service Attacks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Distributed Denial of Service (DDoS) attacks are hard to deal with, due to the fact that it is difficult to distinguish legitimate traffic from malicious traffic, especially since the latter is from distributed sources. To accurately filter malicious traffic one needs (strong but costly) packet authentication primitives which increase the design complexity and typically affect throughput. It is a challenge to keep a balance between throughput and security/protection of the network core and end resources. In this paper, we propose SIEVE, a lightweight distributed filtering protocol/method. Depending on the attacker's ability, SIEVE can provide a standalone filter for moderate adversary models and a complementary filter which can enhance the performance of strong and more complex methods for stronger adversary models.
[cryptographic protocols, network core protection, Lightweight, malicious traffic filter, Servers, Security, Computer crime, distributed Denial of service attack mitigation, end resource protection, DDoS attacks, packet authentication primitives, Bandwidth, IP networks, Overlay, legitimate traffic, Routing, network core security, Partitioning algorithms, SIEVE, computer network security, end resource security, lightweight distributed filtering protocol, distributed sources, design complexity, Authentication, adversary models, telecommunication traffic]
Broadcast Authentication for Resource Constrained Devices: A Major Pitfall and Some Solutions
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Broadcast authentication is an important security mechanism for resource constrained devices, like Wireless Sensor Networks (WSNs). In this paper we revise how broadcast authentication has been enforced in this context, and we show that most of the current implementations (generally based on lightweight hash chain implementing time limited validity of the authentication property) leave open the possibility of a dreadful attack. We detail such an attack, and propose three different protocols to cope with it: PASS, TASS, and PTASS. We further analyze the overhead introduced by these protocols in terms of set-up, transmission overhead, and on device verification.
[telecommunication security, Base stations, Actuators, Protocols, wireless sensor networks, Broadcast Authentication, Receivers, broadcast authentication, broadcast communication, resource constrained devices, security mechanism, Wireless sensor networks, WSN, DAC attack, Authentication, Sensors, protocols]
Impact of Operational Reliability Re-assessment during Aircraft Missions
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
This paper addresses an aircraft mission operational reliability as resulting from component failures, environment changes, and maintenance facilities offered at the various stops involved in the mission. We will show how the on-line assessment of operational reliability will help adjust an aircraft mission, in case of major changes to equipment availability during the mission. The assessment is made possible thanks to the building and validation of a generic dependability model that is easily i) processed for the assignment of an initial mission, and ii) updated during mission accomplishment, following the occurrence of some specific major events. The generic model can be built as early as the design phase, by engineers who are specialist in dependability assessment, based on stochastic processes. Model update and processing, during aircraft operation, can be achieved by operators who are not necessarily familiar with stochastic processes in the way that they are being applied in this research. We will present examples of results that show the valuable role of operational dependability re-assessment during aircraft mission.
[maintenance facility, Stochastic processes, reliability, Aircraft mission reliability, environment change, Aircraft manufacture, design engineering, operational reliability reassessment, component failure, mission planning, Safety, stochastic processes, aircraft mission, stochastic assessment, dependabity modeling, model update, Atmospheric modeling, generic dependability model, aircraft, Maintenance engineering, stochastic process, aerospace engineering, model processing, design phase, Reliability, Aircraft, maintenance]
Automatic Generation of Graceful Programs
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Traditionally, (nonmasking and masking) fault tolerance has focused on ensuring that after the occurrence of faults, the program recovers to states from where it continues to satisfy its original specification. However, a problem with this limited notion is that, in some cases, it may be impossible to recover to states from where the entire original specification is satisfied. For this reason, one can consider a fault-tolerant graceful-degradation program that ensures that upon the occurrence of faults, the program recovers to states from where a (given) subset of its specification is satisfied. Typically, the subset of specification satisfied thus would be the critical requirements. In this paper, we focus on automatically revising a given program to obtain a corresponding graceful program, i.e., a program that satisfies a weaker specification. Specifically, this step involves adding new behaviors that satisfy the given subset of specification. Moreover, it ensures that during this process, it does not remove any behavior from the original program. With this motivation, in this paper, we focus on automatic derivation of the graceful program, i.e., a program that contains all behaviors of the original program and some new behaviors that satisfy the weaker conditions. We note that this aspect differentiates this work from previous work on controller synthesis as well as automated addition of fault tolerance in that this work requires that no new behaviors are added in the absence of faults.
[Printing, automatic programming, fault tolerance, specification satisfaction, Printers, formal specification, controller synthesis, Degradation, Fault tolerance, Fault tolerant systems, System recovery, fault tolerant computing, Safety, automatic program generation, critical requirement, graceful degradation program]
RD2: Resilient Dynamic Desynchronization for TDMA over Lossy Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
We present a distributed TDMA negotiation approach for single-hop ad-hoc network communication. It is distributed, resilient to arbitrary transient packet loss and defines a non-overlapping TDMA schedule without the need of global time synchronization. A participating node can dynamically request a fraction of the static TDMA period T. It will receive its fraction if enough time resources are available. In any case, every node can request and will receive at least a fair fraction of size 1/N. Due to its resilience to arbitrary transient packet loss, the algorithm is well suited for lossy networks like found in wireless communications. Our approach is designed to work in highly dynamic scenarios efficiently. We will show, that it defines a dynamic non-overlapping TDMA schedule even at high packet loss rates. The performance of the TDMA negotiation is analyzed by simulation and compared to results of related work.
[Schedules, distributed TDMA negotiation approach, RD2, TDMA, single-hop ad hoc network communication, Packet loss, arbitrary transient packet loss, Equations, wireless communication, Resilience, Wireless communication, Time division multiple access, time division multiple access, Self-Organizing, dynamic nonoverlapping TDMA schedule, lossy networks, Wireless Communication, Bismuth, Desynchronization, Mathematical model, ad hoc networks, resilient dynamic desynchronization]
Providing Uniform Reliable Broadcast Delivery for Mobile Ad Hoc Networks with MANET Liveness Property
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
The MANET liveness property ensures that no operative host in an ad hoc network is permanently isolated, and for networks that fulfill the property a few crash-tolerant broadcast protocols have been proposed. However, the protocols proposed till now guarantee that only at least an arbitrary majority of operative hosts receives each disseminated message, and one of these protocols has been further modified to fulfill the properties of regular reliable broadcast. Moreover, it has also been proved that the minimum time of direct connectivity between hosts, and thus the correctness of all these protocols, depends on the total number of hosts in a network and on the total number of messages that can be disseminated by each host concurrently. In this paper, we propose a novel uniform reliable broadcast protocol that works correctly, even though the minimum time of a direct connection between hosts allows them to exchange at least only two messages, which makes the correctness of this protocol independent of the total number of messages that can be disseminated by all nodes in a network.
[radio broadcasting, mobile ad hoc network, Protocols, crash-tolerant broadcast protocol, Computer crashes, broadcast communication, radio receivers, MANET liveness property, Mobile ad hoc networks, uniform reliable broadcast delivery, message dissemination, mobile ad hoc networks, telecommunication network reliability, Nickel, Reliability, Arrays, protocols]
Fault Localization in MANET-Hosted Service-Based Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Fault localization in general refers to a technique for identifying the likely root causes of failures observed in systems formed from components. Fault localization in systems deployed on mobile ad hoc networks (MANETs) is a particularly challenging task because those systems are subject to a wider variety and higher incidence of faults than those deployed in fixed networks, the resources available to track fault symptoms are severely limited, and many of the sources of faults in MANETs are by their nature transient. We present a method for localizing the faults occurring in service-based systems hosted on MANETs. The method is based on the use of dependence data that are discovered dynamically through decentralized observations of service interactions. We employ both Bayesian and timing-based reasoning techniques to analyze the data in the context of a specific fault propagation model, deriving a ranked list of candidate fault locations. We present the results of an extensive set of experiments exploring a wide range of operational conditions to evaluate the accuracy of our method.
[service interactions, timing-based reasoning technique, fault localization, decentralized observations, Ad hoc networks, Bayesian reasoning technique, Accuracy, Bayesian methods, mobile ad hoc networks, telecommunication network reliability, fault incidence, fault symptoms, MANET-hosted service-based systems, Software, specific fault propagation model, Transient analysis, Mobile computing, Monitoring]
Fast Optimization Algorithms for Designing Cellular Networks with Guard Channel
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
In this paper we consider the optimal design problems for two cellular networks with guard channel, and develop fast algorithms to derive the optimal number of channels in terms of both the dropping and blocking probabilities. First we examine algebraic properties of the new call blocking probability and the handoff call dropping probability for the base station system with both guard channel and mobile-assisted handoff, and give a stable optimization algorithm under three conjectures which can be numerically validated. Next, we consider an extended model for a cellular network, where the base station system with channel failure and repair are assumed. We provide the exact steady-state probabilities for the associated continuous-time Markov chain, and also develop an optimal design algorithm to determine the number of channels and guard channels simultaneously under the same conjectures.
[Algorithm design and analysis, channel failure, algebraic properties, Optimization algorithms, call blocking probability, Steady-state, Optimization, mobile-assisted handoff, steady-state probabilities, optimisation, System performance, Lead, CTMC, Mobile-assisted handoff, wireless channels, cellular networks, base station system, continuous-time Markov chain, probability, new call blocking probability, Guard channel, Maintenance engineering, Probability, telecommunication network topology, Cellular networks, handoff call dropping probability, fast optimization algorithms, Markov processes, guard channel, channel repair, cellular radio]
A Quick and Reliable Routing for Infrastructure Surveillance with Wireless Sensor Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
In many applications, WSNs are deployed to monitor the impact of the forces of nature on the infrastructure safety, e.g. bridge collapse detection [6]. It is very important for the routing to send surveillance results without any unnecessary delay, which can be caused by extra transmissions in a detour or an unexpected wait for the availability of the relay successor and the corresponding link connection.
[Availability, Schedules, wireless sensor networks, infrastructure surveillance, Routing, Relays, Delay, relay successor, Bridges, Continuous wavelet transforms, WSN, bridge collapse detection, link connection, telecommunication network routing, telecommunication network reliability, reliable routing, infrastructure safety, relay networks (telecommunication)]
Causally Coordinated Snapshot Isolation for Geographically Replicated Data
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
We propose a Snapshot Isolation based transaction execution and consistency model, referred to as causally coordinated snapshot isolation, for geographically replicated data. The data replication is managed through asynchronous update propagation. Our approach provides snapshot-isolation model over multiple sites and ensures causal ordering of transactions. We present here an efficient protocol for precisely capturing the causal data dependencies of transactions and ensuring the causal ordering based on these dependencies when applying transactions' updates at remote sites. Through experimental evaluations, we demonstrate the benefit of this protocol over an alternative approach for providing causal consistency for georeplicated data. We further extend this model to support session consistency guarantees such as read-your-writes and monotonic reads. Additionally, we provide a notion of group-session where a group of users are involved in a collaborative session. We provide various group-session consistency guarantees for users collaborating in a group. We present the mechanisms for providing these session consistency guarantees and evaluate their performance.
[transaction processing, causal data dependencies, Protocols, causal consistency, Data Replication, causally coordinated snapshot isolation, Delay, Session Consistency Models, asynchronous update propagation, protocol, Databases, Causal Consistency, groupware, collaborative session, group-session consistency guarantees, georeplicated data, Silicon, remote site, read-your-writes, geographically replicated data, replicated databases, Snapshot Isolation, causal transaction ordering, monotonic reads, Vectors, Synchronization, transaction update, snapshot isolation based transaction execution, data replication, Transaction Management, consistency model, Geographic Replication, Clocks]
GSFord: Towards a Reliable Geo-social Notification System
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
The eventual goal of any notification system is to deliver appropriate messages to all relevant recipients with very high reliability in a timely manner. In particular, we focus on notification in extreme situations (e.g. disasters) where geographically correlated failures hinder the ability to reach recipients inside the corresponding failed region. In this paper, we present GSFord, a reliable geo-social notification system that is aware of (a) the geographies in which the message needs to be disseminated and (b) the social network characteristics of the intended recipient, in order to maximize/increase the coverage and reliability. GSFord builds robust geo-aware P2P overlays to provide efficient location-based message delivery and reliable storage of geo-social information of recipients. When an event occurs, GSFord is able to efficiently deliver the message to recipients who are either (a) located in the event area or (b) socially correlated to the event (e.g. relatives/friends of those who are impacted by an event). Furthermore, GSFord leverages the geo-social information to trigger a social diffusion process, which operates through out-of band channels such as phone calls and human contacts, in order to reach recipients which are isolated in the failed region. Through extensive evaluations, we show that GSFord is reliable, the social diffusion process enhanced by GSFord reaches up to 99.9\\% of desired recipients even under massive geographically correlated regional failures. We also show that GSFord is efficient even under skewed distribution of user populations.
[Geography, geographically correlated failures, phone calls, Multicast communication, geographic information systems, out-of-band channels, message dissemination, location-based message delivery, reliable geosocial information storage, GSFord, user population skewed distribution, social diffusion process, peer-to-peer computing, Social network services, information dissemination, Diffusion processes, Geographical Failure, geophysics computing, Routing, Educational institutions, Event Notification, social network characteristics, reliable geosocial notification system, geographies, Social Diffusion, overlay networks, human contacts, social networking (online), Reliability, information needs, robust geoaware P2P overlays]
Performance Issue Diagnosis for Online Service Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Monitoring and diagnosing performance issues of an online service system are critical to assure satisfactory performance of the system. Given a detected performance issue and collected system metrics for an online service system, engineers usually need to make great efforts to conduct diagnosis by first identifying performance issue beacons, which are metrics that pinpoint to the root causes. In order to reduce the manual efforts, in this paper, we propose a new approach to effectively detecting performance issue beacons to help with performance issue diagnosis. Our approach includes techniques for mining system metric data to address limitations when applying previous classification-based approaches. Our evaluations on both a controlled environment and a real production environment show that our approach can more effectively identify performance issue beacons from system metric data than previous approaches.
[Measurement, system metric data mining, controlled environment, data mining, online service systems, manual effort reduction, Servers, Data mining, system recovery, monitoring data analysis, Training, performance issue monitoring, Accuracy, performance issue beacon detection, production environment, system performance, Silicon, class association rule, Internet, performance issue diagnosis, Monitoring, software performance evaluation]
An End-to-End Security Auditing Approach for Service Oriented Architectures
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Service-Oriented Architecture (SOA) is becoming a major paradigm for distributed application development in the recent explosion of Internet services and cloud computing. However, SOA introduces new security challenges not present in the single-hop client-server architectures due to the involvement of multiple service providers in a service request. The interactions of independent service domains in SOA could violate service policies or SLAs. In addition, users in SOA systems have no control on what happens in the chain of service invocations. Although the establishment of trust across all involved partners is required as a prerequisite to ensure secure interactions, still a new end-to-end security auditing mechanism is needed to verify the actual service invocations and its conformance to the expected service orchestration. In this paper, we provide an efficient solution for end-to-end security auditing in SOA. The proposed security architecture introduces two new components called taint analysis and trust broker in addition to taking advantages of WS-Security and WS-Trust standards. The interaction of these components maintains session auditing and dynamic trust among services. This solution is transparent to the services, which allows auditing of legacy services without modification. Moreover, we have implemented a prototype of the proposed approach and verified its effectiveness in a LAN setting and the Amazon EC2 cloud computing infrastructure.
[LAN setting, trust, trust broker, Service Oriented Architecture, Security, Servers, distributed application development, dynamic trust, end-to-end security auditing mechanism, taint analysis, legacy service, session auditing, Amazon EC2 cloud computing infrastructure, cloud computing, security auditing, Monitoring, service-oriented architecture, Internet service, WS-security standard, independent service domain, end-to-end security auditing approach, Service oriented architecture, Standards, security of data, Web services, security architecture, WS-trust standard, service oriented architecture, Time factors, trusted computing]
RADAR: Adaptive Rate Allocation in Distributed Stream Processing Systems under Bursty Workloads
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
In the recent years we have witnessed a proliferation of distributed stream processing systems that need to operate under bursty workloads. Examples include road traffic control, processing of financial feeds, network monitoring and real-time sensor data analysis systems. Meeting the QoS requirements of the stream processing systems under burstiness is a challenging process. In this paper we present our approach for adaptive rate allocation within the distributed stream processing system to meet the end-to-end execution time and rate demands of the applications. Our algorithm determines the rates of the application components at runtime, with respect to the QoS constraints, to compensate for delays experienced by the components or to react to sudden bursts of load. Our technique is distributed and low-cost. Our detailed experimental results over our Synergy middleware illustrate that our approach is practical, depicts good performance and has low resource overhead.
[realtime sensor data analysis, road traffic control, distributed processing, Throughput, Optimization, resource allocation, QoS, QoS requirement, QoS constraint, bursty workload, Mathematical model, middleware, Synergy middleware, adaptive rate allocation, financial feeds processing, Radar applications, quality of service, rate allocation, bursty workloads, Equations, network monitoring, resource overhead, distributed stream processing system, rate demand, distributed stream processing systems, end-to-end execution time, Resource management]
Efficient Asynchronous Low Power Listening for Wireless Sensor Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Energy conservation and reliability of wireless communications are two crucial requirements of practical sensor networks. Radio duty cycling is a widely used mechanism to reduce energy consumption of sensor devices and to increase the lifetime of the network. A side effect of radio duty cycling is that it can cause the wireless communications to be unreliable---if a sender node transmits a packet while the receiver is asleep, the communication fails. Early duty cycling protocols like B-MAC that were designed for bit streaming radios achieve low duty cycle by keeping the radio transceiver awake for short time periods. However, they require a transmitter node to precede a packet transmission with a long preamble to ensure the reliability of wireless communication. Furthermore, they cannot be used with modern packet radios like widely used IEEE 802.15.4 based radio transceivers, which cannot transmit arbitrarily long preambles. Recent duty cycling schemes like X-MAC, on the other hand, reduce the length of the preamble and are designed to work with packet radios. However, in order to ensure that a receiver can reliably detect a transmitter's preamble transmission, these schemes need to turn the radio transceiver on for longer time durations than the early schemes like B-MAC. In this paper, we present a novel duty cycling scheme called Quick MAC, that achieves a very low duty cycle without compromising the reliability of wireless communication. Furthermore, Quick MAC is stateless, compatible with packet (and bit stream) radios, and does not require synchronization among sensor nodes. From our experiments using TMote sky motes, we show that Quick MAC reduces duty cycle by a factor of about 4 compared to X-MAC, and yet maintains the same level of reliability of wireless communication as X-MAC.
[TMote sky motes, duty cycling, Protocols, wireless sensor networks, preamble transmission, QuickMAC, sensor networks, duty cycling protocols, Wireless communication, Packet radio networks, B-MAC, telecommunication network reliability, X-MAC, protocols, energy consumption, receiver, Radio transmitters, radio transceivers, Receivers, Communication reliability, wireless communications reliability, synchronisation, radio duty cycling, Wireless sensor networks, IEEE 802.15.4 based radio transceivers, efficient asynchronous low power listening, energy conservation, synchronization, Reliability]
Energy-Efficient and Fault-Tolerant Structural Health Monitoring in Wireless Sensor Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Wireless sensor networks (WSNs) have become an increasingly compelling platform for structural health monitoring (SHM) due to relatively low-cost, easy installation, etc. However, the challenge of effectively monitoring structural health condition (e.g., damage) under WSN constraints (e.g., limited energy, narrow bandwidth) and sensor faults has not been studied before. In this paper, we focus on tolerating sensor faults in WSN-based SHM. We design a distributed WSN framework for SHM and then examine its ability to cope with sensor faults. We bring attention to an undiscovered yet interesting fact, i.e., the real measured signals introduced by faulty sensors may cause an undamaged location to be identified as damaged (false positive) or a damaged location as undamaged (false negative) diagnosis. This can be caused by faults in sensor bonding, precision degradation, amplification gain, bias, drift, noise, and so forth. We present a distributed algorithm to detect such types of faults, and offer an online signal reconstruction algorithm to recover from the wrong diagnosis. Through simulations and a WSN prototype system, we evaluate the effectiveness of our proposed algorithms.
[fault diagnosis, structural engineering, WSN-based SHM, Shape, wireless sensor networks, Noise, fault detection, amplification gain, structural health monitoring, energy-efficient structural health monitoring, Vibrations, Fault tolerance, Fault tolerant systems, distributed WSN framework, signal reconstruction, WSN constraints, online signal reconstruction algorithm, Monitoring, signal detection, fault tolerance, precision degradation, Wireless sensor networks, distributed algorithm, fault-tolerant structural health monitoring, condition monitoring, sensor bonding, sensor faults, energy-efficiency]
Time-Sensitive Utility-Based Routing in Duty-Cycle Wireless Sensor Networks with Unreliable Links
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Utility-based routing is a special routing approach, which takes the reliability and transmission costs into account at the same time. However, the existing utility-based routing algorithms have not yet considered the delivery delay. Thus, they cannot work well in duty-cycle wireless sensor networks (WSNs) since delay is an important factor in such WSNs. In this paper, we propose a novel utility model time-sensitive utility model. Unlike previous work, the utility of a message delivery in our model is not only affected by the reliability and transmission costs but also by the delivery delay. Under the time-sensitive utility model, we derive an iterative formula to compute the time-varying utility of each message delivery. Based on the formula, we propose an optimal time-sensitive utility-based routing algorithm. The theoretical analysis and simulation results show that our proposed algorithm can maximize the average utility of message deliveries, which makes a good tradeoff among reliability, delay, and cost.
[Algorithm design and analysis, wireless sensor networks, unreliable links, Computational modeling, time-sensitive utility-based routing approach, delivery delay, reliability, Routing, time-sensitive utility, Delay, Wireless sensor networks, routing, utility-based routing algorithms, WSN, duty-cycle wireless sensor networks, time-sensitive utility model, delays, telecommunication network routing, telecommunication network reliability, transmission costs, message delivery time-varying utility, Sensors, Reliability, Distributed algorithms]
Efficient and Reliable Multicast in Multi-radio Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
This paper investigates a novel efficient approach to utilize multiple radio interfaces for enhancing the performance of reliable multicasts from a single sender to a group of receivers. In the proposed scheme, one radio channel (and interface) is dedicated only for recovery information transmissions. We apply this concept to both ARQ and hybrid ARQ+FEC protocols, formally analyzing the number of packets each receiver needs to process in both our approach and in the common single channel approach. We also present a corresponding efficient protocol, and study its performance by simulation. Both the formal analysis and the simulations demonstrate the benefits of our scheme.
[multiradio networks, ARQ, radio networks, Protocols, Packet loss, reliable multicast, receivers, multicast communication, telecommunication network reliability, multiple radio interface, wireless channels, multiple radio interfaces, radio channel, FEC, Receivers, common single channel approach, forward error correction, automatic repeat request, Reliable multicast, network protocols, Forward error correction, transmission information recovery, Reliability, Automatic repeat request, hybrid ARQ+FEC protocol, performance analysis]
Fair Comparison of Gossip Algorithms over Large-Scale Random Topologies
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
We present a thorough performance comparison of three widely used probabilistic gossip algorithms over well-known random graphs. These graphs represent some large-scale network topologies: Bernoulli (or Erdos-Re&#x0301;nyi) graph, random geometric graph, and scale-free graph. In order to conduct such a fair comparison, particularly in terms of reliability, we propose a new parameter, called effectual fan out. For a given topology and gossip algorithm, the effectual fan out characterizes the mean dissemination power of infected sites. For large-scale networks, the effectual fan out has thus a strong linear correlation with message complexity. It enables to make an accurate analysis of the behavior of a gossip algorithm over a topology. Furthermore, it simplifies the theoretical comparison of different gossip algorithms on the topology. Based on extensive experiments on top of OMNet++ simulator, which make use of the effectual fan out, we discuss the impact of topologies and gossip algorithms on performance, and how to combine them to have the best gain in terms of reliability.
[Algorithm design and analysis, large-scale networks, Protocols, linear correlation, graph theory, reliability, Complexity theory, Message Complexity, communication complexity, Distributed Algorithms, message complexity, Network topology, random geometric graph, telecommunication network reliability, large-scale random topology, Erdos-Re&#x0301;nyi graph, Random Topologies, scale-free graph, probability, Gossip Algorithms, telecommunication network topology, Probabilistic logic, Topology, network topology, Performance Evaluation, Bernoulli graph, OMNet++ simulator, Reliability, probabilistic gossip algorithms, mean dissemination power]
A Theory of Packet Flows Based on Law-of-Mass-Action Scheduling
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Designing dynamically robust protocols is not a simple task with current classic work-conserving scheduling, where packets are sent out as soon as processing and transmission capacity is available. We show that deviating from this fundamental queuing assumption leads to much more controllable and analyzable forms of protocols. At the core of our work is a queue-scheduling discipline based on the chemical "Law of Mass Action" (LoMA) that serves a queue with a rate proportional to its fill level. In this paper we introduce our LoMA-scheduling approach and provide a solid mathematical framework adopted from chemistry that simplifies the analysis of the corresponding queueing networks, including the prediction of the underlying protocols' dynamics. We demonstrate the elegance of our model by implementing and analyzing a TCP-compatible "chemical" congestion control algorithm C3A with only a few interacting queues (another novelty of our approach). We also show the application of our theory to gossip protocols, explain an effective implementation of the scheduler and discuss possibilities of how to integrate mass-action scheduling into traditional networking environments.
[Protocols, telecommunication congestion control, dynamically robust protocols, Servers, Delay, Analytical models, solid mathematical framework, scheduling, packet flows, work-conserving scheduling, gossip protocols, Mathematical model, queueing networks, queuing assumption, queueing theory, interacting queues, transmission capacity, queue-scheduling discipline, Chemicals, transport protocols, protocols dynamics, TCP-compatible chemical congestion control, law-of-mass-action scheduling, LoMA-scheduling, Queueing analysis]
HybCAST: Rich Content Dissemination in Hybrid Cellular and 802.11 Ad Hoc Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
We design, implement, and evaluate a middleware system, HybCAST, that leverages a hybrid cellular and ad hoc network to disseminate rich contents from a source to all mobile devices in a predetermined region. HybCAST targets information dissemination over a range of scenarios (e.g., military operations, crisis alerting, and popular sporting events) in which high reliability and low latency are critical and existing fixed infrastructures such as wired networks, 802.11 access points are heavily loaded or partially destroyed. HybCAST implements a suite of protocols that: (i) structures the hybrid network into a hierarchy of two-level ad hoc clusters for better scalability, (ii) employ both data push and pull mechanisms for high reliability and low latency dissemination of rich content, and (iii) implement a near-optimal gateway selection algorithm to minimize the transmission redundancy. To demonstrate its practicality and efficiency, we have implemented and deployed the HybCAST middleware on several Android smart phones and an in-network Linux machine that acts as a dissemination server. The system is evaluated via real experiments using a UMTS network and extensive packet-level simulations. Our experimental results from a live network show that HybCAST achieves 100% reliability with shorter latencies and lower overall energy consumption. Simulation results confirm that HybCAST outperforms other state-of-the-art systems in the literature. For example, HybCAST exhibits a 5 times reduction in the dissemination latencies as compared to other hybrid dissemination protocols, while its energy consumption is a third of a cellular-only dissemination system. Furthermore, the simulation results demonstrate that HybCAST scales well and maintains good performance under varying numbers of mobile devices, diverse content sizes, and device mobility.
[3G mobile communication, rich content dissemination, reliability, two-level ad hoc clusters, Mobile handsets, approximation algorithms, Servers, 802.11 access points, dissemination, telecommunication network reliability, protocols, energy consumption, middleware, Base stations, packet-level simulations, IEEE 802.11 Standards, hybrid cellular networks, information dissemination, latency dissemination, wireless networks, Ad hoc networks, smart phones, Android smart phones, HybCAST middleware, device mobility, Middleware, UMTS network, wired networks, multimedia, Linux, dissemination server, 802.11 ad hoc networks, Logic gates, mobile devices, diverse content sizes, in-network Linux machine, Reliability, ad hoc networks, cellular radio, cellular-only dissemination system]
Reliable Distributed Real-Time and Embedded Systems through Safe Middleware Adaptation
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Distributed real-time and embedded (DRE) systems are a class of real-time systems formed through a composition of predominantly legacy, closed and statically scheduled real-time subsystems, which comprise over-provisioned resources to deal with worst-case failure scenarios. The formation of the system-of-systems leads to a new range of faults that manifest at different granularities for which no statically defined fault tolerance scheme applies. Thus, dynamic and adaptive fault tolerance mechanisms are needed which must execute within the available resources without compromising the safety and timeliness of existing real-time tasks in the individual subsystems. To address these requirements, this paper describes a middleware solution called Safe Middleware Adaptation for Real-Time Fault Tolerance (SafeMAT), which opportunistically leverages the available slack in the over-provisioned resources of individual subsystems. SafeMAT comprises three primary artifacts: (1) a flexible and configurable distributed, runtime resource monitoring framework that can pinpoint in real-time the available slack in the system that is used in making dynamic and adaptive fault tolerance decisions, (2) a safe and resource aware dynamic failure adaptation algorithm that enables efficient recovery from different granularities of failures within the available slack in the execution schedule while ensuring real-time constraints are not violated and resources are not overloaded, and (3) a framework that empirically validates the correctness of the dynamic mechanisms and the safety of the DRE system. Experimental results evaluating SafeMAT on an avionics application indicates that SafeMAT incurs only 9-15% runtime fail over and 2-6% processor utilization overheads thereby providing safe and predictable failure adaptability in real-time.
[closed scheduled real-time subsystems, Profiling, SafeMAT, dynamic fault tolerance decisions, safety-critical software, flexible distributed framework, distributed real-time and embedded, middleware solution, avionics application, resource aware dynamic failure adaptation algorithm, adaptive fault tolerance mechanisms, Fault tolerance, Runtime, reliable distributed real-time systems, Fault tolerant systems, dynamic fault tolerance mechanisms, configurable distributed framework, embedded systems, predominantly legacy, Real-time systems, adaptive fault tolerance decisions, Monitoring, middleware, Adaptation, predictable failure adaptability, statically scheduled real-time subsystems, runtime resource monitoring framework, real-time constraints, Middleware, software fault tolerance, Realtime, system-of-systems, real-time tasks, safe middleware adaptation, worst-case failure scenarios, real-time fault tolerance, DRE systems, statically defined fault tolerance scheme, Software Health Management, Resource management, individual subsystems, Fault Tolerance]
Approximation Techniques for Maintaining Real-Time Deployments Informed by User-Provided Dataflows within a Cloud
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Distributed applications are increasingly developed by composing many participants, such as services, components, and objects. When deploying distributed applications into a mobile ad hoc cloud, the locality of application participants that communicate with each other can affect latency, power/\\-battery usage, throughput, and whether or not a cloud provider can meet service-level agreements (SLA). Optimization of important communication links within a distributed application is particularly important when dealing with mission-critical applications deployed in a distributed real-time and embedded (DRE) scenario, where violation of SLAs may result in loss of property, cyber infrastructure, or lives. To complicate the optimization process, the underlying cloud environment can change during operation and an optimal deployment of the distributed application may degrade over time due to hardware failures, overloaded hosts, and other issues that are beyond the control of distributed application developers. To optimize performance of distributed applications in dynamic environments, therefore, the deployment of participants may need adapting and revising according to the requirements of application developers and the resources available in the underlying cloud environment. This paper present two contributions to the study of dynamic optimizations of user-provided deployments within a cloud. First, we present a dataflow description language that allows developers to designate key communication paths between participants within their distributed applications. Second, we describe heuristics that use this dataflow representation to identify optimal configurations for initial deployments and/or subsequent redeployments within a cloud. An experiment is presented to validate the heuristic approaches.
[service-level agreement, optimal deployment, Humans, cloud provider, latency, SLA, resource availability, Batteries, Approximation methods, contracts, distributed application, property loss, Genetic algorithms, key communication path designation, optimization process, mobile computing, power usage, heuristics, embedded scenario, cloud environment, constraint problems, Real-time systems, real-time deployment, dynamic environment, cloud computing, mission-critical application, parallel languages, real time, overloaded host, heuristic approach, approximation technique, dataflow description language, Image edge detection, application participant locality, data flow analysis, dataflow representation, genetic algorithms, distributed real-time scenario, user-provided dataflow, cloud optimization, mobile ad hoc cloud, application developer requirement, battery usage, cyber infrastructure, Approximation algorithms, important communication link optimization, hardware failure, performance optimization]
Response Time Reliability in Cloud Environments: An Empirical Study of n-Tier Applications at High Resource Utilization
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
When running mission-critical web-facing applications (e.g., electronic commerce) in cloud environments, predictable response time, e.g., specified as service level agreements (SLA), is a major performance reliability requirement. Through extensive measurements of n-tier application benchmarks in a cloud environment, we study three factors that significantly impact the application response time predictability: bursty workloads (typical of web-facing applications), soft resource management strategies (e.g., global thread pool or local thread pool), and bursts in system software consumption of hardware resources (e.g., Java Virtual Machine garbage collection). Using a set of profit-based performance criteria derived from typical SLAs, we show that response time reliability is brittle, with large response time variations (order of several seconds) depending on each one of those factors. For example, for the same workload and hardware platform, modest increases in workload burstiness may result in profit drops of more than 50%. Our results show that profitbased performance criteria may contribute significantly to the successful delimitation of performance unreliability boundaries and thus support effective management of clouds.
[predictable response time reliability, n-tier, web application, application response time predictability, reliability, performance unreliability boundary, profit based performance criteria, Servers, contracts, Databases, System performance, performance reliability requirement, cloud environment, system software consumption, Hardware, bursty workload, cloud computing, response time prediction, mission critical Web facing application, response time variation, performance reliability, profit model, hardware resource, service level agreement, high resource utilization, cloud management, soft resource management strategy, Time factors, Reliability, Resource management, n tier applications]
A Quantitative Comparison of Reactive and Proactive Replicated Storage Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Replicated storage systems allow their stored data objects to outlive the life of the nodes storing them through replication. In this paper, we focus on durability, and more specifically on the concept of an object's lifetime, i.e., the duration of time between the creation of an object and when it is permanently irretrievable from the system. We analyze two main replication strategies: reactive, in which replication occurs in response to failures, and proactive, in which replication occurs in anticipation of failures. Our work presents a quantitative analysis that compares reactive and proactive through analytical models and simulations, considering exponentially distributed failures and reactive repairs, and periodic proactive replications. We also present a derivation of the analytical formula for the variance of the lifetime in the reactive model. Our results indicate that a proactive strategy leads to multiple times higher storage requirements than a reactive strategy. In addition, reactive systems are only moderately bursty in terms of bandwidth consumption, with rare peaks of at most five times the bandwidth consumption in proactive systems (given input parameter values that are compatible with real systems). Finally, for both strategies, the standard deviation is very close to the expected lifetime, and consequently, the lifetimes close to being exponentially distributed.
[Replicated, reactive systems, object creation, failure analysis, Analytical models, Histograms, failure response, Bandwidth, failures anticipation, bandwidth consumption, Availability, periodic proactive replication strategies, reactive repairs, replicated databases, reactive replication strategies, Maintenance engineering, Standards, Dispersion, Storage, information retrieval systems, Distributed, storage requirements, system irretrievable, exponentially distributed failures, proactive replicated storage systems, standard deviation]
PGV: A Storage Enforcing Remote Verification Scheme
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
This paper presents a storage enforcing remote verification scheme, PGV (Pretty Good Verification). While existing schemes are often developed to handle a malicious adversarial model, we argue that such a model is often too strong of an assumption, resulting in over-engineered, resource-intensive mechanisms. Instead, the storage enforcement property of PGV aims at removing a practical incentive for a storage server to cheat in order to save on storage space in a covert adversarial model. At its core, PGV relies on the well-known polynomial hash, we show that the polynomial hash provably possesses the storage enforcement property and is also efficient in terms of performance. In addition to the traditional application of a client verifying the storage content at a remote server, PGV can also be applied to de-duplication scenarios where the server wants to verify whether the client possesses a significant amount of information about a file (and not just a partial knowledge/fingerprint of the file) before granting access to an existing file. We theoretically prove the power of PGV by combining Kolmogorov complexity and list decoding, and experimentally show the simplicity and low overhead of PGV by comparing it with existing schemes. Altogether, PGV provides a good, practical way to perform storage enforcing remote verification.
[storage enforcement property, Cloud computing, Protocols, storage content verification, proof of data possession, pretty good verification scheme, proof of ownership, Complexity theory, Servers, storage management, resource-intensive mechanisms, storage server, remote server, Kolmogorov complexity, list decoding, storage space, Polynomials, Cryptography, cloud computing, client-server systems, deduplication scenario, malicious adversarial model, polynomial hash, Probabilistic logic, cryptography, file access, storage enforcing remote verification scheme, decoding, storage enforcement, proof of retrievability, cloud storage, covert adversarial model, PGV, computational complexity]
AAD: Adaptive Anomaly Detection System for Cloud Computing Infrastructures
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Cloud computing has become increasingly popular by obviating the need for users to own and maintain complex computing infrastructure. However, due to their inherent complexity and large scale, production cloud computing systems are prone to various runtime problems caused by hardware and software failures. Autonomic failure detection is a crucial technique for understanding emergent, cloudwide phenomena and self-managing cloud resources for system-level dependability assurance. To detect failures, we need to monitor the cloud execution and collect runtime performance data. These data are usually unlabeled, and thus a prior failure history is not always available in production clouds, especially for newly managed or deployed systems. In this paper, we present an Adaptive Anomaly Detection (AAD) framework for cloud dependability assurance. It employs data description using hypersphere for adaptive failure detection. Based on the cloud performance data, AAD detects possible failures, which are verified by the cloud operators. They are confirmed as either true failures with failure types or normal states. The algorithm adapts itself by recursively learning from these newly verified detection results to refine future detections. Meanwhile, it exploits the observed but undetected failure records reported by the cloud operators to identify new types of failures. We have implemented a prototype of the algorithm and conducted experiments in an on-campus cloud computing environment. Our experimental results show that AAD can achieve more efficient and accurate failure detection than other existing scheme.
[Cloud computing, adaptive anomaly detection system, complex computing infrastructure maintenance, failure analysis, system level dependability assurance, Detectors, cloud execution monitoring, data acquisition, cloud computing infrastructure, Failure detection, cloud computing, learning (artificial intelligence), Kernel, recursive learning, AAD, software failure, recursive functions, runtime performance data collection, Equations, software fault tolerance, cloud operator, Support vector machines, adaptive failure detection, Learning algorithms, Sensitivity, security of data, runtime problem, Autonomic management, Dependable systems, hardware failure, self-managing cloud resource, autonomic failure detection]
Distributed Monitoring of Temporal System Properties Using Petri Nets
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Supervising a system in operation allows to detect a violation of system specification or temporal properties, and is the first step required by any reconfiguration mechanism. In this work, we focus on run-time verification of temporal system properties in distributed and real-time systems. Based on a description of a property that includes events and temporal constraints, expressed as an arc timed Petri net, we automatically derive a monitoring system responsible for checking this property. The proposed approach enables the distributed verification of system properties. Our contribution is twofold. On the theoretical side, we introduce a slight modification of the semantics of Petri nets to be able to execute it in partial executions and noisy observation environments. On the practical side, we show how to use this formal framework to provide a distributed and efficient monitoring system, and describe its current implementation.
[Protocols, program verification, run-time verification, Instruction sets, Petri nets, distributed processing, distributed system, temporal constraints, system specification, formal specification, Semantics, reconfiguration mechanism, distributed monitoring, arc timed Petri net, violation detection, temporal system property, real-time system, event constraints, Monitoring, Distributed Monitoring, Noise measurement, Online Verification, real-time systems, temporal property, system monitoring, Timing]
Improving the Reliability and Availability of Vehicular Communications Using Voronoi Diagram-Based Placement of Road Side Units
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Vehicular Ad-hoc Networks (VANETs) form the basis for critical services that improve traffic safety and alleviate traffic congestion. The reliability of VANET-based services and applications that are based solely on vehicle-to-vehicle (V2V) communications, however, is hindered due primarily to limited and often fluctuating V2V communications. To address this limitation, Road-Side Units (RSU) have been proposed to complement V2V communications by providing stable event and data brokering capability. Effective placement of the RSUs is a key requirement in improving reliability of VANET services. This poster describes a novel Voronoi network-based algorithm for the effective placement of RSUs. The reliability metric considered in placing the RSUs involves bounding both the delay incurred by communication packets and packet loss, which in turn ensure timeliness and correct operation of the VANET services.
[Roads, VANET-based service reliability, computational geometry, data brokering capability, packet loss, reliability improvement, vehicular communications, Delay, Vehicles, Voronoi diagram-based placement, connectivity, telecommunication network reliability, traffic congestion, Availability, vehicular ad hoc networks, vehicle-to-vehicle communications, Voronoi network-based algorithm, communication packets, traffic safety, V2V communications, RSU, Vehicular ad hoc networks, Vehicular networks, placement, road side units]
Secure Cloud Browser: Model and Architecture to Support Secure WEB Navigation
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
A Web browser is probably the main attack vector used by hackers. Solutions to browser's security are difficult to foresee, due to the influence of user-behaviour. In this paper, we show how to tackle the issue of securing web browsers introducing a Secure Cloud Browser (SCB) architecture. The rationale behind the SCB is to reduce browser vulnerability by transporting it to a remote secure environment, making it ephemeral and renovating it in a manner that is transparent to the user. Our scheme allows the user to browse a web application while source code is remotely interpreted. Further, obfuscation techniques are used to increase the lifetime of a browser session. Details and discussion over the SCB architecture are reported, while its implementation and assessment is undergoing work.
[Computers, secure Web navigation model, source coding, Web browser security, Scalability, secure Web navigation architecture, browser vulnerability reduction, user-behaviour influence, Browsers, Security, Servers, software architecture, security of data, SCB architecture, remote secure environment, remote source code interpretation, Web application, obfuscation techniques, Computer architecture, online front-ends, browser session lifetime, Software, cloud computing, secure cloud browser architecture]
Towards Identifying Root Causes of Faults in Service-Based Applications
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
In this paper we study fault localization techniques for identification of incompatible configurations and implementations in service-based applications. We propose an approach using pooled decision trees for localization of faulty service parameter and binding configurations, explicitly addressing temporary and changing fault conditions.
[Industries, pooled decision trees, fault diagnosis, temporary-changing fault conditions, incompatible binding configuration identification, Standards, software fault tolerance, Training, Fault diagnosis, Runtime, faulty service parameter localization techniques, Machine learning, decision trees, service-based application faults, Decision trees, learning (artificial intelligence), business data processing, service-oriented architecture]
Anonymous On-Demand Routing and Secure Checking of Traffic Forwarding for Mobile Ad Hoc Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Anonymous communications in mobile ad hoc networks is an important and effective way against malicious traffic analysis. Various anonymous routing schemes have been proposed for MANETs. However, most works failed to resist the global tracker, and always ignored the reliability of data delivery. In this paper, a comprehensive anonymous communication protocol, called ARSC, is proposed. The ARSC consists of anonymous routing, which is based on identity-based encryption pseudonym and single-round onion, and secure checking of traffic forwarding in data transmission phase, to achieve strong route anonymity and improve reliability of packet delivery in the data transmission phase. From the security analysis, our protocol ARSC is more secure than other schemes such as ANODR, SDAR, AnonDSR, CAR and MASK. Moreover, simulation experiments show that the ARSC has better performance than any other onion-structured anonymous routing protocols.
[telecommunication security, identity-based encryption pseudonym, cryptographic protocols, single-round onion, AnonDSR, MASK, onion-structured anonymous routing protocols, security, on-demand routing, traffic forwarding security checking analysis, data delivery reliability, mobile ad hoc networks, telecommunication network reliability, malicious traffic analysis, Routing protocols, ARSC, packet delivery reliability, SDAR, ANODR, Routing, data transmission phase, MANET, comprehensive anonymous communication protocol, CAR, Public key, routing protocols, MANETs, anonymous, Nickel, anonymous on-demand routing scheme, telecommunication traffic]
Pairwise Key Generation Scheme for Cellular Mobile Communication
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
We present two new key generation schemes for secure communication between a pair of mobile nodes (cell phones). Unlike existing algorithms, our scheme does not (a) store a key chain in the memory from a universal key space, (b) use key broadcast, (c) distribute selected keys to the network nodes, and (d) use database of keys for selecting keys for communication. Rather, the pair of nodes that want to communicate securely generate identical keys independently with the help of a head node. We show the behavior of these schemes through a simple analytical model.
[telecommunication security, key broadcast, cryptography, Mobile nodes, Encryption, symmetric, Key distribution, security, key generation, cellular mobile communication, Authentication, universal key space, communication security, pairwise key generation scheme, location parameters, asymmetric, mobile nodes, cellular radio]
Data De-duplication and Event Processing for Security Applications on an Embedded Processor
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Network security schemes generally deploy sensors and other network devices which generate huge volumes of data, overwhelming the underlying decision making algorithms. An example is corporate networks employing intrusion detection systems where there is a deluge of alert data, confounding the computations involved in sensor information fusion and alert correlation. One way to obtain fast and real-time responses is to preprocess such data to manageable sizes. In this paper, we show that data de-duplication using computationally efficient fingerprinting algorithms can provide real-time results. We present an algorithm which utilizes Rabin Fingerprinting/hashing scheme for the purpose of data de-duplication. We have implemented this algorithm on Intel Atom, which is a powerful, energy efficient embedded processor. Our study is intended to show that the relatively low performing embedded processors are capable of providing the needed computational support if they were to handle security functions in the field. When compared to the algorithmic performance on a high end system, viz. Intel Core 2 Duo processor, the positive results obtained make a case for using the Atom processor in networked applications employing mobile devices.
[Algorithm design and analysis, Correlation, computational support, data deduplication, Instruction sets, energy efficient embedded processor, Fingerprint recognition, sensor fusion, fast response, computationally efficient fingerprinting algorithms, Security, Intel Core 2 duo processor, hashing scheme, Mobile devices, security applications, Rabin fingerprinting scheme, embedded systems, Alert correlation, Polynomials, Real-time systems, Embedded processors, high end system, alert data, sensor information fusion, Atom processor, intrusion detection systems, network security schemes, event processing, Redundancy, handle security functions, cryptography, microprocessor chips, corporate networks, real-time response, decision making algorithms, networked applications, Fingerprinting, decision making, mobile devices, low performing embedded processors, network devices, algorithmic performance, alert correlation, Intel Atom]
Query Plan Execution in a Heterogeneous Stream Management System for Situational Awareness
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Battlefield monitoring involves collecting streaming data from different sources, transmitting the data over a heterogeneous network, and processing queries in real-time in order to respond to events in a timely manner. Nodes in these networks differ with respect to their processing, storage and communication capabilities. Links in the network differ with respect to their communication bandwidth. The topology of the network itself is subject to change, as the nodes and links may become unavailable. Continuous queries executed in such environments must also meet some quality of service (QoS) requirements, such as, response time, throughput, and memory usage. We propose that the processing of the queries be shared to improve resource utilization, such as storage and bandwidth, which, in turn, will impact the QoS. We show how multiple queries can be represented in the form of an operator tree, such that their commonalities can be easily exploited for multi query plan generation. Query plans may have to be updated in this dynamic environment (network topology changes, arrival of new queries, arrival pattern of streams altered), this, in turn, necessitates migrating operators from one set of nodes to another. We sketch some ideas about how operator migration can be done efficiently in such environments.
[situational awareness, Quality of service, Electronic mail, query processing, query plan execution, Network topology, heterogeneous network, network links, Bandwidth, Computer architecture, Real-time systems, data stream management system, dynamic environment, resource utilization, Monitoring, military computing, streaming data, communication bandwidth, battlefield monitoring, computer networks, telecommunication network topology, quality of service, network topology changes, multi query plan generation, data transmission, QoS requirements, heterogeneous stream management system]
Private Anonymous Messaging
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Messaging systems where a user maintains a set of contacts and broadcasts messages to them is very common. We address the problem of a contact obtaining a message that it missed, from other contacts of the user while maintaining anonymity of all parties involved. We identify a set of requirements in addressing this problem and propose a modification to the hierarchical identity based encryption scheme proposed by Boneh et. al. We briefly present an implementation of the proposed cryptographic primitives as a proof of concept.
[Ciphers, cryptographic primitives, Peer-to-peer messaging, Anonymity, cryptography, Generators, messaging system, broadcast message, Encryption, Privacy, XML, message authentication, contact message, Libraries, hierarchical identity based encryption, Arrays, private anonymous messaging, hierarchical identity based encryption scheme]
Regenerating Codes: A System Perspective
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
The explosion of the amount of data stored in cloud systems calls for more efficient paradigms for redundancy. While replication is widely used to ensure data availability, erasure correcting codes provide a much better trade-off between storage and availability. Regenerating codes are good candidates for they also offer low repair costs in term of network bandwidth. While they have been proven optimal, they are difficult to understand and parameterize. In this paper we provide an analysis of regenerating codes for practitioners to grasp the various trade-offs. More specifically we make two contributions: (i) we study the impact of the parameters by conducting an analysis at the level of the system, rather than at the level of a single device, (ii) we compare the computational costs of various implementations of codes and highlight the most efficient ones. Our goal is to provide system designers with concrete information to help them choose the best parameters and design for regenerating codes.
[Availability, practical, implementation, Maintenance engineering, erasure correcting code, Encoding, Decoding, network bandwidth, Reed-Solomon codes, storage management, adaptive, Systematics, data storage, system, regenerating codes design, Bandwidth, data availability, regenerating codes, cloud computing, cloud system]
Banking on Decoupling: Budget-Driven Sustainability for HPC Applications on EC2 Spot Instances
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Cloud providers are auctioning their excess capacity using dynamically priced virtual instances. These spot instances provide significant savings compared to on-demand or fixed price instances. The users willing to use these resources are asked to provide a maximum bid price per hour, and the cloud provider runs the instances as long as the market price is below the user's bid price. By using such resources, the users are exposed explicitly to failures and need to adapt their applications to provide some level of fault tolerance. In this paper we expose the effect of bidding in the case of virtual HPC clusters composed of spot instances. We describe the interesting effect of uniform versus non-uniform bidding, in terms of failure rate and failure model. We propose an initial attempt to deal with the problem of predicting the runtime of a parallel application under various bidding strategies and various system parameters. We describe the relationship between bidding strategies and programming models. We build a preliminary optimization model that uses real price traces from Amazon Web Services as inputs, as well as instrumented values related to the processing and network capacities of clusters instances on the EC2 services. Our results show preliminary insights into the relationship between non-uniform bidding and application scaling strategies.
[fixed price instance, preliminary optimization model, HPC application, excess capacity, Cloud-based Fault Tolerance, budget driven sustainability, Programming, bidding strategy, Approximation methods, maximum bid price, Cost-aware Optimization models, banking, Fault tolerance, Runtime, programming model, Fault tolerant systems, priced virtual instance, failure rate, EC2 spot instances, parallel application, Spot Instances, Amazon Web service, fault tolerance, Computational modeling, decoupling, Auction-based cloud computing, nonuniform bidding, application scaling strategy, Cloud virtual clusters, failure model, budgeting, sustainable development, Decoupling Parallel Programming Models, Web services, virtual HPC clusters, on demand, Timing, cloud providers, pricing]
On the Feasibility of Byzantine Fault-Tolerant MapReduce in Clouds-of-Clouds
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
MapReduce is a framework for processing large data sets largely used in cloud computing. MapReduce implementations like Hadoop can tolerate crashes and file corruptions, but there is evidence that general arbitrary faults do occur and can affect the correctness of job executions. Furthermore, many individual cloud outages have been reported, raising concerns about depending on a single cloud. We present a MapReduce runtime that tolerates arbitrary faults and runs in a set of clouds at a reasonable cost in terms of computation and execution time. The main challenge is to avoid sending through the internet the huge amount of data that would normally be exchanged between map and reduce tasks.
[large data set processing, Cloud computing, file corruption, Redundancy, MapReduce runtime, cloud outage, clouds-of-clouds, Hadoop, Servers, arbitrary fault tolerance, crash tolerance, job execution correctness, Fault tolerant systems, Logic gates, fault tolerant computing, data handling, Internet, cloud computing, Byzantine fault-tolerant MapReduce]
Byzantine Fault-Tolerant Publish/Subscribe: A Cloud Computing Infrastructure
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
The emerging publish/subscribe communication paradigm for building large-scale distributed event notification systems, has been shown to exhibit excellent performance and scalability characteristics. Moreover, some work also focus on providing reliability and availability guarantees in the face of node crash and link failures. Such publish/subscribe systems are commonly used in cloud computing infrastructures. However, addressing the dependability concern due to malicious attacks or unintentional software errors, which can potentially corrupt the system, has largely been left untouched by researchers. In this paper, we first identify some of the potential problem areas related to Byzantine behavior in the publish/subscribe paradigm. Secondly, we propose several directions of research for designing a Byzantine fault-tolerant publish/subscribe system suitable for use as a cloud computing infrastructure.
[node crash, Cloud computing, Protocols, malicious attacks, Subscriptions, software reliability, reliability, availability, system recovery, Fault tolerance, Fault tolerant systems, large-scale distributed event notification systems, cloud computing infrastructure, performance characteristics, cloud computing, publish-subscribe communication paradigm, software performance evaluation, scalability characteristics, message passing, dependability, Computer crashes, unintentional software errors, Topology, software fault tolerance, security of data, link failures, Byzantine fault tolerant publish-subscribe system design]
Energy Efficient Hadoop Using Mirrored Data Block Replication Policy
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
MapReduce scheme has became the state of the art in parallel processing of vast amount of data in distributed systems. Hadoop, as a popular open-source implementation of this technique, makes use of data block replication mechanism to provide a reliable and fault-tolerant design. To maintain data availability, Hadoop takes into account the possibilities of node and rack failures. Hence, it stores multiple copies of each data block to ensure availability and reliability. The current data block placement policy is to randomly distribute the replicas on all servers, satisfying some constraints such as preventing storage of two replicas of a data block on a single node. Our study proposes an efficient placement policy for data block replicas, which can reduce the consumed energy in data centers. The proposed policy is built upon the covering subset (CovSet) method. The effectiveness of the proposed approach is confirmed through simulations. Also, our experiments show that the proposed method becomes more effective whenever the average number of data blocks per server increases, which corresponds to the actual conditions in practice.
[Measurement, datacenter energy consumption, public domain software, CovSet method, mirrored data block replication policy, Servers, parallel processing, system recovery, reliable design, power aware computing, covering subset method, Distributed databases, open-source implementation, data availability, energy efficiency, distributed systems, data block replication mechanism, energy consumption, server replicas, Availability, fault-tolerant design, Peer to peer computing, energy efficient Hadoop, data block replicas, Hadoop, data block placement policy, data block replication, computer centres, rack failures, MapReduce scheme, energy conservation, Data models, fault tolerant computing]
Security Problems of Platform-as-a-Service (PaaS) Clouds and Practical Solutions to the Problems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Cloud computing is a promising approach for the efficient use of computational resources. It delivers computing as a service rather than a product for a fraction of the cost. However, security concerns prevent many individuals and organizations from using clouds despite its cost effectiveness. Resolving security problems of clouds may alleviate concerns and increase cloud usage; in consequence, it may decrease overall costs spent for the computational devices and infrastructures. This paper particularly focuses on the Platform-as-a-Service (PaaS) clouds. Security of PaaS clouds is considered from multiple perspectives including access control, privacy and service continuity while protecting both the service provider and the user. Security problems of PaaS clouds are explored and classified. Countermeasures are proposed and discussed. The achieved solutions are intended to be the rationales for future PaaS designs and implementations.
[computational infrastructures, Protocols, PaaS clouds, practical solutions, computational devices, security problems, Authorization, Privacy, computational resources, Authentication, service continuity, authorisation, platform-as-a-service, data privacy, access control, Cryptography, cloud computing, service provider protection]
Versatile Key Management for Secure Cloud Storage
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Not only does storing data in the cloud utilize specialized infrastructures facilitating immense scalability and high availability, but it also offers a convenient way to share any information with user-defined third-parties. However, storing data on the infrastructure of commercial third party providers, demands trust and confidence. Simple approaches, like merely encrypting the data by providing encryption keys, which at most consist of a shared secret supporting rudimentary data sharing, do not support evolving sets of accessing clients to common data. Based on approaches from the area of stream-encryption, we propose an adaption for enabling scalable and flexible key management within heterogeneous environments like cloud scenarios. Representing access-rights as a graph, we distinguish between the keys used for encrypting hierarchical data and the encrypted updates on the keys enabling flexible join-/leave-operations of clients. This distinction allows us to utilize the high availability of the cloud as updating mechanism without harming confidentiality. Our graph-based key management results in an adaption of nodes related to the changed key. The updates on the keys again continuously create an overhead related to the number of these updated nodes. The proposed scalable approach utilizes cloud-based infrastructures for confidential data and key sharing in collaborative workflows supporting variable client-sets.
[Availability, stream-encryption, Cloud computing, access-rights, Scalability, versakey, Materials, secure cloud storage, confidentiality, cryptography, Encryption, encryption keys, key management, versatile key management, cloud, storage management, data storage, encryption, Permission, cloud computing]
Securing a Wireless Networked Control System Using Information Fusion
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Security of a wireless sensor network practically governs its usability in several applications. Especially, in applications like Industrial control systems which use NCS and SCADA systems, the security affects the stability of the system. We propose to use an information fusion scheme which allows us to profile the different attacks in wireless sensor networks and study their affects on the control systems stability and feedback. We make use of the Bayesian Networks to obtain hypotheses as outputs which form the decisions. These decisions are made based on the feature extraction and estimation process of the entire information fusion scheme. This allows us to make sure that the WNCS works smoothly without any aberrations even under the influence of security attacks. In this paper, we go on to explain the process that we employ in ensuring the stability and security of the system.
[industrial control system, Networked control systems, wireless sensor networks, sensor fusion, Security, wireless networked control system, telecommunication computing, feedback, supervisory control and data acquisition systems, security attack, Sensors, belief networks, control engineering computing, Bayesian network, stability, Job shop scheduling, information fusion, telecommunication control, control system stability, Stability analysis, Wireless sensor networks, control system feedback, Bayesian methods, wireless sensor network security, SCADA system, networked control systems, NCS system]
Exploring Compile Time Caching of Explicit Queries in Programming Codes
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Object oriented programming languages raised the level of abstraction by incorporating first class query constructs explicitly in the program codes. These query constructs allow programmers to express operations over collections as object queries and also provide optimal query execution utilizing query optimization strategies from domain of databases. However, when a query is repeated in the program, it is executed afresh. This paper presents an approach to reduce the run time execution of programs involving explicit queries by caching the results of repeated queries and incrementally maintaining the cached results. We propose determination of cache entries at compile time by performing the program analysis. We also describe the cache heuristics for determining which queries to cache.
[Java, object-oriented programming, program diagnostics, programming code, object oriented programming language, abstraction level, cache storage, object query, optimal query execution, program compilers, query optimization strategy, run time execution, query processing, compile time cache heuristics, Query processing, database domain, program analysis, object-oriented languages, query construct, Reliability, Object oriented programming]
Three Point Encryption (3PE): Secure Communications in Delay Tolerant Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Mobile ad hoc networks (MANET) are a subset of Delay Tolerant Networks (DTNs) composed of several mobile devices. These dynamic environments makes conventional security algorithms unreliable, nodes that are far apart may not have access to the other's public key, making secure message exchange difficult. Other security methods rely on requesting the key from a trusted third party, which can be unavailable in DTN. The purpose of this paper is to introduce two message security algorithms capable of delivering messages securely against either eavesdropping or manipulation. The first algorithm, Chaining, uses multiple midpoints to re-encrypt the message for the destination node. The second, Fragmenting, separates the message key into pieces that are both routed and secured independently from each other. Both techniques have improved security in hostile environments. This improvement has a performance trade-off, however, reducing the delivery ratio and increasing the delivery time.
[Algorithm design and analysis, telecommunication security, fragmenting algorithm, message key separation, Keyless Encryption, Mobile ad hoc Networks, destination node, Encryption, Security, hostile environments, performance improvement, Delay, Mobile ad hoc networks, dynamic environments, public key cryptography, trusted third-party, mobile ad hoc networks, message public key exchange security, Delay Tolerant Networks, delay tolerant network communication security, chaining algorithm, 3PE, delay tolerant networks, MANET, Public key, telecommunication network routing, message key routing, delivery ratio reduction, three-point encryption, mobile devices, message reencryption, trusted computing, delivery time improvement, DTN]
TAIRO: Trust-Aware Automatic Incremental Routing for Opportunistic Resource Utilization Networks
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Oppnets, or Opportunistic Resource Utilization Networks, are a kind of ad hoc computer network that grows by discovering and integrating previously unencountered devices or systems with desirable resources (Oppnets are a predecessor and a generalization of opportunistic networks proposed by others). We present work in progress on TAIRO or Trust-aware Automatic Incremental Routing for Oppnets. TAIRO is based on the innovative AIR routing scheme. AIR uses specially designed prefix labels to manage routing in an ad hoc network, which is especially well-suited for meeting the goals of Oppnets.
[trust, trust-aware routing, automatic incremental routing, TAIRO, Routing, innovative AIR routing scheme, Ad hoc networks, AIR, Security, Oppnets, trust-aware automatic incremental routing, ad hoc computer network, routing, resource allocation, opportunistic resource utilization networks, oppnets, opportunistic resource utilization metworks, telecommunication network routing, MANETs, Routing protocols, Reliability, Resource management, ad hoc networks, Mobile computing]
Strategies for Reliable, Cloud-Based Distributed Real-Time and Embedded Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Cloud computing enables elastic and dynamic resource provisioning while providing cost-effective computing solutions. However, while cloud computing provides customers access to scalable and elastic resources, it does not guarantee the user's expectations of Quality of Service (QoS). This is because a number of customers share resources in the cloud infrastructure simultaneously: compute-intensive processes and network traffic associated with one customer often impact the performance of other applications operated on the same infrastructure in unexpected ways. The inability of the cloud to enforce QoS and provide execution guarantees prevents cloud computing from becoming useful for distributed, real-time and embedded (DRE) systems. Providing the required levels of service to support DRE systems in the cloud is complicated for a variety of reasons: (1) lack of effective monitoring that prevents timely auto-scaling needed for DRE systems, (2) hyper visors and data-center networks that do not support real-time scheduling of resources, and (3) absence of efficient and predictable fault tolerant mechanisms with acceptable overhead and consistency. This paper describes ongoing and proposed doctoral research to address these challenges.
[fault tolerant mechanism, Cloud computing, embedded system reliability, Quality of service, dynamic resource provisioning, Fault tolerance, resource allocation, Cloud Computing, QoS, embedded systems, compute-intensive process, Real-time systems, cloud computing, Monitoring, Availability, quality of service, data-center network, computer centres, software fault tolerance, Quality of Service, elastic resource provisioning, network traffic, Virtual machine monitors, resource sharing, distributed real-time system reliability, cost-effective computing, hyper visor]
Towards Reliable Communication in Intelligent Transportation Systems
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Cyber physical systems (CPS) are increasingly seen as a way to provide solutions for societal benefits. For these systems to become widely adopted, reliability of these systems is a key requirement because CPS appear in safety- and mission critical applications. To bring about the reliability challenges and the scientific principles behind developing solutions for CPS reliability, I am focusing on two CPS domains: intelligent transportation system and reconfigurable conveyor systems. Intelligent transportation system is a system where vehicles collaborate together to improve road safety, alleviate congestion thereby helping the environment, and providing a better travel experience. Reconfigurable conveyor systems represent a class of systems in advanced manufacturing that will make manufacturing agile while reducing the operating costs. This report presents an overview of recent work that highlights the challenges we face and their possible solutions in making communication in ITS reliable. It also discusses the reliability issues in reconfigurable conveyor systems. We then outline the future directions.
[Protocols, Roads, safety-critical software, CPS, vehicles, road safety, reconfigurable conveyor systems, Vehicles, road vehicles, advanced manufacturing, telecommunication network reliability, Safety, publish-subscribe, cyber physical systems, vehicular ad hoc networks, Intelligent transportation, conveyors, Cyber-physical systems, intelligent transportation systems, safety-critical applications, Vehicular ad hoc networks, reliable communication, mission critical applications, automated highways, Reliability]
Reliable On-Chip Memory Design for CMPs
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Aggressive technology scaling in deep sub micron regime makes chips more susceptible to failures. This causes multiple realibility challenges in the design of modern chips, including manufacturing defects, wear-out, and parametric variations. With increasing area occupied by different on-chip memories in modern computing platforms such as Chip Multi-Processors (CMPs), memory reliability becomes a challenging issue. Traditional on-chip memory reliability techniques (e.g., ECC) incur significant power and performance overheads. To tackle such challenges, my research introduces several designs for fault-tolerance of both L1 and L2 cache memories in uni-core processors [1], Last-level Cache (LLC) in CMPs [3][4], and LLC in Networks-on-Chip (NoCs) [2].
[fault diagnosis, LLC, NoC, fault-tolerance, integrated circuit reliability, deep submicron regime, networks-on-chip, Reliability engineering, cache storage, parametric variation, failure analysis, manufacturing defect, power overhead, Fault tolerant systems, integrated circuit design, reliable on-chip memory design, chip multiprocessor, On-chip memory, System-on-a-chip, L2 cache memory, L1 cache memory, multiprocessing systems, network-on-chip, Multicore processing, wear-out, ECC, Redundancy, CMP, unicore processor, microprocessor chips, aggressive technology scaling, performance overhead, last-level cache, realibility, modern computing platform, on-chip memory reliability technique, fault tolerant computing, chip failure, Reliability]
[Publisher's information]
2012 IEEE 31st Symposium on Reliable Distributed Systems
None
2012
Provides a listing of current committee members and society officers.
[]
Message from the General Chair
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Presents the introductory welcome message from the conference proceedings.
[]
Message from the Technical Program Co-chairs
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Presents the introductory welcome message from the conference proceedings.
[]
Organizing Committee
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Provides a listing of current committee members and society officers.
[]
A Fault-Tolerant Routing Algorithm Design for On-Chip Optical Networks
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Optical networks have been considered for on-chip communications due to its advantages on bandwidth density, power efficiency and propagation speed over the electrical counterpart. However, the major optical device-micro ring resonator is very sensitive to manufacturing errors and temperature fluctuations, which results in the bandwidth loss or even the failure of optical link. Thus, this paper proposes a fault-tolerant and deadlock-free routing algorithm to improve the reliability of on-chip optical network without requiring additional virtual channel. In addition, a path selection mechanism taking account of the actual bandwidth of the optical link affected by fabrication and temperature variations is implemented in the routing unit. The simulation results show that compared to the conventional fault-tolerant routing methods, our routing algorithm can improve the transmission latency and throughput of the network under static and dynamic link faults by up to 51% and 22%, respectively.
[Optical network, Ports (Computers), Deadlock-free adaptive routing, onchip communications, Fault tolerance, Fault tolerant systems, deadlock free routing algorithm, Bandwidth, telecommunication network reliability, Optical fiber communication, temperature fluctuations, Fault-tolerant routing, power efficiency, electrical counterpart, optical device micro ring resonator, Routing, bandwidth density, onchip optical networks, onchip optical network reliability, virtual channel, telecommunication network routing, fault tolerant routing algorithm design, propagation speed, System recovery, optical fibre networks, Reliability, manufacturing errors]
PIP: Privacy and Integrity Preserving Data Aggregation in Wireless Sensor Networks
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
With the exponential rise of pervasive computing applications, data privacy has become much more of an important issue than before. When data is aggregated at each hop in a sensor network, it becomes harder to protect its privacy. A number of privacy preserving data aggregation algorithms have recently appeared for wireless sensor networks (WSNs), very few of them however also address the issue of data integrity along with privacy. Data privacy and integrity are two contrasting objectives to achieve in general. In a privacy preserved data aggregation, it becomes easier for an attacker to inject false data hence, we suggest that both privacy and integrity of data should be treated together. In this paper, we present an energy efficient, privacy preserving data aggregation algorithm which also preserves data integrity in WSNs. We analyze the security of the algorithm and provide proofs for confidentiality and integrity. We enhance this algorithm further to localize, to a certain degree, the corrupt aggregator. We provide the results of our implementation of the algorithm on TelosB motes, illustrating that both the computational overhead and the energy consumption are very low. Finally, we compare our algorithm with other schemes having similar objectives demonstrating that our algorithm performs better in terms of band with usage and energy consumption in a WSN environment.
[Data privacy, Base stations, wireless sensor networks, wireless sensor network, Data aggregation, data integrity, Wireless sensor networks, WSN, Aggregates, privacy and integrity preserving data aggregation, PIP, TelosB mote, Polynomials, data privacy, Secret sharing, Peer-to-peer computing, Cryptography, energy consumption]
Virtual Synchrony Guarantees for Cyber-physical Systems
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
By integrating computational and physical elements through feedback loops, CPSs implement a wide range of safety-critical applications, from high-confidence medical systems to critical infrastructure control. Deployed systems must therefore provide highly dependable operation against unpredictable real-world dynamics. However, common CPS hardware-comprising battery-powered and severely resource-constrained devices interconnected via low-power wireless-greatly complicates attaining the required communication guarantees. VIRTUS fills this gap by providing atomic multicast and view management atop resource-constrained devices, which together provide virtually synchronous executions that developers can leverage to apply established concepts from the dependable distributed systems literature. We build VIRTUS upon an existing best-effort communication layer, and formally prove the functional correctness of our mechanisms. We further show, through extensive real-world experiments, that VIRTUS incurs a limited performance penalty compared with best-effort communication. To the best of our knowledge, VIRTUS is the first system to provide virtual synchrony guarantees atop resource-constrained CPS hardware.
[Schedules, Protocols, atomic multicast management, Receivers, safety-critical software, CPS, distributed processing, atomic view management, Computer crashes, Wireless communication, virtual synchrony guarantees, cyber-physical systems, functional correctness, resource-constrained devices, best-effort communication layer, virtually synchronous executions, Sensors, Reliability, VIRTUS]
Fighting Uncertainty in Highly Dynamic Wireless Sensor Networks with Probabilistic Models
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Real-time operation in Wireless Sensor Networks (WSNs) is conditioned not only by the current technological level (e.g., limited computing power) but also inherently by the target problem itself: WSNs are required to operate in very open and uncertain environments, subject to external radio interferences, highly dynamic network load, etc. Current WSN solutions either provide only best-effort real-time guarantees or make (generally implicit) assumptions on the dynamics of the open environment. These assumptions, in turn, are either very relaxed (i.e., compatible only with undemanding real-time requirements) or very hard to justify. When dealing with WSNs supporting highly dynamic applications and operating environments (e.g., media streaming, robot control, vehicle coordination, etc.) this problem cannot be ignored. Accordingly, we argue for, and show the efficacy of, using probabilistic models to characterize dynamic WSN QoS, which is the first step to tackle the problem head on. Using our network monitoring technique, we demonstrate that it is possible to meet probabilistic real-time objectives.
[802.15.4, wireless sensor networks, Quality of service, adaptation, Vehicle dynamics, lightweight, WSN solutions, real-time, QoS, Real-time systems, Monitoring, dependability, probability, external radio interferences, network monitoring technique, Probabilistic logic, real-time operation, quality of service, Wireless Sensor Networks, non-parametric, probabilistic models, Wireless sensor networks, Timing, dynamic WSN QoS, dynamic wireless sensor networks]
A Distributed Polling with Probabilistic Privacy
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
In this paper, we present PDP, a distributed polling protocol that enables a set of participants to gather their opinion on a common interest without revealing their point of view. PDP does not rely on any centralized authority or on heavyweight cryptography. PDP is an overlay-based protocol where a subset of participants may use a simple sharing scheme to express their votes. In a system of M participants arranged in groups of size N where at least 2k-1 participants are honest, PDP bounds the probability for a given participant to have its vote recovered with certainty by a coalition of B dishonest participants by &#x03C0;(B/N)(k+1), where &#x03C0; is the proportion of participants splitting their votes, and k a privacy parameter. PDP bounds the impact of dishonest participants on the global outcome by 2(k&#x03B1; + BN), where represents the number of dishonest nodes using the sharing scheme.
[Protocols, probabilistic privacy parameter, dishonest participant coalition, sharing scheme, probability, Distributed Polling, vote splitting, Probabilistic logic, Complexity theory, PDP bounds, computer network security, Privacy, Collaboration, overlay networks, distributed polling protocol, data privacy, Cryptography, Reliability, protocols, overlay-based protocol, dishonest nodes, PDP]
Consensus with Unknown Participants in Shared Memory
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
The shared memory model matches important classes of systems deployed over dynamic networks, as for example, fault-tolerant and high available data centric services. Consensus is a fundamental building block able to realize such reliable distributed systems. Unlike the classical setting where the full set of participants and their identities are known to every process, dynamic networks preclude such global knowledge to be available. In this paper, we investigate and present protocols to solve fault-tolerant consensus in an environment with unknown participants that communicate via shared memory.
[Protocols, fault-tolerant consensus, Dynamic Networks, Computer crashes, high available data centric services, Registers, Consensus, shared memory model, global knowledge, Fault tolerance, Message passing, Detectors, dynamic networks, shared memory systems, distributed systems, fault tolerant computing, fault-tolerant data centric services, Shared Memory, Fault Tolerance]
Bounded Delay in Byzantine-Tolerant State Machine Replication
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
The paper proposes a new state machine replication protocol for the partially synchronous system model with Byzantine faults. The algorithm, called BFT-Mencius, guarantees that the latency of updates initiated by correct processes is eventually upper-bounded, even in the presence of Byzantine processes. BFTMencius is based on a new communication primitive, Abortable Timely Announced Broadcast (ATAB), and does not use signatures. We evaluate the performance of BFT-Mencius in cluster settings, and show that it provides bounded latency and good throughput, being comparable to the state-of-the-art algorithms such as PBFT and Spinning in fault-free configurations and outperforming them under performance attacks by Byzantine processes.
[Algorithm design and analysis, Protocols, update latency, abortable timely announced broadcast, Throughput, ATAB, Servers, finite state machines, communication primitive, bounded delay, State Machine Replication, partially synchronous system model, BFT-Mencius, cluster setting, protocols, PBFT, performance evaluation, fault-free configuration, Bounded Delay, performance attack, state machine replication protocol, bounded latency, Performance Failures, delays, Byzantine-tolerant state machine replication, fault tolerant computing, Byzantine process, Delays, Byzantine Fault Tolerance, Spinning, Reliability, Byzantine fault]
Stochastic Analysis on RAID Reliability for Solid-State Drives
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Solid-state drives (SSDs) have been widely deployed in desktops and data centers. However, SSDs suffer from bit errors, and the bit error rate is time dependent since it increases as an SSD wears down. Traditional storage systems mainly use parity-based RAID to provide reliability guarantees by striping redundancy across multiple devices, but the effectiveness of RAID in SSDs remains debatable as parity updates aggravate the wearing and bit error rates of SSDs. In particular, an open problem is that how different parity distributions over multiple devices, such as the even distribution suggested by conventional wisdom, or uneven distributions proposed in recent RAID schemes for SSDs, may influence the reliability of an SSD RAID array. To address this fundamental problem, we propose the first analytical model to quantify the reliability dynamics of an SSD RAID array. Specifically, we develop a "non-homogeneous" continuous time Markov chain model, and derive the transient reliability solution. We validate our model via trace-driven simulations and conduct numerical analysis to provide insights into the reliability dynamics of SSD RAID arrays under different parity distributions and subject to different bit error rates and array configurations. Designers can use our model to decide the appropriate parity distribution based on their reliability requirements.
[Error analysis, SSD RAID array reliability dynamics, nonhomogeneous continuous time Markov chain model, data centers, transient reliability solution, numerical analysis, Aging, solid-state drives, CTMC, Numerical models, redundancy, Transient Analysis, Transient analysis, stochastic analysis, parity distribution, reliability requirements, RAID, bit error rate, trace-driven simulation, Solid-state Drives, desktops, Markov processes, array configurations, Arrays, Reliability, disc drives]
A Distributed Abstraction Algorithm for Online Predicate Detection
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Analyzing a distributed computation is a hard problem in general due to the combinatorial explosion in the size of the state-space with the number of processes in the system. By abstracting the computation, unnecessary state explorations can be avoided. Computation slicing is an approach for abstracting distributed computations with respect to a given predicate. We focus on regular predicates, a family of predicates that covers many commonly used predicates for runtime verification. The existing algorithms for computation slicing are centralized - a single process is responsible for computing the slice in either offline or online manner. In this paper, we present first distributed online algorithm for computing the slice of a distributed computation with respect to a regular predicate. Our algorithm distributes the work and storage requirements across the system, thus reducing the space and computation complexity per process.
[distributed abstraction algorithm, program verification, computation slicing, Predicate Detection, Computational modeling, Lattices, distributed computation, distributed online algorithm, Vectors, Distributed Algorithms, online predicate detection, Runtime, distributed algorithms, computation complexity reduction, Silicon, space complexity reduction, storage requirements, runtime verification, Distributed algorithms, program slicing, Clocks, computational complexity]
Linking Resource Usage Anomalies with System Failures from Cluster Log Data
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Bursts of abnormally high use of resources are thought to be an indirect cause of failures in large cluster systems, but little work has systematically investigated the role of high resource usage on system failures, largely due to the lack of a comprehensive resource monitoring tool which resolves resource use by job and node. The recently developed TACC_Stats resource use monitor provides the required resource use data. This paper presents the ANCOR diagnostics system that applies TACC_Stats data to identify resource use anomalies and applies log analysis to link resource use anomalies with system failures. Application of ANCOR to first identify multiple sources of resource anomalies on the Ranger supercomputer, then correlate them with failures recorded in the message logs and diagnosing the cause of the failures, has identified four new causes of compute node soft lockups. ANCOR can be adapted to any system that uses a resource use monitor which resolves resource use by job.
[Algorithm design and analysis, TACC_Stats data, Correlation, Data mining, parallel machines, resource allocation, Large clusters, Resource Anomalies and Failures, cluster log data, Monitoring, Supercomputers, Lustre file-system, comprehensive resource monitoring tool, Linux O/S, system failures, TACC_Stats resource, ranger supercomputer, ANCOR diagnostics system, message logs, Cluster log data, resource usage anomalies, Software, fault tolerant computing, cluster systems, Resource management]
Automatic Problem Localization via Multi-dimensional Metric Profiling
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Debugging today's large-scale distributed applications is complex. Traditional debugging techniques such as breakpoint-based debugging and performance profiling require a substantial amount of domain knowledge and do not automate the process of locating bugs and performance anomalies. We present Orion, a framework to automate the problem-localization process in distributed applications. From a large set of metrics, Orion intelligently chooses important metrics and models the application's runtime behavior through pair wise correlations of those metrics in the system, within multiple non-overlapping time windows. When correlations deviate from those of a learned correct model due to a bug, our analysis pinpoints the metrics and code regions (class and method within it) that are most likely associated with the failure. We demonstrate our framework with several real-world failure cases in distributed applications such as: HBase, Hadoop DFS, a campus-wide Java application, and a regression testing framework from IBM. Our results show that Orion is able to pinpoint the metrics and code regions that developers need to concentrate on to fix the failures.
[Measurement, Algorithm design and analysis, program debugging, Correlation, debugging aids, distributed processing, problem-localization process, large-scale distributed applications, IBM, nonoverlapping time windows, debugging techniques, breakpoint-based debugging, Hardware, ORION, statistical testing, software performance evaluation, campus-wide Java application, tracing, bug locating process automation, Debugging, HBase, automatic problem localization, regression testing framework, multidimensional metric profiling, diagnostics, Hadoop DFS, Computer bugs, performance metrics, performance anomalies, Principal component analysis]
Efficient Verification of Distributed Protocols Using Stateful Model Checking
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
This paper presents efficient model checking of distributed software. Key to the achieved efficiency is a novel stateful model checking strategy that is based on the decomposition of states into a relevant and an auxiliary part. We formally show this strategy to be sound, complete, and terminating for general finite-state systems. As a case study, we implement the proposed strategy within Basset/MP-Basset, a model checker for message-passing Java programs. Our evaluation with actual deployed fault-tolerant message-passing protocols shows that the proposed stateful optimization is able to reduce model checking time and memory by up to 69% compared to the naive stateful search, and 39% compared to partial-order reduction.
[MP-Basset, Java, Protocols, message passing, program verification, distributed protocols, finite-state systems, Virtual machining, Optimization, Fault tolerance, fault-tolerant message-passing protocols, Fault tolerant systems, stateful model checking strategy, distributed software, Model checking, efficient verification, message-passing Java programs, partial-order reduction]
Validity-Based Failure Algebra for Distributed Sensor Systems
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Distributed applications dealing with data from networked sensors need some indication about the quality of remote information. This paper describes how to derive a dynamic validity value that represents a measure for the confidence in remote sensor data. In contrast to conventional systems which treats a typical processing chain as a whole, this paper describes how individual characteristic of sensing, detection and filter mechanisms can be assessed and how this assessment can be evaluated to a single validity value. In particular the paper defines respective operations combining the run-time validity estimates of every stage in a processing chain. The combination is evaluated by rules as part of a failure algebra. The paper presents the generation of an application specific validity value which is finally demonstrated in a robotic application.
[Uncertainty, distributed processing chain, remote sensor data, filtering theory, remote sensing, detection mechanism, Vectors, algebra, sensing mechanism, distributed sensors, Intelligent sensors, sensor systems, failure algebra, Detectors, validity-based failure algebra, Robot sensing systems, Data models, validity estimation, distributed sensor system, filter mechanism]
Rigorous Performance Evaluation of Self-Stabilization Using Probabilistic Model Checking
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
We propose a new metric for effectively and accurately evaluating the performance of self-stabilizing algorithms. Self-stabilization is a versatile category of fault-tolerance that guarantees system recovery to normal behavior within a finite number of steps, when the state of the system is perturbed by transient faults (or equally, the initial state of the system can be some arbitrary state). The performance of self-stabilizing algorithms is conventionally characterized in the literature by asymptotic computation complexity. We argue that such characterization of performance is too abstract and does not reflect accurately the realities of deploying a distributed algorithm in practice. Our new metric for characterizing the performance of self-stabilizing algorithms is the expected mean value of recovery time. Our metric has several crucial features. Firstly, it encodes accurate average case speed of recovery. Secondly, we show that our evaluation method can effectively incorporate several other parameters that are of importance in practice and have no place in asymptotic computation complexity. Examples include the type of distributed scheduler, likelihood of occurrence of faults, the impact of faults on speed of recovery, and network topology. We utilize a deep analysis technique, namely, probabilistic model checking to rigorously compute our proposed metric. All our claims are backed by detailed case studies and experiments.
[Measurement, Algorithm design and analysis, distributed scheduler, deep analysis technique, Performance evaluation, probabilistic model checking, program verification, fault-tolerance, self-stabilization, asymptotic computation complexity, system recovery, Model checking, scheduling, Distributed algorithms, software performance evaluation, Self-stabilization, probability, performance evaluation, Probabilistic logic, Educational institutions, transient faults, network topology, software fault tolerance, self-stabilizing algorithms, distributed algorithms, formal methods, Markov processes, fault occurrence likelihood, computational complexity]
Non-monotonic Snapshot Isolation: Scalable and Strong Consistency for Geo-replicated Transactional Systems
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Modern cloud systems are geo-replicated to improve application latency and availability. Transactional consistency is essential for application developers; however, the corresponding concurrency control and commitment protocols are costly in a geo-replicated setting. To minimize this cost, we identify the following essential scalability properties: (i) only replicas updated by a transaction T make steps to execute T; (ii) a read-only transaction never waits for concurrent transactions and always commits; (iii) a transaction may read object versions committed after it started; and (iv) two transactions synchronize with each other only if their writes conflict. We present Non-Monotonic Snapshot Isolation (NMSI), the first strong consistency criterion to allow implementations with all four properties. We also present a practical implementation of NMSI called Jessy, which we compare experimentally against a number of well-known criteria. Our measurements show that the latency and throughput of NMSI are comparable to the weakest criterion, read-committed, and between two to fourteen times faster than well-known strong consistencies.
[Protocols, Ground penetrating radar, Concurrency Control, Scalability, Distributed Databases, weakest criterion, History, cloud systems, Distributed Transactional Systems, geo-replicated transactional systems, Silicon, Multiversioning, cloud computing, protocols, Jessy, commitment protocols, application developers, strong consistencies, nonmonotonic snapshot isolation, Vectors, transactional consistency, Synchronization, NMSI, scalability properties, concurrency control, Partial Replication, application latency]
Clock-SI: Snapshot Isolation for Partitioned Data Stores Using Loosely Synchronized Clocks
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Clock-SI is a fully distributed protocol that implements snapshot isolation (SI) for partitioned data stores. It derives snapshot and commit timestamps from loosely synchronized clocks, rather than from a centralized timestamp authority as used in current systems. A transaction obtains its snapshot timestamp by reading the clock at its originating partition and Clock-SI provides the corresponding consistent snapshot across all the partitions. In contrast to using a centralized timestamp authority, Clock-SI has availability and performance benefits: It avoids a single point of failure and a potential performance bottleneck, and improves transaction latency and throughput. We develop an analytical model to study the trade-offs introduced by Clock-SI among snapshot age, delay probabilities of transactions, and abort rates of update transactions. We verify the model predictions using a system implementation. Furthermore, we demonstrate the performance benefits of Clock-SI experimentally using a micro-benchmark and an application-level benchmark on a partitioned key-value store. For short read-only transactions, Clock-SI improves latency and throughput by 50% by avoiding communications with a centralized timestamp authority. With a geographically partitioned data store, Clock-SI reduces transaction latency by more than 100 milliseconds. Moreover, the performance benefits of Clock-SI come with higher availability.
[transaction processing, Protocols, distributed processing, snapshot isolation, Distributed databases, application-level benchmark, distributed transactions, transaction latency, loosely synchronized clocks, Silicon, microbenchmark, protocols, software performance evaluation, transaction delay probabilities, centralized timestamp authority, probability, Synchronization, distributed protocol, partitioned data stores, partitioned data, read-only transactions, synchronisation, clocks, transaction throughput, clock-SI, partitioned key-value store, Delays, benchmark testing, snapshot timestamp, Clocks]
Bumper: Sheltering Transactions from Conflicts
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
This paper addresses the issue of maximizing the efficiency and scalability of distributed transactional platforms, by introducing Bumper, a set of innovative techniques to minimize aborts of transactions in high-contention scenarios. At its core, Bumper relies on two key ideas: (1) sparing update transactions from spurious aborts when they access concurrently updated data, by attempting to serialize them in the past via a novel distributed concurrency control scheme that we call Distributed Time-Warping (DTW), and (2) avoiding aborts due to contention hot spots (that cannot be tackled by DTW) via a novel programming abstraction, called delayed actions, which allows to efficiently serialize, in an abort-free fashion, the execution of conflict-prone data manipulations. The techniques used in Bumper can be applied to a wide variety of transactional replication protocols to enhance their performance in contention intensive workloads. In this paper we show how they can be integrated with SCORe, a recent, highly-scalable genuine partial replication protocol. By means of an extensive evaluation using well-known benchmarks and a cluster of 160 nodes, we show that Bumper can boost performance up to 3x in conflict-intensive workloads, while imposing negligible (2.5%) overheads in uncontended scenarios.
[transaction processing, Bumper, delayed actions, Protocols, Scalability, DTW, distributed processing, conflict-intensive workload, high-contention scenario, History, Concurrent computing, concurrently updated data access, Distributed databases, abort minimization, programming abstraction, Computational modeling, performance boost, distributed transactional platform, update transaction, transactional replication protocol, distributed time-warping, Concurrency control, conflict-prone data manipulation execution, serialization, SCORehighly-scalable genuine partial replication protocol, contention intensive workload, concurrency control, distributed concurrency control, negligible overhead]
Analysis of Malware Propagation in Twitter
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Malware propagation in social networks is a potential risk that has not been well-studied yet as there are no formal threat models for social networks. In this paper we investigate the vulnerability and cost of spreading malware via Twitter. Towards this end we present three specific attack scenarios targeted for Twitter and systematically analyze the cost of staging each of these attacks. Our analysis presents the first step for understanding the threats on the security of a class of social networks. We identify the attack related parameters and verify these parameters by testing the attack on a Net Logo based simulator. Our analysis indicates that the cost of staging attacks to infect users of Twitter is low and that the proposed attack scenarios are plausible. Further, even with a low degree of connectivity and a low probability of clicking links, Twitter and its structure can be exploited by such attacks to infect many users with malware.
[invasive software, Twitter, link clicking probability, social network, Analytical models, connectivity degree, security, formal verification, attack testing, malware spreading, Malware, Mathematical model, Probabilistic logic, vulnerability, malware propagation analysis, potential risk, Grippers, Net Logo based simulator, attack scenario, threat understanding, Malware Propagation, Analysis, attack staging cost, attack related parameter identification, social networking (online), Twitter users infection, formal threat model, parameter verification]
Adaptive Anomaly Identification by Exploring Metric Subspace in Cloud Computing Infrastructures
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Cloud computing has become increasingly popular by obviating the need for users to own and maintain complex computing infrastructures. However, due to their inherent complexity and large scale, production cloud computing systems are prone to various runtime problems caused by hardware and software faults and environmental factors. Autonomic anomaly detection is a crucial technique for understanding emergent, cloud-wide phenomena and self-managing cloud resources for system-level dependability assurance. To detect anomalous cloud behaviors, we need to monitor the cloud execution and collect runtime cloud performance data. These data consist of values of performance metrics for different types of failures, which display different correlations with the performance metrics. In this paper, we present an adaptive anomaly identification mechanism that explores the most relevant principal components of different failure types in cloud computing infrastructures. It integrates the cloud performance metric analysis with filtering techniques to achieve automated, efficient, and accurate anomaly identification. The proposed mechanism adapts itself by recursively learning from the newly verified detection results to refine future detections. We have implemented a prototype of the anomaly identification system and conducted experiments in an on-campus cloud computing environment and by using the Google data center traces. Our experimental results show that our mechanism can achieve more efficient and accurate anomaly detection than other existing schemes.
[Measurement, production cloud computing system, Cloud computing, complexity, Correlation, software fault, on-campus cloud computing environment, Servers, cloud execution monitor, system recovery, detection result verification, formal verification, resource allocation, cloud computing infrastructure, Failure detection, cloud computing, learning (artificial intelligence), adaptive mechanism, failure type, emergent cloud-wide phenomena, recursive learning, environmental factor, metric subspace exploration, Time series analysis, system-level dependability assurance, Virtual machining, hardware fault, adaptive anomaly identification, Learning algorithms, Virtual machine monitors, runtime cloud performance data collection, complex computing infrastructure, Google data center traces, runtime problem, anomalous cloud behavior detection, filtering technique, Autonomic management, system monitoring, anomaly identification system, fault tolerant computing, Dependable systems, cloud performance metric analysis, self-managing cloud resource]
A Unified Framework for Measuring a Network's Mean Time-to-Compromise
2013 IEEE 32nd International Symposium on Reliable Distributed Systems
None
2013
Measuring the mean time-to-compromise provides important insights for understanding a network's weaknesses and for guiding corresponding defense approaches. Most existing network security metrics only deal with the threats of known vulnerabilities and cannot handle zero day attacks with consistent semantics. In this paper, we propose a unified framework for measuring a network's mean time-to-compromise by considering both known, and zero day attacks. Specifically, we first devise models of the mean time for discovering and exploiting individual vulnerabilities. Unlike existing approaches, we replace the generic state transition model with a more vulnerability-specific graphical model. We then employ Bayesian networks to derive the overall mean time-to-compromise by aggregating the results of individual vulnerabilities. Finally, we demonstrate the framework's practical application to network hardening through case studies.
[Measurement, Knowledge engineering, network security metrics, network weaknesses, network security, network theory (graphs), Security metrics, Security, zero day attacks, computer network security, network hardening, vulnerability-specic graphical model, Semantics, network mean time-to-compromise, Bayes methods, Safety, computer network reliability, Bayesian networks, known attacks, mean time to compromise]
Message from General Chair
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from Technical Program Co-chairs
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Conference organization
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Provides a listing of current committee members and society officers.
[]
Program Committee
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Provides a listing of current committee members and society officers.
[]
Keynotes
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
PROPS: A PRivacy-Preserving Location Proof System
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
A secure location-based service requires that a mobile user certifies his position before gaining access to a resource. Currently, most of the existing solutions addressing this issue assume a trusted third party that can vouch for the position claimed by a user. However, as computation and communication capacities become ubiquitous with the large scale adoption of smartphones by individuals, we propose to leverage on these resources to solve this issue in a collaborative and private manner. More precisely, we introduce PROPS, for PRivacy-preserving lOcation Proof System, which allows users to generate proofs of location in a private and distributed way using neighboring nodes as witnesses. PROPS provides security properties such as unforgeability and non-transferability of the proofs, as well as resistance to classical localization attacks.
[secure location-based service, Protocols, location based services, privacy, user interfaces, ubiquitous computing, Global Positioning System, collaborative algorithms, Privacy, mobile computing, mobile user, distance bounding, Computer architecture, groupware, privacy-preserving location proof system, private manner, data privacy, collaborative manner, theorem proving, Cryptography, location proof system, PROPS, Testing]
Gemini: An Emergency Line of Defense against Phishing Attacks
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
This paper proposes a simple but very effective approach called Gemini to prevent victim users from exposing sensitive credentials to a phishing site. As an emergency line of defense, Gemini assumes that a victim user is already deceived into a phishing site and starts the user authentication procedure. Gemini springs into action once the username field is filled in, and tackles the phishing problem from a new perspective. In particular, by exploiting username input, Gemini is able to provide more accurate detection of a phishing site and much stronger protection for a password, the most confidential and crucial information for user authentication. To validate the efficacy of Gemini, we implement different prototypes of Gemini as a browser extension for IE, Firefox, and Chrome, respectively, and conduct extensive live experiments over various legitimate and phishing websites for more than one month. Our experimental results show that Gemini can achieve zero false negative rate and less than 1% false positive rate, and Gemini can effectively block the access to a phishing site before a victim user begins to enter in a password. Moreover, Gemini is complementary to existing anti-phishing tools. The performance overhead induced by Gemini is minor and has a negligible effect upon users' browsing activities.
[defense emergency line, phishing attack defense, victim user prevention, unsolicited e-mail, Electronic mail, Internet Security, Engines, Uniform resource locators, phishing site, authorisation, computer crime, user authentication, sensitive credentials, Chrome, phishing Websites, IE, authentication procedure, Phishing, Educational institutions, Browsers, Firefox, user browsing activities, Authentication, Web Application, Web sites, Gemini]
A Practical Experience on the Impact of Plugins in Web Security
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
In an attempt to support customization, many web applications allow the integration of third-party server-side plugins that offer diverse functionality, but also open an additional door for security vulnerabilities. In this paper we study the use of static code analysis tools to detect vulnerabilities in the plugins of the web application. The goal is twofold: 1) to study the effectiveness of static analysis on the detection of web application plugin vulnerabilities, and 2) to understand the potential impact of those plugins in the security of the core web application. We use two static code analyzers to evaluate a large number of plugins for a widely used Content Manage-ment System. Results show that many plugins that are current-ly deployed worldwide have dangerous Cross Site Scripting and SQL Injection vulnerabilities that can be easily exploited, and that even widely used static analysis tools may present disappointing vulnerability coverage and false positive rates.
[Content management, Web application plugin vulnerabilities, program diagnostics, plugins, Manuals, Web applications, static analysis, Security, content management, static code analysis tools, false positive rates, security, Databases, security of data, Web pages, SQL injection vulnerabilities, cross site scripting, vulnerabilities, Internet, Web security, content management system, Testing]
Improving Cloud Service Resilience Using Brownout-Aware Load-Balancing
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
We focus on improving resilience of cloud services (e.g., e-commerce website), when correlated or cascading failures lead to computing capacity shortage. We study how to extend the classical cloud service architecture composed of a load-balancer and replicas with a recently proposed self-adaptive paradigm called brownout. Such services are able to reduce their capacity requirements by degrading user experience (e.g., disabling recommendations). Combining resilience with the brownout paradigm is to date an open practical problem. The issue is to ensure that replica self-adaptivity would not confuse the load-balancing algorithm, overloading replicas that are already struggling with capacity shortage. For example, load-balancing strategies based on response times are not able to decide which replicas should be selected, since the response times are already controlled by the brownout paradigm. In this paper we propose two novel brownout-aware load-balancing algorithms. To test their practical applicability, we extended the popular lighttpd web server and load-balancer, thus obtaining a production-ready implementation. Experimental evaluation shows that the approach enables cloud services to remain responsive despite cascading failures. Moreover, when compared to Shortest Queue First (SQF), believed to be near-optimal in the non-adaptive case, our algorithms improve user experience by 5%, with high statistical significance, while preserving response time predictability.
[Algorithm design and analysis, cloud service architecture, statistical evaluation, control theory, cloud service resilience, self-adaptive paradigm, shortest queue first, cloud, power aware computing, resource allocation, computing capacity shortage, brownout aware load balancing algorithm, file servers, Computer architecture, self-adaptation, cloud computing, Web server, Load modeling, load-balancing, load balancer, SQF, Generators, user experience, Power system faults, Resilience, replica self-adaptivity, Time factors]
The Performance of Paxos in the Cloud
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
This experience report presents the results of an extensive performance evaluation conducted using four open-source implementations of Paxos deployed in Amazon's EC2. Paxos is a fundamental algorithm for building fault-tolerant services, at the core of state-machine replication. Implementations of Paxos are currently used in many prototypes and production systems in both academia and industry. Although all protocols surveyed in the paper implement Paxos, they are optimized in a number of different ways, resulting in very different behavior, as we show in the paper. We have considered a variety of configurations and failure-free and faulty executions. In addition to reporting our findings, we propose and assess additional optimizations to existing implementations.
[Availability, Protocols, Amazon EC2, public domain software, faulty executions, Paxos performance evaluation, Throughput, Educational institutions, Computer crashes, failure-free executions, finite state machines, open-source Paxos implementations, Open source software, state-machine replication, Libraries, fault tolerant computing, cloud computing, fault-tolerant services, software performance evaluation]
Transfer as a Service: Towards a Cost-Effective Model for Multi-site Cloud Data Management
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
The global deployment of cloud datacenters is enabling large web services to deliver fast response to users worldwide. This unprecedented geographical distribution of the computation also brings new challenges related to the efficient data management across sites. High throughput, low latencies, cost-or energy-related trade-offs are just a few concerns for both cloud providers and users when it comes to handling data across datacenters. Existing cloud data management solutions are limited to cloud-provided storage, which offers low performance based on rigid cost schemas. In this paper, we are proposing a dedicated cloud data transfer service that supports large-scale data dissemination across geographically distributed sites, advocating for a Transfer as a Service (TaaS) paradigm. The system aggregates the available bandwidth by enabling multiroute transfers across cloud sites. For users of multi-site or federated clouds, our proposal is able to decrease the variability of transfers and increase the throughput up to three times compared to baseline user options, while benefiting from the well-known high availability of cloud-provided services. For cloud providers, such a service can decrease the energy consumption within a datacenter down to half compared to user-based transfers.
[Availability, Cloud computing, data center energy consumption, data management, multisite cloud data management, Big Data, Throughput, multi-datacenters, Proposals, dedicated cloud data transfer service, computer centres, transfer as a service, Transfer as a Service, Web services, Bandwidth, Pricing, TaaS, Data transfer, federated clouds, data handling, cloud computing, cost-effective model, multiroute transfers, cloud data centers]
Optimistic Parallel State-Machine Replication
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
State-machine replication, a fundamental approach to fault tolerance, requires replicas to execute commands deterministically, which usually results in sequential execution of commands. Sequential execution limits performance and under-uses servers, which are increasingly parallel (i.e., multicore). To narrow the gap between state-machine replication requirements and the characteristics of modern servers, researchers have recently come up with alternative execution models. This paper surveys existing approaches to parallel state-machine replication and proposes a novel optimistic protocol that inherits the scalable features of previous techniques. Using a replicated B+-tree service, we demonstrate in the paper that our protocol outperforms the most efficient techniques by a factor of 2.4 times.
[optimistic protocol, client-server systems, sequential execution, Protocols, fault tolerance, B+-tree service, Throughput, Servers, Synchronization, finite state machines, Mixers, parallel state-machine replication, Atomic layer deposition, fault tolerant computing, tree data structures, Message systems]
ZooFence: Principled Service Partitioning and Application to the ZooKeeper Coordination Service
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Cloud computing infrastructures leverage fault-tolerant and geographically distributed services in order to meet the requirements of modern applications. Each service deals with a large number of clients that compete for the resources it offers. When the load increases, the service needs to scale. In this paper, we investigate a scalability solution which consists in partitioning the service state. We formulate specific conditions under which a service is partitionable. Then, we present a general algorithm to build a dependable and consistent partitioned service. To assess the practicability of our approach, we implement and evaluate the ZooFence coordination service. ZooFence orchestrates several instances of ZooKeeper and presents the exact same API and semantics to its clients. It automatically splits the coordination service state among ZooKeeper instances while being transparent to the application. By reducing the convoy effect on operations and leveraging the workload locality, our approach allows proposing a coordination service with a greater scalability than with a single ZooKeeper instance. The evaluation of ZooFence assesses this claim for two benchmarks, a synthetic service of concurrent queues and the BookKeeper distributed logging engine.
[service scalability, convoy effect reduction, application program interfaces, service semantics, BookKeeper distributed logging engine, History, coordination service, cloud computing infrastructures, Semantics, Parallel processing, general algorithm, cloud computing, geographically distributed services, queueing theory, synthetic service, zookeeper, principled service state partitioning, Nominations and elections, ZooFence, Banking, Partitioning algorithms, ZooKeeper coordination service, Synchronization, state partitioning, dependable consistent partitioned service, automatically coordinated service state splitting, workload locality leveraging, API, concurrent queues, fault-tolerant services]
DATAFLASKS: Epidemic Store for Massive Scale Systems
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Very large scale distributed systems provide some of the most interesting research challenges while at the same time being increasingly required by nowadays applications. The escalation in the amount of connected devices and data being produced and exchanged, demands new data management systems. Although new data stores are continuously being proposed, they are not suitable for very large scale environments. The high levels of churn and constant dynamics found in very large scale systems demand robust, proactive and unstructured approaches to data management. In this paper we propose a novel data store solely based on epidemic (or gossip-based) protocols. It leverages the capacity of these protocols to provide data persistence guarantees even in highly dynamic, massive scale systems. We provide an open source prototype of the data store and correspondent evaluation.
[Algorithm design and analysis, Protocols, Heuristic algorithms, public domain software, epidemic store, Dependability, data management systems, Estimation, distributed processing, very large scale environments, data persistence guarantees, Distributed Systems, Convergence, Epidemic Protocols, DATAFLASKS, nowadays applications, massive scale systems, epidemic protocols, Distributed databases, large scale systems, Peer-to-peer computing, data handling, open source prototype, Large Scale Data Stores]
On the Support of Versioning in Distributed Key-Value Stores
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
The ability to access and query data stored in multiple versions is an important asset for many applications, such as Web graph analysis, collaborative editing platforms, data forensics, or correlation mining. The storage and retrieval of versioned data requires a specific API and support from the storage layer. The choice of the data structures used to maintain versioned data has a fundamental impact on the performance of insertions and queries. The appropriate data structure also depends on the nature of the versioned data and the nature of the access patterns. In this paper we study the design and implementation space for providing versioning support on top of a distributed key-value store (KVS). We define an API for versioned data access supporting multiple writers and show that a plain KVS does not offer the necessary synchronization power for implementing this API. We leverage the support for listeners at the KVS level and propose a general construction for implementing arbitrary types of data structures for storing and querying versioned data. We explore the design space of versioned data storage ranging from a flat data structure to a distributed sharded index. The resulting system, ALEPH, is implemented on top of an industrial-grade open-source KVS, Infinispan. Our evaluation, based on real-world Wikipedia access logs, studies the performance of each versioning mechanisms in terms of load balancing, latency and storage overhead in the context of different access scenarios.
[application program interfaces, versioning, ALEPH, distributed sharded index, Data mining, flat data structure, query processing, Semantics, versioning support, data structures, queries performance, versioned data storing, versioned data storage, indexing, versioned data querying, key-value store, listeners, data forensics, Data structures, access patterns, Vectors, Synchronization, Indexes, correlation mining, versioned data retrieval, configuration management, insertions performance, versioned data access, Web graph analysis, Data models, distributed key-value stores, API, storage layer, collaborative editing platforms, KVS level]
Transaction Management Using Causal Snapshot Isolation in Partially Replicated Databases
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
We present here a transaction management protocol using causal snapshot isolation in partially replicated multi-version databases. We consider here replicated databases consisting of multiple disjoint data partitions. A partition is not required to be replicated at all database sites, and a site may contain replicas for any number of partitions. Transactions can execute at any site and read or write data from any subset of the partitions, and its updates are propagated asynchronously to other sites. The protocol ensures that the snapshot observed by a transaction contains data versions that are causally consistent. The protocol requires propagating updates only to the sites replicating the updated items. In developing this protocol, we address the issues that are unique in supporting transactions with causal consistency together with the snapshot isolation model in partially replicated databases. Through experimental evaluations, we demonstrate the scalability of this model and its performance benefits over full replication models.
[causal snapshot isolation, disjoint data partitions, Protocols, replicated databases, Snapshot Isolation, Data Replication, Vectors, Partitioning algorithms, Distributed Systems, Atomic clocks, Databases, transaction management protocol, Transaction Management, Data models, partially replicated multiversion databases]
pH1: A Transactional Middleware for NoSQL
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
NoSQL databases opt not to offer important abstractions traditionally found in relational databases in order to achieve high levels of scalability and availability: transactional guarantees and strong data consistency. In this work we propose pH1, a generic middleware layer over NoSQL databases that offers transactional guarantees with Snapshot Isolation. This is achieved in a non-intrusive manner, requiring no modifications to servers and no native support for multiple versions. Instead, the transactional context is achieved by means of a multiversion distributed cache and an external transaction certifier, exposed by extending the client's interface with transaction bracketing primitives. We validate and evaluate pH1 with Apache Cassandra and Hyperdex. First, using the YCSB benchmark, we show that the cost of providing ACID guarantees to these NoSQL databases amounts to 11% decrease in throughput. Moreover, using the transaction intensive TPC-C workload, pH1 presented an impact of 22% decrease in throughput. This contrasts with OMID, a previous proposal that takes advantage of HBase's support for multiple versions, with a throughput penalty of 76% in the same conditions.
[Hyperdex, Apache Cassandra, Servers, snapshot isolation, Databases, NoSQL, pH1, transactional middleware, transactional guarantees, middleware, Context, Availability, NoSQL databases, Snapshot Isolation, transaction bracketing primitives, Maintenance engineering, data integrity, relational databases, Middleware, SQL, data consistency, HBase, Transactions, TPC-C workload, Data models, nonintrusive manner]
Exploiting Decoding Computational Locality to Improve the I/O Performance of an XOR-Coded Storage Cluster under Concurrent Failures
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
In today's large data centers, hundreds to thousands of nodes are deployed as storage clusters to provide cloud and big data storage service, where failures are not rare. Therefore, efficient data redundancy technologies are needed to ensure data availability and reliability. Compared to traditional technology based on replication, erasure codes which tolerate multiple failures provide availability and reliability at a much lower cost. However, those erasure-coded, particularly XOR-coded storage clusters, suffer from performance problem caused by degraded reads under concurrent node failures. With the traditional centralized decoding method, a large amount of extra data has to be transmitted over the network to service degraded reads. In particular, the degraded reads in XOR-coded stripes with concurrent failures result in notably high network traffic. To address this problem, we propose a novel decoding approach called Local Decoding First or LDF for short. Via exploiting decoding computational locality of XOR-coded storage clusters, LDF significantly reduces the required network traffic and hence reduces the access latency of degraded reads, thus improving I/O throughput. A prototype of LDF with two typical XOR codes has been implemented in the popular distributed file system HDFS on a storage cluster composed of 40 nodes. The experimental results show that LDF dramatically reduces the network traffic under concurrent node failures and thus improves both the I/O throughput and access latency.
[Strips, decoding computational locality, distributed file system, concurrent node failures, decoding approach, storage clusters, XOR-coded storage clusters, I/O throughput, reliability, Throughput, cloud storage service, XOR-coded stripes, LDF, storage management, data availability, large data centers, distributed systems, Silicon, data redundancy technologies, Data communication, cloud computing, concurrency (computers), Availability, replication, degraded reads, centralized decoding method, replicated databases, Big Data, HDFS, I/O performance, Decoding, Equations, data reliability, network traffic, local decoding first, Big Data storage service, erasure codes, access latency]
A Stack-Based Single Disk Failure Recovery Scheme for Erasure Coded Storage Systems
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
The fast growing of data scale encourages the wide employment of data disks with large storage capacity. However, a mass of data disks' equipment will in turn increase the probability of data loss or damage, because of the appearance of various kinds of disk failures. To ensure the intactness of the hosted data, modern storage systems usually adopt erasure codes, which can recover the lost data by pre-storing a small amount of redundant information. As the most common case among all the recovery mechanisms, the single disk failure recovery has been receiving intensive attentions for the past few years. However, most of existing works in this literature still take the stripe-level recovery as their only consideration, and a considerable performance improvement on single failure disk reconstruction in the stack-level (i.e., a group of rotated stripes) is missed. To seize this potential improvement, in this paper we systematically study the problem of single failure recovery in the stack-level. We first propose our recovery mechanism based on greedy algorithm to seek for the near-optimal solution (BP-Scheme) for any erasure array code in stack level, and further design a rotated recovery algorithm (RR-Algorithm) to eliminate the size of required memory. Through a rigorous statistic analysis and intensive evaluation on a real system, the results show that BP-Scheme gains at most 38.9% higher recovery speed than Khan's Scheme, and owns up to 34.8% higher recovery speed than Luo's U-Scheme.
[Algorithm design and analysis, storage system, stack, RR-algorithm, greedy algorithms, BP-Scheme, Generators, erasure array code, system recovery, Equations, rotated recovery algorithm, storage management, single failure recovery, erasure coded storage systems, Simulated annealing, stack-based single disk failure recovery scheme, Approximation algorithms, Silicon, statistic analysis, Arrays, statistical analysis, greedy algorithm, erasure code]
Fast Repair for Single Failure in Erasure Coding-Based Distributed Storage Systems
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
In order to guarantee data reliability in distributed storage systems, erasure codes are widely used for the desirable storage properties. Nevertheless, the codes have one drawback that overmuch data are needed to repair a failure, resulting in both large bandwidth consuming in the network and high calculation pressure on the replacement node. For repair bandwidth problem, researchers derive the tradeoffs between storage and repair traffic from network coding and propose regenerating codes. However, the constructions of regenerating codes complicate the systems as well as recovery calculation. Hence, this paper proposes a distributed repair method based on general erasure codes to mitigate the burden of both recovery computation and network traffic. We observe that distributing recovery computation among helpers can distract the whole calculation procedure and accelerate repair speed in practical systems. Furthermore, by combining this technique with network topology, we introduce a novel repair tree to minimize repair traffic. Repair tree is also derived from network coding. The performance of the repair tree is preliminarily analyzed and evaluated, which infers that the storage-bandwidth bound of regenerating codes can be broken under this model.
[Strips, general erasure codes, recovery computation, Servers, replacement node, repair tree, repair bandwidth problem, Bandwidth, distributed databases, telecommunication network reliability, regenerating codes, Silicon, repair traffic, network coding, Computational modeling, storage bandwidth, recovery calculation, Maintenance engineering, telecommunication network topology, Vectors, network topology, data reliability, network traffic, erasure coding-based distributed storage systems, distributed repair method, fault tolerant computing]
Self-Stabilizing Byzantine Broadcast
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
We consider the problem of reliably broadcasting messages in a multi-hop network where nodes can fail in some unforeseen manner. We consider the most general failure model: the Byzantine model, where failing nodes may exhibit arbitrary behavior, and actively try to harm the network. Previous approaches dealing with permanent Byzantine failures limit either the number of Byzantine nodes or their density. In dense network, the density criterium is the allowed fraction of Byzantine neighbors per correct node. In sparse networks, density has been defined as the distance between Byzantine nodes. In this context, we first propose a new algorithm for networks whose communication graph can be decomposed into cycles: e.g., a torus can be decomposed into square cycles, a planar graph into polygonal cycles, etc. Our algorithm ensures reliable broadcast when the distance between permanent Byzantine failures is greater than twice the diameter of the largest cycle of the decomposition. Then, we refine the first protocol to make it Byzantine fault tolerant for transient faults (in addition to permanent Byzantine faults). This additional property is guaranteed by means of self-stabilization, which permits to recover from any arbitrary initial state. This arbitrary initial state can be seen as the result of every node being Byzantine faulty for a short period of time (hence the transient qualification). This second protocol thus tolerates permanent (constrained by density) and transient (unconstrained) Byzantine failures. When the maximum degree and cycle diameter are both bounded, both solutions perform in a time that remains proportional to the network diameter.
[Algorithm design and analysis, self-stabilizing Byzantine broadcast, Protocols, cryptographic protocols, self-stabilization, graph theory, Byzantine failure model, cycle, Byzantine fault tolerant, protocol, multihop network, Spread spectrum communication, Cryptography, Transient analysis, transient Byzantine failures, communication graph, Byzantine failures, transient faults, message broadcasting, broadcasting, fault tolerant computing, Byzantine nodes, reliable broadcast, Reliability, Joining processes]
Fair Synchronization in the Presence of Process Crashes and its Weakest Failure Detector
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
A non-blocking implementation of a concurrent object is an implementation that does not prevent concurrent accesses to the internal representation of the object, while guaranteeing the deadlock-freedom progress condition without using locks. Considering a failure free context, G. Taubenfeld has introduced (DISC 2013) a simple modular approach, captured under a new problem called the it fair synchronization problem, to transform a non-blocking implementation into a starvation-free implementation satisfying a strong fairness requirement. This paper extends this approach in several directions. It first generalizes the fair synchronization problem to read/write asynchronous systems where any number of processes may crash. Then, it introduces a new failure detector and uses it to solve the fair synchronization problem when processes may crash. This failure detector, denoted QP (Quasi Perfect), is very close to, but strictly weaker than, the perfect failure detector. Last but not least, the paper shows that the proposed failure detector QP is optimal in the sense that the information on failures it provides to the processes can be extracted from any algorithm solving the fair synchronization problem in the presence of any number of process crash failures.
[fairness requirement, Non-blocking progress property, concurrent access, starvation-free implementation, Registers, concurrent object, Read/write shared memory, system recovery, Concurrent computing, failure free context, Concurrency, Detectors, fair synchronization problem, process crash failure, shared memory systems, process crashes, nonblocking implementation, concurrency (computers), Monitoring, Wait-freedom, Process crash, Asynchronous system, Process scheduling, read/write asynchronous system, failure detector QP, Computer crashes, Synchronization, quasi perfect, quadratic programming, Fairness, synchronisation, Multicore, deadlock-freedom progress condition, Failure detector, Starvation freedom, System recovery]
Convergence Verification of Asynchronous Iterations
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
This paper describes a novel approach to convergence verification of asynchronous iterations in addition to reachability analysis of them. Asynchronous iterations are computation schemes capable of capturing all possible behaviors of distributed systems employing connectionless communication protocols such as UDP/IP. The robustness of these systems against uncertainty in packet delivery can be assured by proving their convergence. We construct some theorems on the basis of fixpoint computations to derive a method for convergence verification, and implement it in trial programs using binary decision diagrams. Theoretical analysis reveals its effectiveness in reducing memory usage, and experimental results support its extreme advantage in execution times. They suggest the possibility of practical use of this method.
[Protocols, Uncertainty, reachability analysis, connectionless communication protocols, distributed processing, Proposals, Indium phosphide, Convergence, UDP protocol, binary decision diagrams, IP protocol, transport protocols, packet delivery, asynchronous iteration convergence verification, Model checking, distributed systems, Robustness, memory usage reduction]
Dependable Admission Control for Mission-Critical Mobile Applications in Wireless Mesh Networks
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Wireless Mesh Networks (WMNs) provide a promising foundation for a flexible and reliable communication infrastructure in industrial environments. Meeting the QoS demands of industrial applications, though, requires the deployment of an admission control to avoid network overload. The presence of mobile stations, however, causes dynamics within the network that severely impacts the available capacity. For instance, if a communication route switches from one to two hops, twice the resources are consumed due to self-interference. To neither jeopardize QoS guarantees nor having to cancel present flows, our dependable admission control scheme foresees and considers those dynamics during resource reservation to handle station mobility. We describe a highly dependable approach and further propose an estimation method that improves network efficiency. The evaluation results show that our novel approach allows for a dependable admission control even in presence of mobile stations and thus enables the network to provide a new class of guarantee for mission-critical mobile applications.
[mission-critical mobile applications, mobile radio, mobility, telecommunication congestion control, flexible communication infrastructure, Mission critical systems, Mobile communication, Routing, admission control, wireless mesh networks, self-interference, station mobility, dependable communication, Upper bound, communication route switches, QoS, Admission control, telecommunication network routing, dependable admission control, WMN, QoS demands, Mobile computing, reliable communication infrastructure]
PCTopk: Privacy-and Correctness-Preserving Functional Top-k Query on Un-trusted Data Storage in Two-Tiered Sensor Networks
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
This paper proposes an efficient mechanism, called PCTopk, for functional top-k query with a combination of multiple conditions/dimensions in two-tiered sensor networks to simultaneously preserve data privacy and correctness (i.e., authenticity and integrity). PCTopk constructs a layered authentication tree, cooperated with an order-preserving symmetric encryption technique, for only permitting storage nodes to systematically process inquired data over encryption domain and enabling querists to efficiently verify the authentic and complete query results. To the best of our knowledge, this is the first research work on the issue of secure functional top-k query with a combination of multiple conditions in two-tiered sensor networks. The performance evaluation results show that PCTopk takes significantly less energy consumption and storage space than prior arts while preserving data privacy and correctness.
[Data privacy, wireless sensor networks, privacy, Encryption, two-tiered sensor networks, data correctness, storage management, PCTopk, security, storage space, Silicon, Sensors, energy consumption, encryption domain, privacy-and correctness-preserving functional top-k query, correctness, order-preserving symmetric encryption technique, query, performance evaluation, cryptography, data integrity, untrusted data storage, data authenticity, computer network performance evaluation, Couplings, authentication tree, sensor network, message authentication, Tin, data privacy, data integration, trusted computing, storage nodes]
Device Collaboration for Stability Assurance in Distributed Cyber-Physical Systems
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Distributed Cyber-Physical Systems (DCPS) are special control systems because of involving distributed systems characteristics. To enable reliable DCPS, stability assurance is of utmost importance. But due to the system's distributed nature, network delay is inevitable and can affect stability adversely. Existing work to assure stability in DCPS has to rely on either a fixed and accurate model of network delay, or the scheduling of messages in the network. However, in reality, it is difficult to obtain an accurate network delay model to support the former approach, because the network can exhibit very complex behaviors. The latter approach also suffers from a notable problem, that is, message scheduling is often not allowed or supported by the network. In this paper, we propose a novel approach to overcome these drawbacks. We augment DCPS devices, including sensor, actuator and controller, with certain distributed intelligence. Thus, they can collaborate to understand the characteristics of network delay at runtime, and then adapt their behaviors accordingly to achieve stability. In this way, we avoid the reliance on network delay modeling and message scheduling, and also make DCPS adaptive to the dynamic network environment. Furthermore, we conduct theoretical analysis, and derive some stability criteria to guide the distributed collaboration and adaptation. The effectiveness of our approach has been validated in a simulated green building application.
[Actuators, distributed systems characteristics, distributed control, simulated green building application, distributed collaboration, Process control, network delay modeling, distributed processing, Thermal stability, distributed cyber-physical systems, Stability criteria, control systems, groupware, scheduling, device collaboration, Delays, control engineering computing, DCPS, distributed intelligence, stability assurance, message scheduling, Lyapunov methods]
ACaZoo: A Distributed Key-Value Store Based on Replicated LSM-Trees
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
In this paper we describe the design and implementation of ACaZoo, a key-value store that combines strong consistency with high performance and high availability. ACaZoo supports the popular column-oriented data model of Apache Cassandra and HBase. It implements strongly-consistent data replication using primary-backup atomic broadcast of a write-ahead log, which records data mutations to a Log-structured Merge Tree (LSM-Tree). ACaZoo scales by horizontally partitioning the key space via consistent primary-key hashing on available replica groups (RGs). LSM-Tree compactions can hamper performance, especially when they take place at RG primaries. ACaZoo addresses this problem by changing RG leadership prior to heavy compactions, a method that can improve throughput by up to 40% in write-intensive workloads. We evaluate ACaZoo using the Yahoo Cloud Serving Benchmark (YCSB) and compare it to Oracle's NoSQL Database and to Cassandra providing serial consistency via an extension of the Paxos algorithm.
[write-ahead log, primary-backup atomic broadcast, Protocols, replicated LSM-trees, Compaction, Servers, Proposals, key-value stores, Databases, ACaZoo, distributed systems, tree data structures, HBase column-oriented data model, Paxos algorithm, RGs, data mutations, distributed key-value store, Apache Cassandra column-oriented data model, Nominations and elections, Yahoo cloud serving benchmark, replica groups, strongly-consistent data replication, Oracle NoSQL database, data models, log-structured merge tree, YCSB, primary-key hashing, Data models, data handling, nosql databases]
End-to-End Congestion Control for Content-Based Networks
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Publish/subscribe or "push" communication has been proposed as a new network service. In particular, in a content-based network, messages sent by publishers are delivered to subscribers based on the message content and on subscribers' long-term interests (subscriptions). In most systems that implement this form of communication, messages are treated as datagrams transmitted without end-to-end or in-network acknowledgments or without any form of flow control. In such systems, publishers do not avoid or even detect congestion, and brokers/routers respond to congestion by simply dropping overflowing messages. These systems are therefore unable to provide fair resource allocation and to properly handle traffic anomalies, and therefore are not suitable for large-scale deployments. With this motivation, we propose an end-to-end congestion control for content-based networks. In particular, we propose a practical and effective congestion-control protocol that is also content-aware, meaning that it modulates specific content-based traffic flows along a congested path. Inspired by an existing rate-control scheme for IP multicast, this protocol uses an equation-based flow-control algorithm that reacts to congestion in a manner similar to and compatible with TCP. We demonstrate experimentally that the protocol improves fairness among concurrent data flows and also reduces message loss significantly.
[TCP, message loss, Protocols, telecommunication congestion control, message content, content-aware protocol, Loss measurement, datagrams, equation-based flow-control algorithm, rate-control scheme, IP multicast, IP networks, network service, Context, publish/subscribe, end-to-end congestion control, message passing, Receivers, congestion-control protocol, concurrent data flows, congestion control, push communication, transport protocols, content-based traffic flows, data flow computing, Delays, congested path, content-based networking, content-based networks, telecommunication traffic]
HardPaxos: Replication Hardened against Hardware Errors
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
State Machine Replication (SMR) is a common technique to make services fault-tolerant. Practical SMR systems tolerate process crashes, but no hardware errors such as bit flips. Still, hardware errors can cause major service outages, and their rate is expected to increase in the future. Current approaches either incur a high overhead by hardening large parts of the system in software, or increase the cost of ownership by introducing additional hardware components. This work presents HardPaxos, an atomic broadcast algorithm for SMR that enables services to tolerate hardware errors, while incurring little performance and state overhead. HardPaxos requires no additional hardware and has only a small part of its functionality hardened using a combination of AN-encoding and duplicated execution. Our evaluation shows a throughput overhead of at most 5% for typical payload sizes. Moreover, fault injection experiments show that our hardening decreases the number of undetected errors from 15% to 0.02%.
[Law, SMR systems, Radiation detectors, state machine replication, state overhead, Computer crashes, Byzantine faults, AN-encoding, Proposals, duplicated execution, atomic broadcast algorithm, HardPaxos, Hardware, Libraries, fault tolerant computing, hardware errors, fault injection, replication hardened against hardware errors, Paxos]
Towards a Practical Survivable Intrusion Tolerant Replication System
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
The increasing number of cyber attacks against critical infrastructures, which typically require large state and long system lifetimes, necessitates the design of systems that are able to work correctly even if part of them is compromised. We present the first practical survivable intrusion tolerant replication system, which defends across space and time using compiler-based diversity and proactive recovery, respectively. Our system supports large-state applications, and utilizes the Prime BFT protocol (providing performance guarantees under attack) with a compiler-based diversification engine. We devise a novel theoretical model that computes how resilient the system is over its lifetime based on the rejuvenation rate and the number of replicas. This model shows that we can achieve a confidence in the system of 95% over 30 years even when we transfer a state of 1 terabyte after each rejuvenation.
[Protocols, cyber attacks, cryptographic protocols, Computational modeling, compiler-based diversity, proactive recovery, Computer crashes, Partitioning algorithms, program compilers, Prime BFT protocol, compiler-based diversification engine, Operating systems, Public key, rejuvenation rate, survivable intrusion tolerant replication system]
ByzID: Byzantine Fault Tolerance from Intrusion Detection
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Building robust network services that can withstand a wide range of failure types is a fundamental problem in distributed systems. The most general approach, called Byzantine fault tolerance, can mask arbitrary failures. Yet it is often considered too costly to deploy in practice, and many solutions are not resilient to performance attacks. To address this concern we leverage two key technologies already widely deployed in cloud computing infrastructures: replicated state machines and intrusion detection systems. First, we have designed a general framework for constructing Byzantine failure detectors based on an intrusion detection system. Based on such a failure detector, we have designed and built a practical Byzantine fault-tolerant protocol, which has costs comparable to crash-resilient protocols like Paxos. More importantly, our protocol is particularly robust against several key attacks such as flooding attacks, timing attacks, and fairness attacks, that are typically not handled well by Byzantine fault masking procedures.
[Protocols, failure types, intrusion detection, finite state machines, system recovery, Fault tolerance, cloud computing infrastructures, attacks, Intrusion detection, failure detector, Detectors, distributed systems, cloud computing, protocols, Monitoring, Byzantine fault tolerance, ByzID, state machine replication, Computer crashes, robust network services, security of data, crash-resilient protocols, replicated state machines, fault tolerant computing, Paxos, Byzantine fault masking]
LO-FA-MO: Fault Detection and Systemic Awareness for the QUonG Computing System
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
QUonG is a parallel computing platform developed at INFN and equipped with commodity multi-core CPUs coupled with last generation NVIDIA GPUs. Computing nodes communicate through a point-to-point, high performance, low latency 3D torus network implemented by the APEnet+ FPGA-based interconnect. Scaling of this cluster towards peta-and possibly exascale is a prominent investigation point and in this context fault tolerance issues are structural. Typical fault tolerance solutions for HPC systems (e.g. checkpoint/restart) need to be triggered to be applied in an automated and transparent way, or at least knowledge about occurring faults needs propagating in order to prompt a readjustment: an effective tool to detect faults and make the system aware of them is required. Thus, as a first step towards a fault tolerant QUonG we designed the Local Fault Monitor (LO|FA|MO), an HW/SW solution aimed at providing systemic fault awareness. LO|FA|MO allows the detection of node faults thanks to a mutual watchdog mechanism between the host and the APEnet+ NIC, moreover, diagnostic messages can be delivered to neighbour nodes through both the 3D network and a secondary connection for service communication. The double path ensures that no fault remains unknown at the global level, guaranteeing systemic fault awareness with no single point of failure. In this paper we describe our LO|FA|MO implementation, reporting preliminary measures that show scalability and its next to nil impact on system performance.
[APEnet+ NIC, QUonG computing system, low latency 3D torus network, systemic fault awareness, Registers, networks, parallel processing, node fault detection, INFN, Temperature sensors, diagnostic messages, Three-dimensional displays, parallel computing platform, NVIDIA GPU, APEnet+ FPGA-based interconnect, Monitoring, point-to-point 3D torus network, mutual watchdog mechanism, computing nodes, multiprocessing systems, fault tolerance, local fault monitor, commodity multicore CPU, HW/SW solution, Temperature measurement, LO|FA|MO, systemic awareness, high performance 3D torus network, Fault detection, system performance, system monitoring, HPC systems, fault tolerant computing, fault tolerant QUonG, high performance computing, Peer-to-peer computing, service communication, fault tolerant systems]
FullReview: Practical Accountability in Presence of Selfish Nodes
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Accountability is becoming increasingly required in today's distributed systems. Indeed, accountability allows not only to detect faults but also to build provable evidence about the misbehaving participants of a distributed system. There exists a number of solutions to enforce accountability in distributed systems, among which PeerReview is the only solution that is not specific to a given application and that does not rely on any special hardware. However, this protocol is not resilient to selfish nodes, i.e., nodes that aim at maximising their benefit without contributing their fair share to the system. Our objective in this paper is to provide a software solution to enforce accountability on any underlying application in presence of selfish nodes. To tackle this problem, we propose the FullReview protocol. FullReview relies on game theory by embedding incentives that force nodes to stick to the protocol. We theoretically prove that our protocol is a Nash equilibrium, i.e., that nodes do not have any interest in deviating from it. Furthermore, we practically evaluate FullReview by deploying it for enforcing accountability in two applications: (1) SplitStream, an efficient multicast protocol, and (2) Onion routing, the most widely used anonymous communication protocol. Performance evaluation shows that FullReview effectively detects faults in presence of selfish nodes while incurring a small overhead compared to PeerReview and scaling as PeerReview.
[Onion routing, Protocols, fault diagnosis, multicast protocol, anonymous communication protocol, Force, FullReview protocol, fault detection, Nash equilibrium, force nodes, selfish nodes, incentives, SplitStream, distributed systems, Hardware, Monitoring, accountability, game theory, Routing, software solution, multicast protocols, Distributed systems, telecommunication network routing, Software, Peer-to-peer computing, PeerReview]
Overnesia: A Resilient Overlay Network for Virtual Super-Peers
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Unstructured P2P networks have been widely used to implement resource location systems that support complex queries semantics. Unfortunately these systems usually rely on search algorithms based on some variant of flooding, which generate a significant amount of duplicate messages. An effective way to minimize the cost of query flooding in unstructured P2P networks is the use of super-peers. On the other hand, super-peers may become overloaded or may fail, and have a negative impact on the performance and connectivity of the overlay. These risks can be circumvented by replicating super-peers. Replication serves the dual purpose of supporting load distribution and fault-tolerance purposes. This paper proposes a novel algorithm to construct an overlay network connecting replicated super-peers. We have called the resulting overlay, Overnesia. The paper also proposes techniques to perform query routing that leverage on the unique properties of Overnesia to effectively distribute the query processing load among replicas.
[Protocols, fault tolerance, peer-to-peer computing, Large-Scale Systems, Overnesia, load distribution, search algorithms, Indexes, Unstructured Overlay Networks, virtual super-peers, resource location systems, Peer-to-Peer Systems, Overlay networks, Resource Location Systems, Query processing, query flooding, complex queries semantics, peer-to-peer overlays, Robustness, unstructured P2P networks, Peer-to-peer computing, Face, resilient overlay network]
AcTinG: Accurate Freerider Tracking in Gossip
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Gossip-based content dissemination protocols are a scalable and cheap alternative to centralized content sharing systems. However, it is well known that these protocols suffer from rational nodes, i.e., nodes that aim at downloading the content without contributing their fair share to the system. While the problem of rational nodes that act individually has been well addressed in the literature, colluding rational nodes is still an open issue. Indeed, LiFTinG, the only existing gossip protocol addressing this issue, yields a high ratio of false positive accusations of correct nodes. In this paper, we propose AcTinG, a protocol that prevents rational collusions in gossip-based content dissemination protocols, while guaranteeing zero false positive accusations. We assess the performance of AcTinG on a testbed comprising 400 nodes running on 100 physical machines, and compare its behaviour in the presence of colluders against two state-of-the-art protocols: BAR Gossip that is the most robust protocol handling non-colluding rational nodes, and LiFTinG, the only existing gossip protocol that handles colluding nodes. The performance evaluation shows that AcTinG is able to deliver all messages despite the presence of colluders, whereas LiFTinG and BAR Gossip, both suffer heavy message losses. Finally, using simulations involving up to a million nodes, we show that AcTinG exhibits similar scalability properties as standard gossip-based dissemination protocols.
[Protocols, live-streaming, zero false positive accusations, content downloading, rational collusions, content management, robust protocol handling noncolluding rational nodes, Fault tolerance, gossip-based content dissemination protocols, Fault tolerant systems, Bandwidth, protocols, accurate freerider tracking in gossip, accountability, rational behaviour, colluders, BAR gossip, centralised content sharing systems, peer-to-peer systems, AcTinG, Gossip, Public key, Collaboration, Peer-to-peer computing, LiFTinG]
Accurate and Efficient Counting in Dynamic Networks
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Dynamic network is the abstraction of networks with frequent topology changes arising from node mobility or other reasons. With the model of dynamic network, fundamental distributed computing problems can be formally studied with rigorous correctness. In this work, we study the counting problem, which plays a significant role in many applications, such as all-to-all information dissemination. Existing counting algorithms for dynamic networks are mostly estimation based and randomized, and cannot achieve accurate results deterministically. To address this problem, we adopt the diffusing computation approach in this paper. Although this approach has been widely used to solve different distributed computing problems, due to dynamic topology changes, existing diffusing computation algorithms cannot work under dynamic network models. To address such challenge, we firstly define a new dynamicity model, named (Q, S)-distance, which is used to describe dynamic changes of information propagation time. Based on (Q, S)-distance, we design two different counting algorithms. The first algorithm focuses on extending typical diffusing computation approach by handling topology changes. Different from existing algorithms, which have two phases, our algorithms contain three phases and the new phase is used to determine and notify the termination of tree growing, which the major challenge is caused by topology dynamicity. The second algorithm further extends the first one by incorporating a cluster-based hierarchy to reduce communication cost. The correctness of both the algorithms is formally proved and their performances in time cost and communication cost are analyzed.
[Algorithm design and analysis, counting, mobile ad hoc network, cluster, topology dynamicity, Heuristic algorithms, Computational modeling, topology, (Q, Topology, diffusing computation, Vehicle dynamics, distributed computing, S)-distance, dynamic network, dynamic network model, distributed algorithm, mobile computing, MANET, Network topology, distributed algorithms, Clustering algorithms, mobile ad hoc networks]
A Systematic Differential Analysis for Fast and Robust Detection of Software Aging
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Software systems running continuously for a long time often confront software aging, which is the phenomenon of progressive degradation of execution environment caused by latent software faults. Removal of such faults in software development process is a crucial issue for system reliability. A known major obstacle is typically the large latency to discover the existence of software aging. We propose a systematic approach to detect software aging which has in a shorter test time and higher accuracy compared to traditional aging detection via stress testing and trend detection with high confidence. The approach is based on a comparative differential analysis where a software version under test is compared with against a previous robust version by observing in terms of behavioral (signal) changes during system tests of resource metrics. A key instrument adopted is a divergence chart, which expresses time-dependent differences between two signals, allowing us to detect changes in the system metrics' values which indicate the existence of software aging. In our experimental study, we focuses on memory-leak detection and the and evaluates divergence charts are computed using various multiple statistical techniques combined paired with different application-level memory related metrics (RSS and Heap Usage). The experimental results show that the statistical process control techniques used in our approach proposed method achieves good performance for memory-leak detection, when compared with other in comparison to techniques widely adopted in previous works (e.g., linear regression, moving average and median).
[Measurement, software development process, program testing, statistical techniques, software reliability, system reliability, anomaly detection, stress testing, heap usage, execution environment, Aging, Market research, Monitoring, trend detection, statistical process control techniques, latent software fault removal, Time series analysis, software aging, software maintenance, systematic differential analysis, progressive degradation, software version, application-level memory related metrics, configuration management, software aging detection, comparative differential analysis, RSS, Memory management, resource metrics, memory leak, divergence chart evaluation, Software, memory-leak detection, software metrics]
Modeling Reliability Requirements in Coordinated Node and Link Mapping
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
High performance systems require high levels of reliability. Many functions involved in telecommunication and IT networks have reliability requirements that can only be achieved by introducing redundant resources. In the telecom sector, recently, there has been a significant effort on moving carrier grade systems and functions to virtualized network infrastructure. The management and coordination of those virtualized systems to achieve an optimal mapping (or embedding) to the physical resources that host them is known as virtual resource orchestration. In our prior work we introduced a novel model, based on Mixed Integer Programming (MIP) problem formulation, as one significant way of achieving this optimality in embedding. This paper extends the model to include reliability requirements, improving prior art techniques as well as implementing a novel approach, denoted as reliability assurance. The confidence of the target reliability of the embedded virtual graphs can be traded with the efficiency of the substrate utilization. Extensive simulation results show that our model provides embedding rates and infrastructure utilization comparable with prior art while fulfilling high reliability requirements.
[Art, integer programming, Redundancy, graph theory, reliability, network theory (graphs), virtual network embedding, Network orchestration, virtualisation, virtual graphs, virtual resource orchestration, telecommunication network, Substrates, Resilience, node mapping, link mapping, coordinated node and link mapping, reliability requirement modelling, IT network, MIP, Bandwidth, telecommunication network reliability, mixed integer programming, Telecommunication network reliability]
A Convex Hull Query Processing Method in MANETs
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
In mobile ad hoc networks (MANETs), locationbased service (LBS) is a typical application. In a LBS, it is valuable for each node to grasp the convex hull of nodes which compose a network by using a convex hull query. However, if the query-issuing node acquires the information on all the nodes, the information on nodes which are not the vertexes of the convex hull is replied, which is unnecessary for a convex hull detection. In this paper, we propose a convex hull query processing method, which reduces traffic and also maintains high accuracy of the query result in MANETs. An experimental result shows that our proposed method reduces traffic and achieves high accuracy of the query result compared with a naive method.
[convex hull query, Educational institutions, Ad hoc networks, Mobile nodes, mobility management (mobile radio), query processing, convex hull query processing method, Accuracy, mobile computing, MANET, Query processing, query-issuing node, mobile ad hoc networks, convex hull detection, LBS, Mobile computing, locationbased service]
A Distributed NameNode Cluster for a Highly-Available Hadoop Distributed File System
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Recently, Hadoop attracts much attention of engineers and researchers as an emerging and effective framework for Big Data. HDFS (Hadoop Distributed File System) can manage huge amount of data with high performance and reliability using only commodity hardware. However, HDFS requires a single master node, called a NameNode, to manage the entire namespace of the file system. This causes the SPOF (Single Point Of Failure) problem because the file system becomes inaccessible when the NameNode fails. This also causes a bottleneck of efficiency since all the access requests to the file system have to contact the NameNode. Finally the scale up of a namespace is difficult because the NameNode manages all metadata of the namespace on its own memory, which is limited and expensive resource. In this paper, we propose a new HDFS architecture consisting of several NameNodes to resolve all the above problems.
[distributed NameNode cluster, single point of failure, reliability, Hadoop, Big Data, HDFS, Educational institutions, Distributed NameNodes, Synchronization, parallel processing, HDFS architecture, software architecture, SPOF, File systems, network operating systems, Distributed databases, distributed databases, commodity hardware, Load management, High-Availability, Hardware, Silicon, Hadoop distributed file system]
Practical Encoded Processing
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Embedded distributed systems are becoming increasingly complex and interconnected. Some of the challenges in building such systems are safety, i.e., the ability to operate correctly even in the face of arbitrary hardware errors, and security, i.e., the ability to withstand hacker attacks. In this paper, an approach to improve both safety and security for embedded distributed systems with low performance overhead is proposed. Preliminary results indicate that applications hardened using the proposed technique have less than 2x performance overhead and fault coverage of 99.9% (assuming no control flow faults).
[embedded distributed systems, Encoded Processing, Redundancy, distributed processing, Encoding, Registers, encoding, performance overhead, Arithmetic Codes, security, Computer hacking, security of data, SIHFT, encoded processing, embedded systems, safety, Hardware, Safety, Fault-tolerance]
Keyword Search with Path-Based Filtering over XML Streams
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Recently, a great deal of attention has been focusing on processing keyword search over static and XML streams. Keyword search is becoming more popular for its simplicity and its user-friendliness in querying XML databases. However, it is hard to express real search intention with just keyword search. There are many cases where the combination of path-based query and keyword search can deal with such issue. To address this problem, we propose a method to integrate XPath with keyword search so that users can express their search demands in more specific ways.
[static streams, XPath, Keyword Search, Keyword search, path-based query, path-based filtering, XQuery, Educational institutions, Throughput, information filtering, Electronic mail, keyword search, XML database querying, Standards, query processing, XML Streams, XML streams, XML, Real-time systems]
A Prototype of Power Saving Storage Method RAPoSDA
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Because of the development of information technologies and the pervasiveness of cloud services, Reducing power consumption in large scale storage systems becomes a very important issue. Recently, we have proposed a method to solve the problem called Replica Assisted Power Saving Disk Array (RAPoSDA) and verified its effectiveness. RAPoSDA utilizes a primary backup configuration on both cache memories and disk drives to ensure system reliability and it dynamically controls the timing and targeting of disk access based on individual disk rotation states. Until now, we have evaluated the effectiveness of RAPoSDA by using simulation program which we had developed. The evaluation shows that RAPoSDA achieves significantly reducing power consumption. However, we have not evaluated it effectiveness on the practical environment. In this paper, we introduce the prototype system of RAPoSDA to evaluate the power saving capability of RAPoSDA.
[Power demand, cloud services, power saving storage method, Cache memory, RAPoSDA, cache storage, power consumption, replica assisted power saving disk array, cache memory, storage management, Disk drives, disk drive, disk rotation state, large scale storage system, Prototypes, Reliability, Arrays, disc drives]
A Study on Efficient Event Monitoring in Dense Mobile Wireless Sensor Networks
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
In dense mobile wireless sensor networks, it is desirable to reduce traffic for data gathering as much as possible while satisfying an application requirement. In this paper, focusing on an application which monitors the time-variant shape of events, we discuss an approach to achieve an efficient monitoring with small traffic in dense mobile wireless sensor networks.
[data gathering traffic, Shape, wireless sensor networks, dense mobile wireless sensor network monitoring, time variant shape monitoring, Educational institutions, Mobile communication, Wireless sensor networks, mobile communication, dense mobile wireless sensor networks, event monitoring, Sensors, Monitoring, Mobile computing, telecommunication traffic, traffic reduction]
Chained Signatures for Secure Program Execution
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Every somewhat complex computer system contains bugs. As it is nearly impossible to fix all bugs in the software stack, the only alternative remains is to make the system secure accepting the fact that software is vulnerable. In this work, a hardware monitor is proposed that checks the correctness of program execution using chained signatures.
[software stack, chained signatures, distributed processing, distributed system, Security, program execution security, Program processors, Computer bugs, Systems engineering and theory, Hardware, digital signatures, Monitoring, hardware monitor]
An Efficient Static Taint-Analysis Detecting Exploitable-Points on ARM Binaries
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
This paper aims to differentiate benign vulnerabilities from those used by cyber-attacks, based on STA (Static TaintAnalysis.) To achieve this goal, the proposed STA determines if a crash is from severe vulnerabilities, after analyzing related exploitable-points in ARM binaries. We envision that the proposed analysis would reduce the complexity of analysis, by making use of CPA (Constant Propagation Analysis) and runtime information of crash points.
[IDA Pro plug-in, STA, cyber-attacks, exploitable, program diagnostics, static taint-analysis, ARM binaries, data flow analysis, constant propagation analysis, reverse engineering, crash point, ARM binary, runtime information, security of data, CPA, exploitable-points detection, benign vulnerabilities, Reliability, taint Analysis]
Building a Strongly Consistent Multi-site File System
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
A distributed file system provides transparent access to files stored at multiple remote sites. State-of-the-art approaches typically provide weak consistency guarantees in order to comply with the limitations formalized by the CAP theorem. However, weak consistency is tailored for domain-specific applications, where system designers can anticipate conflicts and devise appropriate resolution mechanisms. We believe that strong consistency is more suitable for a general-purpose application like a file system. Our goal is to build a geographically-distributed file system that guarantees strong consistency despite failures. The key mechanism behind our approach is to partition state such that the number of operations that need to be totally ordered across geographically distant sites is minimized.
[Availability, Cloud computing, Buildings, geographically-distributed file system, strong consistency, Educational institutions, file system, transparent files access, Computer science, multisite file system, File systems, paxos, Distributed databases, distributed databases, CAP theorem, cloud computing, geographically distant sites]
Adaptive Redundant Data Allocation Using Unused Blocks for Improving Access Efficiency in Dependable Storage Systems
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
This paper has presented a data allocation method with erasure correction coding to design dependable storage system. This adaptive data allocation can reduce the number of access to unavailable storage elements, moreover it also improve the reliability of the system. We proposed variable information word length method which can reduce the number of blocking accesses, and two ideas are presented to reduce the allocation map size and update cost.
[cost reduction, adaptive redundant data allocation, Adaptive systems, dependable storage systems, error correction codes, allocation map size reduction, adaptive data allocation, disk arrays, MDS code, system reliability, update cost reduction, Encoding, Sparse matrices, disc storage, energy saving, access efficiency, variable information word length method, dependable storage system design, Distributed databases, erasure correction code, Resource management, Reliability, Arrays, erasure correction coding]
Improving the Throughput of Wireless LAN by Tuning Carrier Sense Threshold of Access Points Based on Node Positions Detected by a Visible Light Tag System
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
The IEEE 802.11 Distributed Coordination Function (DCF) is based on the Carrier Sense Multiple Access (CSMA) algorithm for avoiding collisions of frames. In CSMA, spatial reuse can be improved by suitably changing the carrier sense threshold (CST). If the increased CST does not incur collisions and helps concurrent transmissions from multiple devices, this improves the throughput of the wireless LAN. For example, when there are two APs and multiple devices exist near each of them, each AP can transmit a frame simultaneously without causing collisions of frames since each device can receive a frame from its associated AP with a high signal power and the interference from another AP is small. In this paper, we propose a system which improves the throughput in CSMA/CA based wireless LANs by controlling the CST values of APs according to positions of mobile hosts. In the system, the CST values of multiple APs are dynamically configured by a controller which has a camera for obtaining the current positions of mobile hosts. The proposed system improves the throughput in WLAN 58% in comparison with basic DCF in a LAN where two APs are arranged at an interval of 50 m and two devices exist within 10 m from each AP.
[IEEE 802.11, Wireless LAN, collision avoidance, carrier sense multiple access algorithm, telecommunication congestion control, access points, location information, Throughput, Mobile handsets, visible light, cameras, CSMA/CA based wireless LAN, distributed coordination function, position measurement, carrier sense threshold, access point, controller, wireless LAN throughput, node positions, Interference, visible light tag system, Tuning, carrier sense threshold tuning, Cameras, wireless LAN, Signal to noise ratio, carrier sense multiple access]
[Publisher's information]
2014 IEEE 33rd International Symposium on Reliable Distributed Systems
None
2014
Provides a listing of current committee members and society officers.
[]
Message from General Chair
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from Technical Program Co-chairs
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Symposium Organization
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Provides a listing of current committee members and society officers.
[]
Program Committee
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Provides a listing of current committee members and society officers.
[]
Probabilistic Byzantine Tolerance for Cloud Computing
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Tolerating Byzantine failures in the context of cloud computing is costly. Traditional BFT protocols induce a fixed degree of replication for computations and are therefore wasteful. This paper explores probabilistic Byzantine tolerance, in which computation tasks are replicated on dynamic replication sets whose size is determined based on ensuring probabilistic thresholds of correctness. The probabilistic assessment of a trustworthy output by selecting reputable nodes allows a significant reduction in the number of nodes involved in each computation task. The paper further studies several reputation management policies, including the one used by BOINC as well as a couple of novel ones, in terms of their impact of the possible damage inflicted on the system by various Byzantine behavior strategies, and reports some encouraging insights.
[probabilistic assessment, probabilistic byzantine tolerance, Cloud computing, Protocols, Computational modeling, Redundancy, trustworthy output, probability, Byzantine behavior strategy, Byzantine failure, dynamic replication set, Probabilistic logic, Byzantine, reputation, reputation management policy, Electronic mail, computation task, failure analysis, BFT protocol, cloud, BOINC, probabilistic threshold, cloud computing, trusted computing]
Privacy-Preserving Content-Based Image Retrieval in the Cloud
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Storage requirements for visual data have been increasing in recent years, following the emergence of many new highly interactive multimedia services and applications for both personal and corporate use. This has been a key driving factor for the adoption of cloud-based data outsourcing solutions. However, outsourcing data storage to the Cloud also leads to new challenges that must be carefully addressed, especially regarding privacy. In this paper we propose a secure framework for outsourced privacy-preserving storage and retrieval in large image repositories. Our proposal is based on IES-CBIR, a novel Image Encryption Scheme that displays Content-Based Image Retrieval properties. Our solution enables both encrypted storage and searching using CBIR queries while preserving privacy. We have built a prototype of the proposed framework, formally analyzed and proven its security properties, and experimentally evaluated its performance and precision. Our results show that IES-CBIR is provably secure, allows more efficient operations than existing proposals, both in terms of time and space complexity, and enables more reliable practical application scenarios.
[visual data, Cloud computing, IES-CBIR scheme, privacy-preserving content-based image retrieval, time complexity, Encryption, Indexes, CBIR queries, content-based retrieval, Image color analysis, image retrieval, data privacy, Outsourcing, storage requirements, image repository, cloud computing, cloud-based data outsourcing solution, encrypted storage, space complexity]
SafeSky: A Secure Cloud Storage Middleware for End-User Applications
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
As the popularity of cloud storage services grows rapidly, it is desirable and even essential for both legacy and new end-user applications to have the cloud storage capability to improve their functionality, usability, and accessibility. However, incorporating the cloud storage capability into applications must be done in a secure manner to ensure the confidentiality, integrity, and availability of users' data in the cloud. Unfortunately, it is non-trivial for ordinary application developers to either enhance legacy applications or build new applications to properly have the secure cloud storage capability, due to the development efforts involved as well as the security knowledge and skills required. In this paper, we propose SafeSky, a middleware that can immediately enable an application to use the cloud storage services securely and efficiently, without any code modification or recompilation. A SafeSky-enabled application does not need to save a user's data to the local disk, but instead securely saves them to different cloud storage services to significantly enhance the data security. We have implemented SafeSky as a shared library on Linux. SafeSky supports applications written in different languages, supports various popular cloud storage services, and supports common user authentication methods used by those services. Our evaluation and analysis of SafeSky with real-world applications demonstrate that SafeSky is a feasible and practical approach for equipping end-user applications with the secure cloud storage capability.
[Computers, Cloud computing, secure cloud storage middleware, Security, user authentication methods, storage management, data availability, Libraries, cloud computing, cloud storage usability, middleware, SafeSky middleware, Applications, cloud storage services, data integrity, data confidentiality, Middleware, Secure storage, security of data, cloud storage accessibility, cloud storage capability, Linux, Cloud storage, cloud storage functionality, Reliability]
Extending Eventually Consistent Cloud Databases for Enforcing Numeric Invariants
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Geo-replicated databases often offer high availability and low latency by relying on weak consistency models. The inability to enforce invariants across all replicas remains a key shortcoming that prevents the adoption of such databases in several applications. In this paper we show how to extend an eventually consistent cloud database for enforcing numeric invariants. Our approach builds on ideas from escrow transactions, but our novel design overcomes the limitations of previous works. First, by relying on a new replicated data type, our design has no central authority and uses pairwise asynchronous communication only. Second, by layering our design on top of a fault-tolerant database, our approach exhibits better availability during network partitions and data center faults. The evaluation of our prototype, built on top of Riak, shows much lower latency and better scalability than the traditional approach of using strong consistency to enforce numeric invariants.
[replicated databases, data center faults, Radiation detectors, reliability, Riak, Eventual Consistency, Geo-Replication, Key-Value Databases, availability, escrow transactions, fault-tolerant database, Distributed Systems, Middleware, software fault tolerance, Fault tolerance, numeric invariants, eventually consistent cloud database, pairwise asynchronous communication, Fault tolerant systems, Distributed databases, Prototypes, network partitions, cloud computing, replicated data type]
Auditable Restoration of Distributed Programs
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
We focus on a protocol for auditable restoration of distributed systems. The need for such protocol arises due to conflicting requirements (e.g., access to the system should be restricted but emergency access should be provided). One can design such systems with a tamper detection approach (based on the intuition of "break the glass door"). However, in a distributed system, such tampering, which are denoted as auditable events, is visible only for a single node. This is unacceptable since the actions they take in these situations can be different than those in the normal mode. Moreover, eventually, the auditable event needs to be cleared so that system resumes the normal operation. With this motivation, in this paper, we present a protocol for auditable restoration, where any process can potentially identify an auditable event. Whenever a new auditable event occurs, the system must reach an "auditable state" where every process is aware of the auditable event. Only after the system reaches an auditable state, it can begin the operation of restoration. Although any process can observe an auditable event, we require that only "authorized" processes can begin the task of restoration. Moreover, these processes can begin the restoration only when the system is in an auditable state. Our protocol is self-stabilizing and can effectively handle the case where faults or auditable events occur during the restoration protocol. Moreover, it can be used to provide auditable restoration to other distributed protocol.
[Self-stabilization, Protocols, fault-tolerance, reactive systems, distributed systems auditable restoration, distributed processing, Security, system recovery, adversary, restoration protocol, distributed programs, Degradation, Fault tolerance, Cellular phones, Fault tolerant systems, formal methods, fault tolerant computing, protocols, self-stabilizing protocol, Transient analysis]
Recurrence in Self-Stabilization
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Self-stabilization ensures that a system converges to a legitimate execution in finite time, where a legitimate execution comprises a sequence of configurations satisfying some safety condition. In this work, we investigate the notion of recurrence, which denotes how frequently a condition is satisfied in an execution of a system. We use this notion in self-stabilization to address the convergence of a system to a behavior that guarantees a minimum recurrence of some condition. We apply this notion to show how the design of distributed mutual exclusion algorithms can be altered to achieve a high service time under various convergence time and space complexities. As a particular contribution, we present a self-stabilizing mutual exclusion algorithm that has optimal service time together with optimal stabilization time complexity of (D/2 - 1) for synchronous executions and under any topology, where D is the diameter of the topology. In addition, we rectify an earlier proof stating that (D/2) is a lower bound, to conclude that (D/2 - 1) is optimal for synchronous executions.
[Algorithm design and analysis, Recurrence, Self-Stabilization, Dependability, convergence, topology, synchronous executions, convergence time, Topology, self-stabilizing mutual exclusion algorithm, Convergence, Distributed Algorithms, recurrence, space complexities, distributed mutual exclusion algorithms, distributed algorithms, Mutual Exclusion, optimal stabilization time complexity, Safety, Transient analysis, Time complexity, computational complexity]
Multi-objective Optimisation of Rolling Upgrade Allowing for Failures in Clouds
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Rolling upgrade is a practical industry technique for online updating of software in distributed systems. This paper focuses on rolling upgrade of software versions in virtual machine instances on cloud computing platforms, when various failures may occur. An operator can choose the number of instances that are updated in one round and system environments to minimise completion time, availability degradation, and monetary cost for entire rolling upgrade, and hence this is a multi-objective optimisation problem. To predict completion time in the presence of failures, we offer a stochastic model that represents the dynamics of rolling upgrade. To reduce the computational effort of decision making for large scale complex systems, we propose a technique that can find a Pareto set quickly via an upper bound of the expected completion time. Then an optimum of the original problem can be chosen from this set of potential solutions. We validate our approach to minimise the objectives, through both experiments in Amazon Web Service (AWS) and simulations.
[Cloud computing, rolling upgrade dynamics, software versions, Amazon Web Service, Rolling upgrade, system recovery, Optimization, cloud failures, distributed systems, online software updating, Pareto set, cloud computing, stochastic processes, AWS, multiobjective optimisation, Pareto optimisation, virtual machine instances, completion time prediction, Computational modeling, Virtual machining, Software reliability, Markov model, software maintenance, software version, large-scale systems, Multi-objective optimisation, Upper bound, virtual machines, large scale complex systems, stochastic model]
PASS: An Address Space Slicing Framework for P2P Eclipse Attack Mitigation
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
The decentralized design of Peer-to-Peer (P2P) protocols inherently provides for fault tolerance to non-malicious faults. However, the base P2P scalability and decentralization requirements often result in design choices that negatively impact their robustness to varied security threats. A prominent vulnerability are Eclipse attacks that aim at information hiding and consequently perturb a P2P overlay's reliable service delivery. Divergent lookups constitute an advocated mitigation technique but are size-limited to overlay networks with tens of thousands of peers. In this work, building upon divergent lookups, we propose a novel and scalable P2P address space slicing strategy (PASS) to efficiently mitigate attacks in overlays that host hundreds of thousands of peers. Moreover, we integrate and evaluate diversely designed lookup variants to assess their network overhead and mitigation rates. The proposed PASS approach shows mitigation rates reaching up to 100%.
[Protocols, mitigation rates, Scalability, Security, Peer-to-Peer Networks, Network topology, divergent lookups, Localized Eclipse Attack, peer-to-peer protocols, Lookup, protocols, P2P Eclipse attack mitigation, peer-to-peer computing, information hiding, P2P address space slicing strategy, Routing, network overhead assessment, Topology, computer network security, Mitigation, PASS, P2P overlay service delivery, Peer-to-peer computing, Reliability, Distributed Hash Table]
ControlFreak: Signature Chaining to Counter Control Flow Attacks
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Many modern embedded systems use networks to communicate. This increases the attack surface: the adversary does not need to have physical access to the system and can launch remote attacks. By exploiting software bugs, the attacker might be able to change the behavior of a program. Security violations in safety-critical systems are particularly dangerous since they might lead to catastrophic results. Hence, safety-critical software requires additional protection. We present an approach to detect and prevent control flow attacks. Such attacks maliciously modify program's control flow to achieve the desired behavior. We develop ControlFreak, a hardware watchdog to monitor program execution and to prevent illegal control flow transitions. The watchdog employs chained signatures to detect any modification of the instruction stream and any illegal jump in the program even if signatures are maliciously modified.
[software bugs, chained signatures, safety-critical software, program execution, signature chaining, control flow attacks, Security, Automobiles, safety-critical systems, Embedded systems, security of data, Computer bugs, embedded systems, security violations, ControlFreak hardware watchdog, Hardware, instruction stream, Monitoring]
Denial of Service Elusion (DoSE): Keeping Clients Connected for Less
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Denial of Service (DoS) attacks continue to grow in magnitude, duration, and frequency increasing the demand for techniques to protect services from disruption, especially at a low cost. We present Denial of Service Elusion (DoSE) as an inexpensive method for mitigating network layer attacks by utilizing cloud infrastructure and content delivery networks to protect services from disruption. DoSE uses these services to create a relay network between the client and the protected service that evades attack by selectively releasing IP address information. DoSE incorporates client reputation as a function of prior behavior to stop attackers along with a feedback controller to limit costs. We evaluate DoSE by modeling relays, clients, and attackers in an agent-based MATLAB simulator. The results show DoSE can mitigate a single-insider attack on 1,000 legitimate clients in 3.9 minutes while satisfying an average of 88.2% of requests during the attack.
[Computers, service disruption protection, computer network economics, Cloud computing, relay network, agent-based MATLAB simulator, Relays, Computer crime, Overlay networks, single-insider attack, Denial of Service attacks, DDoS, IP networks, cloud computing, DoSE, relay networks (telecommunication), Economics, network layer attack mitigation, content delivery networks, computer network security, Denial of Service Elusion, feedback controller, denial of service defense, DoS attacks, cloud infrastructure]
Temporality a NVRAM-based Virtualization Platform
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Power failures in data centers and Cloud Computing infrastructures can cause loss of data and impact revenue. Existing best practice such as persistent logging and checkpointing add overhead during operation and increase recovery time. Other solutions like the use of an uninterruptable power supply incur additional costs and are maintenance-intensive. Novel persistent main memory, i.e. memory that retains stored data without an external source of power, firstly prevents data loss in case of a power outage, secondly reduces the time for a system reboot and thirdly enables to continue operation at full-speed after a recovery. Yet new architectures and programming models are required to utilize persistent main memory. We present Temporality a virtualization layer that runs virtual machines in persistent memory and offers virtual persistent memory. It can be used as a basis for future Cloud platforms to allow applications the utilization of persistent memory without any changes. It provides safety of volatile data, significantly decreases overall recovery time and prevents subsequent performance degradation.
[overall recovery time reduction, Cloud computing, random-access storage, NVRAM-based virtualization platform, NVRAM, Random access memory, Power system restoration, virtualisation, Power system faults, Cloud Computing, Memory management, performance degradation prevention, virtual machines, virtual persistent memory, data protection, Resource management, Temporality, nonvolatile random-access memory, Virtualization, volatile data safety]
A Secure Collusion-Aware and Probability-Aware Range Query Processing in Tiered Sensor Networks
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
With high expansibility and efficient power usage, tiered wireless sensor networks are widely deployed in many fields as an important part of Internet of Things (IoTs). It is challenging to process range query while protecting sensitive data from adversaries. Moreover, most existing work focuses on privacy-preserving range query neglecting collusion attacks and probability attacks, which are more threatening to network security. In this paper, we first propose a secure range query protocol called secRQ, which not only protects data privacy, but also resists collusion attacks and probability attacks in tiered wireless sensor networks. Generalized inverse matrices and distance-based range query mechanism are used to guarantee security as well as high efficiency and accuracy. Besides, we propose the mutual verification scheme to verify the integrity of query results. Finally, both theoretical analysis and experimental results confirm the security, efficiency and accuracy of secRQ.
[telecommunication security, Data privacy, Protocols, wireless sensor networks, tiered wireless sensor network, Probability attack, Collusion attack, inverse matrix, IoT, query processing, mutual verification scheme, secRQ, secure collusion-awareness, Integrity verification, secure range query protocol, protocols, Monitoring, Privacy preservation, probability-aware range query processing, network security, probability attack, probability, collusion attack, Internet of Things, matrix algebra, Heart rate, Range query, Wireless sensor networks, Query processing, data privacy, Biomedical monitoring]
PmDroid: Permission Supervision for Android Advertising
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
It is well-known that Android mobile advertising networks may abuse their host applications' permission to collect private information. Since the advertising library and host app are running in the same process, the current Android permission mechanism cannot prevent an ad network from collecting private data that is out of an ad network's permission range. In this paper, we propose PmDroid to protect the data that is not under the scope of the ad network's permission set. PmDroid can block the data from being sent to advertising servers at the occurrence of permission violation in ad networks. Moreover, we utilize PmDroid to assess how serious the permission violation problem is in the ad networks. We first implement 53 sample apps using a single ad network library. We grant all permissions of Android 4.3 to these apps and record the data sent to the Internet. Then, we further analyze 430 published market apps. In total, there are 76 ad networks identified in our experiments. We compare the permission of data received by these ad networks with their official documents. Our experimental results indicate that the permission violation is a real problem in existing ad network markets.
[advertising data processing, Humanoid robots, Privacy Protection, Mobile communication, permission violation problem, Servers, Android, Ad Networks, mobile computing, ad network library, permission supervision, ad network markets, market apps, Permission, data protection, PmDroid, Libraries, Android mobile advertising networks, Internet, Androids, Advertising, Smart phones]
Signature-Based Top-k Query Processing against Data Replacement Attacks in MANETs
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
In this paper, we propose a signature-based top-k query processing method against data replacement attacks in mobile ad hoc networks (MANETs). In order to rapidly identify a greater number of malicious nodes, nodes share information about identified malicious nodes with other nodes. If nodes share only this information, however, malicious nodes may successfully transmit false information identifying normal nodes as malicious. Therefore, in the proposed method, when nodes send reply messages during query processing, they attach encrypted information about the sent data items (i.e., digital signatures), providing the query-issuing node with critical information about the data items sent by nodes in the network, and thereby enabling it to identify malicious nodes, using the received signatures. After identifying the malicious nodes, it floods the network with a notification message including the signatures in which the identified malicious nodes have replaced higher-score data items to their own lower-score items.
[telecommunication security, mobile ad hoc network, signature, cryptography, Ad hoc networks, Mobile nodes, encrypted information, query processing, MANET, Query processing, query-issuing node, Public key, mobile ad hoc networks, malicious node, top-k query, digital signatures, signature-based top-k query processing, data replacement attack, Mobile computing]
FTDE: Distributed Fault Tolerance for WSN Data Collection and Compression Schemes
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Wireless Sensor Networks (WSNs), being power and communication capacity constrained, often employ data compression schemes to reduce the data volumes. At the same time, various factors, such as low node reliability and communication faults, can compromise the core objective of accurate collection and delivery of the sensor data. However, contemporary compression schemes often fail to consider operational faults, and the resulting data errors arising from low node reliability (node crashes), and communication faults (data and links corruption) result in erroneous data being collected. This paper proposes a novel model-based fault-tolerance technique applicable to varied compression schemes. Being fully distributed, our scheme corrects data errors at the point of origin (faulty sensor nodes), and thus avoids costly transmissions of corrupt data, decreasing message cost, and enhances compression effectiveness by correcting the erroneous samples.
[Adaptation models, data compression, fault tolerance, wireless sensor networks, wireless sensor network, node reliability, Approximation methods, Noise measurement, WSN data collection, data compression scheme, Wireless Sensor Networks, FTDE, Data Compression, Wireless sensor networks, Distributed databases, communication fault, Data models, Fault-tolerance, Reliability, distributed fault tolerance, fault tolerance for data error, data error correction, error correction]
Separating the WHEAT from the Chaff: An Empirical Design for Geo-Replicated State Machines
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
State machine replication is a fundamental technique for implementing consistent fault-tolerant services. In the last years, several protocols have been proposed for improving the latency of this technique when the replicas are deployed in geographically-dispersed locations. In this work we evaluate some representative optimizations proposed in the literature by implementing them on an open-source state machine replication library and running the experiments in geographically-diverse PlanetLab nodes and Amazon EC2 regions. Interestingly, our results show that some optimizations widely used for improving the latency of geo-replicated state machines do not bring significant benefits, while others - not yet considered in this context - are very effective. Based on this evaluation, we propose WHEAT, a configurable crash and Byzantine fault-tolerant state machine replication library that uses the optimizations we observed as most effective in reducing SMR latency. WHEAT employs novel voting assignment schemes that, by using few additional spare replicas, enables the system to make progress without needing to access a majority of replicas. Our evaluation shows that a WHEAT system deployed in several Amazon EC2 regions presents a median latency up to 56% lower than a "normal" SMR protocol.
[Byzantine fault-tolerant state machine replication library, Protocols, public domain software, open-source state machine replication library, distributed processing, WHEAT system, Amazon EC2 regions, finite state machines, Optimization, Fault tolerance, geographically-diverse PlanetLab nodes, quorum systems, Fault tolerant systems, configurable crash-tolerant state machine replication library, voting assignment schemes, protocols, measurements, Wide area networks, Byzantine fault tolerance, state machine replication, geo-replicated state machines, Computer crashes, WHEAT, Standards, weight-enabled active replication protocol, SMR latency reduction, fault tolerant computing]
Replacement: Decentralized Failure Handling for Replicated State Machines
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
We investigate methods for handling failures in a Paxos State Machine and introduce Replacement, a novel approach to handle failures. Replacement is fully decentralized and does not rely on consensus. This allows failed replicas to be replaced quickly, avoiding the bottleneck of a single leader. Instead of handling failures in the order proposed by a leader, concurrent replacements are combined to guarantee that all failed replicas are replaced. Replacement also allows the state machine to process client requests during failure handling, even while disagreeing on the current configuration. As our evaluation shows, this enables Replacement to quickly handle failures, with minimal disruption in the processing of client requests.
[Protocols, Buildings, decentralized failure handling, state machine replication, reconfiguration, Synchronization, finite state machines, system recovery, Paxos state machine, Fault tolerance, failure handling, Fault tolerant systems, replicated state machines, fault tolerant computing, Delays, replacement]
Approximate Hash-Based Set Reconciliation for Distributed Replica Repair
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
The objective comparison of hash-based set reconciliation algorithms for distributed replica repair is challenging. Each algorithm's behaviour can be tuned for a given use case, e.g. low bandwidth or computational overhead, using different sets of parameters. Changes on these parameters, however, often also influence the algorithm's accuracy in recognising differences between replicas and thus hinder objective comparisons. We develop models to deduce parameters for equally accurate set reconciliation algorithms for replica repair in a distributed system and compare equally accurate instances of two trivial hash-based algorithms, an algorithm using Bloom filters and a Merkle tree based algorithm. Instead of using a large fixed hash size for Merkle trees we propose to use dynamic hash sizes to align the transfer overhead with the desired accuracy. We evaluate (a) the transferred volume of data with respect to different entropy levels, data and failure distributions on the set of items, and (b) the scalability in the number of items. Our results allow to easily choose an efficient algorithm for practical set reconciliation tasks based on the required level of accuracy. Our way to find equally accurate configuration parameters for different algorithms can also be adopted to other set reconciliation algorithms and allows to rate their respective performance in an objective manner.
[Protocols, Heuristic algorithms, Maintenance engineering, distributed processing, distributed system, Bloom filters, Set Reconciliation, configuration parameters, Approximate Algorithms, Merkle Tree, Synchronization, Distributed Systems, Accuracy Models, dynamic hash size, Bloom Filter, approximate hash-based set reconciliation algorithm, Approximation algorithms, Replication, data structures, Peer-to-peer computing, distributed replica repair, Yttrium, Synchronisation, Merkle tree based algorithm]
Securing Passive Replication through Verification
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
We show how to leverage trusted computing technology to design an efficient fully-passive replicated system tolerant to arbitrary failures. The system dramatically reduces the complexity of a fault-tolerant service, in terms of protocols, messages, data processing and non-deterministic operations. Our replication protocol enables the execution of a single protected service, replicating only its state, while allowing the backup replicas to check the correctness of the results. We implemented our protocol on Trusted Computing (TC) technology and compared it with two recent replication systems.
[passive replication, Protocols, byzantine fault-tolerante, Radiation detectors, TC technology, fault-tolerant service, trusted computing technology, Fault tolerance, fully-passive replicated system, formal verification, arbitrary failures, Hardware, fault tolerant computing, Cryptography, trusted computing, securing passive replication]
PCM: A Parity-Check Matrix Based Approach to Improve Decoding Performance of XOR-based Erasure Codes
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
In large storage systems, erasure codes is a primary technique to provide high reliability with low monetary cost. Among various erasure codes, a major category called XORbased codes uses purely XOR operations to generate redundant data and offer low computational complexity. These codes are conventionally implemented via matrix based method or several specialized non-matrix based methods. However, these approaches are insufficient on decoding performance, which affects the reliability and availability of storage systems. To address the problem, in this paper, we propose a novel Parity-Check Matrix based (PCM) approach, which is a general-purpose method to implement XOR-based codes, and increases the decoding performance by using smaller and sparser matrices. To demonstrate the effectiveness of PCM, we conduct several experiments by using different XOR-based codes. The evaluation results show that, compared to typical matrix based decoding methods, PCM can improve the decoding speed by up to a factor of 1.5&#x00D7; when using EVENODD code (an erasure code for correcting double disk failures), and accelerate the decoding process of STAR code (an erasure code for correcting triple disk failures) by up to a factor of 2.4&#x00D7;.
[EVENODD code, parity check codes, PCM approach, Encoding, Generators, Decoding, Sparse matrices, decoding performance, triple disk failure correction, matrix algebra, Phase change materials, storage management, Performance Evaluation, XOR-based erasure codes, Erasure Code, Parity-check Matrix, double disk failure correction, storage systems, XOR operations, parity-check matrix based approach, Reliability, Arrays]
MICS: Mingling Chained Storage Combining Replication and Erasure Coding
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
High reliability, low space cost, and efficient read/write performance are all desirable properties for cloud storage systems. Due to the inherent conflicts, however, simultaneously achieving optimality on these properties is unrealistic. Since reliable storage is indispensable prerequisite for services with high availability, tradeoff should therefore be made between space and read/write efficiency when storage scheme is designed. N-way Replication and Erasure Coding, two extensively-used storage schemes with high reliability, adopt opposite strategies on this tradeoff issue. However, unbalanced tradeoff designs of both schemes confine their effectiveness to limited types of workloads and system requirements. To mitigate such applicability penalty, we propose MICS, a MIngling Chained Storage scheme that combines structural and functional advantages from both N-way replication and erasure coding. Qualitatively, MICS provides efficient read/write performance and high reliability at reasonably low space cost. MICS stores each object in two forms: a full copy and certain amount of erasure-coded segments. We establish dedicated read/write protocols for MICS leveraging the unique structural advantages. Moreover, MICS provides high read/write efficiency with Pipeline Random-Access Memory consistency to guarantee reasonable semantics for services users. Evaluation results demonstrate that under same fault tolerance and consistency level, MICS outperforms N-way replication and pure erasure coding in I/O throughput by up to 34.1% and 51.3% respectively. Furthermore, MICS shows superior performance stability over diverse workload conditions, in which case the standard deviation of MICS is 70.1% and 29.3% smaller than those of other two schemes.
[read/write protocols, Cloud computing, Protocols, cloud storage systems, software reliability, mingling chained storage combining replication, I/O throughput, erasure coding, semantics, Servers, Cloud Storage, Microwave integrated circuits, storage management, storage schemes, high reliability, read/write performance, cloud computing, N-way replication, read/write efficiency, erasure-coded segments, random-access storage, fault tolerance, reliable storage, workloads, MICS, Encoding, low space cost, diverse workload conditions, pipeline random-access memory consistency, mingling chained storage scheme, services users, Tradeoff, consistency level, Erasure Coding, Replication, fault tolerant computing, Reliability, Manganese]
Online Behavior Identification in Distributed Systems
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
The diagnosis, prediction, and understanding of unexpected behavior is crucial for long running, large scale distributed systems. However, existing works focus on the identification of faults in specific time moments preceded by significantly abnormal metric readings, or require a previous analysis of historical failure data. In this work, we propose an online behavior classification system to identify a wide range of undesired behaviors, which may appear even in healthy systems, and their evolution over time. We employ a two-step process involving two online classifiers on periodically collected system metrics to identify at runtime normal and anomalous behaviors such as deadlock, starvation and livelock, without any previous analysis of historical failure data. Our approach achieves over 80% accuracy in detecting unexpected behaviors and over 90% accuracy in identifying their type with a short delay after the anomalies appear, and with minimal expert intervention. Our experimental analysis uses system execution traces obtained from a Google cluster and from our in-house distributed system with varied behaviors, and shows the benefits of our approach as well as future research challenges.
[Measurement, pattern classification, unexpected behavior, online behavior classification system, online classifiers, distributed processing, distributed system, performance evaluation, Google cluster, anomaly detection, system metrics, deadlock, starvation, online behavior identification, Runtime, System performance, Bismuth, System recovery, system monitoring, distributed systems, livelock, Monitoring, system execution traces]
Z Codes: General Systematic Erasure Codes with Optimal Repair Bandwidth and Storage for Distributed Storage Systems
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Erasure codes are widely used in distributed storage systems to prevent data loss. Traditional erasure codes suffer from a typical repair-bandwidth problem in which the amount of data required to reconstruct the lost data, referred to as the repair bandwidth, is often far more than the theoretical minimum. While many novel erasure codes have been proposed in recent years to reduce the repair bandwidth, these codes either require extra storage capacity and computation overhead or are only applicable to some special cases. To address the weaknesses of the existing solutions to the repair-bandwidth problem, we propose Z Codes, a general family of codes capable of achieving the theoretical lower bound of repair bandwidth for a single data node failure. To the best of our knowledge, the Z codes are the first general systematic erasure codes that achieve optimal repair bandwidth under the minimum storage. Our in-memory performance evaluations of a 1-GB file indicate that Z codes have encoding and repairing speeds that are approximately equal to those of the Reed-Solomon (RS) codes, and their speed on the order of GB/s practically removes computation as a performance bottleneck.
[RS codes, codes, Failure Tolerance, general systematic erasure codes, repair-bandwidth problem, Maintenance engineering, distributed processing, Encoding, Generators, Distributed Storage System, distributed storage systems, Reed-Solomon code, Reed-Solomon codes, storage management, Systematics, Z codes, data loss prevention, Distributed databases, Bandwidth, Erasure Codes, Repair Bandwidth]
Distributed Attribute Based Access Control of Aggregated Data in Sensor Clouds
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Sensor clouds are large scale wireless sensor networks (WSNs), built by connecting a number of smaller WSNs together. Each of these smaller individual WSNs may be owned by different owners. Sensor clouds are dynamic in nature, where wireless sensors can be provisioned and de-provisioned for the users on demand. In such a multi-user, multi-owner system, user access control is a significant problem. Previous user access control schemes have been centralized and designed for standalone sensors or smaller networks and do not take large networks into consideration. In large networks, data is generally aggregated in-network during data collection. In this paper, we present a user access control scheme, which unlike other schemes, is distributed and works on aggregated data within a sensor network. Our scheme which is based on attribute based encryption is also able to differentiate between users who require data with the same set of attributes, which would be a necessity in a commercial sensor cloud system. Our scheme gives the flexibility to sensor network owners to control user access of data from their sensors. Finally, we compare our scheme with other closely related schemes in terms of attack resilience and computation and communication overhead to show its effectiveness.
[sensor clouds, wireless sensor networks, distributed attribute based access control, cryptography, Sensor Cloud, Encryption, multiowner system, Wireless Sensor Networks, Wireless communication, Authorization, Wireless sensor networks, WSN, Public key, multiuser system, attribute based encryption, Attribute Based Encryption, Access Control, aggregated data, user access control]
Seek-Efficient I/O Optimization in Single Failure Recovery for XOR-coded Storage Systems
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Erasure coding provides an effective means for storage systems to protect against disk failures with low redundancy. One important objective for erasure-coded storage systems is to speed up single disk failure recovery. Previous studies reduce the amount of data read for recovery by reading only a small subset of data, but their approaches often incur high disk seeks, which may negate the resulting recovery performance. We propose SIOR, a seek-efficient I/O recovery algorithm for single disk failures. SIOR carefully balances the trade-off between the amount of data read and the number of disk seeks by considering the data layout at the multi-stripe level. It then greedily determines the data to read for recovery using Tabu search. Experiments show that SIOR achieves similar performance to the brute-force enumeration method while keeping high search efficiency. Also, SIOR reduces 31.8%~65.1% of disk seeks during recovery and provides up to 186.8% recovery speed improvement, when compared to a state-of-the-art greedy recovery approach.
[disk failure recovery, Strips, erasure-coded storage systems, Force, erasure coding, system recovery, Optimization, hard discs, single failure recovery, greedy recovery approach, Seek Efficiency, tabu search, search problems, Single Failure Recovery, SIOR, XOR-coded storage systems, greedy algorithms, Maintenance engineering, Generators, Encoding, disk failure protection, brute-force enumeration method, seek-efficient I/O recovery algorithm, I/O Optimization, Layout, seek-efficient I/O optimization, recovery performance, XOR-Coded Storage Systems]
Communicating Reliably in Multihop Dynamic Networks Despite Byzantine Failures
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
We consider the following problem: two nodes want to reliably communicate in a dynamic multihop network where some nodes have been compromised, and may have a totally arbitrary and unpredictable behavior. These nodes are called Byzantine. We consider the two cases where cryptography is available and not available. We prove the necessary and sufficient condition (that is, the weakest possible condition) to ensure reliable communication in this context. Our proof is constructive, as we provide Byzantine-resilient algorithms for reliable communication that are optimal with respect to our impossibility results. In a second part, we investigate the impact of our conditions in three case studies: participants interacting in a conference, robots moving on a grid and agents in the subway. Our simulations indicate a clear benefit of using our algorithms for reliable communication in those contexts.
[radio networks, Protocols, subway grid, Heuristic algorithms, byzantine failures, cryptography, Topology, mobile robots, wireless mesh networks, byzantine-resilient algorithms, Network topology, Spread spectrum communication, telecommunication network reliability, reliable communication, Reliability, Cryptography, multihop dynamic networks, subway agents]
To Transmit Now or Not to Transmit Now
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Given an unreliable communication link, this paper studies how to build, in an energy-efficient manner, a reliable communication service that is synchronous with high probability. We consider a Partially Observable Markov Decision Process (POMDP) setting in which a communication link's transmission quality: (i) changes according to a classic Markovian model and (ii) can be only partially observed, through feedback relative to previous transmissions. We perform a thorough analysis under several variations of Ack/Nack feedback mechanisms. Despite the general intractability of POMDPs, we prove that our communication service, under reliable feedback, can be inexpensively implemented. We obtain closed form solutions specifying when to transmit over the link, which allows to derive an energy-optimal implementation. We also analyse the impact of lossy feedback on implementing our communication service. Considering multiple lossy feedback mechanisms, we show that an easily implementable structure for our communication service can also be obtained, depending on the feedback mechanism itself.
[Closed-form solutions, unreliable feedback, Throughput, communication complexity, energy-efficient communication service, Markovian model, ack/nack feedback mechanisms, Wireless communication, feedback, power aware computing, communication link transmission quality, energy-optimal implementation, POMDP, Markov processes, Propagation losses, partially observable Markov decision process, Reliability, lossy communication, Distributed algorithms]
Ridge: High-Throughput, Low-Latency Atomic Multicast
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
It has been shown that the highest throughput for broadcasting messages in a point-to-point network is achieved with a ring topology. Although several ring-based group communication protocols have benefited from this observation, broadcasting messages along a ring overlay may lead to high latencies: In a system with n processes, at least n-1 communication steps are necessary for all processes to deliver a message. In this work, we argue that it is possible to reach optimal throughput without resorting to a ring topology (or to ip-multicast, typically unavailable in wide-area networks). This can be done by routing messages through different paths, while carefully using the available bandwidth at each process, resulting in a significantly lower latency for every message (potentially a single communication step). Based on this idea, we propose Ridge, a Paxos-based atomic multicast protocol where each message is initially forwarded to a single destination, the distributor, whose responsibility is to propagate the message to all other destinations. To utilize all bandwidth available in the system, processes alternate in the role of distributor. By doing this, the maximum system throughput matches that of ring-based protocols, with a latency that is not significantly dependent on the size of the system. Finally, we show that Ridge can also deliver messages optimistically, with even lower latency.
[ring topology, wide area networks, Scalability, fault-tolerance, high-throughput atomic multicast, Throughput, scalability, Network topology, broadcasting messages, Bandwidth, routing messages, telecommunication network topology, Multicast protocols, Topology, low-latency atomic multicast, consensus, point-to-point network, atomic multicast, multicast protocols, ring-based group communication protocols, ring-based protocols, telecommunication network routing, Paxos-based atomic multicast protocol, optimal throughput, Ridge]
Identifying Global Icebergs in Distributed Streams
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
We consider the problem of identifying global iceberg attacks in massive and physically distributed streams. A global iceberg is a distributed denial of service attack, where some elements globally recur many times across the distributed streams, but locally, they do not appear as a deny of service. A natural solution to defend against global iceberg attacks is to rely on multiple routers that locally scan their network traffic, and regularly provide monitoring information to a server in charge of collecting and aggregating all the monitored information. Any relevant solution to this problem must minimise the communication between the routers and the coordinator, and the space required by each node to analyse its stream. We propose a distributed algorithm that tracks global icebergs on the fly with guaranteed error bounds, limited memory and processing requirements. We present a thorough analysis of our algorithm performance. In particular we derive a tight upper bound on the number of bits communicated between the multiple routers and the coordinator in presence of an oblivious adversary. Finally, we present the main results of the experiments we have run on a cluster of single-board computers. Those experiments confirm the efficiency and accuracy of our algorithm to track global icebergs hidden in very large input data streams exhibiting different shapes.
[Algorithm design and analysis, Computational modeling, global iceberg attacks, guaranteed error bounds, oblivious adversary, Servers, Computer crime, computer network security, single-board computer cluster, randomised approximation algorithm, Analytical models, distributed algorithm, generalised coupon collector problem, distributed algorithms, telecommunication network routing, distributed streams, distributed Denial of Service attack, Approximation algorithms, Internet, routers, Monitoring, performance analysis, data stream model]
A Framework for the Design Configuration of Accountable Selfish-Resilient Peer-to-Peer Systems
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
A challenge in designing a peer-to-peer (P2P) system is to ensure that the system is able to tolerate selfish nodes that strategically deviate from their specification whenever doing so is convenient. In this paper, we propose RACOON, a framework for the design of P2P systems that are resilient to selfish behaviours. While most existing solutions target specific systems or types of selfishness, RACOON proposes a generic and semi-automatic approach that achieves robust and reusable results. Also, RACOON supports the system designer in the performance-oriented tuning of the system, by proposing a novel approach that combines Game Theory and simulations. We illustrate the benefits of using RACOON by designing two P2P systems: a live streaming and an anonymous communication system. In simulations and a real deployment of the two applications on a testbed comprising 100 nodes, the systems designed using RACOON achieve both resilience to selfish nodes and high performance.
[Protocols, peer-to-peer computing, anonymous communication system, configuration, game theory, live streaming, reputation, accountable selfish-resilient peer-to-peer systems, selfishness, Game theory, peer-to-peer, performance-oriented system tuning, P2P systems design, Games, Bandwidth, design, Streaming media, fault tolerant computing, Peer-to-peer computing, RACOON, Monitoring, accountability]
Chasing the Tail of Atomic Broadcast Protocols
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Many applications today rely on multiple services, whose results are combined to form the application's response. In such contexts, the most unreliable service and the slowest service determine the application's reliability and response time, respectively. State-machine replication and atomic broadcast are fundamental abstractions to build highly available services. In this paper, we consider the latency variability of atomic broadcast protocols. This is important because atomic broadcast has a direct impact on the response time of services. We study four high performance atomic broadcast protocols representative of different classes of protocol design and characterize their latency tail distribution under different workloads. Next, we assess how key design features of each protocol can possibly be related to the observed latency tail distributions. Our observations hint at request batching as a simple yet effective way to shorten the latency tails of some of the studied protocols, an improvement within the reach of application implementers. Indeed, our observation is not only verified experimentally, it allows us to assess which of the protocol's key design principles favor the construction of latency predictable protocols.
[latency predictable protocols, Protocols, request batching, tail chasing, atomic broadcast protocols, latency variability, Throughput, Generators, broadcast communication, Reliability, Time factors, Acceleration, protocols, Message systems, observed latency tail distributions]
A Practical Experience on Evaluating Intrusion Prevention System Event Data as Indicators of Security Issues
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
There are currently no generally accepted metrics for information security issues. One reason is the lack of validation using empirical data. In this practical experience report, we investigate whether metrics obtained from security devices used to monitor network traffic can be employed as indicators of security incidents. If so, security experts can use this information to better define priorities on security inspection and also to develop new rules for incident prevention. The metrics we investigate are derived from intrusion detection and prevention system (IDPS) alert events. We performed an empirical case study using IDPS data provided by a large organization of about 40,000 computers. The results indicate that characteristics of alerts can be used to depict trends in some security issues and consequently serve as indicators of security performance.
[Measurement, Computers, IDPS alert events, security inspection, incident prevention, security metrics, intrusion detection and prevention systems, empirical study, intrusion detection and prevention system, security incident indicators, computer network security, Intrusion detection, Organizations, Market research, security performance indicators, network and security management, IP networks, security incidents, intrusion prevention system event data]
Publisher's Information
2015 IEEE 34th Symposium on Reliable Distributed Systems
None
2015
Provides a listing of current committee members and society officers.
[]
Message from the General Co-Chairs
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Technical Program Co-Chairs
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Symposium Organizers
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Provides a listing of current committee members and society officers.
[]
Technical Program Committee
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Provides a listing of current committee members and society officers.
[]
The Rowhammer Attack Injection Methodology
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
This paper presents a systematic methodology to identify and validate security attacks that exploit user influenceable hardware faults (i.e., rowhammer errors). We break down rowhammer attack procedures into nine generalized steps where some steps are designed to increase the attack success probabilities. Our framework can perform those nine operations (e.g., pressuring system memory and spraying landing pages) as well as inject rowhammer errors which are basically modeled as &#x2265;3-bit errors. When one of the injected errors is activated, such can cause control or data flow divergences which can then be caught by a prepared landing page and thus lead to a successful attack. Our experiments conducted against a guest operating system of a typical cloud hypervisor identified multiple reproducible targets for privilege escalation, shell injection, memory and disk corruption, and advanced denial-of-service attacks. Because the presented rowhammer attack injection (RAI) methodology uses error injection and thus statistical sampling, RAI can quantitatively evaluate the modeled rowhammer attack success probabilities of any given target software states.
[security attacks, statistical sampling, cloud hypervisor, Security evaluation methodology, cloud and hypervisor security, and rowhammer error, Security, security of data, DRAM chips, Software, Hardware, Error correction codes, denial-of-service attacks, error injection, RAI, Reliability, cloud computing, statistical analysis, rowhammer attack injection methodology, quantitative security analysis]
ShareIff: A Sticky Policy Middleware for Self-Destructing Messages in Android Applications
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Self-destructing messaging applications have garnered immense popularity due to the arrival of Snapchat. However, Snapchat's history has shown that building such services on modern mobile platforms is very challenging. In fact, either caused by programming errors or due to the limitations of existing mobile operating systems, in Snapchat and other similar applications it is possible to recover supposedly deleted messages against the senders' expectations, therefore leaving millions of users potentially vulnerable to privacy breaches. This paper presents ShareIff, a middleware for Android that provides an API for secure sharing and display of self-destructing messages. Using this middleware, Snapchat or any similar application, is able to encrypt the message on the sender's endpoint and send it to the recipient such that the message can be decrypted and securely displayed only on the recipient's device for the amount of time specified by the sender. ShareIff provides this property by relying on specialized cryptographic protocols and operating system mechanisms. ShareIff offers application developers a simple programming abstraction and adds marginal overheads to system and app.
[message encryption, cryptographic protocols, Humanoid robots, Programming, Servers, Snapchat, mobile platforms, snapchat, Android (operating system), mobile computing, security, sticky policies, mobile operating system limitations, privacy breaches, Cryptography, sharing security, middleware, Receivers, self-destructing messages, Android middleware, ShareIff, deleted message recovery, programming errors, API, Androids, sticky policy middleware, self-destructing message display, operating system mechanisms]
Lateral Movement Detection Using Distributed Data Fusion
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Attackers often attempt to move laterally from host to host, infecting them until an overall goal is achieved. One possible defense against this strategy is to detect such coordinated and sequential actions by fusing data from multiple sources. In this paper, we propose a framework for distributed data fusion that specifies the communication architecture and data transformation functions. Then, we use this framework to specify an approach for lateral movement detection that uses host-level process communication graphs to infer network connection causations. The connection causations are then aggregated into system-wide host-communication graphs that expose possible lateral movement in the system. In order to provide a balance between the resource usage and the robustness of the fusion architecture, we propose a multilevel fusion hierarchy that uses different clustering techniques. We evaluate the scalability of the hierarchical fusion scheme in terms of storage overhead, number of message updates sent, fairness of resource sharing among clusters, and quality of local graphs. Finally, we implement a host-level monitor prototype to collect connection causations, and evaluate its overhead. The results show that our approach provides an effective method to detect lateral movement between hosts, and can be implemented with acceptable overhead.
[Fuses, Scalability, graph theory, network connection causations, distributed processing, lateral movement detection, distributed data fusion, sensor fusion, communication architecture, resource allocation, coordinated action detection, host-level monitor prototype, Data integration, Intrusion detection, Computer architecture, Robustness, storage overhead, Monitoring, multilevel fusion hierarchy, sequential action detection, fusion architecture robustness, clustering techniques, host-level process communication graphs, resource usage, message updates sent, security of data, pattern clustering, hierarchical fusion scheme, local graph quality, data transformation functions, system-wide host-communication graphs, resource sharing fairness]
Multi-round Master-Worker Computing: A Repeated Game Approach
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
We consider a computing system where a master processor assigns tasks for execution to worker processors through the Internet. We model the workers' decision of whether to comply (compute the task) or not (return a bogus result to save the computation cost) as a mixed extension of a strategic game among workers. That is, we assume that workers are rational in a game-theoretic sense, and that they randomize their strategic choice. Workers are assigned multiple tasks in subsequent rounds. We model the system as an infinitely repeated game of the mixed extension of the strategic game. In each round, the master decides stochastically whether to accept the answer of the majority or verify the answers received, at some cost. Incentives and/or penalties are applied to workers accordingly. Under the above framework, we study the conditions in which the master can reliably obtain tasks results, exploiting that the repeated game model captures the effect of long-term interaction. That is, workers take into account that their behavior in one computation will have an effect on the behavior of other workers in the future. Indeed, should a worker be found to deviate from some agreed strategic choice, the remaining workers would change their own strategy to penalize the deviator. Hence, being rational, workers do not deviate. We identify analytically the parameter conditions to induce a desired worker behavior, and we evaluate experimentally the mechanisms derived from such conditions. We also compare the performance of our mechanisms with a previously known multi-round mechanism based on reinforcement learning.
[Algorithm design and analysis, Computational modeling, master-worker computing, game theory, strategic game, Repeated Games, Algorithmic Mechanism Design, Electronic mail, reinforcement learning, Game Theory, multiround mechanism, stochastic decision making, Games, Learning (artificial intelligence), decision making, repeated game model, Internet, Internet Computing, Reliability, learning (artificial intelligence), stochastic processes, Internet computing system, Master-Worker Task Computing]
Sirius: Neural Network Based Probabilistic Assertions for Detecting Silent Data Corruption in Parallel Programs
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
The size and complexity of supercomputing clusters are rapidly increasing to cater to the needs of complex scientific applications. At the same time, the feature size and operating voltage level of the internal components are decreasing. This dual trend makes these machines extremely vulnerable to soft errors or random bit flips. For complex parallel applications, these soft errors can lead to silent data corruption which could lead to large inaccuracies in the final computational results. Hence, it is important to determine the presence and severity of such errors early on, so that proper counter measures can be taken. In this paper, we introduce a tool called Sirius, which can accurately identify silent data corruptions based on the simple insight that there exist spatial and temporal locality within most variables in such programs. Spatial locality means that values of the variable at nodes that are close by in a network sense, are also close numerically. Similarly, temporal locality means that the values change slowly and in a continuous manner with time. Sirius uses neural networks to learn such locality patterns, separately for each critical variable, and produces probabilistic assertions which can be embedded in the code of the parallel program to detect silent data corruptions. We have implemented this technique on parallel benchmark programs - LULESH and CoMD. Our evaluations show that Sirius can detect silent errors in the code with much higher accuracy compared to previously proposed methods. Sirius detected 98% of the silent data corruptions with a false positive rate of less than 0.02 as compared to the false positive rate 0.06 incurred by the state of the art acceleration based prediction (ABP) based technique.
[temporal locality, Sirius tool, LULESH program, neural networks, reliability, parallel programming, parallel programs, acceleration based prediction, ABP based technique, Hardware, silent data corruption, Mathematical model, soft errors, Meteorology, neural network based probabilistic assertions, Redundancy, probability, silent data corruption detection, CoMD program, Probabilistic logic, security of data, Neural networks, supercomputing clusters, spatial locality, system performance, Transistors, neural nets]
CRUDE: Combining Resource Usage Data and Error Logs for Accurate Error Detection in Large-Scale Distributed Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
The use of console logs for error detection in large scale distributed systems has proven to be useful to system administrators. However, such logs are typically redundant and incomplete, making accurate detection very difficult. In an attempt to increase this accuracy, we complement these incomplete console logs with resource usage data, which captures the resource utilisation of every job in the system. We then develop a novel error detection methodology, the CRUDE approach, that makes use of both the resource usage data and console logs. We thus make the following specific technical contributions: we develop (i) a clustering algorithm to group nodes with similar behaviour, (ii) an anomaly detection algorithm to identify jobs with anomalous resource usage, (iii) an algorithm that links jobs with anomalous resource usage with erroneous nodes. We then evaluate our approach using console logs and resource usage data from the Ranger Supercomputer. Our results are positive: (i) our approach detects errors with a true positive rate of about 80%, and (ii) when compared with the well-known Nodeinfo error detection algorithm, our algorithm provides an average improvement of around 85% over Nodeinfo, with a best-case improvement of 250%.
[event logs, distributed processing, Entropy, anomaly detection, error detection, error logs, faults, Ranger Supercomputer, Clustering algorithms, Nodeinfo error detection algorithm, CRUDE approach, error detection methodology, large-scale HPC systems, true positive rate, detection, console logs, Radiation detectors, anomaly detection algorithm, Supercomputers, large-scale distributed systems, software fault tolerance, unsupervised, resource usage data, Resource management, Detection algorithms, Mutual information]
Achieving High Reliability via Expediting the Repair of Critical Blocks in Replicated Storage Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
High reliability is critical to large data centers consisting of hundreds to thousands of storage nodes where node failures are not rare. Data replication is a typical technique deployed to achieve high reliability. When a node failure is detected, blocks with lost replicas are identified and recovered. Long timeouts are usually used for node failure detection. For blocks with one lost replica, the long timeouts can significantly reduce network traffic induced by data recovery. However, for blocks with two or more lost replicas, which can be caused by concurrent node failures that are not rare in large data centers, the long timeouts will result in a high risk of loss of these blocks. In this paper, we propose MFR to separate the identification of the blocks with two or more lost replicas from that of the blocks with one lost replica in a way that the identification of the blocks with two or more replicas can be accelerated while that of the blocks with one lost replica stays the same. Consequently, MFR can significantly improve data reliability while keeping the network traffic induced by data recovery stable. The results from our simulation and prototype implementation show that MFR improves the reliability of storage systems by a factor of up to 4.0 in terms of mean time to data loss. As blocks with two or more lost replicas are far fewer than blocks with one lost replica, the extra network traffic caused by MFR is less than 0.54% of total network traffic for data recovery.
[Computers, replication, software reliability, data centers, reliability, Maintenance engineering, distributed processing, distributed storage system, data reliability, storage management, Computer network reliability, data recovery, Distributed databases, mean time to data loss, node failure detection, MFR, blocks identification, critical blocks repair, replicated storage systems, Peer-to-peer computing, Reliability, Transient analysis]
Being Accurate Is Not Enough: New Metrics for Disk Failure Prediction
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Traditionally, disk failure prediction accuracy is used to evaluate disk failure prediction model. However, accuracy may not reflect their practical usage (protecting against failures, rather than only predicting failures) in cloud storage systems. In this paper, we propose two new metrics for disk failure prediction models: migration rate, which measures how much at-risk data is protected as a result of correct failure predictions, and mismigration rate, which measures how much data is migrated needlessly as a result of false failure predictions. To demonstrate their effectiveness, we compare disk failure prediction methods: (a) a classification tree (CT) model vs. a state-of-the-art recurrent neural network (RNN) model, and (b) a proposed residual life prediction model based on gradient boosted regression trees (GBRTs) vs. RNN. While prediction accuracy experiments favor the RNN model, migration rate experiments can favor the CT and GBRT models (depending on transfer rates). We conclude that prediction accuracy can be a misleading metric. Moreover, the proposed GBRT model offers a practical improvement in disk failure prediction in real-world data centers.
[Measurement, Cloud computing, Recurrent neural networks, cloud storage systems, data centers, regression analysis, Predictive models, data migration, system recovery, gradient boosted regression trees, disk failure prediction accuracy, data protection, cloud computing, mismigration rate, recurrent neural network, RNN model, computer centres, residual life prediction model, CT model, classification tree model, Hidden Markov models, decision trees, Data models, Reliability, GBRT, neural nets, disc drives]
Tale of Tails: Anomaly Avoidance in Data Centers
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
It is a common practice that today's cloud data centers guard the performance by monitoring the resource usage, e.g., CPU and RAM, and issuing anomaly tickets whenever detecting usages exceeding predefined target values. Ensuring free of such usage anomaly can be extremely challenging, while catering to a large amount of virtual machines (VMs) showing bursty workloads on a limited amount of physical resource. Using resource usage data from production data centers that consist of more than 6K physical machines hosting more than 80K VMs, we identify statistic properties of anomaly instances (AIs) on physical servers, highlighting their burst duration and potential root causes. To strike a tradeoff between a strong performance guarantee and resource provisions, we propose a tail-driven anomaly avoidance policy for boxes, TailGuard, which allows a small fraction of AIs, e.g., 5% of usages can be above the target value, and still avoid severe performance degradation, typically caused by a burst of continuous AI. Specifically, TailGuard first introduces a novel usage tail prediction that explores the similarity patterns across a great number of boxes within a very recent history, and then redistributes the server load in an online fashion by proactive VM cloning and reactive load balancing. Evaluation results show that TailGuard can not only achieve an accuracy comparable with prediction methodology that relies on long history of usage data but also dramatically reduce the number of CPU AIs by 60%, with a tenfold reduction of their duration, from more than 25 time windows to only 2.
[resource usage monitoring, Time series analysis, anomaly avoidance, Random access memory, Cloning, data centers, TailGuard, proactive VM cloning, Servers, History, computer centres, burst duration, resource allocation, security of data, reactive load balancing, Production, virtual machines, VM, anomaly instance property, resource provision, Artificial intelligence]
vMocity: Traveling VMs Across Heterogeneous Clouds
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Current IaaS cloud providers typically adopt different underlying cloud infrastructures and are reluctant to provide consistent interfaces to facilitate cross-cloud interoperability. Such status quo significantly complicates inter-cloud virtual machine relocation and impedes the adoption of cloud services for more enterprises and individual users. In this paper, we propose vMocity, a middleware framework enabling VM relocation across heterogeneous IaaS clouds. vMocity extends the principles of cold migration and decouples VM's storage stack from their underlying virtualization platforms, which presents a homogeneous view of storage to cloud users. We deploy our prototype system across three representative commercial cloud platforms - Amazon EC2, Google Compute Engine, and VMware vSphere-based private cloud. Compared to existing approaches on both synthetic and real-world work-loads, vMocity can significantly reduce the disruption time, up to 27 times shorter, of relocated services and boost the recovery time, up to 1.8 times faster, to pre-relocation performance level. Our results demonstrate that vMocity is efficient and convenient for relocating VMs across clouds, offering freedom of choice to customers when facing a market of IaaS clouds to align with business objectives (cost, performance, service availability, etc.).
[Cloud computing, Amazon EC2, open systems, Booting, Switches, VM storage stack decoupling, business objectives, virtualisation, commerce, cross-cloud interoperability, Engines, virtualization platforms, cold migration, traveling VM, cloud infrastructures, cloud computing, middleware, Google, cloud services, vMocity, heterogeneous IaaS clouds, virtual machines, IaaS cloud providers, Peer-to-peer computing, inter-cloud virtual machine relocation, Google Compute Engine, middleware framework, VM relocation, VMware vSphere-based private cloud, Virtualization]
Storekeeper: A Security-Enhanced Cloud Storage Aggregation Service
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Cloud storage services are currently a commodity that allows users to store data persistently, access the data from everywhere, and share it with friends or co-workers. However, due to the proliferation of cloud storage accounts and lack of interoperability between cloud services, managing and sharing cloud-hosted files is a nightmare for many users. To address this problem, specialized cloud aggregator systems emerged that provide users a global view of all files in their accounts and enable file sharing between users from different clouds. Such systems, however, have limited security: not only they fail to provide end-to-end privacy from cloud providers, but they require users to grant full access privileges to individual cloud storage accounts. In this paper, we present Storekeeper, a privacy-preserving cloud aggregation service that enables file sharing on multi-user multi-cloud storage platforms while preserving data confidentiality from cloud providers and from the cloud aggregator service. To provide this property, Storekeeper decentralizes most of the cloud aggregation logic to the client side enabling security sensitive functions to be performed only on the trusted client endpoints. This decentralization brings new challenges related with file update propagation, access control, user authentication, and key management that are addressed by Storekeeper. This is provided at a low cost (7% on average) when compared with the underlining cloud providers.
[privacy-preserving cloud aggregation service, Cloud computing, security-enhanced cloud storage aggregation service, distributed file systems, cloud storage accounts, Encryption, Servers, key management, cloud-hosted file management, cloud storage aggregation, storage management, security, multiuser multicloud storage platform, file update propagation, security sensitive function, authorisation, user authentication, cloud aggregation logic decentralization, access control, cloud computing, cloud-hosted file sharing, Google, client-server systems, specialized cloud aggregator system, peer-to-peer computing, Secure storage, end-to-end privacy, cloud service interoperability, data confidentiality preservation, data access, Storekeeper, data privacy, trusted client endpoint, cloud providers, data sharing, full access privilege]
VMBeam: Zero-Copy Migration of Virtual Machines for Virtual IaaS Clouds
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Virtual Infrastructure-as-a-Service (IaaS) clouds are emerging for secondary cloud service providers to manage their own IaaS clouds on top of existing IaaS clouds. In virtual IaaS clouds, guest virtual machines (VMs) run inside cloud VMs provided by existing IaaS clouds. Unlike traditional IaaS clouds, they can be migrated between cloud VMs co-located at the same host. However, the performance of such VM migration is low due to slow virtual networks and doubled system loads. To optimize VM migration between co-located cloud VMs, we propose zero-copy migration for virtual IaaS clouds. Zero-copy migration just relocates the memory image of a guest VM without any copy. To enable live migration with negligible downtime, it first makes the memory of a guest VM share with the destination cloud VM and thereafter completes memory relocation. We have implemented a system called VMBeam for enabling zero-copy migration in Xen. According to our experimental results, zero-copy migration could achieve high migration performance and low system loads.
[Xen, Cloud computing, colocated cloud VM, nested virtualization, secondary cloud service providers, memory relocation, zero-copy migration, Virtual machining, infrastructure-as-a-service, guest virtual machines, storage management, Virtual machine monitors, virtual networks, Linux, live migration, virtual machines, Virtual machines, VMBeam, Cryptography, cloud computing, Virtualization, VM migration, virtual IaaS clouds]
Inside-Out: Reliable Performance Prediction for Distributed Storage Systems in the Cloud
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Many storage systems are undergoing a significant shift from dedicated appliance-based model to software-defined storage (SDS) because the latter is flexible, scalable and cost-effective for modern workloads. However, it is challenging to provide a reliable guarantee of end-to-end performance in SDS due to complex software stack, time-varying workload and performance interference among tenants. Therefore, modeling and monitoring the performance of storage systems is critical for ensuring reliable QoS guarantees. Existing approaches such as performance benchmarking and analytical modeling are inadequate because they are not efficient in exploring large configuration space, and cannot support elastic operations and diverse storage services in SDS. This paper presents Inside-Out, an automatic model building tool that creates accurate performance models for distributed storage services. Inside-Out is a black-box approach. It builds high-level performance models by applying machine learning techniques to low-level system performance metrics collected from individual components of the distributed SDS system. Inside-Out uses a two-level learning method that combines two machine learning models to automatically filter irrelevant features, boost prediction accuracy and yield consistent prediction. Our in-depth evaluation shows that Inside-Out is a robust solution that enables SDS to predict end-to-end performance even in challenging conditions, e.g., changes in workload, storage configuration, available cloud resources, size of the distributed storage service, and amount of interference due to multi-tenants. Our experiments show that Inside-Out can predict end-to-end performance with 91.1% accuracy on average. Its prediction accuracy is consistent across diverse storage environments.
[Measurement, cloud storage systems, two-level learning method, Quality of service, Predictive models, Throughput, Inside-Out, storage, black-box approach, machine learning techniques, System analysis and design, storage management, performance interference, performance modeling, performance benchmarking, automatic model building tool, cloud computing, prediction accuracy, Monitoring, storage system performance monitoring, quality of service, cloud resource availability, performance prediction, software-defined storage, time-varying workload, QoS guarantees, complex software stack, system monitoring, distributed SDS system, Reliability, benchmark testing, storage configuration]
DTC: A Dynamic Transaction Chopping Technique for Geo-replicated Storage Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Large Web applications usually require replicating data across geo-distributed datacenters to achieve high locality, durability and availability. However, maintaining strong consistency in geo-replicated systems usually suffers from long latency due to costly coordination across datacenters. Among others, transaction chopping is an effective and efficient approach to cope with such a challenge. In this paper, we propose DTC (Dynamic Transaction Chopping), a novel technique that chops transactions and checks their conflicts in a dynamic and automatic way, during application execution. DTC mainly consists of two parts: a dynamic chopper that chops transaction dynamically according to data partition scheme, and a conflict detection algorithm for determining the safety of the dynamic chopping. Compared with existing transaction chopping technique for geo-replicated systems, DTC has several advantages, including transparency to programmers, flexibility in conflict analysis, high degree of piecewise execution, and adaptability to dynamic partition schemes. We implement our DTC technique and conduct experiments to examine the correctness of DTC and evaluate its performance. The experiment results show that our DTC technique can achieve much more piecewise execution than the existing chopping approach does, and reduce execution time obviously.
[transaction processing, Heuristic algorithms, Memory, Web applications, Servers, storage management, data storage, Databases, conflict detection, dynamic partition, conflict analysis, Safety, data partition, replicated databases, Choppers (circuits), geo-distributed datacenters, strong consistency, Concurrency control, datacenter, computer centres, geo-replication, DTC, transaction chopping, geo-replicated storage systems, data replication, dynamic transaction chopping]
GlobalFS: A Strongly Consistent Multi-site File System
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
This paper introduces GlobalFS, a POSIX-compliant geographically distributed file system. GlobalFS builds on two fundamental building blocks, an atomic multicast group communication abstraction and multiple instances of a single-site data store. We define four execution modes and show how all file system operations can be implemented with these modes while ensuring strong consistency and tolerating failures. We describe the GlobalFS prototype in detail and report on an extensive performance assessment. We have deployed GlobalFS across all EC2 regions and show that the system scales geographically, providing performance comparable to other state-of-the-art distributed file systems for local commands and allowing for strongly consistent operations over the whole system. The code of GlobalFS is available as open source.
[Unix, GlobalFS, application program interfaces, Scalability, public domain software, Metadata, open source code, failure tolerance, Servers, Geographical Distribution, Distributed databases, Prototypes, distributed databases, strongly consistent multisite file system, POSIX-compliant geographically distributed file system, Distributed File System, Strong Consistency, Distributed System, message passing, POSIX-compliant API, file system operation, Systems operation, single-site data store, strong consistency, data integrity, extensive performance assessment, atomic multicast group communication abstraction, file organisation, Reliability, EC2 region]
On the Cost of Safe Storage for Public Clouds: An Experimental Evaluation
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Cloud-based storage services such as Dropbox, Google Drive and OneDrive are increasingly popular for storing enterprise data, and they have already become the de facto choice for cloud-based backup of hundreds of millions of regular users. Drawn by the wide range of services they provide, no upfront costs and 24/7 availability across all personal devices, customers are well-aware of the benefits that these solutions can bring. However, most users tend to forget-or worse ignore-some of the main drawbacks of such cloud-based services, namely in terms of privacy. Data entrusted to these providers can be leaked by hackers, disclosed upon request from a governmental agency's subpoena, or even accessed directly by the storage providers (e.g., for commercial benefits). While there exist solutions to prevent or alleviate these problems, they typically require direct intervention from the clients, like encrypting their data before storing it, and reduce the benefits provided such as easily sharing data between users. This practical experience report studies a wide range of security mechanisms that can be used atop standard cloud-based storage services. We present the details of our evaluation testbed and discuss the design choices that have driven its implementation. We evaluate several state-of-the-art techniques with varying security guarantees responding to user-assigned security and privacy criteria. Our results reveal the various trade-offs of the different techniques by means of representative workloads on top of industry-grade storage services.
[Cloud computing, public cloud, user-assigned security, Encryption, storage, experimental evaluation, Resistance, Privacy, storage management, Google Drive, cloud-based storage services, OneDrive, privacy criteria, public clouds, cloud computing, Dropbox]
Who's On Board?: Probabilistic Membership for Real-Time Distributed Control Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
To increase their dependability, distributed control systems (DCSs) need to agree in real time about which hosts have crashed, i.e., they need a real-time membership service. In this paper, we prove that such a service cannot be implemented deterministically if, besides host crashes, communication can also fail. We define implementable probabilistic variants of membership properties, which constitute what we call a synchronous membership service (SYMS). We present an algorithm, ViewSnoop, that implements SYMS with high-probability. We implement, deploy and evaluate ViewSnoop analytically as well as experimentally, within an industrial DCS framework. We show that ViewSnoop significantly improves the dependability of DCSs compared to membership schemes based on classic heartbeats, at low additional cost. Moreover, ViewSnoop distinguishes, with high probability, host crashes from message losses, enabling DCSs to counteract losses better than existing approaches.
[real-time membership service, distributed control, fault-tolerance, host crashes, dependability, probability, Probabilistic logic, probabilistic membership, Computer crashes, SYMS, industrial DCS framework, synchronous membership service, Heart beat, ViewSnoop, real-time, Decentralized control, Group membership, industrial control, Real-time systems, real-time distributed control systems, Reliability, probabilistic message losses, distributed control systems, failure detection]
UDS: A Novel and Flexible Scheduling Algorithm for Deterministic Multithreading
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Active replication requires deterministic execution in each replica in order to keep them consistent. Debugging and testing need deterministic execution in order to avoid data races and "Heisenbugs". Beside input, multi-threading constitutes a major source of nondeterminism. Several deterministic scheduling algorithms exist that allow concurrent but deterministic executions. Yet, these algorithms seem to be very different. Some of them were even developed without knowing the others. In this paper, we present the novel and flexible Unified Deterministic Scheduling algorithm (UDS) for weakly and fully deterministic systems. Compared to existing algorithms, UDS has a broader parameter set, allowing for many configurations that can be used to adapt to a given work load. For the first time, UDS defines reconfiguration of a deterministic scheduler at run-time. Further, we informally show that existing algorithms can be imitated by a particular configuration of UDS, demonstrating its importance.
[Algorithm design and analysis, program debugging, multi-threading, program testing, Instruction sets, UDS, state machine replication, Debugging, active replication, unified deterministic scheduling algorithm, UDS scheduling algorithm, Scheduling algorithms, deterministic multithreading, scheduling, debugging, Hardware, Timing, deterministic execution]
Practical State Machine Replication with Confidentiality
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
We address the problem of how to store and process data privately in cloud environments that employ state machine replication. We show that the only known solution to the problem (Yin et al., SOSP '03) is potentially susceptible to attacks. We then present a new protocol that is secure in the stronger model we formalize. Our protocol uses only efficient symmetric cryptography, while Yin et al.'s uses costly threshold signatures. We implemented and evaluated our protocol. We show that our protocol is two to three orders of magnitude faster than Yin et al.'s, which is less secure than ours.
[Byzantine fault tolerance, Cloud computing, Protocols, symmetric cryptography, cryptographic protocols, state machine replication, confidentiality, protocol evaluation, Encryption, data confidentiality, finite state machines, Privacy, cloud environment, data privacy, Safety, cloud computing]
Speed for the Elite, Consistency for the Masses: Differentiating Eventual Consistency in Large-Scale Distributed Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Eventual consistency is a consistency model that emphasizes liveness over safety, it is often used for its ability to scale as distributed systems grow larger. Eventual consistency tends to be uniformly applied to an entire system, but we argue that there is a growing demand for differentiated eventual consistency requirements. We address this demand with UPS, a novel consistency mechanism that offers differentiated eventual consistency and delivery speed by working in pair with a two-phase epidemic broadcast protocol. We propose a closed-form analysis of our approach's delivery speed, and we evaluate our complete mechanism experimentally on a simulated network of one million nodes. To measure the consistency trade-off, we formally define a novel and scalable consistency metric that operates at runtime. In our simulations, UPS divides by more than 4 the inconsistencies experienced by a majority of the nodes, while reducing the average latency incurred by a small fraction of the nodes from 6 rounds down to 3 rounds.
[Protocols, replicated databases, update-query consistency with primaries and secondaries, data integrity, large-scale distributed systems, consistency mechanism, Uninterruptible power systems, Convergence, Global Positioning System, data consistency, Eventual consistency, scalable consistency metric, delivery speed, differentiated eventual consistency, two-phase epidemic broadcast protocol, Distributed systems, UPS, Epidemic protocols, Large-scale systems, Clocks, replicated data]
Continuous Authentication and Non-repudiation for the Security of Critical Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
User authentication is a key service, especially for systems that can be considered critical for the data stored and the functionalities offered. In those cases, traditional authentication mechanisms can be inadequate to face intrusions: they usually verify user's identity only at login, and even repeating this step, frequently asking for passwords or PIN would reduce system's usability. Biometric continuous authentication, instead, is emerging as viable alternative approach that can guarantee accurate and transparent verification for the entire session: the traits can be repeatedly acquired avoiding disturbing the user's activity. Another security service that these systems may need is nonrepudiation, which protect against the denial of having used the system or executed some commands with it. The paper focuses on biometric continuous authentication and nonrepudiation, and it briefly presents a preliminary solution based on a specific case study. This work presents the current research direction of the author and describes some challenges that the student aims to address in the next years.
[Protocols, security service, safety-critical software, biometric continuous authentication, authenticity, biometrics (access control), nonrepudiation, continuous authentication, biometrics, security, user activity, critical systems security, non-repudiation, Authentication, message authentication, user authentication, Workstations, Reliability, Usability]
Including Security Monitoring in Cloud Service Level Agreements
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Service providers give assurance on some aspects of the service but, as of today, security monitoring is not one of them. In our work, we aim to allow providers to provide customers with guarantees on security monitoring of their outsourced information system.
[Measurement, Cloud computing, Electronic mail, contracts, cloud service level agreement, security of data, Service Level Agreements (SLAs), outsourced information system, Intrusion Detection Capability, Intrusion detection, SLA verification, Production, security monitoring, IDS, cloud computing, Monitoring, Security Monitoring]
Visualizing and Controlling VMI-Based Malware Analysis in IaaS Cloud
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Security in virtualized environment has known the support of different tools in the low-level detection and analysis of malware. The in-guest tracing mechanisms are now capable of operating at assembly language-, system call-, function call-and instruction-level to detect and classify malicious activities. Therefore, they are producing large amount of data about the state of a target system. However, the integrity of such data becomes questionable whenever the hosting target system is compromised. With virtual machine introspection (VMI), the monitoring tool runs outside the target monitored virtual machine (VM) [1]. Thus, the integrity of retrieved data is ensured even if the target system is compromised. Various works have brought VMI to Infrastructure-as-a-Service (Iaas) cloud environment, allowing the cloud user to run (simultaneous) forensics operations on his production VMs. The associated tracing mechanisms can collect larger amount of data in form of commented behavior traces or unstandardized log records. Thus, a human operator is needed to efficiently parse, represent, visualize and interpret the collected data, to benefit from their security relevance [2]. The use of visualization helps analysts investigate, compare and culster malware samples [3]. Existing visualization tools make use of recorded information to enhance the detection of intrusive behavior or the clustering of malware [4] from the observed system. However, at our knowledge, no existing tools establish a pre-to post-exploitation visualization graphs. We present an approach that enhances the detection and analysis of malware in the cloud by providing the cloud end-users the mean to efficiently visualize the different security relevant data collected through multiple VMI-based mechanisms.
[Cloud computing, invasive software, visualization graphs, Process control, IaaS cloud, Iaas, virtual machine introspection, Security, infrastructure-as-a-service, virtualized environment, Data visualization, data visualisation, Production, virtual machines, cloud environment, Malware, cloud computing, VMI-based malware analysis, Monitoring]
Challenging Anomaly Detection in Complex Dynamic Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Software infrastructures are becoming more and more complex, making performance and dependability monitoring in wide and dynamic contexts such as Distributed Systems, Systems of Systems (SoS) and Cloud environments an unachievable goal. Consequently, it is very difficult to know how all the specific parts, services and modules of these systems behave. This negatively impacts our ability in detecting anomalies, because the boundaries between normal and anomalous behaviors are not always known. The paper describes the context and the targeted problem highlighting the research directions that the student will follow in the next years. In particular, after introducing the relevance of this work with respect to the academic and the industrial state of the art, we carefully define the problem and summarize the main challenges that arise according to such problem definition.
[complex dynamic systems, software infrastructure, performance monitoring, distributed processing, distributed system, anomaly detection, Synchronization, monitoring, Complex systems, multi-layer, complex system, dependability monitoring, systems of systems, Runtime, SoS, Detectors, cloud environment, Feature extraction, system monitoring, distributed systems, Reliability, cloud computing, anomalous behavior, Monitoring]
PlayCloud: A Platform to Experiment with Coding Techniques for Storage in the Cloud
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Cloud based storage services are increasingly popular for storing private and enterprise data. Differentiating themselves over a large range of features, storage providers can catter to the needs of any customer. But among the needs they must satisfy, safety from data loss or data corruption is the most important one. While data corruption stemming from faulty hardware or software is usually covered by those services, it is not the case when the erasure is due to malicious activities from the storage provider itself. Therefore the need for anti-tampering countermeasures needs to be addressed for the customer to feel comfortable enough to use the service. In the context of the SafeCloud project, our goal is to provide an extensible platform to implement and evaluate censorship resistant storage systems for long term storage. By combining classical techniques such as erasure coding and data dispersion with more novel ones like data block entanglement, we aim at giving good anti-tampering guarantees for cloud based storage services. In this paper, we will present some of the work that has been done so far to reach this objective, results and future developments.
[enterprise data storage, data loss, censorship resistant storage systems, erasure coding, storage, cloud based storage services, storage management, coding techniques, Benchmark testing, Libraries, Safety, Cryptography, cloud computing, long term storage, antitampering countermeasures, data block entanglement, Encoding, Censorship, data entanglement, SafeCloud project, security of data, malicious activities, PlayCloud, private data storage, data corruption, classical techniques]
Reliable Event Dissemination in Dynamic Distributed Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
The proposed research addresses the problem of communicating events in a reliable and efficient manner between the producing and consuming devices. To do so, we will focus on the publish/subscribe paradigm. Commonly, this model has assumed a static topology of brokers that optimize event dissemination. The next step is to relax the conditions of the broker network in order to include fault tolerance and mobility in its elements, developing and validating new algorithms for highly dynamic publish/subscribe systems.
[Content-based publish/subscribe, message passing, reliable event dissemination, mobility, fault tolerance, Mobile communication, Topology, dynamic ubiquitous systems, broker network, Simple Routing, Fault tolerance, Wireless sensor networks, dynamic publish/subscribe systems, publish/subscribe paradigm, Network topology, Fault tolerant systems, dynamic distributed systems, communication reliability, client and broker mobility, broker static topology, middleware]
ProCode: A Proactive Erasure Coding Scheme for Cloud Storage Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Common distributed storage systems use data replication to improve system reliability and maintain data availability, but at the cost of disk storage. In order to lower storage costs, data may instead be stored according to erasure codes, but this results in greater network and disk traffic when data blocks are reconstructed following an erasure. These methods are also passive, i.e., they only reconstruct data after failures occur. In this paper, we present a proactive erasure coding scheme (ProCode). We monitor the health of disks via drive failure prediction and automatically adjust the replication factor of data blocks on at-risk disks to ensure data safety. In this way, we achieve fast recovery after disk failures without significantly increasing the storage overhead. ProCode is implemented as an extension to HDFS-RAID used by Facebook. Compared with replication storage and erasure coding, ProCode improves system reliability and availability. Specifically, experimental results show 2 or more orders of magnitude reduction in the average number of data loss events over a 10- year period, a 63% or greater drop in degraded read latency, and a 78% drop in recovery time.
[data safety, cloud storage systems, Predictive models, Drives, system reliability, erasure coding, proactive, distributed storage systems, HDFS-RAID, Fault tolerance, storage management, Fault tolerant systems, drive failure prediction, data availability, cloud computing, proactive erasure coding scheme, disk failures, Monitoring, failure prediction, replication, disk storage, degraded read latency, Encoding, encoding, replication storage, data replication, ProCode scheme]
SwiftER: Elastic Erasure Coded Storage System
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Over the life-cycle of a data object, it may be difficult to determine a priori how much redundancy to store it with. The desired degree of fault-tolerance may change over time, for instance, because the importance of the data changes, or the storage system environment changes. If the redundancy is achieved using replication, then changing the degree of fault-tolerance would mean adding (or removing) replicas - a reasonably straightforward operation. However, if erasure code is used instead (which is preferable, given the significantly lower storage overhead of erasure codes with respect to fully replicated systems), then, while shrinking redundancy can still be achieved similarly, expanding redundancy becomes non-trivial. A naive approach will require re-coding, which is both network resource and computation heavy. In this paper, we explore the possibility of using network coding techniques, to both distribute computational load, as well as reduce network usage, and in the process, speed-up the process of creating additional redundancy. The contributions of this paper are defining the problem and analyzing the theoretical limits by leveraging on and extending the existing literature on regenerating codes to realize erasure coded redundancy elasticity, propose a framework to realize code instances that are amenable to network coding based elastic expansion of redundancy, and integrate and benchmark one such code instance (which happens to be optimal with respect to the aforementioned established theoretical limit) with OpenStack Swift to demonstrate the practicality and advantages of the proposed approach.
[OpenStack Swift, fault-tolerance, program compilers, SwiftER, Minimum-Storage Regenerating code, storage management, elastic erasure coded storage system, Fault tolerant systems, Bandwidth, storage overhead, data object life-cycle, network coding, replicated systems, replicated databases, Redundancy, erasure coded redundancy elasticity, Maintenance engineering, Encoding, Multi-repair code, codes regeneration, Distributed storage, storage system environment, Network coding, Erasure codes, fault tolerant computing]
Encoding-Aware Data Placement for Efficient Degraded Reads in XOR-Coded Storage Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Erasure coding has been increasingly used by distributed storage systems to maintain fault tolerance with low storage redundancy. However, how to enhance the performance of degraded reads in erasure-coded storage has been a critical issue. We revisit this problem from two different perspectives that are neglected by existing studies: data placement and encoding rules. To this end, we propose an encoding-aware data placement (EDP) approach that aims to reduce the number of I/Os in degraded reads during a single failure for general XOR-based erasure codes. EDP carefully places sequential data based on the encoding rules of the given erasure code. Trace-driven evaluation results show that compared to two baseline data placement methods, EDP reduces up to 37.4% of read data on the most loaded disk and shortens up to 15.4% of read time.
[degraded reads, XOR-coded storage systems, Redundancy, distributed processing, Encoding, distributed storage systems, data placement methods, Computer science, EDP approach, trace-driven evaluation, storage management, XOR-based erasure codes, Layout, Fault tolerant systems, Distributed databases, erasure-coded storage, encoding-aware data placement]
Network Aware Reliability Analysis for Distributed Storage Systems
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
It is hard to measure the reliability of a large distributed storage system, since it is influenced by low probability failure events that occur over time. Nevertheless, it is critical to be able to predict reliability in order to plan, deploy and operate the system. Existing approaches suffer from unrealistic assumptions regarding network bandwidth. This paper introduces a new framework that combines simulation and an analytic model to estimate durability for large distributed cloud storage systems. Our approach is the first that takes into account network bandwidth with a focus on the cumulative effect of simultaneous failures on repair time. Using our framework we evaluate the trade-offs between durability, network and storage costs for the OpenStack Swift object store, comparing various system configurations and resiliency schemes, including replication and erasure coding. In particular, we show that when accounting for the cumulative effect of simultaneous failures, the probability of data loss estimates can vary by two to four orders of magnitude.
[Cloud computing, Analytic model, estimation theory, graph theory, software reliability, Estimation, Maintenance engineering, network theory (graphs), Durability, Encoding, durability estimation, Servers, network aware reliability analysis, OpenStack Swift object store, distributed cloud storage system, Repair bandwidth, storage management, Simulation, Bandwidth, Erasure codes, Reliability, Mathematical model, cloud computing]
Collaborative Stabilization
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
In this paper, we present the paradigm of collaborative stabilization that focuses on providing stabilization in the presence of an essential but potentially disruptive environment. By essential, we mean that without the environment actions, stabilization property would be impossible. At the same time, environment actions are not in the control of the program and can be disruptive to the recovery. We demonstrate the need for collaborative stabilization by providing examples where existing paradigms of stabilization are undesirable/insufficient. We compare collaborative stabilization with existing paradigms of stabilization. We identify the complexity of verifying collaborative stabilizing programs and develop theorems that focus on composition of such programs.
[collaborative stabilization, Self-stabilization, fault tolerance, fault-tolerance, Cyber-physical systems, distributed processing, distributed system, Valves, program recovery, system recovery, software fault tolerance, environment, Furnaces, Heating, disruptive environment, Collaboration, Dogs, formal methods, Logic gates, distributed systems]
Adaptive Location Privacy with ALP
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
With the increasing amount of mobility data being collected on a daily basis by location-based services (LBSs) comes a new range of threats for users, related to the over-sharing of their location information. To deal with this issue, several location privacy protection mechanisms (LPPMs) have been proposed in the past years. However, each of these mechanisms comes with different configuration parameters that have a direct impact both on the privacy guarantees offered to the users and on the resulting utility of the protected data. In this context, it can be difficult for non-expert system designers to choose the appropriate configuration to use. Moreover, these mechanisms are generally configured once for all, which results in the same configuration for every protected piece of information. However, not all users have the same behaviour, and even the behaviour of a single user is likely to change over time. To address this issue, we present in this paper ALP (which stands for Adaptive Location Privacy), a new framework enabling the dynamic configuration of LPPMs. ALP can be used in two scenarios: (1) offline, where ALP enables a system designer to choose and automatically tune the most appropriate LPPM for the protection of a given dataset, (2) online, where ALP enables the user of a crowd sensing application to protect consecutive batches of her geolocated data by automatically tuning a given LPPM to fulfil a set of privacy and utility objectives. We evaluate ALP on both scenarios with two real-life mobility datasets and two state-of-the-art LPPMs. Our experiments show that the adaptive LPPM configurations found by ALP outperform static configurations in terms of trade-off between privacy and utility.
[Measurement, Context, Data privacy, user privacy guarantees, Geology, adaptive location privacy, ALP, LPPMs, crowd sensing, mobility data, geolocated data protection, Optimization, Privacy, mobile computing, LBSs, location privacy protection mechanisms, data privacy, location-based services, Sensors, information protection]
On Privacy-Preserving Cloud Auction
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Due to perceived fairness and allocation efficiency, cloud auctions for resource allocation and pricing have recently attracted significant attention. As an important economic property, truthfulness makes bidders reveal their true valuations for cloud resources to maximize their utilities. However, disclosure of one's true value causes numerous security vulnerabilities. Therefore, privacy-preserving cloud auctions are called for to prevent such information leakage. In this paper, we demonstrate how to perform privacy-preserving auctions in clouds that do not leak any information other than the auction results to anyone. Specifically, we design a privacy-preserving cloud auction framework that addresses the challenges posed by the cloud auction context by leveraging the techniques in garbled circuits and homomorphic encryption. As foundations of our privacy preserving cloud auction framework, we develop data-oblivious cloud auction algorithm and basic operations (e.g., comparison, swapping etc.), such that the execution path does not depend on the input. In practical systems with a large number of users and constrained resources, we develop an improved version with a computational complexity of O(n log2 n) in the number of bidders n. We further fully implement our framework and theoretically and experimentally show that it preserves privacy by incurring only limited computation and communication overhead.
[economic property, Cloud computing, Protocols, cloud resources, Encryption, communication overhead, Privacy, resource allocation, allocation efficiency, Pricing, cloud computing, security vulnerabilities, privacy-preserving cloud auction, cryptography, perceived fairness, data-oblivious cloud auction algorithm, homomorphic encryption, information leakage, computation overhead, garbled circuits, data privacy, Resource management, pricing, cloud auctions, computational complexity]
Experiments with Self-Stabilizing Distributed Data Fusion
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
The Theory of Belief Functions is a formal framework for reasoning with uncertainty that is well suited for representing unreliable information and weak states of knowledge. In a previous work, a distributed algorithm for computing data fusion on-the-fly has been introduced, avoiding gathering the data on a single node before computation. In this paper, we present an experimental study of its properties. This algorithm is self-stabilizing and runs on unreliable message passing networks. It converges in finite time whatever is the initialization of the system and for any unknown topology. First we explain the algorithm implementation on an unreliable message passing environment and we implement a simple use-case. Then, by experimenting with this distributed application on a realistic network emulator, we show its interest for enforcing local confidence using close nodes, saving bandwidth and warning dangers. Moreover, we focus on the interesting connections between the data fusion operator and the self-stabilizing properties and we highlight the importance of the discounting.
[Algorithm design and analysis, data fusion, Uncertainty, self-stabilization, discounting, sensor fusion, uncertainty handling, Data integration, experimentation, reasoning with uncertainty, Airplug software distribution, Sensors, belief networks, message passing, belief functions, Receivers, message passing networks, network emulator, distributed algorithm, Message passing, distributed algorithms, Distributed algorithm, self-stabilizing distributed data fusion, finite time, r-operator, fault tolerant computing, self-stabilizing properties, Reliability]
TANGO: Toward a More Reliable Mobile Streaming through Cooperation between Cellular Network and Mobile Devices
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Multimedia streaming is a major mobile application, accounting for more than half of total mobile traffic. Streaming applications usually have a static buffering strategy. For example, buffer size is limited to x minutes of the stream, where x is optimized to provide the best trade-off between minimizing stalls and limiting waste of user's bandwidth and energy resulting from user abandonment. We show that such strategies based on information available on the mobile device alone do not work well when network conditions change dynamically, e.g., connectivity degrades due to congestion. We propose an alternative strategy using the framework called TANGO, based on a novel idea of cooperation between cellular network and mobile devices. By monitoring real-time network conditions and continuously predicting user location, our system is able to predict connectivity degradation in the near term. In such events, a notification is sent to the mobile device so that the streaming application can initiate a mitigation action, such as to pre-cache more content. In simulations based on real user traces, we found that TANGO reduces pause time by 13-72%, significantly outperforming DASH, which is the current state of the art.
[connectivity degradation, mobile streaming, cellular network, Mobile handsets, Global Positioning System, Cellular networks, multimedia streaming, Microprocessors, Computer architecture, Bandwidth, TANGO, static buffering strategy, media streaming, mobile devices, cellular radio]
Model-Checking Assisted Protocol Design for Ultra-reliable Low-Latency Wireless Networks
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Recently, the wireless networking community is getting more and more interested in novel protocol designs for safety-critical applications. These new applications come with unprecedented latency and reliability constraints which poses many open challenges. A particularly important one relates to the question how to develop such systems. Traditionally, development of wireless systems has mainly relied on simulations to identify viable architectures. However, in this case the drawbacks of simulations - in particular increasing run-times - rule out its application. Instead, in this paper we propose to use probabilistic model checking, a formal model-based verification technique, to evaluate different system variants during the design phase. Apart from allowing evaluations and therefore design iterations with much smaller periods, probabilistic model checking provides bounds on the reliability of the considered design choices. We demonstrate these salient features with respect to the novel EchoRing protocol, which is a token-based system designed for safety-critical industrial applications. Several mechanisms for dealing with a token loss are modeled and evaluated through probabilistic model checking, showing its potential as suitable evaluation tool for such novel wireless protocols. In particular, we show by probabilistic model checking that wireless token-passing systems can benefit tremendously from the considered fault-tolerant methods. The obtained performance guarantees for the different mechanisms even provide reasonable bounds for experimental results obtained from a real-world implementation.
[unprecedented latency, Protocols, probabilistic model checking, Token passing, salient features, wireless token-passing systems, ultrareliable low-latency wireless networks, wireless networking community, token-based system, Wireless communication, formal verification, Model checking, model-checking assisted protocol, safety-critical industrial applications, wireless protocols, wireless channels, protocols, validation, fault tolerance, tool-assisted protocol design, Wireless Industrial Networks, Probabilistic timed automata, Probabilistic logic, reliability constraints, formal model-based verification, Automata, token loss, Reliability, EchoRing protocol, fault-tolerant methods, Payloads]
Self-Stabilization - A Mechanism to Make Networked Embedded Systems More Reliable?
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
The erratic behavior of wireless channels is still a major hurdle in the implementation of robust applications in wireless networks. In the past it has been argued that self-stabilization is a remedy to provide the needed robustness. This assumption has not been verified to the extent necessary to convince engineers implementing such applications. A major reason is that the time in which a self-stabilizing system returns to a valid state is unpredictable and potentially unbound. Failure rates typically depend on physical phenomena and in self-stabilizing systems each node tries to react to failures in an inherently adaptive fashion by the cyclic observation of its neighbors' states. When the frequency of state changes is too high, the system may never reach a state sufficiently stable for a specific task. In this paper we substantiate the conditions under which self-stabilization leads to fault tolerance in wireless networks and look at the myths about the power of self-stabilization as a particular instance of self-organization. We investigate the influences of the error rate and the neighbor state exchange rate on the stability and the convergence time on topology information acquired in real network experiments.
[Measurement, Protocols, fault tolerance, wireless sensor networks, self-stabilization, self-stabilizing system, convergence time, wireless networks, topology information, self-stabilization mechanism, Wireless sensor networks, Wireless networks, Fault tolerant systems, embedded systems, networked embedded systems, wireless channels, Reliability]
RDE: Replay DEbugging for Diagnosing Production Site Failures
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Online service failures in production computing environments are notoriously difficult to debug. One of the key challenges is to allow the developer to replay the failure execution within an interactive debugging tool such as GDB. Previous work has proposed in-situ approaches to inferring the production-run failure path within the production environment. However, those tools may sometimes suggest failure execution paths that are infeasible to reach by any program inputs. Moreover, production site often does not record or provide failure-triggering inputs due to the user privacy concern. In this paper, we present RDE, a Replay DEbug system that can replay a production-site failure at the development site within an interactive debugging environment without requiring user inputs. RDE takes an inferred production failure path as input and performs execution synthesis using a new guided symbolic execution technique. RDE can tolerate imprecise or inaccurate failure path information by navigating the symbolic execution along a set of selected paths. RDE synthesizes an input from the selected symbolic execution path which can be fed to a debugging tool to replay the failure. We have implemented an initial prototype of RDE and tested it with a set of coreutils bugs. The results show that RDE can successfully replay all the tested bugs within GDB.
[interactive debugging tool, failure execution, Production systems, program debugging, program diagnostics, replay debugging system, Debugging, production computing, failure analysis, production-run failure, record and replay, RDE system, Computer bugs, Prototypes, symbolic execution, Concrete, Software, data privacy, software tools, production site failure diagnosing, online service failures, user privacy concern]
A Component-Based Middleware for a Reliable Distributed and Reconfigurable Spacecraft Onboard Computer
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Emerging applications for space missions require increasing processing performance from the onboard computers. DLR's project "Onboard Computer - Next Generation" (OBC-NG) develops a distributed, reconfigurable computer architecture to provide increased performance while maintaining the high reliability of classical spacecraft computer architectures. Growing system complexity requires an advanced onboard middleware, handling distributed (real-time) applications and error mitigation by reconfiguration. The OBC-NG middleware follows the Component-Based Software Engineering (CBSE) approach. Using composite components, applications and management tasks can easily be distributed and relocated on the processing nodes of the network. Additionally, reuse of components for future missions is facilitated. This paper presents the flexible middleware architecture, the composite component framework, the middleware services and the model-driven Application Programming Interface (API) design of OBC-NG. Tests are conducted to validate the middleware concept and to investigate the reconfiguration efficiency as well as the reliability of the system. A relevant use case shows the advantages of CBSE for the development of distributed reconfigurable onboard software.
[Computers, reconfigurable computer architecture, application programming interface, component-based middleware, OBC-NG, reconfigurable spacecraft onboard computer, spacecraft computer architecture, space missions, reconfigurable architectures, Computer architecture, reliable distributed spacecraft onboard computer, Monitoring, middleware, distributed reconfigurable onboard software, object-oriented programming, component-based software engineering, space vehicles, distributed computer architecture, Middleware, aerospace engineering, CBSE, Space missions, onboard computer - next generation, API, Reliability]
[Publisher's information]
2016 IEEE 35th Symposium on Reliable Distributed Systems
None
2016
Provides a listing of current committee members and society officers.
[]
Message from the General Co-Chairs
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Message from the Technical Program Co-Chairs
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organizing Committee
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Provides a listing of current committee members and society officers.
[]
Technical Program Committee
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Provides a listing of current committee members and society officers.
[]
Fine-Grained Consistency Upgrades for Online Services
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Online services such as Facebook or Twitter have public APIs to enable an easy integration of these services with third party applications. However, the developers who design these applications have no information about the consistency provided by these services, which exacerbates the complexity of reasoning about the semantics of the applications they are developing. In this paper, we show that is possible to deploy a transparent middleware between the application and the service, which enables a fine-grained control over the session guarantees that comprise the consistency semantics provided by these APIs, without having to gain access to the implementation of the underlying services. We evaluated our middleware using the Facebook public API and the Redis datastore, and our results show that we are able to provide fine-grained control of the consistency semantics incurring in a small local storage and modest latency overhead.
[Algorithm design and analysis, Cloud computing, local storage, transparent middleware, application program interfaces, Session guarantees, Facebook public API, fine-grained consistency upgrades, consistency, Semantics, consistency semantics, social networking (online), Libraries, Internet, modest latency overhead, Facebook, fine-grained control, Redis datastore, Internet online services, middleware, online services]
A Practical Framework for Privacy-Preserving NoSQL Databases
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Cloud infrastructures provide database services as cost-efficient and scalable solutions for storing and processing large amounts of data. To maximize performance, these services require users to trust sensitive information to the cloud provider, which raises privacy and legal concerns. This represents a major obstacle to the adoption of the cloud computing paradigm. Recent work addressed this issue by extending databases to compute over encrypted data. However, these approaches usually support a single and strict combination of cryptographic techniques invariably making them application specific. To assess and broaden the applicability of cryptographic techniques in secure cloud storage and processing, these techniques need to be thoroughly evaluated in a modular and configurable database environment. This is even more noticeable for NoSQL data stores where data privacy is still mostly overlooked. In this paper, we present a generic NoSQL framework and a set of libraries supporting data processing cryptographic techniques that can be used with existing NoSQL engines and composed to meet the privacy and performance requirements of different applications. This is achieved through a modular and extensible design that enables data processing over multiple cryptographic techniques applied on the same database. For each technique, we provide an overview of its security model, along with an extensive set of experiments. The framework is evaluated with the YCSB benchmark, where we assess the practicality and performance tradeoffs for different combinations of cryptographic techniques. The results for a set of macro experiments show that the average overhead in NoSQL operations performance is below 15%, when comparing our system with a baseline database without privacy guarantees.
[generic NoSQL framework, Cloud computing, YCSB benchmark, data processing cryptographic techniques, cloud provider, single combination, configurable database environment, Encryption, NoSQL engines, Cloud Computing, modular database environment, cloud computing, Cryptography, NoSQL framework, cloud computing paradigm, NoSQL databases, privacy guarantees, database services, Cryptographic Technique, Secure Database, cryptography, sensitive information, Encrypted Data, Standards, practical framework, HBase, encrypted data, cloud storage security, NoSQL data storage, data privacy, NoSQL operations performance, privacy-preserving NoSQL databases, Key-Value Store]
Parameterized and Runtime-Tunable Snapshot Isolation in Distributed Transactional Key-Value Stores
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Several relaxed variants of Snapshot Isolation (SI) have been proposed for improved performance in distributed transactional key-value stores. These relaxed variants, however, provide no specification or control of the severity of the anomalies with respect to SI. They have also been designed to be used statically throughout the whole system life cycle. To overcome these drawbacks, we propose the idea of parameterized and runtime-tunable snapshot isolation. We first define a new transactional consistency model called Relaxed Version Snapshot Isolation (RVSI), which can formally and quantitatively specify the anomalies it may produce with respect to SI. To this end, we decompose SI into three "view properties\
[transaction processing, k2-BV, transaction abort rates, data centers, History, k1-version bounded backward view, Chameleon, k2-FV, runtime-tunable consistency, Runtime, Semantics, Prototypes, SI, Silicon, prototype partitioned replicated distributed transactional key-value stores, k1-BV, replicated databases, k2-version bounded forward view, runtime-tunable snapshot isolation, k3-version bounded snapshot view, computer centres, relaxed version snapshot isolation, transactional key-value store, Transactional key-value stores, snapshot view, concurrency control, Delays, RVSI, transactional consistency model]
Robust Multi-Resource Allocation with Demand Uncertainties in Cloud Scheduler
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Cloud scheduler manages multi-resources (e.g., CPU, GPU, memory, storage etc.) in cloud platform to improve resource utilization and achieve cost-efficiency for cloud providers. The optimal allocation for multi-resources has become a key technique in cloud computing and attracted more and more researchers' attentions. The existing multi-resource allocation methods are developed based on a condition that the job has constant demands for multi-resources. However, these methods may not apply in a real cloud scheduler due to the dynamic resource demands in jobs' execution. In this paper, we study a robust multi-resource allocation problem with uncertainties brought by varying resource demands. To this end, the cost function is chosen as either of two multi-resource efficiency-fairness metrics called Fairness on Dominant Shares and Generalized Fairness on Jobs, and we model the resource demand uncertainties through three typical models, i.e., scenario demand uncertainty, box demand uncertainty and ellipsoidal demand uncertainty. By solving an optimization problem we get the solution for robust multi-resource allocation with uncertainties for cloud scheduler. The extensive simulations show that the proposed approach can handle the resource demand uncertainties and the cloud scheduler runs in an optimized and robust manner.
[Cloud computing, optimization problem, Uncertainty, job generalized fairness, Graphics processing units, cloud scheduler, Demand Uncertainties, Optimization, job execution, optimisation, resource allocation, multiresource efficiency-fairness metrics, resource demand uncertainties, scheduling, cost function, Robustness, cloud computing, resource utilization, dynamic resource demands, dominant share fairness, Robust, scenario demand uncertainty, Dynamic scheduling, cloud platform, ellipsoidal demand uncertainty, box demand uncertainty, Cloud Scheduler, robust multiresource allocation problem, Multi-Resource, optimal allocation, cloud providers, Resource management]
Stateless Reliable Geocasting
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
We present two geometric routing algorithms that reliably deliver messages to all devices in a geocast region. One algorithm is based on flooding, the other on concurrent geometric routing. They are the fist known stateless geocasting algorithms. We formally prove the algorithms correct, evaluate their performance through abstract and concrete simulation and estimate their message complexity.
[Algorithm design and analysis, concurrent geometric routing algorithms, simulation, Routing, flooding, Complexity theory, communication complexity, message complexity reliability estimation, stateless routing, Unicast, Wireless networks, geometric routing, telecommunication network routing, multicast communication, telecommunication network reliability, reliable geocasting, Face, Reliability, stateless reliable geocasting algorithms]
Controlling Cascading Failures in Interdependent Networks under Incomplete Knowledge
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Vulnerability due to inter-connectivity of multiple networks has been observed in many complex networks. Previous works mainly focused on robust network design and on recovery strategies after sporadic or massive failures in the case of complete knowledge of failure location. We focus on cascading failures involving the power grid and its communication network with consequent imprecision in damage assessment. We tackle the problem of mitigating the ongoing cascading failure and providing a recovery strategy. We propose a failure mitigation strategy in two steps: 1) Once a cascading failure is detected, we limit further propagation by re-distributing the generator and load's power. 2) We formulate a recovery plan to maximize the total amount of power delivered to the demand loads during the recovery intervention. Our approach to cope with insufficient knowledge of damage locations is based on the use of a new algorithm to determine consistent failure sets (CFS). We show that, given knowledge of the system state before the disruption, the CFS algorithm can find all consistent sets of unknown failures in polynomial time provided that, each connected component of the disrupted graph has at least one line whose failure status is known to the controller.
[Knowledge engineering, recovery strategy, multiple networks, recovery plan, cascading failure, recovery intervention, Power grids, complex networks, power grid, interdependent networks, failure analysis, robust network design, power system faults, failure location, failure status, polynomial time, Communication networks, power grids, Monitoring, damage assessment, communication network, polynomials, failure mitigation strategy, Power Grids, Generators, consistent failure sets, Power system faults, sporadic failures, incomplete knowledge, Cascading failures, Power system protection, Interdependent networks, power system control, CFS algorithm]
A Horizontally Scalable and Reliable Architecture for Location-Based Publish-Subscribe
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
With billions of connected users and objects, location-based services face a massive scalability challenge. We propose a horizontally-scalable and reliable location-based publish/subscribe architecture that can be deployed on a cluster made of commodity hardware. As many modern location-based publish/subscribe systems, our architecture supports moving publishers, as well as moving subscribers. When a publication moves in the range of a subscription, the owner of this subscription is instantly notified via a server-initiated event, usually in the form of a push notification. To achieve this, most existing solutions rely on classic indexing data structures, such as R-trees, and they struggle at scaling beyond the scope of a single computing unit. Our architecture introduces a multi-step routing mechanism that, to achieve horizontal scalability, efficiently combines range partitioning, consistent hashing and a min-wise hashing agreement. In case of node failure, an active replication strategy ensures a reliable delivery of publication throughout the multistep routing mechanism. From an algorithmic perspective, we show that the number of messages required to compute a match is optimal in the execution model we consider and that the number of routing steps is constant. Using experimental results, we show that our method achieves high throughput, low latency and scales horizontally. For example, with a cluster made of 200~nodes, our architecture can process up to 190'000 location updates per second for a fleet of nearly 1'900'000 moving entities, producing more than 130'000 matches per second.
[Scalability, publish/subscribe architecture, query processing, range partitioning, Fault tolerance, mobile computing, Distributed databases, multistep routing mechanism, location-based services, horizontally scalable-reliable architecture, middleware, consistent hashing, Publish subscribe systems, message passing, Data structures, Spatial databases, Mobile nodes, Internet of Things, push notification, min-wise hashing agreement, server-initiated event, telecommunication network routing, Publish-subscribe, active replication strategy, Reliability, node failure]
On the Robustness of a Neural Network
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
With the development of neural networks based machine learning and their usage in mission critical applications, voices are rising against the black box aspect of neural networks as it becomes crucial to understand their limits and capabilities. With the rise of neuromorphic hardware, it is even more critical to understand how a neural network, as a distributed system, tolerates the failures of its computing nodes, neurons, and its communication channels, synapses. Experimentally assessing the robustness of neural networks involves the quixotic venture of testing all the possible failures, on all the possible inputs, which ultimately hits a combinatorial explosion for the first, and the impossibility to gather all the possible inputs for the second.In this paper, we prove an upper bound on the expected error of the output when a subset of neurons crashes. This bound involves dependencies on the network parameters that can be seen as being too pessimistic in the average case. It involves a polynomial dependency on the Lipschitz coefficient of the neurons' activation function, and an exponential dependency on the depth of the layer where a failure occurs. We back up our theoretical results with experiments illustrating the extent to which our prediction matches the dependencies between the network parameters and robustness. Our results show that the robustness of neural networks to the average crash can be estimated without the need to neither test the network on all failure configurations, nor access the training set used to train the network, both of which are practically impossible requirements.
[neuron activation function, Neuromorphic computing, distributed system, Adversarial Machine Learning, Distributed Systems, Machine Learning, Robustness, Hardware, learning (artificial intelligence), communication channels, Lipschitz coefficient, network parameters, Computational modeling, Neurons, Neural Networks, upper bound, Computer crashes, Biological neural networks, Neuromorphics, synapses, neural network based machine learning, mission critical applications, neural nets, polynomial dependency, Fault Tolerance]
Automated Fine Tuning of Probabilistic Self-Stabilizing Algorithms
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Although randomized algorithms have widely been used in distributed computing as a means to tackle impossibility results, it is currently unclear what type of randomization leads to the best performance in such algorithms. This paper proposes three automated techniques to find the probability distribution that achieves minimum average recovery time for an input randomized distributed self-stabilizing protocol without changing the behavior of the algorithm. Our first technique is based on solving symbolic linear algebraic equations in order to identify fastest state reachability in parametric discrete-time Markov chains. The second approach applies parameter synthesis techniques from probabilistic model checking to compute the rational function describing the average recovery time and then uses dedicated solvers to find the optimal parameter valuation. The third approach computes over- and under-approximations of the result for a given parameter region and iteratively refines the regions with minimal recovery time up to the desired precision. The latter approach finds sub-optimal solutions with negligible errors, but it is significantly more scalable in orders of magnitude as compared to the other approaches.
[iterative methods, dedicated solvers, Electronic mail, probabilistic model, optimal parameter valuation, distributed computing, automated fine tuning, rational functions, parametric discrete-time Markov chains, probabilistic self-stabilizing algorithms, probability distribution, protocols, Distributed algorithms, minimum average recovery time, linear algebra, approximation theory, reachability analysis, Computational modeling, probability, Color, Probabilistic logic, symbolic linear algebraic equations, parameter synthesis techniques, randomized algorithms, fastest state reachability, Markov processes, Approximation algorithms, rational function]
Reconfiguring Parallel State Machine Replication
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
State Machine Replication (SMR) is a well-known technique to implement fault-tolerant systems. In SMR, servers are replicated and client requests are deterministically executed in the same order by all replicas. To improve performance in multi-processor systems, some approaches have proposed to parallelize the execution of non-conflicting requests. Such approaches perform remarkably well in workloads dominated by non-conflicting requests. Conflicting requests introduce expensive synchronization and result in considerable performance loss. Current approaches to parallel SMR define the degree of parallelism statically. However, it is often difficult to predict the best degree of parallelism for a workload and workloads experience variations that change their best degree of parallelism. This paper proposes a protocol to reconfigure the degree of parallelism in parallel SMR on-the-fly. Experiments show the gains due to reconfiguration and shed some light on the behavior of parallel and reconfigurable SMR.
[Protocols, multiprocessing systems, parallel SMR, state machine replication, multiprocessor systems, Servers, Synchronization, finite state machines, fault-tolerant systems, Concurrent computing, nonconflicting requests, client requests, Semantics, reconfigurable SMR, Parallel processing, fault tolerant computing, protocols, parallel state Machine Replication, Message systems]
A Statistical Framework on Software Aging Modeling with Continuous-Time Hidden Markov Model
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
This paper considers the statistical approach to model software degradation process from time series data of system attributes. We first develop the continuous-time Markov chain (CTMC) model to represent the degradation level of system. By combining the CTMC with system attributes distributions, a continuous-time hidden Markov model (CT-HMM) is proposed as the basic model to represent the degradation level of system. To estimate model parameters, we develop the EM algorithm for CT-HMM. The advantage of this modeling is that the estimated model is directly applied to existing CTMC-based software aging and rejuvenation models. In numerical experiments, we exhibit the performance of our method by simulated data and also demonstrate estimating the software degradation process with experimental data in MySQL database system.
[system attributes, CTMC-based software aging, CT-HMM, continuous-time Markov chain model, Degradation, hidden Markov models, Software aging, Aging, software engineering, Numerical models, time series data, Mathematical model, continuous time systems, MySQL database system, continuous-time Markov chain, software degradation process, degradation level, Estimation, CTMC model, time series, software aging modeling, continuous-time hidden Markov model, Hidden Markov models, rejuvenation models, Software, estimated model, EM algorithm]
Hybrid-RC: Flexible Erasure Codes with Optimized Recovery Performance and Low Storage Overhead
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Erasure codes are widely used in practical storage systems to prevent disk failure and data loss. However, these codes require excessive disk I/Os and network traffic for recovering unavailable data. As a result, the recovery performance of erasure codes is suboptimal. Among all erasure codes, Minimum Storage Regenerating (MSR) codes can achieve optimal repair bandwidth under the minimum storage during recovery, but some open issues remain to be addressed before applying them in real systems. In this paper, we present Hybrid Regenerating Codes (Hybrid-RC), a new set of erasure codes with optimized recovery performance and low storage overhead. The codes utilize the superiority of MSR codes to compute a subset of data blocks while some other parity blocks are used for reliability maintenance. As a result, our design is near-optimal with respect to storage and network traffic. We show that Hybrid-RC reduces the reconstruction cost by up to 21% compared to the Local Reconstruction Codes (LRC) with the same storage overhead. Most importantly, in Hybrid-RC, each block contributes only half the amount of data when processing a single block failure. Therefore, the number of I/Os consumed per block is reduced by 50%, which is of great help to balance the network load and reduce the latency.
[Minimum Storage Regenerating codes, codes, reliability maintenance, network load, distributed processing, I/O latency, Hybrid-RC, storage management, Systematics, low storage overhead, flexible erasure codes, Distributed databases, Bandwidth, Local Reconstruction Codes, data blocks, degraded read, optimized recovery performance, Maintenance engineering, Encoding, MSR codes, Galois fields, network traffic, Hybrid Regenerating Codes, Reliability, erasure code]
Correlation-Aware Stripe Organization for Efficient Writes in Erasure-Coded Storage Systems
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Erasure coding has been extensively employed for data availability protection in production storage systems by maintaining a low degree of data redundancy. However, how to mitigate the parity update overhead of partial stripe writes in erasure-coded storage systems is still a critical concern. In this paper, we reconsider this problem from two new perspectives: data correlation and stripe organization, and propose CASO, a correlation-aware stripe organization algorithm. CASO captures data correlation of a data access stream. It packs correlated data into a small number of stripes to reduce the incurred I/Os in partial stripe writes, and further organizes uncorrelated data into stripes to leverage the spatial locality in later accesses. By differentiating correlated and uncorrelated data in stripe organization, we show via extensive trace-driven evaluation that CASO reduces up to 25.1% of parity updates and accelerates the write speed by up to 28.4%.
[Correlation, correlation-aware stripe organization algorithm, Redundancy, CASO, erasure coding, Encoding, parity update overhead, encoding, uncorrelated data, data availability protection, storage management, data access stream, parity updates, Organizations, Production, production storage systems, data redundancy, partial stripe]
A Simulation Analysis of Reliability in Erasure-Coded Data Centers
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Erasure coding has been widely adopted to protect data storage against failures in production data centers. Given the hierarchical nature of data centers, characterizing the effects of erasure coding and redundancy placement on the reliability of erasure-coded data centers is critical yet largely unexplored. This paper presents a comprehensive simulation analysis of reliability on erasure-coded data centers. We conduct the analysis by building a discrete-event simulator called SIMEDC, which reports reliability metrics of an erasure-coded data center based on the configurable inputs of the data center topology, erasure codes, redundancy placement, and failure/repair patterns of different subsystems obtained from statistical models or production traces. Our simulation results show that placing erasure-coded data in fewer racks generally improves reliability by reducing cross-rack repair traffic, even though it sacrifices rack-level fault tolerance in the face of correlated failures.
[data center topology, erasure-coded data centers, data centers, reliability, rack-level fault tolerance, erasure coding, storage management, repair pattern, Bandwidth, Production, statistical models, cross-rack repair traffic reduction, computer network reliability, redundancy placement, discrete event simulation, fault tolerance, Redundancy, comprehensive reliabilty simulation analysis, SIMEDC, failure pattern, Maintenance engineering, telecommunication network topology, discrete-event simulator, production traces, Encoding, encoding, computer centres, reliability metrics, production data centers, real-time systems, erasure codes, data storage protection, telecommunication traffic, chunk placement]
Optimal Storage under Unsynchronized Mobile Byzantine Faults
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
In this paper we prove lower and matching upper bounds for the number of servers required to implement a regular shared register that tolerates unsynchronized Mobile Byzantine failures. We consider the strongest model of Mobile Byzantine failures to date: agents are moved arbitrarily by an omniscient adversary from a server to another in order to deviate their computation in an unforeseen manner. When a server is infected by an Byzantine agent, it behaves arbitrarily until the adversary decides to move the agent to another server. Previous approaches considered asynchronous servers with synchronous mobile Byzantine agents (yielding impossibility results), and synchronous servers with synchronous mobile Byzantine agents (yielding optimal solutions for regular register implementation, even in the case where servers and agents periods are decoupled). We consider the remaining open case of synchronous servers with unsynchronized agents, that can move at their own pace, and change their pace during the execution of the protocol. Most of our findings relate to lower bounds, and characterizing the model parameters that make the problem solvable. It turns out that unsynchronized mobile Byzantine agent movements requires completely new proof arguments, that can be of independent interest when studying other problems in this model. Additionally, we propose a generic server-based algorithm that emulates a regular register in this model, that is tight with respect to the number of mobile Byzantine agents that can be tolerated. Our emulation spans two awareness models: servers with and without self-diagnose mechanisms. In the first case servers are aware that the mobile Byzantine agent has left and hence they can stop running the protocol until they recover a correct state while in the second case, servers are not aware of their faulty state and continue to run the protocol using an incorrect local state.
[Protocols, fault diagnosis, synchronous mobile Byzantine agents, Mobile communication, Registers, Servers, History, generic server, regular register implementation, agents periods, Emulation, round free synchronous computation, file servers, regular shared register, asynchronous servers, unsynchronized mobile byzantine faults, case servers, mobile Byzantine failures, synchronous servers, Synchronization, unsynchronized agents, synchronisation, unsynchronized mobile byzantine failures, distributed algorithms, fault tolerant computing, regular register, self-diagnose mechanisms, unsynchronized mobile Byzantine agent movements]
PULP: Achieving Privacy and Utility Trade-Off in User Mobility Data
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Leveraging location information in location-based services leads to improving service utility through geocontextualization. However, this raises privacy concerns as new knowledge can be inferred from location records, such as user's home and work places, or personal habits. Although Location Privacy Protection Mechanisms (LPPMs) provide a means to tackle this problem, they often require manual configuration posing significant challenges to service providers and users. Moreover, their impact on data privacy and utility is seldom assessed. In this paper, we present PULP, a model-driven system which automatically provides user-specific privacy protection and contributes to service utility via choosing adequate LPPM and configuring it. At the heart of PULP is nonlinear models that can capture the complex dependency of data privacy and utility for each individual user under given LPPM considered, i.e., Geo-Indistinguishability and Promesse. According to users' preferences on privacy and utility, PULP efficiently recommends suitable LPPM and corresponding configuration. We evaluate the accuracy of PULP's models and its effectiveness to achieve the privacy-utility trade-off per user, using four real-world mobility traces of 770 users in total. Our extensive experimentation shows that PULP ensures the contribution to location service while adhering to privacy constraints for a great percentage of users, and is orders of magnitude faster than non-model based alternatives.
[Measurement, Data privacy, user mobility data, LPPM, Geology, Urban areas, user-specific privacy protection, geocontextualization, Tuning, model-driven system, Location Privacy Protection Mechanisms, Privacy, mobile computing, Data models, data privacy, location-based services, PULP models]
Incremental Elasticity for NoSQL Data Stores
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Service elasticity, the ability to rapidly expand or shrink service processing capacity on demand, has become a first-class property in the domain of infrastructure services. Scalable NoSQL data stores are the de-facto choice of applications aiming for scalable, highly available data persistence. The elasticity of such data stores is still challenging, due to the complexity and performance impact of moving large amounts of data over the network to take advantage of new resources (servers). In this paper we propose incremental elasticity, a new mechanism that progressively increases processing capacity in a fine-grain manner during an elasticity action by making sub-sections of the transferred data available for access on the new server, prior to completing the full transfer. In addition, by scheduling data transfers during an elasticity action in sequence (rather than as simultaneous transfers) between each pre-existing server involved and the new server, incremental elasticity leads to smoother elasticity actions, reducing their overall impact on performance.
[Schedules, Data store elasticity, NoSQL databases, first-class property, scalable NoSQL data stores, Receivers, Elasticity, Throughput, data transfers, Distributed middleware, Servers, service elasticity, NoSQL data stores, storage management, transferred data, incremental elasticity, Distributed databases, highly available data persistence, scheduling, Data transfer, infrastructure services]
CausalSpartan: Causal Consistency for Distributed Data Stores Using Hybrid Logical Clocks
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Causal consistency is an intermediate consistency model that can be achieved together with high availability and high-performance requirements even in presence of network partitions. In the context of partitioned data stores, it has been shown that implicit dependency tracking using clocks is more efficient than explicit dependency tracking by sending dependency check messages. Existing clock-based solutions depend on monotonic psychical clocks that are closely synchronized. These requirements make current protocols vulnerable to clock anomalies. In this paper, we propose a new clock-based algorithm, CausalSpartan, that instead of physical clocks, utilizes Hybrid Logical Clocks (HLCs). We show that using HLCs, without any overhead, we make the system robust on physical clock anomalies. This improvement is more significant in the context of query amplification, where a single query results in multiple GET/PUT operations. We also show that CausalSpartan decreases the visibility latency for a given data item comparing to existing clock-based approaches. In turn, this reduces the completion time of collaborative applications where two clients accessing two different replicas edit same items of the data store. Like previous protocols, CausalSpartan assumes that a given client does not access more than one replica. We show that in presence of network partitions, this assumption (made in several other works) is essential if one were to provide causal consistency as well as immediate availability to local updates.
[causal consistency, Servers, dependency check messages, query processing, monotonic psychical clocks, Distributed databases, distributed databases, CausalSpartan, intermediate consistency model, Causal consistency, physical clocks, Geo-replication, data store, Key-value stores, Distributed data stores, Hybrid Logical Clocks, Synchronization, partitioned data stores, synchronisation, clocks, implicit dependency tracking, physical clock anomalies, Writing, Delays, explicit dependency tracking, high-performance requirements, hybrid logical clocks, HLC, Clocks, distributed data stores]
DottedDB: Anti-Entropy without Merkle Trees, Deletes without Tombstones
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
To achieve high availability in the face of network partitions, many distributed databases adopt eventual consistency, allow temporary conflicts due to concurrent writes, and use some form of per-key logical clock to detect and resolve such conflicts. Furthermore, nodes synchronize periodically to ensure replica convergence in a process called anti-entropy, normally using Merkle Trees. We present the design of DottedDB, a Dynamo-like key-value store, which uses a novel node-wide logical clock framework, overcoming three fundamental limitations of the state of the art: (1) minimize the metadata per key necessary to track causality, avoiding its growth even in the face of node churn; (2) correctly and durably delete keys, with no need for tombstones; (3) offer a lightweight anti-entropy mechanism to converge replicated data, avoiding the need for Merkle Trees. We evaluate DottedDB against MerkleDB, an otherwise identical database, but using per-key logical clocks and Merkle Trees for anti-entropy, to precisely measure the impact of the novel approach. Results show that: causality metadata per object always converges rapidly to only one id-counter pair; distributed deletes are correctly achieved without global coordination and with constant metadata; divergent nodes are synchronized faster, with less memory-footprint and with less communication overhead than using Merkle Trees.
[Merkle Trees, Distributed Databases, Dynamo-like key-value store, Metadata, node churn, causality metadata, storage management, entropy, Distributed databases, node-wide logical clock framework, Logical Clocks, per-key logical clock, distributed deletes, replicated data, meta data, replicated databases, Generators, Anti-Entropy, DottedDB, delete keys, Partial Replication, Vegetation, antientropy mechanism, metadata minimization, Peer-to-peer computing, Causality, Clocks]
Optimal Cyber-Defense Strategies for Advanced Persistent Threats: A Game Theoretical Analysis
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
We introduce a novel mathematical model that treats network security as a game between cyber attackers and network administrators. The model takes the form of a zero-sum repeated game where each sub-game corresponds to a possible state of the attacker. Our formulation views state as the set of compromised edges in a graph opposed to the more traditional node-based view. This provides a more expressive model since it allows the defender to anticipate the direction of attack. Both players move independently and in continuous time allowing for the possibility of one player moving several times before the other does. This model shows that defense-in-depth is not always a rational strategy for budget constrained network administrators. Furthermore, a defender can dissuade a rational attacker from attempting to attack a network if the defense budget is sufficiently high. This means that a network administrator does not need to make their system completely free of vulnerabilities, they only to ensure the penalties for being caught outweigh the potential rewards gained.
[Cyber attack modeling, Exponential distribution, Network Security, optimal cyber-defense strategies, Cognition, Security, budget constrained network administrators, Zero-Sum Games, graph edges, Game Theory, rational strategy, network administrator, Mathematical model, network security, game theory, rational attacker, expressive model, Game theory, cyber attackers, security of data, Games, defense budget, Data models, Advanced Persistent Threats, zero-sum repeated game, game theoretical analysis]
AutoFlowLeaker: Circumventing Web Censorship through Automation Services
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
By hiding messages inside existing network protocols, anti-censorship tools could empower censored users to visit blocked websites. However, existing solutions generally suffer from two limitations. First, they usually need the support of ISP or the deployment of many customized hosts to conceal the communication between censored users and blocked websites. Second, their manipulations of normal network traffic may result in detectable features, which could be captured by the censorship system. In this paper, to tackle these limitations, we propose a novel framework that exploits the publicly available automation services and the plenty of web services and contents to circumvent web censorship, and realize it in a practical tool named AutoFlowLeaker. Moreover, we conduct extensive experiments to evaluate AutoFlowLeaker, and the results show that it has promising performance and can effectively evade realworld web censorship.
[Automation, anticensorship tools, web services, Tools, Censorship, Encoding, Indexes, computer network security, web censorship, AutoFlowLeaker, network protocols, censored users, publicly available automation services, Web services, blocked websites, censorship system, authorisation, Internet, Cryptography, protocols, Web sites, normal network traffic]
An Unsupervised Multi-Detector Approach for Identifying Malicious Lateral Movement
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Lateral movement-based attacks are increasingly leading to compromises in large private and government networks, often resulting in information exfiltration or service disruption. Such attacks are often slow and stealthy and usually evade existing security products. To enable effective detection of such attacks, we present a new approach based on graph-based modeling of the security state of the target system and correlation of diverse indicators of anomalous host behavior. We believe that irrespective of the specific attack vectors used, attackers typically establish a command and control channel to operate, and move in the target system to escalate their privileges and reach sensitive areas. Accordingly, we identify important features of command and control and lateral movement activities and extract them from internal and external communication traffic. Driven by the analysis of the features, we propose the use of multiple anomaly detection techniques to identify compromised hosts. These methods include Principal Component Analysis, k-means clustering, and Median Absolute Deviation-based outlier detection. We evaluate the accuracy of identifying compromised hosts by using injected attack traffic in a real enterprise network dataset, for various attack communication models. Our results show that the proposed approach can detect infected hosts with high accuracy and a low false positive rate.
[invasive software, lateral movement, graph theory, lateral movement activities, command and control, anomaly detection, Security, Servers, outlier detection, command and control channel, external communication traffic, Command and control systems, graph-based modeling, anomalous host behavior, malicious lateral movement identification, Malware, multiple anomaly detection techniques, private government networks, Monitoring, enterprise network dataset, security state, target system, information exfiltration, attack vectors, attack communication models, advanced persistent threat, injected attack traffic, unsupervised multidetector approach, Anomaly detection, computer network security, lateral movement-based attacks, internal communication traffic, Feature extraction, telecommunication traffic]
Optimal Network Reconfiguration for Software Defined Networks Using Shuffle-Based Online MTD
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
A Software Defined Network (SDN) provides functionalities for modifying network configurations. To enhance security, Moving Target Defense (MTD) techniques are deployed in the networks to continuously change the attack surface. In this paper, we realize an MTD system by exploiting the SDN functionality to optimally reconfigure the network topology. We introduce a novel problem Shuffle Assignment Problem (SAP), the reconfiguration of a network topology for enhanced security, and we show how to compute the optimal solution for small-sized networks and the near-optimal solution for large-sized networks using a heuristic method. In addition, we propose a shuffle-based online MTD mechanism, which periodically reconfigures the network topology to continuously change the attack surface. This mechanism also selects an optimal countermeasure using our proposed topological distance metric in real-time when an attack is detected. We demonstrate the feasibility and the effectiveness of our proposed solutions through experimental analysis on an SDN testbed and simulations.
[Measurement, Moving Target Defense, enhanced security, network topology reconfiguration, Mitigation Technique, Diversity, Security, MTD system, optimal Network reconfiguration, Network topology, Moving Target Defense techniques, large-sized networks, Communication networks, small-sized networks, Software Defined networks, Shuffle, software defined networking, telecommunication network topology, shuffle-based online MTD, topological distance metric, Topology, heuristic method, Computational complexity, computer network security, SDN functionality, Shuffle Assignment Problem, Security Model, Software]
A Greedy-Based Method for Modified Condition/Decision Coverage Testing Criterion
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
During software regression testing, the code coverage of target program is a crucial factor while we perform test case reduction and prioritization. Modified Condition/ Decision Coverage (MC/DC) is one of the most strict and high-accuracy criterion in code coverage and it is usually considered necessary for adequate testing of critical software. In the past, Hayhurst et al proposed a method to implement the MC/DC criterion that complies with regulatory guidance for DO-178B level A software. Hayhurst's MC/DC approach was to find some test cases which are satisfied by MC/DC criterion for each operator (and, or, not, or xor) in the Boolean expression. However, there could be some problems when using Hayhurst's MC/DC approach to select test cases. In this paper, we discuss how to improve and/or enhance Hayhurst's MC/DC approach by using a greedy-based method. Some experiments are performed based on real programs to evaluate as well as compare the performance of our proposed and Hayhurst's approaches.
[Software testing, Greedy algorithms, software regression testing, critical software, Flexible printed circuits, program testing, code coverage, Software algorithms, safety-critical software, greedy-based method, high-accuracy criterion, Fault detection effectivenes, Test suite reduction, Boolean expression, Fault detection, Hayhurst MC-DC approach, test case reduction, DO-178B level A software, Software, modified condition-decision coverage, Modified Condition/ Decision Coverage]
A Resilient Auction Framework for Deadline-Aware Jobs in Cloud Spot Market
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Public cloud providers, such as Amazon EC2, offer idle computing resources known as spot instances at a much cheaper rate compared to On-Demand instances. Spot instance prices are set dynamically according to market demand. Cloud users request spot instances by submitting their bid, and if user's bid price exceeds current spot price then a spot instance is assigned to that user. The problem however is that while spot instances are executing their jobs, they can be revoked whenever the spot price rises above the current bid of the user. In such scenarios and to complete jobs reliably, we propose a set of improvements for the cloud spot market which benefits both the provider and users. Typically, the new framework allows users to bid different prices depending on their perceived urgency and nature of the running job. Hence, it practically allow them to negotiate the current bid price in a way that guarantees the timely completion of their jobs. To complement our intuition, we have conducted an empirical study using real cloud spot price traces to evaluate our framework strategies which aim to achieve a resilient deadline-aware auction framework.
[Checkpointing, Cloud computing, Correlation, Cloud spot market, auction framework, Dynamic scheduling, cloud spot price, Probability distribution, bidding strategy, cloud spot market, social welfare maximization, spot instance prices, public cloud providers, resilient deadline-aware auction framework, cloud users, On-Demand instances, deadline-aware jobs, Pricing, Reliability, cloud computing, pricing, current bid price]
Automated Resource Sharing for Virtualized GPU with Self-Configuration
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
In this paper, we propose Auto-vGPU, a framework of automated resource sharing for virtualized GPU with self-configuration, to reduce manual intervention in system management while ensuring Service Level Agreement (SLA) targets. Auto-vGPU automatically collects the measurements of system metrics and learns a linear model for each application with dimension reduction. In order to fulfill the automated configuration of controller parameters, we propose a self-control-configuration method featuring the theory of automatic tuning of proportional-integral (PI) regulators. The experimental results of cloud gaming implementation demonstrate that Auto-vGPU is able to automatically build the low-dimension model and configure the control parameters without any manual interventions and the derived controller can adaptively allocate virtualized GPU resource to ensure the high performance of cloud applications.
[Auto-vGPU, resourses sharing, self-configuration, SLA, low-dimension model, control, virtualisation, self-control-configuration method, graphics processing units, GPU, cloud gaming implementation, resource allocation, proportional-integral regulators, computer games, PI regulators, automated configuration, automatic tuning, Reliability, cloud computing, Service Level Agreement, virtualized GPU resource]
Performance Modeling of PBFT Consensus Process for Permissioned Blockchain Network (Hyperledger Fabric)
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
While Blockchain network brings tremendous benefits, there are concerns whether their performance would match up with the mainstream IT systems. This paper aims to investigate whether the consensus process using Practical Byzantine Fault Tolerance (PBFT) could be a performance bottleneck for networks with a large number of peers. We model the PBFT consensus process using Stochastic Reward Nets (SRN) to compute the mean time to complete consensus for networks up to 100 peers. We create a blockchain network using IBM Bluemix service, running a production-grade IoT application and use the data to parameterize and validate our models. We also conduct sensitivity analysis over a variety of system parameters and examine the performance of larger networks.
[Protocols, hyperledger fabric, sensitivity analysis, Stochastic processes, SRN, Fault tolerance, IT systems, performance modeling, Fabrics, PBFT consensus process, stochastic processes, Practical Byzantine Fault Tolerance, production-grade IoT application, performance bottleneck, stochastic reward nets, peer-to-peer computing, Computational modeling, permissioned blockchain network, PBFT, Stochastic Reward Nets, Bitcoin, system parameters, cryptography, mean time to complete consensus, Internet of Things, blockchain, IBM Bluemix service, fault tolerant computing]
Detecting TCP-Based DDoS Attacks in Baidu Cloud Computing Data Centers
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Cloud computing data centers have become one of the most important infrastructures in the big-data era. When considering the security of data centers, distributed denial of service (DDoS) attacks are one of the most serious problems. Here we consider DDoS attacks leveraging TCP traffic, which are increasingly rampant but are difficult to detect. To detect DDoS attacks, we identify two attack modes: fixed source IP attacks (FSIA) and random source IP attacks (RSIA), based on the source IP address used by attackers. We also propose a real-time TCP-based DDoS detection approach, which extracts effective features of TCP traffic and distinguishes malicious traffic from normal traffic by two decision tree classifiers. We evaluate the proposed approach using a simulated dataset and real datasets, including the ISCX IDS dataset, the CAIDA DDoS Attack 2007 dataset, and a Baidu Cloud Computing Platform dataset. Experimental results show that the proposed approach can achieve attack detection rate higher than 99% with a false alarm rate less than 1%. This approach will be deployed to the victim-end DDoS defense system in Baidu cloud computing data center.
[telecommunication security, Baidu Cloud Computing data centers, Cloud computing, FSIA, victim-end DDoS defense system, CAIDA DDoS Attack 2007 dataset, TCP-based DDoS, Computer crime, DDoS detection, normal traffic, Real-time systems, IP networks, Decision trees, cloud computing, real-time TCP-based DDoS detection approach, attack modes, attack detection rate, pattern classification, source IP address, distributed denial of service attack, Decision Tree, big-data era, Big Data, random source IP attacks, computer network security, TCP based DDoS attacks, decision tree classifiers, TCP traffic, transport protocols, fixed source IP attacks, decision trees, Feature extraction, malicious traffic, Baidu Cloud Computing Platform dataset, telecommunication traffic, RSIA]
PITR: An Efficient Single-Failure Recovery Scheme for PIT-Coded Cloud Storage Systems
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
In cloud storage systems, the use of erasure coding results in high read latency and long recovery time when drive or node failure happens. In this paper, we design a parity independent array codes (PIT), a variation of STAR code, which is triple fault tolerant and nearly space-optimal, and also propose an efficient single-failure recovery scheme (PITR) for them to mitigate the problem. In addition, we present a "shortened" version of PIT (SPIT) to further reduce the recovery cost. In this way, less disk I/O and network resources are used, thereby reducing the recovery time and achieving a high system reliability and availability.
[Cloud computing, codes, cloud storage systems, reliability, erasure coding, system recovery, parity independent array codes, high system reliability, Fault tolerance, storage management, PITR, disk I/O, PIT-coded cloud storage systems, single-failure recovery scheme, cloud computing, STAR code, erasure coding results, single failure, Encoding, Decoding, recovery time, Upper bound, recovery cost, network resources, fault tolerant computing, Arrays, drive, recovery costs, disc drives]
Runtime Measurement Architecture for Bytecode Integrity in JVM-Based Cloud
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
While Java Virtual Machine can provide applications with safety property to avoid memory corruption bugs, it continues to encounter some security flaws. Real world exploits show that the current sandbox model can be bypassed. In this paper, we focus our work on bytecode integrity measurement in clouds to identify malicious execution and propose J-IMA architecture to provide runtime measurement and remote attestation for bytecode integrity. To the best of our knowledge, our work is the first measurement approach for dynamically-generated bytecode integrity. Moreover, J-IMA has no need for any modification to host systems and any access to source code.
[source code (software), sandbox model, Java, Cloud computing, bytecode integrity measurement, J-IMA architecture, remote attestation, source code, Virtual machining, JVM-based cloud, malicious execution, Security, Java Virtual Machine, Runtime, Current measurement, Computer architecture, virtual machines, dynamically-generated bytecode integrity, safety property, cloud computing, runtime measurement architecture]
An End-To-End Log Management Framework for Distributed Systems
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Logs have been widely employed to ensure the reliability of distributed systems, because logs are often the only data available that records system runtime information. Compared with logs generated by traditional standalone systems, distributed system logs are often large-scale and of great complexity, invalidating many existing log management methods. To address this problem, the paper describes and envisions an end-to-end log management framework for distributed systems. Specifically, this framework includes strategic logging placement, log collection, log parsing, interleaved logs mining, anomaly detection, and problem identification.
[public domain software, data mining, recording, End-To-End Log Management Framework, Complexity theory, Software reliability, distributed system reliability, traditional standalone systems, Anomaly detection, strategic logging placement, Runtime, records system runtime information, system monitoring, learning (artificial intelligence), distributed system logs, interleaved log mining]
Fault-Tolerant Pattern Formation by Multiple Robots: A Learning Approach
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
In the field of multi-robot system, the problem of pattern formation has attracted considerable attention. However, the faulty sensor input of each robot is crucial for such system to act reliably in practice. Existing works focus on assuming certain noise model and reducing the noise impact. In this work, we propose to use a learning-based method to overcome this kind of barrier. By interacting with the environment, each robot learns to adapt its behavior to eliminate the malfunctions in the sensors and the actuators. Moreover, we plan to evaluate the proposed algorithms by deploying it into the multi-robot platform developed in our research lab.
[Pattern formation, learning approach, noise impact reduction, Fault-tolerant, faulty sensor input, Reinforcement learning, multi-robot systems, Multi-roobt system, learning systems, Fault tolerance, Robot kinematics, fault-tolerant pattern formation, Fault tolerant systems, Learning (artificial intelligence), multirobot system, Robot sensing systems, noise model, pattern formation, fault tolerant control]
[Publisher's information]
2017 IEEE 36th Symposium on Reliable Distributed Systems
None
2017
Provides a listing of current committee members and society officers.
[]
