1214
Modeling chaotic behavior of Dhaka Stock Market Index values using the neuro-fuzzy model
2007 10th international conference on computer and information technology
None
2007
Stock market prediction is an important area of financial forecasting, which attracts great interest to stock investors, stock buyers/sellers, policy makers, applied researchers and many others who are involved in the capital market. This paper aims to develop an efficient model to predict the Dhaka stock market index (DSPI) values using the appropriate forecasting model. It is widely believed that stock data are nonlinear, dynamic and chaotic. In this paper, we propose an adaptive network based fuzzy inference system (ANFIS) to predict DSPI values. We used the daily general DSPI values for the period of March 2003 to October10, 2006 for the learning and October 11, 2006 to May 31, 2007 for the validation. Results obtained by this model are also compared to the back-propagation ANN model and the traditional ARIMA model to show advantages of proposed ANFIS model. Findings suggest that the ANFIS model can be used as a better predictor for daily general DSPI values as compared to the ANN and the ARIMA models.
[chaos, fuzzy neural nets, investment, stock market, financial forecasting, Neural network, inference mechanisms, chaotic behavior modeling, adaptive network based fuzzy inference system, stock market prediction, backpropagation, Dhaka Stock Market Index, stock buyers-sellers, back-propagation, ANFIS, economic forecasting, stock markets, fuzzy sets and logics, neuro-fuzzy model, time series forecasting, stock investors, backpropagation ANN model]
A new weighted centroid localization algorithm in wireless sensor networks
2008 11th International Conference on Computer and Information Technology
None
2008
Nodes in a sensor network are often randomly distributed. To assign measurements to locations, each node has to determine its own position. Algorithms for positioning in wireless sensor networks are classified into two groups: approximate and exact. In this paper, we propose a range-based approximate positioning approach which is almost the combination of WCL and EBTB. Then, compare it with two other approximate positioning approaches (WCL with time complexity of O(n)) and EBTB with time complexity of O(n*n) and an exact positioning approach (QR Factorization with time complexity of O(n*n*n)). Finally, it will be shown that EWCL (with time complexity of O(n*n)) is the best localization algorithm with respect to the three other localization algorithms when the noise is high and its accuracy is close to the accuracy of QR when the noise is medium.
[GSM, wireless sensor networks, weighted centroid localization, Electronic components, approximate, time complexity, noisy, Distributed computing, Information technology, Intelligent sensors, Global Positioning System, Wireless sensor networks, positioning, QR factorization, Position measurement, Computer networks, Computer science education, range-based, computational complexity]
A study on distributed diffusion and its variants
2009 12th International Conference on Computers and Information Technology
None
2009
Energy awareness is an essential design issue in wireless sensor networks (WSN). The routing techniques of WSN are classified into three main categories data-centric, hierarchical and location-based. Data-centric technologies perform in-network aggregation of data to yield energy-efficient dissemination; sensor protocols for information via negotiation (SPIN) and directed diffusion (DD) are basic data-centric routing protocols. This paper presents a survey on data-centric routing and specifically focuses on the directed diffusion and its variants (dissemination and aggregation variants) and the protocols that follow the similar concept like directed diffusion.
[Energy consumption, wireless sensor networks, wireless sensor network, Sensor phenomena and characterization, Educational institutions, variants, Floods, Distributed computing, Relays, Wireless sensor networks, distributed diffusion, energy awareness, sensor network, routing protocols, Routing protocols, Computer networks, Energy efficiency, directed diffusion, data centric routing]
A novel ACO technique for fast and near optimal solutions for the Multi-dimensional Multi-choice Knapsack Problem
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper, we have proposed a novel algorithm based on Ant Colony Optimization (ACO) for finding near-optimal solutions for the Multi-dimensional Multi-choice Knapsack Problem (MMKP). MMKP is a discrete optimization problem, which is a variant of the classical 0-1 Knapsack Problem and is also an NP-hard problem. Due to its high computational complexity, exact solutions of MMKP are not suitable for most real-time decision-making applications e.g. QoS and Admission Control for Adaptive Multimedia Systems, Service Level Agreement (SLA) etc. Although ACO algorithms are known to have scalability and slow convergence issues, here we have augmented the traditional ACO algorithm with a unique random local search, which not only produces near-optimal solutions but also greatly enhances convergence speed. A comparative analysis with other state-of-the-art heuristic algorithms based on public MMKP dataset shows that, in all cases our approaches outperform others. We have also shown that our algorithms find near optimal (within 3% of the optimal value) solutions within milliseconds, which makes our approach very attractive for large scale real time systems.
[Algorithm design and analysis, Real time systems, Ant colony optimization, Heuristic algorithms, convergence, Search problems, optimal solution, multidimensional systems, knapsack problems, Convergence, heuristic algorithm, optimisation, Databases, ant colony optimization, real-time systems, NP hard problem, decision making, large scale real time system, search problems, computational complexity, multidimensional multichoice knapsack problem]
Algorithms to predict opening price and trading decision of stocks in Dhaka Stock Exchange
14th International Conference on Computer and Information Technology
None
2011
One of the major tasks in stock market analysis is the discovery of specific events that give rise to a particular event. In this research we emphasize bin partitioning technique on a stock-oriented dataset with a time dimensional approach. We are mainly interested in bringing forward an algorithm for pattern discovery in sequential data streams and also bring out the interdependencies among the events. In our paper, we have proposed and implemented bin partitioning algorithm with real life data from Dhaka Stock Exchange as input. The prime task is of normalizing the data to bring about a form of uniformity among the data, so they could be useful in bringing about the correlation among the attributes in addition to the rules that suggest trading patterns. We also propose a model constructed using the nearest neighbour algorithm, whose main foundation lies behind the fact that stock event/data reflects its own behaviour along the time span. The result found in this research is encouraging enough and offers a new paradigm for stock market forecasting and trading decision about buy or sell a stock.
[nearest neighbours, data mining, data uniformity, regression analysis, precision, accuracy, nearest neighbour algorithm, Data mining, trading decision prediction, stock market analysis, stock markets, opening price prediction, bin partitioning algorithm, stock-oriented dataset, pattern discovery, sequential data stream, prediction theory, Forecasting, data normalization, forecasting theory, regression, stock market forecasting, time dimensional approach, Dhaka Stock Exchange, data handling, temporal data mining]
An approach to real-time portable device for face recognition system
2012 15th International Conference on Computer and Information Technology
None
2012
Face is the most visible organ of human which can be used to differentiate one person from another. Embedded face recognition is quite a challenging era of biometric which can be used in different applications. There are currently commercially available systems for face recognition, but they are bulky, expensive, and proprietary. In this paper we will discuss about a real-time, portable, low cost face recognition system, developed by us, we named it as RPIPD-v1 (Real Time Portable Image Processing Device Version 1) which can work as a standalone system. This system can detect and recognize human faces and there is no such portable device that is capable of having this feature together. Our main aim was to glue these two tasks (Face Detection &amp; Face Recognition) and build a system with the hardware and software to demonstrate the performance. The accuracy of our device is about 60% in distinguishing known and unknown faces. This is a general output of our device for critical situations. For the best performance of recognizing a face we need to highlight the individuals face with proper lighting and the individual's face must be in front of the camera properly. This device is made totally portable and standalone. Small size and low power enables this device to be placed in unique environments, collecting images autonomously for face recognition and other image processing applications. Low cost is another advantage which will allow this device to be used in various places as an effective solution.
[face recognition system, RPIPD-v1, Face Detection, face detection, embedded face recognition, Serial Communication, object detection, arm-Microcontroller, image processing application, Face Recognition, realtime portable image processing device version 1, face recognition, Omni vision CMOS Camera]
Image retrieval using an improved similarity measure: SRIC Similarity with Region Importance and Consistency
2014 17th International Conference on Computer and Information Technology
None
2014
Content based image retrieval has become a major research interest recently. This paper presents an improved image similarity measure for image retrieval system. In the region based image comparison, two images are usually compared in terms of sum of the Euclidean distances among their regions. In this work, the image similarity measure is enhanced through a fuzzyfication of regions' importance and inter-region similarity. First, a &#x201C;Mutual Consistency&#x201D; function is defined for regions' comparison to avoid undesired inter-region similarity. Thereafter, an improved &#x201C;Region Importance&#x201D; function is developed to weight the regions of an image for overall image comparison. Utilizing fuzzy concepts of size and shape features of the regions, these two functions impose additional constrains on similarity measure that helps to improve the image retrieval results. Comparative results with the well-known IRM method are given to illustrate the effectiveness of the proposed approach.
[Computers, inter-region similarity, Shape, image retrieval system, mutual consistency function, fuzzy set theory, fuzzy concepts, Content based image retrieval, Histograms, Accuracy, Image color analysis, Euclidean distances, size features, region based image comparison, region importance function, SRIC, fuzzyfication, Image retrieval, content-based retrieval, Image segmentation, shape features, image retrieval, image similarity measure, content based image retrieval, Region Importance and Mutual Consistency, region importance]
A new background updating model for motion detection considering future frame
16th Int'l Conf. Computer and Information Technology
None
2014
Motion detection is now very much popular for its different type of applications. And the most important thing for motion detection is the background updating procedure. For background updating, all the existing work is done actually depend on the present and past frame. In this paper we proposed a new background updating model for motion detection which mainly differs by considering the future frame. This work is done by taking a number of frames initially to find median from these frames and stored as the past background. Then some frames are taken as future frames and computes a future background from these frames by finding the median. Now if motion occurs in any frame which lies between the past background and this updated future background can be detected easily and accurately by comparing the frames with both of these two updated background. And after some moment the updated future background will be the past background and again some of the frames will be considered as the future frames to calculate the new future background. Our proposed method can update the background accurately as well as can detect motion or moving object perfectly. We have applied this method on various real time data and got promising results.
[Computers, Computational modeling, motion detection, updated future background, object detection, image motion analysis, moving object, future frame, median, background updating, Video surveillance, Cameras, Motion detection, Real-time systems, past and future background, background updating model]
A semantic-based technique for question lassification in question answering systems &#x2014; A hybrid approach
2015 18th International Conference on Computer and Information Technology
None
2015
Question classification plays a substantial role in Question Answering systems. To obtain a semantically rich and syntactically sound question classifier, we present in this paper a semantic-based hybrid approach that explores the meaning underlying the question and seeks to understand the linguistic denotation of the question by constructing a lexically minimalistic syntactic labelled graph from it Questions are classified into one of the six major and fifty finer classes. The system proposes a training method that learns a set of graph traversal rules to detect a question's focus. A standard set containing diverse classes of benchmark questions is used for the training purpose. The system proposes using a semantic memory to store a set of commonly used and unambiguous words that often occur with specific graph traversal rules. A modified technique of the Word Sense Disambiguation using WordNet detects the contextual meaning of a question's focus and maps it to a finer class. A substantial improvement achieved using the approach over other similar systems using similar benchmark data shows that the current approach can be used as a syntactically and semantically rich model in the area of question answering systems.
[pattern classification, major-classes, natural language processing, graph theory, question classification, graph traversal rules, lexically minimalistic syntactic labelled graph, finer-classes, WordNet, question answering systems, semantic-based hybrid approach, word sense disambiguation, question answering (information retrieval), linguistic denotation, semantic memory]
Embedded home surveillance system
2016 19th International Conference on Computer and Information Technology
None
2016
Video surveillance system is widely adopted in order to secure life. This paper presents embedded home surveillance systems to detect intruder in home environment. Proposed system works on embedded Linux board which is equipped with an ordinary web camera. At software level, it uses Open Computer Vision library to detect intruder in two different steps, Histogram of Oriented Gradient and Haar Like Features in association with Support Vector Machine. Moreover, it compares reference histogram to histogram of current image frame by using correlation methods and interpolates change of contents or light intensity in order to optimize the system in terms of false alarm minimization. Along with the Video server, Internet of Things application architecture has been adopted that provides remote monitoring. Alarm service, follows service oriented architecture; execute a set of predefined tasks such as alarming bell, turn on or off switches. This system provides a way to validate each camera module, able to detect device fault, examine image quality and image content changes. The system software provides an architectural solution for intelligent home surveillance system by incorporating Computer Vision and Digital Image Processing algorithms.
[digital image processing, Shape, video server, image content, Service Oriented Architecture, embedded home surveillance, embedded Linux board, Surveillance System, Embedded System, Histograms, embedded systems, Universal Serial Bus, video surveillance, Intruder Detection, support vector machines, Internet of Things, home automation, open computer vision library, image quality, support vector machine, Surveillance, Linux, computer vision, Web camera, Cameras, Feature extraction, service oriented architecture, minimisation, false alarm minimization, Computer Vision]
Optimization of collaborative transportation scheduling in supply chain management with TPL using chemical reaction optimization
2017 20th International Conference of Computer and Information Technology
None
2017
Optimization includes resources and existing technologies at the best possible way. We have studied and analyzed the characteristics of transportation vehicle scheduling problem in supply chain management with third party logistics enterprise. At first, we have classified all the transportation nodes into three distinct classifications. In this paper, a collaborative transportation scheduling strategy is proposed based on two special kinds of transportation nodes that have integrated the self-support vehicle and TPL vehicle resource. Then, depending on the transport criteria of each kind of transportation nodes, we have proposed a chemical reaction optimization algorithm with four reaction operators to minimize the transportation cost. The simulation results demonstrate that the proposed approach is practical and efficient than existing ACO based solution.
[Supply chain management, Supply chains, Transportation, transportation cost, logistics, Optimization, party logistics enterprise, transportation nodes, optimisation, scheduling, transportation vehicle scheduling, Manufacturing, third party logistics, Job shop scheduling, chemical reaction optimization algorithm, transportation vehicle scheduling problem, collaborative transportation scheduling optimization strategy, transportation, transportation cost minimisation, supply chain management, self-support vehicle, collaborative transportation scheduling, TPL vehicle resource, transport criteria, chemical reaction optimization]
On integrating fuzzy knowledge using a Novel Evolutionary Algorithm
2007 10th international conference on computer and information technology
None
2007
Fuzzy systems may be considered as knowledge-based systems that incorporates human knowledge into their knowledge base through fuzzy rules and fuzzy membership functions. The intent of this study is to present a fuzzy knowledge integration framework using a novel evolutionary strategy (NES), which can simultaneously integrate multiple fuzzy rule sets and their membership function sets. The proposed approach consists of two phases: fuzzy knowledge encoding and fuzzy knowledge integration Four application domains, the hepatitis diagnosis, the sugarcane breeding prediction, Iris plants classification, and tic-tac-toe endgame were used to show the performance of the proposed knowledge approach. Results show that the fuzzy knowledge base derived using our approach performs better than genetic algorithm based approach.
[Membership function, Novel Evolutionary Algorithm (NES), evolutionary algorithm, knowledge-based systems, evolutionary computation, fuzzy knowledge encoding, fuzzy knowledge integration, hepatitis diagnosis, tic-tac-toe endgame, knowledge based systems, Fuzzy knowledge, plants classification, sugarcane breeding prediction, Rule set, Evolutionary Algorithms (EA), Mutation, fuzzy systems, Crossover, fuzzy rule sets]
A novel approach of finding planted motif in biological sequences
2007 10th international conference on computer and information technology
None
2007
Motif finding is one of the most important and challenging tasks in computational biology. In this paper, a new algorithm for finding motif is presented. The strategy used in this paper speeds up the operations of finding the motif. Performance comparisons among the existing algorithms and proposed algorithm are also presented which shows that the proposed algorithm performs better in terms of accuracy and time complexity.
[computational biology, biological sequences, Suffix tree, genetics, biology computing, DNA, Genomics, DNA sequence, genomics, planted motif, sequences, Motif]
Human iris recognition for biometric identification
2007 10th international conference on computer and information technology
None
2007
This paper presented an iris recognition system in order to verify both the uniqueness of the human iris and also its performance as a biometric identification. A biometric system provides automatic identification of an individual based on a unique feature or characteristic possessed by the individual. Iris recognition is regarded as the most reliable and accurate biometric identification system available. The iris recognition system consists of an automatic segmentation system that is based on the Hough transform, and is able to localize the circular iris and pupil region, occluding eyelids and eyelashes, and reflections. The extracted iris region was then normalized into a rectangular block with constant dimensions to account for imaging inconsistencies. Finally, the phase data from 1D Log-Gabor filters was extracted and quantized to four levels to encode the unique pattern of the iris into a bit-wise biometric template. The Hamming distance was employed for classification of iris templates, and two templates were found to match if a test of statistical independence was failed.
[iris templates classification, Hamming distance, image classification, biometric identification, Hamming Distance, Hough transform, biometrics (access control), image matching, rectangular block, human iris recognition, automatic segmentation system, statistical independence test, Biometric Identification, 1D Log-Gabor filters, 1D-Log Gabor Filter, image segmentation, Iris Recognition, Hough transforms, face recognition, Automatic Segmentation, Gabor filters, statistical testing, bit-wise biometric template]
Substance based automatic image construction from unordered subjective pieces of digital image
2007 10th international conference on computer and information technology
None
2007
In this paper, we have presented substance or content based image construction from unordered subjective pieces of digital image. Sometime it is necessary to build a complete image from unordered pieces of image to build the complete image. In this developed technique four vector images has been taken from each of the input image to generate training set which in turn will be able to build the expected image. Input vector images have been trained and classified by eigenface method. After classifying the input vector image an efficient algorithm is developed to build the target image.
[image processing, eigenface method, input vector images, Principal Component, Images, digital image, content based image construction, unordered subjective pieces, Eigenface, Cosine Distance, automatic image construction, principal component analysis, Vector Images]
The impact of different types of impurity on theory-softening
2007 10th international conference on computer and information technology
None
2007
Theory-softening is a novel way of utilization of imperfect domain theories. Soft interpretation of given flawed theories provides a way of classifying examples despite the presence of impurities in the theory. However, theory-softening is not equally successful for all possible types of flaws present. This paper investigates the performance of theory-softening and finds that while it is useful for impurities introducing over-specificity, it can become unhelpful and cause degraded performance for impurities introducing over-generality.
[pattern classification, flawed theories, soft interpretation, imperfect domain theories, probability, impurities, Theory revision, learning (artificial intelligence), machine learning, theory-softening, probabilistic theory revision]
Advances in focused retrieval: A general review
2007 10th international conference on computer and information technology
None
2007
When a user is served with a ranked list of relevant documents by the standard document retrieval systems (i.e. search engines), his search task is usually not over. The next step for him is to look into the documents themselves in search for the precise piece of information he was looking for. This method is time consuming, and a correct answer could easily be missed, by either an incorrect query resulting in missing documents or by careless reading. Focused retrieval tries to remove the onus on the end-user, by providing more direct access to relevant information. Focused retrieval is becoming increasingly important in all areas of information retrieval. In this paper we investigate the various aspects of focused retrieval.
[end-user, information retrieval, Question Answering, Passage Retrieval, Element Retrieval, ranked list, focused retrieval, information access, search task, Keywordese, Information Retrieval, document retrieval system, Focused Retrieval]
A new framework for balancing both local and global optimizations in evolutionary algorithms
2007 10th international conference on computer and information technology
None
2007
This paper presents a completely new approach to fulfill both local and global optimization goals simultaneously of the conventional evolutionary algorithm. The basis of the proposed framework is repeatedly alternating three different stages of evolution, each with its own objective and genetic operators. As the stages execute repeatedly, the conflicting goals of local optimization and global exploration are distributed gracefully across the generations of the different stages. The proposed system is compared with classical evolutionary programming (CEP), fast evolutionary programming (FEP) and improved fast evolutionary programming (IFEP) on a number of standard benchmark problems. The experimental results show that the new approach performs better optimization with a higher rate of convergence for most of the problems.
[global optimizations, Evolutionary algorithm, evolutionary algorithms, local optimizations, IFEP, classical evolutionary programming, CEP, fast evolutionary programming, FEP, evolutionary computation, optimisation, local exploitation, improved fast evolutionary programming, optimization, global exploration]
An analytical approach to assess sentiment of text
2007 10th international conference on computer and information technology
None
2007
Sentiment (i.e., bad or good opinion) described in texts has been studied widely, and at three different levels: word, sentence, and document level. This paper describes a well-founded approach for the task of sentence level sentiment analysis by studying the relationship between sentiments conveyed through texts and structure of natural language by a method of numerical analysis. Different approaches have been employed to ldquosenserdquo sentiment, especially from the texts, but none of those ever considered the valence based appraisal structure of sentiments which we have employed. Therefore the paper describes an approach to sense sentiments contained in a sentence by applying a numerical-valence based analysis. To meet this objective a linguistic tool, SenseNet, has been developed that provides lexical-units on the basis of each semantic verb frame obtained from the input sentence; assigns a numerical value to those based on their sense affinity; assesses the values using rules; and finally outputs sense-valence for each input sentence. Several experiments with a variety of datasets containing data from different domains have been conducted. The obtained results indicate significant performance gains over existing state-of-the-art approaches.
[SenseNet, text analysis, linguistic tool, natural language processing, computational linguistics, Affect Sensing from Text, text sentiment analysis, sentence level sentiment analysis, numerical valence based appraisal, semantic verb frame, Sentiment Analysis, natural language]
Automated versus human traffic control for Dhaka and cities of developing nations
2007 10th international conference on computer and information technology
None
2007
Traffic control is a rapidly evolving subject, reflective of new developments in electrical sensor technology, and information and communication technology. In Dhaka, a city of about 12 million people, traffic congestion has worsened dramatically over the last 2-3 years, in spite of the introduction of automated traffic lights. Expensive and drastic measures, such as the construction of flyovers, have been implemented to counter traffic congestion. Although various studies have been conducted on this topic, few or none have identified the many advantages of human traffic control. Dhaka and other cities of developing nations, present traffic situations quite unlike that of developed countries. As a result of improper planning and land management, only about 7 % of the city area consists of road space (compared to about 12% of New Delhi, or 20 % required ideally). In 2004, human traffic control started getting replaced with a modern signaling system, with the good intent of bringing discipline in traffic. In spite of optimistic predictions, it was seen that traffic congestion had become much worse. In the field of information and communication technology, the advantages of human decision-making over automated or microprocessor-based decision making is well recognized. It is proposed and shown here that for Dhaka, human traffic control is a better alternative than automated traffic lights. Human traffic control is preferred for Dhaka, and other developing nations, because of the relatively fewer cars, the few major intersections, and the low cost of human traffic-controllers. As signaling intervals are determined by past measurements of traffic, automated traffic control cannot take into account statistical and event-related variations of traffic. A human traffic controller makes the better decision of allowing through a long line of cars, avoiding the transition times during predetermined changing of signals. He/she does not show green to an empty street with cars waiting in another line. The neural-networks of a traffic controller can assess traffic in visual range, and take intelligent and adaptive decisions. Even with the best road sensor technology, accompanied by telecommunication and fast microprocessors, it is a formidable task for an automated traffic system to match the decision-making capacity of a human traffic controller.
[road traffic, human traffic control, neurocontrollers, traffic control, electrical sensor technology, information and communication technology, neural networks, traffic congestion, microprocessor-based decision making, Dhaka, automated traffic control]
Fully parallel associative memory with human memory type learning model
2007 10th international conference on computer and information technology
None
2007
In this paper, fully parallel associative memory architecture with learning model is proposed. It uses a mixed digital-analog associative memory for reference pattern recognition and a learning model based on a short and long-term memory similar to that in human brain. In addition a ranking mechanism is used to manage the transition of reference vectors between two memories and an optimization algorithm is used to adjust the reference vectors components as well as their distribution continuously. The main advantage of the proposed model is no need to pre-training phase as well as its hardware-friendly structure which makes it implementable by an efficient LSI architecture without requiring a large amount of resources. The system was implemented on an FPGA platform and tested with real data of handwritten and printed English characters and the classification results found satisfactory.
[human memory type learning model, field programmable gate arrays, FPGA, LSI architecture, associative memory, parallel associative memory architecture, optimization, neural net architecture, reference pattern recognition, ranking, learning model, learning (artificial intelligence), automatic learning, content-addressable storage, pattern recognition]
Constrained non-linear optimization by modified particle swarm optimization
2007 10th international conference on computer and information technology
None
2007
This paper presents a modified particle swarm optimization (MPSO) for constrained non-linear optimization problems. Optimization problems are very complex in real life applications. The proposed modified PSO consists of problem (complexity) dependent variable number of promising values (in velocity vector), error-iteration dependent step length, unlocking the dead look of idle particles and so on. It reliably and accurately tracks a continuously changing solution of the complex function and no extra concentration/effort is needed for more complex higher order functions. Constraint management is incorporated in the modified PSO by penalty function. The modified PSO has balance between local and global searching abilities, and an appropriate fitness function helps to converge it quickly. To avoid the method to be frozen, stagnated/idle particles are reset. Finally, benchmark data and methods are used to show the effectiveness of the proposed method.
[modified particle swarm optimization, higher order complex function, evolutionary computation, particle swarm optimisation, error-iteration dependent step length, constraint management, appropriate fitness function, Modified particle swarm optimization, constrained non-linear optimization, constrained nonlinear optimization]
High performance reliable obstacle detection and height measurement by stereo camera for Intelligent Home Service Robot
2007 10th international conference on computer and information technology
None
2007
This paper proposes a new dimensional intelligent robotic application that can track an obstacle throughout its way of moving by using active stereo camera. The central part of the proposed method is the resilient back-propagation algorithm which can intellectually find out the ground even though there have a complex surface. After detecting the obstacle, proposed method measures the height of the object so that it becomes possible for the robot to get on top of the obstacle if its height is negligible. The system is mainly proposed for an Intelligent Home Service Robot to work perfectly in different environment on different condition. A searching algorithm is proposed for the system to detect obstacle by neural-network which can search fast enough to apply it in real-time application. In best case its cost is only a constant and in worst case it also needs short time to find different obstacles.
[stereo camera, collision avoidance, Resilient Back-propagation, Histogram Processing, Triangulation, intelligent home service robot, obstacle detection, height measurement, object detection, Obstacle Height Measurement, intelligent robots, neural network, Obstacle Detection, service robots, backpropagation, stereo image processing, neural nets]
Protein secondary structure prediction with high accuracy using Support Vector Machine
2007 10th international conference on computer and information technology
None
2007
Mining bioinformatics data is an emerging area of research. Proteomics is one of the largest areas of focus in bioinformatics and data mining research. Protein structure prediction is one of the most crucial and decisive problem in all the areas of research. Protein secondary structure can be used for the determination of the tertiary structure via the fold recognition method. Hence, predicting the secondary structures from the proteinpsilas primary sequences has attracted the attention of many researchers. Experimental methods have proved to be complex and expensive. So to develop a simple and accurate method for structure prediction is of great importance. In this paper, a new method has been proposed based on the machine learning technique. The first step of this proposal is to find out frequent patterns of consecutive amino acids in a protein database. After this, a set of frequent words (feature set) is found. Then support vector machine (SVM) is used as a binary/tertiary classifier for the classification of protein secondary structure with these frequent words.
[support vector machines, amino acid, data mining, binary-tertiary classifier, SVM, Support Vector Machine, Protein, support vector machine, biology computing, protein secondary structure prediction, tertiary structure, Secondary Structure, fold recognition method, data mining bioinformatics]
Identification of promoter through stochastic approach
2007 10th international conference on computer and information technology
None
2007
Analysis of a gene sequence, which is transcribed into RNA and then translated into protein, is a difficult task. If this could be achieved, it would make possible better understand how the organisms are developed from DNA information. The behavior of gene is highly influenced by promoter sequences residing upstream or downstream of the Transcription Start Site (TSS). The promoter recognition process is a part of the complex process where genes interact with each other over time and actually regulates the whole working process of a cell. This paper attempts to develop an efficient algorithm that can successfully distinguish promoters and non promoters by analyzing statistical data. A learning model is developed from the known dataset to predict the unknown ones.
[data analysis, Support Vector Machine (SVM), gene sequence, DNA information, Transcriptional Start Site (TSS), promoter recognition process, Promoter, identification, biology computing, deoxyribonucleic acid, DNA, statistical data analysis, learning model, statistical analysis, stochastic processes, promoter sequence, Transcription Start Site, stochastic approach, algorithm]
A novel Fuzzy logic based flux weakening speed control for IPMSM drive with variable direct and quadrature axis inductances
2007 10th international conference on computer and information technology
None
2007
This paper presents a novel flux weakening speed control scheme of an Interior Permanent Magnet Synchronous Motor (IPMSM) using Fuzzy logic controller considering variable direct and quadrature axis inductances. The Fuzzy logic controller (FLC) has been designed on the basis of indirect vector control scheme of the IPMSM drive. The complete vector control scheme of the IPMSM drive incorporating the FLC is simulated for a IPMSM using Matlab/Simulink. The performances of the proposed FLC based IPMSM drive are investigated and compared to those obtained from the conventional proportional integral (PI) controller based drive at various dynamic operating conditions, such as, certain change in command speed, step change in load, etc. The comparative results show that the FLC is more robust and, hence, found to be a suitable replacement of the conventional PI controller for the high performance industrial drive applications.
[magnetic flux, proportional integral controller, PI control, PI Controller, interior permanent magnet synchronous motor, Fuzzy Logic Controller, PI controller, fuzzy control, velocity control, indirect vector control scheme, fuzzy logic controller, High performance drive with variable direct axis and quadrature axis inductances and indirect vector control, machine vector control, IPMSM drive, synchronous motor drives, flux weakening speed control, FLC, Interior Permanent Magnet Synchronous Motor, permanent magnet motors]
Design of EIDI: A cache-based interface to integrate ai and database systems with dynamism
2007 10th international conference on computer and information technology
None
2007
The EIDI (Enhanced Intelligent Database Interface) integrates artificial intelligence and database systems where the AI system can access one or more databases on one or more remote database management systems (DBMSs). The interface can support a wide variety of different DBMSs with static as well as dynamic capabilities. SQL is used to communicate with remote DBMSs and the implementation of the EIDI provides a high degree of portability. The query language of the EIDI is a restricted subset of function free Horn clauses, which are translated to SQL. Results from the EIDI are returned one tuple at a time and the EIDI manages a dynamic cache of result relations to improve efficiency. The objective of this paper is to present firstly a brief overview of the area of AI/DB integration-which worked as the main motivation behind the EIDI, then a discussion on the major features of EIDI and subsequently the architecture and salient components of the EIDI.
[dynamic cache, function free Horn clauses, AI, cache validation, update tracker, query languages, EIDI, cache storage, database management systems, AI/DB integration, artificial intelligence, enhanced intelligent database interface, SQL, remote database management systems, remote procedure calls, EIDI design, cache-based interface, query language, automated schema, database systems]
Recovery of fault-tolerant real-time scheduling algorithm for tolerating multiple transient faults
2007 10th international conference on computer and information technology
None
2007
The consequences of missing deadline of hard real time system tasks may be catastrophic. Moreover, in case of faults, a deadline can be missed if the time taken for recovery is not taken into account during the phase when tasks are submitted or accepted to the system. However, when faults occur tasks may miss deadline even if fault tolerance is employed. Because when an erroneous task with larger execution time executes up to end of its total execution time even if the error is detected early, this unnecessary execution of the erroneous task provides no additional slack time in the schedule to mitigate the effect of error by running additional copy of the same task without missing deadline. In this paper, a recovery mechanism is proposed to augment the fault-tolerant real-time scheduling algorithm RM-FT that achieves node level fault tolerance (NLFT) using temporal error masking (TEM) technique based on rate monotonic (RM) scheduling algorithm. Several hardware and software error detection mechanisms (EDM), i.e. watchdog processor or executable assertions, can detect an error before an erroneous task finishes its full execution, and can immediately stops execution. In this paper, using the advantage of such early detection by EDM, a recovery algorithm RM-FT-RECOVERY is proposed to find an upper bound, denoted by Edm Bound, on the execution time of the tasks, and mechanism is developed to provide additional slack time to a fault-tolerant real-time schedule so that additional task copies can be scheduled when error occurs.
[software error detection mechanisms, rate monotonic scheduling algorithm, NLFT, temporal error masking, larger execution time, fault-tolerant real-time scheduling algorithm, EDM, Real-Time Fault-Tolerant systems, Fixed-Priority Scheduling, node level fault tolerance, recovery mechanism, real-time systems, scheduling, fault tolerant computing, multiple transient faults, TEM]
Integration of GPS-24, GEO &#x00026; LEO: Modeling, receiver selection algorithm and performance analysis for navigation
2007 10th international conference on computer and information technology
None
2007
In this research study, an integrated network of GPS-24, GEO and LEO has been designed to improve the worst-case precision values. The precision values to be realized by the receiver with an algorithm have been proposed. It is expected that a good worldwide availability of best four selected orbiting satellites among many having maximum coverage would be the preferred constellation for navigation. A new approach for minimum dilution of precision (DOP) determination using integrated constellation is developed. Satellite visibility through GPS receiver has been investigated using complex simulations of failure scenarios by giving shadow effect. This work is to improve coverage and satellite visibility for accurate, flexible, fast and cost-effective navigation purpose using integrated network. Using a 3-step algorithm a set of best four satellites selection is possible by minimum calculation by determining minimum DOP. A measurement for positioning, satellites at low elevation angles and at high elevation angles has been compared. The proposed model and algorithm will hopefully be used for navigation purposes in the worst case. The system can also be used academically as well as for commercial purpose.
[satellite visibility, GPS receiver, Navigation, integrated constellation, GPS Receiver, orbiting satellites, radio receivers, worst-case precision values, Global Positioning System, elevation angles, GPS-24, GEO, artificial satellites, dilution of precision determination, LEO, satellite navigation, performance analysis, receiver selection algorithm]
NP-Completeness of the minimum edge-ranking spanning tree problem on series-parallel graphs
2007 10th international conference on computer and information technology
None
2007
The minimum edge-ranking spanning tree (MERST) problem on a graph is to find a spanning tree of G whose edge-ranking needs least number of ranks. Although polynomial-time algorithm to solve the minimum edge-ranking spanning tree problem on series-parallel graphs with bounded degrees has been found, but for the unbounded degrees no polynomial-time algorithm is known. In this paper, we prove that the minimum edge-ranking spanning tree problem on general series-parallel graph is NP-complete.
[NP-Completeness, trees (mathematics), series (mathematics), series-parallel graph, series-parallel graphs, Algorithm, NP-completeness, spanning tree, MERST problem, minimum edge-ranking spanning tree problem, polynomial-time algorithm, edge-ranking, computational complexity]
On the context of popular input methods and analysis of phonetic schemes for Bangla users
2007 10th international conference on computer and information technology
None
2007
This paper analyzes the recently popular Bangla input mechanisms as such phonetic input schemes with traditionally popular touch type layouts. Traditionally, phonetic input schemes are regarded as inefficient. On the ground of popularity of phonetic schemes, this paper tries to find the answer why such kind of supposedly inefficient approach can gain rapid popularity. Is it only due to the learning requirement of another layout or there are real benefits using these? In this regard, the phonological correlation between English and Bangla is established showing that phonetic schemes are not as inefficient as it was supposed to be. Subsequently, the popular phonetic and touch type layouts are analyzed based on Bangla corpus analysis of four million words. The mean number of keystrokes for inputting a single Bangla character is calculated for comparison of different layouts. The analysis establishes that on the basis of keystroke minimization there exists phonetic layouts which are even better than traditional touch type input methods.
[keystroke minimization, Bangla input mechanisms, Bangla corpus analysis, Bangla Computing, natural language processing, Keyboard Layout, speech processing, phonological correlation, phonetic schemes]
Segmenting bangla text for optical recognition
2007 10th international conference on computer and information technology
None
2007
One of the important reasons for poor recognition rate in optical character recognition (OCR) system is the error in character segmentation. Existence of different type of characters in the scanned documents is a major problem to design an effective character segmentation procedure. In this paper, a new technique is presented for identification and segmentation of Bengali printed characters. This paper focuses on the segmentation of printed Bengali characters for efficient recognition of the characters. Our Line segmentation success rate is 99.7 % for 1000 lines, we have tested. Our Word segmentation success rate is 99.8 % for 4900 words tested. From the experiment we noticed that isolated characters fall into isolated group in 99.50 % cases. Most of the errors come from connected characters and characters having tau in front of them as segmenting tau we take the help of width. From the experiment we noticed that most of the errors came from components having multi-touching points between two characters.
[text analysis, word segmentation, Bangla Text segmentation, Bangla Language Processing, optical character recognition, natural language processing, Bangla OCR, character segmentation procedure, Bengali printed characters, line segmentation, text segmentation]
A study on text summarization techniques and implement few of them for Bangla language
2007 10th international conference on computer and information technology
None
2007
Text summarization is the technique which automatically creates an abstract or summary of a text. The technique has been developed for many years. So a survey has been done on different summarization techniques. No work in this area has been done for Bangla language. This paper presents a text summarizer for Bangla, which uses some extraction methods for text summarization.
[summary, abstracting, text analysis, natural language processing, abstraction, text abstracting, text summarization, textsummarization, automated summarization, Bangla language, extraction]
Building a foundation of HPSG-based treebank on Bangla language
2007 10th international conference on computer and information technology
None
2007
Now a day, the importance of a large annotated corpus for NLP researchers is widely known. In this paper, we describe an initial phase of developing a linguistically annotated corpus for non-configurational dasiaBanglapsila language. Since, the formalism differs from those posited for configurational languages; several features have been added for constraint based parsing through HPSG-based formalism. We propose an outline of a semi-automated process by applying both case marking approach and some morphological analysis to constraint the parsing of a relatively free word order language for creating a linguistically rich, highly-lexicalized annotated corpus.
[non-configurational, natural language processing, free word order language, HPSG-based treebank, head-driven phrase structure grammar formalism, hpsg, parsing, lexicalized annotated corpus, context-free languages, NLP, context-free grammars, tree data structures, Bangla language, treebanking, treebank]
WebELS e-learning system: Online and offline viewing of audio and cursor syncronised slides
2007 10th international conference on computer and information technology
None
2007
This paper discusses WebELS e-learning platform for post-graduate education which supports a special type of contents i.e. slide with synchronized audio and cursor in addition to traditional multimedia contents and conceptually it is a fusion of synchronous and asynchronous e-learning system. WebELS provides a Web-based, multimedia enabled multi-platform tool, by which traditional instructors can archive their learning materials on the web and students can do their personal learning over the Internet. Uploaded contents can be used either in standalone or group learning in real-time with discussion. WebELS supports wide range of multimedia contents including text, images, audio, video and slides with synchronized audio and cursor. Majority of the presentation video can be simulated with audio and cursor synchronized slides except introduction and conclusion parts where cameraman focuses on the face of the presenter. With audio and cursor synchronized slides, it is possible simulate presentation video which drastically reduces data volume and improves contents visualisation. The system supports both synchronized online presentation as well as asynchronous off-line viewing.
[Multimedia contents, personal learning, asynchronous off-line viewing, multimedia computing, WebELS e-learning system, multimedia enabled multi-platform tool, contents visualisation, multimedia contents, post-graduate education, web-based education, computer aided instruction, Internet, e-Learning and post-graduate education, offline viewing]
5D virtual constructions: Designer/constructor&#x2019;s perspective
2007 10th international conference on computer and information technology
None
2007
The construction activity level intricacy makes the whole construction process quite complicated and difficult for planners and builders. 4D (3D+Time) construction planning is a promising area, and it presents the ability to represent the construction process with the additional sequential dimension. It makes the virtual models easy to analyze and plan for the sequence of activities. Traditionally, architectural/construction engineering and technology education, especially construction scheduling has been dependent on bar charts and network diagrams. However, students can hardly understand the schedule-construction progress relationship using a CPM network or a bar chart. Using 4D visualization students can learn time-space relationship in construction schedule more effectively. The objective of this research was to create a user-friendly 5D (3D+time+cost) model by adding the cost of project with reference to the time line so that the planners would be able to peg the cost control measures with the schedule. The study provides a structured method and a systematic approach that will enable the students and planners to develop 5D models without worrying about the errors and software hitches. This 5D visualization model will facilitate developing a more realistic approach to the whole construction process. The entire model is integrated to produce an interactive visualization to make the process fairly easier for students as well as the construction industry professionals. In addition to this, changes to the 5D model can be done easily by triggering changes at one level. The techniques demonstrated through 5D virtual construction models can potentially be a valuable tool not only in the classroom, but also as an effective learner-centered self-directed tool to learn planning and construction processes.
[5D virtual constructions, CPM network, Cost Management, Link System, Schedule Management, construction engineering education, civil engineering computing, 4D/5D CAD, construction industry professionals, engineering education, user-friendly 5D model, planning, construction planning, data visualisation, Construction Visualization, scheduling, architectural engineering education, computer aided instruction, construction scheduling, 4D visualization, 3D Drawings]
Designing a low cost microcontroller-based device for multipurpose learning
2007 10th international conference on computer and information technology
None
2007
A low cost microcontroller based device for multipurpose electronic learning has been developed. This paper describes the design methodology and the development of its hardware and software. Four applications and the development of the general purpose device have been explained in this paper. The technique presented can be applied, following the approach described in a small or in a large project.
[design methodology, multipurpose electronic learning, Embedded systems, microcontroller, In-System Programming, embedded systems, integrated circuit design, microcontroller-based device design, IC, microcontrollers]
PAFS: A structured &#x00026; scalable p2p file sharing system over the internet
2007 10th international conference on computer and information technology
None
2007
The term dasiapeer-to-peerpsila (P2P) refers to a class of systems and applications that employ distributed resources to perform a critical function in a decentralized manner. We have successfully implemented a pure P2P system, dasiaPAFSpsila (peer-to-peer application for file sharing) that is fully decentralized in its manner, for sharing files over the Internet. We present all the necessary operations that we have implemented for our system and discuss the scalability of our system. We also propose a model in this paper that can be used to include NAT within our system.
[NAT, decentralized system, scalable system, peer-to-peer computing, File Sharing, file sharing system, distributed resources, peer-to-peer system, Internet, DIHA, p2p]
The impact of data send rate, node velocity and transmission range on QoS parameters of OLSR and DYMO MANET routing protocols.
2007 10th international conference on computer and information technology
None
2007
QoS is the collective effect of service performance which determines the degree of satisfaction of a service. Since it is the collective effect of a service, it must have significant impact on the performance of routing protocols of any network. In MANET where the network has no fixed infrastructure and eventually the topology is continuously changing, QoS is a more challenging issue. However, the ability of a MANET to provide adequate quality of service (QoS) is limited by the ability of the underlying routing protocol and MANETpsilas QoS requirements can be quantified in terms of Packet Delivery Fraction, Average end-to-end delay of data packets and Normalized Routing Load. So, this paper, for the first time, presents the effect of data send rate, node velocity and transmission range on QoS parameters of the two contrasting MANET routing protocols OLSR and DYMO. These two protocols are simulated and compared with NS-2 under Gauss Markov mobility model. The simulation results show significant QoS performance differences.
[Gauss Markov mobility model, Gauss Markov model, quality of service, NS-2, QoS parameter, MANET, mobile communication, QoS, routing protocols, MANET routing protocol, Gaussian processes, DYMO, Markov processes, OLSR, optimized link state routing protocol, ad hoc networks, dynamic MANET ondemand protocol]
Developing an extensible framework for content based searching in super peer p2p network
2007 10th international conference on computer and information technology
None
2007
Searching is an important factor in p2p network for content retrieval. Most of the searches in p2p system are title-based with their limited functionality. Without knowing the unique filename we canpsilat retrieve the content of the file in title based search. Here super peer p2p network is designed that supports content-based search for relevant documents. At the beginning, a general and extensible framework is proposed which is based on hierarchical summary structure for searching similar documents in p2p network. The summary structure is formed by Vector Space Model (VSM), Latent Semantic Indexing (LSI) and Singular Value Decomposition (SVD) techniques. Than an effective document searching is developed by summarizing and maintaining all documents within the network with different factors. Finally at the end, the experimental result is verified on a real p2p prototype and large scale network. The results show the effectiveness, efficiency and scalability of the proposed system.
[super peer p2p network, content retrieval, peer-to-peer computing, latent semantic indexing, vector space model, content-based retrieval, Hierarchical summary, Title-based search, Content search, Peer-to-peer, title-based search, singular value decomposition, content based document searching, Indexing]
A new CAC protocol for optimizing revenue and ensuring QoS
2007 10th international conference on computer and information technology
None
2007
Call admission control (CAC) protocol will try to be fair, fast, reducing inconveniences, and maintaining quality of service (QoS) at the time of deciding whether a call request will be accepted or rejected into a network or communication system. A good CAC should have the ability to select the calls among the requested calls in such a way so that maximum fairness is justified by its performances at the sometime it should achieve some targets such as maximizing the revenue considering the constraints of the network system such as limited bandwidth. In the todaypsilas communication world the demand of bandwidth increases sharply due to the different users different types of service requirements such as audio, video, voice etc. from the same communication system. Besides need to deal with different tariff. So considering all these issues a CAC needs to decide which call requests should be admitted or not, which is obviously a critical and difficult task. In this paper a new CAC protocol is proposed that can select the best calls for the communication system, it can optimize the generated revenue from the network system, it can handle the different call admission situations in a fairness way, it can consider the different priorities based on class, can maximize the revenue of the communication system and also maintains the overall QoS for any network or communication system. Empirical results have shown its excellent performances for any network and communication platform.
[Call Admission Control, radio networks, call admission control protocol, telecommunication congestion control, CAC protocol, QoS, Bandwidth, Revenue, quality of service, Optical Network, protocols, Channel Utilization]
Channel spacing tunable multiwavelength EDF laser based on single segmented polarization maintaining fiber Lyot-Sagnac loop filter
2007 10th international conference on computer and information technology
None
2007
A channel-spacing tunable and switchable multiwavelength Erbium-doped fiber laser (MW-EDFL) operating at room temperature is studied using available experimental data. The multiwavelength generation is achieved mainly based on a single segmented polarization maintaining fiber (PMF) in Lyot-Sagnac loop filter (LSLF) and an all-fiber phase modulator. The channel space tuning is performed by means of a polarizer and the DC offset voltage of a Lithium-Niobate (LiNbO<sub>3</sub>) amplitude modulator and multiwavelenth switching is performed using polarization controller, those are incorporated in the PMF-LSLF. It is found that discrete channel-spacing of 0.27 nm to 1.62 nm and corresponding time delay between two orthogonal polarization states of the beam of 30 ps to 5 ps, respectively, can be achieved. A spectral continuum is also achieved by introducing a time delay of 0.895 ps. On top of the experimental study, a mathematical model of channel-spacing tunable PMF-LSLF, which is used in the laser scheme, is also investigated using MATLABTM.
[Lyot-Sagnac loop filter, multiwavelenth switching, erbium, polarization controller, single segmented polarization maintaining fiber, Lyot-Sagnac Loop Filter (LSLF), Erbium-Doped Fiber Laser (EDFL), channel spacing tunable multiwavelength EDF laser, switchable multiwavelength Erbium-doped fiber laser, multiwavelength generation, orthogonal polarization states, MATLAB, optical fibre polarisation, niobium compounds, optical filters, fibre lasers, Multiwavelength, All-Fiber Phase Modulator (PM), all-fiber phase modulator, Polarization Controller, lithium compounds, optical modulation, amplitude modulation, Lithium-Niobate amplitude modulator, Polarization Maintaining Fiber (PMF), DC offset voltage, phase modulation]
Lower bound on number of planes for vertically stacked optical banyan networks with link failures
2007 10th international conference on computer and information technology
None
2007
Vertically stacked optical banyan (VSOB) networks are attractive for serving as optical switching systems due to the good properties of banyan network structures (such as the small depth and self-routing capability). Crosstalk between optical signals passing through the same DC is an intrinsic drawback in DC-based optical networks. Vertical stacking of multiple copies (planes) of an optical banyan network, namely vertically stacked optical banyan (VSOB) network, can remove the crosstalk problem in switch network, and can also make the network nonblocking. This paper deals with the blocking behavior of VSOB networks when some links are broken or failed. We found the approximate value for lower bound on number of planes required to make a VSOB networks nonblocking allowing link-failures. We have an interesting finding that the blocking probability of the VSOB networks does not always increase with the increase of link-failures; blocking probability decreases for certain range of link-failures, and then increases again. As far as we know that for switching networks, such fluctuating variation of blocking probability with the increase of link failures rate deserves special attention in switch design.
[blocking probability, network nonblocking, Banyan networks, optical switches, multistage interconnection networks, vertically stacked optical Banyan networks, vertical stacking, link-failures, switching networks, crosstalk, optical crosstalk, link failures, optical switching systems]
An efficient approach for NAT traversal problem on security of voice over internet protocol
2007 10th international conference on computer and information technology
None
2007
This paper evaluates an efficient approach for network address translation (NAT) problem on voice over Internet protocol (VoIP) network system. Based on our experiment, we examined the latency, buffer size and voice packet loss under various network conditions. We found that it is possible to establish a call from outside the NAT to inside maintaining the quality issues of VoIP call. With this approach it is possible to use the current network architecture with having few changes in the registrar server. Based on this result, we propose a model for VoIP system that eliminates NAT traversal problem of VoIP call setup. Hence we evaluate our model showing the QoS conditions that achieves both high efficiency and secure voice transmission.
[quality of service, network address translation problem, QoS conditions, Internet telephony, VoIP system, voice over Internet protocol network system]
Reducing congestion collapse and promoting fairness in the internet by optimizing SCTP
2007 10th international conference on computer and information technology
None
2007
The Internetpsilas excellent scalability and robustness result in part from the end-to-end nature of Internet congestion control. End-to-end congestion control algorithms alone however are unable to prevent the congestion collapse and unfairness created by applications that are unresponsive to network congestion. To address these maladies, we propose a modified Queue Management algorithm and Stream Control Transmission Protocol (SCTP) congestion control mechanism. SCTP mechanisms are based upon TCP congestion control principles. It allows network operators to simultaneously achieve high throughput and low average delay. One of the flaws of SCTP is-fast retransmission procedure is vulnerable to being mistakenly triggered multiple times leading to under utilization of the network during recovery and duplicate retransmissions of the lost packet. To solve these problems, we change in determining the congestion window while leaving the basic idea intact and then evaluate its performance using simulation. Simulation results show that our proposed method can effectively eliminates congestion collapse and give better performance than conventional procedure. Based on extensive simulation, we believe that our technique is sufficiently robust for deployment in routers.
[TCP, queueing theory, Internet fairness promotion, telecommunication congestion control, Internet congestion collapse reduction, queue management algorithm, stream control transmission protocol, performance evaluation, TCP congestion control principle, SCTP, quality of service, Congestion collapse, TSN, Internet congestion control mechanism, end-to-end congestion control algorithm, transport protocols, QoS, packet retransmission procedure, Unresponsive flow, Internet]
Inexpensive construction of a 3D face model from stereo images
2007 10th international conference on computer and information technology
None
2007
Construction of a three dimensional face model from stereo images is a challenging task. Most of the currently available systems for reconstruction of 3D models require special hardware for calibration. In this paper, we illustrate a mechanism to construct a three dimensional face model from two stereo images. The developed mechanism does not require any special devices to calibrate the stereo images. We used a hand-held inexpensive digital camera to take the stereo images of a face. We did not use any camera-stand to fix and measure the camera system geometry. The stereo images were taken holding the camera in hand and moving it to two slightly different viewpoints. We constructed a depth map from these two stereo images and utilized this depth map to reconstruct the three dimensional face model. The 3D face model reconstruction process described in this paper uses some existing theories and combines them to develop a new system to generate the depth map. The system requires minimal user interaction for the reconstruction.
[cameras, feature extraction, 3D face model reconstruction, Depth map, stereo image processing, depth map, user interaction, handheld inexpensive digital camera, image reconstruction, stereo images, Stereo images, Image reconstruction]
Segmentation of dynamic textures
2007 10th international conference on computer and information technology
None
2007
Dynamic textures are textures with motion. Characterization of visual processes consisting of multiple dynamic textures is of vital importance to computer vision research, with a diverse set of applications in the field of robot navigation, and remote monitoring applications etc. In the current literature, however, studies are mostly limited to characterization of single dynamic textures. In this paper we aim to address the problem of segmenting image sequences consisting of multiple dynamic textures. More precisely we separate image segments having different characteristic motion patterns - a key attribute of individual dynamic textures. Experimental results demonstrate the ability of the proposed technique by segmenting a wide variety of multiple dynamic texture image sequences.
[robot navigation, temporal texture, image segmentation, computer vision, segmentation, dynamic texture segmentation, remote monitoring, dynamic texture, image sequences, image texture, image motion analysis]
Comparative analysis between two view-based methods: MHI and DMHI
2007 10th international conference on computer and information technology
None
2007
In this paper, we compare the basic motion history image (MHI) and our developed multi-directional motion history image (DMHI) for human gesture recognition. One of the constraints of the MHI is that it erases past motion by overwriting new motion onto the past one, thereby creating a template that does not correspond the motion properly. We have solved this overwrite problem by employing the concept of motion descriptors from optical flow vector. We have separated the optical flow vector into four components based on the four directions, namely up, down, left and right. We have employed Hu moments to calculate the feature vectors for both the MHI and the DMHI methods. We have experimentally verified the superiority of the DMHI method in terms of recognition rate for complex motion. In this paper, we have also analyzed the importance of motion energy image for both methods, and with different motions, we have found that presence of energy image is more evident in the DMHI technique than in the MHI technique.
[gesture recognition, motion energy image, multidirectional motion history image, Motion recognition, motion estimation, overwrite problem, optical flow, history image, Hu moment, energy image, optical flow vector, motion descriptor]
A dynamic load balancing approach for solution adaptive finite element graph applications on distributed systems
2007 10th international conference on computer and information technology
None
2007
Load balancing is the key to the efficient operation of distributed systems. To efficiently utilize computing resources provided by distributed systems, an underlying dynamic load balancing (DLB) scheme must address both heterogeneous and dynamic features of distributed systems. In this paper, a DLB scheme for solution adaptive finite element graph applications on distributed systems is proposed. Experiments show that by using the proposed distributed DLB scheme, the execution time and the number of process migration is close to using condensed binary tree load balancing (CBTLB) scheme which does not consider the dynamic nature and diverse platform of distributed systems.
[adaptive finite element graph applications, dynamic network loads, resource allocation, graph theory, mathematics computing, Distributed systems, distributed systems, finite element analysis, dynamic load balancing, heterogeneity, dynamic load balancing approach]
National Address Database (NAD) on the grid
2007 10th international conference on computer and information technology
None
2007
Address has immense importance to every citizen. This has led to the creation of national address database (NAD), which will allow every citizen to have a valid and verifiable address and also allow diversified services to be provided to them. Traditionally, national address databases have been built and maintained at a single central location. Centralized systems of this sort have inherent drawbacks: single point of failure, congestion and low scalability. To overcome these flaws our research focus is to develop a system with NAD in a grid environment. We have implemented NAD in both centralized and grid environment and done a comparative study between them. In our research we have built a test bed that describes the computational environment in which the comparison is performed. This paper includes proposed grid system architecture for NAD, a table and a graph to show the comparison.
[manager, grid server, grid computing, executor, grid system architecture, Address Verification (Match Address), web client, centralized systems, Grid Server, database management systems, national address database]
A new wide-sense nonblocking optical switching network
2007 10th international conference on computer and information technology
None
2007
In this paper, we present a new wide-sense nonblocking optical switching network. The proposed network combines some building blocks, which is All-To-All Personalized Exchange Multistage Interconnection Network. The architectural details and some other parameters are evaluated and compared with other networks to show the superiority of the proposed network over other networks. To construct the proposed network we require less number of switching elements. In addition, it provides constant first order crosstalk, thus it scales well. The proposed network is a good choice for constructing nonblocking optical switch with less switching element and signal loss.
[switching elements, optical switches, crosstalk, Switching elements, wide-sense nonblocking optical switching network, Signal loss, signal loss, WSNB, all-to-all personalized exchange multistage interconnection network, optical crosstalk, Optical Switching Network, Maximum Crosstalk]
A scheme for the solutions of the h-out of-k mutual exclusion problems using k-arbiters
2007 10th international conference on computer and information technology
None
2007
The k-arbiter is a well-established solution for the problem of the h-out of-k mutual exclusion. Though the solution using k-arbiters are simple and highly fault tolerant, the size of the quorums is high comparing to that of 1-coteries and for large values of k it becomes worse. A k-arbiter needs to have intersection between any k+1 quorums. Here we propose k(h)-arbiter, an efficient and general solution, for the h-out of-k mutual exclusion problems. It is an extension to k-arbiter. Here the quorum construction, using k-arbiters, considers h. The quorum size is variable and inversely proportional to the value of h for a specific value of k. The quorum size is smaller than that of k-arbiters for hGt1. We apply this technique on two simple k-arbiter construction schemes, uniform k-arbiter and binomial k-arbiter.
[Quorum, fault tolerance, Scheme, Distributed Algorithm, mutual exclusion problems, Mutual Exclusion, Arbiter, software fault tolerance, quorum construction]
Performance analysis of beamforming algorithm for noise cancellation
2007 10th international conference on computer and information technology
None
2007
This paper presents performance analysis of beamforming algorithm for canceling multiple channel noise depending on variation of hidden layer of the multilayer feedforward network and the number of epoch (also known as number of iteration)[1]. A multi-layer perception (MLP) has been considered to perform beamforming. A beamforming is an array of sensors connected to the MLP inputs. We have also used the backpropagation algorithm as learning rule for MLP and improving our signal quality. This involves a desired signal whilst removing any noise or interference signals which may come from different sources.
[multilayer perceptrons, multilayer feedforward network, signal quality, μ LMS, interference suppression, multilayer perception, BSS, backpropagation algorithm, beamforming algorithm performance analysis, interference signal, AM, sensor arrays, MLP, DSPs, multiple channel noise cancellation, Nyquist sampling rate, backpropagation, sensor array, learning rule, array signal processing]
Robust speaker identification system using multi-band dominant features with empirical mode decomposition
2007 10th international conference on computer and information technology
None
2007
This paper presents a text independent speaker identification system using multi-band features with artificial neural network. Linear predictive cepstrum coefficients (LPCCs) computed from sub-band signals with higher order statistics (HOS) are employed as the main features to represent the speaker characteristics. The multi-band representation of the speech signal is implemented by empirical mode decomposition (EMD). Dominant feature vectors are derived by applying principal component analysis (PCA) on LPCC space computed from the speech signal. The experimental results show that the proposed system improves the speaker identification performance. The efficiency is also compared for different features with noisy speech signals.
[empirical mode decomposition, bandpass filtering, higher order statistics, matrix decomposition, multiband dominant features, Speaker recognition, neural network, speech signal, vectors, linear predictive cepstrum coefficients, cepstral analysis, text independent speaker identification, dominant feature vectors, linear predictive coding, neural nets, principal component analysis, speaker recognition]
Adaptive beamforming of linear array antenna system with provision of sidelobe cancellation
2007 10th international conference on computer and information technology
None
2007
In case of single element antennas like Yagi-Uda, spiral, dipole, horn etc. have very little capability of variation antenna gain pattern. For better directivity, electronic tracking and reshaping of beam, array antenna is widely used in wireless network. Relative magnitude of feed currents, relative phases/separation between antenna elements, geometry of array are responsible for the shape of radiation pattern. In this paper linear array antenna with adaptive algorithm is used to achieve desired gain pattern at a desired angle of arrival (AOA). Linearly constraint minimum variance (LCMV) algorithm can be used quite comfortably for a desired gain incorporating Lagrange multiplier but side lobes can not be cancelled properly. This paper deals with combination of LCMV, sidelobe cancellation, LMS algorithm to achieve derided antenna gain pattern keeping sidelobe below a threshold level.
[least mean squares methods, antenna gain pattern, sidelobe, LCMV, adaptive beamforming, Linear array, angle of arrival, sidelobe cancellation, LCMV algorithm, Lagrange multiplier, antenna radiation patterns, LMS algorithm, linearly constraint minimum variance algorithm, wireless network, adaptive antenna arrays, antenna radiation pattern, array signal processing, linear antenna arrays, linear array antenna]
Multiple description image transmission for diversity systems over unreliable communication networks
2007 10th international conference on computer and information technology
None
2007
In this article, the development of an efficient and reliable multiple description coding (MDC) scheme over unreliable communication networks is described. A general framework of multiple description robust communication system with 2- and 4-channel cases is presented with a proposed block-based DC separation approach. The advantage of this system is that, if all the channels work, a high quality reconstruction can be achieved, while a lower but still acceptable quality can be achieved if some of the channels are lost. The peak signal-to-noise ratio (PSNR), mean squared error (MSE), bit rate, entropy and redundancy are also calculated with the proposed method to analyze the reconstruction quality. It is found that the proposed method gives comprehensive improvements over the other recently developed methods.
[peak signal-to-noise ratio calculation, multiple description coding, discrete cosine transforms, diversity systems, redundancy calculation, block-based DC separation approach, multiple description image transmission, image reconstruction, block-based dc separation, bit rate calculation, reconstruction quality, discrete cosine transform, entropy, mean squared error calculation, reliable multiple description coding scheme, entropy calculation, nonhierarchical decomposition, transform coding, image coding, mean square error methods, unreliable communication networks, error statistics]
A novel approach of image morphing based on pixel transformation
2007 10th international conference on computer and information technology
None
2007
Image morphing has been the subject of much attention in recent years. It has proven to be a powerful tool for visual effects in film and television, depicting the fluid transformation of one digital image into another. This paper presents a new approach of image morphing based on pixel transformation that depicts the transformation of pixels with their neighborhoods. The method is organized with the replacement of the pixel values of a source image and convolving the neighbor with the help of a mask. This algorithm changes an image from a particular side or from the center. Experimental results demonstrate that the method is fast and efficient for image morphing.
[pixel transformation, Convolution, image metamorphosis, convolution, image morphing, Pixel-transformation, image convolution, Metamorphosis, image resolution, Mash warping, Cross-Dissolve]
Adaptively robust blind audio signals separation by the minimum &#x03B2;-divergence method
2007 10th international conference on computer and information technology
None
2007
Recently, independent component analysis (ICA) is the most popular and promising statistical technique for blind audio source separation. This paper proposes the minimum beta-divergence based ICA as an adaptive robust audio source separation algorithm. This algorithm explores local structures of audio source signals in which the observed signals follow a mixture of several ICA models. The performance of this algorithm is equivalent to the standard ICA algorithms if observed signals are not corrupted by outliers and there exist only one structure of audio source signals in the entire data space, while it keeps better performance otherwise. It is able to extract all local audio source structures sequentially in presence of huge outliers. Our experimental results also support the above statements.
[statistical technique, independent component analysis algorithm, minimum beta-divergence method, adaptive robust blind audio signal separation, Minimum &#x03B2;-divergence method, independent component analysis, Linear and non-linear mixture data, adaptive signal processing, audio signal processing, blind audio source separation, Audio source separation, Robustness, Independent component analysis (ICA), blind source separation, ICA mixture model]
Digital image enhancement with fuzzy rule-based filtering
2007 10th international conference on computer and information technology
None
2007
Image enhancement is a technique to improve the quality of an image. The aim of image enhancement technique is to improve the interpretability or perception of information in images for human viewers, or to provide better input for other automated image processing techniques. This paper presents a new approach for image enhancement with fuzzy rule-based filtering. Compared to other non-linear techniques, fuzzy filter gives the better performance and is able to represent knowledge in a comprehensible way.
[image information perception, human viewers, image information interpretability, image enhancement, knowledge-based representation, fuzzy filtering, filtering theory, fuzzy set theory, knowledge based systems, fuzzy rule-based filtering, digital image enhancement, Image enhancement]
Analysis of composition techniques for combinational switching functions using reduced ordered Binary Decision Diagrams (ROBDDs)
2007 10th international conference on computer and information technology
None
2007
Binary decision diagrams (BDDs) provide a canonical and compact representation of Boolean functions. The canonical property makes it possible to easily detect many useful properties of Boolean functions such as size of the support set, symmetry between variables etc. Furthermore, BDDs compact representation coupled with the use of data structures for caching intermediate computations allows the effective implementation of many Boolean operations. In this paper, we present a new methodology and implementation details of the composition techniques of two combinational switching functions using reduced ordered binary decision diagrams ROBDDs. For instance, given two switching functions A and B, we present the composition formulas AnlandB,A or B,ArarrB etc using ROBDDs. In this regard, we have used the concept of Shannonpsilas expansion to directly build ROBDDs. An adequate number of examples have been used to make our method clear.
[Binary Decision Diagram, caching intermediate computations, Shannonpsilas expansion, combinational switching, combinational switching functions, ROBDDs, composition techniques, binary decision diagrams, Combinational Switching Function, Boolean functions, canonical property, Composition Techniques, data structures, information theory, reduced ordered binary decision diagrams]
A general method for CMOS realization of any logic function in ternary computer system
2007 10th international conference on computer and information technology
None
2007
This paper develops a general method for realization for any logic function using CMOS. By this new method, any ternary logic function can be realized by using only three unary and two binary functions. This method is more efficient than previously published CMOS realization.
[ternary logic function, microcomputers, CMOS realization, Pull-zero-network, CMOS logic circuits, ternary computer system, Ternary Logic Function, binary function, standard ternary inverter, unary function]
Electronic MCQ answering system for classroom e-learning and examination
2007 10th international conference on computer and information technology
None
2007
Computer based MCQ Examination can be a very reliable way of taking exams in class. Since there is an involvement of a machine, the marking and taking of the exam is easier and faster, but the process requires a computer for each examinee, so it cannot be held in ordinary classrooms. In this paper we used combinational logic circuits and readily available ICs to build up a hardware setup that could be used to take and mark exams with the aid of a single central computer for any number of candidates in any place. The setup consists of a single unit of answering console for each candidate, with answering buttons and an in-built memory unit to store the required answer. These consoles are centrally attached to a hub through which the answers are fed into the central computer by software that controls the input and output data lines of the computer. The answers are calculated and displayed on screen dynamically for fast feedback to students and ensure effective classroom learning.
[classroom e-learning, combinational circuits, combinational logic circuits, Classroom e-learning, examination, Console, Answering System, computer aided instruction, Combinational circuits, electronic MCQ answering system]
Information theoretic SOP expression minimization technique
2007 10th international conference on computer and information technology
None
2007
The efficient design of multiple Boolean functions is becoming important and necessary during computer aided design for circuit and systems (CADCS), especially the manufacture of chips have reached a density of several ten thousands transistors per chip, so called very large scale integration (VLSI). To simplify the Boolean expressions by conventional approaches like graphical observation-based K-MAP or other simplification procedures become tedious or even impossible when the number of variables in a truth table exceeds certain limit. In this paper we propose an information theory based circuit designing approach for deriving minimal sum of product (SOP) expressions for unlimited number of variable. We have verified our approach on a number of cases with the conclusion that the proposed approach is a better alternative to conventional approaches particularly when the numbers of variables restrict the use of conventional approaches. The key feature of proposed method is that it performs a hill-climbing search through the state space of Boolean variables using information theoretic heuristic to find the minimal SOP expression.
[sum of product expression, minimization technique, Boolean function, Entropy, K-Map, information theoretic heuristic, Boolean expression, hill climbing search, Boolean functions, circuit computer aided design, Classification, circuit CAD, SOP Expression, information theory, Information Gain, minimisation]
Synthesis of incompletely specified multi-output quaternary function using quaternary quantum gates
2007 10th international conference on computer and information technology
None
2007
Multiple-valued quantum circuits are a promising choice for future quantum computing technology since they have several advantages over binary quantum circuits. In this paper, we propose quaternary multi-qudit controlled gate family and show their realization on the top of liquid ion-trap realizable 1-qudit gates and 2-qudit Muthukrishnan-Stroud gates. Then we show a method of synthesizing incompletely specified multi-output quaternary function using quaternary 1-qudit gates and multi-qudit controlled gates.
[quaternary quantum gates, Logic synthesis, quaternary multiqudit controlled gate family, quantum gates, quantum logic, quaternary logic, quaternary controlled gate family, 2-qudit Muthukrishnan-Stroud gates, multioutput quaternary function, multiple-valued logic, network synthesis, 1-qudit gates, logic circuits]
Adoption of e-Commerce: A decision theoretic framework and an illustrative application
2007 10th international conference on computer and information technology
None
2007
Electronic commerce (e-Commerce or EC), in some form or other, is changing the way organizations do their business. Many organizations (banks etc) are forcing their customers to adopt e-Commerce. Others are adopting e-Commerce for competitive necessity. This raises the obvious question: How can organizations adopt appropriate e-Commerce model judiciously? This paper addresses the above research question. We use a decision theoretic framework based on multiple attributes of e-Commerce. Extensive literature review revealed a number of factors or attributes that either act as drivers or barriers of e-Commerce success. A well known decision theoretic approach based on multiple attribute, called analytic hierarchy process (AHP), is used to develop a comprehensive model of e-Commerce adoption. Our framework can be used as a guide to select the appropriate e-Commerce model. Real world data, from EC consultants, have been collected for a hypothetical SME which is embarking on adopting an e-Commerce model. The paper presents the application of the framework for this SME.
[analytic hierarchy process, decision theory, decision theoretic framework, Decision theoretic approach, E-Commerce adoption, small-to-medium enterprises, e-commerce, SME, electronic commerce]
The opportunities of using ICT by small and medium enterprises in Bangladesh: Case of SMEs in Bangladesh Small &#x00026; Cottage Industries Corporation&#x2019;s industrial estates
2007 10th international conference on computer and information technology
None
2007
The potential contribution of information and communication technology (ICT) to improve the competitiveness of small and medium-sized enterprises (SMEs) has long been recognised. However, the realisation of this potential has been problematic and over recent years there have been a number of initiatives supported by government, non government and foreign agencies which have endeavoured to aid and encourage the up take of ICT to enable access to such promised benefits. Despite strong theoretical arguments suggesting that ICT has much to offer to SMEs, the study would seem to suggest that use of ICT by SMEs in Bangladesh Small and Cottage Industries Corporationpsilas (BSCIC) 70 industrial estates is still in its infancy. One of the factors that have been identified as impacting upon the level of ICT adoption amongst SMEs is access to, and confidence on ICT. This paper is based on the study conducted as part of the IDA funded enterprise growth and bank modernization project and explores the opportunities and potentials of using ICT to provide a necessary lever to enhance competitiveness and productivity of SMEs in Bangladesh.
[information technology, BSCIC, Bangladesh Small and Cottage Industries Corporation, small-to-medium enterprises, SME, B2B, government, productivity, small and medium enterprises, information and communication technology, ICT, electronic commerce, E-business]
Electronic commerce adoption small and medium scale industry in Bangladesh
2007 10th international conference on computer and information technology
None
2007
This paper presents the findings of research relating to electronic commerce adoption in Bangladesh. A survey questionnaire was used to gather opinions about the current status of electronic commerce in Bangladesh organizations. The survey was conducted between April 2006 and August 2006. The focus of the survey was to investigate potential benefits of electronic commerce, potential barriers and risks in electronic commerce. The expectations regarding evolution of electronic commerce also tried to find out.
[Bangladesh, Potential benefits, small-to-medium enterprises, Inter-organizational-relationships, small and medium scale industry, Electronic commerce, Potential barriers, electronic commerce]
A study on computerized Emigration Clearance system of Bangladesh: Step toward e-Governance
2007 10th international conference on computer and information technology
None
2007
The ultimate objective of e-governance is to establish good governance. The elements of good governance can be identified as follows: a. Efficiency b. Reliability c. Accountability d. Accessibility and Transparency e. Visionary in decision making. All of these elements can be achieved through proper implementation of e-governance. Already Government of Bangladesh (GOB) takes several projects to implement it. Here we discuss about a GOB office named Bureau of Manpower Employment &amp; Training (BEMT) who uses ICT very efficiently. Since 1976 BMET has been processing ldquolabour migrationrdquo manually. Measures to develop a congenial atmosphere in migration process and to develop institutional framework a computer database network has been established in 2004. Prospective job-seekers register their name and other particulars at DEMO. Selection of candidates for the overseas market is done from this database. Emigration clearance is also being provided from this database by BMET through computer network. Its capacity and activities are continued mainly to issuing emigration clearance for those who have been offered an overseas employment by either private companies or authorized agents. This system is running nonstop since 2004 without a single day interrupt or crash which is a remarkable achievement in context of a developing country like Bangladesh. Moreover, 4000/5000 job seeker registration done in every working day in 21 District Offices and around 3000-4000 clearance done in BMET head Office with departure checking in airports.
[Bangladesh, computerized emigration clearance system, migration process, employment, human resource management, e-governance, good governance, computer database network, government data processing, overseas employment]
Implementing e-governance using oecd model (modified) and gartner model (modified) upon agriculture of Bangladesh
2007 10th international conference on computer and information technology
None
2007
E-governance is a process of reform in the way governments work share information, engage citizens and deliver services to external and internal clients for the benefit of both government and the clients that they serve. At that point, a new nomenclature will arise reflecting the change articulated in future generations. But this new nomenclature will only be an extension of the discipline that began to evolve in the late twentieth century. Here we successfully implement e-governance in Bangladesh using modified OECD model and modified Gartner Model based upon the infrastructure of telecom industries lying here following the strategy proposed by Riley and Sheridan.
[agriculture, OECD model, Gartner model, PPP, Bangladesh, Voice SMS, Web Portals, e-governance, government data processing, Gartner Model, SCO Mobile Server]
Skills brokerage performance measurement through BSC
2007 10th international conference on computer and information technology
None
2007
The right combination of skills is essential for organizational performance and, in turn, competitive advantage. Organizations currently spend a lot of time and energy on acquiring such a right mix. The skills brokerage model operates as an intermediary, and aims to provide the required human resources, possessing specific skills, for businesses and entrepreneurs. The model uses its relationships with skills providers such as universities, and relates the skilled human resources to the employers; hence, links the offer and demand. Performance measurement of the skills brokerage business can be done by means of the balanced scorecard (BSC), resulting in a framework for measuring both financial and strategic business objectives.
[skills brokerage, human resource management, skills brokerage performance measurement, balanced scorecard, skilled human resources, business model, organisational aspects, organizational performance, Balanced scorecard, performance measurement]
Frequency Based Two-Layer Multitap Bangla Input method for Mobile phones
2007 10th international conference on computer and information technology
None
2007
The interface of mobile phone in Bangladesh is mostly in English. Few mobile operators introduced Short Messaging Services in Bangla. Text entry in Bangla in mobile phone is not simple due to the structure and large number of characters in this script. The objective of this paper is to propose an easy and faster mobile input method. We proposed frequency based two-layer multitap (FBTLM) Bangla input method for mobile phones. The evaluation was done by keystroke comparison and user experiments by text entry. According to the result, we found our proposed FBTLM takes 42% less keystrokes than existing one layer multitap (OLM) and 17% less keystrokes than two layer multitap (TLM). Moreover, our proposed FBTLM method takes 42% less tapping time than OLM and 26% less tapping time than TLM.
[keystroke comparison, mobile radio, electronic messaging, Bangla, mobile phone interface, frequency based two-layer multitap Bangla input method, Bangladesh, Short Messaging Services, mobile phone, Multitap, text entry, two-layer, mobile handsets]
Instant-access start menu for an imaginary WIMP based future operating system
2007 10th international conference on computer and information technology
None
2007
This work has been done as a software engineering research-based course work using the principles of designing a user-interface. The objective of the research was to finding out how the current WIMP based operating systemspsila start menu can be made more usable to the users. In an imaginary WIMP based future operating system, the mouse and voice can be made more usable in regard of accessing the start menu. In this paper I propose a user-interface for the WIMP based start menu which I named dasiaInstant-access start menupsila. I have broadly described the possible dasialook and feelpsila and functionalities of such start menu and its accessibility features. I have listed all the user interface design principles that I have used to design my start menu. I have also mentioned briefly about the user interface metaphors I have used to design it. At the end, I have narrated the usability of my designed start menu along with the usability engineering attributes my start menu contains.
[imaginary WIMP based future operating system, Usability Engineering, graphical user interfaces, software engineering research-based course work, usability engineering, window-icon-menu-pointing device, WIMP (Window, Pointing-device), User-Interface (UI), operating systems (computers), instant-access start menu, User-Interface design principles, interactive devices, user interface design, Icon, Menu, Usability]
A real time speaker identification using artificial neural network
2007 10th international conference on computer and information technology
None
2007
Nowadays it is obvious that speakers can be identified from their voices. In this paper detail of speaker identification from the real-time system point of view is described. Firstly, it have been reviewed the well-known techniques used in speaker identification then the details of every step in identification process and explains the ideas, which leaded to these techniques. We start from the basic definitions used in DSP, then we move to the feature extraction step. Being widely used in pattern recognition tasks, neural networks have also been applied in speaker recognition. In this study, we developed a text-independent speaker identification system based on Back-propagation Neural Network (BPN). BPNs supply flexibility and straightforward design which make the system easily operable along with the successful classification results. In order to analyze the system in practice we made appropriate software and using real data we ran several tests. Empirical results show that proposed approach greatly improves identification speed in feature matching step. From the experiment it is found that the system correctly identify 96% of the speakers, using less then one second of test samples from each speaker.
[DSP, artificial neural network, backpropagation neural network, Identification, Mel-Frequency Cepstral Coefficient, Open-Set Identification, feature extraction, backpropagation, Closed-Set, real time speaker identification, Speaker Identification, neural nets, pattern recognition tasks, speaker recognition]
Boosting the performance of LZW compression through block sorting for universal lossless data compression
2007 10th international conference on computer and information technology
None
2007
In this paper a new data compression technique has been proposed based on Lampel Ziv Welch (LZW) coding and block sorting. At first block sorting is performed on input data to produce permuted data. LZW coding is then applied on that permuted data to produce more compressed output. Though block sorting takes some extra time (the amount is very negligible), it increases the performance of LWZ compression much. The proposed model is compared with respect to LZW compression.
[data compression, reverse block sorting, cyclic rotation, universal lossless data compression, sorting, LZW, permuted string, block sorting, Block sorting, Lampel Ziv Welch coding]
Lossless compression of JPEG and GIF files through lexical permutation sorting with Greedy Sequential Grammar Transform based compression
2007 10th international conference on computer and information technology
None
2007
This paper provides a way for lossless Compression of Color Images through lexical permutation sorting (LPS) and Greedy Sequential Grammar Transform based compression. The proposed model adopts the advantages of lexical permutation sorting for Color Images to produce a permuted data. Greedy Sequential Grammar Transform based compression, which is basically a text compression technique can now be applied easily on that permuted data. For comparison, we have taken Inversion Coding of Burrows Wheeler Compression (BWIC), and Burrows Wheeler Compression (BWC) and the model proposed in ICCIT 2006( on a paper named dasiaA New Approach for Lossless Compression of JPEG and GIF Files Using Bit Reduction and Greedy Sequential Grammar Transformpsila ).
[data compression, lossless compression, greedy sequential grammar transform, burrows wheeler compression, JPEG files, LPS, BWC, BWIC, lexical permutation sorting, grammars, GIF files, inversion coding, color images, image colour analysis, image coding, text compression]
Numerical round-off error in cellular phone services billing system
2007 10th international conference on computer and information technology
None
2007
Cellular phone services billing for per minute tariff plan and 1-second pulse involve floating point division and multiplication operation to calculate per call bill. Monthly customer billing involves addition operations on per call bills, which are floating point numbers. Round-off errors occur due to floating point numberspsila computer representation limitations and for storing limited significant figures during billing. The study analyzed post-paid itemized bills of a cellular phone service operator in Bangladesh and identified that accumulated round-off error for active post-paid subscribers is significantly high for subscriber group of large number. The research recommends a per second tariff plan to completely eliminate round-off error which also reduces floating point number operations.
[multiplication operation, Bangladesh, Cellular Phone Services Billing, floating point division, Numerical Error, invoicing, cellular phone services billing system, numerical round-off error, Round-off Error, cellular radio]
Design and implementation of PC-based high performance automatic voltage regulator
2007 10th international conference on computer and information technology
None
2007
This paper presents an overview of the currently employed AVRs (Automatic Voltage Regulators), the analogue and the digital, microprocessor based sampled data controlled, devices with respect to their control systems. The main design aims and constraints are examined in terms of performance specification concerning the speed, accuracy and stability of control. The contradiction between transient and dynamic stability requirements and its present day resolution by introducing Power System Stabilizing function into the AVR functions is explained. The design is implemented and tested on a 110 W synchronous generator in Power System Laboratory with an IBM personal computer acting as the real time controller. The results show that the proposed PC-based AVR are very effective for operation of a controller. These include its low cost, fast response, higher reliability and ruggedness.
[real time controller, PC-based, power system transient stability, IBM personal computer, low cost, dynamic stability requirement, synchronous generators, power system stabilizing function, voltage regulators, sampled data controlled devices, DC motor, real-time systems, transient stability requirement, automatic voltage regulator, power 110 W, AVR, synchronous generator, control stability]
Impact of user participation on Web-based information system: The Hong Kong experience
2007 10th international conference on computer and information technology
None
2007
The rapid growth of highly sophisticated computers and Web-based information systems (WIS) as integral components of business operations have led to an increased interest in the role of user participation during WIS implementation and its influence on end-user satisfaction and, ultimately, organisational success. The primary purpose of this research is, therefore, to investigate the significance of user-characteristics during WIS implementation. The research is conducted by collecting data via survey among organizations in Hong Kong. The important findings of this study demonstrate that user participation is positively related to user satisfaction and organisational effectiveness. In addition, user satisfaction can be largely applied to mediate the relationship between user participation (through user training, career stage, and empowerment) and organisational effectiveness. A deeper understanding of these concepts will provide organisations in Hong Kong with a richer view of the role of user participation during Web based information system implementation, which in turn has the potential to contribute towards improved business performance.
[WIS, Web-based information system, User satisfaction, Web-based Information Systems, user interfaces, Complexity, Hong Kong experience, IS implementation, User participation, information systems, Internet, user participation, end-user satisfaction, user satisfaction]
An efficient implementation of electronic election system
2007 10th international conference on computer and information technology
None
2007
Voting is usually recognized as one of the main characteristics of Democracy. Electronic election is a very recent idea regarding voting. Voter, once given his vote, has to rely upon the election systempsilas honesty and security. Free and fairness of an election is desired by almost everyone associated with it. Hence designing an election system needs special care. Furthermore, an electronic election should be more secure, transparent and trustworthy, as common people have less faith in computers due to system crashes and hacking threats. In this paper, we are going to describe our implementation of an efficient and secured electronic voting system based on the Fujioka- Okamoto-Ohta protocol which is the most practical and suitable protocol for large scale elections. Our implementation contains the automation of an online voting system providing some features which were absent in the previous implementations. We have made our system even more user friendly and secured but faster than the others using recent technologies and resources.
[public key, Secured Electronic Election, Blind Signature, voting, cryptography, blind signature, Digital Signature, electronic election system, private key, RSA algorithm, Hash Algorithm, security, digital signature, public key cryptography, Fujioka-Okamoto-Ohta protocol, Private key, Public key, private key cryptography, digital signatures, online voting system, Cryptography]
A learning algorithm for metasearching using rough set theory
2007 10th international conference on computer and information technology
None
2007
Metasearching is the process of combining search results of different search systems into a single set of ranked results, which is expected to be better than results of best of the participating search systems. In this paper, we present a supervised learning algorithm for metasearching. Our algorithm learns the ranking rules on the basis of user feedback based metasearching for the queries in the training set. We use rough set theory to mine the ranking rules. The ranking rules are validated using cross validation. The best of the ranking rules is then used to estimate the results of metasearching for the other queries. We compare our method with modified Shimura technique. We claim that our method is more useful than modified Shimura technique as it models userpsilas preference.
[learning algorithm, search engines, metasearch process, modified Shimura technique, supervised learning algorithm, metasearching, query processing, cross validation, participating search engine, ranking rule mining, rough set, user feedback, learning (artificial intelligence), rough set theory]
Designing ANN using sensitivity &#x00026; hypothesis correlation testing
2007 10th international conference on computer and information technology
None
2007
Now a day artificial neural network (ANN) has become one of the most prominent concepts in the field of artificial intelligence. ANN has already been applied in the thousands of real life applications. In the arena of classification problem ANN is used massively. But the key issue is in almost all situations the performance of it depends on the architecture of the ANN. As a result designing a proper ANN is always a vital issue in the field of neural networks. The determination of an appropriate ANN architecture is always a challenging task for the ANN designers. This paper proposes a pruning algorithm for designing a three layered ANN architectures. It is well known that a three layered ANN can solve any kind of linear and nonlinear problems. The proposed algorithm uses some major mathematical concepts: correlation coefficients, standard deviations, and statistical hypothesis testing scheme for designing the ANNs. For that reason the authors propose the new pruning algorithm, ANN designing by sensitivity and hypothesis correlations testing (SHCT), to determine ANN architectures automatically. The salient features of SHCT are that it uses statistical hypothesis testing scheme, standard deviations, correlation coefficients, merging with proper replacements to design the ANNs. To justify the performances of SHCT it has been tested on a number of benchmark problem datasets such as Australian credit cards, breast cancer, diabetes, heart disease, and thyroid.
[artificial neural network, ANN, Standard Deviation, t-test, statistical hypothesis testing, Generalization, artificial intelligence, ANN architecture, Correlations, Back Propagation, pruning algorithm, neural net architecture, sensitivity and hypothesis correlations testing, statistical testing, correlation coefficient, Hypothesis Testing Scheme, correlation methods, standard deviation, Overfitting]
Bangladeshi banknote recognition by neural network with axis symmetrical masks
2007 10th international conference on computer and information technology
None
2007
Automated banknote recognition system can be a very good utility in banking systems and other field of commerce. It can also aid visually impaired people. Although in Bangladesh, bill money recognition machines are not common but it is used in other countries. In this paper, for the first time, we have proposed a Neural Network based recognition scheme for Bangladeshi banknotes. The scheme can efficiently be implemented in cheap hardware which may be very useful in many places. The recognition system takes scanned images of banknotes which are scanned by low cost optoelectronic sensors and then fed into a Multilayer Perceptron, trained by Backpropagation algorithm, for recognition. Axis Symmetric Masks are used in preprocessing stage which reduces the network size and guarantees correct recognition even if the note is flipped. Experimental results are presented which show that this scheme can recognize currently available 8 notes (1, 2, 5, 10, 20, 50, 100 &amp; 500 Taka) successfully with an average accuracy of 98.57%.
[multilayer perceptrons, Axis Symmetric Masks, Backpropagation, Optoelectronic Sensor, backpropagation algorithm, optoelectronic devices, Bangladeshi automated banknote recognition, neural network, image sensors, axis symmetrical mask, Histogram Equalization, Artificial Neural Networks, banking system, backpropagation, multilayer perceptron, optoelectronic sensor, Gray Scale, scanned banknote image, Binary Image, bank data processing, image recognition]
An efficient approach to rotation invariant face detection using PCA, generalized regression neural network and Mahalanobis distance by reducing search space
2007 10th international conference on computer and information technology
None
2007
This paper introduces a new effective method for human face recognition, which employs Mahalanobis distance, a color segment method, to reduce the search space for a face in an image. The color segment method is used to detect the area where a face may exist and searching for a face by the network is localized in that area. The face, with its orientation, is recognized using principle components analysis (PCA), generalized regression neural networks (GRNN) and Mahalanobis distance. The idea behind PCA is to reduce the dimension of the input vectors and extracting a feature vector. GRNN used as a function approximation network to detect whether the input image contains a face or not and if exists then reports about its orientation. The proposed system shows how GRNN can perform better than back-propagation algorithm and give some solution for better regularization. The proposed classifier system demonstrates advantages in: 1) better capability of approximation to underlying functions, 2) faster learning speed and 3) high robustness to outliers. The proposed technique overcomes one of the main problems for searching method reducing the computing time in two ways; firstly reducing the area to be searched and secondly reducing the scaling factor of the selected area of the image.
[color segment method, GRNN, human face recognition, search space reduction, PCA, invariant face detection, function approximation network, function approximation, generalized regression neural network, face recognition, Rotation invariant face detection, neural nets, principal component analysis, Mahalanobis distance, Composite sigmoidal function]
Structure determination of artificial neural network using modified cellular encoding
2007 10th international conference on computer and information technology
None
2007
This paper works with a new evolutionary system to construct and control the structure of feed forward artificial neural networks (ANNs), represented by modified cellular encoding (MCE) that is not subject to the well-known permutation problem. It is shown in this paper that addition or deletion of nodes or connections can evidently be done by crossover automatically. Hence, the number of user specified parameter is also decreased. The ANN architecture determination algorithm is tested on some real world problems. The algorithm is made adaptive.
[modified cellular encoding, evolutionary computation, Permutation problem, artificial neural network structure determination, Artificial Neural Network, Modified Cellular Encoding, evolutionary system, feed forward artificial neural networks, Evolutionary Process, neural net architecture, feedforward neural nets]
Multi-class SVM based iris recognition
2007 10th international conference on computer and information technology
None
2007
We propose an improved iris recognition method to identify the person accurately by using a novel iris segmentation scheme based on the chain code and the collarette area localization. The collarette area is isolated as a personal identification pattern, which captures only the most important areas of iris complex structures, and a better recognition accuracy is achieved. The idea to use the collarette area is that it is less sensitive to the pupil dilation and usually not affected by the eyelids or the eyelashes. The deterministic feature sequence is extracted from the iris images using the 1D log-Gabor wavelet technique and used to train the support vector machine (SVM) as iris pattern classifiers. The parameters of SVM are tuned to improve the overall system performance. Our experimental results also indicate that the performance of SVM as a classifier is far better than the performance of backpropagation neural network (BPNN), K-nearest neighbor (KNN), Hamming distance and Mahalanobis distances. The proposed innovative technique is computationally effective as well as reliable in term of recognition rate of 99.56%.
[Biometrics, support vector machines, backpropagation neural network, biometrics (access control), collarette area localization, iris pattern classifiers, iris segmentation, support vector machine, image segmentation, backpropagation, K-nearest neighbor, iris recognition, personal identification pattern, image recognition, 1D log-Gabor wavelet, Mahalanobis distance]
Design and implementation of a machine vision based but low cost stand alone system for real time counterfeit Bangladeshi bank notes detection
2007 10th international conference on computer and information technology
None
2007
This paper presents the design of a machine vision based system for real time detection of the counterfeit Bangladeshi bank notes. The proposed system works with the denominations of five hundred and one hundred taka. This system relies on a specific feature of the both five hundred and one hundred taka. The relied feature is not possible to replicate for the counterfeit makers or producers. And there is no foreseeable likelihood that they would be capable to imitate this feature even within a pretty long time. The relied feature is the repeatedly printed ldquoBANGLADESHBANKrdquo on some portions of the notes using microprint technique. The proposed stand alone system captures the portions of the notes with a proprietary scanner called the Grid Scanner [1]. The captured image is then processed by a microcontroller PIC-16F648A or ATMega88 (AVR). The microcontroller then determines the validity of the note based on an OCR technique by looking for the characters dasiaBpsila, dasiaApsila and dasiaNpsila in the scanned image. The success-rate of the counterfeit detection with properly captured image is 100% and the average processing time is 250 milliseconds with above mentioned microcontroller.
[image processing, embedded device, Stand alone system, real time counterfeit Bangladeshi bank note detection, low cost stand alone system, optical scanners, image scanners, machine vision, microprint technique, optical character recognition, microcontroller, CMOS sensor, counterfeit detection, computer vision, OCR technique, grid scanner, bank data processing, microcontrollers, pattern recognition]
Iris Recognition: A Java based implementation
2007 10th international conference on computer and information technology
None
2007
Biometric authentication has become increasingly popular in security systems. Recently, the systems based on the human iris, which develops a unique pattern before birth, have produced very high rates of recognition. The iris image is first blurred using a Gaussian filter, and the edge is detected using the Canny edge detection technique. An algorithm, which uses the center of the image as a starting point, is proposed to isolate the pupillary region. The initial estimate of the location of the pupil is then refined, and the iris is located by using the integrodifferential operator. In order to detect the upper and the lower eyelids, we deploy the integrodifferential operator again; however, the path of contour integration is changed from circular to arcuate. A thresholding technique is then applied to locate the eyelashes. The annular iris region is unwrapped from a polar coordinate system to a rectangular canvas. The 2D Gabor wavelets are used to extract the discriminating features. Then, the phase information is extracted to produce an iris code of 2048 bit and a mask, which denotes the noisy regions, of the same length. The Hamming distance is applied for the matching purpose. We also design a graphical user interface (GUI) in Java which allows the comparison of two images, the verification that an image is that of a specific person, and to search through the previously scanned irises for an exact match. The proposed scheme is computationally effective as well as reliable in term of recognition rate of 99.21%.
[Biometrics, Java, 2D Gabor wavelets, Hamming distance, graphical user interfaces, wavelet transforms, integrodifferential operator, graphical user interface, biometrics (access control), eyelashes detection, eyelids detection, Gaussian filter, features extraction, edge detection, iris recognition, biometric authentication, image recognition, Canny edge detection technique, Java based implementation]
Interface development for cost effective automated IC orientation checking systems
2007 10th international conference on computer and information technology
None
2007
IC packages, in semiconductor industries, are being marked through certain sequence by either laser marker or ink markers. A critical step in the final visual inspection is the orientation check of ICs that can be determined by detecting images of pin-1 notch or pin-1 dimple, which are molded/marked on the IC packages. This study accomplishes a software interface for a simple cost-effective approach to develop an automated low cost vision system that is capable of detecting the IC orientation on a JEDEC tray for a laser/ink marker system used in the IC industries. The algorithm has the competence and flexibility to distinguish correct orientation for various types of IC packages (QFP, PGA, BGA, flip chip etc.). The workable prototype possesses input variables, control system and output variables along with software implementation using the pattern recognition template based on RGB (red-green-blue) color method for every pixel of the images. Images of Pin-1 dimple or pin-1 notch of the IC packages are considered to perform image matching with the software development. The accuracy level has been achieved to a threshold value of above 80% of the total pixels scanned between the matched images. With this threshold value, matching efficiency between two similar images has been attained near about 100%, based on the proposed algorithmic results. Illumination variation, vibration of the machine on conveyors and handlers, computerpsilas timing gap with the Web camera and low resolution (320times240 or 277times245 pixels) of the taken images have made the image matching a complicated one. However, all the constraints are taken into account for setting the threshold value and obtaining the matching accuracy and have been achieved the appropriate matching algorithm to implement the cost effective automated IC orientation checking system.
[semiconductor industries, pattern recognition template, IC packages, automated IC orientation checking systems, integrated circuit packaging, QFP, image colour analysis, automated low cost vision system, BGA, image resolution, laser marker, ink markers, flip chip, interface development, software development, red-green-blue color method, RGB color method, electronic engineering computing, PGA, image matching, integrated circuit testing, IC Orientation, software interface, Pattern Recognition, computer vision, JEDEC tray, Web camera, flip-chip devices, Machine Vision, ball grid arrays]
Vision based gesture recognition for human-robot symbiosis
2007 10th international conference on computer and information technology
None
2007
This paper presents a vision based gesture recognition system for human-robot symbiosis. The system is based on the visual information of the face and is commenced with the recognition of face gesture by connected component analysis of the skin color segmentation of images in HSV color model and neural network based pattern-matching strategies. On gesture recognition, robot is being instructed to perform certain tasks by issuing commands. The system is capable of recognizing static gestures comprised of the face poses, and dynamic gestures of face in motion. The effectiveness of the system has been justified over some experiments. The system has been demonstrated with an entertainment robot named ldquoAIBOrdquo as a human-robot symbiotic relationship.
[HSV color model, contrast equalization, pattern matching, face gesture, multi-layer perceptron, pattern-matching strategies, vision based gesture recognition, man-machine systems, AIBO, neural network, human-robot symbiosis, Human-robot symbiosis, gesture recognition, image segmentation, face recognition, image colour analysis, robots, neural nets, skin color segmentation]
Analysis of foot-to-ground clearance measurement techniques for MEMS realization
2007 10th international conference on computer and information technology
None
2007
The foot clearance above the ground is an important clinical and rehabilitation gait analysis parameter. Current bulky and laboratory environment based instruments are not suitable for measurement in the natural environment such as the outdoors. MEMS technology can be used with integration of microelectronic circuitry to enable realization of tiny and efficient devices for this application. This paper analyses three promising distance measurement techniques (capacitive, electric field and ultrasound) that may be suitable for MEMS realization of foot clearance measurement. The study suggests that ultrasound-based technique is very suitable for this application.
[biomedical equipment, biomedical ultrasonics, bioMEMS, ultrasound-based technique, MEMS technology, foot-to-ground clearance measurement technique, gait analysis, patient rehabilitation, distance measurement technique, rehabilitation gait analysis parameter, biomedical measurement, distance measurement, capacitive-based technique, foot-to-ground clearance, microelectronic circuitry, electric field-based technique]
An empirical analysis of software systems for measurement of design quality level based on design patterns
2007 10th international conference on computer and information technology
None
2007
In this paper, we propose a new, simple and quantitative approach to specify design level of object oriented software systems. The exploratory analysis method proposed here uses GoF (gang of four) design patterns as our assessment criteria. We formulate an empirical study and develop a method to measure software quality. We tested our proposed method on several open source projects and also validate it by making a comparison with current approach. Our approach that addresses design patterns can be an excellent alternative to current systems such as OO metric, software fault proneness, visualization and anti-pattern based approaches. Our approach also can be helpful to practitioners for software quality assurance.
[object-oriented programming, software fault proneness, assessment criteria, Design Quality, design quality level measurement, Quality Assessment, software systems empirical analysis, software quality, exploratory analysis method, Object Oriented System, Design Pattern, object oriented software systems, systems analysis, software engineering, software quality assurance, OO Metric, software metrics]
Guidelines for preparing standard software engineering curriculum: Bangladesh and global perspective
2007 10th international conference on computer and information technology
None
2007
In recent years, tremendous technical changes have happened in software development, accompanied by increased complexity of the systems that are being developed. Businesses are now regularly deploying systems that employ and integrate a wide range of computing technologies and paradigms. Thus, it is inevitable for the academics to be prepared to educate their students to meet this unique challenge. However, the development of software engineering (SE) programs is a difficult &amp; daunting task. In Bangladesh, curricula of undergraduate and graduate SE programs are not very old and maturing at a slower pace as compared to other curricula. There are problems for the development of such programs like: a) the SE profession in Bangladesh is immature; b) confusion about the difference between computer science (CS) and SE; c) lack of understanding and appreciation among CS faculty about the need for SE education; d) little available material on SE curriculum guideline; e) absence of local accreditation mechanism for SE programs, etc. In this paper, the authors studied the current status of SE curriculum in Bangladesh and abroad, and proposed guidelines for developing such curriculum adaptable by local academia and industry. The paper includes a curriculum design guideline, descriptions of probable knowledge components that can be used to design SE curricula, and a sample curriculum.
[computer science education, standard software engineering curriculum, software development, educational courses, curriculum design guideline, software engineering, accreditation, global perspective, curriculum design, Bangladesh perspective]
Rank-based quality measurement of software systems in standardized source code
2007 10th international conference on computer and information technology
None
2007
We propose a new and straight forward approach to measure design quality of object-oriented software systems. We use well known object oriented design quality metrics and find correlation among them to formulate a quality rank that is an indicator to the overall quality of any object oriented software. We tested our proposed formula on several open source software systems of different levels of design quality and validate by comparing the test result with expected levels of design quality.
[design metrics, quality measurement, object-oriented programming, public domain software, object oriented design quality metrics, software systems, Object Oriented, software quality, rank-based quality measurement, quality rank, standardized source code, open source software systems, object-oriented software system, software metrics]
Approaching intelligent environment through sentient artefacts
2007 10th international conference on computer and information technology
None
2007
Sentient artefacts are everyday objects augmented with value added digital services. Being the building blocks of our surroundings, these artefacts can incrementally integrate computing into environment and can convert it into an intelligent one economically. In this paper, we report our experiences with sentient artefacts to rationalise intelligent environment. We discuss the design principles of sentient artefacts and present a sensor selection framework to convert a regular artefact into a sentient one. In addition, we discuss the application development guideline and lessons that we have learnt through prototyping sentient artefacts and proactive applications.
[Intelligent Environment, radiofrequency identification, Ubiquitous Computing, intelligent structures, sensor selection framework, intelligent environment, intelligent sensors, value added digital service, ubiquitous computing, Sentient Artefact, sentient artefact]
An inter organization communication architecture model for real time activity base online transmission to increase the work process efficiency of container terminal
2007 10th international conference on computer and information technology
None
2007
The purpose of this research is to analyze the impact of instituting automated container yard management on the operation of a prototype EBILCY. This paper studies a total yard operation inflow and outflow of containers and cargos where different stakeholders depend on the operation of off-dock terminal. The primary challenge is to efficiently operate the operations and acquire the real-time data to minimize the operation lag due to information passing delay. In particular, to automate the every process, an online activity base strategy is used. This strategy groups jointly co-operate with OLAP and data mining process to reduce the manual work. An approach has been introduced to automate the all operational activity of off-dock as well as container terminal in real time basis. The methodology is approaching a cost-effective and authenticated communication through different stake holders of container terminal i.e. main line operator (MLO), freight forwarder, C&amp;F agents, shipper, consignee etc. The model is solved using message passing strategy. Due to required interaction and multiple hub access issues a standard structure is introduced with details specification. The communication architecture has implemented through the authenticated portable database tool, extended markup language (XML). To establish the easy communication through the end point, email client is introduced. For ensuring the information security MD5 encryption method has also used. This approach helps to atomize the full industry segment to work in one umbrella. The standard message structure ensures data validity among the stakeholders. This approach also can be used to communicate within different application where this approach would be work as a communication agent.
[message passing, Container Terminal, Automation, real time activity base online transmission, data mining, cargo handling, cryptography, OLAP, freight handling, containerisation, Data mining, MD5 encryption method, Communication Architecture, inter organization communication architecture model, message passing strategy, Off-Dock, Extended Markup Language, authenticated communication, message authentication, XML, automated container yard management, EBILCY prototype, off-dock terminal operation, authenticated portable database tool]
Algorithms for synthesis and average distribution of variable sized MOS components for efficient Analog VLSI devices
2007 10th international conference on computer and information technology
None
2007
In the field of Analog VLSI layout design, large variation of MOS component sizes causes mismatches and reduces the performance and splitting is necessary to reduce the variation. On the other hand, intensity of imposing always varies during fabrication. In this ongoing research, the solutions of above problems are introduced with some algorithm implementations. Two different sizes of components can be split into optimized number of pieces and an algorithm distributes them in an average and symmetrical (better) arrangement such that it can ensure average imposing and the efficiency increases. The computer generated solutions are compared with other possible solutions and proved better.
[analog VLSI devices, VLSI Layout, MOS component, integrated circuit layout, object oriented algorithm, MOS analogue integrated circuits, MOS components, parasitic capacitance]
VLSI implementation of Inverse Discrete Wavelet Transform for JPEG 2000
2007 10th international conference on computer and information technology
None
2007
This paper presents hardware design flow of the inverse discrete wavelet transform (IDWT) core which is the second-most computationally intensive block in JPEG 2000 image compression standard. Lifting scheme (LS) is implemented in designing the IDWT hardwire module that reduces the number of execution steps involved in computation to almost one-half of those needed with a conventional convolution approach. In addition, the LS is amenable to ldquoin-placerdquo computation, so that the IDWT can be implemented in low memory systems. The IDWT module does not comprise any hardware multiplier unit and therefore suitable for development of high performance image processor. The IDWT module has been developed in VHDL using Quartus II from Altera. The VHDL model is validated through simulation using ModelSim-Altera. Simulation results show the IDWT module can perform three levels inverse transform on a 256times256 forward transformed image in 8.7 ms. Latency of the system is calculated 50 ns and the power dissipation by the device is 662 mW. The IDWT module consumes just 57 combinational ALUTs and 60 logic registers of a Stratix II device, and runs at 300 MHz clock frequency, reaches a speed performance suitable for several real-time applications. Throughput in terms of input coefficients processed per second of the IDWT core is 7.13Msamples. The motivation in designing is to reduce its complexity, enhance its performance and to make it suitable development on a reconfigurable FPGA based platform for VLSI implementation.
[JPEG 2000, DWT, data compression, hardware multiplier unit, VLSI, discrete wavelet transforms, FPGA, image compression, inverse discrete wavelet transform, hardware description languages, Image transforms, lifting scheme, low memory systems, high performance image processor, Quartus II, VHDL, VLSI implementation, ModelSim-Altera, image coding]
An efficient packet scheduling algorithm for downlink queue to provide lossless handoff and QoS in 4G mobile networks
2007 10th international conference on computer and information technology
None
2007
Next generation mobile networks are expected to provide seamless personal mobile communication and quality of service (QoS). Lossless handoff is a key issue for providing the QoS. This paper presents a two-layer downlink queuing model and proposes a scheduling mechanism for providing lossless handoff and QoS in mobile networks, which exploit IP as a transport technology for transferring datagram between base stations and the high-speed downlink packet access (HSDPA) at the radio layer. In order to reduce handoff packet dropping rate at the radio layer and packet forwarding rate at the IP layer and to provide high system performance, new scheduling algorithms are performed at both IP and radio layer, which exploit handoff priority scheduling principles and take into account buffer occupancy and channel conditions. Performance results obtained by computer simulation show that, by exploiting the downlink queuing model and scheduling algorithms, the system is able to provide low handoff packet dropping rate, low packet forwarding rate, and high downlink throughput.
[Packet Scheduling, queueing theory, seamless personal mobile communication, quality of service, packet scheduling algorithm, Quality of Service, 4G mobile network, Downlink Queue, 4G mobile communication, QoS, 4G mobile networks, Soft-handoff, downlink queuing model, lossless handoff]
Distribution of energy usage in wireless sensor networks for polygonal coverage
2007 10th international conference on computer and information technology
None
2007
The increasing concentration of data traffic on sensors towards the base station in multi hop and escalating rate of energy drainage due to higher transmission distance at sensors towards the boundary in single hop communication cause an extremely non-uniform energy usage. This non-uniformity in energy usage makes some nodes to die early reducing the networkpsilas lifetime drastically leaving considerable amount of residual energy in others. In this work, we extend the single hop, multi hop and their hybrid transmission policies proposed in V. Mhatre and C. Rosenberg (2004) for polygonal network area instead of circular area. Through mathematical analysis we present the distribution of energy usage among sensors in different rings for polygonal network. Our work presents a more generalized theoretical analysis of network lifetime from transmission policy perspective considering practical deployment scenario. Numerical results show that multi hop transmission performs better than single hop transmission for path loss factor four and hybrid transmission policy performs better in terms of network lifetime and energy distribution than single hop and multi hop transmissions irrespective of network size.
[wireless sensor networks, energy usage distribution, path loss factor, ring segment, mathematical analysis, single hop communication, data traffic, polygonal network area, polygonal coverage, critical ring, critical energy, data cycle, energy consumption, telecommunication traffic, ring thickness]
Performance analysis of navigation by the integration of GPS-24 with LEO &#x00026; GEO
2007 10th international conference on computer and information technology
None
2007
An integrated network consisting of GPS-24 (global positioning system), LEO (Low earth orbit) and GEO (geostationary earth orbit) is analyzed in order to find the position coordinates of a GPS receiver or user position by calculating the position coordinates of GPS satellites. The integrated network is designed to extend the coverage area of GEO satellites up to the polar regions as well as to improve the user position accuracy. Satellite visibility through GPS receiver has been investigated using complex simulations of failure scenarios by giving shadow effect. An algorithm is proposed to estimate the userpsilas position from the known parameters of GPS satellites. Numerical analysis is carried out for the selection of best four satellites to be used for the calculation of userpsilas position. A new approach is also developed for the determination of minimum dilution of precision (DOP) using such integrated network.
[GPS-24, GEO, Navigation, low earth orbit, shadow effect, DOP, geostationary earth orbit, LEO, satellite navigation, Global Positioning System]
Performance limitation of M-ary pulse position modulation and on-off keying in a direct detection optical wireless communication system
2007 10th international conference on computer and information technology
None
2007
Performance analysis is carried out for an optical wireless link in the presence of atmospheric scintillation using OOK and M-ary PPM modulation schemes. The bit error rate (BER) performance results evaluated at a bit rate of 1 Gbps show that system suffers significant penalty in required photon count per bit to achieve a given BER due to atmospheric scintillation effect. It is noticed that penalty is approximately 128.2, 186.7, 215 and 356.6 photons per bit corresponding to OPPM, QPPM, OOK and BPPM respectively at a scintillation variance of 0.01.
[radio links, on-off keying, optical links, M-ary pulse position modulation, optical wireless link, performance limitation, amplitude shift keying, atmospheric scintillation, pulse position modulation, direct detection optical wireless communication system, performance analysis]
Automated river monitoring system for Bangladesh using wireless sensor network
2007 10th international conference on computer and information technology
None
2007
Today is the age of fast and automated system. In every part of our live we want the automated system. In communication system, this automated system is vastly needed. Now Bluetooth is compatible for large Scale sensor network. Bluetooth device is also very cheap and available. So we can use this technology in many practical cases in our country. We know flood destroy our poor country every year for lack of information about the rivers. So if we can monitor the rivers properly and timely we can overcome this destruction. To monitor these rivers we can use sensor network with the help of Bluetooth. If we can establish the sensor nodes that are integrated with Bluetooth in the river side we can easily monitor the rivers timely and properly. By this we can setup an automated river monitoring system using sensor network.
[rivers, floods, large scale sensor network, Bluetooth, geophysical techniques, wireless sensor networks, flood destruction, wireless sensor network, geophysics computing, TinyDB, Bangladesh, sensor network, TinyOS, Gateway, automated river monitoring system]
Cross-layer design of wireless networking for Parallel Loading of Access Points (PLAP)
2007 10th international conference on computer and information technology
None
2007
This paper processes a new cross-layer framework design for Parallel Download and Upload (PDU). This technique has been called as Parallel Loading of Access Points (PLAP). We have shown here our PLAP for Access Point (AP) and integrate Ad-hoc Procedure with AP to support the parallel download or upload applications. The results show that the proposed PLAP mechanism is faster and efficient for service setup and maintenance.
[IEEE 802.11 networks, cross-layer framework design, ad-hoc mode, infrastructure mode, telecommunication network planning, BSS, access points, cellular architecture, ESS, parallel download-upload, Access Point (AP), Parallel Loading of Access Points (PLAP), Parallel Download and Upload (PDU), Degree of Parallelism, PLAP technique, parallel loading, ad hoc networks, wireless LAN, cellular radio]
Modification of DSR and its implementation in Ad Hoc City
2007 10th international conference on computer and information technology
None
2007
Few real-world applications of mobile ad hoc networks have been developed or deployed outside the military environment, and no traces of actual node movement in a real ad hoc network have been available. In this paper we proposed a security scheme for an Ad Hoc City. We have designed an algorithm and corresponding packet structures to secure the pure ad hoc portion of the architecture. In order to achieve our goal, we have modified the Dynamic Source Routing Protocol (DSR) to adapt it in the environment under consideration. The Dynamic Source Routing protocol (DSR) is a simple and efficient routing protocol designed specifically for use in multi-hop wireless ad hoc networks of mobile nodes. DSR allows the network to be completely self-organizing and self-configuring, without the need for any existing network infrastructure or administration. The protocol is composed of the two mechanisms of Route Discovery and Route Maintenance. We have modified the Route Discovery and Route Maintenance phase in order to provide more security features.
[Mobile ad hoc network, proactive vs. reactive routing, mobile radio, multi-hop communication, dynamic source routing protocol, packet structures, multihop wireless networks, adhoc city, routing protocols, mobile ad hoc networks, route discovery, SK, ad hoc city, PK, ad hoc networks, route maintenance, mobile nodes, CA]
On the performance of M-ary modulation schemes for efficient communication systems
2007 10th international conference on computer and information technology
None
2007
In this paper, the performances of M-ary modulation schemes (MPSK, MQAM and MFSK) are analyzed in terms of symbol error probability as a function of SNR per bit. The channel capacity in terms of bit rate per bandwidth is also investigated to analyze the performance. In this constraint, the mathematical expressions for error probability in a closed form are derived using bounds and approximations to calculate and compare the performance of different modulation schemes theoretically. Simulation results for symbol error probability are also obtained using Monte Carlo method with 10% accuracy. Then the simulated results are compared with those of the theoretical calculations. A very good agreement is found between them.
[Monte Carlo method, Monte Carlo methods, symbol error probability, modulation, MQAM, error analysis, M-ary modulation schemes, MFSK, channel capacity, digital modulation, Monte Carlo simulation, MPSK]
Performance limitations of a subcarrier multiplexed optical fiber link impaired by optical bit interference and intensity noise
2007 10th international conference on computer and information technology
None
2007
Theoretical performance analysis is presented for an optical transmission system with RF subcarriers multiplexing (SCM) for radio on fiber technology considering the effect of PM-IM conversion due to Mach-zehnder modulator (MZM) and optical beat interference (OBI) generated during photodetection process. The computed bit error rate (BER) performance results show that there is significant deterioration in system BER due to OBI and PM-IM conversion and the system suffers significant power penalty at a given BER. The penalty is found to be approximately 5.8 dB, 11.0 dB, 13.5 dB, 15.0 dB, 17.0 dB for PM-IM conversion factor of -1, -2, -3, -4, -5 and 18.0 dB, 18.5 dB, 20.5 dB, 21.5 dB, 22.0 dB for number of subcarriers of 2, 4, 8, 16, 32 respectively.
[Mach-Zehnder modulator, optical links, intensity noise, radio on fiber technology, photodetection process, RF subcarriers multiplexing, light interference, performance evaluation, Optical link, theoretical performance analysis, optical transmission system, optical bit interference, subcarrier multiplexing, Optical beat Interference (OBI), subcarrier multiplexed optical fiber link, Subcarrier multiplexing, PM-IM conversion, performance limitations, optical beat interference]
An efficient anti-collision technique for Radio Frequency Identification Systems
2007 10th international conference on computer and information technology
None
2007
Collision is a problem which degrades the performance of the radio frequency identification (RFID) system while multiple tags are communicating with the reader simultaneously. Currently this problem is handled by various procedures such as framed ALOHA protocol, binary tree protocol, tree walking algorithm or using spread spectrum modulation scheme. The current solutions are slow, power wasting and cannot solve collision problem in better aspect. The proposed system eliminates collision while multiple tags are responding to the reader under its network and provides better power savings than existing systems.
[anti-collision technique, radio frequency identification systems, Time Delay Protocol, radiofrequency identification, modulation, trees (mathematics), Collision, RFID, Reader, access protocols, spread spectrum communication, multiple tags, framed ALOHA protocol, spread spectrum modulation scheme, binary tree protocol, Tag, tree walking algorithm]
To enhance bit rate in orthogonal frequency division multiplexing by carrier interferometry spreading code
2007 10th international conference on computer and information technology
None
2007
In orthogonal frequency division multiplexing (OFDM) based wireless communication systems; extra guard period is inserted to eliminate ISI (inter symbol interference) and ICI (inter carrier interference) effect. The guard period decreases the symbol rate. The authors describe here a model that eliminates the effects of ICI and ISI without inserting extra guard period. The sub-carrier frequencies and carrier interferometry (CI) code distribute alternatively between consecutive OFDM symbols to eliminate the ISI effect. The CI code is used to each sub-carrier in such a way that the subcarrier frequency, orthogonal to each other, when Doppler frequency shift occurs due to relative movement of the transmitter and receiver. A pilot symbol is used in between the two symbols for linear operation of all the electronic devices. A properly designed matched filter at the transmitter and receiver can reduce the effect of ICI. Here, we consider the effect of ICI for carrier frequency synchronizing error, Doppler shift and phase error with a time invariant channel as well.
[Fading, intersymbol interference, orthogonal frequency division multiplexing, OFDM, CI, Doppler frequency shift, carrier interferometry spreading code, ICI, Doppler shift, subcarrier frequencies, COFDM, bit rate, OFDM based wireless communication systems, OFDM modulation, wireless channels, intercarrier interference, ISI]
Modeling and performance of TCP in a MC-CDMA system for 4G communications
2007 10th international conference on computer and information technology
None
2007
Market research finds that mobile commerce for 4G wireless systems will be dominated by basic human communication such as messaging, voice, and video communication. Because of its typically large bandwidth requirements, broadband communication is expected to emerge as the dominant type of traffic in 4G wireless systems. In this paper a new TCP based Multicarrier access technique named MC-CDMA for mitigating 4G requirements is proposed. This paper also presents analytical information regarding the transfer of TCP data flows on paths towards interconnected wireless systems, with emphasis on 4G cellular networks. The focus is on protocol modifications in face of problems arising from terminal mobility and wireless transmission. We advocate the use of TCP as the transport layer protocol for high-speed data in a Multi-Carrier CDMA (MC-CDMA) system for 4G wireless communications.
[TCP, radio networks, MC-CDMA, layer mechanisms, TCP based multicarrier access technique, broadband networks, code division multiple access, mobile commerce, transport layer protocol, 4G mobile communication, 4G wireless communication system, 4G, transport protocols, broadband communication, 4G cellular network, cellular radio, MC-CDMA system]
Almost Delaunay triangulation routing in wireless sensor networks
2007 10th international conference on computer and information technology
None
2007
To obtain a satisfactory performance in the wireless sensor network, optimal routing is the most important aspect. The lifetime of the sensor network depends mostly on the routing topology. In order to route the data in an efficient way, we propose an almost Delaunay triangulation routing approach, which could be generated locally. This routing graph is generated only once and can be used during the whole lifetime of the wireless sensor network.
[wireless sensor networks, graph theory, Delaunay triangulation routing, telecommunication network topology, mesh generation, routing graph, Delaunay triangulation, routing topology, Wireless sensor network, cluster head, Voronoi diagram, telecommunication network routing, sensor node]
City planning and development using geographic information systems
2008 11th International Conference on Computer and Information Technology
None
2008
In recent years, GIS technology is used for managing natural disasters such as landslides to habitat mapping through remote sensing. GIS system has also been used to address land use issues. Many local governments are adding GIS technology as part of their regular administrative management system. High cost of GIS system, shortage of appropriate GIS specialists, lack of understanding and policies for homogeneous integration of GIS into planning are making the system, in many public sector cases, less than successful. We worked with a city government in the middle USA and participated in evaluating their system. Although small in size (population - thirty thousand), it's location next door to a large metropolitan city made things complicated. The city incorporated a GIS system into many aspects of public sector development, offered Web-based services to its community and published maps with search tools. The GIS based services gave the city government a high-tech reputation and a need-based analysis of public sector projects. The system gave the city officials access to real and accurate data which were not previously available. This often added more policy debates on sensitive and long range issues. It added a new dimension to city development and helped to find a combination of solutions in city wide constructions, drainage system, traffic control, and other engineering applications. Overall, the GIS system improved the efficiency of the city government and their administration processes as well as improved quality of decisions made by the city planners and managers.
[Geographic Information Systems, Costs, geographic information systems, Terrain factors, GPS, government administrative management system, Remote sensing, Technology management, natural disaster, Land use planning, Disaster management, Cities and towns, geographic information system, Local government, Urban planning, disasters, city planning, remote sensing, GIS technology, town and country planning, GIS, planning, Web services, decision making, Web-based service, government data processing]
SAGLET-Secure agent communication model
2008 11th International Conference on Computer and Information Technology
None
2008
Mobile agent offers a new programming paradigm in which a program can move from one node to another node in a network. Since data as well as code move at the same time, security issue becomes more important than traditional RPC system. To overcome the security hurdles, this paper presents a new model based on existing Aglets architecture. A new service agent along with a policy file is incorporated with the existing Aglets architecture. This novel service agent which controls &amp; co-ordinates all communication between the agent and the platform provides an extra layer of security by a set of API. This extra layer of security in Aglets will enhance the overall security of the platform.
[application program interfaces, Data security, Mobile communication, Communication system security, Communication standards, Aglets architecture, Network servers, security, security of data, programming paradigm, Mobile agents, application program interface, mobile agent, SAGLET, Authentication, mobile agents, Aglets, Computer networks, API, mobile agent security, service agent, Resource management, Cryptography]
Sensitivity analysis in robust and kernel canonical correlation analysis
2008 11th International Conference on Computer and Information Technology
None
2008
A number of measures of canonical correlation coefficient are now used in pattern recognition in the different literature. Some robust forms of classical canonical correlation coefficient are introduced recently to address the robustness issue of the canonical coefficient in the presence of outliers and departure from normality. Also a few number of kernels are used in canonical analysis to capture nonlinear relationship in data space, which is linear in some higher dimensional feature space. But not much work has been done to investigate their relative performances through simulation and also from the view point of sensitivity. In this paper an attempt has been made to compare performances of kernel canonical correlation coefficients (Gaussian, Laplacian and Polynomial) with that of classical and robust canonical correlation coefficient measures using simulation and influence function. We investigate the bias, standard error, MSE, qualitative robustness index, sensitivity curve of each estimator under a variety of situations and also employ boxplots and scatter plots of canonical variates to judge their performances. We observe that the class of kernel estimators perform better than the class of classical and robust estimators in general and the kernel estimator with Laplacian function has shown the best performance for large sample size.
[robust estimator, Sensitivity analysis, estimation theory, sensitivity analysis, Laplacian function, Influence Function and Sensitivity Curves, kernel canonical correlation analysis, Monte Carlo Simulation, scatter plot, Pattern Recognition, KCCA, data space, Robustness, Robust CCA, Kernel, correlation methods, pattern recognition, robust canonical correlation coefficient]
An improved keyword extraction method using graph based random walk model
2008 11th International Conference on Computer and Information Technology
None
2008
Keywords can be considered as condensed versions of documents, which can play important role in some text processing tasks such as text indexing, summarization and categorization. However, there are many digital documents especially on the Internet that do not have a list of assigned keywords. Assigning keywords to these documents manually is a difficult task and requires appropriate knowledge of the topic. Automatic keyword extraction process can solve this problem. In this paper, we introduce a new improved method for keyword extraction using random walk model by considering position of terms within the document and information gain of terms corresponds to the whole set of documents. We also incorporate mutual information (MI) of terms with random walk model to extract keywords from documents. The experiments on standard test collections show that our method outperforms the previously proposed methods.
[information gain, text analysis, graph theory, graph-based random walk model, Data mining, Voting, document keyword assignment, text processing task, mutual information, Keyword extraction, digital document, Citation analysis, text summary, information retrieval, random processes, improved keyword extraction method, Information technology, Text processing, term position, Computer science, Casting, random walk model, Internet, Mutual information, Indexing]
Action recognition with various speeds and timed-DMHI feature vectors
2008 11th International Conference on Computer and Information Technology
None
2008
Usually, various motion recognition approaches can not perform well on actions that have variable speeds, or part of the dataset is speedy and vice versa. In this paper, we present the timing issue of our Directional Motion History Image method. This method can solve overwriting or motion self-occlusion problem significantly and thereby it performs well for complex and repetitive activities. However, it is important to analyze the method with activities having various speeds to show its robustness. The experimental results demonstrate that it can perform well with variable paces of the actions though the recognition rate is compromised a bit compared to the dataset having usual speed. We also improve the classification method with the incorporation of motion duration in the final feature vector so that for similar type of activities having different pace, it can show better result. Experiments on another dataset show better performance with the timing incorporation. The achieved recognition rates are very encouraging for further research and implementation is some intelligent systems for surveillance and related applications.
[Image recognition, Shape, image classification, Humans, History, motion self-occlusion problem, moment, Histograms, motion classification method, motion recognition approach, Robustness, timed-DMHI feature vector, repetitive activity, DMHI, appearance-based model, variable speed, image motion analysis, human motion recognition, feature vector, Image analysis, avtion recognition, Hidden Markov models, action recognition, directional motion history image method, Timing, Motion analysis, image recognition]
Regression diagnostics in large and high dimensional data
2008 11th International Conference on Computer and Information Technology
None
2008
ldquoLearning methodsrdquo play a key role in the fields of statistics, data mining, and artificial intelligence, intersecting with areas of engineering and other disciplines. These methods for analyzing and modeling data come in two flavors: supervised and unsupervised learning. Regression analysis and classification are two well known supervised learning techniques. To get an effective model from regression analysis it is necessary to check and preprocess the data set in astronomy, bio-informatics, image analysis, computer vision etc, especially when the data sets are large and high dimensional. In these industries large or fat data appear with unusual observations (outliers) very naturally. Checking raw data for outliers in regression is regression diagnostics. Most of the popular diagnostic methods are not good enough for large and high dimensional data. The aim of this paper is to provide a new measure for identifying influential observations in linear regression for large high dimensional data.
[pattern classification, Data analysis, regression diagnostics, data analysis, large dimensional data, supervised learning, regression analysis, influential observation, outlier, Data engineering, Regression analysis, Data mining, Statistics, outlier detection, Unsupervised learning, unsupervised learning, Image analysis, high dimensional data, Supervised learning, high dimensional data modeling, learning (artificial intelligence), regression classification, Artificial intelligence, Astronomy]
Implementation of e-governance: Only way to build a corruption-free Bangladesh
2008 11th International Conference on Computer and Information Technology
None
2008
This paper focuses on various aspects of implementing e-governance in developing countries like Bangladesh. In this paper, we present a comparative analysis of present government architecture and the prospects and problems of implementing e-governance in Bangladesh emphasizing on the usage and effectiveness of e-governance to eradicate corruption from various sectors of governance. We especially present the adaptability of e-governance in the prime sectors of government and provide a methodical study on the strategies of involving mass people in the governance process improving information and service delivery with their participation in overall decision-making. The potential to ensure highest level of transparency in all the sectors of government with the implementation of e-governance is also presented in this paper. Moreover, we provide specific recommendations for implementing the e-governance in the most effective and efficient manner.
[Decision-making, E-learning, E-Commerce, Adaptability, E-Governance, E-Transparency, government architecture, M-Governance, Operating systems, service delivery, Charge carrier processes, ICT Infrastructure, Business, Electronic government, E-Administration., corruption-free Bangladesh, Decision making, e-governance process, Boosting, Information technology, Computer science, Electronic learning, Collaboration, decision making, government data processing]
A system for locating users of WLAN using dynamic mapping in indoor and outdoor environment-LOIDS
2008 11th International Conference on Computer and Information Technology
None
2008
The development of portable, light weight, hand held computing devices and high speed wireless local area networks enable user to remain connected while moving in indoor and outdoor environment. Wireless local area network has a wide range of utilization in moving from one location to another location. When a user is moving from one location to another, how the other user knows about the required station inside WLAN, for that we designed and implemented a system to locate a mobile user inside the wireless local area network based on RSSI. We designed four different architectures and used dynamic radio mapping of indoor and outdoor locations. These architectures are based on dynamic configuration of radio map in indoor and outdoor environment with the help of Sniffer, a specially designed application to locate a station at that time. We found a better location of a mobile user in WLAN with the help of Pamvotis a WLAN simulator. We tested this work in indoor and outdoor environment with different locations practically.
[Wireless LAN, Portable computers, OFDM, Electronic switching systems, Vehicle dynamics, RPM, ad-hoc network, received signal strength indication, dynamic radio mapping, Computer networks, Large-scale systems, indoor radio, Fading, mobile radio, LOIDS, Educational institutions, WLAN, Information technology, wireless local area network, AP, RSSI, mobile user, ad hoc networks, wireless LAN, indoor-outdoor environment]
Notice of Violation of IEEE Publication Principles<BR>Active noise control for industrial applications
2008 11th International Conference on Computer and Information Technology
None
2008
Notice of Violation of IEEE Publication Principles<BR><BR>"Active Noise Control for Industrial Applications"<BR>by S.F. Islam, A.H.M.S. Islam, M.M.R. Billah,<BR>in the Proceedings of the 11th International Conference on Computer and Information Technology, December 2008, pp. 312-316<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:<BR><BR>"Active Noise Control for Hearing Protection using a Low Power Fixed Point Digital Signal Processor"<BR>by Benny Sallberg, Lars Hakansson, and Ingvar Claesson<BR>in the Proceedings of the International Workshop on Acoustic Echo and Noise Control September 2005, pp. 65-68<BR><BR> <br/>In this paper we will show the construction of acoustic active noise control in hear defenders by the help of Filtered-x Least Mean Squares structure and using a low power fixed point digital signal processor. This system is implemented for 20 dB to 30 dB attenuation of broad band noise and ca 60 dB for sinusoidal interference. This is well suited for industrial applications.
[least mean squares methods, active noise control, hear defender, Industrial control, Adaptive filters, IIR filters, Digital filters, sinusoidal interference, acoustic active noise control, Acoustic noise, Attenuation, low power fixed point digital signal processor, Digital Signal Processors, Protection, Digital signal processors, Microphones, industrial application, filtered-x least mean squares structure, Active noise reduction, Signal processing algorithms, Ear, digital filters, industrial control, broad band noise, acoustic signal processing]
Side channel attack prevention for AES smart card
2008 11th International Conference on Computer and Information Technology
None
2008
This paper describes an AES smart card implementation highly tamper resistant to side channel attacks. Smart cards are gaining popularity in applications that require high security and store sensitive information. Modern smart cards, highly capable of complicated cryptology, provide a high assurance of tamper resistance and thus commonly used in payment application. Although advanced smart cards can not protect attackers from being defrauded by different side channel attacks. Small, embedded integrated circuits (ICs) such as smart cards are vulnerable to side-channel attacks (SCAs). We describe the development of differential power attacks and describe how to perform differential power kind of side-channel attack on an AES implementation, using simulated power traces. We also discusses the security prevention from such corresponding attacks, such as randomized masking techniques for software implementations.
[DPA prevention, Smart cards, Energy consumption, smart cards, Information analysis, tamper resistance, embedded integrated circuit, authorisation, simulated power trace, Hardware, Cryptography, Protection, side channel attack prevention, Circuit simulation, cryptography, power consumption model, Application software, randomized masking technique, Information technology, AES, SCA, AES smart card, DPA, Information security, cryptanalysis, payment application]
Performance evaluation of dode of a voice/data integrated wireless mobile network
2008 11th International Conference on Computer and Information Technology
None
2008
An analytical model has been developed to improve the performance of a wireless mobile communication network with voice and data integrated traffic system. To reduce the probability of no channel available, which is an important quantity for performance evaluation, delays to the last data end user is introduced to the system. It has been found that by applying a very small delay to the last data end user, the probability of no channel available can be drastically reduced compared to the usual erlang case.
[Packet switching, telecommunication channels, wireless mobile communication network, Circuits, voice/data integration, Telecommunication traffic, data integrated traffic system, Last data end user, performance evaluation, Mobile communication, Data engineering, Electronic mail, DODE, Delay, mobile communication, probability of no channel available, DH-HEMTs, Traffic control, Frequency, voice integrated traffic system, telecommunication traffic]
Evaluation of traffic parameters of multidimensional traffic of a combined link using a tabular method
2008 11th International Conference on Computer and Information Technology
None
2008
In many communication systems, two or more traffic links are merged together to form a common trunk line. Usually, a Markovian technique is used to determine the different traffic parameters of aforementioned combined trunk line. But the technique does not hold good and also the Markov chain takes a complex shape for a network where offered traffics are more than two different types. In this paper, we propose a tabular method instead of a Markovian technique for calculating various traffic parameters of a network with multimedia traffic, applicable in mobile cellular networks. It is found in this work that the proposed tabular method overcomes the complexities of the Markovian approach and yields similar results.
[Gold, Multidimensional systems, Shape, Telecommunication traffic, Quality of service, Electronic mail, tabular method, Equations, multimedia traffic, mobile cellular network, Markov chain, Land mobile radio cellular systems, Call blocking, M/M/n traffic model, Traffic control, Computer networks, combined trunk, multimedia communication, telecommunication traffic, cellular radio, multidimensional traffic parameter evaluation]
An efficient system for recognition of human face in different expressions by some measured features of the face using laplacian operator
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper we present an efficient system for face recognition with high recognition rate. In our proposed method, at first we detect the face from an image, then two main significant edge lines - chin-line and nose-line are determined and next we apply third order polynomial regression on these two lines to get a third order polynomial equation with four coefficients for each line. Here we use Laplacian operator to determine the curve of chin line and nose line. Then we measure distances from the middle point of the nose curve to the chin line horizontally and vertically, the width of the two eyebrows, the width of forehead and the distance from pupil to eyebrow. We also determine two regions - eye-region and nose-region. For each of these regions, we determine the average value of each three basic colors: red, green and blue. We then store the coefficients of the detected edge lines, the average values of the three basic colors and other distances and perform the task of recognition process. The existing face is recognized for which the weighted error is minimum and higher than a predefined threshold value. Experimental results show that our proposed method successfully recognizes face at a very high rate.
[chin line, Humans, regression analysis, human face recognition, predefined threshold value, Forehead, Gauss Gordan Method, feature extraction, Nose, face recognition, Polynomials, edge detection, image colour analysis, minimum weighted error, third order polynomial regression equation, nose curve, Laplace equations, Face recognition, Image edge detection, polynomials, nose line, Color, edge line detection, Independent Component Analysis (ICA), Polynomial Regression, Laplacian Operator, mathematical operators, Face detection, Laplacian operator, human expression, feature detection, three basic color, curve fitting, Principal Component Analysis (PCA), Eyebrows]
A fuzzy logic-based adaptive handoff management protocol for next-generation wireless systems
2008 11th International Conference on Computer and Information Technology
None
2008
In the integrated next-generation wireless systems (NGWS), users are always connected to the best available networks and used to switch between different networks based on their service needs. It is an important and challenging issue to support seamless handoff management in NGWS. The objective of this paper is to develop a seamless handoff management protocol for NGWS. In this paper, a fuzzy logic-based adaptive handoff (FLAH) management protocol is developed which is then integrated with an existing cross layer handoff protocol. Afterward, the handoff performance comparison of the existing protocol and our proposed protocol is carried out. The simulation results exhibit that, proposed fuzzy logic-based handoff management protocol has much better performance than conventional protocols for both intra and intersystem handoffs.
[Call Admission Control, Mobility Management, radio networks, Call admission control, telecommunication network management, Wireless application protocol, Intelligent Systems, fuzzy set theory, Switches, Mobile communication, Wireless Networks, fuzzy reasoning, Mobile radio mobility management, telecommunication computing, Next generation networking, Handoff Management, fuzzy inference engine, Wireless networks, protocols, Fuzzy Logic, Fuzzy systems, Base stations, next-generation wireless system, fuzzy logic, fuzzy rule base, Fuzzy logic, adaptive handoff management protocol, fuzzy logic system]
Performance maximization for question classification by subset tree kernel using support vector machines
2008 11th International Conference on Computer and Information Technology
None
2008
Question answering systems use information retrieval (IR) and information extraction (IE) methods to retrieve documents containing a valid answer. Question classification plays an important role in the question answer frame to reduce the gap between question and answer. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with machine learning algorithms Support Vector Machines (SVM) using kernel methods. An effective way to integrate syntactic structures for question classification in machine learning algorithms is the use of tree kernel (TK) functions. Here we use SubSet Tree kernel with Bag of words. Trade-off between training error and margin, Cost-factor and the decay factor has significant impact when we use SVM for the mentioned kernel type. The experiments determined the individual impact for Trade-off between training error and margin, Cost-factor and the decay factor and later the combined effect for Trade-off between training error and margin, Cost-factor. Depending on these result we also figure out some hyperplanes which can maximize the performance. Based on some standard data set outcomes of our experiment for question classification is promising.
[Computer interfaces, Machine learning algorithms, Optical computing, decay factor, SVM, optimisation, document retrieval, cost-factor, question answering system, Question Classification, training error statistics, Recall, learning (artificial intelligence), Kernel, Classification tree analysis, error statistics, machine learning approach, maximisation, support vector machines, trees (mathematics), subset tree kernel function, kernel, information retrieval, Question Answering, Information retrieval, automatic question classification, classification, Support vector machines, information extraction, support vector machine, Text categorization, Precision, SST, Support vector machine classification, Machine learning]
A lossless image compression technique using generic peano pattern mask tree
2008 11th International Conference on Computer and Information Technology
None
2008
Digital image processing has become ubiquitous in our daily life and the demands to produce and process images are ever increasing. Large amounts of space are required to store these images. Image compression techniques are in high demand as they allow reduction in this storage space. The basis for image compression, as is for most other compression techniques, is to remove redundant and unimportant data. Lossless image compression techniques retain the original information in compact form and do not introduce any errors when decompressed. In this paper, we discuss such a lossless technique using a data structure that we name ldquogeneric Peano pattern mask treerdquo. It is an improvement over a previously discussed Lossless Image compression technique - ldquoPeano pattern mask treerdquo. Both these structures are based on the data structure - Peano mask tree.
[digital image processing, Neodymium, image classification, data mining, Data mining, image storage space, peano mask tree data structure, Image reconstruction, Image coding, generic peano pattern mask tree, Peano Tree, Peano Pattern Mask Tree, Generic Peano Pattern Mask Tree, tree data structures, Image storage, Lossless Image Compression, Tree data structures, data compression, data-mining, Digital images, probability, P-tree based decision tree classification, Peano Mask Tree, Bayesian probability, Data structures, lossless image compression technique, Information technology, Computer science, Data mining Ready, decision trees, Bayes methods, image coding]
Proposal for an efficient quantum key distribution system using entanglement
2008 11th International Conference on Computer and Information Technology
None
2008
We propose a multiuser quantum key distribution system based on entanglement property which is simple, more efficient, cost effective and easy to implement in laboratory environment. Existing protocols uses base communication to provide security, as 50% of the time sender and receiver's base is not agreed there are 50% bit loss during key distribution but our protocol does not need the base communication hence we achieve 3 dB performance improvement.
[telecommunication security, Protocols, Quantum entanglement, cryptographic protocols, Data security, multiuser quantum key distribution system, Electrooptic modulators, data security, Photonic crystals, Telecommunication switching, quantum entanglement, cryptographic protocol, quantum channel, QKD, Proposals, Optical retarders, secure communication, Optical fiber polarization, multiuser channels, Quantum computing, quantum networking, quantum cryptography]
A concurrent approach to clustering algorithm with applications to VLSI domain
2008 11th International Conference on Computer and Information Technology
None
2008
Circuit partitioning plays an important role in physical design automation of very large scale integration (VLSI) chips. In this brief we present a new connectivity based top down as well as bottom up approach to clustering algorithm for VLSI circuit partitioning. The proposed clustering algorithm partitions the circuit by focusing on highly interconnected cell groups. This clustering algorithm leads to a parallel implementation in which multiple processors are used to identify clusters simultaneously. The process starts with forming clusters by grouping the cells that are tightly connected and as well as the cells that are loosely connected. Considering both types of groups has the advantage that clusters formed from this technique will be highly connected and compact too. Therefore the proposed clustering method can reduce the size and also speed-up the large-scale partitioning problem without loosing partitioning solution qualities. The performance of the proposed clustering algorithm is evaluated on a standard set of partitioning benchmark-ISPD98 benchmark suite.
[cluster, Clustering methods, Circuits, graph theory, Very large scale integration, threshold, Concurrent computing, very large scale integration chips, VLSI circuit partitioning, partition, Clustering algorithms, integrated circuit design, push-pop, clustering algorithm, VLSI, partitioning benchmark-ISPD98 benchmark suite, highly interconnected cell groups, Partitioning algorithms, Application software, Information technology, Base, multiple processors, Computer science, concurrent approach, Iterative algorithms]
Symbol timing estimation using near ML techniques and statistical performance evaluation for binary communications
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we analyze the performance of a non-data aided near maximum likelihood (NDA-NML) estimator for symbol timing recovery in binary digital communications. The analysis of the estimator is performed for an additive noise only channel and the simulation results are extended to a flat fading channel. The probability distribution of the timing estimates is derived, presented and compared with simulation results. The performance of the estimator is presented in terms of the bit error rate (BER) and the error variance of the estimates. The BER is computed when the estimator is operating under additive white Gaussian noise (AWGN) channel and Rayleigh fading channel. The variance of the estimates is computed for the noise only case and compared with the Cramer Rao bound (CRB) and modified Cramer Rao bound (MCRB).
[Rayleigh fading channel, Flat fading channel, AWGN, Bit error rate, Symbol Timing Error, binary digital communication, maximum likelihood estimation, AWGN channels, maximum likelihood estimator, Timing Synchronization, ML technique, additive white Gaussian noise channel, Additive white noise, digital communication, Digital communication, Performance analysis, error statistics, statistical performance evaluation, Fading, Maximum likelihood estimation, MCRB, Computational modeling, Rayleigh channels, symbol timing estimation, flat fading channel, bit error rate, BER, Cramer-Rao bounds, CRB, Timing, Near Maximum Likelihood Estimation]
An energy-efficient data security system for wireless sensor network
2008 11th International Conference on Computer and Information Technology
None
2008
Wireless sensor networks are a new type of networked systems, characterized by severely constrained computational and energy resources, and an ad hoc operational environment. Because sensor networks may interact with sensitive data and operate in hostile unattended environments, it is imperative that these security concerns be addressed from the beginning of the system design. However, due to inherent resource and computing constraints, security in sensor networks poses different challenges than traditional network security. There is currently enormous research potential in the field of wireless sensor network security. In this paper we introduce a message expantion problem which can be solved by a data link-layer security architecture called dasiaCipher-text Stealing methodpsila for wireless sensor networks. Conventional security protocols tend to be conservative in their security guarantees, typically adding 16-32 bytes of overhead. With small memories, weak processors, limited energy, and 30 byte packets, sensor networks cannot afford this luxury. TinySec addresses these extreme resource constraints with careful design; we explore the tradeoffs among different cryptographic primitives and use the inherent sensor network limitations to our advantage when choosing parameters to find a sweet spot for security, packet overhead, and resource requirements.
[telecommunication security, Wireless, wireless sensor networks, wireless sensor network, Message Expansion, Sensor phenomena and characterization, Sensor systems, Security, Energy resources, Computer architecture, ad hoc operational environment, Computer networks, Cryptography, cipher-text stealing method, energy-efficient data security system, Data security, network security, cryptographic primitives, cryptography, Cryptographic protocols, TinySec, Wireless sensor networks, data link-layer security architecture, Cipher text Stealing, Energy efficiency, ad hoc networks, Sensor]
An efficient implementation scheme for multidimensional online analytical processing
2008 11th International Conference on Computer and Information Technology
None
2008
Multidimensional online analytical processing (MOLAP) processes data that is already stored in a multidimensional array in which all possible combinations of data are reflected. It employs multidimensional array as their basic data structure. The intent of this paper is to develop a MOLAP model based on the scheme called extended Karnaugh map representation (EKMR) and evaluate it. The basic idea of EKMR scheme is to present multidimensional array by a set of two dimensional arrays. This reduces data access time from secondary storage if the tupple of subscripts are known. Effectiveness of this scheme over traditional multidimensional array is shown with sufficient analysis and experimental results.
[Multidimensional systems, Data analysis, data mining, Relational databases, data structure, Dice, Data warehouses, extended Karnaugh map representation, Data structures, Data engineering, Information retrieval, EKMR, database management systems, Information technology, Information analysis, multidimensional online analytical processing, Slice, multidimensional array, Data models, data structures, MOLAP, Multidimensional Array, MOLAP database]
Short messaging service (SMS) based m-banking system in context of Bangladesh
2008 11th International Conference on Computer and Information Technology
None
2008
M-banking has become one of the most familiar banking service providing technologies in different western countries. Now-a-days billions of inhabitants of Bangladesh are within a network through mobile network coverage. But in the commercial sectors like banking, m-commerce technology has not been adopted broadly yet. Considering m-commerce perspective in Bangladesh a SMS based m-banking system has been proposed which is able to provide several essential banking services only sending SMS to bank server from any remote location. This proposed system is divided into five major phases: interfacing module, SMS technology adoption module, SMS banking registration module, service generation module, and data failover module. This system facilitates bank customers by providing four major services like balance enquiry , balance transfer between authenticated customers, DPS payment and bill payment without going to bank physically and save their precious time. At least, after evaluating each module of this developed system a satisfactory accuracy rate 93.18 % is obtained.
[electronic messaging, DPS, Mobile communication, Mobile handsets, Balance Enquiry, Electronic commerce, SMS technology adoption module, banking, mobile computing, Web and internet services, balance enquiry, mobile network coverage, short messaging service, Business, data failover module, Context-aware services, m-banking system, service generation module, Banking, m-banking, Information technology, Data failover, Computer science, Bangladesh, SMS banking registration module, balance transfer, SMS, Message service, interfacing module]
An application program interface for vBulletin
2008 11th International Conference on Computer and Information Technology
None
2008
vBullletin Calendar API is built on calendar feature of vBulletin, a commercial Internet forum software produced by Jelsoft Enterprises Ltd. It acts as a remote agent for client applications intended to be developed by third party software developers. Such applications can be Facebook application, desktop application, IRC-bot and mobile applications for instant event notification and reminder. Important notices, announcements posted from departments of educational institutions often do not reach to target people, in time, due to various reasons. In this paper, we have proposed to develop an interface through which individual client applications monitor a common source of notice service center and notify users instantly.
[IEEE news, Computer interfaces, application program interfaces, vBulletin, DoS, HTML, PHP-BB, Electronic mail, URI, application program interface, mobile applications, MySQL, commercial Internet forum software, SMS flag, Calendars, vBullletin Calendar API, desktop application, Educational institutions, Facebook application, Application software, remote agent, Information technology, IRC-bot, instant event notification, client applications, XML, Seals, API, Internet]
Cutting a cornered convex polygon out of a circle
2008 11th International Conference on Computer and Information Technology
None
2008
The problem of cutting a convex polygon P out of a piece of paper Q with minimum total cutting length is a well studied problem in computational geometry. Researchers studied several variations of the problem, such as P and Q are convex or non-convex polygons and the cuts are line cuts or rays cuts. In this paper we consider yet another variation of the problem where Q is a circle and P is convex polygon such that P is bounded by a half circle of Q and all the cuts are line cuts. We give a simple linear time O(log n)-approximation algorithm for this problem where n is the number of vertices of P.
[approximation theory, Costs, computational geometry, Paper technology, Information technology, Algorithm, cornered convex polygon, Computer science, Computational geometry, minimum total cutting length, polygon cutting, line cut, approximation algorithm, Approximation algorithms, Polynomials, circle]
A new distributed evolutionary computation technique for solving large number of equations
2008 11th International Conference on Computer and Information Technology
None
2008
Evolutionary algorithm is more effective to gain optimal solution to solve equations than traditional methods. It also provides quick solution for solving large number of equations having huge parameters with less expense in distributed manner. This paper presents a new distributed evolutionary computation technique, which decomposes decision vectors into smaller components and achieves optimal solution in short time. In this technique, A Jacobi-based Time Variant Adaptive (JBTVA) Hybrid Evolutionary Algorithm is distributed. Moreover, a new selection method named Best All Selection (BAS) is introduced for selecting best individuals. Experimental results show that optimal solution is achieved for different kinds of problems having huge parameters and considerably speedup is formed in proposed distributed system.
[Master-Slave architecture, Speedup, hybrid evolutionary algorithm, Computational modeling, Genetic mutations, Linear Equations, Evolutionary computation, Master-slave, distributed processing, distributed evolutionary computation, distributed system, optimal solution, Distributed computing, Information technology, Equations, Jacobian matrices, Computer science, evolutionary computation, best all selection, Hybrid Algorithm, Jacobi-based time variant adaptive, Computer architecture, Evolutionary Algorithms]
MIMOME channel secrecy capacity
2008 11th International Conference on Computer and Information Technology
None
2008
Although conventional cryptographic security mechanisms are essential to the overall problem of security, the openness of wireless medium poses both threats and opportunities for securing transmission. Information theoretic security is attracting researchers for its robust nature. In this paper, a Gaussian multiple input multiple output multiple eavesdropper (MIMOME) channel is considered where a transmitter is communicating to a receiver in the presence of an eavesdropper. The transmitter is equipped with multiple antennas, while the receiver and the eavesdropper also contains multiple antennas. We present a technique for determining the secrecy capacity of the MIMO channel under Gaussian noise. To do so, we transform the channel into multiple single input multiple output (SIMO) Gaussian wiretap channel and then use scalar approach using standard techniques of communications theory. Then we consider the effect of fading and found out the limiting values of the secrecy capacity.
[telecommunication security, information theoretic security, communications theory, Transmitting antennas, MIMOME channel secrecy capacity, Communication system security, single input multiple output channel, covariance matrix, Transmitters, Gaussian noise, cryptographic security, SIMO Gaussian wiretap channel, MIMO, Robustness, wiretap channel, information theory, wireless channels, multiple antennas, Cryptography, MIMO communication, transmitter, Gaussian multiple input multiple output multiple eavesdropper, Channel capacity, receiver, cryptography, radio receivers, secure transmission, MIMO channel, fading, wireless medium, Information security, Receiving antennas, radio transmitters, Secrecy capacity]
Resource capability discovery and description management system for bioinformatics Data and service Integration - an experiment with gene regulatory networks
2008 11th International Conference on Computer and Information Technology
None
2008
Traditional legacy HTML based web sites/ page can be thought of as web services because the dynamic web pages can take user input argument via web forms and response to user query. The ability of agents and services to automatically locate and interact with unknown partners is a goal for Web based Data Integration system. This ldquoserendipitous interoperabilityrdquo is hindered by the lack of an explicit means of describing what web pages are able to do and in order to do it what input it takes and what output it produces, that is what is their capabilities [1]. The tremendous success of the WWW is countervailed by the efforts needed to search and find relevant information. For tabular structures embedded in HTML documents, typical keyword or link-analysis based search fails. The next phase envisioned for the WWW is automatic ad-hoc interaction between intelligent agents, web services, databases and semantic web enabled applications. A large amount of information available on the Web is formatted in HTML tables, which are mainly presentation oriented and are not suited for database applications. As a result, how to capture information in HTML tables semantically and integrate relevant information is a challenge. We are envisioning another layer of web abstraction where user can query intra web document table like structure. Our prototype application is based on WebFusion and an ad hoc query language BioFlow [2], [3], [4], [5], [6] a software agent that can simulate a person interacting with web search forms and extracting information from the resulting pages by means of an API. We need to develop a framework which is able to query search web forms and the web page tables in a SQL way. In this context we also report a Java based implementation for integrating Flybase and AlignACE site.
[HTML, automatic ad hoc interaction, WebFusion, intelligent agents, gene regulatory networks, Web Data Integration, Web abstraction, Bioinformatics, HTML documents, Software prototyping, Web based data integration system, databases, semantic Web, dynamic Web pages, Web document, Table structure, extraction ontology, Intelligent agent, SQL, Semantic Web, Web services, keyword analysis based search, link analysis based search, bioinformatics, ad hoc query language, Resource management, Web Information Extraction, Table modeling, open systems, tabular structures, table like structure, World Wide Web, Flybase site, Hidden Web, Intelligent Wrapper, query processing, bioinformatics data, Databases, genetics, AlignACE site, Web mining, description management system, Ontology generation, Web Automation, hypermedia markup languages, Java based implementation, serendipitous interoperability, Web forms, software agent, service integration, HTML forms, resource capability discovery, BioFlow, Web pages, API, Web sites]
On the design of an effective corpus for evaluation of Bengali Text Compression Schemes
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we propose an effective platform for evaluation of Bengali text compression schemes. We perform a methodical study on the formulation-approaches of text corpus for data compression and present an effective corpus named Ekushe-Khul for evaluating the Bengali text compression schemes, which is the first initiative in the context of Bengali text compression. To design the Bengali text compression corpus, we consider type to token ratio as the selection criteria with a number of secondary considerations. This paper also presents a mathematical analysis on data compression performance with structural aspects of corpora. The proposed corpus is effective for evaluating compression efficiency of small and middle sized text files.
[Performance evaluation, text analysis, Costs, Dictionaries, type-to-token ratio, Bengali Text, Compression Efficiency, Bengali text compression scheme evaluation, Data compression, Dictionary Coding, Corpus, Design engineering, Image coding, Mathematical analysis, Performance analysis, corpus design, data compression, Data Management, Type to Token Ratio (TTR), mathematical analysis, Bengali Text Compression, Evaluation Platform, Information technology, Computer science, natural languages]
An energy saving program for bangladesh, for reducing load shedding, and for continuity of power for IT sector
2008 11th International Conference on Computer and Information Technology
None
2008
The success of computers, IT, and telecommunications hinges greatly on the inexpensive and uninterrupted availability of power. Even with the gradual depletion of fossil fuel, and the rise in fuel prices, there has been insufficient effort towards saving electricity and making the most of remaining fuel reserves. It is time to adopt energy saving programs in Bangladesh, creating awareness that extravagant consumption is detrimental to national, global, and long-term interests. Such energy saving programs have been implemented to various extents in countries like Brazil, Norway, New Zealand, and the USA. With its high population density, low development, poor economic infrastructure, and dynamic river network, Bangladesh provides a unique backdrop to an Energy Saving programs. Such a program should reduce load-shedding, distribute electricity among more users, and divert electricity to productive uses like irrigating fields and preventing manufacturing stoppage. Continuity of electricity would also save our huge investment in generators and UPS equipment. An energy saving program should be well-understood and researched by policymakers and experts. An important part of such a program should be changing consumer behavior, such as turning off appliances when not in use. Power-hungry air-coolers develop acclimatization and dependence in users, who then perceive normal outside temperatures as uncomfortable. Low-power alternatives to air-coolers are ceiling fans and ventilator fans on walls. LED lights, compact fluorescent lamps, and IT and telecommunications are to be promoted as efficient uses of electricity. Urban sprawl to distant suburbs should be contained in favor of compact townships, which are better adapted to buses. Buses should be promoted over smaller cars, which should be promoted over larger hungry cars and SUVs. This paper pursues such common sense approaches, together with recommendations by experts, and implementation by policymakers.
[load shedding, Fans, fossil fuel, information technology, UPS equipment, Power system economics, Load shedding, Conservation, Fasteners, Telecommunication computing, telecommunications, ventilator fans, IT sector, Energy, Investments, Fuel economy, Manufacturing, Power generation economics, compact fluorescent lamps, LED lights, load distribution, ceiling fans, Fossil fuels, Rivers, air-coolers, Fossil Fuel, Air conditioner, fossil fuels, Bangladesh, buses, energy conservation, generators, energy saving program, Power, electricity distribution, Power Outage]
Performance evaluation of a wireless Orthogonal Frequency Division Multiplexing system under various concatenated FEC channel-coding schemes
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we study the effect of various concatenated forward error correction (FEC) codes on the performance of a wireless orthogonal frequency division multiplexing (OFDM) system. In FEC concatenated channel code, the OFDM system incorporates Reed-Solomon (RS) encoder of (255,239,8), Cyclic Encoder of (15,11), Bose-Chadhuri-Hocquenghem (BCH) encoder of (127,64) with Convolution encoder of 2/<sub>3</sub> and frac34-rated codes under different combinations of digital modulation (QPSK, 8PSK, 32-QAM and 64-QAM). The simulation study is made with the development of a computer program written in Matlab source code on the processing of recorded audio signal under additive white Gaussian noise (AWGN) channel. The simulation results of estimated Bit error rate (BER) show that the implementation of concatenated RS(255,239,8) code with frac34-rated Convolutional code under QPSK modulation technique is highly effective to combat inherent interference in the communication system. Due to constraint in data handling capability of the Matlab editor, a segment of the recorded audio signal is used for analysis. The transmitted audio message is found to have retrieved effectively under noisy situation.
[Reed-Solomon encoding, AWGN, convolution encoder, OFDM, channel coding, audio signal, Bit error rate, cyclic codes, Concatenated codes, quadrature phase shift keying, Bose-Chadhuri-Hocquenghem encoder, audio coding, Reed-Solomon codes, AWGN channels, Convolution, QPSK modulation, Convolution encoding (CC), Additive white noise, OFDM modulation, wireless channels, OFDM system, concatenated FEC channel-coding scheme, error statistics, Reed-Solomon encoder, wireless orthogonal frequency division multiplexing system, error correction codes, Computational modeling, Cyclic encoding, performance evaluation, forward error correction, Matlab editor, bit error rate, Orthogonal Frequency Division Multiplexing, Quadrature phase shift keying, concatenated codes, Bose-Chadhuri-Hocquenghem (BCH) Bit error rate (BER) and Additive White Gaussian Noise (AWGN), Forward error correction, digital modulation, BCH codes, cyclic encoder, data handling, convolutional codes, AWGN channel]
A study of major Mobile payment systems' functionality in Europe
2008 11th International Conference on Computer and Information Technology
None
2008
Mobile payment is an exciting domain, which will rapidly evolve in the years to come. The key to mobile payment acceptance is in the hands of customers. In this paper first some of the major European mobile payment systems are introduced and evaluated base on security, cost , convenes and functionality requirement criteria, then the results of past researches about the customer and merchant adoption of mobile payment systems (mp) are reviewed and barriers and drivers from both customer and merchant view are compared. Here by, this paper tries to find shortcomings in major mp systems and presents the major barriers and drivers to wide spread adoption of mobile payment from both customer and merchant point of view.
[Costs, mobile radio, Europe, Mobile handsets, Security, Electronic commerce, Information technology, electronic money, mobile commerce, Mobile commerce, Customer adoption, customer satisfaction, mobile payment system functionality, Personal digital assistants, customer adoption, Consumer electronics, Business, Driver circuits, mobile Payment system]
Achieving guaranteed performance on computational Grid
2008 11th International Conference on Computer and Information Technology
None
2008
For the dynamic nature of the grid, the applications need to adapt to the changing resource status and usage scenarios. This is required to ensure that the performances of the components do not suffer. Objective of this paper is to present an adaptive execution scheme for achieving guaranteed performance as has been agreed upon through service level agreements (SLAs). The scheme has been implemented based on the notion of performance properties and by deploying a set of autonomous agents within an integrated performance-based resource management framework.
[grid computing, performance properties, Grid, adaptive execution, Degradation, guaranteed performance, Runtime, resource allocation, computational grid, Grid computing, Autonomous agents, Performance analysis, Contracts, software performance evaluation, Performance Properties, Application software, integrated performance, Information technology, service level agreement, resource management framework, Computer science, resource status, Adaptive Execution, Resource management, usage scenarios, autonomous agents]
Neural network and genetic algorithm approaches for forecasting bangladeshi monsoon rainfall
2008 11th International Conference on Computer and Information Technology
None
2008
True information about rainfall is crucial for human activities such as the use and the management of water resources, hydroelectric power projects, warning to impend droughts or floods, urban areas sewer systems and many others. This paper investigates the development of an efficient model to forecast monthly monsoon rainfall for a number of stations, namely Barishal, Chittagong, Dhaka, Khulna, Rajshahi and Sylhet. It is believed that rainfall forecasting is difficult and also a challenging task for anyone because rainfall data are multi-dimensional and nonlinear. Therefore, to model rainfall data, the AI models, namely artificial neural network (ANN), adaptive neuro fuzzy inference system (ANFIS) and genetic algorithm (GA) have been used. Results found by the AI models are also compared to the linear regression model to show advantages of selecting these models. Findings suggest that ANFIS and GA approaches could be used more accurately than the other two selected approaches to forecast the Bangladeshi monsoon rainfall.
[artificial neural network, adaptive neuro fuzzy inference system, Project management, Floods, neural network, artificial intelligence, Genetic algorithms, genetic algorithm, water resources, fuzzy inference system, Human resource management, Water resources, rain, hydroelectric power projects, Power system management, fuzzy neural nets, forecasting, Artificial neural networks, genetic algorithms, inference mechanisms, Bangladeshi monsoon rainfall forecasting, Atmospheric dynamics, Neural networks, forecasting theory, Artificial intelligence, Energy management]
A cost effective multi-granular location information strategy for MANET
2008 11th International Conference on Computer and Information Technology
None
2008
In the literature there are several location based routing schemes available for MANET. The effective management of location information has the maximum impact on the performance of this type of routing protocols. In this paper first we have made a study of the different categories of existing location services. Then we have proposed a new location management scheme which effectively implements the major location management functions; namely location update and location query. In this scheme we have incorporated the multi-granular location information strategy. This is particularly aimed at reducing the orthogonal costs associated with location update and location query which we have clearly shown in the analysis part of this paper.
[Costs, Location Service, routing schemes, Resonance light scattering, Multi-Granular Location Information, location query function, Mobile ad hoc networks, Network servers, Mobile Ad-Hoc Networks, Routing protocols, Computer networks, Location Update Cost, cost reduction, mobile radio, orthogonal cost reduction, information management, Position Based Routing Protocols, Maintenance engineering, location update function, Ad hoc networks, location information management, Information technology, multigranular location information strategy, MANET, routing protocols, Frequency, Location Query Cost, ad hoc networks]
On hot-spot traffic pattern of TESH network
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we present a deadlock-free routing algorithm for the Tori connected mESH (TESH) network using 2 virtual channels - 2 being the minimum number for dimension-order routing - and evaluate the networks dynamic communication performance under the hot-spot traffic pattern, using the proposed routing algorithm. We evaluate the dynamic communication performance of TESH, mesh, and torus networks by computer simulation. It is shown that the dynamic communication performance of the TESH network is better than that of the mesh and torus networks. It is also shown that the relative difference in maximum throughput between torus and TESH networks decreases with the increase of hot spot traffic.
[Multiprocessor interconnection networks, network routing, multiprocessor interconnection networks, Telecommunication traffic, Routing, deadlock-free routing, Electronic mail, hot-spot traffic pattern, Concurrent computing, TESH network, dynamic communication performance, tori connected mesh network, virtual channel, deadlock-free routing algorithm, System recovery, Traffic control, Hypercubes, computer simulation, Computer networks, Hardware]
An effective term weighting method using random walk model for text classification
2008 11th International Conference on Computer and Information Technology
None
2008
Text classification may be viewed as assigning texts in a predefined set of categories. However there are many digital documents that are not organized according to their contents. So it is difficult task to find relevant documents for a user. Automatic text classification problem can solve this problem. In this paper we introduce a new random walk term weighting method for improved text classification. In our approach to weight a term, we exploit the relationship of local (term position, term frequency) and global (inverse document frequency, information gain) information of terms (vertices). Moreover, we weight terms by considering co-occurrence and semantic relation of terms as a measure of dependency. To evaluate our term weighting approach we integrate it in Rocchio text classification algorithm and experimental results show that our method performs better than other random walk models.
[Performance evaluation, Algorithm design and analysis, information gain, text analysis, term weighting method, digital document, semantic relation, Citation analysis, graph theory, Classification algorithms, classification, Information technology, Computer science, Casting, Voting, Text categorization, random walk model, automatic text classification, Frequency, Rocchio text classification algorithm, Text classification]
QoS based Fair Load-Balancing: Paradigm to IANRA Routing Algorithm for Wireless Networks (WNs)
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, a new algorithm, intelligent agent antnet based routing algorithm (IANRA) is proposed to enhance load balancing strategy in wireless networks (WNs). IANRA is based on ants behaviour with some important factors such as: the specific self-organizing behaviour of ant colonies, the shortest path discovery and the related framework of ant colony optimization (ACO). The main focus in IANRA is to find optimum and near optimum route by means of genetic algorithm (GA) using breeding capability of ants. Here, ants can produce a number of generations with the target to discover an optimized route. Hence IANRA is able to prevent of the difficulties which exist in existing routing algorithms, such as ad hoc on-demand distance vector (AODV), AntNet etc. The obtained results show that the efficiency of IANRA algorithm is better than AODV and AntNet or any other related algorithm. In addition, the proposed algorithm (IANRA) is able to reduce the end-to-end delay and increase the packet delivery ratio significantly.
[radio networks, load balancing, Routing Algorithm, Laboratories, IANRA, Quality of service, Load Balancing, Delay, genetic algorithm, resource allocation, Wireless networks, QoS, ant colony optimization, Computer networks, wireless network, Wireless Communication Networks, Ant colony optimization, Ant Colony Optimization (ACO), Routing, genetic algorithms, quality of service, shortest path discovery, Information technology, intelligent agent AntNet-based routing algorithm, telecommunication network routing, Load management, Systems engineering and theory, near optimum route]
Codebook design method for speaker identification based on Genetic Algorithm
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, a novel method of designing a codebook for speaker identification purpose utilizing genetic algorithm has been proposed. Speech features have been extracted using standard speech parameterization method such as LPC, LPCC, RCC, MFCC, DeltaMFCC and DeltaDeltaMFCC. For each of these techniques, the performance of the proposed system has been compared. In this codebook design method, genetic algorithm has the capability of getting global optimal result and hence improves the quality of the codebook. Experimental results show the superiority (i.e. 92% accuracy has been achieved) of this proposed codebook design method.
[Algorithm design and analysis, codebook design, Acoustic distortion, speech parameterization method, Design methodology, speech feature extraction, codebook design method, Decoding, genetic algorithms, speaker identification, speech parameterization, Mel frequency cepstral coefficient, Genetic algorithms, Loudspeakers, genetic algorithm, Design engineering, feature extraction, Speech, Genetic engineering, speech pre-processing, Automatic speaker identification, speaker recognition]
Analysis and synthesis of Bangla Dental consonants
2008 11th International Conference on Computer and Information Technology
None
2008
This paper describes in detail the analysis results of the Bangla dental consonants [t], [th], [d] and [dh] analyzed through the spectral estimation techniques in four long vowel contexts [a], [i], [o], and [u] for an accurate description of their acoustic characteristics and features and the differences between the corresponding cognate sounds. Various parameters like duration of closure/voice bar, duration of burst, voice onset time, duration of aspiration, rate of second formant transition and burst frequencies and amplitudes have been studied in details. For the synthesis of dental sounds, the source and vocal tract parameters of the synthesizer configuration were selected very carefully along with formant frequencies and their relative amplitudes besides the burst amplitude. The quality of synthetic speech was evaluated through subjective listening along with matching the spectra of synthetic speech with original speech.
[Tongue, Speech analysis, pattern matching, aspiration duration, spectral analysis, spectra matching, voice bar duration, Speech synthesis, subjective listening, voice onset time, Information analysis, Speech Processing, Bangla dental consonants, synthetic speech quality, Synthesis, burst frequencies, Phoneme, speech synthesis, natural language processing, Natural languages, Teeth, Dentistry, spectral estimation techniques, Spectral analysis, burst duration, Computer science, closure bar duration, acoustic characteristics, long vowel, Frequency, formant transition, acoustic signal processing, dental sound synthesis, LPC]
Analysis of broadband slotted microstrip patch antenna
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, a novel multiple slot microstrip patch antenna with enhanced bandwidth is presented. The design adopts contemporary techniques namely; probe feeding, inverted patch structure and multiple slotted patch. The composite effect of integrating these techniques and by introducing the novel multiple shaped patch, offer a low profile, wide bandwidth, high gain and compact antenna element. The proposed patch has a compact dimension of 0.707 lambda<sub>0</sub> times 0.354 lambda<sub>0</sub> (where lambda<sub>0</sub> is the guided wavelength of the center operation frequency). With the proposed concept, an antenna prototype is simulated and analyzed. The proposed antenna achieves a fractional bandwidth of 27.15% (1.91 to 2.51 GHz) at 10 dB return loss while maintaining a maximum gain of 10.5 dBi. The design is suitable for array applications especially for base station.
[Broadband antennas, contemporary techniques, Broadband antenna, Information analysis, Space technology, Microstrip antennas, Bandwidth, inverted patch structure, multiple slot microstrip patch antenna, Probes, enhanced bandwidth, broadband slotted microstrip patch antenna, microstrip patch antenna, multiple slotted patch, broadband antennas, microstrip antennas, Information technology, multiple shaped patch, probe fed, slot antennas, Patch antennas, Resonance, probe feeding, Impedance, composite effect]
A Corpus-based evaluation of lexical components of a domain-specific text to Knowledge Mapping prototype
2008 11th International Conference on Computer and Information Technology
None
2008
The aim of this paper is to evaluate the lexical components of a text to knowledge mapping (TKM) prototype. The prototype is domain-specific, the purpose of which is to map instructional text onto a knowledge domain. The context of the knowledge domain of the prototype is physics, specifically DC electrical circuits. During development, the prototype has been tested with a limited data set from the domain. The prototype now reached a stage where it needs to be evaluated with a representative linguistic data set called corpus. A corpus is a collection of text drawn from typical sources which can be used as a test data set to evaluate NLP systems. As there is no available corpus for the domain, we developed a representative corpus and annotated it with linguistic information. The evaluation of the prototype considers one of its two main components-lexical knowledge base. With the corpus, the evaluation enriches the lexical knowledge resources like vocabulary and grammar structure. This leads the prototype to parse a reasonable amount of sentences in the corpus.
[Knowledge engineering, System testing, Vocabulary, text analysis, linguistics, Lexicon, Circuit testing, NLP system, Corpus, Design engineering, domain-specific text, vocabulary, grammar, Prototypes, lexical knowledge resources, NLP Systems, Knowledge Mapping, corpus-based evaluation, natural language processing, linguistic data set, Information technology, Physics, knowledge domain, Computer science, grammars, Morphology, Tagging, text to knowledge mapping, instructional text, lexical component]
Comparisons of maximum system lifetime in diverse scenarios for body sensor networks
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, the main target is to compare the system lifetimes in diverse scenarios based on our new energy dissipation model in the body sensor network as well as maximize the system lifetime of sensor nodes when they will make communication among body sensors and personal communication unit. Nowadays, the ultra low energy consumption is the very much important challenge for medical applications. The best compression technique like LPC is selected for energy saving based on some calculations. This research work studies and analyzes the transceiver energy consumption for different compression algorithms and select the best technique like LPC for energy saving. Formulation of a linear programming problem is also the important part of this research work, where is to maximize the system lifetime which is equivalent to the time until the first node runs out of battery. Maximum system lifetimes are calculated by MATLAB optimization technique using and without using efficient compression algorithm like LPC in various environments. Results show that maximum system lifetimes calculated in different scenarios using efficient compression technique like LPC is better than without using compression technique.
[Algorithm design and analysis, Energy consumption, personal communication unit, wireless sensor networks, MATLAB optimization technique, linear programming problem, Medical services, Sensor systems, linear programming, medical application, Compression algorithms, WBSN, heart-beat signal diagnosis, Energy dissipation, linear prediction coding (LPC), system lifetime maximization, LPC compression algorithm, Mathematical model, transmitting energy and receiving energy, Biomedical equipment, personal communication networks, maximum system lifetime, Linear predictive coding, body area networks, wireless body sensor network, energy saving, low energy consumption, biomedical telemetry, cardiology, Body sensor networks, ultra-low energy consumption, linear problem, patient diagnosis, energy dissipation model]
The opportunities and challenges of using ICT by State Owned Enterprises in Bangladesh: Case of SOEs under Privatization Commission
2008 11th International Conference on Computer and Information Technology
None
2008
The potential contribution of information and communication technology (ICT) to increase the productivity of state owned enterprises (SOEs) in developing countries has long been recognized. However, the realization of this potential has been problematic and over recent years there have been a number of initiatives supported by government, non government and foreign agencies which have endeavored to aid and encourage the up take of ICT to enable access to such promised benefits. Despite strong theoretical arguments suggesting that ICT has much to offer to SOEs, the study would seem to suggest that use of ICT by SOEs listed under privatization commission is still in its infancy. It also identifies some significant barriers which are impacting upon the level of ICT adoption amongst SOEs, and confidence on ICT of the SOEs in Bangladesh. This paper is based on the study conducted as part of the IDA funded Enterprise Growth and Bank Modernization Project and explores the opportunities and barriers of using ICT to provide a necessary lever to enhance competitiveness and productivity of SOEs in Bangladesh.
[Privatization, information and communication technology, SOE, ICT, state owned enterprise, privatisation, privatization commission, e-business, electronic commerce]
Layer-2 protocol adaptation method to improve fast handoff for mobile IPv6 vertical handoffs
2008 11th International Conference on Computer and Information Technology
None
2008
Inter-technology roaming is known as one of the interesting challenges toward fourth generation of mobile and wireless communication. While FMIPv6 standardizes the fast handoff solutions in IP layer, the issues of media independency are being investigated through IEEE802.21 project. The integration of these two standards is believed to result in solutions for vertical handoffs between different network technologies. This paper presents an improved link layer mechanism to assist FMIPv6 for seamless vertical handoffs. We introduce a new access router discovery method and propose a vertical handoff algorithm accordingly. Further, we report the implementation details performed through simulations. The simulations evidence performance improvements in terms of latency and packet loss. It is also analytically shown that by enabling access router discovery method and improving link layer event services, an MN can be well prepared for handoff and perform faster movements.
[seamless vertical handoffs, access router discovery method, inter-technology roaming, mobile IPv6 vertical handoffs, Roaming, Delay, wireless communication, Wireless communication, Bandwidth, link layer mechanism, MIMO, Heterogeneous networks, Performance analysis, IP networks, protocols, Access router discovery, IEEE 802.21 project, Access protocols, Information technology, MIH, fast handoff improvement, mobile communication, Vertical handoff, layer-2 protocol adaptation method, telecommunication network routing, media independency, Performance loss, FMIPv6, Mobile computing]
On the minimization of complete test set of reversible k-CNOT circuits for Stuck-at Fault model
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we propose an algorithm that produces the complete test set (CTS) of a reversible circuit for Single stuck-at fault (SSF) and multiple-stuck-at fault (MSF) models. Our algorithm works only for an important subclass of reversible circuits - the circuits consisting of k-CNOT gates (k ges 2) though, any n-wire circuit having 0-CNOT or 1-CNOT gates can be converted to a (n + 2) wire circuit having only k-CNOT gates with k ges 2 with some additional hardware cost. Generated complete test set is not necessarily optimal, but minimizing the size of the complete test set is our key concern. Finally we provide some experimental results for the proposed method and compare it with existing methods to show how it outperforms almost all of the existing algorithms in terms of number of elements of CTS but is outperformed by some of the existing ones in terms of hardware cost.
[automatic test pattern generation, fault diagnosis, automatic test pattern generator, Multiple Stuck at Fault (MSF), Design for Testability (DFT), Minimization, Complete Test Set (CTS), logic testing, stuck-at fault model, Circuit faults, complete test set, Test Vector (TV), Circuit testing, K-CNOT Circuit, Single Stuck at Fault (SSF), design for testability, logic gates, k-CNOT gates, reversible k-CNOT circuits, Automatic Test Pattern Generator (Atpg)]
Parallel performance of domain decomposition method on distributed computing environment
2008 11th International Conference on Computer and Information Technology
None
2008
Distributed parallel computing, which uses general-purpose workstations connected by a network as a large parallel computing resource, is one of the most promising trends in parallel computing. Over the last two decades, the growth in the use of such parallel computing environment has ensured the interest on the parallel finite element method. In this work, parallel finite element method using domain decomposition technique has been adapted to the distributed parallel environment of networked workstations and the supercomputer. Using the developed code, several model problems are solved and the parallel performances are analyzed on the network of eight Pentium IV PC cluster and GP7000F workstations. Finally, very large size of problem over 11 million DOFs has been solved using 1024 SR8000 supercomputer.
[workstation clusters, domain decomposition method, supercomputer, finite element, Supercomputers, Vectors, Finite element methods, Matrix decomposition, finite element analysis, distributed parallel computing environment, Distributed computing, parallel processing, workstation cluster, Concurrent computing, resource allocation, finite element method, Parallel processing, Computer networks, Workstations, Performance analysis, parallel performance, Domain decomposition, parallel computing]
Decision Tree based Routine Generation (DRG) algorithm: A data mining advancement to generate academic routine for open credit system
2008 11th International Conference on Computer and Information Technology
None
2008
Mining data from a database using classification algorithm is not an innovative technique. This paper presents a technique namely decision tree based routine generation (DRG) algorithm to create an academic routine. The concept of open credit course registration system (after completion of pre-requisite courses any student may choose any course in any semester) makes the research more challenging and complex to accomplish. This is an NP-hard problem and hence unsolvable to satisfy all students and teachers at the same time. A level of tolerance has to be added to make the evaluation efficient and effective to achieve the goal, a conflict free routine. OLAP representation helps to classify the courses along with the proposed algorithm to eliminate some constraints. Day-based pattern, minimum Manhattan distance between courses of same teacher, minimum conflicted course distribution has been stage-managed to classify the courses. To alleviate the algorithm, decision tree based models and sequential search methods are espoused with computational results.
[data mining, Data engineering, Classification algorithms, academic routine, Data mining, Day-time slot pattern, open credit course registration system, Databases, Favorite Slot, Simulated annealing, Conflict List, Decision trees, Manhattan distance, Classification tree analysis, Crosstable, Faculty Choice, OLAP representation, OLAP, Information technology, decision tree based routine generation algorithm, Computer science, NP-hard problem, Course Color, educational courses, decision trees, System recovery, data mining advancement, computational complexity]
Analysis of antennas for the wireless communication and radar applications
2008 11th International Conference on Computer and Information Technology
None
2008
This paper signifies the distributed electromagnetic simulation for different thin wired antennas or structures mounted on earth or vehicles or space shuttle for communication purposes. A lot of electromagnetic analysis softwares are available commercially. This paper presents some simulation results of different antennas using Wire-MoM based on thin-wire approximation. The method gives frequency domain results of currents and fields which is further converted into time domain for better understanding.
[Antenna transient analysis, Wire-MoM, thin-wire approximation, electromagnetic analysis software, distributed electromagnetic simulation, Electromagnetic analysis, Distributed computing, radar applications, wireless communication, Wireless communication, Voltage, Antenna feeds, antenna feeds, mobile antennas, antenna radiation pattern, Antenna proximity factors, Antenna radiation patterns, Antenna measurements, Time domain analysis, Radar applications, radar application, mobile communication, Lightning, thin wired antenna, Electromagnetic modeling, Radar antennas, antenna measurement]
Extended algorithm for spatial characterization and discrimination rules
2008 11th International Conference on Computer and Information Technology
None
2008
Spatial data mining, i.e., mining knowledge from large amounts of spatial data, is a demanding field since huge amounts of spatial data have been collected in various applications, ranging from remote sensing to geographical information systems (GIS), computer cartography, environmental assessment and planning. The collected data far exceed people's ability to analyze it. The number and the size of spatial databases are rapidly growing which results in an increasing need for spatial data mining. In this paper, we have presented a new spatial data mining algorithm for spatial characterization and discrimination rules. For spatial data mining algorithm, it is important that class membership of a database object is not only determined by its non-spatial attributes but also by the attributes of objects in its neighborhood. We have implemented the algorithm within a general framework for spatial data mining providing a small set of database primitives on top of a commercial spatial database management system. At the end, performance analysis using a real geographic database demonstrates the effectiveness of the proposed algorithms.
[data mining, Relational databases, visual databases, spatial characterization, geographic information systems, Spatial databases, Data mining, Application software, Information technology, Remote sensing, Diseases, Information systems, Computer science, Characterization, data mining algorithm, extended algorithm, Database systems, discrimination, spatial database management system, spatial discrimination rule, spatial data mining, spatial database primitives]
BER Performance of coded MSK/Multi-channel CDMA fourth generation synchronous downlink cellular systems in the presence of MUI and FO
2008 11th International Conference on Computer and Information Technology
None
2008
This paper studies the (BER) performance of synchronous coded minimum shift keying/multi-channel code division multiple access (MSK/MC-CDMA) systems in the presence of multi-access user interference (MUI), and carrier frequency offset (FO). The theoretical effect of MUI on the BER performance was considered assuming the standard Gaussian approximation (GA) in AWGN channel. Furthermore, explicit simulations were carried out in AWGN and frequency selective Rayleigh fading channels for various parameters. The un-coded MSK/MC-CDMA system under investigation outperforms the corresponding system with BPSK modulation. Furthermore, a careful choice of coded MSK/MC-CDMA system parameters can achieve a better performance than the theoretical performance in AWGN channel for a single user and with no frequency offset or extra bandwidth requirements.
[multiaccess user interference, Bit error rate, Downlink, Frequency conversion, Multiaccess communication, minimum shift keying, fourth generation synchronous downlink cellular system, AWGN channels, Multicarrier code division multiple access, radiofrequency interference, Synchronous, Coding, Frequency shift keying, coded minimum shift keying, Multiaccess User Interference (MUI), carrier frequency offset, error statistics, approximation theory, modulation coding, MC-CDMA, Multi-Carrier, MSK, Interference, Rayleigh channels, CDMA, code division multiple access, BER, Gaussian approximation, 4G mobile communication, Frequency division multiaccess, Algorithms, frequency selective Rayleigh fading channel, Synchronous generators, and Minimum shift Keying (MSK) Modulation, AWGN channel, cellular radio, multichannel code division multiple access]
Amplitude banded technique and parallel structure for Godard and Sato algorithms of blind channel equalization
2008 11th International Conference on Computer and Information Technology
None
2008
The least-mean-squares (LMS) algorithm which updates the filter coefficients by gradient-based method is the most popular adaptive filtering one. In this paper we propose a novel amplitude banded (AB) technique with LMS on Godard (ABGodard) and Sato (ABSato) algorithms for the equalization of communication channels. The nonlinear properties of the AB technique with LMS algorithm are inherited into the ABGodard and ABSato algorithms, resulting in an improvement of equalization performance in favor of the ABSato algorithm. Mean square error (MSE) as well as bit error rate (BER) are investigated on a simple communication channel model. Observations on simulations show that ABGodard and ABSato algorithms provide better performance than the standard Godard and Sato algorithms, respectively. The parallel structure of the Sato and ABSato algorithms provides a further performance improvement. This is also observed from MSE and BER performances.
[least mean squares methods, telecommunication channels, Adaptive algorithm, Bit error rate, Adaptive filters, amplitude banded Sato algorithm, Nonlinear filters, least-mean-square algorithm, blind channel equalization, communication channel equalization, Concurrent computing, filter coefficient, amplitude banded Godard algorithm, ABSato Algorithm, Digital communication, gradient methods, Blind equalizers, adaptive filters, adaptive filtering, filtering theory, Adaptive equalizers, Least squares approximation, ABGodard Algorithm, gradient-based method, Least-Mean-Square, Communication channels, blind equalisers, Blind Equalization, Non-linear Adaptive Algorithm]
Routing algorithm for vertically stacked optical banyan networks with the time complexity O(N) using pipelined processors
2008 11th International Conference on Computer and Information Technology
None
2008
Centralized control routing algorithm for optical banyan networks on vertical stacking scheme has time complexity O(Nlog<sub>2</sub>N) for rearrangebly nonblocking structure. A distributed algorithm with O(log<sub>2</sub>N) has been recently proposed in which authors have considered an N completely connected processors to take routing decision which practically would be very difficult to implement for large N. In this paper we have proposed a distributed routing algorithm with processors work in pipelined fashion and take the routing decision in linear time. Also they do not need to be completely connected which makes it more practical for implementation.
[wavelength division multiplexing, Costs, Telecommunication traffic, stacked optical Banyan networks, Optical fiber networks, pipelined processors, Optical waveguides, Hardware, optical communication, Bipartite graph, optical crosstalk, centralized control routing algorithm, Nonblocking Switch Networks, Completely connected graph, Optical interconnections, Optical switches, time complexity, Routing, Wavelength division multiplexing, multistage interconnection networks, distributed routing algorithm, Vertically Stacked optical Switch networks, distributed algorithms, telecommunication network routing, Optical crosstalk, pipeline processing, computational complexity]
Short text compression for smart devices
2008 11th International Conference on Computer and Information Technology
None
2008
Short text compression is a great concern for data engineering and management. The rapid use of small devices especially, mobile phones and wireless sensors have turned short text compression into a demand-of-the-time. In this paper, we propose an approach of compressing short English text for smart devices. The prime objective of this proposed technique is to establish a low-complexity lossless compression scheme suitable for smart devices like cellular phones and PDAs (personal digital assistants) having small memory and relatively low processing speed. The main target is to compress short messages up to an optimal level, which requires optimal space, consumes less time and low overhead. Here we propose a new static-statistical context model to obtain the compression. We also propose an efficient probabilistic distribution based content-ranking scheme for training the statistical model. We analyze the performance of the proposed scheme as well as the other similar existing schemes with respect to compression ratio, computational complexity and compression-decompression time. The analysis shows that, the required number of operations for the proposed scheme is less than that of other existing systems. The experimental results of the implemented model gives better compression for small text files using optimum resources. The obtained compression ratio indicates a satisfactory performance with reduced memory requirements and lower complexity. The compression time is also lower because of computational simplicity. In overall analysis, the simplicity of computational requirement encompasses the compression effective and efficient.
[text analysis, data management, mobile phones, Data engineering, Mobile handsets, short English text, Statistical Model, statistical distributions, mobile computing, Engineering management, Smart Devices, compression-decompression time, Performance analysis, Personal digital assistants, demand-of-the-time, Syllable, personal digital assistants, content-ranking, data compression, Text-ranking, natural language processing, Static Coding, Short Text Compression, Computational complexity, data engineering, Intelligent sensors, wireless sensors, Wireless sensor networks, short text compression, Cellular phones, smart devices, data handling, probabilistic distribution, Context modeling, computational complexity]
Automatic Life-Logging: A novel approach to sense real-world activities by environmental sound cues and common sense
2008 11th International Conference on Computer and Information Technology
None
2008
There are many studies that collect and store life log for personal memory. The paper explains how a system can create someone's life log in an inexpensive way to share daily life events with family or friends through socialnetwork or messaging. In the modern world where people are usually busier than ever, family members are geographically distributed due to globalization of companies and humans are inundated with more information than they can process, ambient communications through mobile media or internet based communication can provide rich social connections to friends and family. People can stay connected to their loving ones ubiquitously that they care about by sharing awareness information in a passive way. For users who wish to have a persistent existence in a virtual world - to let their friends know about their current activity or to inform their caretakers - new technology is needed. Research that aims to bridge real life and the virtual worlds (e.g., Second Life, Face book etc.) to simulate virtual living or logging daily events, while challenging and promising, is currently rare. Only very recently the mapping of real-world activities to virtual worlds has been attempted by processing multiple sensors data along with inference logic for realworld activities. Detecting or inferring human activity using such simple sensor data is often inaccurate, insufficient and expensive. Hence, this paper proposes to infer human activity from environmental sound cues and common sense knowledge, which is an inexpensive alternative to other sensors (e.g., accelerometers) based approaches. Because of their ubiquity, we believe that mobile phones or hand-held devices (HHD) are ideal channels to achieve a seamless integration between the physical and virtual worlds. Therefore, the paper presents a prototype to log daily events by a mobile phone based application by inferring activities from environmental sound cues. To the best of our knowledge, this system pioneers the use of environmental sound based activity recognition in mobile computing to reflect one's real-world activity in virtual worlds.
[multiple sensors, Ambient Communication, virtual reality, Auditory Scene Analysis, Humans, mobile phones, Mobile communication, recording, Mobile handsets, virtual worlds, Sound Cues, common sense, mobile computing, social-network, Acoustic sensors, Second Life, Books, automatic life-logging, daily life events, Activity Recognition, hand-held devices, Globalization, environmental sound cues, Life Logging, Face detection, inference logic, Virtual World, Bridges, personal memory, internet based communication, social networking (online), Internet, mobile media, mobile handsets, sound based activity recognition]
Notice of Violation of IEEE Publication Principles<BR>An pproach to recognize handwritten bengali numerals for postal automation
2008 11th International Conference on Computer and Information Technology
None
2008
Notice of Violation of IEEE Publication Principles<BR><BR>"An Approach to Recognize Handwritten Bengali Numerals for Postal Automation"<BR>by Md. Saidur Rahman, G. M. Atiqur Rahaman, Asif Ahmed and G. M. Salahuddin<BR>in the Proceedings of 11th International Conference on Computer and Information Technology (ICCIT 2008, 25-27 December, 2008, Khulna, Bangladesh)<BR><BR>After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:<BR><BR>"Handwritten Bangla Numeral Recognition System and Its Application to Postal Automation"<BR>by YingWen,Yue Lu, Pengfei Shi<BR>in Pattern Recognition (Jan 2007, vol. 40, pp.99-107) Elsevier<BR><BR><br/>A recognition approach for handwritten Bengali numerals and its application for Bangladesh Postal system are presented in this paper. The approach consists of preprocessing, feature extraction and recognition. Using the canny edge detector the top left corner of the post-code box has been detected and the four handwritten numeral images have been segmented. Each image then goes through the steps of normalization, filtering and thinning. Kirsch mask has been used to get the edges through the horizontal, vertical, right and left diagonal. Principal component analysis (PCA) has been used for dimension reduction as the final feature vector consists of the four directional feature vector along with the normalized image. Then the output of the PCA is passed to a trained Support Vector Machine (SVM) to determine which class the input belongs to. Experiments demonstrate that the average recognition rate, error rate and reliability achieved by the proposed system are 92.5%, 7.5% and 92.5% respectively.
[Error analysis, Feature Extraction, Support Vector Machine, handwriting recognition, Bengali Numeral, feature extraction, image segmentation, Detectors, edge detection, image preprocessing, learning (artificial intelligence), Principal Component Analysis, postal automation, Kirsch mask, Kirsch Mask, Automation, Filtering, support vector machines, Image edge detection, filtering theory, handwritten Bengali numeral, canny edge detection, Support vector machines, Handwriting recognition, Image segmentation, support vector machine, Feature extraction, principal component analysis, image recognition, Principal component analysis]
Intelligent video surveillance for monitoring fall detection of elderly in home environments
2008 11th International Conference on Computer and Information Technology
None
2008
Video surveillance is an omnipresent topic when it comes to enhancing security and safety in the intelligent home environments. In this paper, we propose a novel method to detect various posture-based events in a typical elderly monitoring application in a home surveillance scenario. These events include normal daily life activities, abnormal behaviors and unusual events. Due to the fact that falling and its physical-psychological consequences in the elderly are a major health hazard, we monitor human activities with a particular interest to the problem of fall detection. Combination of best-fit approximated ellipse around the human body, projection histograms of the segmented silhouette and temporal changes of head position, would provide a useful cue for detection of different behaviors. Extracted feature vectors are fed to a MLP neural network for precise classification of motions and determination of fall event. Reliable recognition rate of experimental results underlines satisfactory performance of our system.
[Event detection, intelligent video surveillance, Humans, posture-based event, intelligent home environment, Security, Human Shape, Histograms, feature extraction, Home Surveillance, video surveillance, Monitoring, Posture Recognition, Fall Detection, multilayer perceptrons, Elderly Monitoring, fall detection, Hazards, home surveillance, MLP Neural Network, MLP neural network, elderly monitoring application, feature vector, Domestic safety, Senior citizens, Video surveillance, Feature extraction]
Simulation and bit error rate performance analysis of 4G OFDM systems
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper a simulation based system developed for analysis and bit error rate (BER) performance measurement of orthogonal frequency division multiplexing (OFDM) systems. This paper involves the basics of OFDM system. In this paper bit error rate is calculated and measured with respect to Signal to Noise Ratio (SNR), Doppler Effect, and guard interval. BPSK, QPSK and 16PSK are used as modulation techniques. Additive White Gaussian Noise (AWGN) is used as a communication channel. The effect of SNR, Doppler Effect, and guard intervals on OFDM signals improves the system performance.
[AWGN, Error analysis, orthogonal frequency division multiplexing, OFDM, bit error rate performance analysis, Bit error rate, communication channel, Guard interval, Doppler effect, AWGN channels, simulation-based system developement, BPSK modulation technique, Bit Error Rate (BER), Analytical models, phase shift keying, Doppler Effect, OFDM modulation, Performance analysis, error statistics, Additive White Gaussian Noise, guard interval, Binary phase shift keying, Noise measurement, 4G OFDM system, Signal-to-Noise Ratio (SNR), 4G mobile communication, signal-to-noise ratio, Signal to noise ratio, QPSK modulation technique]
Isolating significant phrases in common natural language queries to databases
2008 11th International Conference on Computer and Information Technology
None
2008
This paper describes a novel methodology for extracting significant phrases from common natural language queries to databases. We show the usability of the methodology for developing database front ends, and propose it as an acceptable alternative to developers of databases that are supposed to be subject to massive interaction by non-technical people. Here we localize the matching procedures by isolating the phrases that are important for mapping the queries to database details, and propose to use heuristics and soft computing techniques for inexact matching of the phrases.
[pattern matching, natural language processing, Natural languages, natural language interfaces, phrase extraction, Data engineering, Natural Language Query, database management systems, Information technology, Computer science, query processing, natural language query interface, phrase matching procedure, Object Identification, Databases, Query processing, Database Interface, Isolation technology, database front end development, Attribute Value Phrase, Internet, soft computing technique, Face, Usability]
Emotion-driven learning agent for setting rich presence in mobile telephony
2008 11th International Conference on Computer and Information Technology
None
2008
Presence or personal status information is going to be an integral part of human life in the near future. With the possibility of personalizing user preferences in a fine grained way, mobile presence will appeal to most users. Among all the advantages, one of the most annoying problems is to set the presence status manually each time. This paper discusses the development of an intelligent agent based presence client that will learn and make decisions on behalf of the user about his or her presence status. The decision is emotion driven and the learning depends on real world experience. The proposed system utilizes a neural network (NN) based emotion-driven agent to learn user preferences. As a NN learning algorithm, two approaches based on Differential Evolution and Reinforcement have been proposed, of which either one can be used. Rich presence status is set using a scripting language named Call Processing Language; and SIP is used for publishing the presence to others.
[ANN, DE, scripting language, call processing language, Humans, Context awareness, emotion-driven agent, learning agent, Mobile radio mobility management, neural network, rich presence, mobile computing, intelligent agent, Publishing, authoring languages, Telephony, Presence, reinforcement, learning (artificial intelligence), emotion-driven learning agent, personalizing user preferences, Artificial neural networks, context aware, Information technology, software agents, Intelligent agent, differential evolution, Neural networks, telephony, mobile telephony, SIP, Mobile computing, neural nets, CPL]
Low-cost realization of toffoli gate for the low-cost synthesis of quantum ternary logic functions
2008 11th International Conference on Computer and Information Technology
None
2008
Reversible quantum computer system is one of the best choices for future computer systems. Multiple-valued logic especially ternary logic is a good candidate for the realization of reversible quantum computer. An efficient logic synthesis mechanism is essential for the low-cost realization. Toffoli gate is an important gate for quantum logic synthesis. It is the basic element for the Galois Field Sum of Product (GFSOP) expression based logic synthesis mechanism. So, for low-cost realization of any ternary logic function, a low-cost implementation of Toffoli gate is very necessary. This paper shows a low-cost, practically realizable, and efficient realization of 3-qutrit ternary Toffoli gate by using ion-trap realizable Muthukrishnan-Stroud gate. This realization is more efficient and less costly than other realizations.
[quantum ternary logic functions, Costs, Toffoli gate, Galois field sum of product expression, Multivalued logic, minimal realization, Quantum computing, CMOS logic circuits, Logic circuits, logic functions, multivalued logic, network synthesis, quantum logic synthesis, Logic functions, circuit synthesis, Muthukrishnan-Stroud gate, low-cost synthesis, reversible quantum computer system, quantum gates, Information technology, multiple valued logic, DH-HEMTs, Network synthesis, Circuit synthesis, ternary logic, logic design]
Structural operational semantics of packages in Java
2008 11th International Conference on Computer and Information Technology
None
2008
A good understanding of programming language can be developed and understood in a machine independent manner called structural operational semantics. It is a mathematical model for specifying the meaning of a language by defining the effect of a running program in terms of its structure. The emphasis of this work is to present the structural operational semantics of several important aspects of packages in Java programming. With that we have showed how structural operational semantics is helpful for understanding the structure of packages.
[Java, package, abstraction, programming language semantics, Information technology, Programming profession, Packaging machines, Computer science, Computer languages, package structure, Program processors, Java programming, environment, Algebra, software packages, structural operational semantics, mathematical model, programming language, Mathematical model, Joining processes]
Two-Level Dictionary-Based Text Compression Scheme
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper a new dictionary and memory based text compression technique is presented called a two-level dictionary based text compression scheme. The original words in a text file are transformed into codewords having length 2 and 3 using a dictionary comprising 73680 frequently used words in English language. Among these words most frequently used words use 2 length codewords and the rest use 3 length codewords for better compression. The codewords are chosen in such way that the spaces between words in the original text file can be removed altogether recovering a substantial amount of space. Another unique feature of our compression scheme is that we have recovered unused bit of ASCII character representation from each character to save one byte per 8 ASCII characters. Lastly a back end existing compression algorithm is used to finally compress the file. We have achieved about 75% (compression ratio of 2.01 bits per input character) reduction in size using our new compression strategy with gzip and bzip2.
[data compression, text analysis, Text compression, Dictionaries, Costs, English language, natural language processing, codeword, Natural languages, Data compression, memory-based text compression, text file, Huffman code, ASCII character representation, Decoding, Information technology, Compression algorithms, two-level dictionary-based text compression, Runtime, Propagation losses, word transformation, Dictionary based compression, Mathematical model, dictionaries]
A new SS7-SIGTRAN protocol interchanger software and hardware to implement an improved distributed database based ATM network using existing IP network
2008 11th International Conference on Computer and Information Technology
None
2008
This paper presents new Signaling System No. 7 (SS7) to IP based Signaling Transport (SIGTRAN) protocol interchanger software and related hardware to implement an improved distributed database based Automated Teller Machine (ATM) network where the countrywide existing IP network can be used. Currently the ATM network is based on fully centralized database environment. This leads to varying transaction time according to the ATM and central database. Moreover, link failure between central database and ATM leads to transaction failure. To overcome this problem, an improved distributed database system for ATM network is proposed. To deploy such network, a new software and related hardware is also presented which discards the need of costly new leased line or dial up connection. Using the software and hardware, the existing IP network can be used for expanding ATM network. Also the cost and benefits of the network is discussed by allocating fragment on different sites.
[transaction processing, Protocols, Costs, automatic teller machines, Automated Teller Machine (ATM), Transaction databases, IP-based signaling transport protocol interchanger software, signaling system 7, signalling protocols, Nearest neighbor searches, Distributed Database, automated teller machine network, centralized database, transport protocols, Web and internet services, Distributed databases, SS7, distributed databases, Modems, Hardware, Database systems, SIGTRAN, IP networks, distributed database system]
Design and implementation of microprocessor based electronic voting system
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we described the design and hardware implementation of an electronic voting machine. The main aim of this project is not to design a power efficient perfect device but is to design a mother device that can be adoptable to any recent technologies. The design of the machine and its usage is as simple as an illiterate common people can easily use it. A vote casting procedure is also proposed to use the machine where we prescribed a new procedure called DEMO vote casting which is a testing mechanism to test the device before vote casting and also at the run time. The vote casting system is almost similar to traditional system, only ballot box will be replaced by the device. The machine uses voter ID to identify a valid voter and restrict multiple vote casting.
[microprocessor, Electronic voting systems, Costs, Vote, Nominations and elections, voter ID, DEMO vote casting procedure, ballot box, microprocessor chips, Security, Information technology, Election, Casting, Microprocessors, Electronic Voting System, Hardware, electronic voting system, Microprocessor, Electronic voting, government data processing, Electronic Voting Machine, Testing]
Automated digital archive for land registration and records
2008 11th International Conference on Computer and Information Technology
None
2008
Land management is an important issue for Bangladesh, where the land registration process is a major barrier to the economic growth of the country. Technology and IT can greatly facilitate the creation of a proper land management system. This paper focuses on various issues and problems related to the current system of land management. It offers an alternative management system using IT features and shows a comparison between the existing system and the proposed system. It is shown that a digitized land management system in Bangladesh can bring positive outcomes in the entire process by minimizing hassle, expenditure, time, and staff dishonesty.
[Printing, automated digital archive, Automated, Conference management, Information technology, land management system, Land, Computer science, land use planning, Technology management, Bangladesh, land registration, Engineering management, information retrieval systems, Seals, Auditory system, Writing, Marketing and sales, land records, Records]
Face recognition using Gabor Filters
2008 11th International Conference on Computer and Information Technology
None
2008
Gabor based face representation has achieved enormous success in face recognition. This paper addresses a novel algorithm for face recognition using neural networks trained by Gabor features. The system is commences on convolving some morphed images of particular face with a series of Gabor filter co-efficient at different scales and orientations. Two novel contributions of this paper are: scaling of RMS contrast, and contribution of morphing as an advancement of image recognition perfection. The neural network employed for face recognition is based on the multy layer perceptron (MLP) architecture with back-propegation algorithm and incorporates the convolution filter response of Gabor jet. The effectiveness of the algorithm has been justified over a morphed facial image database with images captured in different illumination conditions.
[Image recognition, morphed images, neural networks, Morphing, image morphing, backpropegation algorithm, Histograms, convolution filter response, Convolution, Lighting, RMS Contrast, face recognition, convolution, Gabor filters, multilayer perceptrons, Face recognition, face representation, RMS contrast, multilayer perceptron architecture, Information technology, Face Recognition, Computer science, Gabor features, Gabor jet, Neural networks, Gabor Filters, backpropagation, image representation, Frequency, neural net architecture, Gabor filter coefficient]
Meliorated approach for extracting bilingual terminology from Wikipedia
2008 11th International Conference on Computer and Information Technology
None
2008
With the demand of accurate and domain specific bilingual dictionaries, research in the field of automatic dictionary extraction has become popular. Due to lack of domain specific terminology in parallel corpora, extraction of bilingual terminology from Wikipedia (a corpus for knowledge extraction having a huge amount of articles, links within different languages, a dense link structure and a number of redirect pages) has taken up a new research in the field of bilingual dictionary creation. Our method not only analyzes interlanguage links along with redirect page titles and link text titles but also filters out inaccurate translation candidates using pattern matching. Score of each translation candidate is calculated using page parameters and then setting an appropriate threshold as compared to previous approach, which was solely, based on backward links. In our experiment, we proved the advantages of our approach compared to the traditional approach.
[Dictionaries, pattern matching, search engines, Terminology, Natural languages, Encyclopedias, information retrieval, Wikipedia, Wikipedia Mining, Data mining, Information technology, domain specific bilingual dictionaries, Computer science, bilingual terminology, Automated translation, automatic dictionary extraction, knowledge extraction, natural languages, Bilingual Dictionary, Pattern analysis, Pattern matching, dictionaries]
Using Log likelihood relation for BER analysis of QAM in space diversity
2008 11th International Conference on Computer and Information Technology
None
2008
The bit error rate (BER) performance for quadrature amplitude modulation (QAM) with different diversity combining scheme is derived for Rayleigh fading channel using Log-likelihood ratio (LLR). Three combining techniques, maximal ratio combiner (MMC), selection combining (SC), and Equal Gain Combiner (EGC) are considered for the BER analysis in this paper. The average BER performance of QAM symbol through a flat fading channel is derived from the individual bits forming the QAM symbol with MRC, SC, and EGC space diversity. The analytical and simulation of BER performance of the QAM symbol through the fading channel with MMC, SC, and EGC combiner is compared. The both analytical and simulation result shows that the probability of bit error decreases with the order of diversity for all the cases of MRC, SC, and EGC. It is also seen from the results that MRC provides better BER performance than the other two combiners (SC and EGC) for the same environment of the fading channel.
[Rayleigh Fading Channel, Quadrature amplitude modulation, Bit error rate, Transmitting antennas, log likelihood relation, Diversity, space diversity combining scheme, quadrature amplitude modulation, space communication links, Diversity Combiner, Analytical models, LLR, equal gain combiner, QAM BER analysis, selection combining, error statistics, Rayleigh flat fading channel, Fading, diversity reception, probability, Rayleigh channels, bit error rate, Land mobile radio, Computer science, Equalizers, Diversity reception, Receiving antennas, maximal ratio combiner, QAM]
Generation of random channel specifications for channel routing problem
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper we develop algorithms for generating random channel specifications of channel routing problem in VLSI design. A channel is a rectangular routing region containing two sets of fixed terminals on two of its opposite sides and the other two opposite sides (of the rectangle) are open ends, may or may not contain any terminal of a net but the terminal position is not fixed before a routing solution is computed. Most of the problems in two-, three-, and multi-layer channel routing are beyond polynomial time computable. Hence for each of these problems it is unlikely to design a polynomial time deterministic algorithm. Developing heuristic algorithm might be a probable way out that hopefully provides good solutions for most of the instances occur in practice. Novelty of a heuristic algorithm is judged better if it works for a variety of large number of randomly generated instances of the problem. In fact, convergence of results of a heuristic algorithm is well established when the algorithm of a problem is executed for a huge number of randomly generated similar instances and the final result is computed making an average on all of them.
[Algorithm design and analysis, Heuristic algorithms, Channel specification, Crosstalk, Channel routing problem, Very large scale integration, polynomial time deterministic algorithm, heuristic algorithm, Cities and towns, rectangular routing region, multi-layer channel routing, Polynomials, Random number generation, General channel instance, network routing, polynomials, VLSI, Routing, channel routing problem, VLSI design, Information technology, Algorithm, Random generation, Computer science, NP-hardness, random channel specifications, Simple channel instance]
Vision system for human-robot interface
2008 11th International Conference on Computer and Information Technology
None
2008
A vision system has been developed for human-robot symbiosis. The two novel contributions of this paper are 1) Detection of human faces and 2) Localizing the eye. The method is based on visual attributes of human skin colors and geometrical analysis of face skeleton. This paper introduces a spatial domain filtering method named dasiafuzzily skewed filterpsila which incorporates fuzzy rules for deciding the gray level of pixels in the image in their neighborhoods and takes advantages of both the median and averaging filters. The effectiveness of the method has been justified over human-robot interaction system.
[Face Detection, Humans, vision system, human-robot interaction system, Filters, Image color analysis, face recognition, Skeleton, image colour analysis, Skin Color Segmentation, Symbiosis, human-robot interaction, geometrical face skeleton analysis, Filtering, Eye Localization, filtering theory, robot vision, human skin colors, AIBO, Face detection, human face detection, fuzzily skewed filter, Human-Machine Interface, Machine vision, Skin, Pixel]
A crosstalk free routing algorithm of Generalized Recursive Non-blocking Network
2008 11th International Conference on Computer and Information Technology
None
2008
Crosstalk is an intrinsic drawback of an optical network, and avoiding crosstalk is important for making fruitful application of optical switching network. In this paper, we apply the well-known semi-permutation technique on generalized recursive network (GRN) to make it crosstalk free. GRN has reasonable path independent signal loss and crosstalk. Using the methodology presented in this paper, that reasonable crosstalk is avoided. For the GRN network, we show that any semi-permutation is realizable in one pass and any permutation is realizable in two passes under the constraint of avoiding crosstalk.
[GRN, optical switches, Optical switches, Multiprocessor interconnection networks, Crosstalk, Optical fiber networks, crosstalk free routing algorithm, Routing, Optical network units, Information technology, Optical losses, optical switching network, telecommunication network routing, Distributed control, optical fibre networks, Crosstalk-free routing algorithm, Optical crosstalk, generalized recursive nonblocking network, Computer networks, optical communication, Semi-permutation, optical crosstalk, semipermutation technique]
Learning intrusion detection based on adaptive bayesian algorithm
2008 11th International Conference on Computer and Information Technology
None
2008
Recent intrusion detection have emerged an important technique for information security systems. Its very important that the security mechanisms for an information system should be designed to prevent unauthorized access of system resources and data. Last few years, many intelligent learning techniques of machine learning applied to the large volumes of complex and dynamic audit data for the construction of efficient intrusion detection systems (IDS). This paper presents, theoretical overview of intrusion detection and a new approach for intrusion detection based on adaptive Bayesian algorithm. This algorithm correctly classify different types of attack of KDD99 benchmark intrusion detection dataset with high detection accuracy in short response time. The experimental result also shows that, this algorithm maximize the detection rate (DR) and minimized the false positive rate (FPR) for intrusion detection.
[Machine learning algorithms, false positive rate, intelligent learning, Data security, adaptive Bayesian algorithm, information security system, detection rate, intrusion detection, machine learning, classification, Information technology, Delay, High-speed networks, security of data, Bayesian algorithm, Bayesian methods, Intrusion detection, Information security, Machine learning, Bayes methods, learning (artificial intelligence), Computer security]
Comparing FLV and MPEG4 (H.264/AVC) multimedia file format with wireless network parameters
2008 11th International Conference on Computer and Information Technology
None
2008
Accommodating multimedia resources in the wireless media is a long cherished service. In this paper, we compared the FLV and MPEG4 (H.264/AVC) format of multimedia content using frame rates, sizes and byte rates. Thus we showed that MPEG4 performs better than FLV.
[radio networks, MPEG 4 Standard, macroblock, FLV, CoQV, Wireless networks, frame rate, Bit rate, Bandwidth, Video compression, pixel, multimedia communication, MBMS, Video sequences, wireless network parameter, Transcoding, Encoding, multimedia file format, video coding, H.264/AVC, MPEG4, screenshare, Automatic voltage control, Iterative algorithms, byte rate]
Lower bound on blocking probability for vertically stacked optical banyan networks with given crosstalk constraint
2008 11th International Conference on Computer and Information Technology
None
2008
Vertical stacking of multiple copies of an optical banyan network is a novel scheme for building nonblocking optical switching networks. The resulting network, namely vertically stacked optical banyan (VSOB) network, preserves all the properties of the banyan network, but increases the hardware cost significantly under first order crosstalk-free constraint. Therefore, blocking behavior analysis could be an effective approach to studying network performance, and finding a graceful compromise between hardware costs and blocking probability with different degree of crosstalk constraint. However, lower bound on number of planes for such networks with zero crosstalk constraint has been determined previously. In this paper, we present the simulation results for lower bound on blocking probability with given degree of crosstalk. The results show that blocking probability decreases to low value if we allow some CSEs to the network. The simulation results presented in this paper can guide network designer in finding a tradeoff among the blocking probability, and the degree of crosstalk of VSOB networks.
[blocking probability, nonblocking optical switching network, hardware cost, Lower bound, Costs, Optical switches, Stacking, Buildings, probability, Optical fiber networks, multistage interconnection networks, Optical waveguides, telecommunication switching, Mesh networks, network performance, crosstalk, crosstalk constraint, Optical attenuators, VSOB network, Optical crosstalk, vertically stacked optical banyan network, Hardware, optical communication, blocking behavior analysis]
Performance of fast routing algorithm for Vertically Stacked Optical Banyan networks with link failures and given crosstalk constraint
2008 11th International Conference on Computer and Information Technology
None
2008
For faster connection establishment, previously proposed Pruned Vertically Stacked Optical Banyan (P-VSOB) networks used plane fixed routing (PFR) algorithm, and has O(log<sub>2</sub>N) routing complexity. Blocking probability has also been analyzed for these kinds of networks with given amount of crosstalk. In EP-VSOB networks, a few regular banyan planes are added with the PVSOB networks. Necessary routing algorithm, namely, PFR_LS and PFR_RS show that this switching network can reduce the blocking probability to very low value while keeping the hardware cost almost the same as that of PVSOB networks. Both these algorithm also have the time complexity O(log<sub>2</sub>N). This paper deals with the blocking behavior of EP-VSOB (Extended pruned VSOB) networks having link-failures and allows crosstalk constraint. Our simulation results show that crosstalk adds a new dimension to the performance analysis of practical EP-VSOB networks where some links are failed. The results show that if we allow some CSEs to the network then blocking probability decreases to low value for both PFR_LS and PFR_RS algorithm. The results also show that the blocking probability of the EP-VSOB networks does not always increase with the increase of link-failures; blocking probability decreases for certain range of link-failures, and then increases again.
[blocking probability, Costs, optical switches, Optical fiber networks, WDM networks, banyan planes, pruned vertically stacked optical banyan networks, communication complexity, link-failure, fast routing algorithm, routing complexity, crosstalk constraint, Hardware, blocking behavior, Performance analysis, optical crosstalk, switching network, EP-VSOB network, Optical switches, time complexity, Routing, Wavelength division multiplexing, multistage interconnection networks, crosstalk, telecommunication network routing, plane fixed routing, optical fibre networks, Optical crosstalk, link failures, High speed optical techniques, performance analysis]
Direction of Arrival algorithms for adaptive beamforming in next generation wireless systems
2008 11th International Conference on Computer and Information Technology
None
2008
Different beamforming algorithms like side-lobe cancellors, linearly constrained minimum variance (LCMV), least mean squares (LMS), recursive LMS, and direction of arrival (DOA) exist in literature. Among the direction of arrival (DOA) algorithms, MUSIC and ESPRIT play the most important role. These two algorithms were implemented and their performances were compared. The algorithms were simulated for different signal levels and the DOAs were computed for use in next generation wireless. ESPRIT was found to be a better DOA technique for uncorrelated source used in beamforming.
[Array signal processing, next generation wireless communication system, adaptive beamforming, ESPRIT algorithm, Interference, Vectors, Multiple signal classification, direction-of-arrival estimation, MUSIC algorithm, Least squares approximation, uncorrelated source beamforming, Sensor arrays, signal classification, DOA estimation, Beamforming, Receiving antennas, direction-of-arrival algorithm, Eigenvalues, radiocommunication, Linear antenna arrays, Direction of arrival estimation, Antenna array, Antenna arrays, array signal processing]
A combined local-global optical flow approach for cranial ultrasonogram image sequence analysis
2008 11th International Conference on Computer and Information Technology
None
2008
Tissue-motions can represent the strength of artery pulsation of a newborn baby for pediatrics diagnosis. In this paper, we propose a novel method to estimate the tissue-motion quantitatively by combining local and global motion estimation methods, namely combined local-global (CLG) optical flow technique in cranial ultrasonogram of newborn babies. Tissue motions are estimated in different coronal sections successfully. Further, we analyze the time variant tissue-motion by using discrete Fourier transform. We also observed the pulsation in the time variant motion images and strong pulsation is observed in the harmonic frequency of tissue-motion that has a relation to the heartbeat frequency of a newborn baby.
[Pediatrics, Tissue-motion, biomedical ultrasonics, CLG optical flow, Cranial, Arteries, paediatrics, newborn baby head, Ultrasonic imaging, combined local-global optical flow approach, Heart beat, motion estimation, newborn baby, tissue-motion estimation, image sequences, medical image processing, artery pulsation, harmonic frequency, discrete Fourier transforms, Motion estimation, cranial ultrasonogram image sequence analysis, Image sequence analysis, biological tissues, cardiology, discrete Fourier transform, Image motion analysis, pediatrics diagnosis, heartbeat frequency, Adaptive optics, Optical noise, cranial ultrasonogram]
The channel capacity of a GSM frequency hopping network with intelligent underlay-overlay
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper a complete step by step method is described to increase the capacity of Global System for Mobile Communication (GSM) using the Intelligent Underlay Overlay(IUO) with frequency hopping and Discontinuous Transmission (DTX). The IUO splits the available frequencies into two layers, super and regular. Super layer frequencies have tighter reuse factor and can be accessed by only mobile stations (MS) having good C/I ratio while the regular frequencies can be used by MSs throughout the cell. The most promising work of this paper is that C/I ratio (hence reuse factor) is obtained by considering the first tier co-channel cells are also using IUO network structure. Analysis is performed by taking some default parameters into account shows that capacity enhancement up to 31 percent can be obtained with respect to a normal frequency hopping network.
[Quality of service, Telecommunication traffic, channel capacity, Intelligent Underlay Overlay (IUO), Grade of Service (GOS), Carrier to Interference Ratio (C/I), Intelligent networks, intelligent underlay-overlay network structure, discontinuous transmission, normal frequency hopping network, Spread spectrum communication, Traffic control, Computer networks, intelligent underlay overlay, GSM, Base stations, Channel capacity, Global System for Mobile Communication (GSM), channel capacity enhancement, GSM frequency hopping network, Global System for Mobile Communication, mobile station, Discontinuous Transmission (DTX), carrier to interference ratio, super layer frequency, Frequency, cellular radio, tier co-channel cell]
Genetic Algorithm based synthesis of ternary Reversible/Quantum circuit
2008 11th International Conference on Computer and Information Technology
None
2008
Reversible/quantum circuits are believed to be one of the future computer technologies. In this paper, a genetic algorithm (GA) based synthesis of ternary reversible/quantum circuits using Muthukrishnan-Stroud gates is presented. The circuit generated by GA may contain redundant gates. We have used post GA reduction to eliminate these redundant gates. We have experimented with ternary half-adder circuit. The proposed GA converges for many combinations of crossover and mutation.
[Genetic mutations, ternary reversible/quantum circuit synthesis, Genetic algorithms, genetic algorithm, redundant gate, Quantum computing, Logic circuits, Muthukrishnan-Stroud gate, Reversible logic, quantum gates, computer technology, genetic algorithms, Information technology, Computer science, quantum circuit, ternary half-adder circuit, DH-HEMTs, post GA reduction, Genetic engineering, half-adder, Circuit synthesis, Power dissipation, ternary logic, ternary quantum logic, logic design]
Analytical performance evaluation of space time coded MIMO OFDM systems impaired by timing jitter
2008 11th International Conference on Computer and Information Technology
None
2008
An analytical approach is presented to determine the impact of time selective fading, timing jitter and AWGN on the bit error rate performance of an OFDM system with DQPSK and QPSK modulation. The expression for the conditional BER conditioned on a given timing error and fading, is derived and the average BER is evaluated. The BER performance results are evaluated for different values of fading and jitter variances. The performance of the OFDM system in Rayleigh and Rician fading channels is also compared. Analytical approach is also developed to evaluate the BER performance of a quasi-orthogonal space-time block coded (STBC) OFDM system having multiple transmitting and single receiving antennas. The analysis is extended for a MIMO-OFDM system using the ldquoselection methodrdquo for combining multiple receiving antennas, which offers significant improvement in the system performance. The effects of increase in number of OFDM subcarriers and increase in Doppler frequency are also investigated.
[Rayleigh fading channel, Convolutional coding, AWGN, intersymbol interference, OFDM, STBC, Bit error rate, OFDM subcarrier, quadrature phase shift keying, DQPSK modulation, Doppler frequency, receiving antennas, block codes, MIMO, OFDM modulation, Performance analysis, Rician fading channel, MIMO communication, error statistics, Timing jitter, Fading, time selective fading, timing jitter, quasiorthogonal space-time block coded OFDM, Rayleigh channels, bit error rate, space-time codes, Space time codes, Receiving antennas, Rician channels, space time coded MIMO-OFDM system, Rayleigh fading]
Cell database based time specific resource Reservation scheme
2008 11th International Conference on Computer and Information Technology
None
2008
Increasing cellular network offers services with diverse quality-of-service supports users to access telephony, paging, instant messaging, and trivial Web-browsing on the same device. The mobility of communication devices and channel imperfection makes the service offerings much more challenging with limited resources. Resource reservation plays a very important role in cellular communication in terms of availability of services. There are various schemes present to manage the resources in an efficient way. In this paper, we have tried to analysis several resource management schemes and their drawbacks. We have proposed a time dependent resource reservation scheme which has significant advantages over other schemes.
[Mobility Management, Call admission control, mobility management (mobile radio), Call dropping probability (CDP), Cell Visiting Probability (CVP), database management systems, mobile computing, resource allocation, Bandwidth, time specific resource reservation scheme, channel imperfection, Call Blocking Probability (CBP), Computer networks, wireless channels, Call Admission Control (CAC), cellular network database, quality-of-service, Spatial databases, resource management scheme, quality of service, handoff, cellular communication, Information technology, Equations, Computer science, Land mobile radio cellular systems, Admission control, Resource management, cellular radio]
New methodologies for high level modeling and synthesis of low density parity check decoders
2008 11th International Conference on Computer and Information Technology
None
2008
Low density parity check (LDPC) codes are the error-correcting codes which offer huge advantages in terms of coding gain, throughput and power dissipation. Error correction algorithms are often implemented in hardware in order to ensure fast processing. The hardware implementation of LDPC decoders using traditional hardware description language (HDL) based approach is a complex and time consuming task. This paper investigates new high level approaches to design and synthesis of LDPC decoders using a combination of high level modelling tools. It compares the high level design approaches to traditional HDL-based approach. The results presented in this paper provide some useful insight into the high level design approaches, their efficiencies and possible future directions with a view to develop an efficient design and modelling framework for hardware implementation of complex LDPC decoders.
[error correction codes, error correcting codes, FPGA, parity check codes, Throughput, hardware description languages, Iterative decoding, Delay, digital systems, low density parity check decoders, coding gain, hardware description language, digital communication, Turbo codes, Parity check codes, Error correction coding, Error correction codes, Power dissipation, Error correction, Mathematical model, Hardware design languages, logic design, power dissipation]
Efficient Generation of Combinatorial Families
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we propose a unifying framework for efficient generation of combinatorial objects by giving recursive definition of an abstract combinatorial class. The definition can be instantiated to an array of specific combinatorial classes by specifying the framework parameters appropriately. Our framework defines a rooted spanning tree over a representative graph of that abstract combinatorial class where exhaustive generation can be done by a depth-first traversal. As an illustration, we show an instantiation for the combinatorial class of combinations as well as give a novel constant time generation algorithm for them.
[Combinatorial Generation, trees (mathematics), depth-first traversal, Combination Combinatorial Algorithm, tree searching, combinatorial object generation, representative graph, Information technology, abstract combinatorial class, recursive definition, Computer science, Tree graphs, Binary trees, Concrete, Iterative algorithms, Reflective binary codes, Unifying Framework, rooted spanning tree, Combinatorial Family]
High-speed distance measurement between moving vehicles with NIR-LED markers
2008 11th International Conference on Computer and Information Technology
None
2008
An image-based technique with high measurement rate has been proposed to measure the distance between two moving vehicles. By combining several near-infrared (NIR) LED markers mounted on the forward vehicle with an NIR-filtered CMOS camera mounted on the backward vehicle, one can clearly recognize the marker shape and positions without any adjusting the focus lens of camera, and hence he can extremely reduce the time of digital preprocessing such as noise elimination and marker segmentation. The distance may be easily calculated from the geometrical transformation of the marker positions obtained in sub-pixel resolution from the gravity center of the pixel value. The image-based technique proposed here is actually examined in the vehicles moving on linear and curved tracks. It is experimentally confirmed that the measurement rate attains to 1000 frames/sec in the windowed image of 752 times 34 pixels, with a small processor with the clock of 600 MHz, including the capture time of the image, the calculation time of the gravity center, and the geometrical transformation time. The measurement error is less than 1 % for the distance range of 0.5 to 3 m.
[Shape, Noise reduction, CMOS image sensors, computational geometry, near-infrared light emitting diode, Digital cameras, object detection, NIR-LED marker, Vehicles, geometrical transformation, cameras, high-speed distance measurement, distance measurement, road vehicles, sub-pixel resolution, motion control, Focusing, shape recognition, marker shape recognition, image resolution, Gravity, moving vehicles, image processing, filtering theory, Light emitting diodes, traffic engineering computing, light emitting diodes, image motion analysis, infrared imaging, NIR-filtered CMOS camera, sub-pixel image resolution, Distance measurement, moving vehicle detection, Lenses]
An Algorithm for Segmenting Modifiers from Bangla Text
2008 11th International Conference on Computer and Information Technology
None
2008
Script segmentation is an essential and preprocessing task for any OCR system. Bangla is one of the most popular scripts in the world. Since segmentation effects the recognition process, accurate and proper segmentation is necessary to implement Bangla OCR. Many works have been done for both handwritten and printed Bangla text. This paper presents the segmentation process of different Bangla modifiers from printed Bangla words.
[Optical Character Recognition, handwritten character recognition, Bangla Modifier, Military computing, Piecewise linear techniques, handwritten Bangla text, Segmentation, document image processing, Optical character recognition software, Character recognition, Floods, Information technology, optical character recognition, image segmentation, printed Bangla words, script segmentation modifier algorithm]
Density based clustering technique for efficient data mining
2008 11th International Conference on Computer and Information Technology
None
2008
Clustering analysis is an important function of data mining. There are various clustering methods in data mining. Based on these methods various clustering algorithms are developed. A recent approach for clustering analysis is based on ldquoswarm intelligencerdquo. Based on this ldquoswarm intelligencerdquo an algorithm was proposed named ldquoant-cluster algorithmrdquo. However, existing ldquoant clusteringrdquo algorithm has a limitation in finding the value of two constant K<sub>1</sub> and K<sub>2</sub>, which is user defined., for computing the value of the picking up probability Pp and dropping probability P<sub>d</sub>. In this paper our approach is to gain the value of Pp and P<sub>d</sub> without giving the user defined value of K<sub>1</sub> and K<sub>2</sub>. We also intend to retain the Pp and P<sub>d</sub> in between 0 to 1 in order to get optimized result.
[Clustering methods, ant-cluster algorithm, data mining, probability, Credit cards, Data Mining, Partitioning algorithms, Data mining, Particle swarm optimization, Information technology, Density, Swarm Intelligence, Computer science, Image databases, pattern clustering, Clustering algorithms, Signal processing algorithms, density based clustering technique, swarm intelligence, Ant Clustering methods, computational complexity]
Crowd behaviour monitoring on the escalator exits
2008 11th International Conference on Computer and Information Technology
None
2008
This paper presents an approach to detect an abnormal situation in a crowd scene. The proposed approach estimates sudden changes and abnormal motion variations in a set of interest points. The number of tracked points of interest is reduced by using a mask that corresponds to the hot areas of the built motion heat map. Optical flow technique tracks the points of interest. There are sufficient variations in the optical flow patterns in a crowd scene when there are cases those showing abnormal situations. Statistical treatment of optical flow information has been thresholded. To demonstrate the interest of this approach, we present the results based on the detection of collapsing events in real videos of airport escalator exits.
[Event detection, Motion heat map, abnormal motion variation estimation, Entropy, Videos, crowd behaviour monitoring, crowd scene, Points of interest, Escalator exits, motion estimation, abnormal situation detection, Monitoring, Injuries, image sequences, Electric breakdown, Breakdown situation, Motion estimation, Airports, optical flow, monitoring, escalator exit, Image motion analysis, Layout, Hidden Markov models, motion heat map, statistical treatment, behavioural sciences computing, statistical analysis, Optical flow]
Development of a highly optimized Preemptive Real Time Operating System (pRTOS)
2008 11th International Conference on Computer and Information Technology
None
2008
A real-time operating system (RTOS) is software which ensures that time critical events are processed as efficiently as possible. In this paper, an attempt has been taken to develop a real time operating system, named preemptive real time operating system (pRTOS), in which all of the important issues regarding to a real time application have been considered. In this pRTOS, strictly preemptive scheduling algorithm has been used. This scheduling policy makes sure that important tasks are handled first and the less important later. The Bitmap technique has been used to find out the highest priority task from the unsorted ready list. The complexity of this technique for selecting the highest priority task is O(1), which is much faster than the linear search technique having complexity of O(n). This pRTOS can support 64 priority levels ranges from 0 to 63. In addition with this, it is a highly configurable RTOS. Moreover, it can be adopted in a board range of hardware platform, say, Intel x86, MIPS, Hitachi SH, Power PC and Strong ARM processors. This RTOS has been tested on Intel x86 and from the obtained result, it has been found that our developed pRTOS can be used for various application, say, for automated industrial systems, control-systems, high-tech electronics/electrical products and home applications.
[Real time systems, RTOS, Job shop scheduling, preemptive scheduling algorithm, linear search technique, preemptive real time operating system, Priority Resolution, Electronic equipment testing, Scheduling, Application software, Scheduling algorithm, Electrical products industry, Operating systems, Automatic testing, scheduling, Software systems, operating systems (computers), Hardware, Context Switching, real-time operating system, Preemptive Real Time Operating System (pRTOS), computational complexity, Task Synchronization]
SEI and SHI representations for human movement recognition
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we propose spatio-temporal silhouette representations, called silhouette energy image (SEI) and silhouette history image (SHI) to characterize motion and shape properties for recognition of human movements such as human actions, activities in daily life. The SEI and SHI are constructed by using the silhouette image sequence of an action. The span or difference of the end time and start time is used to make the SHI. For addressing the human shape variability, we used the variation of the anthropometry of the person. We extract the features based on geometric shape moments. We tested our approach successfully in the indoor and outdoor environment. Our experimental results show that the proposed method of human action recognition is robust, flexible and efficient.
[Image recognition, Shape, silhouette history image, Humans, Anthropometry, anthropometry, Image sequences, History, spatio-temporal silhouette representation, feature extraction, human shape variation, Robustness, shape recognition, silhouette image sequence, image sequences, Testing, human action recognition, silhouette energy image, human movement recognition, Character recognition, spatiotemporal phenomena, shape variability, image motion analysis, characterize motion, geometric shape moment, Feature extraction, geometry]
Design and construction of Direct Sequence Spread Spectrum CDMA transmitter and receiver
2008 11th International Conference on Computer and Information Technology
None
2008
Spread spectrum communication techniques have been widely accepted in mobile and wireless communications. They have very beneficial and tempting features, like Antijam, security, and multiple accesses. It is the purpose of this paper to describe the features of spread spectrum systems. The emphasis will be on the direct sequence spread spectrum (DS-SS) scheme, pseudo noise signals (PN), modulators and demodulators, hardware implementation and illustrate some of the DS-SS system features.
[PN Code, demodulator, spread spectrum communication, Multiaccess communication, Time division multiple access, Transmitters, CDMA transmitter, Modulation, Spread spectrum communication, Bandwidth, pseudo noise signal, DS-SS system, Interference, demodulation, CDMA, Decoding, code division multiple access, radio receivers, Equations, Signal processing, radio transmitters, CDMA receiver, DS-SS, Demodulation, direct sequence spread spectrum]
Comparative performance analysis of DSR and AODV protocol based on mobility factor
2008 11th International Conference on Computer and Information Technology
None
2008
Ad hoc wireless networks are decentralized in nature and hence routing is a central challenge in this type of network. In this paper we describe a simulation model to simulate the routing protocols for wireless ad hoc networks and analyze the results from the simulation for performance evaluation. The simulation model incorporates the number of parameter values with changing environmental factors. Specially we have chosen the dynamic source routing (DSR) and ad-hoc on demand distance vector routing (AODV) protocol to be implemented in the simulation software with that model. We carried out the extensive simulations and we conclude with the comparative statistical analysis of those two protocols based on mobility factor.
[Measurement, mobile radio, Wireless ad-hoc, DSR, Computational modeling, mobility factor, dynamic source routing protocol, ad-hoc on demand distance vector routing protocol, Environmental factors, Mobile ad hoc networks, Analytical models, routing, MANET, Network topology, Transmitters, wireless ad hoc network, routing protocols, simulation model, Routing protocols, Performance analysis, ad hoc networks, AODV, Propagation delay]
Performance comparison of IPv4 and IPv6 on various windows operating systems
2008 11th International Conference on Computer and Information Technology
None
2008
Internet is a ubiquitous part of businesses and individuals worldwide. With its popularity on an incline, operating system vendors are developing end-systems that support the new version of Internet Protocol (IPv6) that eventually will replace IPv4. The new version resolves issues that IPv4 has and takes the Internet into the 21st century. Performance of the IP stack and how it associates with operating systems is critical to the efficiency of all network related activities on any computing infrastructure. Hence it is essential to evaluate performance of IP stack with different operating systems. In this paper, two Microsoft operating systems namely, Windows XP and Windows Server 2003 are configured with the two versions of IP and empirically evaluated for performance difference. The experimental results demonstrate that theoretical and practical performance values are different. It also shows that network performance depends not only on IP version and traffic type, but also on the choice of the operating system.
[Pervasive computing, IP stack, Microsoft operating system, Protocols, IPv6, Telecommunication traffic, IPv4, Information technology, Windows operating system, Degradation, Network servers, Windows server 2003, network performance, Operating systems, transport protocols, operating systems (computers), Computer networks, Internet, Internet Protocol, Payloads]
An empirical framework for translating of Phrasal verbs of English sentence into Bangla
2008 11th International Conference on Computer and Information Technology
None
2008
Generally phrasal verbs comprise a verb followed by a preposition that is commonly occurring feature in English. Each of the phrasal verbs acquires absolutely different meanings in different contexts. Having highly context dependent meanings, phrasal verbs may be disambiguated only by devising a technique involving utilization of semantic information pertaining to the context. This paper presented a framework for translating the phrasal verbs in English sentence into its equivalent Bangla. Semantics is necessary in the field of natural language processing to detect the meaning of the individual words in the sentence. This paper also describes different types of semantic features that perform semantic based disambiguation of phrasal verbs. A translation engine works to find out the exact Bangla meaning of the phrasal verb from different English sentences. We have tested our model for different type of English sentences containing phrasal verbs that are taken from the English newspaper and English textbook. We got successful results for most of the test cases.
[phrasal verb translation, Phrasal Verb, natural language processing, Natural languages, Bangla, Translation Engine, Example Base, Character recognition, Information technology, Engines, empirical framework, Leg, Computer science, English, Parse Tree, Speech recognition, Ear, context dependent meaning, Natural language processing, Semantic Features, Testing, language translation, semantic based disambiguation]
Generation of mountain ranges by modifying a controlled terrain generation approach
2008 11th International Conference on Computer and Information Technology
None
2008
The terrain generation approach proposed in dasiaParametrically controlled terrain generation, (Proceedings of GRAPHITE 2007)psila focuses on generation of terrains having mountains of prespecified heights and spreads of base regions and peaks nearly at prespecified locations. However, inability to produce mountain ranges is a shortcoming of the algorithm. In this paper, we present a modification to the original algorithm that will enable generation of whole mountain ranges while preserving the similar parametric control over the resulting terrain. Experimental results of our algorithm have also been compared with results of existing algorithms.
[Geography, terrain mapping, Military computing, Shape, Physically based modeling, Geology, Data structures, Application software, Information technology, controlled terrain generation approach, Computer science, procedural modeling, mountain ranges generation, Computer graphics, terrain generation, Rendering (computer graphics), rendering (computer graphics), parametric control]
File sharing in advanced collaborating environment: Node registration &#x00026; cache optimization techniques
2008 11th International Conference on Computer and Information Technology
None
2008
A major challenge in a collaborative environment is to device an efficient file sharing and adaptation mechanism. Through this, the users at different nodes can achieve the highest degree of collaboration. Some authors have already proposed file sharing and adaptation framework. As per their proposed framework, users are allowed to share adapted files among them, invoking their file sharing and adaptation service built on the top of advanced collaborating environment (ACE). For file adaptation, a hybrid approach has been mentioned for adapting files which considers users preferences as well as user's device capabilities. The goal of this adaptation approach is to provide the best possible adaptation scheme and convert the file according to the user's preferences and device capabilities. In this paper, we propose some new features for file sharing and adaptation framework to have faster and more efficient and meaningful collaboration among users in advanced collaborating environment. We are proposing a Key based registration system as well as an algorithm for key generation to automate the registration process for slave ACE nodes. We also propose an automated mechanism to distinguish trusted (registered) Slave ACE nodes, which are privileged for p2p file sharing under certain conditions. This approach will completely eliminate the manual intervention in the purposive node selection process. Moreover there would be a network traffic monitoring process which will radically reduce the network overhead to the master ACE node and introduce faster network collaboration in a distributive manner. Consequently, the approach leads towards better collaboration among the users of ACE.
[node registration, P2P File Sharing, peer-to-peer computing, Peer to peer computing, file adaptation service, Telecommunication traffic, Master-slave, File servers, International collaboration, cache storage, Information technology, P2P file sharing, Computer science, advanced collaborating environment, Node Registration, Authentication, Collaboration, groupware, Cache Optimization, Collaborative work, Liquid crystal displays, cache optimization technique, key based registration system]
A machine vision based automatic system for real time recognition and sorting of Bangladeshi bank notes.
2008 11th International Conference on Computer and Information Technology
None
2008
This paper presents an efficient machine vision algorithm for real time image analysis and recognition of different features of Bangladeshi bank notes by using an automatic banknotes sorting system. The features recognized by this algorithm include denominations, orientations and sides of the bank notes. In a mechatronic system the Bangladeshi bank notes are fed together. The system draws the notes one by one and gets their images using a CCD sensor from a specific side. Then the system determines the denomination, orientation and side of the bank notes by analyzing the images grabbed by the CCD sensor. The average recognition speed is 8 to 9 bank notes per second and the rate of success is 100% for the banknotes having no extremely fatal damage.
[Real time systems, object recognition, Image recognition, Image processing, mechatronic system, Sensor systems, Simplified Bi-directional associative memory, Sensor arrays, machine vision, CCD sensor, CCD image sensors, Charge-coupled image sensors, Charge coupled devices, real time image analysis, automatic banknotes sorting system, real time Bangladeshi bank note recognition, Sorting, Image analysis, Machine vision, Neural networks, Signal processing algorithms, computer vision, Banknotes sorting, bank data processing, image recognition]
An HPSG analysis of Arabic passive
2008 11th International Conference on Computer and Information Technology
None
2008
In spite of being a successful syntactic theory in many respects, head-driven phrase structure grammar (HPSG) has inadequate coverage for morphological constructions, especially for nonconcatenative morphology, which is prominent in the Semitic languages such as Arabic, Hebrew etc. Among various syntactic and semantic phenomenon, passive constructions draws attention of many researchers in theoretical linguistics due to their diversity. Arabic exhibits lexical passives with nonconcatenative morphology. In this paper, we extend the HPSG framework to support the nonconcatenative construction of Arabic passives along with the necessary lexical rules and type hierarchy.
[theoretical linguistics, computational linguistics, Constraint-based Grammar, lexical rules, Head-driven Phrase Structure Grammar, Information analysis, Passive Construction, Arabic passive, mathematical morphology, Plugs, Nonconcatenative Morphology, Arabic Morphology, Natural languages, head-driven phrase structure grammar, Helium, Information technology, type hierarchy, nonconcatenative morphology, Computer science, grammars, Morphology, Writing, Packaging, Particle measurements, HPSG analysis]
Cluster analysis of microarray data of Shewanella oneidensis
2008 11th International Conference on Computer and Information Technology
None
2008
This paper presents a comprehensive analysis of a novel temporal dataset of Shewanella oneidensis. Here we propose to cluster the temporal gene expression data of Shewanella oneidensis to define its molecular response at different time intervals following acidic pH and basic pH exposure and to find a relation of these temporal data at different environmental conditions. A mapping between those clusters is also defined to describe the interaction and reaction of those genes in different time in two separate conditions. Several conceptually variant clustering algorithms are applied to find the actual structure within data and the results of clustering algorithms are evaluated by several robust internal indices.
[microarray data, Shewanella oneidensis, temporal data, Information analysis, genetics, Clustering algorithms, acidic pH exposure, Genetics, gene interaction, cluster analysis, Bioinformatics, Data analysis, data analysis, temporal gene expression data, molecular response, Gene expression, Clustering, Extracellular, Information technology, Stress, basic pH exposure, gene reaction, pattern clustering, validation techniques, microorganisms, biochemistry, bioinformatics, Time factors, molecular biophysics, statistical analysis]
Smallest and some new equiprojective polyhedra
2008 11th International Conference on Computer and Information Technology
None
2008
A convex polyhedron P is k-equiprojective if for all of its orthogonal projections, except those parallel to the faces of P, the number of vertices in the shadow boundary is k. Finding an algorithm to construct all equiprojective polyhedra is an open problem first posed in 1968. In this paper we give lower bounds on the value of k and the size of an equiprojective polyhedron. We prove that there is no 3- or 4-equiprojective polyhedra and a triangular prism is the only 5-equiprojective polyhedron. We also discover some new equiprojective polyhedra.
[Art, Service oriented architecture, computational geometry, lower bound, History, Information technology, Algorithm, equiprojective polyhedra, Computer science, Concurrent computing, Geometry, shadow boundary, orthogonal projections, orthogonal projection, Web pages, Solids, convex polyhedron, Books, Johnson solid, triangular prism]
On the multi-channel noise cancellation using beamforming algorithm
2008 11th International Conference on Computer and Information Technology
None
2008
This paper presents performance analysis of beamforming algorithm for canceling multiple channel noise depending on variation of hidden layer of the multilayer feedforward network and the number of epoch (also known as number of iteration). We consider the learning rate variation of the multi layer perception (MLP) to determine the adaptive learning rate of the network. An MLP has been considered to perform beam-forming, which is an array of sensors connected to the MLP inputs. We use the backpropagation algorithm as the learning rule for MLP and improving the signal quality. This involves a desired signal whilst removing any noise or interference signals which may come from different sources.
[signal quality, adaptive learning rate, Array signal processing, Noise reduction, beamforming algorithm, Nyquist Sampling Rate, multichannel noise cancellation, Sensor arrays, Multi-layer neural network, MLP, DSPs, array signal processing, multilayer perceptrons, multilayer feedforward network, BSS, backpropagation algorithm, Neurons, multi layer perception, Feedforward neural networks, AM, Neural networks, Signal processing algorithms, Digital signal processing, backpropagation, signal processing techniques, Noise cancellation, &#x00B5;LMS]
A new approach to wrap legacy programs into web services
2008 11th International Conference on Computer and Information Technology
None
2008
Reengineering a legacy system to provide Web Services is a great challenge. Wrapping legacy systems is a proper solution to expose legacy program functionalities as Web Services. In this paper we propose a new approach in two phases to integrate legacy programs written in legacy languages such as C or Pascal into Web Services. This will be done by analyzing the legacy program and creating a Service Bus Class. Also, an automatic wrapping tool is developed to generate Web Services and related source code in Microsoft .Net environment. To evaluate this approach a legacy student learning program was wrapped into Web Services.
[service bus class, SOA, Reverse engineering, Service oriented architecture, microsoft .net environment, Web service, source code, wrap legacy student learning program, Legacy Programs, Application software, software maintenance, C language, Service Bus Class, Wrapping, Information technology, Web Service, Business communication, Connectors, Computer languages, systems re-engineering, Web services, Software systems, system reengineering]
A New Proposal for Choosing a Deflection Link in a Deflection Network
2008 11th International Conference on Computer and Information Technology
None
2008
In high-speed networks with greater connectivity, packets can be deflected through a sub-optimal path to reach the destination if the desired path is congested, failed or the buffer is full. However, in a combined store-and-forward and deflection network, packets are deflected when there is congestion in the network. For choosing a link for deflection in such a combined network, currently existing strategy considers only local information to the node in need to deflect a packet. In this paper, a new proposal has been introduced to select an outgoing link for deflection. Under the proposed strategy, packets are guaranteed to traverse through the best cost path available to the destination when they are deflected. Simulation shows better results than that of the existing strategy in terms of packet delivery, discard and average cost per delivered packet.
[Computer Networks, Costs, Buffer storage, Optical wavelength conversion, computer networks, Optical fiber networks, deflection network, Routing, Proposals, Information technology, packet delivery, Optical buffering, telecommunication network routing, deflection link, Computer networks, deflection routing, High speed optical techniques, Deflection Routing]
A comprehensive analysis of degree based condition for hamiltonian cycles
2008 11th International Conference on Computer and Information Technology
None
2008
Rahman and Kaykobad introduced a shortest distance based condition for finding the existence of Hamiltonian paths in graphs as follows: Let G be a connected graph with n vertices, and if d(u) + d(v) + delta(u, v) ges n + 1, for each pair of distinct non-adjacent vertices u and v in G, where delta(u, v) is the length of a shortest path between u and v , then G has Hamiltonian path. Rao Li proved that under the same condition, the graph is Hamiltonian or belongs to two different classes of graphs. Recently, Mehedy, Hasan and Kaykobad showed case by case that under the condition of Rahman and Kaykobad, the graph is Hamiltonian with exceptions for delta(u, v) = 2. Shengjia Li et. al. mentions a graph to be Hamiltonian whenever d(u) + d(v) ges n - 1, for all delta(u, v) = 2, otherwise n is odd and the graph falls into a special class. This paper relates the results of Mehedy, Hasan and Kaykobad with the two exceptional classes of graphs introduced by Rao Li and the graph class introduced by Shengjia Li et. al. The paper also provides a thorough analysis of the graph classes and shows the characteristics of a graph when it falls into one of those classes.
[Pervasive computing, Hamiltonian cycles, Hamiltonian cycle, graph theory, Hamiltonian path, Ubiquitous computing, Information technology, Information analysis, Computer science, Hamiltonian paths, Graphs, connected graph, computational complexity]
End-to-end mobility management solutions for TCP: An analysis
2008 11th International Conference on Computer and Information Technology
None
2008
Today's mobile devices are equipped with multiple network interfaces belonging to different access networks, these interfaces may be used in different requirements. A network connection should be established using the best possible interface. During communication, characteristics of a network may change or a new access network is reached by mobile node thus an already established connection should be moved from one interface to another or parallel transmission is done over two or more connections. In the last several years' variety of mobility management solutions have been proposed that operate at different layers of the communication protocol stack (e.g link layer, network layer, transport layer and application layer), but the network and transport layer mobility management solutions are more mature. In this paper we have discussed various end-to-end mobility management solutions for TCP. We have also identified a number of evaluation parameters and on the basis of these parameters different mobility management techniques are analyzed.
[Transport protocols, TCP, network layer mobility management, Mobility Management, multiple network interface, end-to-end, Mobile communication, mobility management (mobile radio), Mobile radio mobility management, Delay, Information analysis, network interfaces, end-to-end mobility management, transport layer, TCPIP, Computer networks, Peer to peer computing, Information technology, Hip, mobile device, communication protocol stack, transport protocols, mobile node, transport layer mobility management, vertical handover]
Dynamic communication performance of a TESH network under the nonuniform traffic patterns
2008 11th International Conference on Computer and Information Technology
None
2008
Interconnection networks play a crucial role in the performance of massively parallel computers. Hierarchical interconnection networks provide high performance at low cost by exploring the locality that exists in the communication patterns of massively parallel computer systems. The Tori-connected mESH (TESH) Network is a 2D-torus network of multiple basic modules, in which the basic modules are 2D-mesh networks that are hierarchically interconnected for higher level networks. In this paper, we present a deadlock-free routing algorithm for the TESH network using 2 virtual channels - 2 being the minimum number for dimension-order routing - and evaluate the network's dynamic communication performance under various nonuniform traffic patterns, using the proposed routing algorithm. We evaluate the dynamic communication performance of TESH, mesh, and torus networks by computer simulation. It is shown that the dynamic communication performance of the TESH network is better than that of the mesh and torus networks.
[Costs, Multiprocessor interconnection networks, multiprocessor interconnection networks, Telecommunication traffic, Concurrent computing, TESH network, dynamic communication performance, interconnection network, deadlock-free routing algorithm, tori-connected mesh network, Traffic control, Computer networks, nonuniform traffic pattern, multiprocessing systems, Computer simulation, network routing, Routing, deadlock-free routing, network topology, parallel computer system, High performance computing, virtual channel, concurrency control, System recovery, nonuniform traffic patterns]
WalkSAT approach in solving the staff transfer problem
2008 11th International Conference on Computer and Information Technology
None
2008
Staff transfer is an important issue in human resource management. It is concerned with the assignment of transfer postings to employees in large organizations. A large organization has offices and work sites at multiple locations and it is customary to transfer a subset of employees at periodic intervals. The staff transfer problem (STP) can be viewed as a constraint satisfaction problem(CSP). The deterministic methods were proved to be inferior to local search methods in solving this problem. In earlier investigation GSAT(L) easily outperformed the simulated annealing (SA) which was the then best among all approaches. In this work computer experiments indicate that WalkSAT(L,p) easily outperforms GSAT(L) in most of the instances.
[Local search, staff transfer problem, simulated annealing, Computational modeling, Government, constraint theory, computability, constraint satisfaction problem, Tabu Search, human resource management, WalkSAT approach, Information technology, SA, Computer science, Staff Transfer, Search methods, satisfiability, operations research, Simulated annealing, Lab-on-a-chip, Constraint Satisfaction, Human resource management, Satisfiability]
A novel fuzzy method to traffic light control based on unidirectional selective cellular automata for urban traffic
2008 11th International Conference on Computer and Information Technology
None
2008
Vehicular travel which demands on the concurrent operations and parallel activities is increasing throughout the world. In this paper to control urban traffic, we study the optimization of traffic light controller in a city and present a fuzzy algorithm based on cellular automata. In existent system factors like priority of streets of intersection and width/length of streets are assumed equal. However, in real situations parameters like time during the day, density of vehicles of street and number of shopping centers have determinant effects on amount of traffic of streets. To overcome such limitations we propose a three leveled fuzzy system. At first level priority of street is computed based on fuzzy rules. At second level real velocity of vehicles is calculated. We use CA for simulating vehicles' density transmission. At third level by regarding priority of street and amount of density, decision for changing status of traffic light is done.
[Traffic Control, fuzzy algorithm, Telecommunication traffic, Automated highways, Urban Traffic, urban traffic control, Vehicles, cellular automata, fuzzy control, Fuzzy control, road vehicles, Unidirectional Selective Cellular Automata, fuzzy rule, Automatic control, Traffic control, Cities and towns, Computer networks, Cellular Automata Model, Fuzzy systems, road traffic, vehicle density transmission, vehicular travel, Fuzzy Control, Lighting control, Traffic Light Control, unidirectional selective cellular automata, traffic light control, three leveled fuzzy system]
A least square approach for TDOA/AOA wireless location in WCDMA system
2008 11th International Conference on Computer and Information Technology
None
2008
Cellular mobile networks use time of arrival (TOA), time difference of arrival (TDOA), angle of arrival (AOA) or received signal strength (RSS) based measurement techniques for mobile position estimation. Each of these measurement techniques has its degree of accuracy and level of complexity. Individual methods are usually not able to give adequate correct position of a mobile user. Therefore, a common trend is to combine these methods with other suitable techniques to attain enhanced performance. There are several algorithms to compensate the position errors by combining different methods. In this paper, a two step least square algorithm for wireless position estimation technique is implemented in WCDMA system where an angle of arrival (AOA) assisted and correlation slope-based time difference of arrival (TDOA) measurement method is considered and their performance is presented in signal to noise ratio conditions.
[time-of-arrival estimation, cellular mobile networks, Multiaccess communication, least square algorithm, AOA wireless location, AOA, mobile position estimation, Measurement techniques, received signal strength, Performance analysis, least squares approximations, WCDMA system, Time difference of arrival, WCDMA, TDOA, angle of arrival, wireless position estimation technique, Time measurement, code division multiple access, Noise measurement, Least squares approximation, time difference of arrival, Least squares methods, Cellular networks, Least squares, Position measurement, time of arrival, TDOA wireless location, Signal to noise ratio, cellular radio]
Distortion measurement using arc-length-parameterisation within a vertex-based shape coding framework
2008 11th International Conference on Computer and Information Technology
None
2008
Existing vertex-based shape coding algorithms use a number of different distortion measurement techniques including the shortest absolute distance (SAD), distortion band (DB) or tolerance band (TB), accurate distortion measurement technique for shape coding (ADMSC) and distortion measurement based on chord-length-parameterisation (DMCLP). Among these techniques DMCLP is computationally the fastest. It however, is a relaxed measurement technique and does not fully utilise the admissible distortion within the rate-distortion optimisation. This paper presents a novel distortion measurement technique based on arc-length-parameterisation (DMALP) within the vertex-based shape coding framework. Experimental results vindicate its superior performance over the existing geometric distortion measurement techniques.
[Video coding, Real time systems, distortion measurement technique, arc-length-parameterisation, Shape measurement, distortion measurement, rate distortion theory, rate-distortion optimisation, optimisation, Image coding, Rate-distortion, tolerance band, image/video coding and transmission, Distortion measurement, distortion band, chord-length-parameterisation, Encoding, video coding, Computational complexity, Information technology, Computer science, shortest absolute distance, Shape control, vertex-based shape coding framework, image coding]
Bangla vowel sign recognition by extracting the fuzzy features
2008 11th International Conference on Computer and Information Technology
None
2008
This paper presents a fuzzy based Bangla vowel sign recognition system. Vowel and consonant are the main criteria to express any language. Exceptionally, Bangla has some vowel sign which are used to express Bangla word. Therefore, to recognize any Bangla word, at first it is necessary to recognize which vowel sign is used to express that word. Due to its low computational requirement and very high accuracy, fuzzy logic system is probably the most efficient method available for online vowel sign recognition. The most important task for this implementation is the building of the rule base using fuzzy logic that would describe the recognized vowel sign, which is more complicated as different people write the same vowel sign in completely different ways. In this paper, we propose a system that uses automatically generated fuzzy membership value that describes the handwritten Bangla vowel sign written by different individuals.
[Segmentation, fuzzy set theory, fuzzy reasoning, Data mining, Fuzzy sets, feature extraction, knowledge based systems, Geometric Features, Fuzzy systems, handwritten character recognition, fuzzy logic, Positional Features, Character recognition, handwritten Bangla vowel sign recognition system, Information technology, Fuzzy logic, Handwriting recognition, fuzzy membership value, Speech recognition, Vowel Sign, Writing, Feature extraction, natural languages, fuzzy logic system, rule base, image recognition]
Speech enabled operating system control
2008 11th International Conference on Computer and Information Technology
None
2008
Our goal is to provide the computer with a natural interface, including the ability to understand human speech. For this purpose, we propose a way how to operate an operating system with voice command. At first, the user initiates a given command by his voice through the microphone then the recognition software of the proposed system will take over to recognize the command. If the recognition is succeeded or matched with one of the given voice command then it will perform the operation on the operating system according to speaker's speech command. In our proposed system we have used Markov chain model for voice recognition process and voice-XML for creating the voice grammar in the software part. It has the flexibility to work with the speech of any user means and remove the problem of dasiahesitationpsila which is very effective for spontaneous speech recognition. The proposed system minimizes the error rate up to 30% which is very precise for any speech recognition system.
[Computer interfaces, Error analysis, Process control, Markov chain model, Control systems, user interfaces, voice-XML, speech enabled operating system control, Microphones, Voice recognition, natural interface, speech recognition, Operating systems, Keyboards, XML, Speech recognition, Voice command, Markov processes, operating systems (computers), voice recognition process, Voice-XML Operating System, Speech, Mice, Man machine systems]
Peer-to-Peer mobile applications using JXTA/JXME
2008 11th International Conference on Computer and Information Technology
None
2008
With the advancement in mobile wireless communication technology and the increasing number of mobile users, peer-to-peer (P2P) computing, in both academic research and industrial development, has found a new dimension of communication, collaboration and resource sharing. In the recent years P2P have evolved rapidly in Internet especially for file-sharing. The major issue of this popularity is that no longer the networks are constrained on unreliable central servers; sharing and connectivity is the main focus. The popularity of peer-to-peer file sharing applications such as Gnutella, KaZaA, or Napster has created an outbreak in recent Internet history. But P2P file sharing applications have not yet been widely adopted in mobile devices. In this paper, possibilities of P2P applications in mobile devices are presented. To illustrate the possibilities two demo applications are presented here. One is a message passing application and another is a file sharing application. The open source JXTA/JXME framework is used here.
[Java, message passing, peer-to-peer computing, Peer to peer computing, JXME, Mobile communication, peer-to-peer mobile application, P2P, Wireless communication, mobile device, mobile computing, Communication industry, Collaboration, mobile applications, Computer industry, JXTA, Communications technology, Internet, Resource management, JSR75 API, Mobile computing, file sharing]
Time-frequency representation of audio signals using Hilbert spectrum with effective frequency scaling
2008 11th International Conference on Computer and Information Technology
None
2008
The efficiency of Hilbert spectrum (HS) in time-frequency representation (TFR) of audio signals is investigated in this paper. HS is derived by applying empirical mode decomposition (EMD), a newly developed data adaptive method for nonlinear and non-stationary signal analysis together with Hilbert transform. EMD represents any time domain signal as a sum of a finite number of bases called intrinsic mode functions (IMFs). The instantaneous frequency responses of the IMFs derived through Hilbert transform are arranged to obtain the TFR of the analyzing signal yielding the HS. A new frequency scaling method is introduced here for proper interpretation of the energy spectra in HS. The performance of HS is compared with well known and widely used short-time Fourier transform (STFT) technique for TFR. The experimental results show that HS based method performs better than STFT in time-frequency representation of the audio signals.
[Fourier transforms, Speech analysis, empirical mode decomposition, Empirical mode decomposition (EMD), Wavelet analysis, instantaneous frequency, time-frequency representation, time-frequency representation (TFR), Hilbert transforms, Hilbert transform, audio signals, time-frequency analysis, time domain signal, data adaptive method, Performance analysis, Wavelet transforms, frequency scaling, Wavelet domain, nonstationary signal analysis, nonlinear signal analysis, Hilbert spectrum, Time frequency analysis, intrinsic mode functions, Information technology, Computer science, Hilbert-Huang transformation (HHT), audio signal processing, short-time Fourier transform, Signal analysis, frequency response]
Connecting telephone exchange centers over metro ethernet networks by using Diffserv-MPLS
2008 11th International Conference on Computer and Information Technology
None
2008
Recent large deployment of high-speed metro Ethernet networks creates opportunity to connect legacy telephone exchange centers and cellular telephone base stations to the core telephony network. The success of data and legacy voice services convergence shall depend on whether it can provide the customer with the required QoS or not. Packet-base telephony service requires stringent bounds on end-to-end packet delay, jitter, and loss. New protocols such as MPLS and Diffserv are realized due to the major advantages associated with them such as QoS. This paper studies the performance of connecting telephone exchange centers over a metro Ethernet network using MPLS and DiffServ QoS model. We will implement some simulations to evaluate the performance in terms of delay, jitter and loss.
[Ethernet networks, Metropolitan area networks, Jitter, end-to-end packet delay, local area networks, multiprotocol label switching, Delay, Convergence, Multiprotocol label switching, Telephony, DiffServ QoS model, Data communication, Diffserv-MPLS, core telephony network, Telephone exchanges, Base stations, telephone exchanges, metro Ethernet networks, Cellular networks, DiffServ networks, Diffserv networks, cellular telephone base stations, Queuing analysis, telephone exchange centers, Joining processes]
Rice disease identification using pattern recognition techniques
2008 11th International Conference on Computer and Information Technology
None
2008
The techniques of machine vision are extensively applied to agricultural science, and it has great perspective especially in the plant protection field, which ultimately leads to crops management. The paper describes a software prototype system for rice disease detection based on the infected images of various rice plants. Images of the infected rice plants are captured by digital camera and processed using image growing, image segmentation techniques to detect infected parts of the plants. Then the infected part of the leaf has been used for the classification purpose using neural network. The methods evolved in this system are both image processing and soft computing technique applied on number of diseased rice plants.
[classification purpose, image classification, rice diseases detection, Digital cameras, pattern recognition techniques, crops, neural network, machine vision, agriculture, soft computing, image growing, Plants (biology), image segmentation, rice disease identification, crops management, plant protection field, Protection, infected rice plants, image processing, Software prototyping, leaf blast(Magnaporthe grisea), Crops, Fractional zooming, diseases, Pattern recognition, software prototype system, Diseases, SOM, Image segmentation, digital camera, Machine vision, agricultural science, computer vision, Software systems, brown spot (Cochiobolus Miyabeanus), neural nets]
The study of tracking control of a robotic manipulator actuated by shape memory alloy
2008 11th International Conference on Computer and Information Technology
None
2008
Shape memory alloy (SMA) composites offer a great potential as actuators because of its excellent power to weight ratio, smooth motion and silent actuation. However, SMA actuated robotic manipulator systems have severe hysteresis, system nonlinearities, model and parametric uncertainties those are often responsible for position inaccuracy in a regulation or tracking system. A simple and upstanding control algorithm for tracking control of a SMA actuated robotic manipulator system is presented in this paper. The controller is developed based on the combination of a variable structure control approach and a heuristic based intelligent control approach like fuzzy logic control (FLC), so that the resulting control algorithms have a superior performance both in stabilization and tracking the desired trajectories. A desktop prototype of the robotic manipulator system is recalled from our previous work. A nonlinear form of proportional-integral-derivative (PID) controller is also employed to the system for comparison of performance. Several experiments have been performed in tracking both stationary and periodically varied input signals. Experimental results from real time control verify the effective and robust performance of the controller.
[Actuators, PID controller, Uncertainty, Shape memory alloys, proportional-integral-derivative controller, Robot control, manipulators, controller design, Control systems, tracking, shape memory effects, hysteresis, Position contro, fuzzy control, SMA actuator, control nonlinearities, position control, tracking control system, Fuzzy Logic, Hysteresis, stability, Intelligent control, parametric uncertainty, three-term control, uncertain systems, intelligent actuators, variable structure control approach, system stabilization, Manipulators, control system synthesis, intelligent robots, Power system modeling, system nonlinearity, Manipulator, Shape control, heuristic-based intelligent control approach, shape memory alloy composites, SMA actuated robotic manipulator system, Sliding Mode control, system hysteresis, fuzzy logic control, variable structure systems]
Security enhancement of MD5 hashed passwords by using the unused bits of TCP header
2008 11th International Conference on Computer and Information Technology
None
2008
When a password is encrypted by a hash algorithm the resultant is called hashed password. In a server client based communication system such as Yahoo Messenger, AIM, passwords of clients are hashed by MD5 and passed to the server for authentication. This type of transmission is always a subject of interception by the hackers. These hashed passwords are passed through the Internet as a data packet. TCP header is a most common part of the data packet. In a TCP header there are six reserved bits which remains always unused. In this paper we propose a new approach to enhance the security of hashed passwords by using the six reserved bits of a TCP header. Here we encrypt the hashed password by a random key using simple mathematical function. The information needed to decrypt the encrypted hashed password is carried by the six bits of TCP header.
[Dictionaries, MD5 hash algorithm, Security, random key, mathematical function, TCP header, Network servers, Computer hacking, user authentication, Computer network security, Cryptography, Web server, Computer security, hashed password encryption, client-server systems, message digest 5, Client-server systems, Data security, cryptography, transport protocols, Information security, Authentication, message authentication, security enhancement, unused bit, Internet, server client-based communication system]
Design of an interactive bangla mobile keypad based on ergonomics
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, a Bangla mobile keypad layout is proposed based on ergonomics. The finger movement across the keypad is analyzed and an optimal keypad layout is proposed for right handed one thumb operation. To minimize the effort of the user's thumb movement, the characters are organized in such a way that the most frequently used characters are positioned in the most comfortable zone of the keypad. A couple of equations are derived to calculate the required time to insert a character in different existing layouts. The performance is analyzed by comparing time which reveals that the proposed layout outperforms the other layouts.
[interactive Bangla mobile keypad, Mobile phone, keyboards, Mobile keypad, Thumb, Switches, Bangla, Mobile handsets, Information technology, Nanoelectronics, Computer science, Design engineering, Bangla Characters, Ergonomics, Frequency, right handed one thumb operation, Mobile computing, optimal keypad layout]
A practical analysis of the robustness and stability of the network stack in smartphones
2008 11th International Conference on Computer and Information Technology
None
2008
Smartphones are widely used nowadays and their popularity will certainly not slow down in the near future due to improved functionality and new technology improvements. Becoming more and more similar to PCs and laptops, they will also begin to face the same security problems especially in terms of network security. In this paper, we provide an overview of security issues for smartphones and give a brief introduction of the network stack architecture of Windows Mobile 5.0 platform in order to motivate and plan for efficient penetration tests against Windows Mobile 5.0 powered smartphones. Furthermore, a number of attacks have been done against different layers of the network stack and a list of supporting penetration tools are provided for interested practitioners. Detailed results are provided from the penetration tests performed which should be very useful for security vendors, researchers and OS vendors to give more attention to security architectures and development of security software (e.g. firewalls, antivirus) for the mobile operating systems. In a whole, robustness and stability will be assessed for the mobile OSs used by the smartphones throughout the paper.
[Software testing, Performance evaluation, System testing, Portable computers, program testing, Network scanning, mobile operating systems, security problem, security issues, Penetration testing, Smartphones, software architecture, mobile computing, Computer architecture, technology improvement, stability, network stack architecture, Robust stability, efficient penetration tests, Windows Mobile 5.0 powered smartphones, network security, robustness, Stability analysis, Power system security, practical analysis, Network stack, security of data, security architecture, Vulnerability scanning, operating systems (computers), Windows Mobile 5.0 platform, Personal communication networks, Smart phones, security software development, mobile handsets]
Lightpath reconfigurations in IP-over-CWDM networks with stackable ROADMs
2008 11th International Conference on Computer and Information Technology
None
2008
Stackable ROADMs (S-ROADMs) have been proposed for use in regional IP-over-CWDM networks. The S-ROADM can be constructed by connecting modules with different wavelengths required in the node. The experimental results clarified that the S-ROADM could mux and demux the wavelengths successfully, and gave no limit to the passing-through wavelengths, making the network be wavelength transparent. Contrary to the S-ROADM, the existing fixed ROADMs were not wavelength transparent. The bidirectional S-ROADMs have also been proposed, and lightpaths could be reconfigured successfully in an IP-over-CWDM network with the bidirectional S-ROADMs.
[wavelength division multiplexing, IP-over-CWDM network, CWDM, Scalability, lightpath reconfiguration, Optical fiber networks, Wavelength division multiplexing, Transceivers, Network interfaces, reconfigurable optical add/drop multiplexer, coarse wavelength-division multiplexing, Optical add-drop multiplexers, optical fibre networks, Wavelength Transparency, Communication system traffic control, Computer networks, IP networks, Joining processes, Lightpath Reconfiguration, Local area networks, wavelength transparency, multiplexing equipment, bidirectional stackable ROADM, Stackable ROADM]
Prediction of the density of active wireless device using markov model
2008 11th International Conference on Computer and Information Technology
None
2008
Location management is one of the key issues in wireless networks to provide an efficient and low-cost service. Realistic modeling of user mobility is a critical research area in wireless network. Mobility data based on real human behaviors may give us the opportunity to improve wireless and Mobile services for users in many ways. At present, several mobility models are proposed based on the analysis of real traces . In this paper, we investigate the feasibility of next state prediction using sequences of previously observed state and analyze the efficiency of MARKOV MODEL . The scenario concerns servicing wireless devices by wireless access point in the Dartmouth college campus over some period of time. The performance of the method has been verified for prediction accuracy. It is found that, on average, the choice of training data leads to prediction accuracy of 78.45%, in some cases the accuracy achieves about 95.55%.
[state prediction, user mobility data, Military computing, telecommunication network management, location management, Predictive models, Conference management, low-cost service, Access Point, Educational institutions, mobility management (mobile radio), Markov model, Information technology, radio access networks, active wireless network, Accuracy, Wireless networks, Training data, human behavior, Markov processes, Computer networks, observed state, prediction accuracy]
Crosstalk and bit error rate performance of a proposed optical cross-connect topology
2008 11th International Conference on Computer and Information Technology
None
2008
This paper presents an optical wavelength division multiplexed (WDM) cross-connect (OXC) topology, which is the modified form of the existing topologies. The analytical approach for the proposed topology is derived to explore the effect of crosstalk induced in multichannel WDM network due to OXC. The factors that affect the magnitude of crosstalk in the OXC are investigated and identified. The bit error rate (BER) performance of a WDM optical system is evaluated at a bit rate of 10 Gb/s on account of OXC-induced crosstalk and different noises. Power penalty is evaluated at BER of 10-9 shows that there is a significant impact of crosstalk on the number of wavelengths and input fibers. The results obtained from the present study demonstrate that the crosstalk performance of the proposed topology can be improved significantly than the existing topologies.
[wavelength division multiplexing, optical switches, multichannel WDM network, crosstalk performance, Bit error rate, WDM cross-connect topology, WDM networks, Optical fiber networks, optical cross-connect topology, optical filter, Circuit topology, optical cross connect, Network topology, WDM optical system, OXC-induced crosstalk, optical communication, optical crosstalk, error statistics, optical wavelength division multiplexing, Optical filters, Optical fibers, Optical switches, telecommunication network topology, Wavelength division multiplexing, bit error rate, Optical crosstalk]
Recurrent neural network classifier for Three Layer Conceptual Network and performance evaluation
2008 11th International Conference on Computer and Information Technology
None
2008
Contextual analysis in dialog is a hard problem. In this paper three layers memory structure is adopted to address the challenge which we refer to as three layer conceptual network (TLCN). This highly efficient network simulates the human brain by episodic memory, discourse memory and ground memory. An extended case structure framework is used to represent the knowledge. The knowledge database is constructed by the collection of target system information and utterances. This knowledge is updated after every dialog conversation. A Recurrent Neural Network classifier is also introduced for classifying the knowledge for the target system. This system prototype is based on doctor-patients dialogs. 78% disease classification accuracy is observed by this system prototype. Disease identification accuracy is depending on number of disease and number of utterances. This performance evaluation is also discussed in details.
[Recurrent neural networks, Humans, knowledge database, ground memory, human brain, Databases, recurrent neural network classifier, Prototypes, Delta modulation, three layer conceptual network, recurrent neural nets, performance evaluation, diseases, Knowledge management, brain, doctor-patient dialog, Biological neural networks, Diseases, episodic memory, Neural Network, disease classification, Memory management, knowledge representation, contextual analysis, Brain modeling, medical computing, discourse memory, Three Layer Conceptual Network]
Performance investigation of earth-to-satellite microwave link due to rain fade in Bangladesh
2008 11th International Conference on Computer and Information Technology
None
2008
The fast growth in communication systems has resulted congestion in lower frequency bands and system designers are forced to explore higher and higher frequencies. Rain is a dominant source of attenuation at higher frequencies and consequently degrades the system performance in tropical and subtropical regions. The knowledge of rain fade and its performance is essential in order to optimize system capacity and meet quality and reliability. This paper has investigated performance of earth-to-satellite link due to rain fades operating in a subtropical country Bangladesh. The rain intensity data are derived from forty years measured annual rainfall data. The converted rain intensity data is used to estimate rain fades at C, Ku and Ka-bands. The rain fade is also estimated using ITU-R recommended rain intensity. Noise generated during rains is predicted for all three bands and carrier-to-noise ratios are estimated to compare the performances of earth-to-satellite link at different frequency bands in a subtropical region.
[Availability, satellite links, rain, earth-to-satellite microwave link, rainfall data, Rain fading, CNR, Earth-to-satellite, rain fade, Frequency estimation, Noise generators, ITU-R recommended rain intensity, Information technology, Rain Fade, Degradation, Design engineering, microwave links, Bangladesh, Rainfall Data, System performance, Attenuation, Link Budget, Signal to noise ratio]
Developing a model of e-governance for urban and rural areas of Bangladesh
2008 11th International Conference on Computer and Information Technology
None
2008
Several developing countries including Bangladesh are realizing the role ICT (information &amp; communication technology) can play in the governance sector, and are putting into practice innovative e-governance models that may be technologically simple but are drastically changing the way information is distributed in the society. This paper depicts and implements a model of e-governance for urban and rural areas in Bangladesh. The project deeply considered existing ICT infrastructure in Bangladesh and put effort to implement the model in a cost effective and flexible manner. A clear, comprehensive layout and network architecture for rural and city areas is given separately. We also described complementary tasks such as database implementation, system development, web related task etc. for e-governance. The main objective of the project is to implement e-governance in an easy and cost effective way to suit with existing ICT infrastructure in Bangladesh to provide better information and service delivery of government to the people.
[Costs, information &amp; communication technology, Urban areas, complementary tasks, e-governance, town and country planning, Network servers, Databases, government, Dial-up network, VPN, Cities and towns, Telephony, Modems, Communications technology, Virtual private networks, ICT, Private network, Public network, Baud rate, Electronic government]
Solar cell efficiency improvement using compound parabolic concentrator and an implementation of sun tracking system
2008 11th International Conference on Computer and Information Technology
None
2008
This paper presents how the efficiency of solar cell can be increased with the design of compound parabolic concentrator and the implementation of sun tracking system. Sun tracking systems is an application of the machine vision (MV) and collaboration with data acquisition (DAQ) to systems by using a Web camera as a sensor and sound card as an output channel to drive a motor. The motors react as the mechanism of the camera to make sure it always focuses on the target of sun. By using a MatLab software programming environment, closes loop control is implemented to provide interaction between camera and sound card as a device to control the motor to track the sun.
[compound parabolic concentrator, machine vision, close loop control, concentrator, Photovoltaic cells, closes loop control, Acoustic sensors, data acquisition, control engineering computing, Target tracking, sun tracking, solar cell efficiency improvement, Data acquisition, Sensor systems and applications, Sun, power engineering computing, MatLab software programming environment, Programming environments, closed loop systems, Solar cell, Machine vision, Collaboration, sun tracking system, computer vision, Web camera, Cameras, solar cells]
Server consolidation using OpenVZ: Performance evaluation
2008 11th International Conference on Computer and Information Technology
None
2008
Virtualization is abstracting computer resources. It is the ability to run multiple operating systems simultaneously on a single physical machine. In this paper we evaluate the performance of the operating system level virtualization software (OpenVZ). The evaluation will be based on Web hosting, in other words, we evaluate the performance of Web servers on two different machines (dubbed as Baru and Lama) and we observe the behavior of OpenVZ and its scalability for Web servers on different architectures (32 bits and 64 bits). The study evaluates the CPU, Memory and Throughputs under different loading conditions.
[Costs, Dictionaries, virtualization, server consolidation, OpenVZ, performance evaluation, Web hosting, Virtual machining, operating system level virtualization software, Web servers, Information technology, Lama, Application virtualization, Resource virtualization, word length 64 bit, performance, Operating systems, Physics computing, word length 32 bit, file servers, operating systems (computers), Hardware, Web server, Baru]
Secure online sealed bid Auction
2008 11th International Conference on Computer and Information Technology
None
2008
Security and privacy have become crucial factors in auction design. Various schemes to ensure the safe conduction of sealed bid auctions have been proposed recently. We propose a secure sealed bid auction protocol. We introduce a new standard of privacy for auctions, that prevents extraction of bid information despite any collusion of participants. Present cryptographic protocol which is the realization of an electronic auction being the component of the e-government or e-commerce system. Our protocol provide non-reputation and correctness.
[auction protocols, cryptographic protocols, e-government system, cryptographic protocol, Data mining, Cryptographic protocols, auction corruption, Privacy, online sealed bid auction security, electronic auction, Information security, e-commerce system, Auction security, Sealed bid auction, data privacy, Internet, Electronic government, electronic commerce]
An artificial Ant based novel and efficient approach of regular geometric shape detection from digital image
2008 11th International Conference on Computer and Information Technology
None
2008
The paper presents a novel and efficient method of regular geometric shape detection from gray scale images. Artificial ant based methods have not been used much in the field of image processing. This paper demonstrates how artificial ants can be used effectively to extract regular geometric shapes from images. We propose here ant regeneration and recombination system (ARRS), an entirely new approach developed by ourselves. Our scheme of detection of shapes comprises of three steps. Firstly, MATLAB edge detection operator converts a gray scale image into a binary one. Ant regeneration and recombination system algorithm is then applied on this binary image to detect closed loops. Finally, these closed loops are tested for different geometric shapes like circle, ellipse, rectangle and square. The most important aspect of the scheme is it can detect both intersecting as well as non intersecting regular shapes from images consisting of different open and closed loop configurations. It is the incredible time and memory efficiency of the scheme that makes it useful in real time applications where decisions have to be taken within a very small time interval by observing an image.
[Shape, Image processing, gray scale image, mathematics computing, ant regeneration recombination system, binary image, artificial ant-based method, computational geometry, closed loop configuration, Telecommunication computing, MATLAB, Shape detection, optimisation, Edge Detection, Recombination System, non intersecting regular shape, shape recognition, edge detection, Testing, image processing, Computer vision, MATLAB edge detection operator, Image converters, regular geometric shape detection, Digital images, Image edge detection, time complexity, digital image, Information technology, Ant Regeneration, Loop detection, computational complexity]
Comparison of GARCH and neural network methods in financial time series prediction
2008 11th International Conference on Computer and Information Technology
None
2008
Many researchers have already published huge number of papers comparing Autoregressive (AR) model, a model based on Box-Jenkins methodology, and Back Propagation Artificial Neural Network (BPANN) in financial time-series forecasting. Among them, some compared SVMs and BPs taking AR as a benchmark in forecasting the six major Asian stock markets. They showed that both the SVMs and BPs outperform the traditional models, ARs. They did prediction of transformed data, but not level data. They did not take account of GARCH model, specially developed to model financial time series. Generalized Autoregressive Conditional Heteroskedastic (GARCH) model is needed to capture high volatility for better forecasts. This article applies GARCH model instead AR or ARMA model to compare with standard BP in forecasting of the four international including two Asian stock markets indices. Our fitted GARCH models give better forecasts than the fitted standard BP models in forecasting of the four international markets indices except one market.
[international markets, ARMA model, back propagation artificial neural network, Predictive models, Environmental economics, generalized autoregressive conditional heteroskedastic model, Autoregressive Moving Average (ARMA), Back Propagation Artificial Neural Network (BPANN), financial time series prediction, Economic forecasting, GARCH, stock markets, Stock markets, Box-Jenkins methodology, Finance, Artificial neural networks, Power generation economics, autoregressive processes, time series, Neural networks, Generalized Autoregressive Conditional Heteroskedastic (GARCH), forecasting theory, backpropagation, Econometrics, financial time-series forecasting, neural nets, Autoregressive processes, Asian stock markets]
Solutions to motion self-occlusion problem in human activity analysis
2008 11th International Conference on Computer and Information Technology
None
2008
Human motion self-occlusion due to motion overlapping in the same region is a daunting task to solve. Various motion-recognition methods either bypass this problem or solve this problem in complex manner. Appearance-based template matching paradigms are simpler and hence faster approaches for activity analysis. In this paper, we concentrate on motion self-occlusion problem due to motion overlapping in various complex activities for recognition. This paper illustrates the directional motion history image concept and compares this motion representation approach with multilevel motion history representation and hierarchical motion history histogram representation to solve the self-occlusion problem of basic motion history image representation. We employ some complex aerobics and find the robustness of our method compared to other methods for this self-occlusion problem. We employ seven higher order Hu moments to compute the feature vector for each activity. Afterwards, k-nearest neighbor method is utilized for classification with leave-one-out paradigm. The comparative results clearly demonstrate the superiority of our method than other recent approaches.
[image classification, MMHI, Humans, human motion self-occlusion problem, template matching, motion recognition, History, Information analysis, MHI, Histograms, hierarchical motion history histogram representation, Image representation, Robustness, multilevel motion history representation, k-nearest neighbor method, HMHH, motion-recognition method, DMHI, Information technology, image matching, image motion analysis, feature vector, human activity analysis, Control engineering, vectors, leave-one-out paradigm, appearance-based template matching paradigm, Surveillance, image representation, Motion analysis, complex activity]
Performance analysis on an efficient human motion database with various motion representations
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, our proposed structured human motion database is adopted for different motion representations. The motions are first represented as a sequence of frames of 2D images, which were compressed using three recognized motion representation techniques: exclusive-OR, MEI (motion energy image), and MHI (motion history images). The representation is a 2D feature image. The feature image is compressed by characterizing the eigenvectors. A complete vector space called an eigenspace is constructed that represents the image feature vectors for the feature image. The motions are indexed using the projections onto the eigenspace. For the purpose of efficient searching within the database, our proposed B-tree motion database is created and maintained. The comparative performance evaluations for the aforesaid representations were investigated and satisfactory performances (about 90% recognition rate and smaller searching time) were realized for all of the cases using our proposed motion database structure.
[Image recognition, Humans, image compression, visual databases, History, eigenvalues and eigenfunctions, motion history images, Motion database, Image coding, motion energy image, image feature vector, Performance analysis, motion compression, image sequences, data compression, Medical robotics, indexing, B-Tree, Spatial databases, image sequence, eigenspace, image motion analysis, motion representation, eigenvector, B-tree motion database, structured human motion database, Image databases, Hidden Markov models, image representation, vector space, Motion analysis]
A data hiding capacity estimation technique for biometric images
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper an estimation technique for the data hiding capacity in biometric images is presented. We consider the QSWT algorithm for data hiding in biometric images and investigate the effect of message strength in increasing capacity in the presence of different types of attacks.
[Biometrics, DWT, data hiding capacity estimation, data hiding, Channel capacity, image watermarking, Watermarking, channel capacity, Wavelet analysis, biometrics (access control), watermarking, message strength, Discrete transforms, Authentication, Data encapsulation, QSWT, Mathematical model, Discrete cosine transforms, data encapsulation, image coding, biometric images, Gaussian channels, QSWT algorithm]
Design of a fractional-order self-tuning regulator using optimization algorithms
2008 11th International Conference on Computer and Information Technology
None
2008
The self-tuning regulators form an important sub-class of adaptive controllers. This paper introduces a novel scheme for designing a fractional order self-tuning regulator. Original designs for all the sub-modules of the self-tuning regulator are proposed. The particle swarm optimization algorithm is utilized for online identification of the parameters of the dynamic fractional order process while the subsequent tuning of the controller parameters is performed by differential evolution. Results show that the proposed self-tuning regulator is both precise and robust.
[Algorithm design and analysis, Tuners, Three-term control, particle swarm optimisation, fractional-order self-tuning regulator, parameter identifier, adaptive control, control system synthesis, self-adjusting systems, Telecommunication computing, particle swarm optimization, Adaptive control, Particle swarm optimization, Fractional calculus, Design optimization, differential evolution, Automatic control, Robustness, online parameter identification, Controller tuner, particle swarm optimization algorithm, fractional order self-tuning regulator]
An intelligent aviation obstruction warning light
2008 11th International Conference on Computer and Information Technology
None
2008
A scheme for aviation warning light is presented here. Unlike the conventional systems used for this purpose, the system presented here is using intelligent way of sensing daylight sensor fault, intelligent way of sensing light fault, intelligent way of activating standby light in case of the main light fault, ground cable monitoring and responding system with beep for any fault occurs. Microcontroller from 8051 family with tiny operating systems is implemented here for controlling and calculating in a multitasking environment so that the system becomes dependable with advanced feature.
[ground cable monitoring, Poles and towers, Control systems, Sensor systems, daylight sensor fault, responding system, RTX51 tiny OS, Sensor arrays, aerospace computing, intelligent aviation obstruction warning light, LDR, stand by, microcontrollers, Microcontrollers, Cable TV, Microcontroller, Light emitting diodes, avionics, Circuit faults, lighting, Intelligent sensors, line of sight, GCM, microcontroller, standby light, tiny operating systems, operating systems (computers), Aircraft, aerospace safety]
Evaluation of TCP performance over mobile IP wired-cum-wireless networks
2008 11th International Conference on Computer and Information Technology
None
2008
Mobility of user devices connecting to the Internet is of major interest in today's research in networking. For users as well as for applications, network mobility should be transparent. Reliable transport protocols like TCP has served well the wired Internet where the packet losses are mainly due to congestion, but is not ready for mobile IP wired-cum-wireless environments where the significant packet losses are due to bit errors and handoffs. In this paper, we have investigated the performance of TCP among the various TCP variants. We have observed both TCP senders (Newreno &amp; Vegas) and TCP receivers (Base &amp; Delayed-Ack). Using ns-2, we have evaluated the TCP throughput and packet delay over a single TCP connection. The simulation results suggest that a particular combination (one TCP sender with one TCP receiver) of TCP shows the best result in such mobile IP networks.
[Transport protocols, TCP, Mobile IP, Handoff, Bit error rate, TCP performance, mobile IP wired-cum-wireless networks, performance evaluation, Mobility, Mobile communication, Wired-cum-Wireless Networks, Delay, Degradation, TCP senders, mobile computing, Wireless networks, transport protocols, TCP receivers, TCPIP, ns-2, Computer networks, Internet, IP networks]
Geospatial Web Services in environmental planning
2008 11th International Conference on Computer and Information Technology
None
2008
As more and more mapping resources are becoming available, the Web is becoming a confluence of collaborative interactions, negotiations and planning platforms. The integration of Web services with geographic information systems (GIS) has triggered a new wave of interest for using the Web to compose dynamic mapping services for environmental planning. Web services could offer a framework for service integration and composition for collaborative environmental modeling. By wrapping geo-processing services of interoperable standards of XML and SOAP, we demonstrate a prototype implementation of environmental decision support systems GEO- ELCA (exploratory land use change analysis) where Web service-enabled middleware adds core functionality to a Web mapping service. The system demonstrates how geo-processing services can be integrated with environmental simulation models using OGC (Open GIS Consortium) compliant connectors that support WMS (Web mapping service) and WPS (Web processing services).
[Decision support systems, Geographic Information Systems, open systems, Environmental Planning, geographic information systems, access protocols, geospatial Web service, Simple object access protocol, Interoperability, interoperable standard, Land use planning, Prototypes, groupware, geographic information system, service composition, collaborative planning, middleware, service integration, environmental planning, collaborative negotiation, environmental decision support system, Wrapping, Middleware, SOAP, decision support systems, GIS, exploratory land use change analysis, land use planning, Web services, collaborative interaction, Collaboration, XML, Web Services, dynamic mapping service]
Significant cancer risk factor extraction: An association rule discovery approach
2008 11th International Conference on Computer and Information Technology
None
2008
Cancer is the top most death threat for human life all over the world. Current research in the cancer area is still struggling to provide better support to a cancer patient. In this research our aim is to identify the significant risk factors for particular types of cancer. First, we constructed a risk factor data set through an extensive literature review of bladder, breast, cervical, lung, prostate and skin cancer. We further employed three association rule mining algorithms, apriori, predictive apriori and Tertius algorithm in order to discover most significant risk factors for particular types of cancer. Discovery risk factor has been identified to shows highest confidence values. We concluded that apriori indicates to be the best association rule-mining algorithm for significant risk factor discovery.
[predictive apriori, Tertius algorithm, risk factor discovery, Humans, data mining, Bladder, Association rules, Data mining, Skin cancer, risk analysis, association rule mining, cancer risk factor extraction, association rule discovery, Lungs, Breast, cancer patient, cancer, medical computing, Cervical cancer, Data envelopment analysis, Logistics]
A Study of white matter and skull inhomogeneous anisotropic tissue conductivities on EEG forward head modeling
2008 11th International Conference on Computer and Information Technology
None
2008
The aim of this study is to investigate the effects of white matter (WM) and skull inhomogeneous anisotropic tissue conductivities on human head modeling. The inhomogeneity of WM and skull is included using fractional anisotropy (FA) method and the anisotropy is included according to volume constraint in the head model construction. A five-layered spherical head model implemented using finite element method (FEM) is used as a volume conductor with a known current source to measure the electroencephalogram (EEG) on the head surface. Statistical measurement techniques are applied to analyze the EEGs obtained from inhomogeneous anisotropic head models and a homogeneous isotropic model. This study finds that the effects of WM and skull inhomogeneous anisotropy on EEG are significant.
[skull inhomogeneous anisotropic tissue conductivities, head model construction, Humans, volume conductor, forward problem, Electroencephalography, Finite element methods, white matter, Anisotropic magnetoresistance, finite element method, Volume measurement, electroencephalogram forward head modeling, statistical measurement techniques, spherical head model, electroencephalography, fractional anisotropy method, EEG, Conductivity, Conductors, finite element analysis, electroencephalogram, Current measurement, Fractional anisotropy, Skull, Brain modeling, statistical analysis, inhomogeneous anisotropic conductivity, homogeneous isotropic model]
Selecting signature optical emission spectroscopy variables using sparse principal component analysis
2008 11th International Conference on Computer and Information Technology
None
2008
Principal component analysis (PCA) is a widely used technique in optical emission spectroscopy (OES) sensor data analysis for the low dimension representation of high dimensional datasets. While PCA produces a linear combination of all the variables in each loading, sparse principal component analysis (SPCA) focuses on using a subset of variables in each loading. Therefore, SPCA can be used as a key variable selection technique. This paper shows that, using SPCA to analyze 2046 variable OES data sets, the number of selected variables can be traded off against variance explained to identifying a subset of key wavelengths, with an acceptable level of variance explained. SPCA-related issues such as selection of the tuning parameter and the grouping effect are discussed.
[Spectroscopy, Plasma applications, Input variables, sensor data analysis, sparse principal component analysis, Optical devices, Chemicals, Plasma chemistry, Plasma materials processing, Stimulated emission, optical emission spectroscopy variables, data handling, Optical sensors, principal component analysis, Principal component analysis, infrared spectroscopy]
A Data mining approach for finding optimal discount of retail assortments
2008 11th International Conference on Computer and Information Technology
None
2008
Due to the recent competition in the retailing industry, retailers are striving to improve their operations in order to run their stores more efficiently. One of the most important factors that encourages customers to buy products is discount. The effects of discount on sales have rarely been dealt with academically. Moreover, in few previous researches in this case, the temporal characteristics of product discount have not been noticed. The problem addressed in this paper is the consideration of products' discounts in discovering association rules in different time intervals that a specific discount appears on a specific product and finding optimal discount for each product with the aim of maximizing total profits. Additionally, experiments on real world data demonstrate the effectiveness of the proposed approach.
[Algorithm design and analysis, profitability, Profitability, data mining, Industrial engineering, optimal discount, Data mining, Association rules, product discount, total profit maximization, sales, sales management, Itemsets, association rule discovery, Marketing and sales, retailing industry, retailing, retail assortment, Artificial intelligence, Mining industry, Business]
Identifying Stock Similarity Based on Episode Distances
2008 11th International Conference on Computer and Information Technology
None
2008
Predicting stock market movements is always difficult. Investors try to guess a stock's behavior, but it often backfires. Thumb rules and intuition seems to be the major tools. One approach suggested that instead of trying to predict one particular stock's movement with respect to the whole market, it may be easier to predict a stock A's movement based on another stock B's movement, because A may get affected by B after B's movement. This may provide the investor invaluable time advantage. It would be very useful if a general framework can be introduced that can predict such dependence between stocks based on any user defined criterion. This article attempts to lay down one such framework, where the stock time series is encoded as a binary string. This binary representation depends on the user defined criterion. The string distances between two such encoded time series has been used as a measure of dependence. Further, this technique has been used in the dasiapairs trading strategypsila; in fact, it is more powerful as varied user defined criterion can be handled while detecting similarity. The presented technique has been demonstrated with one typical user defined criterion.
[Fluctuations, binary representation, Thumb, stock market predictions, Time measurement, episode distances, stock time series, user defined criterion, Data mining, Association rules, binary string, pairs trading strategy, stock similarity, stock markets, Artificial intelligence, Stock markets]
Behavior-based Robotics And The Reactive Paradigm A Survey
2008 11th International Conference on Computer and Information Technology
None
2008
Behavior-based robots have come under the spotlight in recent years by making their presence known in diverse fields of human interest. Applications in behavior-based robotics have continued to grow in areas such as demining, search and rescue, office automation, health care, etc and continue to replace human beings in risky and menial tasks. With this inspiration, this paper investigates a variety of aspects in behavior-based robotics and the reactive paradigm in the context of their origins, concepts, applications and current research and is intended to provide a comprehensive overview about this area in robotics. The paper will review several central issues including a brief history of robotics, transition of robotic systems from hierarchical paradigm to reactive paradigm, biological/ethological inspiration for behavior-based robots, fundamentals of the reactive paradigm, architectures for controlling robotic behavior and the hybrid deliberative/reactive paradigm.
[Actuators, Robot control, Humans, reactive paradigm, Control systems, Cognitive robotics, History, behavior-based robotics, Robotics and automation, Intelligent robots, Robot kinematics, Robot sensing systems, hybrid deliberative paradigm, robots]
Reversible Data Hiding using Increased Peak Histogram
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper, we present a reversible watermarking algorithm which provides higher embedding capacity and higher PSNR value compared to the existing algorithms. In the proposed algorithm, we utilize a peak point of image histogram and increase the height of the peak by transferring the pixels from the neighbouring gray values to achieve the higher embedding capacity.
[PSNR, increased peak histogram, Watermarking, Educational institutions, Data mining, neighbouring gray values, watermarking, reversible data hiding, Histograms, reversible watermarking, Data encapsulation, Internet, Protection, Artificial intelligence, data encapsulation, Pixel]
Web document clustering approach using wordnet lexical categories and fuzzy clustering
2008 11th International Conference on Computer and Information Technology
None
2008
Web mining is defined as applying data mining techniques to the content, structure, and usage of Web resources. The three areas of Web mining are commonly distinguished: content mining, structure mining, and usage mining. In all these areas, a wide range of general data mining techniques, in particular association rule discovery, clustering, classification, and sequence mining, are employed and developed further to reflect the specific structures of Web resources and the specific questions posed in Web mining. In this paper, we introduced a Web document clustering approach that uses WordNet lexical categories and fuzzy c-means algorithm to improve the performance of clustering problem for Web document. Experiments show that fuzzy c-means algorithm achieves great performance optimization with comparison with the recent algorithms for document clustering.
[Clustering methods, data mining, fuzzy set theory, Web resource, Data mining, structure mining, usage mining, Optimization, content management, WordNet lexical category, Information science, Web document clustering, association rule discovery, Clustering algorithms, Web mining, fuzzy clustering, fuzzy c-means algorithm, Text mining, document handling, content mining, Association rules, Computer science, pattern clustering, Internet, Artificial intelligence, sequence mining]
A unifying viewpoint of some clustering techniques using Bregman divergences and extensions to mixed data sets
2008 11th International Conference on Computer and Information Technology
None
2008
We present a general viewpoint using Bregman divergences and exponential family properties that contains as special cases the three following algorithms: 1) exponential family principal component analysis (exponential PCA), 2) Semi-Parametric exponential family principal component analysis (SP-PCA) and 3) Bregman soft clustering. This framework is equivalent to a mixed data-type hierarchical Bayes graphical model assumption with latent variables constrained to a low-dimensional parameter subspace. We show that within this framework exponential PCA and SPPCA are similar to the Bregman soft clustering technique with the addition of a linear constraint in the parameter space. We implement the resulting modifications to SP-PCA and Bregman soft clustering for mixed (continuous and/or discrete) data sets, and add a nonparametric estimation of the point-mass probabilities to exponential PCA. Finally, we compare the relative performances of the three algorithms in a clustering setting for mixed data sets.
[parameter space, Data engineering, exponential family properties, Data mining, Jacobian matrices, Graphical models, Clustering algorithms, Bregman divergence, low-dimensional parameter subspace, Density functional theory, point-mass probabilities, Subspace constraints, clustering technique, probability, Bregman soft clustering, mixed data sets, mixed data-type hierarchical Bayes graphical model, semiparametric exponential family principal component analysis, pattern clustering, Euclidean distance, nonparametric estimation, Bayes methods, nonparametric statistics, linear constraint, Artificial intelligence, principal component analysis, Principal component analysis]
Making good choices of non-redundant n-gramwords
2008 11th International Conference on Computer and Information Technology
None
2008
A new complete proposal to solve the problem of automatically selecting good and non redundant n-gram words as attributes for textual data is proposed. Generally, the use of n-gram words is required to improve the subjective interpretability of a text mining task, with n ges 2. In these cases, the n-gram words are statistically generated and selected, which always implies in redundancy. The proposed method eliminates only the redundancies. This can be observed by the results of classifiers over the original and the non redundant data sets, because, there is not a decrease in the categorization effectiveness. Additionally, the method is useful for any kind of machine learning process applied to a text mining task.
[Text mining, text analysis, subjective interpretability, Decision making, data mining, Manuals, Frequency estimation, Mathematics, nonredundant n-gram words, Data mining, Proposals, Supervised learning, Machine learning, statistical analysis, Artificial intelligence, text mining task, machine learning process]
gSVMT: Aggregating SVMs over a dynamic grid learned from data
2008 11th International Conference on Computer and Information Technology
None
2008
Addressing the problem of adaptively modelling a classifier as a modular system, a new type of SVM aggregating method termed gridding SVM tree (gSVMT) is proposed in this paper. The proposed gSVMT achieves to discover data subregions with principal discriminant knowledge through a recursive SVM-supervised data partitioning procedure. For each subregion, an individual SVM is allocated to extract the subregion knowledge. A set of such SVMs are aggregated in a specific order, resulting in a globally reliable decision rule to predict new coming samples. Experiments on a synthetic Gaussian data set and 13 benchmark machine learning data sets, have highlighted the usability of the gSVMT on its competitive classification capability. In particular, the proposed gSVMT is found to have better generalization performance than SVM classifiers for data sets with high sparseness and/or class-imbalance. Its performance has been further demonstrated with the successful real application on a face membership authentication system.
[pattern classification, gridding SVM tree, support vector machines, gSVMT, trees (mathematics), SVM aggregating method, Data mining, Support vector machines, SVM classifiers, Support vector machine classification, Authentication, Machine learning, SVM-supervised data partitioning, face membership authentication system, Feature extraction, Usability, Artificial intelligence, Classification tree analysis, Testing]
Runtime thread rescheduling: An extended scheduling algorithm to enhance the performance of the gridbus broker
2008 11th International Conference on Computer and Information Technology
None
2008
Grid computing is becoming a requirement for the processing of large amount of data now-a-days. The Gridbus broker schedules jobs depending on data and compute resources. Current scheduling process does not reassign a job from lower compute resource to higher compute resource if higher compute resource is available. In this paper, we have proposed a technique to reassign a thread to higher grade executor by preempting the thread in lower grade executor by using the data restoration technique which track the information of the thread so far ran on a lower rate compute resource. It is done only if there is an idle higher computer resource is available. The performance as well as the reliability of the Grid has been improved by this approach in a considerable extent.
[runtime thread rescheduling, grid computing, extended scheduling algorithm, data restoration technique, Data mining, Yarn, Distributed computing, Middleware, Scheduling algorithm, gridbus broker, Adaptive scheduling, Runtime, Processor scheduling, resource allocation, Computer architecture, scheduling, Grid computing, grade executor]
An efficient clustering based texture feature extraction for medical image
2008 11th International Conference on Computer and Information Technology
None
2008
In some medical applications where a tissue of interest covers a large fraction of the image or a prior knowledge on the region of interest is available, extracting features by fixed blocs in the image is sufficient. However in the general case, one would like to identify features for each tissue in the image. This would require prior image segmentation. Medical image segmentation is one of the most challenging problems in medical image analysis and a very active research topic. Therefore, there is no algorithm available in the general case for isolating medical image regions. This paper presents an accurate method for extracting texture features from medical image for classification. It is based on bloc wise clustering of medical images. The proposed technique extracts accurate and general set of texural features. Experimental result showed the high accuracy of the extracted textural features. Experiments held on mammographic image analysis society (MIAS) dataset.
[mammographic image analysis society dataset, image classification, texture feature extraction, feature extraction, image segmentation, Feature extraction, medical image segmentation, image texture, medical image processing, medical image analysis, Biomedical imaging]
Ant colony clustering by expert ants
2008 11th International Conference on Computer and Information Technology
None
2008
In this article a new ant clustering algorithm based on case based reasoning (CBR) is presented. Every ant has a case base which is updated iteratively by the process of CBR. The ant which is successful in dropping an item becomes an expert and can use its knowledge for future picked up items. Also expert ants are capable of cooperating to share their knowledge for even better clustering. Our simulation results demonstrated better performance than previous approaches.
[Ant colony optimization, expert systems, case based reasoning, knowledge sharing, Data engineering, Data mining, Particle swarm optimization, Unsupervised learning, unsupervised learning, expert ant, optimisation, ant colony clustering, CBR, pattern clustering, ant colony optimization, Clustering algorithms, ant based clustering, case-based reasoning, Robustness, Libraries, Iterative algorithms, Artificial intelligence]
Detection of faulty products using data mining
2008 11th International Conference on Computer and Information Technology
None
2008
The manufacturing process is complex due to the large number of processes, diverse equipment set and nonlinear process flows. Manufacturers constantly face yield and quality problems as they constantly redesign their processes for the rapid introduction of new products and adoption of new process technologies. Solving product yield and quality problems in a manufacturing process is becoming increasingly difficult. There are various types of failures and their causes have complex multi-factor interrelationships. High innovation speed forced today's manufacturers to find failure causes quickly by examining the historical manufacturing data. Data mining offers tools for quick discovery of relationships, patterns, and knowledge in large databases. This has been applied to many fields such as biological technology, financial analysis, medical information, etc. Application of data mining to manufacturing is relatively limited mainly because of complexity of manufacturing data. Growing self-organizing map (GSOM) algorithm has been proven to be an efficient algorithm to analyze unsupervised DNA data. However, it produced unsatisfactory clustering when used on some manufacturing data. Moreover, there was no benchmark to monitor improvement in clustering. In this study a method has been proposed to evaluate quality of the clusters produced by GSOM and to remove insignificant variables from the dataset. With the proposed modifications, significant improvement in unsupervised clustering was achieved with complex manufacturing data. Results show that the proposed method is able to effectively differentiate good and faulty products.
[Algorithm design and analysis, fault diagnosis, growing self-organizing map algorithm, data mining, large database, manufacturing processes, faulty product detection, Data mining, manufacturing data mining, unsupervised clustering, Information analysis, Manufacturing processes, Databases, self-organising feature maps, very large databases, product quality, Clustering algorithms, product development, Technological innovation, Data analysis, quality control, manufactured products, Face detection, manufacturing data processing, Fault detection, pattern clustering, product yield, flaw detection, manufacturing process]
A new approach of modified transaction reduction algorithm for mining frequent itemset
2008 11th International Conference on Computer and Information Technology
None
2008
Association rule mining is to extract the interesting correlation and relation between the large volumes of transactions. This process is divided into two sub problem: first problem is to find the frequent itemsets from the transaction and second problem is to construct the rule from the mined frequent itemset. Frequent itemsets generation is the requirement and most time vast process for association rule mining. Nowadays, most efficient apriori-like algorithms rely heavily on the minimum support constraints to prune the vast amount of non-candidate itemsets. These algorithms store many unwanted itemsets and transactions. In this paper propose a novel frequency itemsets generation algorithm called MTR-FMA (modified transaction reduction based frequent itemset mining algorithm) that maintains its performance even at relative low supports. The experimental reports also show that proposed MTR-FMA algorithm on an outset is faster than high efficient AprioriTid and other some algorithms.
[frequent itemsets generation, modified transaction reduction algorithm, data mining, Modified Transaction Reduction, Data mining, Association rules, association rule mining, apriori-like algorithms, Association rule mining, Itemsets, Databases, MTR-FMA, Frequent itemsets, Frequency, Marketing and sales, Artificial intelligence]
A trust-based distributed data fault detection algorithm for wireless sensor networks
2008 11th International Conference on Computer and Information Technology
None
2008
Fault detection is a difficult and complex task in WSN because of there are many factors that influence data and could cause faults. Large-scale sensor networks impose energy and communication constraints, thus it is difficult to collect data from each individual sensor node and process it at the sink to detect faulty sensors. The proposed approach saves energy and improves network lifetime by detecting data faults locally in cluster head and therefore reducing the number of transmissions required to convey relevant information to the sink. This paper presents a novel approach for detecting sensors which produce faulty data in a distributed way as well as identifying the type of data faults using trust concepts to gain a high degree of confidence. We validate our method with simulations results.
[Base stations, fault diagnosis, Event detection, wireless sensor networks, Data engineering, large-scale sensor networks, Wireless Sensor Networks, Distributed computing, Trust, Counting circuits, Distributed Fault Detection, Fault diagnosis, Wireless sensor networks, security of data, Fault detection, trust-based distributed data fault detection algorithm, Computer networks, Reputation, IP networks]
Variability model and management for web service: A petri net based approach
2008 11th International Conference on Computer and Information Technology
None
2008
Variability management is an efficient solution to support service's reuse, but most of the existing variability models pay attention to service's functions and care little about the service's behavior that how these functions are invoked. In this paper, we provide a variability model based on Petri net that both the variable functions and the invocation constraint of these functions are handled. We use general service to refer the service developed with variability for reuse; by variability binding service instance will be generated for use. Variable functions are modeled by transitions of Petri net; the service behavior is shown by the firing of transitions.
[Petri net based approach, variability management, Laboratories, Petri nets, Web service management, Distributed computing, variable functions, Technology management, Runtime, Web services, Web and internet services, software reusability, Concrete]
Feasibility of PKC in resource-constrained wireless sensor networks
2008 11th International Conference on Computer and Information Technology
None
2008
In this paper we present a detailed review of the works on public key cryptography (PKC) in wireless sensor networks (WSNs). In the early days of sensor networks, public key cryptography was thought to be completely unfeasible considering its computational complexity and energy requirements. By this time, several works have proved that the lightweight versions of many well-known public key algorithms can be utilized in WSN environment. With the expense of a little energy, public key based schemes could in fact be the best choice for ensuring data security in high-security demanding WSN applications. Here, we talk about the notion of public key cryptography in WSN, its applicability, challenges in its implementation, and present a detailed study of the significant works on PKC in WSN.
[telecommunication security, wireless sensor networks, Data security, data security, PKC, Paper technology, resource-constrained wireless sensor network, Distributed computing, Computational complexity, Wireless sensor networks, WSN, public key cryptography, Public key, Writing, Public key cryptography, Hardware, IP networks, energy requirement, computational complexity]
A calculus for composite authorities' policy derivation in shared domains of pervasive computing environments
2008 11th International Conference on Computer and Information Technology
None
2008
The decentralized security management in a pervasive computing environment, requires apportioning the environment into several security domains. In each security domain, an administrator (we call it authority) is responsible for specifying the security policies of the domain. Overlapping of security domains results in the requirement of cooperative security management in the shared/ overlapping domains. To satisfy this requirement, we propose an abstract security model, as well as its supplementary calculus of composite authorities. The security model is based on deontic logic and is independent of the domains' heterogeneity. The model's policy language (we call it MASL) enables multiple authorities to specify their domain policies, including obligations and authorizations. Our proposed calculus of composite authorities, enables the security system to infer policy statements of composite authorities from the cooperating primitive authorities. The calculus offers three styles of cooperative administration including collaborative, disjunctive, and delegative administration. Abstraction and automated composite authorities' policy derivation are the main advantages of the proposed logical model.
[Pervasive computing, Access control, Logic programming, authorities policy derivation, cooperative administration, Ontologies, deontic logic, authorizations, Calculus, ubiquitous computing, pervasive computing, Distributed computing, Environmental management, Authorization, security of data, Collaboration, supplementary calculus, cooperative security management, Computer security, decentralized security management]
A Grid-enabled framework of expertise search engine using Web-based online communities
2008 11th International Conference on Computer and Information Technology
None
2008
Existing search engines like Google, Yahoo!, Live etc. are not yet capable to answer queries that require deep semantic understanding of the query or the document. Instead, it is preferable to find and ask someone who has related expertise or experience on a topic and thus Web-based online communities have become important places for people to seek and share expertise. We need to gather the data that describes these online communities. But extracting, aggregating and analyzing data from these communities for finding experts on a single framework is a challenging task. Also there is no universal standard data structure for the outline of user participation in these communities. In this paper, we present a Grid-enabled framework of expertise search (GREFES) engine which utilizes online communities as sources for experts on various topics. This is a work in progress. We also propose an open data structure called SNML (Social Network Markup Language) to outline user participation in online communities. The architecture addresses major challenges in crawling of community data and query processing by utilizing Grid. An expert ranking algorithm is also discussed and evaluated.
[Data analysis, search engines, Social network services, grid computing, Data structures, SNML open data structure, Distributed computing, community data crawling, query processing, expertise search engine, social network, grid-enabled framework, Query processing, Social Network Markup Language, Search engines, Grid computing, social networking (online), Computer networks, data structures, expert ranking algorithm, IP networks, Online Communities/Technical Collaboration, Web-based online community]
Extensive video quality evaluation: A scalable video testing platform
2008 11th International Conference on Computer and Information Technology
None
2008
With the advent of new upcoming online video services such as IPTV, video on demand (VoD) and peer-to-peer (P2P) video streaming, content providers are gaining more and more interest in measuring and monitoring video quality as perceived by end-users; also known as quality of experience (QoE). Objective video quality metrics provide a means of measuring visual quality degradations but in order to be able to measure QoE, these objective metrics should incorporate all quality affecting parameters such as encoding bitrate, network impairments and error concealment techniques. As a consequence, in order to construct or validate a proper objective video quality metric, extensive video evaluation tests must be performed. In this paper we present a scalable video testing platform that simplifies the management and execution of such video quality evaluation tests. Results indicate that the use of our testing platform drastically reduces overall experiment duration.
[Video on demand, program testing, Peer to peer computing, objective video quality metrics, video quality evaluation, Encoding, IPTV, scalable video testing platform, test execution automation, quality-of-experience, Degradation, visual quality degradation measurement, online video service, Bit rate, Streaming media, Gain measurement, video streaming, video signal processing, Monitoring, management software, Testing]
Disassembling SLAs for follow-up processes in an SOA system
2008 11th International Conference on Computer and Information Technology
None
2008
This work presents a novel scheme for Service Level Agreements (SLAs) in a service-oriented architecture (SOA). An SLA is a contract that guarantees quality of service (QoS) between service providers and consumers. An SLA can be split into several parts for an SOA system which is consisting of different services. A split SLA is used to guarantee quality of services for one of services in an SOA system. However, these contract rules do not regulate follow-up process in services. SLA violations may occur consequently. For instance, interactions between services are not encrypted because the encryption parameter isn't included in the split SLA contract. Confidential data would be transmitted through unsafe channels. There would be a security issue of SLA Management for follow-up processes in an SOA system. The novel scheme in this work addresses these problems. Using SLAs management proposed in this work can identify the services that consumer needs and split the contracts for follow-up services. SLAs are not only guaranteed between service providers and consumers but also assured in follow-up processes. The proposed method improves the security and completeness of SLA using in an SOA system.
[service level agreements, Data security, Service oriented architecture, Quality of service, quality of service, Distributed computing, Semiconductor optical amplifiers, Jacobian matrices, software architecture, Web services, security of data, SOA system, Internet, Service-oriented architecture, Cryptography, security issue, Service Level Agreement, Contracts, service-oriented architecture]
A history based semantic aware access control model using logical time
2008 11th International Conference on Computer and Information Technology
None
2008
With the advent of semantic technology, access control cannot be done in a safe way unless the access decision takes into account the semantic relationships among the entities in a semantic-aware environment. The SBAC model (semantic based access control model) considers this issue in its decision making process. However, time plays a crucial role in new computing environments, which is not supported in SBAC. In this paper, we propose the temporal semantic based access control (TSBAC) model, as an extension of SBAC, which enhances the specification of user-defined authorization rules by constraining time interval and temporal expression over users' history of accesses. TSBAC uses logical time, rather than to real time, in its authorization rules. A formal semantics for temporal authorizations is provided and conflicting situations (due to the semantic relations of the SBAC model and a sub-interval relation between authorizations) are investigated and resolved in our proposed model. An architecture for the access control system based on TSBAC is presented.
[Access control, semantic Web, temporal logic, Control systems, logical time, History, Distributed computing, formal specification, Authorization, formal semantics, authorisation, decision making, Permission, Computer networks, IP networks, user-defined authorization rule, Protection, Computer security, history based temporal semantic aware access control model]
A new adaptive routing approach based on Ant Colony Optimization (ACO) for Ad hoc Wireless Networks
2008 11th International Conference on Computer and Information Technology
None
2008
The goal of this work is to design a new adaptive routing technique for ad hoc wireless networks. This paper proposed the basic deign of the algorithm that works based on the principle of ant colony optimization (ACO). This is a probabilistic adaptive technique that changes its routes with the change of network topology over the period of time by learning its environment. It identifies appropriate paths with the feedback of previously travelled packets and maintains routing table accordingly. A self-made simulator implemented on C++ is used to evaluate performance of this algorithm on the basis of diverse adaptive issues such as change of probability, growth of pheromone intensity, randomness of the selection and packet sending rate through different paths.
[Adaptive systems, Routing Protocol, Distributed computing, Mobile ad hoc networks, optimisation, Wireless networks, ant colony optimization, Bandwidth, Routing protocols, probabilistic adaptive technique, IP networks, Ant Colony Optimization, Ant colony optimization, mobile radio, C++, adaptive routing, Ad hoc Network, telecommunication network topology, Educational institutions, ad hoc wireless networks, network topology, Multicast algorithms, MANET, telecommunication network routing, Adaptive System, ad hoc networks]
Capacity based channel assignment in multi-interface wireless mesh networks
2008 11th International Conference on Computer and Information Technology
None
2008
To utilize the available frequency channel space in wireless mesh networks (WMNs), recent work has significantly been focusing on the channel assignment in multi-interface and multi-channel wireless mesh network. This paper presents our design of a distributed channel assignment algorithm that assigns channels based on capacity of the channels. It is shown that the performance of the network is significantly improved when interference and capacity are considered to choose a channel. Simulation results show that our channel assignment algorithm performs better than a similar channel assignment algorithm which is distributed and based only on interference.
[radio networks, multi-interface wireless mesh network, Channel capacity, Stability, distributed channel assignment algorithm, capacity based channel assignment, Interference, channel capacity, Throughput, Routing, Distributed computing, frequency channel space, radiofrequency interference, Wireless mesh networks, telecommunication network routing, Bandwidth, Computer networks, wireless channels, interference, IP networks, channel allocation]
Anycast routing in Delay Tolerant Networks using genetic algorithms for route decision
2008 11th International Conference on Computer and Information Technology
None
2008
Delay/Disruption Tolerant Networking (DTN) architecture adopts the concept of intermittent networks that may suffer frequent disconnections, with the possibility of never having end-to-end connectivity between the source and destination over a given period of time. Therefore, the design of protocols for those networks becomes a unique challenge. Routing in DTNs is one of the key components and remains open for discussion. In this paper we treat the routing for anycast delivery that is useful in situations where a host, applications or users wish to locate a host which supports a particular service. However, if several servers support the service, any of these servers can be used. Then we implement a routing algorithm using genetic algorithm (GA), because the route and destination decision influence many parameters simultaneously. Our simulation results have shown that the routing using GA performs better than the shortest path algorithm.
[Disruption tolerant networking, shortest path algorithm, Protocols, Delay effects, anycast routing, Routing, delay tolerant networks, Partitioning algorithms, genetic algorithms, Distributed computing, Genetic algorithms, intermittent networks, telecommunication network routing, multicast communication, route decision, Genetic engineering, end-to-end connectivity, Internet, IP networks]
LCRACO - A new load and congestion controlled routing based on ant colony optimization
2008 11th International Conference on Computer and Information Technology
None
2008
Designing efficient, cost-effective routing solutions for the infrastructure-less MANETs remains a daunting task till date. A proven optimization technique, the Ant colony optimization has been used in this paper to find the optimum in terms of the shortest route in a MANET. Special attention has been given to the load balancing and congestion control in the network. The simulated performance graphs show an improved congestion control for the proposed routing protocol.
[Ant colony optimization, mobile ad hoc network, mobile radio, telecommunication congestion control, load balancing, congestion controlled routing, Control systems, Distributed computing, Mobile ad hoc networks, cost-effective routing solution, Bridges, optimisation, MANET, resource allocation, ant colony optimization, Clustering algorithms, telecommunication network routing, Broadcasting, Load management, Routing protocols, Internet, ad hoc networks]
Exploring wireless device driver vulnerabilities
2008 11th International Conference on Computer and Information Technology
None
2008
IEEE 802.11 set of standards grew from a highly obscured and unsecured technology to a robust and reliable solution for many of us. Broad acceptance and relatively simple implementation also made 802.11 a perfect target for possible exploitation. Recently discovered wireless driver vulnerabilities exposed a new critical threat that legitimate users community is facing. What makes this vulnerability so dangerous to many of us is the fact that the actual exploitation cannot be prevented by commonly used tools such as firewalls, antivirus, and intrusion detection system's programs. Access to the victims machine is gained through a flawed third-party wireless driver, giving full kernel level access rights to the attacker. While kernel-level exploitation domain covers everything running beneath the operating systems, this paper describes wireless device drivers vulnerability and proposes methods for vulnerability mitigations. This paper would assist IT professionals in evaluating, creating, and assessing security requirements associated with wireless networks connectivity.
[telecommunication security, operating system kernels, network security, IEEE 802.11 standard, operating system, device drivers, wireless driver, Knowledge management, vulnerability, Communication system security, Distributed computing, kernel-level exploitation domain, Computer science, Wireless sensor networks, wireless device driver vulnerability, security, Information security, Spread spectrum communication, Internet, Resource management, wireless LAN, Protection, 802.11]
Energy-efficient TDMA MAC protocol for wireless sensor networks applications
2008 11th International Conference on Computer and Information Technology
None
2008
The availability of low-powered and cheap microprocessors, radio frequency integrated circuits and the development of new wireless communication techniques, make the wireless sensor networks (WSN) one of todays most promising technologies. Minimizing energy consumption and maximizing the lifetime of the networks are key requirements in the design of sensor network applications. Optimally designed medium access control (MAC) and routing protocols minimize energy consumption and prolong the network life. In this study, we have investigated an energy-efficient adaptive TDMA (EA-TDMA) protocol for railway applications that used in communication between sensor nodes and the cluster-head (CH) placed in a railway wagon. This protocol is suitable for medium traffic applications and reduces energy consumption by shortening the idle period when devices have no data to transmit. We have developed an analytical model for EA-TDMA and compared its performance with conventional TDMA and bit-map-assisted (BMA) protocols.
[railway rolling stock, microprocessor, Energy consumption, railway communication, wireless sensor networks, Wireless application protocol, wireless sensor network, Wireless sensor networking, access protocols, Time division multiple access, Microprocessors, Rail transportation, radio frequency integrated circuit, radiofrequency integrated circuits, Access protocols, railway application, medium access control, routing protocol, Wireless sensor networks, energy consumption minimization, energy-efficient TDMA MAC protocol, time division multiple access, routing protocols, bit-map-assisted protocol, MAC protocol, Media Access Protocol, Energy efficiency, Radiofrequency integrated circuits, railway wagons, energy-efficiency, telecommunication traffic]
Packing identical circles in a minimized circular container by Monotonic Basin Hopping heuristic approach
2009 12th International Conference on Computers and Information Technology
None
2009
Packing problems have mathematical as well as practical application point of interest. We present, here, Monotonic Basin Hopping heuristic approach to solve the problem of packing identical circles within a minimum size of circular container. Extensive computational experiments have been performed for analyzing the problem as well as for choosing an appropriate way the parameter values for the proposed methods. Several improvements with respect to the best results reported in the literature have been detected.
[Shape, Computer aided manufacturing, Containers, monotonic basin hopping heuristic approach, Mathematics, Application software, History, Circle Packing, Information technology, bin packing, Monotonic Basin Hopping, Multistart, packing problems, identical circles packing, Computer science, Production, containers, Performance analysis, minimisation, circular container]
Multiple-case outlier detection in least-squares regression model using quantum-inspired evolutionary algorithm
2009 12th International Conference on Computers and Information Technology
None
2009
In ordinary statistical methods, multiple outliers in least-squares regression model are detected sequentially one after another, where smearing and masking effects give misleading results. If the potential multiple outliers can be detected simultaneously, smearing and masking effects can be avoided. Such multiple-case outlier detection is of combinatorial nature and 2N -1 sets of possible outliers need to be tested, where N is the number of data points. This exhaustive search is practically impossible. In this paper, we have used quantum-inspired evolutionary algorithm (QEA) for multiple-case outlier detection in least-squares regression model. An information criterion based fitness function incorporating extra penalty for number of potential outliers has been used for identifying the most appropriate set of potential outliers. Experimental results with four datasets from statistical literature show that the QEA effectively detects the most appropriate set of outliers.
[ordinary statistical method, Data analysis, least squares approximations, Statistical analysis, fitness function, Evolutionary computation, regression analysis, Predictive models, multiple-case outlier detection, least-squares regression, information criterion, Information technology, Equations, Computer science, Strontium, least-squares regression model, evolutionary computation, Quantum computing, Information criterion based fitness function, quantum-inspired evolutionary algorithm, learning (artificial intelligence), Testing]
Hybrid Real-coded Quantum Evolutionary Algorithm based on particle swarm theory
2009 12th International Conference on Computers and Information Technology
None
2009
This paper proposes a hybrid real-coded quantum evolutionary algorithm (HRCQEA) for optimizing complex functions on the basis of the concept of quantum computing such as qubits and superposition of states and particle swarm optimization (PSO). It combines PSO with real-coded quantum evolutionary algorithm (RCQEA) to improve the performance of RCQEA. The main idea of HRCQEA is to embed the evolutionary equation of PSO in the evolutionary operator of RCQEA. In HRCQEA, each triploid chromosome represents a particle and the position of the particle is updated using complementary double mutation operator (CDMO) and quantum rotation gate (QRG), which can make the balance between exploration and exploitation. Discrete crossover (DC) is employed to expand the search space and Hill-climbing selection (HCS) helps to accelerate the convergence speed. Simulation results of four benchmark complex functions with high dimensions show that HRCQEA performs better than other algorithms in terms of ability to discover of global optimum.
[particle swarm optimisation, Genetic mutations, global optimum, Evolutionary computation, particle swarm theory, Biological cells, triploid chromosome, Convergence, discrete crossover, Quantum computing, qubits, quantum rotation gate, search space, hybrid real-coded quantum evolutionary algorithm, Hill-climbing selection, Quantum Evolutionary Algorithm, convergence of numerical methods, particle swarm optimization, Particle swarm optimization, Information technology, Equations, Particle Swarm Algorithm, Computer science, Evolutionary Algorithm, evolutionary computation, Hybrid Algorithm, Quantum mechanics, quantum computing, benchmark complex functions, complementary double mutation operator]
A reduced complexity message passing algorithm with improved performance for LDPC decoding
2009 12th International Conference on Computers and Information Technology
None
2009
In this paper, a simplified message passing algorithm for decoding low-density parity-check (LDPC) codes is proposed with a view to reduce the implementation complexity. The algorithm is based on simple hard-decision decoding techniques while utilizing the advantages of soft channel information for improvement in decoder performance. The algorithm has been validated through simulation using LDPC code compliant with wireless local area network (WLAN -IEEE 802.11n) standard. The results show that the proposed algorithm can achieve significant improvement in bit error rate (BER) performance and average decoding iterations compared to fully hard-decision based decoding algorithms.
[Wireless LAN, iterative methods, Bit error rate, parity check codes, hard-decision decoding technique, Sparse matrices, communication complexity, Iterative decoding, bit error rate performance, Digital communication, Parity check codes, Block codes, Low-Density Parity-Check, error statistics, reduced complexity message passing algorithm, message passing, low-density parity-check code decoding, WLAN-IEEE 802.11n standard, decoding, wireless local area network, Message passing, average decoding iterations, Iterative algorithms, Error correction coding, wireless LAN, Signal to noise ratio, Digital video broadcasting]
A two-layer hierarchical permission based mutual exclusion algorithm
2009 12th International Conference on Computers and Information Technology
None
2009
Due to the growing application of peer-to-peer computing, the distributed applications are continuously spreading over extensive number of nodes. To cope with this large number of participants, various cluster based hierarchical solutions have been proposed. Cluster based algorithms are scalable by nature. Several of them are quorum based solutions. All of these solutions exploit the idea of coordinator/leader of cluster. Thus, fault tolerance of these algorithms is low. If any coordinator fails, election of new one is required. Here we propose a two-layer hierarchical cluster based solution where no coordinator is used. We simulate our proposed algorithm and show that it outperforms related ME algorithms.
[Hierarchical, Protocols, Quorum, peer-to-peer computing, fault tolerance, Distributed Algorithm, Nominations and elections, Cluster, two-layer hierarchical permission, Application software, Distributed computing, Information technology, distributed applications, Computer science, Fault tolerance, pattern clustering, Clustering algorithms, Computer applications, Permission, Mutual Exclusion, mutual exclusion algorithm, cluster based algorithms]
On the fast computation of decimal logarithm
2009 12th International Conference on Computers and Information Technology
None
2009
The paper presents a new and fast algorithm to efficiently compute radix-10 logarithm of a decimal number. The algorithm uses 32-bit floating-point arithmetic, and is based on a digit-by-digit iterative computation that does not require look-up tables, curve fitting, decimal-binary conversion, or division operations; the number of iterations depends on the user defined precision. The algorithm produces error-free (infinite precision) results up to 7 decimal digits. A numerical example is shown for the purpose of illustration. The accuracy is analyzed for several decimal digits showing compliance with the IEEE 754-2008 standard. When implemented on to the Xilinx VirtexII FPGA, the architecture costs only 1,053 logic cells, runs at a maximum frequency of 44 MHz, and consumes 79 mW of power.
[Costs, field programmable gate arrays, digit-by-digit iterative computation, decimal logarithm, radix-10 converter, floating point arithmetic, lookupt tables, decimal-binary conversion, table lookup, Computer architecture, Decimal logarithm, floating-point arithmetic, Hardware, Performance analysis, iterative computation, Application software, Information technology, Floating-point arithmetic, radix-10 logarithm, Xilinx VirtexII FPGA, error-free production, Iterative algorithms, curve fitting, IEEE 754-2008 standard, IEEE754-2008, Curve fitting, Field programmable gate arrays]
Central Base-Station Controlled Density Aware Clustering Protocol for wireless sensor networks
2009 12th International Conference on Computers and Information Technology
None
2009
As wireless sensor networks are equipped with sensor nodes which have a limited energy and sensing capabilities, a good routing protocol must be designed to make the network energy efficient. In this paper, we propose a centralized routing protocol called Central Base Station Controlled Density Aware Clustering Protocol (CBCDACP) where the base station centrally performs the cluster formation task. In this protocol, an optimum set of cluster heads are selected by using a new cluster head selection algorithm focusing on both the density of the sensor nodes and the minimum distances among the cluster head and its neighbor nodes. The performance of CBCDACP is then compared with some prevalent clustering-based schemes such as Low Energy Adaptive Clustering Hierarchy (LEACH), Centralized LEACH (LEACH-C). Simulation results show that CBCDACP can improve system life time and energy efficiency in terms of different simulation performance metrics over its comparatives.
[wireless sensor networks, Wireless application protocol, centralized routing protocol, setup phase, Batteries, central base-station controlled density aware clustering protocol, low energy adaptive clustering hierarchy, Centralized control, Temperature sensors, sensor nodes, energy efficiency, density factor, Acoustic sensors, cost function, cluster head selection algorithm, Routing protocols, Computer networks, network energy, Base stations, cluster formation task, centralized LEACH, system life time, sensor density, Wireless Sensor Networks, steady-state phase, Wireless sensor networks, routing protocols, Energy efficiency]
Performance analysis of DS-CDMA under perfect and imperfect power control
2009 12th International Conference on Computers and Information Technology
None
2009
CDMA refers to multiple access method in which the individual terminals uses spread spectrum techniques and occupy the entire spectrum whenever they transmit. This feature makes CDMA different from FDMA and TDMA. In the wireless communication, the signal to interference ratio (SIR) and bit error rate (BER) are the predominant parameter that characterizes the system performance. This paper presented here standard Gaussian approximation (SGA) methods presented in the international literature concerning the computation of the SIR and the BER in DS-CDMA systems under perfect and imperfect power control over fading and non-fading channel. The content and conclusions of this paper have driven to take many important decisions by varying different DS-CDMA communication parameters such as processing gain, number of interfering cells, multipath components etc. using SGA techniques. As SGA is analytically developed and is very computationally efficient solution for the system performance estimate in terms of SIR and BER, it helps us to avoid the tedious and cost-inefficient simulations.
[Bit error rate, Gaussian distribution, spread spectrum communication, Multiaccess communication, multiple access method, spread spectrum, Wireless communication, DS-CDMA, Time division multiple access, fading channels, System performance, Spread spectrum communication, Performance analysis, Gaussian channels, error statistics, standard Gaussian approximation, fading channel, Interference, CDMA, code division multiple access, nonfading channel, bit error rate, SGA, signal to interference ratio, BER, Frequency division multiaccess, Power control, radiocommunication, power control, SIR]
Capacity enhancement of limited user traffic of mobile cellular networks using DOVE technique
2009 12th International Conference on Computers and Information Technology
None
2009
In this paper, a simple scheme is presented to improve the performance of mobile cellular networks by using delay of voice end user (DOVE) to the new originating calls over handoff calls in a two-dimensional (2D) traffic model for finite number of users. Expressions for probability of forced termination of handoff calls and the blocking probability of new originating calls have been derived. From this study, it has been found that the probability of forced termination of handoff calls is significantly reduced due to the incorporation of the delay of voice end user compared to the case when no such delay of voice end user is used in the system.
[capacity enhancement, blocking probability, Costs, mobile radio, 2D traffic model, Limited user traffic, mobile cellular networks, new originated calls, Telecommunication traffic, Information technology, handoff calls, Wireless communication, Analytical models, Land mobile radio cellular systems, Distributed databases, voice end user delay, Load management, Computer networks, DOVE, Mobile computing, telecommunication traffic, cellular radio]
A new hashing and caching approach for reducing call delivery cost and location server's load in wireless mobile networks
2009 12th International Conference on Computers and Information Technology
None
2009
This paper proposes a new approach for reducing average call delivery cost and location server's load of wireless mobile networks. It uses caches whose up-todate information is responsible for dropping these costs and these caches are updated not only during call arrival moment from the calling mobile hosts (MHs) but also during call receiving moment to those MHs. To achieve load balancing among replicated home location registers (HLRs), hashing technique is also used and this load is also affected by up-to-date cache information. The analytical model and experimental results show that our proposed method prepares the cache with up-to-date information more frequently with the increase of average call arrival rate as well as average call receiving rate. This increases probability of finding MH's location as well as hit ratio of the cache. As a result, both the average call delivery cost and load on a particular HLR are minimized considerably than all other previous approaches.
[Costs, telecommunication network management, load balancing, location server load, Roaming, Wireless communication, Visitor Location Resister (VLR), Analytical models, hashing technique, mobile hosts, Distributed databases, call arrival rate, Computer networks, call receiving moment, Call arrival and receiving rate, cost reduction, home location registers, mobile radio, caching technique, Information technology, Computer science, wireless mobile networks, Home Location Register (HLR), call delivery cost, Load management, hashing and caching, call arrival moment, Mobile computing, call receiving rate, call delivery cost reduction]
A modified MAP in performance evaluation of asynchronous packet traffic
2009 12th International Conference on Computers and Information Technology
None
2009
Markov arrival process (MAP) is the most widely used traffic model to analyze performance of a node of an asynchronous packet traffic. Recent literatures make transition between overload and underload states of a node without arrival but sometimes transition may take place from underload to overload with an arrival which deviates our analysis from the present literature of Markov modulated Poisson process (MMPP) traffic model. In this paper two different rates of packet `arrival' and `non arrival' is incorporated in state transition of underload to overload and a comparison is made with previous literatures.
[MMPP, signal processing, Stochastic processes, Telecommunication traffic, packet non arrival, state transition, State-space methods, asynchronous packet traffic performance evaluation, Information technology, Switching circuits, Computer science, Markov arrival process, sojourn time and transition probability matrix, DH-HEMTs, PH distribution, Traffic control, Probability density function, Markov processes, Generator matrix, packet arrival rate, MAP, telecommunication traffic]
Impact of optical fiber dispersion and self phase modulation on the performance of DS-OCDMA
2009 12th International Conference on Computers and Information Technology
None
2009
The performance of direct sequence optical code division multiple access with cascaded optical amplifiers is analytically investigated in presence of fiber group velocity dispersion (GVD) and self phase modulation (SPM). In our analysis, Gaussian-shaped optical orthogonal codes are employed as address sequence and avalanche photodiode is used in an optical correlator receiver. The signal to noise power for the proposed system is evaluated on account of receiver, optical amplifier and multiuser access interference noises. The system performance is determined as a function of optical signal power, code length, code weight, number of simultaneous users, and fiber length. The power penalty suffered by the system is evaluated at bit error rate (BER) of 10-9. The numerical results show that the BER performance of the system is highly dependent on the signal input power, bit rate, and fiber length. It is found that the performance of the proposed system can be improved by the combined effect of GVD and SPM.
[signal to noise power, optical amplifier, optical correlator receiver, Bit error rate, self-phase modulation, Optical fiber dispersion, optical correlation, Semiconductor optical amplifiers, orthogonal codes, SPM, avalanche photodiodes, optical fiber group velocity dispersion, Optical modulation, self phase modulation, error statistics, Optical fibers, avalanche photodiode, Phase modulation, IM/DD, Scanning probe microscopy, DS-OCDMA performance, Optical receivers, code division multiple access, cascaded optical amplifiers, Gaussian-shaped optical orthogonal codes, bit error rate, direct sequence optical code division multiple access, DS-OCDMA, Stimulated emission, GVD, BER performance, optical fibre dispersion, optical fibre amplifiers, Optical noise, multiuser access interference noises]
Migration to the next generation passive optical network
2009 12th International Conference on Computers and Information Technology
None
2009
Due to the rapid growth of Internet with new generation of services and applications, demand for faster and cheaper access network has been rising. To address the present and future demand, broadband fiber access technologies such as passive optical networks (PONs) are a potential solution. Mostly, time division multiplexed (TDM)-PON is deployed in all parts of the world. In order to mitigate the future demand, some next-generation PON systems have been investigated by the researchers. In this paper, we examine the current status of PONs and investigate the probable future PONs. We also explain the smooth migration process from the current status to the future technologies. Architecture of a self-restored tree-type hybrid wavelength division multiplexed/TDM-PON (WDM/TDM-PON) has been proposed, for migrating from TDM to WDM-PON. Due to the restorable capacity of the architecture, the availability of the system is increased. In addition, cost analysis of different PON architectures are performed and compared with the cost of the proposed architecture. It is found that, the proposed architecture provides more cost effective solution.
[FTTXS, wavelength division multiplexing, Costs, time division multiplexed-PON, Optical fiber networks, Time division multiplexing, broadband fiber access technologies, Passive optical networks, Next generation networking, self-restored tree-type hybrid wavelength division multiplexing, Web and internet services, access network, Performance analysis, subscriber loops, IP networks, TDM, WDM/TDM-PON, Availability, WDM, Wavelength division multiplexing, smooth migration process, PON, optical fibre networks, BSWDM FILTER, Internet, next generation passive optical network]
Raman gain spectrum equalization by using Polarization-diversity Loop Filter
2009 12th International Conference on Computers and Information Technology
None
2009
Raman amplifier plays an important role in wavelength division multiplexing (WDM) systems. The drawback of Raman amplifier is that the Raman gain spectrum is flat for very small narrowband. So, it is necessary to equalize the Raman gain spectrum to achieve the desire performance for the WDM systems. This paper has proposed an extrinsic gain equalization technique by using polarization-diversity loop filter. It is found that by using PDLF, an equalize bandwidth of 90nm (1550nm - 1640nm) can be achieved with a gain ripple of 0.5dB.
[Disruption tolerant networking, Polarization, wavelength division multiplexing, gain ripple, PDLF, Floods, Delay, optical fibre polarisation, Raman amplifier, Raman gain spectrum, optical fibre filters, optical communication equipment, Routing protocols, Computer networks, polarization-diversity loop filter, Informatics, WDM, Peer to peer computing, extrinsic gain equalization, Information technology, Raman gain spectrum equalization, optical fibre amplifiers, Resource management, Raman lasers, FRAs]
A probabilistic position-based routing scheme for delay-tolerant networks
2009 12th International Conference on Computers and Information Technology
None
2009
Observably, participants in realistic scenarios repeatedly navigate specific locations based on routine behavior, leading to inherently structured movement patterns. In this paper we propose a delay-tolerant routing scheme, called Probabilistic Routing with Minimum Proximity (PRMP), which aims to utilize prior movement patterns of peers to predict future probability of forwarding messages to a location proximal to a destinations home address. A source considers next-hop forwarding based on a probabilistic benefit-metric; which takes into account a nodes frequented trajectories and current position, its spatial distance from a destinations stationary home location and the probability of any of its immediate trajectories minimizing the spatial distance to the destinations home. Delivering a message to a nodes' home address in the network is synonyms to delivering mail to an individual's designated mailbox. The protocol avoids flooding completely in efforts to optimize use of network resources. Simulations of PRMP reflect low buffer occupancy at both high and low loads in the network. It also maintains resource optimization in varying node densities compared to two prominent DTN flooding protocols - Epidemic and PRoPHET.
[Disruption tolerant networking, mobility management (mobile radio), Routing Protocol, Floods, node densities, delay-tolerant routing scheme, intermittent connectivity, Delay, inherently structured movement patterns, Intermittent Connectivity, forwarding messages, Location-based Routing, minimum proximity, Routing protocols, Computer networks, Informatics, Epidemic, Navigation, Peer to peer computing, routine behavior, home address, probability, probabilistic routing, probabilistic position-based routing scheme, delay-tolerant networks, Information technology, DTN flooding protocols, resource optimization, next-hop forwarding, PRoPHET, routing protocols, network resources, buffer occupancy, stationary home location, Resource management, probabilistic benefit-metric, DTN]
Fair slots assignment mechanisms of IEEE 802.11 networks for multiple access points
2009 12th International Conference on Computers and Information Technology
None
2009
This paper is a research work to find out an efficient algorithm for slots assignments of multiple access points (APs) of IEEE 802.11 wireless LANs in a framework for seeking an approach which balances between quality of services (QoS) and fairness while achieving maximum network throughput. The explosive growth of wireless systems coupled with the extensive use of laptop and palmtop computers indicate bright future for wireless networks both as standalone systems and as part of large networking infrastructure. IEEE 802.11 wireless LAN has been improving in terms of data rates, radio services, security etc. ever since. Now-a-days WLAN is an essential part for providing both real time and non-real time services to a massive mobile users. But there are still lacking for WLAN to provide both QoS guarantee and fairness which makes it inappropriate for real time applications such as voice and video services. We analyze for a new framework which based on centralized coordination of APs, is a complementary to the emerging 802.11e standard for QoS and guarantee to overcome the hidden node and overlapping cell problems. We simulate three algorithms for slots assignment of multiple APs during contention-free period (CFP) of IEEE 802.11 point coordination function (PCF) medium access control (MAC) mode for ensuring the QoS, inter-AP fairness with maximizing the network throughput.
[IEEE 802.11 networks, Wireless LAN, Portable computers, Quality of service, Throughput, 802.11e, Communication system security, Point Coordination Function (PCF), laptop computers, disk location, Quality of Service (QoS), Wireless networks, QoS, frequency etc., Wireless LAN (WLAN), Computer networks, multi-access systems, mobile users, Personal digital assistants, Data security, palmtop computers, multiple access points, contention-free period, medium access control, wireless networks, quality of service, Distribution Coordination Function (DCF), MAC, Fairness, telecommunication standards, fair slots assignment mechanisms, point coordination function, Access Points (APs), Explosives, wireless LAN]
Performance evaluation of heterogeneous network for next generation mobile
2009 12th International Conference on Computers and Information Technology
None
2009
It is a universally stated design requirement that next generation mobile systems will be compatible and inter-operable with IPv6 and with various access technologies such as 802.11x. The current growth of WLANs worldwide has yielded a demand to integrate with existing 3G mobile technologies. Interworking incorporates all of the best features of an individual network into a single integrated system thus providing ubiquitous data services with high data rates in WLAN hotspots. The attempt to build hybrid networks has been linked with many technical challenges such as seamless vertical handovers across WLAN/3G radio technologies, security, common authentication, unified accounting &amp; billing, etc. This paper evaluates the performance of two 3G/WLAN integration schemes: Tight and Loose Coupling. Mobile IP is used as a mobility management scheme and EAP-AKA for common authentication.
[Wireless LAN, Packet switching, Mobile IP, open systems, 3G mobile communication, Telecommunication traffic, VOIP Codec, loose coupling, WLAN, 3G mobile systems, EAP-AKA, Next generation networking, Switching circuits, next generation mobile systems, 3G, heterogeneous network, Authentication, Computer networks, IP networks, wireless LAN, Mobile computing, IPv6 networks, tight coupling]
Performance analysis of a MIMO-OFDM wireless link with STBC in the presence of fading and timing jitter
2009 12th International Conference on Computers and Information Technology
None
2009
The performance of a wireless communication system is analyzed considering multi-carrier OFDM with Multiple- Input-Multiple-Output (MIMO) wireless channel and Space-Time Block Coding (STBC). Bit Error Rate (BER) expression for MIMO-OFDM system without and with STBC is presented considering fading and timing jitter with QPSK, DPSK and DQPSK modulation schemes. The performance results are evaluated without and with rate ¿ convolution coding with hard decision decoding. The results are presented in terms of BER, power penalty due to fading and coding gain due to error correction coding. It is noticed that there is significant power penalty due to fading and can be reduced by increasing the number of receiving antennas. Among the different modulation schemes QPSK is found to provide the best performance.
[convolution coding, differential phase shift keying, OFDM, STBC, Bit error rate, Diversity, quadrature phase shift keying, multiple input multiple output, DQPSK modulation, Wireless communication, fading channels, QPSK modulation, DPSK modulation, error correction coding, block codes, Differential quadrature phase shift keying, hard decision decoding, MIMO, OFDM modulation, wireless channel, Performance analysis, Block codes, MIMO communication, error statistics, Fading, Timing jitter, Rayleigh and Rician fading, timing jitter, error correction codes, bit error rate, fading, decoding, space-time codes, Quadrature phase shift keying, wireless communication system, multicarrier OFDM, convolutional codes, space-time block coding, power penalty]
A compact loop type antenna for millimeter band applications
2009 12th International Conference on Computers and Information Technology
None
2009
A millimeter band microstrip patch antenna was developed to achieve a bandwidth of 2GHz from 58.995 GHz to 60.995 GHz with linear polarization capability. In this paper the key configuration of the antenna consists of two loops having an opening at opposite side. The bandwidth of the antenna is enhanced by taking the advantage of this double loop arrangement and the mutual coupling between the inner and outer loops. In this paper, properties of the proposed two loop antenna are investigated in the frequency range from 58.995 GHz to 60.995 GHz which is efficient for satellite millimeter band applications.
[satellite applications, linear polarization capability, bandwidth 58.995 GHz to 60.995 GHz, Satellite broadcasting, millimeter band applications, UC-PBG structure, Electromagnetic radiation, microstrip antennas, Broadband antennas, two loop antenna properties, Broadband antenna, Satellite antennas, Electromagnetic wave absorption, Weapons, compact loop antenna, loop antennas, Microstrip antennas, Object detection, Bandwidth, Frequency, satellite millimeter band applications, millimetre wave antennas, millimeter band microstrip patch antenna, compact microstrip antenna]
Effectiveness of selection and maximal ratio combining diversity techniques on a DS-CDMA wireless communication system impaired by fading
2009 12th International Conference on Computers and Information Technology
None
2009
The effectiveness of different diversity combining techniques in improving the bit error rate (BER) performance of a direct-sequence code-division multiple-access (DS-CDMA) system is presented in this paper. The overall performance of a DS-CDMA system is degraded by multipath fading and additive white Gaussian noise (AWGN). To evaluate the degradation, the expression for the conditional BER, conditioned on a given level of fading is derived and then the unconditional BER is evaluated. This approach is also extended to estimate the BER performance of the system having multiple receiving antennas. The selection method and maximal ratio combining (MRC) method are employed as diversity combining schemes and it is found that both the schemes offer almost the same BER improvement. Moreover, convolutional channel coding is applied in the DS-CDMA system and it is noted to provide more enhancements in the system performance.
[Convolutional codes, AWGN, DS-CDMA wireless communication system, maximal ratio combining, Bit error rate, spread spectrum communication, Multiaccess communication, Wireless communication, Degradation, DS-CDMA, diversity techniques, multipath fading, receiving antennas, fading channels, multiple receiving antennas, Additive white noise, maximal ratio combining method, error statistics, Fading, convolutional coding, Diversity Combining, direct-sequence code-division multiple-access system, multipath channels, code division multiple access, bit error rate, BER, Diversity reception, Receiving antennas, additive white Gaussian noise]
Young consumers' m-banking choice in urban Bangladesh: Preliminary indication
2009 12th International Conference on Computers and Information Technology
None
2009
With banking channels changing rapidly and multi-channeling becoming increasingly widespread, there is a need to understand consumers' choice of channel and the banking tasks for the channel. Using the example of banking services in urban Bangladesh, where multichanneling is expected to become a norm, this research reports on an initial study comparing young consumers' choice of banking channels. Based on the results of a survey among 500 young adults between 18 and 30 years of age in Dhaka city, this study finds that although majority of the respondents did not use mobile banking (m-banking), most of them are interested to do banking on mobile phones. They are interested to do a range of banking related tasks on mobile phones if services are made available. Clearly young consumers in urban Bangladesh are in favour of m-banking and thus, m-banking has a chance to be a success story in Bangladesh.
[Pervasive computing, consumers choice, mobile radio, Banking, m-banking, consumer behaviour, Mobile handsets, Application software, Information technology, banking, mobile computing, Bangladesh, Cellular phones, Web and internet services, Consumers' choice, mobile phone, Cities and towns, technology acceptance, urban Bangladesh banking service, Australia, Portfolios, mobile banking]
An adoption-diffusion model for RFID applications in Bangladesh
2009 12th International Conference on Computers and Information Technology
None
2009
Radio Frequency Identification (RFID) technology is at its early stage of adoption in Bangladesh with a few business applications and pilot studies. It is expected that the next stage will be the organizational adoption and diffusion in more applications inspired by business cases. Eventually, the adoption will drive this technology into `extended use' stage provided that the adopters and users are `satisfied'. At this moment the RFID initiative is discrete in Bangladesh and do not have any long-term strategy leading into implementation-to-diffusion process. Without having a clear strategy to achieve large scale diffusion discrete RFID adoption may not inspire the process in reaching the full potential of RFID. This study, therefore, investigates the factors influencing the adoption diffusion of RFID and its extended use process; and then proposes a conceptual framework. The framework is an objective integration of innovation-diffusion theory and expectation-confirmation theory, with some logical modifications. It posits that while adoption of RFID is important but the long-term viability of RFID and its ultimate success depends on its continued and extended use, which is judged against `satisfaction' and `performance' derived from RFID. The implications of the framework in the context of Bangladesh are discussed.
[diffusion, radiofrequency identification, adoption-diffusion model, extended use, radio frequency identification technology, adoption, innovation-diffusion theory, Agriculture, Hardware, Large-scale systems, Technological innovation, RFID application, Government, technology transfer, implementation-to-diffusion process, RFID, expectation-confirmation theory, business application, Application software, Information technology, discrete RFID adoption, Bangladesh, Animals, Australia, Radiofrequency identification]
Geometric model for minimizing node discovery load in network simulators for large ad hoc networks
2009 12th International Conference on Computers and Information Technology
None
2009
The area of wireless ad hoc networking has been receiving increasing attention among researchers in recent years. NS2 is the most common simulator which is used to simulate wireless ad hoc networks. But, NS2 does not scale the simulation well, where there are a large number of nodes in a simulation area. In this paper a geometric approach is proposed targeting the optimization of the area that exists between the approximated area for processing and the area outside the coverage area of a transmitter. Thus, this proposed algorithm reduces the number of unaffected nodes (considering the transmission signal range), which are currently considered by the NS2 simulator for checking, whether the node resides inside the transmission area or not. The current version of NS2 uses a block based optimization but the algorithm, proposed here, reduces the coverage area of the blocks near the boundary of the transmission range, targeting the improvement of the performance of NS2 for the simulation of large ad hoc networks. These theoretic assumptions have been followed by extensive realistic test conditions for generating a set of sensible result-set for achieving the optimum from the proposed physical propagation model to facilitate NS2 with a faster simulation performance for larger wireless ad hoc mobile network. The proposed approach saves the coverage area from at least 12.5% upto 78.15% than in existing solution.
[Solid modeling, radio networks, Carrier-Sense Threshold (CST), Minimal Rectangle, wireless ad hoc networks, Receiving Threshold (RT), Grid based node organization, telecommunication computing, Mobile ad hoc networks, Analytical models, Transmitters, Wireless networks, Computer networks, physical propagation model, List based node organization, geometric model, Computational modeling, transmitter coverage area, Interference, Ad hoc networks, radiowave propagation, node discovery load, Network Simulator NS2, Computer science, MANET, network simulator, geometry, ad hoc networks]
A social relation aware semantic access control
2009 12th International Conference on Computers and Information Technology
None
2009
Social relations are often used to identify an individual. In the digital world, such relations can be exploited to provide controlled access to private or community contents. This paper proposes an access control model that employs the social relations. Semantic technologies are used for formal specification of the model. The semantic access control model is composed of a knowledge base and access policies. The Web Ontology Language represents the knowledge base and the access policies are expressed through semantic rules. Execution of rules derives the access authorization decisions that state which user can access which contents with a specific privilege. The paper provides a detail evaluation of the proposed access control model.
[Access control, social relation aware semantic access control, knowledge base, Web Ontology Language, OWL, community contents, Ontologies, Control systems, rule, Formal specifications, Information technology, formal specification, knowledge representation languages, relation, Authorization, semantic technologies, Authentication, Information security, semantic rules, authorization decisions, authorisation, Permission, private contents, ontology]
Automated fundamental analysis for stock ranking and growth prediction
2009 12th International Conference on Computers and Information Technology
None
2009
In this paper we present the automated ranking by fundamental analysis (ARFA), a new fundamental analysis (FA) tool developed for aiding the research of fundamental indicators. ARFA provides a flexible and easy to use yet powerful platform to create and test new fundamental indicators for analyzing and comparing fundamentals of the companies in a stock market. ARFAoffers a software interface for FA that is straight forward, easy to learn and at the same time exceptionally expressive without the need of any programming or customization. In this work, we present a detailed description of ARFAs indicator creation platform with demonstration of its power by showing a number of well-known indicators written in ARFAs terminology. ARFA is intended for researcheras well as share market investors. It is web-based, free and open source. ARFA has a simple programming interface for future extensions of its terminology andability with easily pluggable modules.
[Terminology, application program interfaces, Input variables, public domain software, Companies, Data Processor, user interfaces, Dynamic variable, System analysis and design, Information analysis, automated fundamental analysis, fundamental analysis tool, indicator creation platform, open source, Investments, Economic forecasting, stock ranking, stock markets, Business, Testing, Automated ranking, pluggable modules, growth prediction, investment, stock market, Fundamental indicator, Parser, software interface, fundamental indicators, market investors, Fundamental Analysis, automated ranking, programming interface, Power engineering and energy]
A crosstalk free routing algorithm of optical Multistage Interconnection Networks
2009 12th International Conference on Computers and Information Technology
None
2009
Crosstalk is an intrinsic drawback of optical networks and avoiding crosstalk is important for making fruitful application of optical switching networks. Rearrangeable optical Multistage Interconnection Networks (MINs) are feasible since they have lower complexity than their strictly counterparts. In this paper, we propose a crosstalk free routing algorithm of optical MINs and we apply it to three examples of optical MINs, the Generalized Recursive Network (GRN), the Banyan Network and the Benes Network. The routing algorithm is derived based on the idea of the semi-permutation and it completes the decomposition of a permutation.
[Multiprocessor interconnection networks, Benes network, Crosstalk, Optical fiber networks, crosstalk free routing algorithm, semipermutation, generalized recursive network, optical multistage interconnection networks, Optical losses, Optical Switch, fiber optic communications, optical crosstalk, Optical interconnections, Optical switches, Banyan network, Routing Algorithm and Semi-permutation, Routing, Optical network units, multistage interconnection networks, Communication switching, optical switching networks, optical interconnections, telecommunication network routing, Distributed control, optical fibre networks, Optical crosstalk, parallel computing system]
Heuristic algorithm of the multiple-choice multidimensional knapsack problem (MMKP) for cluster computing
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents two heuristic algorithms of the MMKP (a variant of 0-1 knapsack problem) for cluster computing. We present an architecture of a cluster, such that algorithm requires small message passing. The algorithms divide the problem among computational nodes. Each node solves its sub problem using a sequential heuristic. This nai¿ve divide and conquer approach cannot achieve good revenue. The revenue is the value achieved by the solution of MMKP. To improve the revenue, it accumulates the unused resources from every node, and assigns to the node, which gives maximum revenue over all nodes. This is the residue exploitation (RE) strategy. The solution quality can be improved by a novel resource-division policy rather than equal division. The policy divides the resource among all nodes such that total revenue increases. A sequential heuristic calculates the solution incrementally for different amounts of resource capacity, and the best combination is taken as the solution. This is the resource adjustment (RA) strategy. We experiment the algorithm using MPI (Message Passing Interface). The proposed algorithms show encouraging results.
[workstation clusters, Admission control architecture, divide and conquer methods, naive divide and conquer approach, Heuristic algorithms, Heuristic algorithm, resource adjustment, cluster computing, knapsack problems, heuristic algorithm, Clustering algorithms, Computer architecture, Parallel processing, sequential heuristics, resource capacity, Multidimensional systems, message passing, message passing interface, 0-1 knapsack problem, Memory architecture, multiple-choice multidimensional knapsack problem, Scheduling, Knapsack problem, Computer science, residue exploitation strategy, Message passing, Admission control]
Examining branch and bound strategy on multiprocessor task scheduling
2009 12th International Conference on Computers and Information Technology
None
2009
The multiprocessor task graph scheduling problem has been extensively studied as academic optimization problem which occurs in optimizing the execution time of parallel algorithm with parallel computer. The problem is already being known as one of the NP-hard problems. There are many good approaches made with many optimizing algorithm to find out the optimum solution for this problem with less computational time. One of them is branch and bound algorithm. In this paper, we studied the branch and bound algorithm for the multiprocessor scheduling problem.
[Costs, Multiprocessor interconnection networks, Lower Bound, parallel algorithm, A Multiprocessor scheduling problems, Parallel algorithms, Delay, processor scheduling, parallel computer, Upper Bound, Concurrent computing, optimisation, NP-hard problems, optimizing algorithm, branch and bound algorithm, parallel algorithms, multiprocessor task graph scheduling, Task Graph Scheduling, Application software, tree searching, execution time optimization, Information technology, Scheduling algorithm, academic optimization problem, Computer science, Processor scheduling, Branch and Bound]
Symmetric Tori connected Torus Network
2009 12th International Conference on Computers and Information Technology
None
2009
A Symmetric Tori connected Torus Network (STTN) is a 2D-torus network of multiple basic modules, in which the basic modules are 2D-torus networks that are hierarchically interconnected for higher-level networks. In this paper, we present the architecture of the STTN, addressing of node, routing of message, and evaluate the static network performance of STTN, TTN, TESH, mesh, and torus networks. It is shown that the STTN possesses several attractive features, including constant degree, small diameter, low cost, small average distance, moderate bisection width, and high fault tolerant performance than that of other conventional and hierarchical interconnection networks.
[Symmetric Tori connected Torus Network, Costs, Node Degree, Multiprocessor interconnection networks, fault tolerant performance, interconnections, Cost, Bisection Width, Concurrent computing, Fault tolerance, Network topology, Arc Connectivity, Computer networks, Large-scale systems, Diameter, 2D torus network, Routing, LAN interconnection, message routing, Information technology, static network performance, Average Distance, STTN, Artificial intelligence, hierarchical interconnection networks]
Design and analysis of online testability of reversible sequential circuits
2009 12th International Conference on Computers and Information Technology
None
2009
Reversible logic plays an important role in the synthesis of circuits having application in quantum computing, low power CMOS design and nanotechnology-based system. In this paper, we have proposed the online testability of reversible sequential circuits, which is first ever proposed in literature. On the way to propose the online testability of reversible sequential circuits, we have proposed an improved rail-check circuit that significantly improves the performance of the overall circuit in terms of gate cost and garbage cost parameters. We have also used our improved and efficient rail-check circuit to realize the testability of different benchmark circuits.
[Costs, reversible logic, online testability, Testable block, Circuit testing, rail-check circuit, automatic testing, Sequential analysis, Quantum computing, CMOS logic circuits, integrated circuit design, sequential circuits, Benchmark testing, circuit synthesis, Online testability, Benchmark circuits, Logic design, Rail-check circuit, integrated circuit testing, logic gates, reversible sequential circuit, Computer applications, garbage cost parameter, Sequential circuits, Circuit synthesis, gate cost parameter]
Minimized reversible synthesis of non-reversible quinary logic function
2009 12th International Conference on Computers and Information Technology
None
2009
Reversible multiple-valued logic circuit has several advantages over reversible binary logic circuit. In this paper, we propose a method of minimization of Galois field sum of products (GFSOP) expression for non-reversible quinary logic function. We also propose a method of reversible realization of quinary GFSOP expression as cascade of quinary reversible gates. Experimental results show that a significant minimization can be achieved using the proposed minimization method.
[reversible logic, GFSOP expression, Input variables, Paper technology, GFSOP minimization, Galois field expansion, GFSOP synthesis, reversible binary logic circuit, Logic circuits, quinary logic, multivalued logic, nonreversible quinary logic function, Logic functions, Minimization methods, reversible synthesis, Galois field sum of products, quinary reversible gates, Galois fields, Information technology, Computer science, reversible multiple-valued logic circuit, minimization, DH-HEMTs, Circuit synthesis, minimisation, logic circuits]
Realization of systolic array using ternary reversible gates
2009 12th International Conference on Computers and Information Technology
None
2009
Multi valued logic synthesis is a very promising and affluent research area at present because of allowing designers to build much more efficient computers than the existing classical ones. Ternary logic synthesis research has got impetus in the recent years. Many existing literature are mainly perceptive to the realization of efficient ternary reversible processors. This research is based on the design of a reversible systolic array, which is one of the best examples of parallel processing, using micro level ternary Toffoli gate. General architecture of the ternary reversible systolic array multiplier is shown along with example. Lower bound for the garbage outputs produced in the proposed design and the quantum cost of the entire circuit is calculated here to prove the compactness of the design.
[multiplying circuits, Costs, Temperature, Circuits, Optical computing, systolic arrays, Multivalued logic, parallel processing, Garbage Output, ternary reversible gates, Design engineering, Quantum computing, Sections, Systolic arrays, ternary logic synthesis, ternary reversible systolic array multiplier, Ternary Logic, Quantum Cost, Reversible Gate, microlevel ternary Toffoli gate, Systolic Array, multivalued logic synthesis, multivalued logic circuits, DH-HEMTs, ternary logic, logic design]
Development of a novel quaternary algebra with the design of some useful logic blocks
2009 12th International Conference on Computers and Information Technology
None
2009
A completely new scheme for quaternary logic is proposed. Instead of conventional fuzzy logic or Galois Field theory, the logic system is based on the extension of Boolean algebra. The logic is capable of handling both quaternary and coupled-binary inputs, where binary operands are coupled in pairs to form quaternary entities. All necessary operators are defined and several theorems and properties are derived to develop a way of expressing arbitrary truth tables with sum-of-product functions. To demonstrate the functionality of this novel logic scheme, some useful logic blocks such as decoder, multiplexer, and half-adder are designed.
[Multiplexing, sum-of-product functions, quaternary algebra, Multivalued logic, Logic design, Inverters, quaternary logic, Boolean algebra, Decoding, decoder, Decoder, Fuzzy logic, adders, multiplexer, Logic circuits, coupled-binary inputs, arbitrary truth tables, Logic functions, half-adder, logic blocks, Logic devices, logic design, multiplexing equipment]
Design of quaternary sequential circuits using a newly proposed quaternary algebra
2009 12th International Conference on Computers and Information Technology
None
2009
Using a novel quaternary algebra, several sequential circuits are designed. The quaternary logic scheme used here is obtained by extending Boolean algebra into quaternary domain. A set of operators capable of handling both coupled-binary and ordinary inputs are used to design the sequential circuit elements such as latches, flip-flops, registers and counters. Sufficient conditions for stable operation of the sequential blocks are derived mathematically and demonstrated graphically using simulated timing diagrams.
[Latches, counting circuits, Circuit simulation, quaternary algebra, sequential circuit elements, algebra, latches, Boolean algebra, Registers, Flip-flops, Counting circuits, timing diagram simulation, Sufficient conditions, Coupling circuits, sequential circuits, quaternary logic scheme, quaternary domain, Logic functions, registers, Counters, Sequential circuits, quaternary sequential circuits, flip-flops]
A new image compression scheme using repeat reduction and arithmetic coding
2009 12th International Conference on Computers and Information Technology
None
2009
Based on some simple arithmetic operation, an efficient lossless image compression technique is proposed. We use repeat reduction and arithmetic coding to improve the results of compression. We get better results than log-exp based compression and arithmetic modulo based compression. From the experimental results, it can be seen that the performance of proposed method is better for all standard images. The complexity of our proposed method with respect to compression ratio has also been analyzed. The proposed method is most applicable for those images where lossy compression is to be avoided.
[data compression, repeat reduction, Redundancy, Multimedia databases, Image compression, lossless image compression technique, Image reconstruction, compression ratio, Image quality, difference snakescan, Image coding, Transform coding, arithmetic coding, Digital arithmetic, Internet, Discrete cosine transforms, image coding, Pixel]
Evaluation of performances of digital adaptive filters in acoustic echo cancellation
2009 12th International Conference on Computers and Information Technology
None
2009
Wireless network link is debilitated by fading, attenuation, non-linear distortion and noise. Analogically sound system in a conference room shows similar phenomena due to the fact that the reflected feedback signals resemble the multi-path propagation of wireless communication; exception is that the noise level is very low. The signal path of the feedback signal is a non-linear system can be replaced by a finite impulse response filter. The aim of the paper is to compare the performance of three well known adaptive filters (frequency domain adaptive filter, least mean square filter, Kalman filter) in context of echo cancellation under adaptive white Gaussian noisy environment.
[Performance evaluation, least mean squares methods, AWGN, FDAF, Adaptive filters, Kalman filter, echo suppression, acoustic echo cancellation, finite impulse response filter, FIR filters, echo cancellation, adaptive signal processing, wireless communication multipath propagation, Acoustic noise, Wireless networks, Feedback, adaptive Kalman filters, Attenuation, Fading, Predictor-corrector algorithm, adaptive white Gaussian noise, Echo cancellers, least mean square filter, Noise cancellation, Finite impulse response filter, radiocommunication, near and far-end signal and Kalman equation, frequency domain adaptive filter]
A modified 2-D logarithmic search technique for video coding with reduced search points
2009 12th International Conference on Computers and Information Technology
None
2009
Video coding is a process for representing video sequences in a compact manner. A significant step in video coding is searching for similar segments in previous frames and use only the difference information for reconstruction thus reducing space requirement. Different search techniques including full search and 2-D logarithmic search etc. are used in the current literature. Full search restricts its application because of its computational load. 2D logarithmic search is computationally less expensive although there are some spaces for improvement. In this paper we propose a new search technique by modifying the 2-D logarithmic search that requires less search points with insignificant loss in visual quality. Experimental results demonstrate the effectiveness of the proposed technique.
[Video coding, 2-D logarithmic search, Data engineering, reduced search points, Cleaning, video coding, Data mining, Information technology, Sorting, Computer science, 2D logarithmic search technique, Databases, video sequence representation, image representation, Detection algorithms, image sequences, space requirement]
Diphone preparation for Bangla text to speech synthesis
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents methodologies involved in diphone preparation for Bangla text to speech synthesis. A concatenation based synthesis system comprises basically two modules- one is natural language processing and other is digital signal processing (DSP). Natural language processing implies converting text to its pronounceable text, called text normalization and the diphone selection method based on the normalized text is called Graphene to Phoneme (G2P) conversion. We developed a speech synthesizer for Bangla using diphone based concatenative approach. Diphone preparation, labeling and selection techniques are described in this paper.
[text analysis, text normalization, Data engineering, Bangla text, Speech synthesis, Data mining, Databases, normalized text, concatenation based synthesis system, digital signal processing, diphone selection method, grapheme-to-phoneme, speech synthesis, natural language processing, diphone preparation, Cleaning, phoneme conversion, Information technology, Sorting, Computer science, graphene conversion, pronounceable text, Detection algorithms, diphone, speech synthesizer]
An approach to improve collusion set detection using MCL algorithm
2009 12th International Conference on Computers and Information Technology
None
2009
Many malpractices in stock market trading e.g. price manipulation, circular trading, use the modus-operandi of collusion. Generally, a set of traders is a candidate collusion set when they are ¿trading heavily¿ among themselves in cross trading or circular trading. In real life not all colluders always trade with each other. In a perfectly circular collusion set of size 4, trader A will trade with B, B with C, C with D and D with A; there will be no cross trading among these traders. An existing method using shared, mutual nearest neighbor and collusion graph clustering algorithm fails to detect purely circular trading which is also a collusion set. In this paper, we have proposed a new approach to detect collusion sets using Markov Clustering Algorithm (MCL). Proposed method can detect purely circular collusions as well as cross trading collusions. We have used MCL at various strength of ¿residual value¿ to detect different cluster sets from the same stock flow graph. We have combined our collusion clusters with the existing method using Dempster Schafer theory of evidence. The experimental result shows that MCL algorithm provides better collusion clusters and the performance improved significantly.
[stock market trading, stock flow graph, Law, Dempster Schafer theory of evidence, uncertainty handling, collusion graph clustering algorithm, Guidelines, collusion set detection, Inflation, Clustering algorithms, Stochastic Matrix, stock markets, Stock markets, Expansion, Data security, flow graphs, Sparse Graph, Transaction databases, Flow graphs, inference mechanisms, Information technology, MCL algorithm, Computer science, cross trading, Collusion Cluster, Surveillance, pattern clustering, circular trading, MCL, Markov processes, Markov clustering algorithm]
Improvement of speech enhancement techniques for robust speaker identification in noise
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents an approach of speech enhancement techniques to improve the performance of the robust speaker identification under noisy environments. Start-end points detection, silence part removal, frame segmentation and windowing technique have been used to pre-process and Wiener filter has been used to remove the silence parts from the speech utterances. To extract the features from the speech various speech parameterization techniques that is LPC, LPCC, RCC, MFCC, ¿MFCC and ¿¿MFCC have been simulated. Finally, to measure the performance of the proposed speech enhancement techniques, genetic algorithm has been used as a classifier for the noise robust automated speaker identification system and various experiments have performed on genetic algorithm to select the optimum parameters. According to the NOIZEOUS speech database, the highest identification rate of 70.31 [%] for text-dependent and of 61.26 [%] for text-independent speaker identification system have been achieved.
[Performance evaluation, Wiener filters, start-end points detection, Genetic Algorithm, Mel frequency cepstral coefficient, robust speaker identification, Genetic algorithms, genetic algorithm, Noise Robust Speaker Identification, Wiener filter, NOIZEOUS speech database, feature extraction, Speech enhancement, Noise robustness, speech enhancement techniques, frame segmentation, text-independent speaker identification system, Linear predictive coding, genetic algorithms, Noise measurement, noise classifier, Working environment noise, Speech Preprocessing, Feature extraction, Speech Parameterization, speech enhancement, speaker recognition]
Speaker identification system using PCA &#x00026; eigenface
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents a speech-based speaker identification system and an efficient approach for selection of acoustic parameters closely related to the vocal track shape of the speaker. Speech endpoint detection algorithm is developed in order to discard the room noise and non-speech signal to achieve high accuracy of the system. Windowing and fast Fourier transform (FFT) are used to determine the spectrum of the speech signal and PCA has been used to extract feature of speech of individual speaker. Eigenface algorithm has been used here as a classification and recognition tool. Eigenspace of individual speaker is generated by the feature of the speech signal. The experimental results show the noticeable performance of the proposed system.
[fast Fourier transforms, Shape, classification tool, speech endpoint detection algorithm, eigenvalues and eigenfunctions, eigenface algorithm, Fast Fourier transforms, speech recognition, Acoustic noise, feature extraction, FFT, Speech enhancement, Hamming window, Eigenvectors, acoustic parameter selection, Endpoint detection, Noise shaping, fast Fourier transform, signal classification, PCA, Eigenface, Loudspeakers, Feature extraction, Signal generators, speech-based speaker identification system, recognition tool, Detection algorithms, acoustic signal processing, principal component analysis, Principal component analysis]
Variable rate Steganography in gray scale digital images using neighborhood pixel information
2009 12th International Conference on Computers and Information Technology
None
2009
In order to improve the security by providing the stego image with imperceptible quality, three different steganographic methods for gray level images are presented in this paper. Four Neighbors, diagonal neighbors and eight neighbors methods are employed in our scheme. These methods utilize a pixel's dependency on its neighborhood and psycho visual redundancy to ascertain the smooth areas and edged areas in the image. In smooth areas we embed three bits of secret information. In the edged areas, variable rate bits are embedded. From the experimental results it is seen that the proposed methods achieve a much higher visual quality as indicated by the high peak signal-to-noise ratio (PSNR) in spite of hiding a larger number of secret bits in the image. In addition, to embed this large amount of secret information, at most only half of the total number of pixels in an image is used. Moreover, extraction of the secret information is independent of original cover image.
[psycho visual redundancy, Psychology, pixel dependency, gray scale digital images, secret information extraction, feature extraction, variable rate steganography, diagonal neighbor method, eight neighbors methods, PSNR, Digital images, Data security, high peak signal-to-noise ratio, Redundancy, Diagonal Neighbors, neighborhood pixel information method, Eight Neighbors, visual quality, Information technology, Peak Signal-to-Noise Ratio, Steganography, Four Neighbors, steganography, Internet, Artificial intelligence, image coding, Pixel]
Extracting unique patterns from human actions
2009 12th International Conference on Computers and Information Technology
None
2009
Human walks, runs, dances and left behind interesting information on their actions. This paper presents how chaotic dynamics help to interpret and classify human actions. The trajectories of two legs are extracted during a motion such as walk. These trajectories of foot points are collected from an artificial human video arrangement. Each dimension of trajectory represents a time series. The phase space for each time series is reconstructed using appropriate time delay and dimension. The plot exhibited a characteristic trajectory representing the regularity of the time series. Analysis of time series obtained in human with three different motions revealed that the trajectory behaves in such a way that the time series is governed with a deterministic rule. Unique patterns are observed for a particular motion. This can be revealed from the phase space and self organizing map (SOM) network. The motions (walk, run, and jump) can be categorized in terms of different shape of phase space and output of the SOM network. The results are validated with correlation dimension. These representations are very useful in classifying the human motions.
[Chaos, Shape, correlation dimension, Humans, foot points, Data mining, Human action, phase space, self-organising feature maps, chaotic dynamics, self organizing map network, video signal processing, unique patterns, deterministic rule, chaos, Delay effects, Time series analysis, human actions, time series, time delay, classification, Organizing, artificial human video arrangement, Leg, Foot, Motion analysis]
An audible Bangla text-entry method in Mobile phones with intelligent keypad
2009 12th International Conference on Computers and Information Technology
None
2009
Communication through mobile device is the most effective and fastest way that becomes a part of our daily life nowadays. Particularly, short message service (SMS), is one of the most popular and relatively cheaper application of cell phone. In Bangladesh, there has been an enormous growth of mobile users and in this context, it is quite reasonable that people like to send SMS with their own language. Taking these challenges, a few initiatives were taken to introduce Bangla text in mobile phones, more specifically, in messaging. But unfortunately, Bangla SMS is not popular yet in our country due to the presence of large number of characters and complex script. In fact, it is very much difficult to map Bangla characters efficiently on a mobile with 12 keys only. That's why, it motivated us to design an intelligent mobile keypad layout, which significantly reduces the number of keystrokes than existing methods, and remove difficulties of entering text. We eliminate traditional multi-tapping on a single button and introduce two key press technique, by arranging characters in two-dimensional matrix. Using this keypad the system becomes faster, reliable and flexible. Predictive text input method is also added in the system for further speeding up messaging. Moreover, our text-entry system ensures accessibility to visual impaired people by avoiding disambiguation caused by multi-tapping and introducing audio feedback for inserting each Bangla characters, which is not supported by existing systems. Finally, after analyzing the result on different sample SMS data, we showed that, our proposed keypad with the predictive input system reduces the number of key presses than the common Bangla keypad by 60.34%.
[text analysis, electronic messaging, multitapping, Mobile communication, Mobile handsets, short message service, cell phone, Bangla character, mobile computing, Bangla SMS, Feedback, audio feedback, audible Bangla text entry, visual impaired people, mobile radio, keyboards, natural language processing, Urban areas, intelligent mobile keypad layout, Application software, Text Entry, Information technology, Computer science, mobile device, Predictive Text Input, Visual Impaired, Cellular phones, mobile phone, A Mobile Keypad, Message service, Mobile computing, cellular radio]
Phoneme recognition based on distinctive phonetic features (DPFs) incorporating a syllable based language model
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents a phoneme recognition method based on distinctive phonetic features (DPFs). The method comprises three stages. The first stage extracts 3 DPF vectors of 15 dimensions each from local features (LFs) of an input speech signal using three multilayer neural networks (MLNs). The second stage incorporates an Inhibition/Enhancement (In/En) network to obtain more categorical DPF movement and decorrelates the DPF vectors using the Gram-Schmidt orthogonalization procedure. Then, the third stage embeds acoustic models (AMs) and language models (LMs) of syllable-based subwords to output more precise phoneme strings. The proposed method provides a higher phoneme correct rate as well as phoneme accuracy with fewer mixture components in hidden Markov models (HMMs).
[Inhibition/Enhancement network, acoustic models, syllable based language model, Data mining, Multi-layer neural network, speech signal, Information science, hidden Markov models, speech recognition, phoneme recognition, Decorrelation, DPF vectors, local features, inhibition/enhancement network, Spatial databases, Information technology, Hospitals, grammars, Neural networks, Hidden Markov models, multilayer neural networks, Speech, distinctive phonetic features, Gram-Schmidt orthogonalization procedure, speech enhancement, acoustic signal processing, neural nets]
Computational intelligence approach to load forecasting - a practical application for the desert of Saudi Arabia
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents the development of an Artificial Neural Networks and Particle Swarm Optimization (ANN-PSO) based short-term load forecasting model with improved generalization technique for the Regional Power Control Center of Saudi Electricity Company, Western Operation Area (SEC-WOA). Weather, load demand, wind speed, wind direction, heat, sunlight, etc. are quite different in a desert land than other places. Thus this model is different from a typical forecasting model considering inputs and outputs. In this research paper two steps have been introduced, first load forecasting made by mapping mechanism and then optimization technique applied to improve its accuracy. This paper includes ANN and PSO models for 24-hours ahead load forecasting. ANN is an effective mathematical tool for mapping complex relationships. It is also successful for doing forecasting, categorization, classification, and so forth. On the other hand, PSO is the most promising optimization tool. It has better information sharing and conveying mechanism; it has better balance of local and global searching abilities; and can handle huge multi-dimensional optimization problems efficiently with hundreds of thousands of constraints. Thus PSO is chosen as the optimization model of the weight matrix of ANN. Results show that the proposed ANN-PSO performs much better than ANN for the load forecasting in a desert like Saudi Arabia.
[electrical engineering computing, desert of Saudi Arabia, optimization technique, particle swarm optimisation, Weather forecasting, Artificial neural networks, neural networks, Predictive models, particle swarm optimization, Particle swarm optimization, Solar heating, mapping mechanism, Saudi Arabia, artificial neural networks, Load forecasting, Wind speed, Power control, load forecasting, Computational intelligence, neural nets, short-term load forecasting, Load modeling]
Predictive power of the daily Bangladeshi exchange rate series based on Markov model, neuro fuzzy model and conditional heteroskedastic model
2009 12th International Conference on Computers and Information Technology
None
2009
Forecasting exchange rate is very important for many international agents e.g. investors, money managers, investment banks, funds makers and others. We forecasted the daily Bangladeshi exchange rate series for the period of January 1992 to March 2009 using popular non-linear forecasting models, namely Markov switching autoregressive model, fuzzy extension of artificial neural network model (ANFIS) and generalized autoregressive conditional heteroscedastic model. Our target is to investigate whether selected models can serve as useful forecasting models to find volatile and non-linear behaviors of the considered series. By most commonly used statistical measures: mean absolute percentage error, root mean square error and coefficient of determination, we found that ANFIS is a superior predictor than other two selected predictors. We believe findings of this paper will be helpful to make a wide range of policies for multinational companies who are involved with various international business activities.
[Space vector pulse width modulation, Pulse width modulation, investors, fuzzy extension, exchange rates, neuro fuzzy model, ANFIS, mean square error methods, fuzzy neural nets, investment, statistical measures, inference mechanisms, money managers, Markov processes, international trade, Artificial neural network models, Circuits, Predictive models, time series model, nonlinear forecasting models, funds makers, Voltage, investment banks, artificial neural network model, absolute percentage error, multinational company, international business activity, Pulsed power supplies, Digital modulation, international agents, root mean square error, forecasting, fuzzy logic, non-linearity, DC motors, autoregressive processes, Markov model, Exchange rates, predictive power, Bangladeshi exchange rate series, coefficient of determination, heteroscedasticity, forecasting theory, Markov switching autoregressive model, conditional heteroskedastic model, generalized autoregressive conditional heteroscedastic model, Frequency, forecasting exchange rate, statistical analysis]
Design and development of a wall climbing Robot and its control system
2009 12th International Conference on Computers and Information Technology
None
2009
The Robot, named as TRAIN WALL BOT, is designed to navigate on smooth vertical surfaces with the capability to avoid obstacles and overcome if the height is about 1 cm. The design is inspired from train steel wheel movement that contains two actuated legs with rotary motion provided by a DC motor. The Robot uses pneumatic system and the suction force is supplied by an air compressor that turns on intermittently. The suction force ensures the attachment of the robot with the wall by using 3 vacuum valves and 6 vacuum pads. The robot is controlled using PIC 16F877A. Two limit switches are used to acknowledge the contact with its navigating surface. Vacuum suction is controlled based on the ON OFF priority of the limit switches. Though the design is quiet simple but it is capable to walk, climb vertical smooth surfaces and avoid obstacles. Forward and backward movements are also faster, smoother and more stable (because of the coupling design) than other existing wall climbing Robots. In this paper, various aspects of prototype design and development of the Climbing Robot are conveyed including the body, leg, feet design and gait dynamics.
[collision avoidance, obstacle avoidance, Train Wall Bot, Robot control, Wheels, Switches, Control systems, Mobile robots, Wall Climbing Robot, train steel wheel movement, smooth vertical surfaces, rotary motion, compressors, pneumatic system, pneumatic control equipment, PIC 16F877A, limit switches, robots, control system, TRAIN WALL BOT, Navigation, wall climbing robot, air compressor, DC motors, control system synthesis, Steel, Leg, suction force, DC motor, Climbing robots]
An empirical assessment of customer satisfaction with Internet Banking applications: An Australian experience
2009 12th International Conference on Computers and Information Technology
None
2009
In recent years, Internet-based banking (IB) applications are gaining popularity among retail banking customers. The long term success of these applications is however influenced by customer satisfaction because it affects customers' perceptions about banks' innovative ability and customer caring intentions. Hence, measuring customer satisfaction with IB applications is important. In this article, we report the development of an instrument to operationalise customer satisfaction following a rigorous mixed approach, and then apply that instrument to measure customer satisfaction with IB applications in Australia. Several interesting findings have emerged which are useful to research and practice alike.
[instrument evaluation, retail banking, mixed approach, Instruments, Banking, Electronic mail, Application software, Security, satisfaction, Information technology, Human computer interaction, internet banking, customer satisfaction, Customer satisfaction, Web and internet services, e-commerce, Internet, Australia, Internet banking, bank data processing]
Adaptive array antenna system in cancellation of jammer and noise of wireless link
2009 12th International Conference on Computers and Information Technology
None
2009
Single element antennas have very little capability of variation of antenna gain pattern. For a desired directivity, shape of beam and steer able beam, array antenna is widely used in wireless network. Relative magnitude of feed currents, relative phases or separation between antenna elements, geometrical configuration of array are responsible for the overall radiation pattern. The weighting factor of each antenna element is governed by an adaptive algorithm based on input signal and desired signal to achieve dynamic shaping of antenna beam. In this paper, both single and multiple elements adaptive array antenna system is used to tune the gain in such a way that the gain is enhanced in the direction of desired signal and reduced in the direction of interference or jamming signals.
[radio links, antenna gain pattern, Adaptive systems, Shape, reference and primary omni, steer able beam, Jamming, Phased arrays, adaptive array antenna, beam steering, weighting factor, Wireless networks, wireless link, adaptive antenna arrays, antenna radiation pattern, array signal processing, interference suppression, cross-correlation vector, Adaptive arrays, auto-correlation, Noise shaping, Adaptive beamforming, jammer, radiation pattern, jammer cancellation, Noise cancellation, Feeds, Antenna arrays, feed current]
Logical clock based Last Update Consistency model for Distributed Shared Memory
2009 12th International Conference on Computers and Information Technology
None
2009
Excessive locking and cumulative updates in distributed shared memory (DSM) not only reduces the parallelism for block access but also causes a serious degradation in response time for a dense network. This paper proposes a new consistency model in DSM named last update consistency (LUC) model, where the model uses logical clock counter to keep the DSM consistent. The logical clock always increases never decreases. So the increasing order of the logical clock value is used to provide the request to the DSM. In this model, multiple nodes can perform READ operations over the same block at a time. For WRITE operation over the same block, only the last modification will exist and the earlier WRITE operations will be treated as obsolete WRITE and should be discarded. The experimental and analytical analysis showed that the proposed model effectively reduces the unnecessary network traffic and cumulative block updates that exist in the sequential consistency model and release consistency model.
[release consistency model, Protocols, last update consistency model, distributed shared memory, Telecommunication traffic, Delay, Counting circuits, Degradation, Traffic control, Broadcasting, Timestamp Protocol, Computer networks, DSM, Distributed System, WRITE operations, Distributed Shared Memory, logical clock, READ operations, Information technology, network traffic, sequential consistency model, distributed shared memory systems, Shared Memory, cumulative block updates, Consistency Model, Clocks]
Can Information Retrieval techniques automatic assessment challenges?
2009 12th International Conference on Computers and Information Technology
None
2009
In Information Retrieval (IR), the similarity scores between a query and a set of documents are calculated, and the relevant documents are ranked based on their similarity scores. IR systems often consider queries as short documents containing only a few words in calculating document similarity score. In Computer Aided Assessment (CAA) of narrative answers, when model answers are available, the similarity score between Students' Answers and the respective Model Answer may be a good quality-indicator. With such an analogy in mind, we applied basic IR techniques in the context of automatic assessment and discussed our findings. In this paper, we explain the development of a web-based automatic assessment system that incorporates 5 different text analysis techniques for automatic assessment of narrative answers using vector space framework. We apply Uni-gram, Bi-gram, TF.IDF, Keyphrase Extraction, and Keyphrase with Synonym Resolution before representing model answers and students' answers as document vectors; and then we compute document similarity scores. The experimental results based on 30 narrative questions with 30 model answers, and 300 student's answers (from 10 students) show that the correlation of automatic assessment with human assessment is higher when advanced text processing techniques such as Keyphrase Extraction and Synonym Resolution are applied.
[System testing, text analysis, Text analysis, computer aided assessment, Humans, synonym resolution, Data mining, vector space framework, model answer, keyphrase extraction, Natural language processing, Computer aided analysis, relevant documents, information retrieval, Computer Aided Instruction, Natural Language Processing, Intelligent Text Analysis, Information retrieval, Information technology, Web based automatic assessment system, Text processing, document vectors, Automatic testing, short documents, document similarity scores, Information Retrieval]
Development of a translation model from HTML to WML using component based information extraction technique
2009 12th International Conference on Computers and Information Technology
None
2009
Now-a-days mobile is part of our daily life. So mobile browsing is becoming popular issue day by day. Today most Web pages are written in HTML. Mobile Browsers are optimized for displaying Web content. It has low-memory capacity and low-bandwidth. So, mobile browser cannot render all HTML documents in its limited area. Specific information of particular web page needs to visualize for mobile users. So the effective conversion from HTML to WML becomes a key issue for reducing tedious work. In this technique, not only convert static content, but also some sort of dynamic content will be converted and it improved the conversion effectively in speed and accurate rate, whilst presented the useful information conveniently to the users.
[mobile browsing, Visualization, HTML-to-WML Converter, HTML, Data mining, mobile browsers, Web content, Hierarchical Data Extraction, mobile computing, SGML, HyperText Markup Language, Communications technology, Web server, HTML documents, Business, hypermedia markup languages, HTML Parser, Information technology, HTML Tree, translation model, component based information extraction, Wireless Markup Language, Web pages, XML, WML, now-a-days mobile]
Neural network ensembles based on Artificial Training Examples
2009 12th International Conference on Computers and Information Technology
None
2009
Ensembles with several neural networks are widely used to improve the generalization performance over a single network. Proper diversity among component networks is considered an important parameter for ensemble construction so that failure of one may be compensated by others. Data sampling, i.e., different training sets for different networks, is the most investigated technique for diversity than other approaches. This paper presents a data sampling based neural network ensemble method where individual networks are trained on the union of original training set and a set of some artificially generated examples. Generated examples are different for different networks and are the element to produce diversity among the networks. The effectiveness of the method is evaluated on a suite of 20 benchmark classification problems. The experimental results show that the performance of this ensemble method is better or competitive with respect to the existing popular methods.
[artificial training, sampling methods, Humans, Ada-Boost, Artificial neural networks, bagging, Information technology, Computer science, diversity, negative correlation learning, generalization, Training data, neural network ensemble, Sampling methods, Computer networks, Error correction, learning (artificial intelligence), data sampling, Artificial training example, Artificial intelligence, Bagging, neural nets]
Three algorithms for learning artificial neural network: A comparison for induction motor flux estimation
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents a comparative study of three algorithms for learning artificial neural network. As neural estimator, back-propagation (BP) algorithm, uncorrelated real time recurrent learning (URTRL) algorithm and correlated real time recurrent learning (CRTRL) algorithm are used in the present work to learn the artificial neural network (ANN). The approach proposed here is based on the flux estimation of high performance induction motor drives. Simulation of the drive system was carried out to study the performance of the motor drive. It is observed that the proposed CRTRL algorithm based methodology provides better performance than the BP and URTRL algorithm based technique. The proposed method can be used for accurate measurement of the rotor flux.
[induction motor flux estimation, Recurrent neural networks, learning artificial neural network, Learning, Induction motor, Recurrent neural network, neural estimator, Motor drives, Induction motors, Low pass filters, Rotors, Rotor flux, Artificial neural networks, induction motor drives, DC motors, Correlated real time recurrent learning (CRTRL), power engineering computing, Back-propagation (BP), Induction motor drives, Uncorrelated real time recurrent learning (URTRL), high performance induction motor drive, backpropagation, Artificial intelligence, neural nets, uncorrelated real time recurrent learning]
Performance analysis of a novel fuzzy logic and MTPA based speed control for IPMSM drive with variable d- and q-axis inductances
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents a novel speed control scheme of an Interior permanent magnet synchronous motor (IPMSM) using fuzzy logic controller considering variable direct and quadrature axis inductances. The fuzzy logic controller (FLC) has been designed on the basis of indirect vector control scheme of the IPMSM drive. The complete vector control scheme of the IPMSM drive incorporating the FLC is simulated for a IPMSM using Matlab/Simulink. The performances of the proposed FLC based IPMSM drive are investigated and compared to those obtained from the maximum torque per ampere (MTPA) based drive at various dynamic operating conditions, such as, certain change in command speed, step change in load, etc. The comparative results show that the FLC is more robust and, hence, found to be a suitable replacement of the MTPA control for the high performance industrial drive applications.
[machine control, Synchronous motors, dynamic operating conditions, speed control scheme, quadrature axis inductance, MTPA based speed control, direct axis inductance, fuzzy control, indirect vector control scheme, Velocity control, fuzzy logic controller, IPMSM drive, Performance analysis, Matlab/Simulink, Mathematical model, Rotors, interior permanent magnet synchronous motor, Permanent magnet motors, Fuzzy Logic Controller, Sliding mode control, Machine vector control, MTPA contol, Fuzzy logic, maximum torque per ampere based drive, velocity control, AC motors, High performance drive with variable direct axis and quadrature axis inductances and indirect vector control, high performance industrial drive applications, synchronous motor drives, Interior Permanent Magnet Synchronous Motor, permanent magnet motors, performance analysis]
A currency recognition system using negatively correlated neural network ensemble
2009 12th International Conference on Computers and Information Technology
None
2009
This paper represents a currency recognition system using ensemble neural network (ENN). The individual neural networks (NN) in an ENN are trained via negative correlation learning. The object of using negative correlation learning (NCL) is to expertise the individuals in an ensemble on different parts or portion of input patterns. The available currencies in the market consist of new, old and noisy ones. It is often difficult for machine to recognize these currencies; therefore we propose a system that uses ENN to identify them. We performed our experiment for seven different types of TAKA (Bangladeshi currency) they are 2, 5, 10, 20, 50, 100 and 500 TAKA. The image of different types note is converted in gray scale and compressed in our desired range. Each pixel of the compressed image is given as an input to the network. This system is able to recognize highly noisy or old image of TAKA. Ensemble network is very useful for the classification of different types of currency. It reduces the chances of misclassification than a single network and ensemble network with independent training. In experimental results we have shown this. We also find good result for similar pattern available in market.
[Image recognition, Image processing, image classification, gray scale, image compression, Diversity, Bangladeshi currency, Image coding, negative correlation learning, Voting, Currency recognition, Computer networks, financial data processing, image colour analysis, learning (artificial intelligence), Image edge detection, ensemble neural network training, Artificial neural networks, Neural network ensemble, Information technology, Neural networks, Feature extraction, image coding, image recognition, neural nets, Negative correlation learning, correlation methods, currency recognition system]
Personnel selection method using Analytic network Process (ANP) and fuzzy concept
2009 12th International Conference on Computers and Information Technology
None
2009
Due to complex functionality of the organization, it requires the most competent and skilled personnel to challenge the global trends. To focus such personnel an integrated scientific selection procedure is a must. Analytic thinking approaches are the most suitable methods to address this problem of personnel selection. Analytic Hierarchy Process (AHP) is more suitable when the hierarchical levels are independent of each other and Analytic network Process (ANP) is used when there are factors, which are dependent of the other factors in the same hierarchical levels or other levels. The method proposed in this paper successfully models the ambiguity and imprecision associated with the pair wise comparison process and reduces the personal biasness. The model allows many qualitative and qualitative factors and provides the importance level of each criteria/sub-criteria so that the decision maker has the ability to determine which factors are to be given emphasize. All the members of the decision maker have the opportunity to express their judgments and hence participative decision-making is achieved.
[Analytic Hierarchy Process, Analytic Network Process, Human Resource Management, human resource management, Personnel, Supermatrix, analytic hierarchy process, Personnel Selection and Fuzzy, personnel selection method, decision making, fuzzy concept, analytic network process, analytic thinking approaches]
Automatic speech recognition for Bangla digits
2009 12th International Conference on Computers and Information Technology
None
2009
In this paper, we introduce a system for Bangla digit automatic speech recognition (ASR). Though Bangla is one of the largely spoken languages in the world, only a few works on Bangla ASR can be found in the literature, especially on Bangladeshi accented Bangla. In this work, the corpus is collected from natives in Bangladesh. Mel-frequency cepstral coefficients (MFCCs) based features and hidden Markov model (HMM) based classifiers are used for recognition. Experimental results show comparatively high recognition performance (more than 95%) for first six digits (0 - 5) and low performance (less than 90%) for the next four digits (6 - 9). We notice two confused pairs of digits: one with (6) and (9), and the other with (7) and (8), in the experiments. We also find that different dialects in Bangladesh have a greater role on this confusion.
[Mel-frequency cepstral coefficients, natural language processing, Bangla digits, hidden Markov model, Natural languages, Artificial neural networks, Educational institutions, Speech synthesis, automatic speech recognition, Information technology, Bangla digit, Bangla phoneme, Automatic speech recognition, hidden Markov models, Bangladesh, Databases, Cepstral analysis, speech recognition, Hidden Markov models, Speech recognition, recognition performance]
OSDT: Outer Shape Detection Technique for Recognition of Bangla Optical Character
2009 12th International Conference on Computers and Information Technology
None
2009
Optical character recognition is one of the challenging fields in recognition of printed Bangla text. The main difficulties are that there are no precise techniques or algorithms for separating lines, words, and characters from printed Bangla text and efficiently recognize these separate characters. In this paper, we introduce new methods to separate lines, words, and characters from printed Bangla text. We also propose outer shape detection technique (OSDT), a new technique to recognize each character based on its outer shape which is unique. To successfully accomplish this, proposed technique scans each character both from its left side and right side. Finally our experimental results are compared with some prevalent ones which precisely show that our approach performs smoothly on different font sizes and number of characters.
[Optical filters, Optical Character Recognition, text analysis, Shape, Filtering, Segmentation, font sizes, Optical computing, object detection, Bangla optical character recognition, Character recognition, Optical character recognition software, Information technology, Computer science, optical character recognition, Text recognition, Neural networks, Hidden Markov models, outer shape detection technique, printed Bangla text, Outer Shape Detection Technique]
Bangla Hand Written digit recognition using supervised locally linear embedding algorithm and Support Vector Machine
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents Bangla numeral character recognition system using supervised locally linear embedding algorithm and support vector machine (SVM). The locally linear embedding (LLE) algorithm is an unsupervised technique proposed for nonlinear dimensionality reduction. In this paper, we describe its supervised variant (SLLE). Where class membership information is used to map overlapping high dimensional data into disjoint clusters in the embedded space. we combined it with support vector machine (SVM) for classifying handwritten digits from the on-line handwritten Bangla numeral database.
[Image recognition, image classification, LLE, SVM, unsupervised technique, Clustering algorithms, class membership information, Bangla numeral character recognition system, Embedded computing, handwritten character recognition, support vector machines, natural language processing, Digit Recognition, Character recognition, Information technology, Nearest neighbor searches, unsupervised learning, Support vector machines, handwritten digit classification, Handwriting recognition, Image databases, support vector machine, online handwritten Bangla numeral database, SLLE, Support vector machine classification, Bangla hand written digit recognition, nonlinear dimensionality reduction, supervised locally linear embedding algorithm, Bangla Handwritten]
A new word separation algorithm for continuous Bangla Speech Recognition
2009 12th International Conference on Computers and Information Technology
None
2009
Word separation algorithm is needed to separate words from continuous speech for improving performance of Speech Recognition. Studies show that existing techniques suffer from separating words into disjoint sub-words and then we present a new word separation algorithm to overcome this problem. Prosody has great impact on Bangla speech and the algorithm is developed by considering prosodic feature with energy. At first continuous Bangla speech signals are fed into the system and the word separation algorithm separate speech into isolate words. Performance of the proposed algorithm is compared to the existing algorithms and result shows that 98% word boundaries are correctly detected.
[continuous speech, Vocabulary, Natural languages, Word Separation, Spatial databases, continuous Bangla speech signals, word separation, prosodic feature, Information technology, Stress, Continuous Bangla Speech, Human computer interaction, continuous Bangla speech recognition, speech recognition, Speech recognition, Speech enhancement, Pitch, Frequency, Autocorrelation]
Constructing SURF visual-words for pornographic images detection
2009 12th International Conference on Computers and Information Technology
None
2009
Pornographic images detection is necessary for us to filter out objectionable information on the Internet. Bag-of-visual-words (BoVW) based pornographic images detection is promising because it can compensate the defect of the traditional approach. However, there are many choices to construct visual-words which are crucial to the tradeoff between the speed and the performance. We propose a novel method of constructing SURF (speeded up robust features) visual-words in skin regions and combining it with color moments. The results show that the performance of SURF visual-words is better than that of SIFT (scale-invariant feature transform) visual-words and our method is more effective to detect pornographic images than many existing methods.
[SIFT, Shape, Humans, information filtering, object detection, Manufacturing processes, visualwords, feature extraction, Robustness, Computer networks, image colour analysis, scale-invariant feature transform, Computer aided manufacturing, Image edge detection, SURF visual-words, pornographic images detection, Information technology, Matched filters, SURF, color moments, bag-of-visual-words, Internet objectionable information filtering, Skin, Internet, speeded up robust features]
Palmprint based verification system robust to rotation, scale and occlusion
2009 12th International Conference on Computers and Information Technology
None
2009
This paper proposes an efficient palmprint based verification system which is robust to rotation, scale and occlusion. Images are obtained using a flat bed scanner. Scale invariant feature transform (SIFT) operator is used to extract features from the palmprint. Nearest neighbor ratio method is used to determine the similarity between extracted features of live and enrolled palmprints and to make matching decision. The proposed system has been tested using three databases-IITK database having 549 hand images, CASIA database with 5239 hand images and PolyU database of size 7752. Accuracy of the proposed system is found to be 99.97% with FAR of 0.06% in case of IITK database, while for CASIA and PolyU database is more than 99%. Further the robustness of the system with respect to scale rotation and occlusion has been studied.
[Biometrics, Fourier transforms, flat bed scanner, Discrete Fourier transforms, transforms, Spatial databases, Discrete wavelet transforms, scale invariant feature transform operator, biometrics (access control), Information technology, image matching, IITK database, Computer science, PolyU database, nearest neighbor ratio method, CASIA database, Image databases, feature extraction, decision making, Feature extraction, Robustness, palmprint based verification system, matching decision making]
Computer Vision-based Bangladeshi Sign Language Recognition System
2009 12th International Conference on Computers and Information Technology
None
2009
Sign language is a specific area of human gesture communication and a full-edged complex language that is used by various deaf communities. In Bangladesh, there are many deaf and dumb people. It becomes very difficult to communicate with them for the people who are unable to understand the sign language. In this case, an interpreter can help a lot. So it is desirable to make computer to understand the Bangladeshi sign language that can serve as an interpreter. In this paper, a computer vision-based Bangladeshi sign language recognition system (BdSL) has been proposed. In this system, separate PCA (principal component analysis) is used for Bengali Vowels and Bengali numbers recognition. The system is tested for 6 Bengali vowels and 10 Bengali numbers.
[Computer vision, Deafness, System testing, Vocabulary, handicapped aids, American Sign Language, Handicapped aids, Natural languages, full-edged complex language, human gesture communication, Pattern recognition, Bangladeshi Sign Language, Bengali vowel recognition, Bengali number recognition, gesture recognition, Hidden Markov models, Speech recognition, computer vision, Computer-Vision, Bangladeshi sign language recognition, deaf community, Principal Component Analysis, principal component analysis, Principal component analysis]
Multicore cluster implementations of hierarchical Bayesian cortical models
2009 12th International Conference on Computers and Information Technology
None
2009
We examine the parallelization of two recent biologically inspired hierarchical Bayesian cortical models onto two multicore processor based clusters. The models examined have been developed recently based on new insights from neuroscience and have several advantages over traditional neural network models. In particular, they need far fewer network nodes to simulate a biological scale cortical system than traditional neural network models, thus making them computationally more efficient. The two architectures examined are the Sony/Toshiba/IBM Cell BE and the Intel quad-core Xeon processors. Our results indicate that optimized implementations of the models on clusters of multicore processors can provide significant speedups and that such clusters are a promising approach for developing large scale simulations of the models. We show that for small scale implementations of the models, multicore clusters can provide speedups of about 850 times over serial implementations on the cell power processor unit.
[Multicore architectures, multicore cluster implementations, Sony/Toshiba/IBM Cell BE, Concurrent computing, computer architecture, biologically inspired hierarchical Bayesian cortical models, neural network models, Computer networks, multicore processor based clusters, Large-scale systems, biological scale cortical system, multiprocessing systems, Multicore processing, Biological system modeling, Computational modeling, multicore clusters, Neurons, neuroscience, large scale simulations, Bayesian methods, High performance computing, Biology computing, cell power processor unit, Brain modeling, Bayes methods, Artificial intelligence, Intel quad-core Xeon processors]
Performance study and simulation analysis of CSMA and IEEE 802.11 in wireless sensor networks and limitations of IEEE 802.11
2009 12th International Conference on Computers and Information Technology
None
2009
Since its birth wireless communication became an indispensible part of the modern society. One major area that has a gigantic impact on the performance of wireless sensor networks (WSNs) is the Medium Access Control (MAC) layer. Many random access protocols exist in wireless sensor networks. Some of these protocols include Carrier Sense Multiple Access (CSMA), Multiple Access with Collision Avoidance (MACA), Multiple Access with Collision Avoidance for wireless (MACAW) and IEEE 802.11. All the protocols mentioned above except CSMA use Request To Send/Clear To Send (RTS/CTS) packets to avoid collisions (hidden terminal problem) which was a great problem for CSMA and that is the reason CSMA is almost obsolete for wireless communications. But after using RTS/CTS packets the protocols have to encounter some extra problems such as, energy consumption and end-to-end delay. The objective of this paper is to show the pros and cons of using RTS/CTS packets by comparing CSMA (does not use RTS/CTS) and IEEE 802.11 (uses RTS/CTS packets). We also portray that under some specific scenario the IEEE 802.11 is outperformed by CSMA which is also the novelty and contribution of this research work. This observation suggests that a lot of works have to be done to consider IEEE 802.11 an approximate perfect MAC layer protocol for WSNs.
[IEEE 802.11, Energy consumption, wireless sensor networks, Wireless application protocol, MAC layer protocol, access protocols, Multiaccess communication, wireless communication, Wireless communication, Analytical models, packet radio networks, end-to-end delay, RTS/CTS packets, Performance analysis, CSMA, random access protocols, energy consumption, Access protocols, medium access control, hidden terminal problem, request to send/clear to send packets, telecommunication standards, Wireless sensor networks, multiple access with collision avoidance for wireless, Media Access Protocol, RTS/CTS, Collision avoidance, MAC layer protocols, MACAW, carrier sense multiple access]
An intelligent SMS-based remote Water Metering System
2009 12th International Conference on Computers and Information Technology
None
2009
The Integrated Prepaid Water Meter System is a technology for prepaid billing of water along with sufficient monitoring of the water meter readings automatically from a remote place without any human intervention. This system promises fast and accurate billing of water as well as prevents any misuse of it. In this paper, a technique having adequate security support, for prepaid billing of water using Short Message Service (SMS) has been illustrated. Existing Global System for Mobile communications (GSM) networks have been used for sending and receiving SMS. A prototype of the system has been designed and developed for system exploration and experiment.
[GSM, prepaid billing, Automatic meter reading, Computerized monitoring, Data security, Microcontroller, Humans, electronic messaging, water meters, integrated prepaid water meter system, global system for mobile communications, Meter reading, short message service, Information technology, Central server, Network servers, intelligent SMS-based remote water metering, Databases, Information security, SMS, Database, water meter readings, GSM networks, cellular radio]
A logical formal model for verification of Web Service Choreography
2009 12th International Conference on Computers and Information Technology
None
2009
Several methods and languages have been developed to describe computer system specifications and some of them have been deployed to validate and verify the functionality of the system. No one method is equally appropriate for all application domains. Service oriented architecture and using Web services is a wide growing domain of applications in software engineering these days, so to achieve more goals and benefits in large business works, we need to compose the existing Web services from different organizations. In this paper we discuss about Web service composition mechanisms and related formal methods and verification issues. One of the solutions in highest level is to use Web Service Choreography Definition Language (WS-CDL) to design the composition of services. With regard to this, we need to get sure about the correctness of choreography behavior, before to employ it for developing and executing services collaboration. In this paper we present a method using first order logic notation based on the partial-order planning problems. This method can be used for interactive systems that all participants have a common understanding of interaction rules. This method will be able to model variable specification of the system. It is consist of three parts included precondition, action and effect, so has a simple structure to understand. In this method we can check the properties of the model for reachability of ideal and safe states. We show the method in a Web service interaction case study and verify it in Prolog tool.
[Rule-based Model, variable specification, partial-order planning problems, formal specification, software architecture, Web Service Choreography, formal verification, interactive systems, computer system specifications, software engineering, Logic, logical formal model, Service oriented architecture, Prolog tool, Verification, Application software, Web service composition mechanisms, Information technology, first order logic notation, Web services, Interactive systems, Collaboration, formal methods, System recovery, service oriented architecture, Web Service Choreography Definition Language, Time factors, Software engineering]
Performance analysis of Datagram Congestion Control Protocol (DCCP)
2009 12th International Conference on Computers and Information Technology
None
2009
Applications like streaming audio, Internet telephony and multi-player online games prefer timeliness in packet delivery to reliability. TCP's reliability through packet retransmission and abrupt rate control features are unsuitable for these applications. As a result, these applications prefer UDP as the transport layer protocol. UDP does not have any congestion control mechanism which is vital for the overall stability of the Internet. For this reason, a new transport layer protocol datagram congestion control protocol (DCCP) has been introduced by the Internet Engineering Task Force (IETF). DCCP is suitable for these applications because of its exclusive characteristics. It can be useful for those applications which need a session and congestion control unlike UDP and do not need reliability or retransmission like TCP. However, since DCCP is a new protocol, its performance for these applications has to be analyzed thoroughly before it emerges as a de facto transport protocol for these applications. This paper describes the basic principle of DCCP, its congestion control mechanism and measures the performance of DCCP. The results show that DCCP provides better performance for those applications that suffers the tradeoff between delay and in-order delivery.
[Protocols, telecommunication congestion control, congestion control, streaming multimedia, IETF, transport layer protocol, DCCP, transport protocols, datagram congestion control protocol, session control, CCID3, CCID2, Performance analysis, DCCP protocol]
Performance evaluation of time dependent micro macro cellular network using MMPP traffic
2009 12th International Conference on Computers and Information Technology
None
2009
Two-dimensional (2D) and three-dimensional (3D) steady state Markov chains are widely used to analyze the traffic performance of communications networks. When the characteristics of the network changes with time, such steady state Markov chain is unable to determine different probability states. Markov Modulated Poisson Process (MMPP) is a special case of Markov Arrival Process (MAP) where arrival rate depends on probability states. In this paper, a traffic model of micro-macro cellular network of time dependent traffic load is modeled and its probability states are evaluated using MMPP varying load condition of the network under different parts of observation time.
[telecommunication network management, MMPP, Probability state and Sojourn time, Telecommunication traffic, time dependent traffic load, Counting Process, Kronecker sum, Steady-state, Information analysis, Markov arrival process, Traffic control, Computer networks, time dependent micro macro cellular network, 2D steady state Markov chains, communication networks, probability, performance evaluation, probability states, State-space methods, Information technology, 3D steady state Markov chains, micro-macro cellular network, Computer science, Land mobile radio cellular systems, Markov processes, Markov modulated Poisson process, MAP, cellular radio]
Design and analysis of smart antenna system for DECT radio base station in wireless local loop
2009 12th International Conference on Computers and Information Technology
None
2009
Digital Enhanced Cordless Telecommunication (DECT) can be a latent solution for wireless local loop (WLL) based communication system planning. In this paper, the design and simulation of a 8×8 planar microstrip antenna array and signal processing techniques of smart antenna systems for DECT radio base stations are presented. MUltiple SIgnal Classification (MUSIC) and Least Mean Square (LMS) signal processing algorithm techniques are analyzed and simulated for smart antenna system. Simulation results of the radiation characteristics, gain and return loss of the fixed beam planar array antenna have been produced by EM simulation software Zeland IE3D. Signal processing simulations were run in MATLAB. This smart antenna system is designed and simulated for DECT system in 1.88-1.90GHz frequency band.
[Algorithm design and analysis, Microstrip antenna arrays, least mean squares methods, bandwidth 1.88 GHz to 1.90 GHz, Array signal processing, signal processing technique, Planar array, smart antenna system, radio stations, multiple signal classification, WLL, Microstrip antennas, EM simulation software Zeland IE3D, adaptive antenna arrays, Signal design, least mean square algorithm, Base stations, MUSIC, telecommunication network planning, Multiple signal classification, Least squares approximation, planar antenna arrays, DECT, radio access networks, microstrip antenna arrays, signal classification, cordless telephone systems, planar microstrip antenna array, LMS, Smart antenna System, Signal processing algorithms, radio base station, Signal analysis, wireless local loop, digital enhanced cordless telecommunication, Matlab]
Moving object tracking - a parametric edge tracking approach
2009 12th International Conference on Computers and Information Technology
None
2009
In this paper, an edge based tracking algorithm is proposed. Our algorithm makes efficient use of edge-segment on the Canny edge map by utilizing the edge structure in the moving object region. Curvature-based features are used for moving edge registration. We use the maximum curvature correspondences between two edge segments then the 2D affine transformation computes their movement by solving a system of linear equations. The registration error is then minimized. A Kalman filter is used to track each individual edge segments. Segments are clustered using a k-mean algorithm. Finally, a group motion tracker is used for tracking moving object from each cluster. Experiments show that our edge-segment based tracking algorithm can track moving objects efficiently under varying illumination conditions.
[Tracking, Shape, Curvature Point, Kalman Filter, image registration, Kalman filter, Affine Transformation, Canny edge map, Information filtering, Data mining, edge segment, image segmentation, Clustering algorithms, Lighting, Information filters, object tracking, Robustness, Kalman filters, Kernel, moving edge registration, Image edge detection, group a motion tracker, 2D affine transformation, k-mean algorithm, curvature-based features, parametric edge tracking approach, Edge Segment based Moving Object Tracking, target tracking, linear equations, K-means Clustering]
Novel objective criteria for perceptual separation of two kinds of Distortion in speech enhancement applications
2009 12th International Conference on Computers and Information Technology
None
2009
There is an increasing interest in the development of robust quantitative speech quality measures that correlate well with subjective measures. This paper presents two objective criteria-the perceptual signal to audible noise ratio (PSANR) and the perceptual signal to audible distortion ratio (PSADR), to characterize the two kinds of degradation (i.e., residual background noise, speech distortion or both) in speech enhancement applications. For performance evaluation of speech enhancement algorithms it is necessary to determine with accuracy the kind of degradation present in the enhanced signal. Experimental results for speech enhancement using different well-known approaches depict the usefulness of the proposed objective criteria.
[Speech analysis, masking threshold, perceptual separation, Time measurement, PSANDR, Application software, Information technology, objective criteria, residual background noise degradation, Degradation, perceptual signal-to-audible noise ratio, speech quality measures, objective quality measure, Speech enhancement, Signal processing, speech distortion, Distortion measurement, Speech processing, speech enhancement, perceptual signal-to-audible distortion ratio, Testing, distortion]
Adaptation of ATAMSM to software architectural design practices for organically growing small software companies
2009 12th International Conference on Computers and Information Technology
None
2009
The architecture of a software application determines the degree of success of both operation and development of software. Adopted architectural options not only affect the functionality and performance of the software, but they also affect delivery related factors such as cost, time, changeability, scalability, and maintainability. It is thus very important to find appropriate means of assessing benefits as well as liabilities of different architectural options to maximize the life-time benefit and reduce the overall cost of ownership of a software application. The Architecture Tradeoff Analysis Method (ATAMSM) developed by Software Engineering Institute (SEI) is that kind of tool. Considerably this is a very big framework for dealing with architectural tradeoff issues faced by large companies for developing large as well as complex software applications. The practicing of full blown ATAM without taking into consideration of diverse forces affecting the value addition from its practice does not maximize benefits from its adoption. Related forces faced by small software companies are significantly different than those faced by large software companies. Therefore, ATAM should be adapted to make it suitable for the practice by small software companies. This paper presents the information about the architectural practice level of organically grown small software companies within the context of ATAM followed by the gap analysis between the industry practices and ATAM, and adaptation recommendations. Both literature review and field investigation based on key informant interview have been performed for this purpose. Based on the findings of this study an adaptation process of ATAM for the small companies has been proposed.
[Software maintenance, ATAM, software architectural design practices, Scalability, adaptation of ATAM, small software companies, Software performance, software quality, overall cost reduction, Information analysis, software architecture, architecture tradeoff analysis method, Computer architecture, Cost function, Samarium, ATAMSM adaptation, DP industry, gap analysis, Application software, complex software applications, Software Engineering Institute, project forces, Computer industry, organically growing small software companies, value optimization, Software engineering]
A recommended market research based approach for small software companies for improving systematic reuse capability in delivering customized software solutions
2009 12th International Conference on Computers and Information Technology
None
2009
This paper suggests the application of market research methodology to screen new software application ideas based on market analysis and shows how a software company can combine market research with new software product development to provide exciting customized software applications that better meet consumer requirements and make the company profitable. Both state-of-art-review and filed investigation have been performed to assess global as well as local practices of market research methodology in different industries including the software industry. Upon analysis of review outputs and field level investigation findings, a set of recommendations for practicing market research methodology for small software companies have been derived for improving systematic reuse based sustained capability improvement in delivering customized software applications in attractive market segments in an increasing profitable manner.
[profitability, Customized applications, Costs, Software reuse, small software companies, Companies, market research, consumer requirements, market analysis, software industry, Product-Line approach, Small software companies, software houses, Market research, software product development, Domain-specific Engineering, Productivity, DP industry, software development management, software company, Software market research, Application software, Computer science, customized software solutions, Software quality, Computer industry, Software systems, Systematic reuse capability, systematic reuse capability, customized software applications, Software engineering]
Using an old technique in a new technology &#x2014; A novel method for defining the scope of ISs in BPM projects
2009 12th International Conference on Computers and Information Technology
None
2009
Nowadays, using business process management (BPM) systems is so prevalent through the world, so that deploying information systems (IS) based on this technology has become one of the application development methods in the new age of software engineering. Due to acquisition and using this technology in many organizations without awareness or program, that is the main reason which would fail this type of projects, we are looking for an efficient method in order to grain information systems, based on organization's business processes for the sake of defining the scope of their related software development projects, in the shortest time and with the lowest cost. We have started our scientific researches on this field in a few projects with the nature of information technology (IT) planning and gained noteworthy results that one of them is described in this paper as a case study. This new method will be used as the first step in defining project portfolios according to BPM approach, and in some actions like enterprise architecture (EA) or IT/IS planning.
[Costs, information technology planning, Programming, Information systems, Information System (IS), software architecture, Technology management, Value Chain, Engineering management, business process management systems, Management information systems, Business Process Management (BPM), software engineering, information systems, project management, software development management, enterprise architecture, Affinity Analysis, Application software, Clustering, Information technology, Software development management, planning, software development projects, business data processing, organisational aspects, Software engineering]
Process centric work breakdown structure for easing Software Project management challenges: Business case analysis example
2009 12th International Conference on Computers and Information Technology
None
2009
Software project management involves coordinating various aspects of a project in order to bring forth a positive result. There have been increasing challenges faced by project managers. The major challenges of project management include unrealistic deadlines, communication deficit, uncertain dependencies, failure to manage risk, not well-defined vision and goals, and invisibility of the final product. Meeting these challenges is very difficult and in many cases they are not met which eventually leads to the project failure. This situation is even worse where project dispersion is higher. Different studies reveal that an IT project is more likely to be unsuccessful than successful and 7 out of 10 IT projects fail in some respect. The process centric work breakdown structure (WBS) can play a very vital role in this regard to improve the situations significantly. Instead of hierarchical work breakdown structures as a tree, the process centric WBS takes whole software production process and splits whole process into smaller process units which take right set of inputs with the right set of standar, put right set of practices in place that eventualy produce the work products with expected standards and quality. This process centric work breakdown structure helps project managers plan, estimate, monitor and control the project activities in greater accuracy and efficiency. In this article, business case analysis phase of software projects has been taken as an example to demonstrate how process centric WBS can help for easing project management challenges.
[project activity control, Project management, Easing Project Management Challenges, software quality, uncertain dependencies, business case analysis, Information analysis, unrealistic deadlines, Project Management Challenges, Process Centric Work Breakdown Structure, Production, software project management, process centric work breakdown structure, Estimation error, risk management, project dispersion, project management, Electric breakdown, software development management, project failure, software production process, project activity monitoring, Face detection, Information technology, Dispersion, Work Breakdown Structure, quality, Computer science, standards, Work Products, Software tools, communication deficit, software standards, IT project]
Offshore-outsourced software development risk management model
2009 12th International Conference on Computers and Information Technology
None
2009
Offshore-outsourced software development is gaining popularity because companies are continuously forced to reduce production costs while keeping sustainable competitive strength. However, this trend of software development increases projects' complexity and brings up risks to the overall project environment. Therefore, risks of offshore software development require to be managed as early as possible for a successful project. This paper considers a risk management model from a holistic perspective to manage offshore software development risk, integrated into early stages of development. The approach effectively identifies and specifies the goals of a project and the related risk factors. This is done at the basis of selected software development components within the running project. We show how to trace and control these risks already during early requirements engineering activities. The model at hand is implemented into an ongoing offshore software development project to (1) identify goals and risk factors from the local context and finally (2) to determine its applicability of the approach in offshore software development projects from a vendor's perspective.
[risk management, requirement engineering, Costs, Software development risk, Laboratories, Project management, software development management, Companies, Programming, Information technology, Software development management, requirements engineering, offshore-outsourced software development, risk management model, Continuous production, Risk management, goal modeling language, offshore outsourced software development, Context modeling]
Process centric Work Breakdown Structure of software coding for improving accuracy of estimation, resource loading and progress monitoring of code development
2009 12th International Conference on Computers and Information Technology
None
2009
This study introduces process centric Work Breakdown Structure (WBS) for managing challenges for coding phase of software development cycle. The approach represents WBS as a process centric view to understand the inter relationships and dependencies between activities. Suggested WBS reduces complexities and gaps of the project plan and its execution by early simulation of diverse coding activities through process centric view. It is believed that such process centric approach instead of tree form representation of WBS will improve the visibility of coding process during planning stage for increasing the accuracy of estimation, resource loading, and progress monitoring.
[Work breakdown structure, Costs, Process centric WBS, Project management, Manpower loading, Work product, Technology management, Production, software development cycle, Monitoring, process centric work breakdown structure, Electric breakdown, software development management, code development, Estimation, software coding, Estimation accuracy, progress monitoring, process centric view, coding process, Statistics, Information technology, coding phase, resource loading, Meeting planning, SW-CMMI, Resource management]
Novel Adaptive Step Power Control algorithm for 3G WCDMA cellular system
2009 12th International Conference on Computers and Information Technology
None
2009
Power control is an essential radio resource management method in CDMA cellular communication systems, where co-channel and adjacent-channel interferences are the primary capacity limiting factors. Power control intends to control the transmission power levels in such a way that required quality of service for the users is guaranteed with lowest possible transmission powers. In this paper, a modulation of power control algorithm is proposed for the 3G WCDMA system. The algorithm is figured on a modification of the transmitted power update step size. Instead of the fixed value presently suggested, the step size is modified dynamically in order to obtain more adapted power variations as well as the step is also represented as a function of the difference between the target and estimated SIR of the MS to obtain more stability of the system. A general form of this algorithm is presented and it is then studied in a simple simulation. Performance of the algorithm was evaluated with the outage percentage, which is the percentage of the number of MS's whose received SIR falls below the fixed threshold. The focused requirement, which had been tried to achieve by this algorithm, is the stability, which was studied and represented through simulation.
[Signal to Interference ratio, adaptive step power control algorithm, telecommunication network management, modulation, Bit error rate, 3G mobile communication, cochannel interferences, Multiaccess communication, Adaptive control, Power engineering computing, resource allocation, power control algorithm modulation, adjacent-channel interferences, radio resource management method, Average outage percentage, Stability, telecommunication control, WCDMA, Interference, adaptive control, code division multiple access, Power system modeling, Information technology, cochannel interference, 3G WCDMA cellular system, Novel ASPC, Programmable control, Power control, power control, ASPC]
Microstrip antenna array with four port Butler matrix for switched beam base station application
2009 12th International Conference on Computers and Information Technology
None
2009
In this paper, the design of a microstrip antenna array with four port Butler matrix is presented. The Butler matrix is used as a beamforming network and it produces orthogonal beams that can be steered in different directions. Simulated butler matrix has 10 dB return loss bandwidth of 20%. This matrix feeds four single element microstrip antennas that can be operated from 2.412 GHz to 2.484 GHz. The circuit is designed by considering a single layer microstrip structure that makes it simpler. The design of wide band microwave devices such as branch-line coupler; cross-coupler and phase-shifters are also incorporated. The switched beam antenna is designed for 2.4 GHz band Wi-Fi (Wireless Fidelity) system.
[Microstrip antenna arrays, single layer microstrip structure, Transmission line matrix methods, Array signal processing, microstrip antenna array, matrix feeds, switched beam base station application, Microstrip antennas, Bandwidth, Antenna feeds, antenna feeds, frequency 2.412 GHz to 2.484 GHz, array signal processing, Base stations, beamforming network, Circuit simulation, Wi-Fi, Butler matrix, orthogonal beams, wireless fidelity system, four port Butler matrix, microstrip antenna arrays, single element microstrip antennas, single layer structure, wireless LAN, Wideband]
Design of meandering probe fed microstrip patch antenna for wireless communication system
2009 12th International Conference on Computers and Information Technology
None
2009
A wideband, low cross-polarization patch antenna fed by meandering probe is presented in this paper. The patch antenna consists of inverted patch structure with air-filled dielectric, slotted patch and meandering probe. The composite effect of integrating these techniques and by introducing the proposed patch, offer a low profile, broadband, high gain, and low cross-polarization level. The results for the VSWR and co-and cross-polarization patterns are presented. The antenna operating the band of 1.84-2.29 GHz shows an impedance bandwidth (2:1 VSWR) of 22% and a gain of 10.6 dBi with a gain variation of 1.02 dBi. Good radiation characteristics, including cross-polarization level in xz- and yz-plane are -50 dB and -23 dB respectively, have been obtained.
[crosspolarization pattern, meandering probe, frequency 1.84 GHz to 2.29 GHz, low cross polarization patch antenna, Broadband antennas, Wireless communication, antenna radiation patterns, wideband antenna, Microstrip antenna, low cross-polarization, Microstrip antennas, Bandwidth, UHF antennas, inverted patch structure, antenna feeds, Probes, Dielectric constant, meandering probe fed microstrip patch antenna, broadband antenna, broadband antennas, microstrip antennas, slotted patch antenna, Geometry, wireless communication system, VSWR, slot antennas, Patch antennas, air filled dielectric, copolarization pattern, Frequency, radiocommunication, Impedance]
STP: In-network aggregation through proximity queries in a Sensor Network
2009 12th International Conference on Computers and Information Technology
None
2009
Event detection and notification is a common task in a wireless sensor networks (WSN). Efficient data aggregation and minimization of energy consumption are the great research challenges in WSN. In WSN, aggregated event information is more important than individual event information for energy saving and reliability. Proximity queries or query approximation can be used to reduce the complexity of data aggregation and energy consumption. This paper presents an efficient and scalable hybrid framework for processing spatial and temporal proximity queries in WSN which we call STP. STP builds tree structure with less overhead, and reduces the event propagation cost through proximity queries. STP reduces energy consumption by reducing the number of aggregator nodes, which ultimately increases the network life time. STP eliminates the unnecessary aggregation of events using a tunable temporal proximity threshold. We compare STP's performance with another spatial query processing method and we show that, STP performs better.
[Energy consumption, query approximation, Costs, Military computing, wireless sensor networks, tree structure, wireless sensor network, telecommunication computing, spatial-and-temporal proximity query processor), query processing, in-network aggregation, Aggregation, spatial query processing method, tree data structures, spatial and temporal query, Tree data structures, Base stations, Redundancy, Routing, tunable temporal proximity threshold, Computer science, Wireless sensor networks, energy consumption minimization, WSN, Query processing, sensor network, proximity, event detection, event notification]
Adaptive energy detection for cognitive radio: An experimental study
2009 12th International Conference on Computers and Information Technology
None
2009
A cognitive radio (CR) is able to sense spectral environment over a wide range of frequencies, and provide opportunistic access to frequency bands temporarily unoccupied by an incumbent. Accurate channel sensing is the first important task for a CR, and energy detector is often used for this purpose. While a normal energy detector works well with well chosen window size based on prior knowledge about possible primary users, it often fails with signals that are narrow compared to the detector window, or if only a fraction of the signal is inside the detector window. We propose an adaptive energy detector that can adjust its detection window, and evaluate such detector's performance using experimental results obtained through a real time implementation.
[frequency bands access, adaptive energy detection window, FCC, spectral environment, energy detector performance, Cognitive radio, Information technology, labview, cognitive radio, channel sensing, Detectors, Chromium, energy detection, Frequency, Signal detection, Signal to noise ratio, Power engineering and energy, Testing, energy detector model]
A QoS aware route selection mechanism using Analytic Hierarchy Process for Mobile Ad Hoc Network
2009 12th International Conference on Computers and Information Technology
None
2009
To truly realize potential of MANET, multimedia services must be provisioned with a minimum level of QoS. To meet the QoS requirement of such services, many attributes need to be considered. To keep the routing process lightweight, standard QoS aware routing protocol in MANET works with one or two such parameters. In this paper, we have proposed an on-demand source routing protocol for MANET that works with six important QoS attributes by varying priority for different category of traffic flow. We have reflected this variation by incorporating Analytic Hierarchy Process in the proposal.
[routing process, route selection mechanism, Traffic Categorization, Telecommunication traffic, Throughput, AHP, Delay, Mobile ad hoc networks, Information analysis, analytic hierarchy process, QoS, Routing protocols, Computer networks, mobile ad hoc network, mobile radio, Decision making, routing protocol, quality of service, Information technology, Weight, Computer science, MANET, telecommunication network routing, ad hoc networks, statistical analysis]
Improved Needham-Schroeder protocol for secured and efficient key distributions
2009 12th International Conference on Computers and Information Technology
None
2009
A key distribution procedure is an essential constituent of secured exchange of information between the participants. In this paper, a fast symmetric key distribution technique with additional security services is presented. The aim of the proposed technique is to improve the conventional Needham and Schroeder five-message protocol in four aspects. First aspect is to introduce an additional authentication level in originator's identity. Second aspect is to provide the integrity of the originator's message. Third aspect is to reduce the time needed to distribute session-key between pair of entities. And the fourth aspect is to develop the key freshness security issue. A comparative analysis between conventional and proposed technique is presented to visualize the improvements of the proposed one. C programming language is used to implement the technique and running time is measured by using the time function and it is found that proposed technique is faster than the conventional one. For the purpose of threat analysis, several attacks, such as altering the message information, are applied by force on the proposed technique to check whether it will provide the security services or not. And the result of threat analysis is that the proposed technique provides all the security services. Hence, the proposed technique will bring a new dimension in key distribution paradigm.
[key freshness security issue, Entity Authentication, Protocols, authentication level, cryptographic protocols, Needham-Schroeder protocol, key distribution paradigm, C language, programming languages, Distributed computing, key distribution technique, Cities and towns, message information, five-message protocol, Secret Key, Cryptography, security services, Random number generation, message passing, Data security, Message Integrity and Security, Information technology, Computer science, Key Distribution, Authentication, Information security, message authentication, C programming language, key distribution procedure, Key Confirmation]
Analysis and design of individual Tax return systems in Bangladesh
2009 12th International Conference on Computers and Information Technology
None
2009
Proper analysis and good design are essential for the successful implementation of any computer aided system. The sensitivity of the Tax return system amplifies the significance of analysis and design by many fold. Developed countries have achieved high tax return rates by introducing simple, Internet-based, computer-aided Tax return systems. Valuable insights can be gained by analyzing the tax re- turn systems of the developed countries, which can serve as a base for designing an effective tax return system for an underdeveloped country. In this work we analyze the relative merits and demerits of individual tax return systems in United Kingdom (UK), Canada and Bangladesh. Based on this analysis we propose a design for automating individual tax return system for the National Board of Revenue, Bangladesh.
[Economic indicators, National Board of Revenue, Government, Europe, CAD, United Kingdom, Information technology, North America, System analysis and design, Information analysis, Automated tax return, Bangladesh, Cellular phones, individual Tax return systems, Internet-based systems, Modems, Internet, computer-aided Tax return systems, Continents, taxation, Tax return system, Evaluation criteria]
Proposed domain name system (DNS) for improved e-government services of Bangladesh
2009 12th International Conference on Computers and Information Technology
None
2009
In recent time Bangladesh government has given lot of attention to the electronic government (e-government) to fulfill the vision of digital Bangladesh. This paper focused specifically on whether Bangladesh e-government domain names comply with the standard of other countries Domain Name or not. I have also proposed a standard domain name system for all government web sites so that the citizens and others will able to locate the site easily and will get better, secure e-government services. The consequences of this issue together with how to address it are discussed in this paper. The results of this study will provide Bangladesh government a clear concept of the current situation of e-government websites' domain names which should be helpful to the evaluation of e-government websites in Bangladesh.
[Bangladesh government, Computer vision, Costs, website, e-government services, DNS, Domain Name System, government Web sites, Information technology, E-government, URL, Uniform resource locators, Computer science, standard domain name system, Digital Bangladesh, Investments, Communications technology, Internet, Web sites, electronic government, government data processing, digital Bangladesh, Electronic government]
Challenges in building trust in B2C e-Commerce and proposal to mitigate them: developing countries perspective
2009 12th International Conference on Computers and Information Technology
None
2009
In this technology mediated world, e-commerce is becoming more and more powerful medium for doing business, globally. Existing security technologies are proven to protect online transaction and fund transfer. However, while transacting with global e-merchants, trustworthiness of secure fund transfer and delivery of products/services are affected, and they could not leverage the potential benefit of e-commerce. In this paper, the authors firstly overview the current state-of-threat by surveying was people's perception about trust in e-commerce in developing countries like Bangladesh and argue that the security requirements of e-commerce service generally go beyond the more traditional requirements of network security. The result analysis shows that, challenges in building trust for business-to-consumer (B2C) e-commerce venture (local/international) is the major concern. The authors propose an e-commerce enabled model for secure electronic fund transfer, and discuss ways to mitigate challenges in building trust in B2C.
[Local Trusted Third Party, Protocols, B2C e-commerce, Law, Psychology, Proposals, Electronic commerce, Security, electronic fund transfer, Information technology, Trust, e-Commerce, global e-merchant, security of data, B2C, Transaction, Secure Electronic, security technology, Internet, business-to-consumer service, Consumer electronics, Legal factors, electronic commerce]
Biometric authentication from low resolution hand images using radon transform
2009 12th International Conference on Computers and Information Technology
None
2009
Biometric authentication refers to the automatic verification of a person's identity from physiological or behavioral characteristics presented by him or her. In this paper an authentication scheme from hand images is presented. Instead of dealing with hand measurements, typically termed as `hand geometry', this method verifies with entire hand shape. Peg free and position invariant features are calculated using Radon Transform. Low resolution hand images captured by a document scanner are processed to extract feature vectors. The proposed scheme is tested on a data set of 136 images with simple Euclidian norm based match score. The method attained an Equal Error Rate (EER) of 5.1%.
[Biometrics, object recognition, Error analysis, Shape measurement, Humans, Fingerprint recognition, biometrics (access control), Hand Geometry, low resolution hand images, feature vector extraction, Fingers, feature extraction, Radon transform, Radon transforms, equal error rate, biometric authentication, Morphological Image Processing, image resolution, Testing, Image resolution, hand shape, Radon Transform, Euclidian norm based match score, image matching, Geometry, Biometric Authentication, identity verification, Authentication, document scanner, position invariant features, peg free features]
Transmembrane helix prediction using feed-forward neural network
2009 12th International Conference on Computers and Information Technology
None
2009
Neural network is one of the successful methods for protein secondary structure prediction. Day to day this technology is modified, improved, even other methods also combined with it to get better result. In this paper we trained feed-forward neural network with trans-membrane protein for helix prediction. Using Java object oriented neural engine (JOONE) our achieved accuracy is 71%. This paper is expected to benefit researchers in proteomics by presenting a summary of developments of neural network in this area.
[Java, search engines, transmembrane protein, proteomics, feed-forward neural network, Amino acids, transmembrane helix prediction, Java object oriented neural engine, Feedforward neural networks, Information technology, Biological neural networks, Proteins, feedforward neural network, biology computing, Neural networks, bioinformatics, Biology computing, protein secondary structure prediction, Biomembranes, Feedforward systems, Bioinformatics, a-helix, feedforward neural nets]
Detection of various denial of service and Distributed Denial of Service attacks using RNN ensemble
2009 12th International Conference on Computers and Information Technology
None
2009
Denial-of-Service (DoS) and Distributed Denial-of-Service (DDoS) are widely known security attacks which attempt to make computer resources unavailable to its intended users. In this paper, I discuss some well known DoS and DDoS attacks. Experience shows that in the detection of these attacks human brain is more perfect than mathematical computation. Therefore, I propose a technique to incorporate the representative of human brain, Recurrent Neural Networks (RNN), to identify these attacks.
[security attacks, Recurrent neural networks, Distributed-Denial-of-Service, Flood attack, Zombie, recurrent neural nets, Denial-of-Service, distributed processing, Computer crashes, Floods, distributed denial of service attacks, Computer crime, Distributed computing, Information technology, computer network security, RNN ensemble, Operating systems, recurrent neural networks, Internet, Artificial intelligence, Computer security, IP spoofing]
Bayesian networks application for representation and structure learning of gene regulatory networks
2009 12th International Conference on Computers and Information Technology
None
2009
The cell functions and development are regulated by complex networks of genes, proteins and other components by means of their mutual interactions. These networks are called gene regulatory networks (GRNs). GRNs are used to reveal the fundamental gene regulatory mechanisms, to determine the reasons for many diseases and interactions between drugs and their targets. The introduction of experimental technologies such as microarrays, ChIP-chip which combines chromatin immunoprecipitation (ChIP) with microarrays and ChIP-Seq which combines ChIP with DNA sequencing, has provided a large number of available datasets related to gene expression and transcription factors (TFs) and their interactions. These datasets are basis for further analysis to reveal the gene regulation mechanisms. Many models have been applied to represent gene regulatory networks. We have used the dynamic Bayesian network model which is able to cope with missing data and can include a prior knowledge about transcription factors and their activation/inhibition of corresponding genes. We describe the obtained results and survey the common structure learning algorithms for learning of GRN's structure. We tested the obtained GRN for test datasets with different sizes and in the paper describe obtained dependencies between the ratio of Bayesian score and BIC and dataset size.
[cell function, transcription factor, cellular biophysics, Genomics, Proteins, genetics, gene regulatory networks, biology computing, proteins, Complex networks, Management information systems, protein, Computer networks, belief networks, learning (artificial intelligence), Bioinformatics, disease, drugs, diseases, Information technology, gene regulatory network, structure learning, Diseases, Bayesian methods, DNA, dynamic Bayesian network, Bayesian networks]
Steps to defend against DoS attacks
2009 12th International Conference on Computers and Information Technology
None
2009
Defending against DoS attacks is extremely difficult; effective solutions probably require significant changes to the Internet architecture. We present a series of architectural changes aimed at preventing most flooding DoS attacks, and making the remaining attacks easier to defend against. The goal is to stimulate a debate on tradeoffs between the flexibility needed for future Internet evolution and the need to be robust to defend attack.
[telecommunication security, Steps to Defend Against DoS Attacks, Denial-of-Service, Telecommunication traffic, Internet architecture, Reflection, Computer crime, Information technology, Network servers, Filters, security of data, DoS Attacks, DoS attacks, Robustness, Internet evolution, Internet, Defend Against DoS Attacks, denial of service, Protection, Web server]
Secure e-cash model using Java based smartcard
2009 12th International Conference on Computers and Information Technology
None
2009
Association of a true observer guaranties electronic cash not to be double-spent by any means. Java card is a smartcard which represents one of the smallest computing platforms. A major challenge influencing the design and implementation of e-cash observer in Java card is the limited availability of computing resources in it. In this paper, we show a new methodology of blending and associating high-level CORBA based bank server, user wallets and resource-constrained Java based observer. We choose a realistic e-cash scheme and show its successful implementation. We also analyze performance of Java card with various lengths of secret keys used for generating electronic coins.
[high-level CORBA based bank server, secure e-cash model, secret keys, electronic money, electronic cash, e-cash observer, Observer, computing resources, electronic coins, resource-constrained Java based observer, Hardware, Performance analysis, e-cash scheme, Cryptography, Communication networks, distributed object management, Java card, Availability, Java, Binary Tree, Blending Technique, Information technology, Java based smartcard, Cryptographic protocols, Computer science, user wallets, security of data, Binary trees]
Present status and critical success factors of e-Commerce in Bangladesh
2009 12th International Conference on Computers and Information Technology
None
2009
Online trading between businesses or individuals has employed attention of corporations worldwide as they are challenged to remain viable through difficult economic conditions. Despite the losses of so many businesses two years ago when the ¿dot-com bubble¿ burst, no serious business analyst disagrees that electronic commerce is steadily transforming how business is done, hence changing the business environment globally. Businesses everywhere need to understand if, when and how to use electronic commerce. The organizational factors, which are critical to the success of e-commerce, are investigated in this research. Different pieces of literature report different factors as key to success and generally based on subjective, perceptual data. A synthesis of existing literature is a basis for survey questions. The data is collected from Bangladesh e-commerce based organizations who are offering their goods &amp; services on electronic channels, using postal questionnaires and Interview technique. The top factors found to be most critical for the success in e-commerce are: quick responsive products/services, organizational flexibility, services expansion, systems integration and enhanced customer service. An important lesson from this research is that organizations need to view the e-commerce initiative as a business critical area rather that just a technical issue. They need to give attention to internal integration, which may include channels, technology and business process integration, and improving the overall services to their customers.
[business process integration, business analyst, Electronic commerce, Environmental economics, organizational factors, economic conditions, electronic channels, quick responsive products/services, Computer networks, Business, electronic commerce, online trading, enhanced customer service, Statistical analysis, business critical area, services expansion, Customer service, business environment, Information technology, customer services, Critical Success Factors, Bangladesh, e-commerce initiative, Asia, dot-com bubble, systems integration, Internet, international trade, Consumer electronics, organisational aspects, organizational flexibility]
Extraction of interesting rules from internet search histories
2009 12th International Conference on Computers and Information Technology
None
2009
Rule extraction aims to ultimately improve business performance through an understanding of past and present search histories of customers. A challenging task is to determine interesting rules from their heterogeneous search histories of shopping in the Internet. For this purpose neural network (NN) and canonical correlation analysis (CCA) are used. Customers visit Web pages one after another and leave their valuable search information behind. Firstly we produce a homogeneous data set from their heterogeneous search histories. It is difficult task to produce a homogeneous data from heterogeneous data without changing their characteristics of data. Secondly these data are trained by unsupervised NN to get their significant class. Thirdly we extract the maximally correlated customers by using CCA and then interesting rules are extracted among their maximally correlated customer. This is important for the traders, marketers and customers for making future business plan.
[data mining, Companies, consumer behaviour, Internet search history, History, Data mining, neural network, Web page visit, customers, Pattern analysis, interesting rule extraction, electronic commerce, Data analysis, information retrieval, Maintenance engineering, business performance, canonical correlation analysis, Heterogeneous data, business planning, Information technology, Statistics, marketing, customer profiles, Neural networks, Web pages, Internet shopping, Internet, information search, Rule Extraction, neural nets]
FM-Chord: Fault-tolerant Chord supporting misspelled queries
2009 12th International Conference on Computers and Information Technology
None
2009
Searching with partial knowledge and misspelled keywords is a challenging problem in peer-to-peer networks. This paper presents FM-chord, a novel searching mechanism for information retrieval with queries containing spelling and pronunciation mistakes in keywords using a widely-used structured overlay Chord. FM-chord uses double metaphone encoding and 3-grams of keywords to reduce the effect of edit distance between the advertisement and query keywords. FM-Chord replicates advertisement to achieve fault tolerance to node failures. We evaluate the performance of our proposed mechanism in FM-Chord with necessary simulation results.
[Knowledge engineering, search engines, Military computing, peer-to-peer computing, Peer to peer computing, peer-to-peer networks, Natural languages, information retrieval, Routing, fault-tolerant chord, Information technology, P2P networks, Computer science, Fault tolerance, FM-chord, Bandwidth, distributed systems, Computer networks, fault tolerant computing, spelling mistakes, Chord, double metaphone encoding]
Integrated data warehousing for telecommunication industries
2009 12th International Conference on Computers and Information Technology
None
2009
Data warehousing provides an excellent opportunity in transforming operational data into useful and reliable information to support the decision making process in any organization. Data warehouse (DW) generalizes and consolidates multidimensional (MD) data. Hence, DW has become an important platform for online analytical processing (OLAP) which is based on a MD data model. In this paper, we propose an integrated data warehouse system for the telecommunication companies in Bangladesh .Our integrated DW provides a common framework based on temporal data from different telecommunication operators rendering their services. We develop a dimensional model architecture, data extraction methodology, transformation and loading techniques which provides analytical options with the implementation of relational OLAP.
[Bangladesh telecommunication companies, data mining, Companies, Data engineering, Reliability engineering, decision making process, Data mining, relational OLAP, telecommunication computing, Information analysis, telecommunication industries, Dimensional modeling, CDR, data extraction methodology, Warehousing, feature extraction, rendering (computer graphics), dimensional model architecture, Data analysis, multidimensional data, telecommunication industry, Data warehouses, OLAP, service rendering, Telecommunications, online analytical processing, data warehousing integration, Communication industry, Data Warehousing, ETL, data warehouses]
Content clustering of Computer Mediated Courseware using data mining technique
2009 12th International Conference on Computers and Information Technology
None
2009
Computer Mediated Courseware (CMC) has been developed so far for individual courses considering single or multiple text books. A group of courseware can be developed by using multiple text books and in this case, it is a requirement to cluster the contents of different books to form a generalized clustered content. No work has been found to develop this generalized clustered content. We have proposed a methodology based on data mining techniques to construct a hierarchical general structure of a group of courseware combining the individual structure of a set of books. The clustering will help the courseware developer to dynamically allocate contents to develop different courses using a group of books. We have applied this methodology for different level of courses on database. The methodology is generalized and can be applied to any other courses.
[Courseware, Proximity, data mining, World Wide Web, Data engineering, Educational institutions, computer mediated courseware, Data mining, Clustering, Engineering education, Information technology, content clustering, Databases, Sections, pattern clustering, Synonym, Books, courseware]
An extendible data structure for handling large multidimensional data sets
2009 12th International Conference on Computers and Information Technology
None
2009
Multidimensional array is widely used in large number of scientific research and engineering applications for handling large multidimensional data. There exist many data structures to represent multidimensional data. But most of these data structures are static (such as traditional multidimensional array) and can not handle the dynamic extension or reduction of the array. The Traditional Multidimensional Array (TMA) is efficient in terms of accessing the elements of the array by random computing the addressing function but TMA is not extendible during run time. In this paper we propose a new scheme, Karnaugh Representation of Extendible Array (KEA), to represent the multidimensional data. The main idea of this scheme is to represent n dimensional array by a set of two dimensional extendible arrays. The scheme can be extended in any direction during run time. To evaluate our proposed scheme, we implement and compare with the existing systems for different operations with the Traditional Multidimensional Array (TMA), and Traditional Extendible Array (TEA). Our experimental result shows that the KEA scheme outperforms TMA and TEA.
[Multidimensional systems, Costs, Extendible Multidimensional Array, data analysis, Karnaugh Map, 2D extendible arrays, Relational databases, Data structures, Data engineering, Application software, Information technology, traditional multidimensional array, Computer science, Information science, large multidimensional data sets, Distributed databases, Range Query, Karnaugh representation, Time and storage cost, random computing, data structures, Multidimensional Array, extendible data structure]
Performance evaluation of MIMO system incorporating water filling model and minimum eigenvalue constraints
2009 12th International Conference on Computers and Information Technology
None
2009
In this paper, both equal power and water filling models are simulated for comparison of their performance in a multiple-input multiple-output environment. The effects of fast fading and the shadowing effects have been incorporated in the models. Minimum eigenvalue required for successful transmission for individual link is evaluated from the probability density functions of eigenvalue and equivalent uncoupled multiple-input multiple-output link. Impact of the number of antenna elements of an array on the minimum eigenvalue of an uncoupled channel is analyzed based on probability density function of eigenvalue of the channel matrix. The analysis shows that the cutoff eigenvalue decreases with increase in the number of antenna elements and signal to noise ratio of the received signal.
[water filling model, channel capacity, channel matrix, antenna elements, eigenvalues and eigenfunctions, shadowing effects, minimum eigenvalue constraints, antenna arrays, Probability density function, MIMO, Filling, Eigenvalues and eigenfunctions, multiple-input multiple-output environment, MIMO communication, uncoupled channel, Fading, MIMO system, probability, equivalent uncoupled multiple-input multiple-output link, Rayleigh channels, probability density functions, Antenna elements, Power system modeling, Shadow mapping, shadowing effects and SNR, Receiving antennas, Rayleigh fading, Signal analysis, fast fading effects, Antenna arrays]
WiMAX security analysis and enhancement
2009 12th International Conference on Computers and Information Technology
None
2009
The importance of IEEE 802.16, worldwide interoperability for microwave access (WiMAX) is growing and will compete with technologies such as 3G. The acceptance and adoption of technologies also depend on security. Therefore, this article shows security vulnerabilities found in WiMAX and gives possible solutions to eliminate them. We find the initial network procedure is not effectively secured that makes man-in-the-middle attack possible. Focusing on this attack, we propose Diffie-Hellman (DH) key exchange protocol to enhance the security level during network initialization. We modify DH key exchange protocol to fit it into mobile WiMAX network as well as to eliminate existing weakness in original DH key exchange protocol. Finally we found that the proposed algorithm shows 2.5 times better performance in comparison with existing systems.
[telecommunication security, Man-in-the-Middle, mobile radio, Sealing Function, mobile WiMAX network, Data security, Diffie-Hellman key exchange protocol, IEEE 802.16, Access protocols, WiMAX, WiMax, Physical layer, network initialization, Privacy, Microwave technology, Information security, DH-HEMTs, WiMAX security analysis, Key Generation, IEEE standards, Cryptography, protocols, Protection, worldwide interoperability for microwave access]
Performance evaluation of a mobile cellular network with two hop ad-hoc relaying
2009 12th International Conference on Computers and Information Technology
None
2009
In a mobile cellular network, both new originating calls and handoff calls can be dropped at any time in the midst of a conversation when a user enters in a dead spot. Because of this, there is always some discrepancy between traffic originated by the mobile station (MS) and that of received by the base station (BS). A probability model is used in this paper to evaluate this discrepancy considering geometry of the service area and probability density function of user distribution. Blocking probability experienced by the BS is found less than that of evaluated from the offered traffic of user end, which is the central idea of this paper. Furthermore, the same traffic model is applied to an ad- hoc two hop relay mobile cellular system for performance evaluation.
[blocking probability, wireless mobile communication, probability model, Telecommunication traffic, Mobile communication, Relays, handoff calls, new originating call, originating calls, Traffic control, Computer networks, traffic model, base station, probability density function, Base stations, mobile radio, handoff call, probability, telecommunication network topology, performance evaluation, Ad hoc networks, mobile station, ad-hoc two hop relay mobile cellular system, mobile cellular network, ad-hoc relaying, Land mobile radio cellular systems, Signal processing, ad hoc networks, Mobile computing, telecommunication traffic, cellular radio, Ad-hoc relaying]
Numerical analysis of impedance matched Inverted-L antennas for Wi-Fi operations
2009 12th International Conference on Computers and Information Technology
None
2009
This paper presents the numerical simulations of inverted-L and stair inverted-L antennas capable of generating high gain with less than 1.5 dBi gain variation within the -10 dB return loss bandwidth for 5.5 GHz wireless-fidelity (Wi-Fi) operation with and without resistor-inductor-capacitor (RLC) impedance matching network. Moreover, the proposed antennas can provide bandwidth of 510 and 120 MHz respectively, making it easily cover the required bandwidths for Wi-Fi operation in the 5.5 GHz band. In application of matching network, the input impedance of the antennas well matched to the feeding cable, also improvement in return loss and voltage standing wave ratio (VSWR) is achieved.
[Costs, bandwidth 510 MHz, resistor-inductor-capacitor impedance matching network, impedance matched inverted-L antennas, frequency 5.5 GHz, Impedance matching, Microstrip antennas, Bandwidth, antenna theory, wireless-fidelity operation, Antenna feeds, Computer networks, Inverted-L antenna, voltage standing wave ratio, Dipole antennas, microwave antennas, Wi-Fi, Wireless local area network, Loaded antennas, Matching network, impedance matching, VSWR, Slot antennas, Numerical analysis, stair inverted-L antennas, Wi-Fi operations, wireless LAN, return loss, bandwidth 120 MHz, Stair inverted-L antenna]
Performance evaluation of fast TCP and TCP Westwood+ for multimedia streaming in wireless environment
2009 12th International Conference on Computers and Information Technology
None
2009
Although TCP is one of the key protocols of the Internet infrastructure, it is not optimized for either wireless environment or multimedia streaming applications. It fails to meet the service requirement of streaming applications because of its strict adherence to congestion control. Besides, non-congestion packet loss fools TCP to slow down its sending rate over wireless links. Fast TCP and TCP Westwood+ are two well known TCP variants which are supposed to resolve these problems. This paper analyzes the performance of TCP Westwood+ and Fast TCP for various issues in wireless network for multimedia streaming. Simulation results show that, Fast TCP shows better goodput, average throughput, and fairness index but TCP Westwood+ shows less jitter and better performance in presence of heavy reverse flows.
[Transport protocols, transport control protocols, Wireless Network, telecommunication congestion control, Jitter, Throughput, TCP Westwood+, wireless links, Delay, Multimedia streaming, service requirement, Wireless networks, wireless environment, media streaming, fast TCP, Performance analysis, wireless network, multimedia streaming applications, Fast TCP, congestion control, non-congestion packet loss, Application software, Information technology, sending rate, Internet infrastructure, transport protocols, Streaming media, Internet]
A novel two-tier multiple sequence alignment algorithm
2010 13th International Conference on Computer and Information Technology
None
2010
Multiple sequence alignment is one of the basic research areas in bioinformatics. In most cases while doing multiple sequence alignment, a totally unaligned set of sequences are used as an initial data. In this work, we have used Clustal X 2.0.11 program to process those initial data in some aligned way. Then roulette wheel selection operator is used to select the optimal cut points of sequences. Dynamic programming is used to align the subset sequences. By comparing both these two process we found that the result is gradually improving. Our result has proved that we got higher score. In the paper &#x201C;Shyi-Ming Chen, Chung-Hui Lin, and Shi-Jay Chen; Multiple DNA Sequence Alignment Based on Genetic Algorithms and Divide-and-Conquer Techniques International Journal of Applied Science and Engineering; 2005; 3, 2:89-100&#x201D; we have observed that for the same sequences final score was 25. But using our method we got the final score 32. Some experimental results has shown using graph in order to test the proposed approached.
[multiple DNA sequence alignment, Gallium, divide and conquer methods, two tier multiple sequence alignment algorithm, Multiple sequence alignment, Wheels, dynamic programming, Clustal X 2.0.11 program, genetic algorithms, Optimization, genetic algorithm, Dynamic Programming, Hidden Markov models, DNA, roulette wheel selection operator, divide and conquer technique, bioinformatics, Matrices, Dynamic programming, subset sequence, Clustal X]
A low cost realization of quantum double qutrit multiplier using ion-trap scheme
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper, we have shown a new design for double qutrit multiplier for quantum computer. This multiplier can be an essential part of quantum arithmetic logic unit (ALU). This proposal is based on ion-trap scheme of quantum computer, which is practically realizable using current technology.
[Computers, multiplying circuits, quantum computer, Muthukrishnan-Stroud gate, Ion-trap scheme, Quantum computer, quantum gates, Double qutrit multiplier, Multivalued logic, Ions, Proposals, ion-trap scheme, quantum double qutrit multiplier, Quantum computing, quantum arithmetic logic unit, Logic gates, Ternary logic, Adders]
An approximation algorithm for bounded degree closest phylogenetic 2nd root problem
2010 13th International Conference on Computer and Information Technology
None
2010
The degree &#x0394;-closest phylogenetic 2nd root problem (&#x0394;CPR<sub>2</sub>) is an NP-hard problem concerning phylogenetic tree reconstruction from a graph representing the similarities of the species concerned. Here we present an approximation algorithm for this problem for any fixed &#x0394; &gt;; 3. When |V| &gt;; 3&#x0394; - 1, our algorithm yields an approximation ratio of max((&#x0394;-2)/&#x03B1;, 2), where &#x03B1; &gt;; 1 is a constant whose value depends on the values of |V| and &#x0394;.
[Algorithm design and analysis, approximation ratio, approximation theory, &#x0394;-closest phylogenetic second root problem, trees (mathematics), phylogenetic root, Phylogeny, genetic algorithms, Approximation methods, phylogenetic tree reconstruction, Equations, NP-hard problem, evolutionary biology, Clustering algorithms, approximation algorithm, Approximation algorithms, phylogeny, computational complexity]
An efficient seeded tree alignment algorithm for finding the similarity score of two RNA secondary structures
2010 13th International Conference on Computer and Information Technology
None
2010
This paper presents an efficient O(n3) time algorithm for solving the seeded tree alignment problem that finds the similarity score of two RNA secondary structures. In the seeded tree alignment problem, a large tree, representing an RNA secondary structure, is converted into a small tree known as seeded tree. After conversion, a comparison operation is being placed to find the similarity score of necessary seed pair of two seeded trees and finally the overall trees. The algorithm is more efficient than the best known algorithm that needs O(n3.5) time.
[Algorithm design and analysis, macromolecules, RNA similarity score, RNA, time algorithm, Primary Seeds, Complexity theory, Normalized Weighted Tree, Equations, Niobium, RNA secondary structures, biology computing, comparison operation, Seeded Tree, Secondary Seeds, Mathematical model, molecular biophysics, seeded tree alignment algorithm, Similarity Score, Periodic structures, computational complexity]
Quantum Evolutionary Algorithm based on Particle Swarm theory in multiobjective problems
2010 13th International Conference on Computer and Information Technology
None
2010
Quantum Evolutionary Algorithm (QEA) is an optimization algorithm based on the concept of quantum computing and Particle Swarm Optimization (PSO) algorithm is a population based intelligent search technique. Both these techniques have good performance to solve optimization problems. PSEQEA combines the PSO with QEA to improve the performance of QEA and it can solve single objective optimization problem efficiently and effectively. In this paper, PSEQEA is studied to solve multi-objective Optimization (MO) problems. Some well-known non-trivial functions are used to observe the performance of PSEQEA to detect the Pareto optimal points and the shape of the Pareto front using both Fixed Weighted Aggregation method and Adaptive Weighted Aggregation method. Moreover, Vector Evaluated PSEQEA (VEPSEQEA) borrows concept from Schaffer's Vector Evaluated Genetic Algorithm (VEGA) that can also cope with MO problems. Simulation results show that PSEQEA and VEPSEQEA perform better than PSO and VEPSO to discover the Pareto frontier.
[adaptive weighted aggregation method, Pareto optimisation, quantum evolutionary algorithm, Multi objective Optimization, Pareto optimal points, particle swarm optimisation, Quantum Evolutionary Algorithm, Weighted Aggregation method, genetic algorithms, Particle swarm optimization, Optimization, Equations, Biological cells, artificial intelligence, vector evaluated genetic algorithm, multiobjective optimization, Quantum computing, Pareto frontier, intelligent search technique, quantum computing, Logic gates, particle swarm optimization algorithm, vector evaluated PSEQEA, Particle Swarm Optimization]
Two dimensional Range Minimum/Maximum Query revisited
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper, we present a cache oblivious efficient data structure to solve the two dimensional variant of the Range Minimum/Maximum Query (RMQ) problem.
[Algorithm design and analysis, Range Minima/Maxima Query.A, Computational modeling, data structure, cache storage, Complexity theory, Partitioning algorithms, cache complexity, Range Minima/Maxima Query.lgorithms, Computer science, query processing, Cache Complexity, lgorithms, data structures, Arrays, range minimum-maximum query]
Comparison of artificially intelligent methods in short term rainfall forecast
2010 13th International Conference on Computer and Information Technology
None
2010
Rainfall forecasting has been one of the most scientifically and technologically challenging task in the climate dynamics and climate prediction theory around the world in the last century. This is due to the great effect of forecasting on human activities and also for the significant computational advances that are utilized in this research field. In this paper our main objective is to forecast over a very short-term and specified local area weather using local data which is not always available by forecast center but will be available in the future by social network or some other methods. For this purpose in this paper we have applied three different algorithms belonging to the paradigm of artificial intelligence in short-term forecast of rainfalls (24 hours) using a regional rainfall data of Bihar (India) as a case study. This forecast is about predicting the categorical rainfall (some pre-defined category based on the amount of total daily rainfall) amount for the next day. We have used two classifier ensemble methods and a single classifier model for this purpose. The ensemble methods used in this paper are LogitBoosting (LB), and Random Forest (RF). The single classifier model is a Least Square Support Vector Machine (LS-SVM). We have optimized each of the models on validation sets and then forecast with the optimum model on the out of sample (or test) dataset. We have also verified our forecast results with some of the latest verification tools available. The experimental and verification results suggest that these methods are capable of efficiently forecasting the categorical rainfall amount in short term.
[least mean squares methods, verification tool, climate prediction theory, Weather forecasting, Frequency measurement, random forest, Radio frequency, Rain, weather forecasting, Accuracy, short term rainfall forecasting, Forecast verification, artificially intelligent method, single classifier model, Rainfall amount, Artificially intelligent method, pattern classification, rain, support vector machines, LogitBoosting, local area weather, Forecasting, classifier ensemble method, Support vector machines, climatology, least square support vector machine, Bihar rainfall data]
Curvelet texture based face recognition using Principal Component Analysis
2010 13th International Conference on Computer and Information Technology
None
2010
A vital issue for face recognition is to represent a face image by effective and efficient features. To-date a numerous feature extraction techniques have been proposed in the literature. Among them, content based image retrieval (CBIR) using curvelet transform captures accurate texture features to represent the image. In this paper, we propose a novel face recognition method that uses curvelet texture features for face representation. Features are computed by low order statistics like mean and standard deviation of transformed face images. Since the spectral domain of curvelet has no hole or overlap, there is no loss of frequency information in face images. Moveover, such feature representation has considerably low dimension. Thus, computation within the face-space becomes easier. Furthermore, the dimension of features is independent of face image resolution. As a result, it can support face images of different resolution as input. To build the classifier, we apply PCA on the concatenated feature representation of subdivisions. We test our system with 4 and 5 levels of scales of curvelet transform. We also experiment by dividing the face image into different number of sub-divisions on three standard databases. The experimental results confirm that curvelet texture features achieve satisfactory performance for face recognition.
[feature representation, Transforms, transforms, Content based image retrieval (CBIR), feature extraction, curvelet transform, face recognition, Face, image resolution, Gabor filter, Face recognition, low-order statistics, Image retrieval, Curvelet transform, content-based retrieval, image texture, curvelet texture features, Image Retrieval, feature extraction techniques, face image resolution, image representation, image retrieval, Feature extraction, face-space computation, Principal Component Analysis (PCA), content based image retrieval, principal component analysis, Principal component analysis]
Designing an effective Fuzzy Logic Controller based on Quantum Evolutionary Algorithm
2010 13th International Conference on Computer and Information Technology
None
2010
This paper proposes a new approach based on Quantum Evolutionary Algorithm (QEA) for effective selection and definition of fuzzy if-then rules to design Fuzzy Logic Controllers (FLCs). The majority of works done on designing FLCs were based on knowledgebase derived from imprecise heuristic knowledge of experienced operators or persons but they were difficult and time consuming to evaluate. The proposed approach decomposes the test problem in such a way that leads to more effective knowledge acquisition and improved control performance in fuzzy control. In this self-learning adaptive method, Truck backer-upper problem, an excellent test-bed for fuzzy control systems is considered as test problem. Each rule base is represented by a real-coded triploid chromosome. At each generation of QEA, rules are updated using Complementary Double Mutation Operator (CDMO) and Discrete Crossover (DC). The experimental results on backing up the truck problem show that the proposed approach to design FLCs do better in terms of time needed to backing up the truck.
[Gallium, self-learning adaptive method, fuzzy set theory, Evolutionary computation, real-coded triploid chromosome, Biological cells, fuzzy control, fuzzy if-then rules, discrete crossover, truck backer-upper problem, Loading, knowledge based systems, Trajectory, quantum evolutionary algorithm, Fuzzy Rule base, Backing up a truck, Quantum Evolutionary Algorithm, knowledge acquisition, quantum theory, control system synthesis, Fuzzy Logic Controller, Pragmatics, unsupervised learning, Fuzzy logic, evolutionary computation, Defuzzification, effective fuzzy logic controller design, test bed, complementary double mutation operator]
Evolutionary multi-objective clustering with adaptive local search
2010 13th International Conference on Computer and Information Technology
None
2010
In many real-world applications, the accurate number of clusters in the data set may be unknown in advance. In addition, clustering criteria are usually high dimensional, nonlinear and multi-model functions and most existing clustering algorithms are only able to achieve a clustering solution that locally optimizes them. Therefore, a single clustering criterion sometimes fails to identify all clusters in a data set successfully. This paper presents a novel multi-objective evolutionary clustering algorithm based on adaptive local search that mitigates the above disadvantages of existing clustering algorithms. Unlike the conventional local search, the proposed adaptive local search scheme automatically determines whether local search is used in an evolutionary cycle or not. Experimental results on several artificial and real data sets demonstrate that the proposed algorithm can identify the accurate number of clusters in the data sets automatically and simultaneously achieves a high quality clustering solution. The superiority of the proposed algorithm over some single-objective clustering algorithms and existing multi-objective evolutionary clustering algorithms is also confirmed by the experimental results.
[Gallium, multiobjective evolutionary clustering algorithm, data set, Search problems, multi-objective evolutionary optimization, Encoding, set theory, Biological cells, Iris, evolutionary computation, Accuracy, optimisation, pattern clustering, Clustering algorithms, Adaptive local search, adaptive local search, search problems, multi-objective clustering]
On the performance of Recurring Multistage Evolutionary Algorithm for continuous function optimization
2010 13th International Conference on Computer and Information Technology
None
2010
Recurring Multistage Evolutionary Algorithm is a novel evolutionary approach that is based on repeating conventional, explorative and exploitative genetic operations in order to perform better optimization with improved robustness against local optima. This work compares the performance of RMEA with that of classical evolutionary algorithm, differential evolution and particle swarm optimization on a test suite of 50 different benchmark functions. The test functions include unimodal and multimodal, separable and non-separable, regular and irregular, low and high dimensional functions. Very few works have been tested on a similar range of benchmark problems. The experimental results show that the performance of RMEA is comparable to and often better than the other mentioned algorithms.
[continuous function optimization, explorative genetic operations, exploration, particle swarm optimisation, Stochastic processes, Evolutionary algorithm, Evolutionary computation, particle swarm optimization, function optimization, Optimization, differential equations, differential evolution, exploitation, evolutionary computation, RMEA, recurring multistage evolutionary algorithm, Euclidean distance, Benchmark testing, Genetics, exploitative genetic operations, Manganese]
Self-adaptation of mutation step size in Artificial Bee Colony algorithm for continuous function optimization
2010 13th International Conference on Computer and Information Technology
None
2010
This paper introduces a novel adaptation scheme of mutation step size for the Artificial Bee Colony algorithm and compares its results with a number of swarm intelligence and population based optimization algorithms on complex multimodal benchmark problems. The Artificial Bee Colony (ABC) is a swarm based optimization algorithm mimicking the intelligent food foraging behavior of honey bees. The proposed scheme dynamically adapts the mutation step size for better exploration and exploitation of the search space. Mutation with large step size is likely to produce large variations which would facilitate better exploration of the undiscovered regions of the search space while small step size usually produces small variations that are better for exploitation of the already found solutions. The appropriateness of small or large steps changes dynamically depending on the current stage and maturity of the ongoing search process as well as the properties of the search space. So, dynamic adaptation of mutation step size is a promising and interesting research direction that has not been explored so far with the ABC algorithm. This paper introduces Artificial Bee Colony with Exponentially Distributed Mutation (ABC-EDM) that incorporates exponential distributions to produce mutation steps with varying lengths and suitably adjusts the current step length. ABC-EDM is compared on a number of benchmark functions with the original ABC algorithm, Genetic Algorithm (GA), Particle Swarm Optimization (PSO) and Particle Swarm Inspired Evolutionary Algorithm (PS-EA). Results demonstrate that ABC-EDM performs better optimization with lower dimensionality, but the improvement fades away with increased number of dimensions.
[Algorithm design and analysis, artificial bee colony algorithm, Gallium, continuous function optimization, Heuristic algorithms, Exponential distribution, particle swarm optimisation, complex multimodal benchmark problem, particle swarm inspired evolutionary algorithm, Optimization, artificial intelligence, genetic algorithm, Function optimization, Benchmark testing, search space, ABC algorithm, search problems, population based optimization algorithm, exponentially distributed mutation, genetic algorithms, Particle swarm optimization, artificial immune systems, exponential distribution, Artificial bee colony, Genetic algorithm, ongoing search process, mutation step size, intelligent food foraging behavior, swarm intelligence]
Soft computing models to predict daily temperature of Dhaka
2010 13th International Conference on Computer and Information Technology
None
2010
Soft computing forecasting tools play an important role to forecast many complicated systems. In this paper, an effort has been made to use soft computing approaches to predict Dhaka daily temperatures for the period of 28 February 1945 to 27 August 2006. We have selected the fuzzy neuro model, the neuro genetic algorithm model as soft computing techniques. To compare results, a popular time series statistical technique, namely autoregressive moving average model is selected and based on error analysis, a suitable model to predict temperature for Dhaka city is proposed. The performance comparisons of different models due to root mean square error, correlation coefficient and coefficient of determination between observed and predicted temperatures indicate that the neuro genetic algorithm model predicts temperatures with maximum accuracy, followed by the fuzzy neuro model. Our believe findings of this paper will be useful for those who are interested about Bangladeshi important atmospheric parameter, namely temperature.
[neuro genetic algorithm, atmospheric temperature, Statistical error measures, autoregressive moving average processes, forecasting tool, Predictive models, Dhaka city, soft computing, daily temperature prediction, weather forecasting, mean square error methods, correlation coefficient, fuzzy neuro model, Computational modeling, error analysis, root mean square error, Integrated systems, fuzzy neural nets, Artificial neural networks, fuzzy logic, Prediction, geophysics computing, time series, genetic algorithms, Soft computing, Forecasting, Time series model, Temperature measurement, Fuzzy logic, coefficient of determination, Genetic algorithm, Data models, Artificial neural network, autoregressive moving average model, Autoregressive processes]
Spatio-temporal template discovery using rough set theory
2010 13th International Conference on Computer and Information Technology
None
2010
Real-time stream data is characterized by spatial and temporal variability and is subject to unbounded or constantly evolving entities. The challenge is how to aggregate these unbounded data streams at different spaces and times to provide effective decisions making in real-time. This paper proposes a rough set-based sliding window framework for stream data aggregation. Based on current data streams, it identifies interesting spatio-temporal patterns, and generates rough set If ... Then decision rules. Proposed formalism has been tested on sea surface temperature data from NOAA's TAO/TRITON project. Such a pattern-based data aggregation scheme has the potential to significantly reduce data communications in decision making.
[Real time systems, Heuristic algorithms, data mining, NOAA TAO project, TRITON project, Data mining, Ocean temperature, Information systems, sea surface temperature data, Clustering algorithms, decision making, Temporal Template, Set theory, real-time stream data aggregation, Soft Computing, spatio-temporal template discovery, data handling, pattern-based data aggregation scheme, Rough Set Theory, rough set theory, rough set-based sliding window framework, Data Stream]
A context free grammar and its predictive parser for bangla grammar recognition
2010 13th International Conference on Computer and Information Technology
None
2010
Parsing is a process of transforming natural language into an internal system representation, which can be trees, dependency graphs, frames or some other structural representations. If a natural language be successfully parsed then grammar checking from this language becomes easy. In this paper we describe a context free grammar for Bangla language and hence we develop a Bangla parser based on the grammar. Our approach is very much general to apply in Bangla Sentences and the method is well accepted for parsing a language of a grammar. The scheme is based on Top down parsing method and to avoid the left recursion the idea of left factoring is adopted.
[Context, Computers, natural language processing, Context Free Grammar, Natural languages, Bangla grammar recognition, Predictive Parser, Grammar, Parse Table, left factoring idea, top-down parsing method, grammar checking, context free grammar, XML, Production, Top down and Bottom up Parser, Syntactics, context-free grammars, Bangla sentences, natural language, Bangla parser, predictive parser, Bangla Language processing]
A multidimensional partitioning scheme for developing English to Bangla dictionary
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper we describe a multidimensional implementation scheme for developing English to Bangla dictionary using multidimensional Array. We have converted the string into an integer key and partitioned the keys based on number of letters a word. Multidimensional arrays are good to store dense data. It is hard to use multidimensional array for sparse data. We have compressed the sparse multidimensional array by computing the offset value. We found good results for storage and retrieval costs. Our proposed model is explained with sufficient example and performance analysis is described with experimental results. The proposed scheme shows superiority over traditional schemes.
[Computers, Dictionaries, Computational modeling, natural language processing, information retrieval, English to Bangla dictionary, retrieval costs, Equations, sparse multidimensional array, Bangla Dictionary, NLP, Partitioning, storage costs, Speech, integer key, Arrays, Mathematical model, multidimensional partitioning scheme, Storage and Retrieval cost, dictionaries, Multidimensional array]
An efficient speech generation method based on character and modifier of Bangla PDF Document
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper we present a method for Bangla speech generation from Bangla PDF document. Our main goal is to generate almost natural speech from Bangla PDF document. For this we have proposed a method which performs three major tasks. One is PDF to text conversion, then text to ASCII conversion, and then follows the character and modifier rules while reading text and finally speech generation from the concatenate sound files of text. We have analyzed the proposed method and the existing methods with respect to error rate of reading all the Bangla words and also the time complexity.
[Computers, speech generation, text analysis, Error analysis, Bangla PDF document, concatenate, PDF-to-text conversion, Complexity theory, Engines, natural speech, TTS, Databases, sound files, PDF, modifier rules, speech processing, Performance analysis, Modifier, natural language processing, Character, Bangla, time complexity, text-to-ASCII conversion, Bangla words, Speech, character rules]
Bangla phoneme recognition for ASR using multilayer neural network
2010 13th International Conference on Computer and Information Technology
None
2010
This paper presents a Bangla phoneme recognition method for Automatic Speech Recognition (ASR). The method consists of two stages: i) a multilayer neural network (MLN), which converts acoustic features, mel frequency cepstral coefficients (MFCCs), into phoneme probabilities and ii) the phoneme probabilities obtained from the first stage and corresponding &#x0394; and &#x0394;&#x0394; parameters calculated by linear regression (LR) are inserted into a hidden Markov model (HMM) based classifier to obtain more accurate phoneme strings. From the experiments on Bangla speech corpus prepared by us, it is observed that the proposed method provides higher phoneme recognition performance than the existing method. Moreover, it requires a fewer mixture components in the HMMs.
[Computers, Automatic Speech Recognition, Conferences, hidden Markov model, regression analysis, linear regression, Phoneme Probabilities, multilayer neural network, automatic speech recognition, Information technology, hidden Markov models, speech recognition, Acoustic Features, mel frequency cepstral coefficients, Multilayer Neural Network, Hidden Markov Models, Bangla phoneme recognition, neural nets]
Conversion of Bangla sentence for Universal Networking Language
2010 13th International Conference on Computer and Information Technology
None
2010
Conversion from Bangla language to another native language using Universal Networking Language (UNL) is highly demanding due to increasing the usage of Internet based application. Since Bangla case structure plays a fundamental role in Bangla grammartical structures, this paper presents some rules for Bangla case structures that will be used to convert Bangla sentence to UNL expression. The theoretical analysis shows that the defined rules can be used successful conversion of Bangla sentence.
[Dictionaries, Universal Networking Language, Instruments, natural language processing, Natural languages, Enconverter, Computer science, Internet based application, Bangla grammartical structures, Semantics, universal networking language, Bangla case structure, Bangla Case, Enconversion Rule, Internet, Bangla language, Bangla sentence conversion, Periodic structures]
Special feature extraction techniques for Bangla speech
2010 13th International Conference on Computer and Information Technology
None
2010
This paper describes several feature extraction techniques, which will facilitate Automatic Speech Recognition (ASR) for Bangla speech. These techniques are applied on different sound-packets, which are essentially segments of Bangla speech. The key temporal regions in a sound-packet that contain vital information about the speech signal are identified. Some novel feature extraction methods are developed using the information contained within these key regions. It has been observed that a single feature cannot provide enough information to achieve successful automatic speech recognition; rather a combination of the features can be used effectively to increase the accuracy.
[Bangla speech, Zero Crossing Rate, automatic speech recognition, Mel frequency cepstral coefficient, Frequency domain analysis, Automatic speech recognition, speech recognition, feature extraction techniques, feature extraction, Mel-Frequency Cepstral Coefficients, Sound-Packet, Speech, Feature extraction, Formant Frequency, Sound-Unit]
A new approach of Extendable Multicast Routing Protocol in Mobile Ad Hoc Networks
2010 13th International Conference on Computer and Information Technology
None
2010
Multicasting is a challenging task that facilitates group communication among the nodes using the most efficient strategy to deliver the messages over each link of the network. In spite of significant research achievements in recent years, efficient and extendable multicast routing in Mobile Ad Hoc Networks (MANETs) is still a difficult issue. To enhance performance and to enable scalability we have proposed a domain-based Extendable Multicast Routing Protocol (EMRP) for hierarchical multicasting in MANET environments. In the proposed technique, each domain has a sub-source that reduces the path length between the original source and intended receiver which solute the scalability issue. We have analyzed the performance with respect to a variety of parameters for different mobility speed and group sizes. Results obtained through simulations demonstrate enhanced performance in packet delivery ratio and end-to-end delay of the proposed technique as compared to the existing ones.
[Extendable Multicasting, mobile ad hoc network, Hierarchical Routing, Receivers, Routing, Domain Based Multicasting, mobility speed, radio receivers, extendable multicast routing protocol, packet delivery ratio, Mobile ad hoc networks, packet radio networks, MANET, end-to-end delay, multicast protocols, mobile ad hoc networks, routing protocols, hierarchical multicasting, Mobile Ad Hoc Networks, Routing protocols, Tree Based Protocol, Mobile computing]
Bivariate gamma distribution: A plausible solution for joint distribution of packet arrival and their sizes
2010 13th International Conference on Computer and Information Technology
None
2010
Network traffic has been studied extensively since new findings by Taqqu et. al. (1994), which has shown that network traffic is not memoryless. Such traffic has been called self similar with Long Range Dependence (LRD) and their distribution is commonly known as heavy-tailed. It is very hard to estimate buffer size to protect against overflow in presence of such traffic as packet sizes and their arrival count is positively correlated. Queuing analysis of network devices consider only arriving packets irrespective of their sizes, but existing network protocols allow for variable packet sizes. This can lead to higher overflow probability. This paper examines network traffic heavy-tailedness assumption via number of experimentation on connectionless service traffic. Number of arrival per second and number of bytes transferred per second are found to be highly correlated across lags. Based on these findings, this work proposes bivariate gamma distribution for joint probability distribution of these parameters.
[Bivariate Gamma distribution, Ethernet networks, Correlation, Exponential distribution, Switches, Heavy-tail distribution, Mean excess, Delay, long range dependence, packet radio networks, gamma distribution, queuing analysis, protocols, Local area networks, queueing theory, bivariate gamma distribution, probability, Self similarity, joint probability distribution, network protocols, network traffic, packet arrival, Queueing analysis, telecommunication traffic, LRD]
Construction of a multi-level Hierarchical Quasi-Cyclic matrix with layered permutation for partially-parallel LDPC decoders
2010 13th International Conference on Computer and Information Technology
None
2010
Implementation of partially-parallel (Low-Density Parity-Check) LDPC decoders using unstructured random matrices is very complex and requires huge hardware resources. To alleviate the complexity and minimize resource requirements, structured LDPC matrices are used. This paper presents a novel technique for constructing a multi-level Hierarchical Quasi-Cyclic (HQC) structured matrix for LDPC decoders. A unique multi-level structure of the proposed matrix provides flexibility in generating different code lengths and code rates for various applications such as WiMAX, WLAN and DVB-S2. In addition, different combinations of permuted sub-matrices are inserted in layers, to provide virtual randomness in the LDPC matrix. Simulations results show that the HQC matrices generated using the proposed technique have a marginal loss of less than 0.1 dB at a bit error rate (BER) performance of 10-5, compared to unstructured random matrices. The proposed matrix therefore provides BER performance close to random matrices while significantly reducing hardware resource requirements.
[Wireless LAN, flexible structures, error correction codes, Bit error rate, parity check codes, layered permutation, WiMAX, WiMax, cyclic codes, WLAN, Decoding, Complexity theory, Sparse matrices, multilevel hierarchical quasicyclic matrix, matrix algebra, wireless local area network, structured LDPC matrices, bit error rate performance, partially-parallel LDPC decoders, DVB-S2, digital communication, Parity check codes, wireless LAN, low-density parity-check decoders]
Development of the smart QoS monitors to enhance the performance of the NS2 Network Simulator
2010 13th International Conference on Computer and Information Technology
None
2010
A widespread methodology for performance analysis in the field of communication systems engineering is network simulation. Network Simulator 2 (NS2) is one of the most popular tools in academia for evaluation of network, protocols and topologies. It has become virtually the standard of network simulation. It represents a discrete, event-based simulator that has an ability to be easily extendible and modifiable due to its open source nature. However, a major shortcoming of NS2 is its limited scalability in terms of memory usage and simulation run-time which get worsen due to further post processing of huge, multi-format output files. In this paper, to alleviate the short comings of ns2, we have developed two smart `quality of service' (QoS) monitors for NS2. They produce a very small, well formatted output file for wired, wireless or hybrid networks, especially to evaluate the quality of service of any IP based network reducing the memory usage. In addition, the QoS monitors are capable of generating the user friendly graphical representation of throughput, jitter, loss probability and delay without any post processing which greatly reduces the post processing time also.
[Protocols, NS2 network simulator, graphical user interfaces, Quality of service, user friendly graphical representation, Network Simulator 2, loss probability, discrete simulator, simulation run-time, QoS parameters, IP based network, IP networks, protocols, discrete event simulation, Monitoring, communication systems engineering, smart QoS monitors, Object oriented modeling, open source nature, QoS monitors, memory protocols, performance evaluation, Agents, memory usage, quality of service, multiformat output file processing, jitter, delay, Memory management, performance analysis, network evaluation, event-based simulator]
Dynamic TDMA slot reservation protocol for cognitive radio ad hoc networks
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper, we propose a dynamic TDMA slot reservation (DTSR) protocol for cognitive radio ad hoc networks. Quality of Service (QoS) guarantee plays a critically important role in such networks. We consider the problem of providing QoS guarantee to users as well as to maintain the most efficient use of scarce bandwidth resources. A dynamic frame length expansion and shrinking scheme that controls the excessive increase of unassigned slots has been proposed. This method efficiently utilizes the channel bandwidth by assigning unused slots to new neighboring nodes and increasing the frame length when the number of slots in the frame is insufficient to support the neighboring nodes. It also shrinks the frame length in an effective way. Our proposed scheme, which provides both QoS guarantee and efficient resource utilization, be employed to optimize the channel spatial reuse and maximize the system throughput. Extensive simulation results show that the proposed mechanism achieves significant performance improvement in multichannel cognitive radio ad hoc networks.
[Protocols, TDMA, Quality of service, Throughput, Ad hoc networks, quality of service, Cognitive radio, dynamic TDMA slot reservation protocol, QoS guarantee, dynamic frame length expansion, channel bandwidth, Time division multiple access, dynamic frame length shrinking scheme, cognitive radio, time division multiple access, dynamic frame length, Bandwidth, multichannel cognitive radio ad hoc network, wireless channels, ad hoc networks]
Influence of polarization mode dispersion on the BER performance of DS-OCDMA
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper, a theoretical analysis is presented to evaluate the influence of polarization mode dispersion (PMD) on the bit error rate (BER) performance of direct sequence optical code division multiple access system. In this analysis, intensity modulation direct detection technique is employed in single mode fiber operating at 1550nm, and optical orthogonal codes are used as address sequence. Optoelectronic conversion is performed by an avalanche photodiode in an optical correlator receiver. The system BER performance is determined on account of receiver, optical amplifier, and multiuser access interference noises. The power penalty suffered by the system is evaluated at BER of 10-9 as a function of number of simultaneous users, chip rates, and PMD co-efficient. The system performance is found to degrade more at higher chip rate, and longer fiber length due to the PMD.
[optical correlator receiver, Bit error rate, spread spectrum communication, polarization mode dispersion, intensity modulation direct detection technique, optical receivers, intensity modulation, optoelectronic conversion, optical communication, error statistics, light polarisation, avalanche photodiode, OOC, PMD, optical orthogonal codes, Optical receivers, code division multiple access, Optical transmitters, bit error rate, BER Performance, direct sequence optical code division multiple access, Optical fiber amplifiers, DS-OCDMA, MAI, optical dispersion]
Novel design of Address Generator for WiMAX multimode interleaver using FPGA based finite state machine
2010 13th International Conference on Computer and Information Technology
None
2010
Wireless technology is the fastest growing segment of the modern communication industry. The IEEE 802.16e standard, commonly known as mobile WiMAX, is the latest wireless technology that has promised to offer Broadband Wireless Access over long distance. The concept of OFDM is used in WiMAX to obtain high data rate in addition to reducing the effects like inter symbol interference and inter channel interference. It has proved to be the air interface for next generation Broadband Wireless System. In this paper, we present a finite state machine based novel technique to model the Address Generation circuitry of WiMAX multimode interleaver using VHDL on FPGA platform with all code rates and modulation schemes of IEEE 802.16e standard. Our approach provides better performance in terms of maximum operating frequency, use of flip-flops with negligible loss in terms of logic cells utilized compared to existing FPGA based implementations. Measured circuit parameters and software simulation of this model are also provided.
[software simulation, intersymbol interference, OFDM, field programmable gate arrays, FPGA, next generation networks, broadband networks, address generation circuitry, hardware description languages, mobile WiMAX multimode interleaver, finite state machines, VHDL, interchannel interference, broadband wireless access technology, flip flops, flip-flops, next generation broadband wireless system, FSM, Multimode Interleaver, WiMAX, WiMax, Generators, Address Generator, modern communication industry, radio access networks, Phase shift keying, address generator, logic cell, IEEE 802.16e standard, finite state machine, Integrated circuit modeling, Field programmable gate arrays, adjacent channel interference]
Pagemap: A dynamic user guiding approach to the most relevant and popular pages in a website
2010 13th International Conference on Computer and Information Technology
None
2010
In this blooming age of information technology the web developers always intend to provide a vast information space to the Internet surfers. On the other side, the users always want to reach the desired information by least effort. As a result, the increasing volume of information in each website also enlarges the searching domain for the user. In many cases, the user fails to spot his desired information at a quick outlook and then frequently switches to other similar websites. Some approaches like Sitemap, Footprints etc. give an overview of the site contents but often fail to guide the user to his destined information. This paper proposes a new approach named Pagemap, which can guide a user to the most popular and relevant pages of that website based on user's current location within that domain. With the assistance of the access log file, Pagemap uses the previous browsing history to find the most popular pages in a website. Pagemap uses the vector space model to find out the pages with maximum matching in compare to the current page of the user. Usability analysis method is used to justify the efficiency of Pagemap which shows it's the usefulness of this approach.
[Web developers, Website, Web Usability, Data mining, dynamic user guiding approach, Internet surfers, Guidelines, Information Space, Sitemap, Web pages, Search engines, Footprints, usability analysis method, Web Analytics, Internet, Web sites, Usability, Pagemap]
Performance evaluation of AODVH: An Ad hoc networking scheme for hybrid nodes
2010 13th International Conference on Computer and Information Technology
None
2010
Mobile Ad hoc networks (MANETs) are composed of a collection of independent nodes usually connected by low bandwidth Radio Frequency (RF) links which are challenging for transmitting video and high bandwidth applications for rescue workers in emergency situations. Earlier we proposed a routing protocol, called AODVH to use for high bandwidth communication for search and rescue operations in a disaster area which consists of hybrid nodes using Free Space Optical (FSO) and RF links, with FSO being the primary link and RF as backup in the case of failure of &#x201C;FSO only&#x201D; paths. The objective of this paper is to evaluate the performance of AODVH using ns-2 simulations with varying network size and compare with three other Ad hoc routing protocols. We also implemented and tested AODVH in the laboratory using a testbed consisting of Linux based Ad hoc nodes. Results show that AODVH performs better in terms of packet loss, end-to-end delay, overhead, packet delivery ratio, route discovery frequency and throughput when compared to with the other three protocols.
[radio links, optical links, ns-2 simulation, computer networks, performance evaluation, routing protocol, Throughput, Routing, Ad hoc networks, free space optical links, Network mobility, Delay, hybrid nodes, Radio frequency, AODVH, Linux, mobile ad hoc networks, routing protocols, mobility model, Routing protocols, radio frequency links, performance analysis]
Performance limitations in fiber Bragg grating based optical add-drop multiplexer due to crosstalk
2010 13th International Conference on Computer and Information Technology
None
2010
Wavelength division multiplexing (WDM) optical networks are attracting more and more attention because of their ability to provide increased capacity and flexibility. Optical add-drop multiplexer (OADM) become key components for add or drop wavelengths in high bit rate optical networks. Crosstalk in OADM degrades the performance of WDM system. In this article, we have developed a fiber Bragg-grating based OADM with low crosstalk. We have also developed analytical models for relative intensity noise (RIN), bit error rate (BER) and power penalty to study the performance limitations of this OADM. Results show that crosstalk, RIN and BER of the proposed OADM are lower and provide better performance than the existing OADMs.
[Optical fibers, Isolators, wavelength division multiplexing, relative noise, Noise, Bit error rate, Crosstalk, Optical fiber networks, bit error rate, Bragg gratings, crosstalk, optical network, optical fibre networks, Optical crosstalk, fiber Bragg grating, optical add drop multiplexer, optical add/drop multiplexer, optical crosstalk, relative intensity noise, power penalty, multiplexing equipment, OADM]
Performance of sub-carrier mapping in single carrier FDMA systems under radio mobile channels
2010 13th International Conference on Computer and Information Technology
None
2010
Single carrier-frequency division multiple access (SC-FDMA) utilizes single carrier modulation at the transmitter and frequency domain equalization at the receiver. It has almost similar performance and essentially the same overall structure of an orthogonal frequency division multiple access (OFDMA) system with low peak to average power ratio (PAPR). Recently, the SC-FDMA has drawn great attention as an attractive alternative to OFDMA, in the uplink communications where lower PAPR greatly benefits the mobile terminal in terms of manufacturing cost as well as transmit power efficiency. Such power efficiency can provide considerable performance improvements in future wireless communication networks. This article outlines the basic principles of SC-FDMA systems with different types of sub-carrier mapping schemes. Simulation results demonstrate the performance of different sub-carrier mapping schemes under ideal and pedestrian channel condition.
[Bit error rate, Peak to average power ratio, Mobile communication, Wireless communication, radio mobile channels, sub-carrier mapping, equalisers, uplink communications, PAPR, wireless communication networks, peak to average power ratio, OFDM modulation, single carrier modulation, wireless channels, OFDMA, Long Term Evolution, transmitter, frequency domain equalization, frequency division multiple access, receiver, Discrete Fourier transforms, orthogonal frequency division multiple access, frequency-domain analysis, radio receivers, Frequency domain analysis, Frequency division multiaccess, OFDMA systems, radio transmitters, SC-FDMA, LTE]
SQL Tunnelling through HTTP
2010 13th International Conference on Computer and Information Technology
None
2010
A novel SQL Tunnelling system to allow communication with an SQL database is developed by utilising the standard Internet protocol HTTP. The communication with the database is allowed by a client interacting with a Web Service running on a standard off-the-shelf HTTP Server. With this tunnelling system, access to a database can be provided without providing access to the database native communication port. This can be useful if a database application needs to be made available on insecure networks, such as Internet, or if firewalls between client and server does not accept anything other than the normal HTTP communications.
[client-server systems, Protocols, off-the-shelf HTTP Server, Internet protocol, Hypertext Transfer Protocol (HTTP), hypermedia, communication port, insecure network, HTTP communication, Browsers, Servers, SQL, Web Service, Proxy Server, Tunnelling, Databases, Web services, transport protocols, SQL database, Structured Query Language (SQL), Fires, Database, SQL tunnelling]
An analytical framework for identifying redundant sensor nodes from a dense sensor network
2010 13th International Conference on Computer and Information Technology
None
2010
Redundant node deployment has an impact on network lifetime because redundant nodes consume excess energy by performing unnecessary repetitious tasks. A distributed node redundancy identification method, called Self-Calculated Redundancy Check (SCRC), is proposed to eliminate redundant tasks. A grid is assumed over the field to help each node to calculate its own redundancy by checking the coverage degree of its sensing region. This optimises the active node set while providing complete network coverage and connectivity. An analytical framework is presented for SCRC using the expected value optimisation technique. The framework is used to predict potentially redundant nodes under various node distributions.
[redundant sensor node deployment, distributed node redundancy identification method, Redundancy, Sensor Networks, dense sensor network coverage, SCRC, Probability distribution, redundant task, Information technology, Equations, Optimization, expected value optimisation technique, active node set, optimisation, sensor placement, Network Optimisation, Redundant Sensor Node, self calculated redundancy check, telecommunication network reliability, Sensors, redundancy, Mathematical model, network lifetime]
Application of wireless sensor networks in forest fire detection under uncertainty
2010 13th International Conference on Computer and Information Technology
None
2010
This paper proposes a soft computing approach to manage uncertainty and rule discovery by reasoning over inconsistent, incomplete and fragmentary information using dominance-based rough set theories. A methodological and computational basis is illustrated in a sensor network application scenario of a forest fire detection system.
[dominance-based rough set theories, forest fire, Uncertainty, wireless sensor networks, geophysics computing, uncertainty handling, rule discovery, Indexes, uncertainty, soft computing, Wireless sensor networks, Accuracy, fires, forest fire detection system, Fires, Set theory, rough set theory, forestry, uncertainty management]
Distributed memory caching for the fail safe computation to improve the Grid performance
2010 13th International Conference on Computer and Information Technology
None
2010
Grid computing organizes geographically distributed resources under a single platform and let the users access this combined power. In this paper we have discussed the application of distributed memory caching system in the Grid computing environment to improve its computational environment. For our experiment, we used Alchemi, a .net based Grid computing framework and Memcached, a distributed memory caching technique. We completed couple of experiments in this environment and they demonstrated two very important outcomes. One of the outcomes outlined that distributed memory caching technique can provide fail safe computation for the Grid environment. The second result represented the reduction of the total computational time of the Grid applications. Based on the results of these current experiments and also previous experiments completed in our distributed computing laboratory we have proposed a new technique for the Grid computing environment that can provide performance improvement as well as the fail safe Grid computing environment.
[Instruction sets, Merging, Fail Safe Computation, grid computing, .NET based grid computing framework, distributed memory caching, Servers, Grid Computing, fail safe computation, File systems, network operating systems, Alchemi, distributed computing laboratory, grid performance, Distributed databases, Distributed Memory Caching, distributed memory systems, Grid computing, fault tolerant computing, geographically distributed resource, Memcached]
Dynamic communication performance enhancement in Hierarchical Torus Network by selection algorithm
2010 13th International Conference on Computer and Information Technology
None
2010
The static network performance and dynamic communication performance of the Hierarchical Torus Network (HTN) using dimension-order routing algorithm have already been evaluated and shown to be superior to the performance of other interconnection networks. However, the assessment of the dynamic communication performance improvement of HTN by the efficient use of both the physical link and virtual channels has not yet been evaluated. This paper addresses three adaptive routing algorithms - link-selection, channel-selection, and a combination of link-selection and channel-selection - for the efficient use of physical links and virtual channels of an HTN to enhance dynamic communication performance. It also proves that the proposed adaptive routing algorithms are deadlock-free with 3 virtual channels. The dynamic communication performances of an HTN is evaluated by using dimension-order routing and proposed adaptive routing algorithms under various traffic patterns. It is found that the dynamic communication performance of an HTN using these adaptive routing is better than when the dimension-order routing is used, in terms of network throughput.
[hierarchical torus network, HTN, Heuristic algorithms, network routing, multiprocessor interconnection networks, virtual channels, Switches, Routing, Throughput, microprocessor chips, interconnection networks, static network performance, dimension order routing algorithm, wormhole routing, deadlock free routing, Multiprocessor interconnection, dynamic communication performance, selection algorithm, dimension order routing, dynamic communication performance enhancement, System recovery, Clocks]
High performance Hierarchical Torus Network under adverse traffic patterns
2010 13th International Conference on Computer and Information Technology
None
2010
A Hierarchical Torus Network (HTN) is a 2D-torus network of multiple basic modules, in which the basic modules are 3D-torus networks that are hierarchically interconnected for higher level networks. The dynamic communication performance of the HTN using the dimension-order routing under common traffic patterns have been evaluated, and have been shown to be good. However, dynamic communication performance of HTN under adverse traffic patterns has not been evaluated yet. In this paper, we evaluate the dynamic communication performance of HTN using a deadlock-free dimension order routing with 3 virtual channels, and compare it with H3D-mesh, mesh, and torus networks. It is shown that even under adverse traffic patterns, the HTN yields high throughput and very low zero load latency, which provide better dynamic communication performance than H3D-mesh, mesh, and torus networks.
[hierarchical torus network, 3D-torus network, HTN, adverse traffic pattern, Heuristic algorithms, Computational modeling, network routing, multiprocessor interconnection networks, virtual channels, adverse traffic patterns, Routing, Throughput, deadlock-free routing, dimension-order communication routing, H3D-mesh network, Multiprocessor interconnection, dynamic communication performance, mesh network, zero load latency, System recovery, Tornadoes, deadlock-free order routing, 2D-torus network]
Robust synchronization technique for mobile DTV broadcasting system
2010 13th International Conference on Computer and Information Technology
None
2010
Synchronization is considered an important design issue for the implementation of digital television (DTV) receiver, specially for proper reception of multipath distorted signal at low signal to noise ratio (SNR). This challenge is more pronounced in complex multipath channel scenarios and for high mobility applications in presence of carrier frequency offset (CFO). In this paper, time domain correlation technique based on multiple PN-511 sequences is proposed to improve the synchronization performance of DTV broadcasting system that is robust against multipath impairments and CFO. Due to self-resolving capability of the multipath components, proposed synchronization algorithm provides better performance than conventional single PN-511 based algorithm in complex multipath channel. Since only the received adjacent PN-511 sequences are considered in the correlation process, the proposed technique is robust against CFO as well. In addition, the proposed technique provides higher peak-to-noise ratio (PNR), therefore this technique can be effectively used at low SNR conditions. Further, maximum likelihood (ML) estimation for multiple PN-511 based algorithm is carried out to evaluate the feasibility of the proposed algorithm.
[Correlation, maximum likelihood estimation, CFO, Multipath channels, digital television receiver, television broadcasting, robust synchronization technique, Robustness, carrier frequency offset, ML algorithm, PN-511 sequence, mobile television, mobile DTV broadcasting system, digital television, PN-511, peak-to-noise ratio, Mobile DTV, Synchronization, synchronisation, time domain correlation technique, complex multipath channel, multipath distorted signal, self-resolving capability, signal to noise ratio, Digital TV, Signal to noise ratio]
A high-speed and low-power ternary CAM design using match-line segmentation and feedback in sense amplifiers
2010 13th International Conference on Computer and Information Technology
None
2010
Ternary content-addressable memory (TCAM) has become popular in high-speed lookup intensive applications. Large dynamic power consumption of TCAMs makes power reduction techniques an active area of research. In this paper we present a new match-line (ML) sensing scheme employing ML segmentation and positive feedback in the sense amplifier of the first segment. Simulation using 130nm 1.2V CMOS logic shows 26% speed enhancement and more than 47% energy reduction compared to conventional current-race sensing scheme at the cost of insignificant area overhead. Additionally, number of control signals required for TCAM operation has been kept minimum.
[Energy consumption, content-addressable memory, feedback amplifiers, positive feedback, feedback, CMOS logic circuits, lookup intensive application, sensing scheme, CMOS analogue integrated circuits, dynamic power consumption, Sensors, power reduction technique, energy consumption, sense amplifier, content addressable memory, match line segmentation, Tin, Logic gates, Capacitance, Threshold voltage, ternary logic, Transistors, low-power electronics, high speed low power ternary CAM design, CMOS logic, ternary, logic design, content-addressable storage]
A novel method for the synthesis of odd base quantum full adder
2010 13th International Conference on Computer and Information Technology
None
2010
For various reasons in recent years the interest in building quantum computers has increased gradually. To do the calculations in quantum computer we need quantum arithmetic logic unit (ALU). The building block of quantum ALU is quantum adder. In quantum computer multivalued logic is possible. In this paper a generalized circuit has been proposed to build odd base multivalued quantum full adder. A novel approach has been taken to minimize the total no. of gates. Muthukrishnan-Stroud gates and quantum shift gates have been combined to achieve the minimal circuit.
[Computers, ALU, Proposals, Muthukrishnan Stroud gate, Quantum computing, adders, quantum arithmetic logic unit, digital arithmetic, multivalued logic, Logic functions, quantum adder, Ternary full adder, Adders, Septenary full adder, quantum computer, quantum gates, Multivalue logic, Quantum computation, multivalued logic circuits, DH-HEMTs, Logic gates, odd base quantum full adder, quantum shift gate, minimisation, Quantum full adder, Quinary full adder]
An encoding technique for design and optimization of combinational logic circuit
2010 13th International Conference on Computer and Information Technology
None
2010
A neural representation of combinational logic circuit is proposed, called `Logical Neural Network' (LNN). LNN is a feed-forward neural network (NN) where the weights of the network indicate the connections of digital circuit. The logic operations of the circuit such as AND, OR, NOR etc are performed with the neurons of LNN. A modification of Simple Genetic Algorithm (mSGA) is applied to design and optimize the LNN for a given truth table. The proposed technique is experimentally studied on four bit parity checker, two bit multiplexer, two bit full adder, full subtractor, and two bit multiplier circuits. LNN is compared with conventional `Cell Array' method. LNN outperforms the Cell Array method in terms of number of required gates.
[logic operations, Cell array, parity check codes, feed-forward neural network, Combinational circuits, Digital circuits, Optimization, truth table, genetic algorithm, bit parity checker, bit multiplexer, optimization, combinational logic circuit design, encoding technique, Design and optimization, bit multiplier circuits, combinational circuits, Neurons, Artificial neural networks, Logical Neural Network, genetic algorithms, encoding, bit full adder, Genetic algorithm, Logic gates, digital circuit connections, Arrays, Combinational logic circuit, logic design, feedforward neural nets, logical neural network]
Analyzing Carbon Nanotube interconnects in VLSI application
2010 13th International Conference on Computer and Information Technology
None
2010
Single Wall Carbon Nanotubes exhibit outstanding contributions in the recent VLSI interconnections. Interconnects analyzed in VLSI circuits depends on the electrical properties of carbon nanotubes. Metallic carbon nanotubes are very distinct for their ballistic conductivity in nano level interconnections. Different peaks are analyzed in Raman spectroscopy technique for characterizing metallic carbon nanotubes. The performance analysis of metallic carbon nanotubes is compared with the conventional Cu interconnects. In this study we analyzed resistivity and capacitance of carbon nanotubes interconnects which indicates carbon nanotubes interconnect are the most prominent solution for the future VLSI technologies.
[ballistic conductivity, nanolevel interconnections, VLSI, Interconnects, Carbon nanotubes, Conductivity, Very large scale integration, Raman spectroscopy, integrated circuit interconnections, Resistance, Cu, carbon nanotubes, Raman scattering, Capacitance, copper, VLSI interconnections, Raman, single wall carbon nanotube interconnects, Carbon Nanotube]
On the design of quaternary comparators
2010 13th International Conference on Computer and Information Technology
None
2010
Quaternary logic requires a dedicated comparator circuit besides the usual add/sub unit which may not be optimal due to several reasons. In this paper, we have thoroughly discussed various alternative expressions for equality operator which serves as the basis for quaternary comparator. Then we have derived the necessary equations for single qudit comparator and extended it to serial multi qudit comparator. We have also shown the equations and design of single stage parallel comparator where restriction of fan-in is sacrificed for constant speed. We have ended our discussion with the design of a logarithmic stage parallel comparator which can compute the comparator output within log<sub>2</sub>(n) time delay for n qudits.
[alternative expression, equality operator, serial multiqudit comparator, Serial comparator, Equality operator, Quaternary comparator, Single stage comparator, single qudit comparator, Logarithmic stage comparator, Inverters, quaternary logic, time delay, Compounds, Delay, Equations, single stage parallel comparator, Algebra, digital arithmetic, Logic gates, logic circuits, Adders, comparators (circuits), logic design, quaternary comparator design]
A new z-scanning scheme for directional spatial prediction of AVS intra coding
2010 13th International Conference on Computer and Information Technology
None
2010
In traditional directional spatial prediction based video coding standards such as H.264/AVC, MPEG4 and AVS, zigzag scan is the fixed scan pattern which designed to organize quantized transform coefficients in order to bring the high-frequency components as more as possible, so that the coefficients can be encoded more efficiently using the entropy encoding. However, zigzag scan cannot efficiently organize the transform coefficients due to different residual energy distribution produced by different intra prediction. To resolve this problem, in this paper, we propose a z-scan scheme to further improve intra coding efficiency for the AVS standard. In our method, traditional zigzag scan, horizontal z-scan, and vertical z-scan are used depending on the spatial prediction directions. It is relatively easy to implement our z-scan scheme into AVS codec without changing the syntax. Experimental results demonstrate that the z-scan scheme can remarkably reduce bitrates by approximately 2.1% compared with AVS codec using zigzag scan, while the PSNR of video sequences are maintained.
[Video coding, video coding standard, Coding Efficiency, Z-Scan, Transforms, Intra coding, audio coding, Image coding, Bit rate, AVS codec, residual energy distribution, Zigzag Scan, fixed scan pattern, directional spatial prediction, Codecs, AVS intracoding, video codecs, entropy encoding, entropy codes, Encoding, video coding, quantized transform coefficient, H.264-AVC, horizontal z-scanning scheme, zigzag scan, Automatic voltage control, vertical z-scanning scheme, high frequency component, AVS]
A parametric formulation to Detect Speech Activity of noisy speech using EDON
2010 13th International Conference on Computer and Information Technology
None
2010
The most critical and difficult problem in speech analysis is reliable discrimination among Silence, Unvoiced and Voiced speech. Several methods have been proposed for making this three levels decision and most of them need Speech Activity Detection (SAD). In this study, we propose the Estimated Degree of Noise (EDON) to adjust the threshold of speech activity. To estimate the degree of noise, a function was previously prepared using the least-squares (LS) method, from the given (true) DON and the estimated parameter of DON. This parameter is obtained from the Auto-Correlation Function (ACF) of the noisy speech on a frame basis. Issues associated with this EDON for SAD approach are discussed, and experiments are done using the TIMIT database. Experimental result shows that using EDON improves the classification performance specially voiced and silent parts and the efficiency is compared with other existing published algorithms.
[correlation theory, least mean squares methods, Correlation, voiced speech, Least square method, Estimation, estimated degree of noise, Parameter of noise, Noise measurement, Estimated Degree of Noise, speech activity detection, unvoiced speech, auto-correlation function, Accuracy, noise (working environment), Speech activity Detection, SAD, Speech, speech processing, least-squares method, silence, EDON, Signal to noise ratio, speech analysis]
Application of Canny filter and DWT in fingerprint detection a new approach
2010 13th International Conference on Computer and Information Technology
None
2010
Fingerprint detection is one of the most important applications of image processing in a security system. Recent literature deals with fingerprint detection based on pattern recognition, moment based image recognition and discrete wavelet transform (DWT). Among them DWT based analysis requires the least amount of memory space which considers only wavelet coefficient of an image in matching with preserved coefficient. This paper proposes a new approach, which combines image filtering and color inversion in enhancement of contrast along with two dimensional DWT to get the matrix of the image. Finally four statistical parameters: cross correlation coefficient, skewness, kurtosis and convolution of approximate coefficient of one dimensional DWT are compared in detection of fingerprint of a person.
[fingerprint identification, image processing, Correlation, discrete wavelet transforms, Approximation and detail function, Canny filter, cross correlation coefficient, Fingerprint recognition, fingerprint detection, discrete wavelet transform, Discrete wavelet transforms, Approximation methods, kurtosis, moment based image recognition, Convolution, normalized convolution vectors and skewness]
Imaging investigation on the spray penetration of Gasohol blends image
2010 13th International Conference on Computer and Information Technology
None
2010
Simplified visualisation of the fuel spray developing process of varied ethanol content in an open spray by the liquid injection technique. For obtaining spray characteristics in the liquid, the visualisation principles were based on spray penetrations images. The video imaging and schlieren technique have been used to investigate the spray development and behaviours at different injection time defined as the delay time from the start of injection (SOI) to the image recording in the viewing area. Five different gasoline-ethanol fuel blend rates by volume are prepared. The Image of the spray is captured using schlieren imaging technique and image processing is employed to extract macro spray characteristics- spray tip penetration and spray cone angle. Based on the extracted tip penetration and cone angles E55 gasoline-ethanol blend found out to have the largest cone angle and tip penetration among the tested blends.
[Visualization, image recording, fuel systems, gasohol blends, Edge Detection, Spray Tip Penetration, delay time, schlieren imaging technique, data visualisation, start of injection, spray penetration image, Spray Cone Angle, spray tip penetration, blending, edge detection, petroleum, gasoline-ethanol fuel blend, image processing, gasoline-ethanol blend, spray cone angle, Ethanol, Image edge detection, fuel spray, Image Segmentation, liquid injection technique, video coding, video imaging, flow visualisation, Petroleum, visualisation principle, Image segmentation, Schlieren Imaging, macrospray characteristics, sprays, organic compounds, Gasohol Blend, Pixel, schlieren systems]
Optimization of control parameter of Differential Evolution algorithm for efficient design of FIR filter
2010 13th International Conference on Computer and Information Technology
None
2010
Modern signal processing applications invoke various evolutionary algorithms in different areas such as aerodynamic shape optimization, pattern recognition, digital filter design, automated mirror design etc. Presently, Differential Evolution (DE) algorithm has proved to be quite efficient in these areas. Digital filters of various kinds can be designed using this particular evolutionary technique. In this paper, we have studied the effect of an important control parameter namely, the Weighting Factor, on the convergence speed of the DE algorithm, associated with the efficient design of low-pass FIR filter being used subsequently as a pulse-shaping filter in a QPSK modulated system. Finally, the optimized value of the Weighting Factor for this specific design problem has been suggested. Experimentally measured Eye diagrams confirm the optimized value.
[Algorithm design and analysis, FIR filter, DE, signal processing, evolutionary algorithms, Cost Function, quadrature phase shift keying, Convergence, FIR filters, QPSK, optimisation, weighting factor, optimization, Finite Impulse Response (FIR) filter, Pulse Shaping filter, Filtering algorithms, control parameter, Cost function, low-pass filters, low-pass filter, pulse-shaping filter, Eye diagram, evolutionary computation, differential evolution algorithm, digital filters, Finite impulse response filter, Band pass filters]
Performance evaluation of MLPC and MFCC for HMM based noisy speech recognition
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper auditory like features MLPC and MFCC have been used as front-end and their performance has been evaluated on Aurora-2 database for Hidden Markov Model (HMM) based noisy speech recognition. The clean data set is used for training and test set A is used to examine the performance. It has been found that almost the same recognition performance has been obtained both for MLPC and MFCC and the average word accuracy for MLPC and for MFCC is found to be 59.05% and 59.21%, respectively. It has also been observed that the MLPC is more effective than MFCC for noise type subway and exhibition, on the other hand, MFCC is more superior for babble and car noises.
[Aurora-2 database, Computational modeling, hidden Markov model, Noise, performance evaluation, HMM, Bilinear transformation, Noisy speech recognition, Mel frequency cepstral coefficient, hidden Markov models, Accuracy, speech recognition, Hidden Markov models, MLPC, Speech recognition, Speech, noisy speech recognition, MFCC]
Cognitive change in women's empowerment in rural Bangladesh
2010 13th International Conference on Computer and Information Technology
None
2010
Rural women in Bangladesh have limited access to resources and public spheres due to socio-cultural restrictions. Women suffer from severe discrimination, due partly to a lack of access to information. Information and communication and technologies (ICT) are tools that potentially can reach rural women and address their knowledge and information needs. Considering this scenario, the aim of this paper is to examine the situation of rural women using ICT tools provided by non-government and government organizations, and investigate whether access to ICT has changed their lives in terms of socio-economic development. Using a structured questionnaire, data was collected from women in villages where two different ICT projects have been introduced. The change in women's awareness, skills and knowledge of the wider environment on various issues (including health, education, legal rights) is described. These cognitive changes were compared in women with ICT intervention and women who did not use ICT. The overall cognitive awareness of the women indicates more changes among women with ICT intervention than without. Therefore, ICT intervention in rural villages in Bangladesh is leading to empowerment.
[Computers, Economics, Cognitive change, Pediatrics, government organizations, cognition, cognitive change, rural women, Educational institutions, cultural aspects, ICT intervention, Training, empowerment, non government organizations, Space technology, public administration, rural Bangladesh, socio cultural restrictions, women empowerment, gender issues, Information and communication and technologies]
Employee characteristics and their value perceptions about web-based B2E systems use in Bangladesh
2010 13th International Conference on Computer and Information Technology
None
2010
Business-to-Employees (B2E) systems represent a new breed of web-enabled eBusiness application that helps develop productive and committed workforce. However, even though eBusiness technology vendors advocate that enormous values organisations and their employees can gain from the introduction of B2E systems, no systematic empirical study is yet reported from employees' viewpoint in particular for the developing countries' context. Furthermore, as value perceptions are not formed in isolation, there is a need to evaluate how employees' value perceptions about B2E systems are influenced by their demographic and attitudinal characteristics. We thus report the value perceptions of 109 employees from 2 telecommunication companies located in Bangladesh, and highlight that except attitudes, popular demographic characteristics of employees are not related to their value perceptions. The implications of these findings are discussed.
[Context, social aspects of automation, Web based B2E systems, demographic characteristics, employee characteristics, telecommunication industry, business-to-employees systems, Companies, Information technology, B2E systems, Bangladesh, Education, personnel, Internet, eBusiness, value perceptions, employees, value, business data processing]
How experience affects technology acceptance: A quest for ICT development strategies in Bangladesh
2010 13th International Conference on Computer and Information Technology
None
2010
Despite an increasing amount of initiatives and policy priorities remain in Bangladesh for ICT development and establishing an e-based society by 2021, lower level of internet penetration made the whole initiatives inconclusive although there are enormous potentials. This study attempts to examine individuals' intention and actual internet usage behaviour applying an extended version of technology acceptance model (TAM). A descriptive research design was administered in explaining the joint impact of the study constructs. Structural equation modeling approach was used with the data collected from 291 individuals in Bangladesh through a questionnaire survey. The proposed model was first measured through factor loadings, composite reliability and the constructs correlation for convergent and discriminant validity. The structural model estimation results show that experience has direct significant relation with perceive ease of use, intention and actual behaviour and indirect relation with perceived usefulness through perceived ease of use. On the other hand, perceive usefulness has direct effects on intention while perceived ease of use doesn't but indirectly related through perceived usefulness. The path analysis furthered the significant effects of intention on actual internet usage behaviour in Bangladesh. The study concludes with implications.
[ICT development strategy, composite reliability, correlation construction, e-based society, Government, Internet usage behaviour, user interfaces, perceived ease of use, structural equation modeling approach, experience, perceived usefulness, Analytical models, intention and actual behaviour, Bangladesh, information and communication technology, Internet penetration, perceive intention, perceive ease-of-use, Internet, Mathematical model, Reliability, technology acceptance model, Load modeling, factor loadings]
Sensor Proxy Mobile IPv6 (SPMIPv6) - A framework of mobility supported IP-WSN
2010 13th International Conference on Computer and Information Technology
None
2010
IP based Wireless Sensor Networks (IP-WSN) are gaining importance for its broad range of applications in health-care, home automation, environmental monitoring, security &amp; safety and industrial automation. In all of these applications mobility in sensor network with special attention to energy efficiency is a major issue to be addressed. Host based mobility management protocol is inherently unsuitable for energy inefficient IPWSN. So network-based mobility management protocol can be an alternative to the mobility supported IP-WSN. In this paper we propose a mobility supported IP-WSN protocol based on PMIPv6 called Sensor Proxy Mobile IPv6 (SPMIPv6). We present its architecture, message formats and also analyze its performance considering signaling cost and mobility cost. Our analyses show that the proposed scheme reduces the signaling cost by 67% and 60% as well as reduces mobility cost by 55% and 60% with comparison to MIPv6 and PMIPv6 respectively.
[Protocols, mobile radio, environmental monitoring, IP-WSN, wireless sensor networks, IP based wireless sensor networks, network-based mobility management protocol, PMIPv6, wireless sensor network, mobility management (mobile radio), IETF, Mobile radio mobility management, sensor proxy mobile IPv6, home automation, Wireless sensor networks, mobility supported IP-WSN protocol, Hospitals, energy efficiency, industrial automation, IEEE 802.15.4., 6LoWPAN, IP networks, Mobile computing, Manganese]
Towards mobile based e-learning in Bangladesh: A framework
2010 13th International Conference on Computer and Information Technology
None
2010
Considering the rapid expansion of the usage of mobile communication devices in Bangladesh, development of technology and reduction in cost, well designed mobile based e-learning framework is expected to contribute significantly in educational development and thereby having a long term effect on poverty alleviation. This paper shows an M-Learning framework for Bangladesh.
[cost reduction, Ground penetrating radar, mobile communication devices, mobile based e-learning, Wireless application protocol, m-learning in Bangladesh, Mobile communication, educational development, poverty alleviation, Mobile handsets, Mobile based e-learning, Electronic learning, mobile computing, Bangladesh, mobile communication, e-learning, Asia, M-learning framework, computer aided instruction, mobile handsets, m-learning]
Comparative performance analysis of MPEG4, FLV and 3GP multimedia file formats using wireless network parameters
2010 13th International Conference on Computer and Information Technology
None
2010
Many recommendations are proposed for accommodating multimedia content into wireless media with lower bandwidth. These formats of the content have been developed with different objectives. Now it has become really challenging to compare all these diversified formats. In this paper, we proposed an algorithm to compare the FLV, MPEG4 and 3GP format of multimedia content using the parameters that determines their performance in wireless network. Finally, exhaustive experiments are done and the results are shown to demonstrate the merits and capabilities of the different file formats.
[radio networks, PSNR, 3G mobile communication, performance evaluation, Multimedia communication, MPEG4, MPEG 4 Standard, FLV, CoQV, Image coding, Transform coding, SNR, Streaming media, multimedia file formats, 3GP, wireless network, video signal processing, multimedia communication, performance analysis]
Data exchange: Query answering for positive query with at most one inequality
2010 13th International Conference on Computer and Information Technology
None
2010
In a data exchange system the correctness of the system depends on data exchange semantics. This can be well verified by using query answering. In the close world assumption (CWA) two types of answers are obtained, one is certain answer, which ignores the null values, another is maybe answer, which considers the null values. In this thesis, we give a procedure for computing a finite representation of the maybe answer. Here we use positive conjunctive queries with at most one inequality. Finally, the queries are tested on some practical data exchange sceneries and the results are compiled.
[Data exchange, most one inequality, Relational databases, data exchange system, close world assumption, Open wireless architecture, positive conjunctive query, Query Answering, Cost accounting, Engines, query processing, query answering, electronic data interchange, Semantics, Positive Query, Polynomials, Inequality]
Developing an efficient search suggestion generator, ignoring spelling error for high speed data retrieval using Double Metaphone Algorithm
2010 13th International Conference on Computer and Information Technology
None
2010
Finding desire information from a large text database is one of the most important issues of modern information processing systems. In this regard different types of searching techniques are used. Though some of them are vary useful, they frequently fail to show appropriate performance when a user enters misspelled data as searching keyword. In this paper we have developed an efficient search suggestion generator using Phonetic algorithm namely, Double Metaphone Algorithm. Here we use a technique to reduce total searching comparisons by creating an index on a specific field, we have defined it as keyCode field, in a table of our database where all of the values of Code field are produced by that algorithm acted on records. Results show that generator not only quickly finds the required information but provides possible search suggestion avoiding the misspelled words entered as search key.
[Phonetic algorithm, high-speed data retrieval, information retrieval, phonetic algorithm, Data structures, Information retrieval, text database, Search Comparisons, Generators, Encoding, database indexing, Double Metaphone Algorithm, Books, double metaphone algorithm, information processing systems, search suggestion generator, Indexing, keycode field]
Facial expression recognition based on a weighted Local Binary Pattern
2010 13th International Conference on Computer and Information Technology
None
2010
We introduce a facial expression recognition method, which incorporates a weight to the Local Binary Pattern (LBP), and generates solid expression features. Furthermore, we use Adaboost to select a small set of prominent features, which is used by the Support Vector Machine (SVM) to classify facial expressions efficiently. Experimental results demonstrate that our method outperforms the state-of-the-art methods in terms of both accuracy and complexities.
[support vector machines, Face recognition, SVM, Support vector machine, emotion recognition, Local Binary Pattern, Histograms, weighted local binary pattern, Accuracy, Databases, support vector machine, Support vector machine classification, Expression recognition, face recognition, learning (artificial intelligence), Kernel, facial expression recognition, Pixel, Adaboost, Prominent features]
Kinetisation of view of 3D point set
2010 13th International Conference on Computer and Information Technology
None
2010
Given a set of n points in the plane, the problem of computing the circular ordering of the points about a viewpoint v and efficiently maintaining this ordering information as v moves is well defined in computer graphics and animation. Each of the unique circular ordering in respect to v is called as view. In this paper, our task is to generalize this idea for 3D point set and to propose a kinetic data structure named Kinetic Neighborhood Graph to maintain the view dynamically with efficiency O(m&#x03BB;<sub>s</sub>(n2)), locality O(1) and responsiveness O(m).
[graph theory, Dynamic Maintenance, Maintenance engineering, computational geometry, Data structures, Complexity theory, Computational Geometry, Kinetic Neighborhood Graph, animation, computer animation, Computer Graphics, computer graphics, Memory management, 3D view, Kinetic Data Structure, Davenport-Schinzel Sequence, Animation, 3D point set, Three dimensional displays, data structures, kinetic neighborhood graph, Kinetic theory, kinetic data structure]
Session management protocol for virtual classroom in teleteaching
2010 13th International Conference on Computer and Information Technology
None
2010
Classroom teaching has always been a face to face interaction between students and teachers and here in the proposed system we would like to preserve this aspect of teaching with help of information and communication technology. We like to bring this quality along with the concept of geographical independence in a distributed system. Teleteaching as in any collaborative system is difficult to model. Before any implementation we need to model the system. Here we have used ontology to create a level of abstraction which broadly defines a model and the dependencies of activities, thus, making the system easy to interact and modify.
[Protocols, student-teacher interaction, Heuristic algorithms, Ontology, Ontologies, distributed processing, distributed system, teaching, teleteaching, geographical independence, Learning systems, collaborative system, Education, classroom teaching, information and communication technology, groupware, virtual classroom, Face, Protocol, Teleteaching, Synchronization, face to face interaction, Session Management, distance learning, ontologies (artificial intelligence), computer aided instruction, session management protocol]
A composition technique of multiple switching functions based on BDD
2010 13th International Conference on Computer and Information Technology
None
2010
Binary Decision Diagram has a great impact on the Boolean function manipulation for its compressed and canonical presentation. In switching function organization ROBDD (Reduced Ordered Binary Decision Diagram) with a fixed variable ordering plays a significant role for its distinctiveness. Combination of multiple functions using switching operations has more redundant states. For removing these redundant states we apply ROBDD in this joining process. In this paper, we presented the way of combining n number of functions using ROBDD with a fixed variable ordering. In our proposed method at first, we compute each function's ROBDD over Shannon's expression. Then switching operations are going to be performed over these functions and ROBDD of the combinations of these functions are also computed. By this method, appearance of redundant states will be less. To construct an understandable and simple method, some examples have been used.
[decision theory, Reduced Ordered Binary Decision Diagram, Switches, multiple switching function, Data structures, Minimization, Boolean function manipulation, Indexes, binary decision diagram, Information technology, ROBDD, Boolean functions, Shannon expression, Switching operation, Space technology, composition technique, BDD Switching function, canonical presentation, Composition of switching functions]
A multivalued storage system using memristor
2010 13th International Conference on Computer and Information Technology
None
2010
For many years the only known passive circuit elements were resistor, inductor and capacitor. In 1971, Leon Chua showed using argument that there should exist another passive element which he named memristor. It is basically a resistor whose resistance increases when current flows through it from one direction and decreases when current flows from the other direction. After 37 years researchers of HP became able to build world's first working memristor and surprised the electronics community. Chua proved that the characteristics of a memristor cannot be simulated using resistor, capacitor and inductor only and hence it is a fundamental element. Many new types of circuits can be built using memristor in it. But as it is a very new element so the no. of circuits is only a few. As memristor can have many resistance levels, we can use some discrete value as different logic level and thus can use in multivalued logic system. In this paper a method has been shown to achieve those different levels. Using this method we can use an array of memristor for making a non-volatile multivalued data storage system..
[memristors, non-volatile multivalued data storage system., Data storage system, resistance levels, Multivalued logic, Memristor, Equations, Resistance, Low energy, Linear drift model, multivalued logic, Memristors, memristor, Mathematical model, Arrays, low-power electronics]
A safe system with safe logistics support for Chittagong Port
2010 13th International Conference on Computer and Information Technology
None
2010
This paper presents a prototype implementation of an intelligent system called safe system that monitors container security in the port logistics. Logistics supports in the port require safe container status. The main focus of our contribution is to design a system that supports RFID (Radio Frequency Identification) Readers of all types (such as Reader Protocol complaint and vendor provided Readers), real time monitoring of sensing information and notify management about the status of the container in harbor logistics.
[Protocols, radiofrequency identification, prototypes, harbor logistics, Containers, logistics, Reader, radio frequency identification, containerisation, real time monitoring, Temperature sensors, Container, safety systems, safe container status, Protocol, sensing information, Monitoring, Chittagong port, container security, RFID, safe logistics support, safe system, security of data, prototype implementation, intelligent system, Logistics, Radiofrequency identification]
A study of privacy policy enforcement in access control models
2010 13th International Conference on Computer and Information Technology
None
2010
Internet has gained huge popularity over the last decade. It offers its users reliable, efficient and exciting online services. However, the users reveal a lot of their personal information by using these services. Websites that collect information state their practices with data in their privacy policies. However, it is difficult to ensure if the policies are enforced properly in their practices. This can lead to unintentional leakage of private information to unauthorized parties and thus increase the chance of private data to be misused. Taking the help of legal systems is expensive, time consuming and cannot compensate the loss completely. Therefore, an effective way to protect privacy is to use privacy policy to control data access. In this paper, we review some distinguished research works that address this problem. We also discuss the completeness of the privacy definition used in these works.
[Access control, Context, Data privacy, privacy policy enforcement, unintentional private information leakage, Privacy, database, policy language, security, Databases, data access control models, legal systems, Organizations, authorisation, Data models, data privacy, access control, Internet, Web sites, online services]
Automated essay scoring using Generalized Latent Semantic Analysis
2010 13th International Conference on Computer and Information Technology
None
2010
Automated Essay Grading (AEG) is a very important research area in educational technology. Latent Semantic Analysis (LSA) is an information retrieval technique used for automated essay grading. LSA forms a word by document matrix and then the matrix is decomposed using Singular Value Decomposition (SVD) technique. Existing AEG systems based on LSA cannot achieve higher level of performance to be a replica of human grader. We have developed an AEG system using Generalized Latent Semantic Analysis (GLSA) which makes n-gram by document matrix instead of word by document matrix. We have evaluated this system using details representation and showed the performance of the system. Experimental results show that our system outperforms the existing system.
[document handling, Correlation, educational computing, Automatic Essay Grading, Humans, information retrieval, automated essay grading, n-gram, matrix decomposition, Matrix decomposition, latent semantic analysis, educational technology, Training, Singular Value Decomposition, N-gram, Semantics, Latent Semantic Analysis, document matrix, singular value decomposition, Singular value decomposition]
Issues in M-banking: Challenges and opportunities
2010 13th International Conference on Computer and Information Technology
None
2010
The increased prevalence of mobile phones provides exciting opportunities for the growth of mobile banking (m-banking). This article reviews the emerging research literature on m-banking. It presents a classification framework for m-banking research based on 65 m-banking articles published between 2000 and mid-2010 in Information Systems (IS), technology innovation, management, and marketing journals, and major IS conferences. These articles are classified into five main categories: m-banking overview and conceptual issues, m-banking applications and cases, m-banking behavioural issues, infrastructures of mobile users and networks, and strategic, legal and ethical issues. It is expected that the comprehensive list of references and assessments presented in this article will provide a useful anatomy of young m-banking literature to anyone who is interested in m-banking and help stimulate further interest.
[pattern classification, strategic issue, Law, Conferences, m-banking behavioural issues, mobile phones, Banking, m-banking, m-banking applications, Literature review, Mobile communication, m-banking overview, ethical issue, Information systems, banking, mobile user infrastructures, mobile computing, Future research, legal issue, m-banking conceptual issues, Mobile banking (m-banking), Business, mobile banking, classification framework]
Subscribers fluctuations of different cell operators based on different offers
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper the authors have shown the ebb and flow of subscribers of different cell operators in Bangladesh based on their call rate, network, and other external facilities and suggests a new approach for the constituency of these operators to find out how all the operators can ensure customer satisfaction. Performance of a cellular system mainly depends on circuit merits (CM) of voice quality, service quality, special feature such as call waiting, voice stored box, automatic roaming and other opportunities. In the recent year cellular communication has been explored tremendously in Bangladesh. Existing six mobile operators plays a vital role in our social and economical life. So every operator should be careful about their call rate, network &amp; other external facilities.
[GSM, Cell operators, circuit merits, Urban areas, Companies, Mobile communication, Service, subscribers fluctuations, automatic roaming, Telecommunications, Multiaccess communication, cellular communication, call waiting, Bangladesh, customer satisfaction, Offer, Network, Cities and towns, cell operators, voice stored box, cellular radio]
TCO model for server operating system
2010 13th International Conference on Computer and Information Technology
None
2010
Over the last decade many but totally different Total Cost of Ownership reports were generated for a same system. It was even more puzzling since most of the times these reports were sponsored by the vendors themselves. There is more or less a war between Windows and Linux about which one has the best TCO. It is highly likely that the generated TCO is based on vendors' feelings for their operating system rather than the exact calculated costs. In the research a novel mathematical model as well as a tool is developed which can be used as standard for creating reliable TCO reports for server operating systems.
[Server Operating System, Companies, costing, Time measurement, total ownership cost, Windows, Servers, Printers, Time Usage, Linux, Operating systems, file servers, operating systems (computers), Total Cost of Ownership, Mathematical Model, server operating system, Measuring Points]
Efficient computation for k-dominant skyline queries with domination power index
2010 13th International Conference on Computer and Information Technology
None
2010
Skyline queries have recently attracted a lot of attention for its intuitive query formulation. It can act as a filter to discard sub-optimal objects. However, a major drawback of skyline is that, in datasets with many dimensions, the number of skyline objects becomes large and no longer offer any interesting insights. To solve the problem, recently k-dominant skyline queries have been introduced, which can reduce the number of skyline objects by relaxing the definition of the dominance. This paper addresses the problem of k-dominant skyline objects for high dimensional dataset. We propose algorithms for k-dominant skyline computation. Through extensive experiments with real and synthetic datasets, we show that our algorithms can efficiently compute k-dominant skyline queries.
[Algorithm design and analysis, Skyline, Scalability, k-dominant skyline, Dataset, Search problems, Fuels, Indexes, Nearest neighbor searches, query processing, Query processing, k-dominant skyline queries, domination power index, query formulation, k-dominant skyline computation, Domination power]
Mining classification rules via an apriori approach
2010 13th International Conference on Computer and Information Technology
None
2010
Classification rules are the interest of most data miners to summarize the discrimination ability of classes present in data. A classification rule is an assertion, which discriminates the concepts of one class from other classes. The most classification rules mining algorithm aims to providing a single solution where multiple solutions exist. Moreover, it does not guarantee the optimal solution and user has not any control over the classification error rate. In this paper, we addressed these problems inherent in mostly used classification algorithms. A solution has been proposed to solve these problems and it has been tested with experimental data.
[pattern classification, Characteristic Rules, data mining, Artificial neural networks, Birds, Data Mining, Association Rules, data miners, Classification algorithms, Classification Algorithm, Apriori, Data mining, classification rules mining algorithms, Classification Rules, apriori approach, Classification tree analysis]
Spatial data mining on literacy rates and educational establishments in Bangladesh
2010 13th International Conference on Computer and Information Technology
None
2010
Data mining is the process of extracting non-trivial patterns from large volume of data. It generates insight and turns the data into valuable information. A critical yet common flaw when performing data mining is to ignore the geographic locations from where the data is taken. When this geospatial attribute of the data is taken into consideration, the process is known to be geospatial data mining. This task essentially deals with the detection of spatial patterns in the data, the formulation of hypotheses and the assessment of descriptive or predictive spatial models. Spatial data mining could provide interesting and useful information to government, environmentalists and relevant decision makers' in the assessment of the relative performance of a particular geographic area. The results could also be used for causal analysis by domain experts. In our research we perform spatial data mining using literacy rates and the number of educational establishments. The data is from the 64 well defined administrative units of Bangladesh known as Zilas. This paper contains a summary of the theory, methodology and detailed analysis of results. We compare the results found by spatial model with classical regression model. The results demonstrate that spatial lag model outperforms the classical model in different perspectives.
[Spatial Regression, Correlation, Data analysis, Geographic Information Systems, Zilas administrative units, Biological system modeling, spatial lag model, data mining, regression analysis, visual databases, Data Mining, Spatial Autocorrelation, Spatial databases, educational administrative data processing, Geospatial analysis, Data mining, literacy rate mining, geospatial data mining, regression model, Bangladesh, Exploratory Spatial Data Analysis, Data models, spatial data mining, educational establishment mining]
Sustainable approach to segmented digital display: Complete, precise and economic
2010 13th International Conference on Computer and Information Technology
None
2010
Different approaches have been proposed for digital display of bangla numerals and also characters. But a complete and accurate scheme not has been done yet. We approach a complete low cost and fair design to display bangla numerals, bangla characters (vowel, consonant and shortened form of these characters), also English and Arabic numerals and all English character's represented as 120 character displayed by only 34 segments with tiny bits of binary code and differently usable architecture made this approach favorable. This scheme can be effectively used instead of other alternatives as less complex, economic and complete. With this framework, other characters also can be displayed as manufacturer specification.
[Economics, binary codes, bangla numerals, Shape, natural language processing, bangla characters, 5 variable representation, SOP/DNF expression, Arabic numerals, Minimization, Complexity theory, display instrumentation, mux, Accuracy, English numerals, 6 variable representation, DH-HEMTs, Computer architecture, sustainable approach, English character representation, binary code, combination vector, segmented digital display, 34-segment display]
Towards a tableau based high performance automated theorem prover
2010 13th International Conference on Computer and Information Technology
None
2010
Automated Theorem Proving systems are enormously powerful computer programs capable of solving immensely difficult problems. The extreme capabilities of these systems lie on some well-established proof systems. Semantic tableau is such a proof system used to prove the validity of a formula by contradiction and can produce a counterexample if it fails. It can also be used to prove whether a formula is a logical consequence of a set of formulas. Tableau can be used in propositional logic, predicate logic, modal logic, temporal logic, and in other non-classical logics. In this paper, we describe the implementation of a sequential tableau algorithm for propositional logic using a procedural programming language rather then logic programming language. We also propose a tableau based proof system in a distributed environment using the message passing interface. Successful implementation of the proposed high performance approach will un-wrap an efficient paradigm for automated theorem proving.
[high performance automated theorem prover, proof systems, message passing interface, Computational modeling, semantic tableau, temporal logic, Data structures, Calculus, Cognition, prepositional logic, predicate logic, modal logic, Automated theorem proving, logic programming languages, formal logic, Program processors, Message passing, Semantics, procedural programming, logic programming language, sequential tableau algorithm, high performance computing, theorem proving]
Chaotic dynamics of supervised neural network
2010 13th International Conference on Computer and Information Technology
None
2010
It is important to study the neural network (NN) when it falls into chaos, because brain dynamics involve chaos. In this paper, the several chaotic behaviors of supervised neural networks using Hurst Exponent (H), fractal dimension (FD) and bifurcation diagram are studied. The update rule for NN trained with back-propagation (BP) algorithm absorbs the function of the form x(1-x) which is responsible for exhibiting chaos in the output of the NN at increased learning rate. The H is computed with the time series obtained from the output of NN. One can comment on the classification of the network from the values of Hs. The chaotic dynamics for two bit parity, cancer, and diabetes problems are examined. The result is validated with the help of bifurcation diagram. It is found that the values of H are repositioned marginally depending on the size of NN. The effect of the size of NN on chaos is also investigated.
[Chaos, two-bit parity problem, Back-propagation, Fractals, diabetes problem, Training, chaotic dynamics, bifurcation diagram, multilayer perceptrons, pattern classification, supervised neural network, backpropagation algorithm, Time series analysis, Artificial neural networks, Bifurcation, fractal dimension, time series, Hurst exponent, cancer problem, Neural network, neural network classification, Bifurcation diagram, Fractal dimension, backpropagation, Cancer]
Dynamic modeling and fuzzy logic control of a two-link flexible manipulator using genetic optimization techniques
2010 13th International Conference on Computer and Information Technology
None
2010
Flexible manipulator systems exhibit many advantages over their traditional (rigid) counterparts. However, they have not been favored in production industries due to its obvious disadvantages in controlling the manipulator. This paper presents theoretical investigation into the dynamic modeling and characterization of a constrained two-link flexible manipulator, by using finite element method. The final derived model of the system is simulated to investigate the behavior of the system. A Genetic Algorithm (GA) based hybrid fuzzy logic control strategy is also developed to reduce the end-point vibration of a flexible manipulator without sacrificing its speed of response. An uncoupled fuzzy logic controller approach is employed with individual controllers at the shoulder and the elbow link utilizing hub-angle error and hub-velocity feedback. GA has been used to extract and optimize the rule base of the fuzzy logic controller. The fitness function of GA optimization process is formed by taking weighted sum of multiple objectives to trade off between system overshoot and rise time. Moreover, scaling factors of the fuzzy controller are tuned with GA to improve the performance of the controller. A significant amount of vibration reduction has been achieved with satisfactory level of overshoot, rise time and settling time and steady state error.
[hub angle error, Gallium, couplings, genetic optimization technique, fitness function, Finite element methods, Vibrations, fuzzy control, elbow link, finite element method, hub velocity feedback, knowledge based systems, end- point vibration, Dynamic Modeling, Mathematical model, constrained two-link flexible manipulator, Fuzzy Logic Control, vibration reduction, Flexible Manipulator, uncoupled fuzzy logic controller, control system synthesis, genetic algorithms, finite element analysis, weighted sum, Fuzzy logic, flexible manipulators, hybrid fuzzy logic control strategy, dynamic modeling, rule base, Genetic Algorithms, Manipulator dynamics, vibration control]
Maximization of the gradient function for efficient neural network training
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper, a faster supervised algorithm (BPfast) for the neural network training is proposed that maximizes the derivative of sigmoid activation function during back-propagation (BP) training. BP adjusts the weights of neural network with minimizing an error function. Due to the presence of derivative information in the weight update rule, BP goes to `premature saturation' that slows down the training convergence. In the saturation region, the derivative information tends to zero. To overcome the problem, BPfast maximizes the derivative of activation function together with minimizing the error function. BPfast is tested on five real world benchmark problems such as breast cancer, diabetes, heart disease, Australian credit card, and horse. BPfast exhibits faster convergence and good generalization ability over standard BP algorithm.
[Heart, faster supervised algorithm, gradient function maximization, Convergence, Training, optimisation, Australian credit card, heart disease, learning (artificial intelligence), gradient methods, Testing, Gradient information, Maximization, backpropagation training, Artificial neural networks, breast cancer, Neural network, sigmoid activation function, Generalization ability, horse, backpropagation, Horses, Diabetes, neural network training, diabetes, neural nets]
Multi-layer neural network classification of tongue movement ear pressure signal for human machine interface
2010 13th International Conference on Computer and Information Technology
None
2010
Tongue movement ear pressure (TMEP) signals have been used to generate controlling commands in assistive human machine interfaces aimed at people with disabilities. The objective of this study is to classify the controlled movement related signals of an intended action from internally occurring physiological signals which can interfere with the inter-movement classification. TMEP signals were collected, corresponding to six types of controlled movements and activity relating to the potentially interfering environment including when a subject spoke, coughed or drank. The signal processing algorithm involved TMEP signal detection, segmentation, feature extraction and selection, and classification. The features of the segmented TMEP signals were extracted using the wavelet packet transform (WPT). A multi-layer neural network was then designed and tested based on statistical properties of the WPT coefficients. The average classification performance for discriminating interference and controlled movement related TMEP signal achieved 97.05%. The classification of TMEP signals based on the WPT is robust and the interferences to the controlling commands of TMEP signals in assistive human machine interface can be significantly reduced using the multi-layer neural network when considered in this challenging environment.
[handicapped aids, Tongue, multilayer neural network classification, tongue movement ear pressure, wavelet transforms, physiological signals, wavelet packet transform, TMEP signal, signal processing algorithm, Classification algorithms, disabled people, Tongue movement ear pressure signals, feature extraction, ear, Gold, assistive human machine interface, signal detection, Interference, physiology, signal classification, signal segmentation, Feature extraction, Wavelet packets, human computer interaction, multi-layer neural network, neural nets]
On Bangla character recognition
2010 13th International Conference on Computer and Information Technology
None
2010
A recent surge of interest is to recognize Bangla characters. Bangla characters represent complex, multidimensional and meaningful visual information and developing a computational model for Bangla character recognition is a challenging job. This research presents a hybrid neural network solution for Bangla character recognition which combines local image sampling and artificial neural network. The method is based on BAM for dimensional reduction and multi-layer perception with backpropagation algorithm has been used for training the network. It has been found from practical observations that the number of iterations required to train the network is enormous. The capability of recognition of a neural network increases with increasing the training accuracy. For this process each character is converted to a designated M&#x00D7;N feature matrix. These feature matrices of characters are then fed into the neural network as input patterns .The neural network is trained with the set of input patterns of the digits to acquire separate knowledge corresponding to each Bangla character. In order to justify the effectiveness of the system, different test patterns of the characters are used to verify the system. Experimental results demonstrate that the system is capable of recognizing Bangla characters with 98% accuracy.
[iterative methods, artificial neural network, multilayer perception, hybrid neural network solution, Matra Detection, Training, Histograms, image sampling, dimensional reduction, feature matrix, visual information, Bangla Character Recognition, visual perception, natural language processing, backpropagation algorithm, Neurons, Artificial neural networks, character recognition, Character recognition, Bangla Text Segmentation, BAM, matrix algebra, Bangla character recognition, Image segmentation, Neural Network, backpropagation, Pixel, image recognition, neural nets]
Reconstruction of Gene Regulatory Networks using Differential Evolution
2010 13th International Conference on Computer and Information Technology
None
2010
Gene Regulatory Network (GRN) is an abstract mapping of gene regulations in living cells that can help to predict the system behavior of living organisms. In this research, we use a model based inference method to reconstruct GRN from gene expression data. We use linear time variant model which is of particular interest among all other models because of its capability of discovering the non-linear interactions among genes in a reasonably short time even while dealing with noisy time-series data. Here, Differential Evolution (DE), a versatile, robust and well-known Evolutionary Algorithm (EA) has been used. The potency of the proposed method has been verified in gene network reconstruction experiments, varying the network dimension and characteristics, the amount of gene expression data used for inference, and the noise level present in gene expression profiles. Real expression dataset of SOS DNA repair system in Escherichia coli is used to reconstruct the regulatory network. All these experiments have proved the efficacy of the proposed reconstruction method.
[living systems, noisy time-series data, linear time variant model, inference, Noise, nonlinear interactions, Reverse engineering, Escherichia coli, gene regulatory network reconstruction, living organisms, living cells, abstract mapping, Differential evolution, Mathematical model, SOS DNA repair system, gene expression data, time series, Gene expression, Noise measurement, inference mechanisms, genetic engineering, evolutionary algorithm, differential evolution, evolutionary computation, Global optimization, microorganisms, DNA, bioinformatics, Data models, Linear time variant model]
An Inhibition/Enhancement network for noise robust ASR
2010 13th International Conference on Computer and Information Technology
None
2010
This paper describes an evaluation of Inhibition/Enhancement (In/En) network for noise robust automatic speech recognition (ASR). In articulatory feature based speech recognition using neural network, the In/En network is needed to discriminate whether the articulatory features (AFs) dynamic patterns of trajectories are convex or concave. The network is used to achieve categorical AFs movement by enhancing AFs peak patterns (convex patterns) and inhibiting AFs dip patterns (concave patterns). We have analyzed the effectiveness of the In/En algorithm by incorporating it into a system which consists of three stages: a) Multilayer Neural Networks (MLNs), b) an In/En Network and c) the Gram-Schmidt (GS) algorithm for orthogonalization. From the experiments using Japanese Newspaper Article Sentences (JNAS) database in clean and noisy acoustic environments, it is observed that the In/En network plays a significant role on the improvement of phoneme recognition performance. Moreover, the In/En network reduces the number of mixture components needed in Hidden Markov Models (HMMs).
[Computers, multilayer perceptrons, Local Features, Conferences, inhibition network, Gram-Schmidt orthogonalization algorithm, Inhibition/Enhancement Network, Information technology, Japanese newspaper article sentences, hidden Markov models, AF dip patterns, speech recognition, multilayer neural networks, enhancement network, Hidden Markov Model, Multilayer Neural Network, articulatory feature based speech recognition, phoneme recognition performance, Distinctive Phonetic Features, noise robust automatic speech recognition, AF peak patterns, Articulatory Features]
Character recognition using wavelet compression
2010 13th International Conference on Computer and Information Technology
None
2010
The objective of this project is to build a character recognition system, which is able to recognize printed and handwritten character from A to Z. the typical optical character recognition systems, regardless the character's nature, are based mainly on three stages, preprocessing, features extraction, and discrimination. Each stage has its own problems and effects on the system efficiency which is the time consuming and the recognition errors. In order to avoid these difficulties this project presents new construction of character recognition tool using the technique similar to that is used in image compression such as wavelet compression or JPEG compression. Wavelet compression is chosen as the technique implemented for this project. Wavelet compression technique extracted the important coefficient from the images. The Euclidean distance between the coefficient of the test images and training images is computed. Character is considered recognized if the Euclidean distance calculated is smaller than the Global threshold value of 258. This character recognition system also has 18.81% of false rejection rate and 21.88% for false acceptance rate.
[Computers, data compression, Compression, handwritten character, Character, image compression, JPEG compression, JPEG, character recognition, Character recognition, Wavelet, Handwriting recognition, Image coding, Databases, printed character, feature extraction, Transform coding, Euclidean distance, features extraction, wavelet compression, Extraction, image coding, character recognition system]
Leaf shape identification based plant biometrics
2010 13th International Conference on Computer and Information Technology
None
2010
This paper presents a simple and computationally efficient method for plant species recognition using leaf image. This method works only for the plants with broad flat leaves which are more or less two dimensional in nature. The method consists of five major parts. First, images of leaf are acquired with digital camera or scanners. Then the user selects the base point of the leaf and a few reference points on the leaf blades. Based on these points the leaf shape is extracted from the background and a binary image is produced. After that the leaf is aligned horizontally with its base point on the left of the image. Then several morphological features, such as eccentricity, area, perimeter, major axis, minor axis, equivalent diameter, convex area and extent, are extracted. A unique set of features are extracted from the leaves by slicing across the major axis and parallel to the minor axis. Then the feature pointes are normalized by taking the ratio of the slice lengths and leaf lengths (major axis). These features are used as inputs to the probabilistic neural network. The network was trained with 1200 simple leaves from 30 different plant species. The proposed method has been tested using ten-fold cross-validation technique and the system shows 91.41% average recognition accuracy.
[Shape, plant biometric, Plant recognition, binary image, image scanners, Training, cameras, Accuracy, ten fold cross validation technique, feature extraction, scanner, mathematical morphology, Leaf identification, shape recognition, Probabilistic neural network, morphological feature, probabilistic neural network, leaf shape identification, Artificial neural networks, Probabilistic logic, plants with broad flat leaf, Plant biometrics, plant species recognition, digital camera, Feature extraction, Pixel, image recognition, neural nets]
Multi-objective optimisation for multi-drug chemotherapy scheduling
2010 13th International Conference on Computer and Information Technology
None
2010
This paper presents a multi-drug chemotherapy scheduling method for cancer treatment using multi-objective optimisation technique. Cancer cells, very often, grows resistance to a drug if it is administered alone for a long time and drug resistance eventually causes failure to treatment in most cases. The adaptation of multi-drug treatment in cancer increases the drug performance by reducing the drug resistance. But care must be taken to design the multi-drug scheduling so as to equilibrium the drug beneficial and adverse side effects of the treatment. Conventional clinical methods can hardly find optimum dosages of drugs that can kill maximum cancerous cells with minimum toxic side effects. This is because of the inherent conflict between the cell killing and the toxic side effects in case of cancer. This paper presents a novel method of multi-drug scheduling using multi-objective genetic algorithm (MOGA) that can trading-off between the cell killing and toxic side effects during the whole period of treatment. A close-loop control method, namely Integral-Proportional-Derivative (I-PD) is designed to control dosages of drugs to be infused to the patient's body and MOGA is used to find suitable/acceptable parameters of the controller. A cell compartments model is developed and used to describe the effects of the drugs on different type of cells, plasma drug concentration and toxic side effects. Results show that drug scheduling obtained through the proposed method can reduce the tumour size more than 99% with relatively lower toxic side effects. Moreover, the drug dosage and drug concentration remain at low level throughout the whole period.
[Drugs, cancer treatment, Cancer chemotherapy, Feedback control, Optimization, Multi-drug, patient treatment, scheduling, Mathematical model, multiobjective genetic algorithm, Immune system, multiobjective optimisation, three-term control, close loop control method, genetic algorithms, multidrug chemotherapy scheduling, integral proportional derivative, Multi-objective optimisation, Resistance, closed loop systems, plasma drug concentration, cell compartments model, cancer, clinical methods, Drug scheduling, Cancer, Tumors]
Optical encryption system employing orthogonal code and multiple reference-based joint transform correlation
2010 13th International Conference on Computer and Information Technology
None
2010
The paper proposes a new technology to ensure enhanced security of personal identification information or binary images through the use of encryption and multiplexing processes. Orthogonal codes are employed to encrypt the given input images so that multiple encoded images can then be mixed together to save storage space or transmission bandwidth. The multiplexed and encoded image is further encrypted employing multiple reference-based joint transform correlation. The address code is fed into four channels after performing phase shifting on them by different amount. The input image is introduced to the channels to obtain joint power spectra (JPS) signals through Fourier transformation. The resultant signals are phase-shifted and then combined to form a modified JPS signal which is then inverse Fourier transformed to yield the final and highly secure encrypted image. At the receiver, the image is Fourier transformed and multiplied by the address code used in encryption. Inverse Fourier transformation yields the multiplexed and encoded image which is then decoded using orthogonal codes. The proposed technique enhances the security performance by implementing orthogonal coding and nonlinear encryption algorithms so that no unauthorized access to or interception of information is possible. Performance of the technique is evaluated through computer simulation.
[Multiplexing, Fourier transforms, multiple image encoding, Encryption, inverse transforms, binary images, orthogonal codes, Optical polarization, joint power spectra signals, authorisation, computer simulation, personal identification information security, multiple reference based joint transform correlation, Nonlinear optics, optical images, phase shifting, orthogonal coding, joint transform correlation, performance evaluation, inverse Fourier transformation, unauthorized access, Optical imaging, cryptography, nonlinear encryption algorithms, joint power spectrum, encoding, decoding, multiplexing, image multiplexing, orthogonal code, optical image encryption system, image coding]
Performance of MPEG-7 edge histogram descriptor in face recognition using Principal Component Analysis
2010 13th International Conference on Computer and Information Technology
None
2010
Face recognition is considered as a high dimensionality problem. To handle high dimensionality, a numerous methods have been proposed in literature. In this paper, we propose a novel face recognition method that efficiently solves that problem using MPEG-7 edge histogram descriptor. To the authors' knowledge, this is the first attempt to use edge histogram descriptor in face recognition. Although MPEG-7 standard represents only local edge histogram we use global and semi-global edge histogram also. We find that local edge histogram mostly helpful for face recognition. We test our system not only using the entire face image as input but also dividing the image into different sub-divisions. PCA is then applied to the edge histogram descriptors of sub-divisions in-stead of raw pixel intensity values of images which traditional methods do. Since we use normalized edge histogram, our face recognition method becomes scale, translation and rotation invariant. Furthermore, our proposed method does not necessarily require all images to be of same resolution as input. We evaluate the proposed method using ORL, Yale and Face94 face databases and achieve superior performance.
[MPEG-7, Image edge detection, Face recognition, MPEG-7 edge histogram descriptor, Image retrieval, PCA, Edge Histogram Descriptor, Histograms, high dimensionality problem, Databases, Transform coding, face recognition, edge detection, semiglobal edge histogram, Principal Component Analysis (PCA), Face, Texture Descriptor, principal component analysis, Principal component analysis]
Selection of distinguishing features for fabric defect classification using neural network
2010 13th International Conference on Computer and Information Technology
None
2010
Over the years significant research has been performed for automated, i.e. machine vision based fabric inspection systems in order to replace manual inspection, which is time consuming and not accurate enough. Automated fabric inspection systems mainly involve two challenging problems, one of which is defect classification. The amount of research done to date to solve the defect classification problem is insufficient. Scene analysis and feature selection play a very important role in the classification process. Insufficient scene analysis results in an inappropriate set of features. Selection of an inappropriate feature set increases complexities of subsequent steps and makes the classification task harder. Considering this observation, we present a possibly appropriate feature set in order to address the problem of fabric defect classification using neural network (NN). We justify the features from the point of view of distinguishing quality and feature extraction difficulty. We perform some experiments in order to show the utility of proposed features. Promising classification accuracy has been found.
[Feature selection, production engineering computing, Gray-scale, machine vision based fabric inspection systems, neural network, Backpropagation algorithm, Image color analysis, feature extraction, Fabric defect, Fabrics, inspection, pattern classification, Artificial neural networks, distinguishing feature selection, Inspection, scene analysis, fabrics, fabric defect classification, Machine vision, Neural network (NN), computer vision, Feature extraction, Defect classification, Pixel, neural nets, Defect detection]
Speech segmentation using divergence algorithm with Zero Crossing property
2010 13th International Conference on Computer and Information Technology
None
2010
Divergence algorithm is a statistical segmentation approach which finds segmentation point via detection of abrupt changes without any previous information of the acoustic signal. The approach could get high match of segmentation but also gives a lot of false segmentation points. This work introduced a property based on the usage of Zero Crossing Rate (ZCR) in enhancing segmentation by divergence algorithm. The work starts via optimizing divergence algorithm segmentation performance via parameters tuning. Then the proposed property based on ZCR is applied to divergence algorithm to reduce insertion points. The results of tuning divergence parameters achieved match rate of 99.4% at time tolerance of 0.09 seconds with 69% insertion rate occurrences in comparisons to reference points. The result in applying the introduced ZCR property to divergence algorithm shows that tuning of some ZCR property parameters could reduce insertion between 4% to 45%. However, it would also reduce the match rate. Nevertheless, the method could reduced insertion rate by 5.5% while maintaining match rate of 99.4%.
[Statistical analysis, Humans, Zero Crossing Rate, zero crossing property, Divergence Algorithm, Acoustics, zero crossing rate, Tuning, divergence algorithm, statistical segmentation approach, Speech recognition, Speech, speech processing, statistical analysis, Statistical Approach, Speech processing, speech segmentation, Speech Segmentation]
An agent-based cooperative communication method in wireless sensor network for port logistics
2010 13th International Conference on Computer and Information Technology
None
2010
Cooperative communication has been considered as a promising method for benefiting the advantages from MIMO (multiple-input multiple-output) system for single antenna mobiles in the wireless network, and many important milestones in this area have been achieved, leading to a flurry of research activity. Especially, cooperative communication in wireless sensor networks (WSNs) has gained so much interest due to the limitations of WSNs. This paper proposes a new cooperative communication method for WSNs in port logistics according to the observation of real application in port. In our proposed method, we use some fixed agents to help the sensor nodes to gain more efficient communication in the network. The simulation results and analysis show that our method achieves a good performance in enhancing the transmission and energy efficiency, resulting in improvement of the communication performance of the whole network.
[Energy consumption, Error analysis, wireless sensor networks, Cooperative communication, wireless sensor network, logistics, Relays, Wireless Sensor Network (WSN), sensor nodes, energy efficiency, MIMO, single antenna mobiles, Data communication, MIMO communication, MIMO system, Agent, diversity reception, antennas, communication network performance, performance evaluation, port logistics, multiple-input multiple-output, cooperative communication, Wireless sensor networks, agent-based cooperative communication method, sensor, Logistics]
Cost analysis of NEMO protocol entities
2010 13th International Conference on Computer and Information Technology
None
2010
To support IP-mobility of networks in motion, IETF proposed Network Mobility (NEMO) protocol that uses various signaling messages to ensure connectivity of the mobile nodes with the Internet. However, there has been no comprehensive cost analysis of mobility protocol entities that considers all possible costs. In this paper, we have developed analytical models to estimate total costs of key mobility management entities of NEMO. We have presented numerical results to demonstrate the impact of network size, mobility rate, traffic rate and data volume on these costs. Our cost analysis will thus help network engineers in estimating actual resource requirements for the key entities of the network in future design.
[data volume, Protocols, key mobility management entities, Internet connectivity, Mobile communication, mobility rate, mobility management (mobile radio), NEMO protocol entities, IP-mobility, mobility management, network size, network mobility protocol, Wireless communication, mathematical modeling, Internet, IP networks, protocols, cost analysis, NEMO, Mobile computing, Manganese, traffic rate]
Development of mobile phone based surveillance system
2010 13th International Conference on Computer and Information Technology
None
2010
In today's world, ensuring security for important locations is a burning issue. Different surveillance methodologies such as alarm system, CCTV, PC based video system are used to ensure this security. But using all these systems, it is not possible for a person to monitor the security of his or her desired location when they are outside. Now-a-days anybody can communicate with anyone at anytime around the globe with the help of mobile phone technology. By keeping the technological facility of mobile phone in mind, a mobile phone based surveillance system has been described in this paper. This paper will give a solution for the security of corporate houses as well as corporate personnel. In this system there are server and client end. Server will store images being captured by the webcam of an important location. Then based on clients request sending from their mobile phone to the server, clients will be able to view the images from their mobile phone that is stored in the server. Clients are also able to move the webcam by sending control instruction from their mobile phone to view the images of their desired position. The developed system has been tested first using the GUI (emulator) designed by NetBeans IDE. It has also been tested using different mobile phones to see the images in real time and it provides the true sense of real mobility and security by accessing the desired location from the mobile phone anytime anywhere whenever wishes. This surveillance system can also be implemented for other handheld devices like PDA.
[NetBeans IDE, graphical user interfaces, usage of mobile phone as GUI, Mobile handsets, Servers, Security, corporate personnel, Java 2 micro edition (J2ME), real time images, security, surveillance system, Hardware, GUI, video surveillance, Graphical user interfaces, image processing, client-server systems, Java programming for server and client software, Java Media Framework (JMF), corporate houses, security of data, Surveillance, mobile phone, mobile handsets, server-client system]
Lessons learned from real MANET experiments and simulation-based evaluation of UDP and TCP
2010 13th International Conference on Computer and Information Technology
None
2010
Although more than a decade of research has been done but pure general-purpose MANET is still not available rather than few prototypes within laboratory due to both technical and socio-economic point of view. Lacking in appropriate guidelines for realistic user traces, mobility models, routing protocols and considering real-life challenges, it is difficult to reproduce any typical scenario in reality apart from simulation. In this paper, difficulties faced to regenerate real-life scenarios have been discussed to clearly identify the gaps in simulation and real-time experiments. Four laptops are used in an open field environment for different scenarios to evaluate a TCP based streaming video application using real OLSR implementation within a IEEE 802.11g wireless network. Corresponding simulations are performed in ns-2 based on the realistic setup parameters achieved from real experiments and finally a comprehensive analysis identifies the generic gaps between these two approaches to evaluate network protocols. Simulation results show better performance than the real-life results due to differ in external influences and protocol implementation although maintaining realistic simulation setups.
[UDP, TCP, radio networks, mobile ad hoc network, Protocols, Peer to peer computing, mobility models, Real MANET, Throughput, Mobile ad hoc networks, Wireless communication, MANET, transport protocols, mobile ad hoc networks, routing protocols, Streaming media, OLSR, ns-2, wireless network]
Performance analysis of NEMO using city section mobility model
2010 13th International Conference on Computer and Information Technology
None
2010
Mobile networks can be formed in bus, train, aircrafts, satellites with a wide variety of on-board IP-enable devices and Network Mobility (NEMO) protocols are required to support uninterrupted services to ongoing sessions. Node mobility has a direct impact on the performance evaluation of various NEMO protocols. However, most of the analysis on mobility protocols used random waypoint mobility model which does not always represent real-world movement patterns in city streets. In this paper, we have used city section mobility model, a realistic street mobility model, to analyze the performance of the basic network-mobility protocol. We have used ns-2 simulation to compare the performance of NEMO using city section and random waypoint models. Results have been obtained for average throughput, packet drop probability, end-to-end delay, handoff frequency, and signaling overhead and show significant deviation between the mobility models. Our analysis thus can help in estimating the various performance metrics of mobile network deployed in city streets.
[uninterrupted services, city section mobility model, ns-2 simulation, Mobile communication, Throughput, Delay, node mobility, real world movement pattern, Analytical models, packet radio networks, end-to-end delay, Cities and towns, IP networks, protocols, mobile network mobility protocol, mobile radio, random waypoint mobility model, NEMO protocol, computer networks, performance evaluation, random waypoint models, onboard IP-enable devices, network mobility, packet drop probability, street mobility model, handoff frequency, mobility model, Mobile computing, performance analysis, Manganese, mobile network]
A cross layer framework for WLANs: Joint radio propagation and MAC protocol
2010 13th International Conference on Computer and Information Technology
None
2010
This paper proposes a cross-layer design (CLD) framework called channel-aware buffer unit multiple access (C-BUMA) for improving wireless local area network (WLAN) performance. In the framework, the radio propagation (i.e. PHY layer) is combined with the medium access control (MAC) protocol for packet transmissions. By sharing channel information with the MAC protocol, the approach reduced unnecessary packet transmissions and hence improved system performance. Through performance evaluation, we demonstrate that our CLD can significantly improve network throughput and packet delay. The proposed C-BUMA is simple and can easily be implemented in 802.11 networks without changing hardware infrastructure and no additional costs. In this paper we describe C-BUMA and present two algorithms for the implementation of the framework.
[IEEE 802.11 Standards, medium access control protocol, Bit error rate, cross layer design, Throughput, Cross-layer design, WLAN, access protocols, radiowave propagation, 802.11 network, Delay, wireless local area network, Wireless communication, channel aware buffer unit multiple access, 802.11 networks, MAC protocol, Media Access Protocol, Prediction algorithms, multi-access systems, wireless channels, wireless LAN, radio propagation, packet transmission, C-BUMA]
A Cyclic Quorum based multi-channel MAC protocol with layered approach for mobile ad-hoc networks
2010 13th International Conference on Computer and Information Technology
None
2010
The IEEE 802.11 MAC layer protocol is designed for single channel. The improvement of network throughput can be done through the enhancement of spatial reuse and the reduction of transmission collusions by the utilization of multiple channels is important. Channel allocation can exploit the multi-channel missing receiver problem besides there are other problems such as the primordial hidden terminal and exposed terminal problem. Existing multi-channel MAC protocols experience either higher hardware cost (because of applying multiple transceivers) or lower channel utilization (due to limited transmission opportunity). Previous approaches that use RTS/CTS or other modified ideas sometimes are not enough to meet the demands. In this paper, a fully distributed channel hopping solution has been discussed as, The Modified Cyclic-Quorum-based Multichannel (M-CQM) MAC protocol. Cyclic quorum is used here in an efficient way. The proposed protocol has several attractive features. Sender is guaranteed to meet its receiver in a short time. Each node's channel hopping sequence is derived from its node ID and the layer it belongs. This proposed idea also tries to divide the physical location into three basic zones which removes the load of RTS/CTS or other similar approaches. We have evaluated our proposed approach (M-CQM) in network simulator (ns-2.34) and it outperforms existing proposals.
[IEEE 802.11, spatial reuse, IEEE 802.11 Standards, Switches, Receivers, Transceivers, access protocols, Delay, IEEE 802.11 MAC layer protocol, transmission collusions, MANET, cyclic quorum, mobile ad-hoc networks, mobile ad hoc networks, multi-channel MAC protocol, multi-channel missing receiver problem, Media Access Protocol, Modified CQM (M-CQM) and Quorum Systems, channel allocation]
A novel strategy to discover Internet gateways in mobile ad hoc networks
2010 13th International Conference on Computer and Information Technology
None
2010
Integrating nodes in a MANET to the Internet require either a connection to the Internet or they can connect to the Internet through the Internet gateways or servers. For the second case a node in a MANET has to find out the gateway or server to connect to the fixed nodes in the Internet. End to end packet delay and throughput are strongly dependent on the time needed to discover the gateways. A source node in a MANET can discover a gateway either by broadcasting a gateway discovery message or it may depend on periodic gateway advertisement messages from the gateways. The first one is renowned as reactive gateway discovery strategy and with this one a gateway in a MANET may receive the same gateway discovery message from different intermediate nodes. This paper presents a novel solution for connecting nodes in ad hoc network to the Internet. Here a gateway sends separate reply to the requestor for each request message having the same Broadcast ID it receives from that source in stead of sending only one unicast reply to the requestor. The AODV routing protocol has been used for routing in the MANET domain. We investigated our new strategy of gateway discovery in NS-2. Simulation results show that our one has better performance having lower delay and fewer packets drop.
[radio broadcasting, Internet Gateway, internetworking, routing protocol, Mobile communication, Throughput, NS-2 simulation, Re-send, Delay, Gateway Advertisement, Mobile ad hoc networks, broadcasting, MANET, intermediate nodes, Intermediate Nodes, mobile ad hoc networks, routing protocols, Logic gates, Mobile Ad hoc network, Internet, internet gateways, AODV]
A stable clustering method for efficient geographic routing in Mobile Ad-Hoc Networks
2010 13th International Conference on Computer and Information Technology
None
2010
The Routing Approaches in Mobile Ad-hoc Network have almost entered the saturated arena. But still there are available options of perfection in this meadow. The task of improving the clustering technique is one of the fields which is under rated as a research topic for further improvement of routing. Clustering based MANET routing protocols, which are popular in terms of efficiency, simplicity and usability mostly use circular based clustering techniques. And the election of cluster head is done through the use conventional approaches. This particular paper is mainly focused to launch a new-fangled clustering technique through which it is possible to improve the routing performance of existing protocols. Another important aspect of this paper is to provide a cluster head selection algorithm which can effectively maintain the clusters and provides more stability. Proposed clustering idea is useful for geographically related nodes effectively in different turf of routing. To maintain the clusters and their stability, it provides a new idea to select cluster head within the cluster, also the election of secondary cluster head for avoiding further election immediately after the unavailability of primary cluster head. This idea is evaluated in network simulator and it outperforms the existing clustering techniques.
[Primary-CH, clustering method, Secondary-CH, Nominations and elections, geographic routing, Routing, Magnetic heads, Cluster Head Selection, Clustering, Mobile ad hoc networks, MANET, Modified Graham Scan (MGS), network simulator, Clustering algorithms, mobile ad hoc networks, routing protocols, cluster head selection algorithm, Routing protocols, statistical analysis]
Channel estimation techniques and LTE terminal implementation challenges
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper least square (LS) and linear minimum mean square error (LMMSE) channel estimation (CE) techniques are presented for long term evolution (LTE) single carrier-frequency division multiplexing (SC-FDMA) systems. The main purpose of LTE is to increases data rate but energy utilization both on the mobile terminal as well as network side is important. For doing this, major challenges for LTE terminal are CE and equalization. This paper discusses the CE techniques and challenges imposed by developments in the LTE terminal implementation. Simulation results shows that the LMMSE CE algorithms outperforms the LS in term of mean square error (MSE) by more than around 3dB. Hence, based on a given LTE systems resources and specifications, a appropriate method among the presented methods can be applied.
[LMMSE CE algorithm, least mean squares methods, long term evolution, OFDM, Estimation, Receivers, LTE terminal, LS, mobile terminal, Computational complexity, LMMSE, least square mean square error, single carrier frequency division multiplexing, Wireless communication, channel estimation, Channel estimation, frequency division multiplexing, linear minimum mean square error, CE technique, SC-FDMA, LTE, Long Term Evolution, energy utilization]
Map outage factor optimization for WiMAX
2010 13th International Conference on Computer and Information Technology
None
2010
The emergence of WiMAX has attracted significant interests from all fields of wireless communications. WiMAX has been tipped to bring a revolution in the way where broadband services have been used today; those have been strengthened by the optimization of RF. The systematic investigation to establish facts of necessary modification about the theory for smooth optimization is targeted. This paper has considered the map outage factor of the objective function for the invention of a method to produce suitable values which help to realize better output. Simulation results found to be more precise over conventional RF of WiMAX in finding the modification of the existing theory.
[wireless communications, telecommunication services, RF signals, Interference, WiMAX, WiMax, Throughput, broadband networks, Broadband communication, Optimization, Radio frequency, map outage factor optimization, optimisation, Map Outage Factor, RF, broadband services, SINR, Object Function, Signal to noise ratio]
Multitaper spectrum: A promising method in spectrum sensing cognitive radio
2010 13th International Conference on Computer and Information Technology
None
2010
Cognitive radio (CR) has been proposed as a promising and effective technology to improve radio spectrum utilization. The primary objective of the CR is to handle the non-interference rules with any primary users (PUs). Highly sensitive and optimal spectrum sensing detectors are required in order to avoid harmful interference to PUs. Multitaper spectrum seems to be the most appealing one for spectrum sensing CR because of its accurate identification and estimation and low computational complexity. Mulitaper uses small set of tapers and multiple orthogonal prototype filters to reduce the variance. The Fourier transform of a Slepian sequence, originally known as discrete prolate spheroidal sequences (DPSS), gives maximum energy density inside a given bandwidth and less spectral leakage with better specifications has been investigated in this paper and shows that no other window in signal processing can satisfy this property.
[Fourier transforms, primary users, spectrum sensing cognitive radio, noninterference rules, Fourier transform, Cognitive radio, multitaper spectrum, Spectral analysis, Slepian sequence, discrete prolate spheroidal sequences, Multitaper spectrum, Discrete prolate spheroidal sequences, cognitive radio, Spectrum sensing, Coherence, Band pass filters, Sensors, computational complexity]
Optimization technique for configuring IEEE 802.11b access point parameters to improve VoIP performance
2010 13th International Conference on Computer and Information Technology
None
2010
The performance of wireless LANs is greatly affected by path loss, RF interference and other sources of signal attenuation in addition to network congestion. The primary factors involved in effective real-time communication, namely delay and loss, must be within certain controlled limits in such a scenario. In this paper, we analyze the various factors driving IEEE 802.11b access points through extensive simulations and thereafter develop an optimization technique to configure the parameters of the Access Point. We simulate our test bed scenario and apply the developed algorithm. Finally, we implement the configured parameters in our testbed to provide optimum Voice over IP (VoIP) performance. Simulation and measured results have been included.
[Real time systems, optimization technique, telecommunication congestion control, Quality of service, Loss measurement, MATLAB, Delay, Optimization, radiofrequency interference, optimisation, real time communication, Transmitters, voice communication, VoIP performance, IP networks, IEEE 802.11b, network congestion, Access Point, VoIP, RF interference, signal attenuation, quality of service, Quality of Service, IEEE 802.11b access point parameter, wireless LAN, Internet telephony, voice over IP]
PAIZM-CPC: Enhancement of spectrum co-existence in wireless networks
2010 13th International Conference on Computer and Information Technology
None
2010
Next generation wireless networks will be heterogeneous, where several primary users (PU e.g. licensed users) and secondary users (SU e.g. unlicensed users) can operate in the same dynamic and reconfigurable networks at a given time. The major challenge in this heterogeneous radio environment is to enable the coexistence between PU and SU which will further improve the efficient use of radio spectrum. Most of the existing coexistence techniques encounter with challenges due to lack of a priori knowledge about the primary system. Therefore Cognitive pilot channel (CPC) is a proposed approach which could enhance the coexistence by conveying some priori information. However, to achieve a peaceful coexistence it is essential to adopt a mitigation technique according to the CPC information. There is no algorithm has been described so far to integrate the CPC information with existing mitigation technique. In this paper, we proposed a novel power adaptation and integrated zone model (PAIZM) CPC algorithm for peaceful coexistence in heterogeneous networks. Moreover we have implemented and evaluated the PAIZM-CPC model as a coexistence enabler. The results show an enhancement compared with the existing coexistence techniques.
[radio spectrum management, heterogeneous networks, reconfigurable networks, coexistence techniques, next generation networks, Interference, Receivers, WiMAX, Coexistence, Proposals, Delay, Radio spectrum management, Reconfigurable networks, cognitive pilot channel, radio spectrum enhancement, cognitive radio, next generation wireless networks, power adaptation and integrated zone model, CPC, Mitigation techniques, wireless channels, Resource management, Cognitive Pilot Channel]
Peak to average power reduction using precoding with clipping and filtering
2010 13th International Conference on Computer and Information Technology
None
2010
3rd Generation Partnership Project Long Term Evolution (3GPP LTE) has adopted orthogonal frequency division multiple access (OFDMA) as downlink and single carrier frequency division multiple access (SC-FDMA) as uplink for multiple access scheme. SC-FDMA is adopted in uplink due to its low peak to average power ratio (PAPR). To reduce PAPR in OFDMA, several techniques are used in literature. SC-FDMA is adopted in 3GPP as it shows better PAPR characteristics than OFDMA. Many researches are still going on to find the impact of different parameters on PAPR in SC-FDMA. In this paper, a PAPR reduction technique is proposed where the precoding is done in the first step and at the second step, clipping is performed using SC-FDMA environment. Result shows that the PAPR is reduced at the increase in number of clips.
[3GPP LTE, clipping, frequency division multiple access, power reduction, 3rd Generation Partnership Project Long Term Evolution, OFDM, Discrete Fourier transforms, orthogonal frequency division multiple access, 3G mobile communication, Peak to average power ratio, Receivers, Time domain analysis, Decision feedback equalizers, precoding, Frequency domain analysis, PAPR, single carrier frequency division multiple access, peak to average power ratio, OFDM modulation, OFDMA, SC-FDMA, filtering]
Performance comparison of Selection Combining diversity receivers for different modulation schemes in OFDM system
2010 13th International Conference on Computer and Information Technology
None
2010
This paper deals with the performance analysis (i.e., Symbol Error Rate) of M-ary Phase-shift-keying (M-PSK) and M-ary Quadrature Amplitude Modulation (M-QAM) to transmit information using Selection Combining (SC) diversity technique over flat Rayleigh communication channel with Orthogonal Frequency Division Multiplexing (OFDM). It is well known that the multipath fading phenomenon is an inherent characteristic of wireless channel. In this paper, it is shown how selection combining diversity scheme enhances reliability in wireless communication by minimizing the channel fluctuations due to fading. The simulation results show that the Signal-to-Noise Ration (SNR) varies between M-PSK and M-QAM but better SNR gain achieved as the number of receiving antenna increases like for two, three and four receiving antennas around 20-27 dB, 22-30 dB, 23-31 dB diversity gains are found respectively.
[multipath fading phenomenon, orthogonal frequency division multiplexing, OFDM, quadrature amplitude modulation, phase-shift-keying, wireless communication, Rayleigh communication channel, phase shift keying, channel fluctuations, M-ary PSK, OFDM modulation, wireless channel, OFDM system, Fading, Rayleigh channels, M-ary QAM, Phase shift keying, selection combining diversity receivers, Diversity reception, Receiving antennas, signal-to-noise ratio, receiving antenna, Rayleigh fading Channel, performance analysis, Signal to noise ratio, Selection Combining(SC)]
Performance evaluation of multidimensional traffic in micro-macro cellular system
2010 13th International Conference on Computer and Information Technology
None
2010
We evaluate performance of hierarchical network (overlay-underlay cellular system) based on convolution method under mixed offered traffic. In most of the cases, different offered traffic of a network follows different probability density functions and they are correlated in sharing channel environment and cannot be analyzed by equivalent random theory (ERT) model. Here, three different types of offered traffic are considered for determining the blocking probability in the higher-tier cells.
[blocking probability, hierarchical network, telecommunication network planning, probability, ERT, Quality of service, performance evaluation, probability density functions, micro-macro cellular system, Alternate routing and MMPP, sharing channel environment, Wireless communication, Convolution, CAS, Land mobile radio cellular systems, multidimensional traffic, Bandwidth, convolution method, wireless channels, telecommunication traffic, Asynchronous transfer mode, cellular radio]
Radio Network Planning for Fixed WiMax in Dhaka City and it's performance analysis
2010 13th International Conference on Computer and Information Technology
None
2010
This paper, proposes a Radio Network Planning for Fixed WiMax Network for Dhaka City. To develop the plan a cell radius of 400 meter is assumed. For the justification of this assumption several numbers of experimental analyses are done. Two experiments are executed to know the data speed with respect to the distance both for uplink and downlink communication. Another two experiments are executed for both downlink and uplink to know the most preferable WiMax frequency band for Dhaka city. The experiments are done for different types of modulation schemes. From the performance tests it is decided that the designed network will perform pretty well with the cell radius of 400 meter and at the frequency band of 2.5 GHz.
[Base stations, adaptive modulation, Duplexing, Frequency modulation, Cost231 model, telecommunication network planning, frequency 2.5 GHz, WiMAX, WiMax, fixed WiMax, Downlink, Dhaka city, PMP, WiMax frequency band, distance 400 m, Cities and towns, MIMO, Planning, radio network planning, Adaptive modulation, performance analysis, modulation schemes]
Secret data communication in a degraded practical multiple input multiple output multiple eavesdropper channel
2010 13th International Conference on Computer and Information Technology
None
2010
In this paper, a Gaussian multiple input multiple output multiple eavesdropper (MIMOME) channel is considered where a transmitter communicates to a receiver in the presence of an eavesdropper. We present a technique for determining the secrecy capacity of the multiple input multiple output (MIMO) channel under Gaussian noise. We transform the degraded MIMOME channel into multiple single input multiple output (SIMO) Gaussian wire-tap channels and then use scalar approach to convert it into two equivalent multiple input single output (MISO) channels. The secrecy capacity model is then developed for the condition where the channel state information (CSI) for main channel only is known to the transmitter. The results show that the secret communication is possible when the eavesdropper channel noise is greater than a cutoff noise level. The outage probability is also analyzed of secrecy capacity is also analyzed. The effect of fading and outage probability is also analyzed.
[Fading, secrecy capacity model, eavesdropper channel noise, Channel capacity, probability, multiple input multiple output multiple eavesdropper channel, channel state information, fading, secret data communication, covariance matrix, outage probability, Gaussian noise, Receiving antennas, Gaussian wire-tap channels, MIMO, Secrecy capacity, wiretap channel, Gaussian channels, MIMO communication, Capacity planning, Signal to noise ratio]
Shared key vulnerability in IEEE 802.16e: Analysis &amp; solution
2010 13th International Conference on Computer and Information Technology
None
2010
As a promising broadband wireless technology, WiMAX has many salient advantages over such as: high data rates, quality of service, scalability, security and mobility. Many sophisticated authentication and encryption techniques have been embedded into WiMAX but it still facing a lot of challenging situations. This paper shows different security vulnerabilities found in IEEE 802.16e and gives possible solutions to eliminate them. These vulnerabilities are the possibilities to forge key messages in Multi- and Broadcast operation, some unauthenticated messages which are susceptible to forgery and the unencrypted management communication which reveals important management information. We modify DH key exchange protocol to fit it into mobile WiMAX network as well as eliminate existing weakness in original DH key exchange protocol.
[telecommunication security, Protocols, mobility, IEEE 802.16e, Mobile communication, Encryption, mobility management (mobile radio), scalability, DH key exchange protocol, hash chaining solution, unencrypted management communication, telecommunication network reliability, multi- and broadcast service, shared key vulnerability, protocols, mobile WiMax network, Multi- and Broadcast operation, IEEE 802.16e security, unauthenticated message, sophisticated authentication, WiMAX, WiMax, cryptography, broadband wireless technology, quality of service, Authentication, message authentication]
Theoretical maximum throughput of IEEE 802.11e EDCA mechanism
2010 13th International Conference on Computer and Information Technology
None
2010
The goal of this paper is to validate a formulae, originally derived to calculate the throughout upper bound of the IEEE 802.11 networks, over a single Access Point (AP) based WLAN system combined with the IEEE 802.11e EDCA mechanism. In this paper some important assumptions are made to keep the formulae very simple. The most important assumption is that, the AP is providing service to client nodes with only one type of Access Category (AC) traffic at an instant. Comparing both the theoretically achieved maximum throughput and simulation results, for different access categories (ACs) and for different packet sizes we observed that the formulae is pretty much accurate except for only one test case.
[theoretical maximum throughput, IEEE 802.11e, Throughput, Access Point (AP), WLAN system, Delay, Equations, EDCA mechanism, wireless local area network, Theoretical Maximum Throughput (TMT), Access Category (AC), IEEE 802.11e Standard, single access point, Data models, IEEE 802.11 network, Mathematical model, wireless LAN, EDCA]
Towards enhancing system lifetime and maximizing data communication in Wireless Sensor Networks
2010 13th International Conference on Computer and Information Technology
None
2010
Enhancing lifetime of sensor nodes should be considered as the key design objective in Wireless Sensor Networks (WSN). A sensor node can only be equipped with a limited energy supply and it loses its energy during data communication. In some application scenarios, replenishment of energy resources might be impossible since the sensor nodes are distributed in remote environment. Hence, the nodes lose their energy quickly and become dead. The frequent topology changes due to the die of sensors make the network quite unstable. A good cluster head selection protocol is, therefore, required to enhance system lifetime and data communication. This paper proposes a new methodology for cluster head selection based on sensor nodes' energy per unit cost. Experimental study shows that the proposed method, by adopting few selection criteria on choosing cluster head, increases the system lifetime and maximize data communication in comparison to existing dominant approaches.
[Base stations, Head, Protocols, data communication maximization, wireless sensor networks, energy per unit cost, telecommunication network topology, cluster head selection protocol, Sensor Nodes, Cluster Head Selection, network topology, system lifetime enhancement, Wireless sensor networks, pattern clustering, Energy dissipation, Clustering algorithms, energy resources, remote environment, Enhancing Lifetime, Data communication, protocols, sensor node, Energy Dissipation]
Welcome
14th International Conference on Computer and Information Technology
None
2011
Presents the welcome message from the conference proceedings.
[]
Committees
14th International Conference on Computer and Information Technology
None
2011
Provides a listing of current committee members.
[]
Quaternary quantum algorithm for determining properties of quaternary logic function
14th International Conference on Computer and Information Technology
None
2011
A multiple-valued (d-dimensional with d &gt;; 2) quantum system enables a much more compact and efficient information encoding than for binary (d = 2) quantum system. As multiple-valued quantum system is physically realizable, it is worth to explore multiple-valued quantum algorithm. In this paper, we propose quaternary (4-dimensional) quantum algorithm for testing whether a quaternary logic function is either constant or balanced. In classical computer, this test would require at least 4n-1 +1 function evaluations, where n is the number of inputs of the function. But the proposed quantum algorithm requires only one function evaluation.
[Computers, multiple-valued quantum algorithm, function evaluation, balanced logic function, quantum algorithm, quaternary controlled gate, multiple-valued quantum system, quaternary quantum algorithm, information encoding, binary quantum system, formal logic, quaternary quantum system, quantum computing, constant logic function, quaternary unitary reversible transformation, Multiple-valued quantum system, quaternary logic function]
A Semi-Distributed and Mobile Agent based architecture for load balancing of heterogeneous wireless networks
14th International Conference on Computer and Information Technology
None
2011
Heterogeneous wireless network pose many interesting research challenges. Among them, resource management and load balancing in such a hybrid environment is still an open problem. Consider these issues an efficient semi distributed load balancing architecture for multiple access networks is introduced in this paper. Since both centralize and distributed architecture has their own pros and cons, in partially distributed architecture we successfully overcome the problem of both design. In this grid based design multiple Load and Mobile Agent Management Units is incorporated. To prove the compactness of the design signaling overhead and total processing time is calculated. And finally simulation result shows that overall system performance is improved by reducing overhead and time.
[resource management, radio networks, load balancing, grid computing, Load Balancing Architecture, Wireless communication, semi-distributed architecture, resource allocation, multiple access networks, Heterogeneous network, design signaling overhead, mobile agents, mobile agent based architecture, partially distributed architecture, Overhead, grid based design, multi-access systems, heterogeneous wireless networks, Mobile agent, Reliability, Bulletin Broad]
Fuzzy Logic based snooze schema for wireless sensor network MAC protocol
14th International Conference on Computer and Information Technology
None
2011
Energy competence is an imperative aspect to intend an extensive existence wireless sensor network. Several methods have been proposed to increase the battery lifetime of sensor nodes. One of the methods is sporadically turn off the radio receivers of the sensor nodes in a synchronized mode. The sleep and wake up mode are included with this corresponding nature. Arbitrarily elected node form virtual clusters based on universal snooze scheme. However, protocols resembling the extensively known S-MAC possibly will require some nodes to pursue several snooze schemes causing them to wake up more frequently than the other nodes. In this paper we present the wireless sensor networks by means of S-MACF (for S-MAC Fuzzy) where a significant amount of the nodes may have to stay awake much extended than expected. An adjustment of the protocol is then projected to get rid of the requirement for the nodes to keep on awake longer than the other nodes. The customized edition improves the energy efficiency and enhances the life length of a wireless sensor network (WSN).
[wireless sensor networks, synchronized mode, energy competence, fuzzy logic, access protocols, Multiaccess communication, Synchronization, radio receivers, virtual cluster, frames synchronization, Wireless sensor networks, S-MACF, snooze scheme, wireless sensor network MAC protocol, sensor fuzzy logic based snooze schema, battery lifetime, energy efficiency, universal snooze scheme, radio receiver, Robustness, Energy efficiency, sensor node]
An empirical investigation into the factors influencing the intention to adopt RFID and guidelines for Bangladesh
14th International Conference on Computer and Information Technology
None
2011
Because of its unique capability to identify, track, and trace-back objects, Radio Frequency Identification (RFID) technology has been used by organizations in innovative applications. Australian livestock industry is regarded as one of the biggest in the world and currently has adopted RFID for cattle identification. However, still a large number of farms have not adopted RFID for sheep and other animal identification. Thus, it is quite important to examine their intention toward RFID adoption. The finding of this study revealed that management atittude toward RFID is the main factor that may increase RFID adoption. Pressure from external environment also came up as significant. Results from Australian study set a guideline for Bangladeshi organizations to adopt RFID. To introduce RFID, organizations shoul d build a positive mindset whereas government and other external bodies can also contribute significantly. RFID manufacturers should concentrate on reducing RFID costs and developing a consistent standard. Therefore, this study has implications for government, external organizations, and vendors.
[Technological innovation, Uncertainty, radiofrequency identification, Government, Bangladesh organizations, RFID, trace-back objects, object detection, sheep identification, Bangladesh, adoption, Standards organizations, object tracking, Agriculture, object identification, animal identification, organisational aspects, Radiofrequency identification, cattle identification]
Mind-mapping: An effective technique to facilitate requirements engineering in agile software development
14th International Conference on Computer and Information Technology
None
2011
Merging agile with more traditional approaches in software development is a challenging task, especially when requirements are concerned: the main temptation is to let two opposite schools of thought become rigid in their own assumptions, without trying to recognize which advantages could come from either side. Mind mapping seems to provide a suitable solution for both parties: those who develop within an agile method and those who advocate proper requirements engineering practice. In this paper, mind mapping has been discussed as a suitable technique to elicit and represent requirements within the SCRUM model: specifically, we have focused on whether and how mind maps could lead to the development of a suitable product backlog, which in SCRUM plays the role of an initial requirements specification document. In order to experimentally assess how effectively practitioners could rely on a product backlog for their first development sprint, we have identified the adoption of mind maps as the independent variable and the quality of the backlog as the dependent variable, the latter being measured against the "function points" metric. Our hypothesis (i.e., mind maps are effective in increasing the quality of product backlogs) has been tested within an existing SCRUM project (the development of a digital library by an academic institution), and several promising data have been obtained and further discussed.
[Mind Map, software prototyping, mind-mapping technique, product backlog development, Documentation, formal specification, academic institution, agile method, digital library, Accuracy, requirements engineering, function points metric, Agile, Scrum, SCRUM project, Requirement Engineering, agile software development, Usability, requirements specification document, Analysis of variance, SCRUM model]
Efficient implementation of adaptive LZW algorithm for medical image compression
14th International Conference on Computer and Information Technology
None
2011
The paper presents an efficient adaptive Lempel-Ziv-Welch (LZW) data compression algorithm for medical image compression applications. The encoder is based on custom-sized library and custom-valued threshold where both the size of the library and the threshold parameters are adjusted by the user. The algorithm is compared with other existing compression schemes applicable to medical imaging. Because of the adaptive nature, the output of encoder can be controlled as per the bandwidth requirement of the data transmission and storage capacity. The simulation is carried out on several medical and endoscopic images and the results show that a compression ratio of 94% can be achieved with a reconstruction quality of 40 dB. The estimated energy consumption is 21.6mJ for a 256&#x00D7;256 color image with a frame rate of 2fps.
[data compression, Medical imaging, LZW, adaptive LZW algorithm, image reconstruction, threshold parameter, Lempel-Ziv-Welch algorithm, reconstruction quality, custom-sized library, medical image compression, data transmission, color image, storage capacity, custom-valued threshold, library size parameter, adaptive algorithms, image colour analysis, endoscopes, energy consumption, image coding, endoscopic image, medical image processing, Biomedical imaging, entropy encoder]
High speed detection of fundus optical disc
14th International Conference on Computer and Information Technology
None
2011
In this paper a fast method has been proposed for the detection of optical disc in retinal fundus image using mean intensity values, to be used in retinal image based person authentication system. The proposed method uses pixel intensity in green channel of RGB image and candidate based approach to detect optical disc location. Proposed system has been successfully tested on DRIVE, MESSIDOR, VARIA, VICAVR and DIARETDB_01 publicly available standard databases and produced 97.5%, 97.8%, 94%, 93.1% and 86.5% accuracy respectively.
[Retinopathy, DIARETDB_01 database, Image processing, VICAVR database, visual databases, red-green-blue image, object detection, DRIVE database, Accuracy, Databases, retinal fundus image detection, Imaging, medical image, Medical image, Optical disc, image colour analysis, VARIA database, fundus optical disc detection, medical image processing, retinal image based person authentication system, eye, Retinal image, MESSIDOR database, RGB image, pixel intensity, mean intensity value]
Dashboard - a novel approach to re-find information in a website through building personalized navigational menus
14th International Conference on Computer and Information Technology
None
2011
Re-visitation or re-finding information in a website is a very frequent activity in web browsing. When a user re-visits a website especially after a long time, she encounters some problems among which being oblivious about the information structure of the site is very prominent. Another problem is that in a website, users have no way to organize the information space or the navigational structure of the website. In this paper, we have proposed a novel approach named Dashboard which supports building custom navigational menus system and allows each user build her own hierarchical information structure. We have also demonstrated how this new approach helps quickly re-finding information on re-visitation of a website.
[Web Navigation, hierarchical information structure, Navigation, Personalization, Re-visitation, information retrieval, Dashboard, Website, navigational structure, Engines, personalized navigational menus, information revisitation, information refinding, Web browsing, human computer interaction, Web sites, Human Computer Interaction]
Active queue management with adaptive codec bit rate variation in WLAN for efficient VoIP performance
14th International Conference on Computer and Information Technology
None
2011
VoIP over Wireless LANs is greatly affected by path-loss, RF interference and other sources of signal attenuation in addition to network congestion. The primary factors involved in effective real-time communication, namely delay and loss, must be within certain controlled limits in such a scenario. Wireless LAN Access Points must, therefore, act in tune with the codecs to ensure high quality of the ongoing VoIP sessions. In this paper, we analyze the various codec parameters with respect to wireless LAN APs. Thereafter, we develop an optimization algorithm based on proactive strategy to keep loss and latency within tolerable limits. Finally, we implement the algorithm in our test-bed to provide optimum Voice over IP (VoIP) performance.
[Voice over IP performance, telecommunication network management, Switches, Throughput, wireless LAN access points, optimization algorithm, adaptive codec bit rate variation, Degradation, radiofrequency interference, Codec, VoIP performance, proactive strategy, Local area networks, queueing theory, codecs, VoIP over wireless LAN, Access Point, RF interference, signal attenuation, WLAN, adaptive codes, Voice over IP, Active Queue Management, Approximation algorithms, active queue management, Packets Per Second, wireless LAN, Internet telephony, VoIP sessions]
Performance of Turbo coded wireless link for SISO-OFDM, SIMO-OFDM, MISO-OFDM and MIMO-OFDM system
14th International Conference on Computer and Information Technology
None
2011
In this paper, performance of a Turbo coded OFDM wireless link is evaluated in the presence of Rayleigh fading for SISO, SIMO, MISO and MIMO system. Data are encoded using turbo encoder then modulated by QPSK or 16 QAM or 64 QAM and further encoded using STBC, and the encoded data are split into n streams which are modulated by OFDM and simultaneously transmitted using n transmit antennas. It is observed that the turbo coded SISO-OFDM system provides 21 dB coding gain at 10-4, turbo coded SIMO-OFDM system provides 20 and 13 db coding gain for 2 and 4 receive antennas respectively at a BER of 10-6, turbo coded MISO-OFDM system provides 17 and 12 dB coding gain for 2 and 4 transmit antennas respectively at a BER of 10-6 and turbo coded MIMO-OFDM system provides 11 to 13 dB coding gain for different combination of transmit and receive antennas at BER 10-6 compare to uncoded SISO-OFDM, SIMO-OFDM, MISO-OFDM and MIMO-OFDM system.
[radio links, OFDM, STBC, Space Time Block Code, 64 QAM, quadrature amplitude modulation, turbo codes, QPSK, SISO, phase shift keying, turbo coded OFDM wireless link, OFDM modulation, MIMO, MISO, turbo encoder, MIMO communication, SIMO, MIMO-OFDM system, 16 QAM, turbo coded wireless link, Rayleigh channels, MISO-OFDM system, SIMO-OFDM system, SISO-OFDM system, Turbo Code, Rayleigh fading, Artificial intelligence, Antennas]
Power control algorithm for cognitive radio systems
14th International Conference on Computer and Information Technology
None
2011
Cognitive radio (CR) is the enabling technology for dynamic spectrum access, which may potentially mitigate the radio spectrum scarcity problem encountered in many countries. This paper is concerned with power allocation spectrum sharing CR networks, where a secondary user (SU) communicates simultaneously over the same frequency band with an existing primary users (PUs) link. In addition, this method uses PU feedback information. It is assumed that the SU transmitter has the perfect channel state information (CSI) on the fading channels from SU transmitter to both PU and SU receivers, as well as the fading channel from PU transmitter to PU receiver. Furthermore, the proposed method takes into account the statistical variation of the signal to interference plus noise ratio (SINR) of each transmitter/receiver pair and optimally allocates power to minimize probability of fading-induced outage probability. Such approaches can guarantee convergence towards the optimal solution with an acceptable convergence speed. Therefore, equipment designers and system operators can use their judgment to find the best SU pairs to meet their specific needs. Simulation results are provided for the optimal power control performance.
[power allocation spectrum sharing, optimal power control, fading induced outage probability, telecommunication control, dynamic spectrum access, Receivers, channel state information, optimal control, Frequency estimation, convergence speed, Cognitive radio, secondary user, Transmitters, cognitive radio, outage probability, cognitive radio systems, primary user, power control, channel allocation, SINR, power control algorithm, Message systems, frequency band]
Evaluate the performance of FSO communication link with different modulation technique under turbulent condition
14th International Conference on Computer and Information Technology
None
2011
Free Space Optical (FSO) Communications are able to deliver us to an age of unprecedented bandwidth, low signal attenuation, small space requirements and ultimately low cost. In FSO communications the influence of atmospheric effects can be specified by the attenuation and fluctuations of the transmitted optical power, caused by the atmospheric turbulence. This paper investigate the performance of FSO communication systems employing on-off keying (OOK), subcarrier binary phase-shift keying (BPSK) modulation and Q-ary pulse position modulation (QPPM) in turbulence regime. The performance results are evaluated in terms of bit error rate (BER) employing subcarrier OOK, BPSK, and QPPM as modulation technique. It is found that the BER performance under BPSK modulation is better compared to other modulation techniques but in Q-ary PPM if we increase the order of Q then the performance will improve and it provide maximum 4dB improvement.
[Freespace optics (FSO), free space optical communication system, gamma-gamma distribution, Optics, Binary phase-shift keying (BPSK), light scattering, BPSK modulation technique, phase shift keying, optical communication, error statistics, optical links, Q-ary pulse position modulation technique, subcarrier OOK, FSO communication link, Optimized production technology, subcarrier binary phase shift keying modulation technique, on-off keying (OOK), unprecedented bandwidth, signal attenuation, Optical receivers, turbulent condition, Optical transmitters, amplitude shift keying, atmospheric turbulence, bit error rate, Q-ary pulse position modulation (QPPM), subcarrier multiplexing, transmitted optical power, on-off keying, BER performance, QPPM, pulse position modulation, Demodulation]
SDA-2H: Understanding the value of background cover against statistical disclosure
14th International Conference on Computer and Information Technology
None
2011
The statistical disclosure attack (SDA) is an effective method for compromising the anonymity of users in a mix based system. Cover traffic, in the form of fake or dummy messages sent by other users of the mix, is an effective defense to make the task of the attacker difficult. Our aim is to examine the effect that background cover - the cover traffic sent by other users - has on the effectiveness of statistical disclosure attacks. Since the original SDA does not explicitly account for background traffic volumes, we developed an extension to the SDA called SDA-2H that uses this information to improve upon the SDA. Based on this attack, we are able to quantify the importance of background cover traffic, which we show in simulation to be effective in many scenarios.
[background cover, dummy message, cover traffic, Computational modeling, Receivers, Dynamic range, Vectors, SDA-2H method, Equations, Delay, fake message, user anonymity, online privacy, mix based system, data privacy, statistical disclosure attack, Mathematical model, anonymous communications]
Self-healing by means of runtime execution profiling
14th International Conference on Computer and Information Technology
None
2011
A self-healing application brings itself into a stable state after a failure put the software into an unstable state. For such self-healing software application, finding fix for a previously unseen fault is a grand challenge. Asking the user to provide fixes for every fault is bad for productivity, especially when the users are non-savvy in technical aspect of computing. If failure scenarios come into existence, the user wants the runtime environment to handle those situations autonomically. This paper presents a new technique of finding self-healing actions by matching a fault scenario to already established fault models. By profiling and capturing runtime parameters and execution pathways, stable execution models are established and later are used to match with an unstable execution scenario. Experimentation and results are presented that showed that even with additional overheads; this technique can prove beneficial for autonomically healing faults and reliving system administrators from mundane troubleshooting situations.
[runtime execution profiling, execution pathway, fault scenario, software fault tolerance, Autonomic computing, Code transformation, failure scenario, self-healing software application, productivity, Self-adaptive application, Fault similarity, runtime environment, stable execution model, runtime parameter]
A new technique for high speed decimal logarithm computation of decimal floating-point number
14th International Conference on Computer and Information Technology
None
2011
This paper presents a new design and a fast technique for implementation of a 32-bit decimal floating-point (DFP) logarithmic computation to efficiently calculate radix-10 logarithm of a decimal number. Conventional techniques first convert decimal inputs to binary, then perform base-2 logarithm operations, and finally results are converted back to decimal radix. It sometimes causes errors due to the back and forth conversions of the bases. The technique described in this paper uses a 32-bit floating-point arithmetic, and utilizes only addition and subtraction operations. It does not require any decimal to binary conversion, or division operation. As a result, the described algorithm offers a low-cost, hardware-efficient and lower power consumption method for computing decimal floating-point numbers.
[Algorithm design and analysis, radix-10 floating-point arithmetic, VLSI, decimal floating-point logarithmic computation, FPGA, subtraction operations, addition operations, floating point arithmetic, power consumption, decimal floating-point number, Convergence, decimal radix, base-2 logarithm operations, high speed decimal logarithm computation, Decimal logarithm, floating-point arithmetic, Hardware, radix-10 logarithm calculation, power consumption method]
An efficient YCgCo-based image compression algorithm for capsule endoscopy
14th International Conference on Computer and Information Technology
None
2011
An efficient YCgCo color space based image compression algorithm for the capsule endoscopy application is presented in this paper. The algorithm is developed taking the advantage of the special characteristics of the endoscopic images and the energy compaction capabilities of the YCgCo color plane. The compression scheme provides high reconstruction image quality (PSNR of over 52dB) and competitive compression ratio (over 80%). The algorithm uses integer-based forward transform followed by a simple implementation-friendly quantization stage that requires less hardware cost, and thus is suitable for power limited wireless capsule endoscope systems.
[high reconstruction image quality, hardware cost, implementation-friendly quantization stage, Image coding, power limited wireless capsule endoscope systems, energy compaction capabilities, Quantization Table, Hardware, Colon, image colour analysis, Discrete cosine transforms, endoscopes, competitive compression ratio, Integer DCT, medical image processing, Biomedical imaging, data compression, discrete cosine transforms, competitive algorithms, Capsule Endoscopy, integer-based forward transform, Encoding, image reconstruction, YCgCo color space-based image compression algorithm, capsule endoscopy, Image segmentation, DTC, Image Compression, YCgCo color plane, image coding]
Dual band two elements inverted-L antenna for 2.3 GHz mobile WiMAX and 5.5 GHz Wi-Fi operations
14th International Conference on Computer and Information Technology
None
2011
A high gain two elements inverted-L antenna (ILA) capable of generating two bands for 2.3 GHz mobile worldwide interoperability for microwave access (mobile WiMAX) and 5.5 GHz wireless-fidelity (Wi-Fi) operation is presented in this research. Geometry parameters of the antenna are optimized for 50 &#x03A9; antenna input impedance using method of moments (MoM's) in super-numerical electromagnetic code (super-NEC). The antenna occupies a small area of 60&#x00D7;80 mm2 on a portable circuit board (PCB) and has 10-dB return loss bandwidth (BW) of 250 MHz (2180-2430 MHz) for 2.3 GHz (mobile WiMAX) band and 310 MHz (5320-5630 MHz) for 5.5 GHz (Wi-Fi) band making it easily cover the required bandwidths for mentioned applications. The simulated and measured return loss, radiation patterns, voltage standing wave ratio (VSWR) of the antenna shows good agreement and simulated radiation efficiency indicates the suitability of the antenna for the mobile WiMAX and Wi-Fi operation. The peak antenna gains of 4.75 dBi at 2.3 GHz and 8.65 dBi at 5.5 GHz are measured.
[portable circuit board, Wireless LAN, Mobile WiMAX, Mobile communication, MoM, dual band two elements inverted-L antenna, super-NEC, mobile worldwide interoperability, frequency 5.5 GHz, Wireless Local Area Network (WLAN), high gain two elements, antenna radiation patterns, microwave access, BW, wireless-fidelity operation, method of moments, bandwidth 250 MHz, frequency 2.3 GHz, loss 10 dB, return loss bandwidth, voltage standing wave ratio, IEEE 802.11 Standards, microwave antennas, Wi-Fi, WiMAX, WiMax, PCB, VSWR, simulated radiation efficiency, mobile WiMAX, geometry parameters, bandwidth 310 MHz, bandwidth 5320 MHz to 5630 MHz, Wi-Fi operations, super-numerical electromagnetic code, Feeds, wireless LAN, bandwidth 2180 MHz to 2430 MHz, radiation patterns, ILA]
Ultra-wideband LLI-slit slotted antenna for WLAN, Bluetooth and Wi-Fi operations
14th International Conference on Computer and Information Technology
None
2011
This paper presents compact ultra-wideband (UWB) LLI-slit slotted antenna for 5.2 GHz WLAN, 5.5 GHz Wi-Fi, and 5.8 GHz WLAN and Bluetooth operations. The proposed antenna provide a wide operating bandwidth of 1.9 GHz (4.85 - 6.75 GHz), thus allowing it to easily cover the required bandwidths for WLAN, Wi-Fi and Bluetooth operation in the 5.15-5.35/5.47-5.725/5.725-5.875-GHz bands respectively. The dimension of proposed antenna has compact size of 15 mm &#x00D7; 12mm &#x00D7; 7 mm. The maximum antenna gain is 5.25 dBi. The simulated return loss, gain, VSWR and radiation patterns indicate the suitability of this antenna for the WLAN, Bluetooth and Wi-Fi applications.
[Wireless LAN, Bluetooth, ultra wideband antennas, Double-L Slit Antenna, Microstrip, frequency 5.5 GHz, bandwidth 5.15 GHz to 5.35 GHz, bandwidth 5.725 GHz to 5.875 GHz, Wireless Local Area Network (WLAN), Wireless-Fidelity (Wi-Fi), frequency 5.8 GHz, LLI-slit slotted antenna, ultra wideband, Wi-Fi, size 15 mm, WiMAX, bandwidth 1.9 GHz, WLAN, size 7 mm, slot antennas, UWB antennas, size 12 mm, frequency 5.2 GHz, wireless LAN, Antennas, Wideband, bandwidth 5.47 GHz to 5.725 GHz]
Eclectic extraction of propositional rules from Neural Networks
14th International Conference on Computer and Information Technology
None
2011
Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a &#x201C;Black Box&#x201D; learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. They have been broadly categorized into three types based on their approach to use internal model of the Network. Eclectic Methods are hybrid algorithms that combine the other approaches to attain more performance. In this paper, we present an Eclectic method called HERETIC. Our algorithm uses Inductive Decision Tree learning combined with information of the neural network structure for extracting logical rules. Experiments and theoretical analysis show HERETIC to be better in terms of speed and performance.
[multilayer perceptrons, eclectic extraction, Irrigation, artificial neural network, logical rule extraction, supervised learning, Artificial neural networks, medical diagnosis, Neural Networks, inductive decision tree learning, Eclectic Method, hierarchical and eclectic rule extraction via tree induction and combination, rule extraction method, HERETIC eclectic method, feedforward neural network, decision trees, propositional rule extraction, black box learning, Decision Trees, learning (artificial intelligence), Rule Extraction, feedforward neural nets]
Computational model for calculating the integrated environmental contamination of nuclear hazard
14th International Conference on Computer and Information Technology
None
2011
In this research work we have investigated the pathways of nuclear radiation transferring to human via environment to establish a computational model to estimate its contaminant amount calculating the effective doses on environment as well as on human. For this, first we have independently considered three pathways of exposure- Airborne releases of radioactive air, Aqueous releases of radioactive water into watercourses and Aqueous releases of radioactive water into sewage. Finally, integrated them to estimate the overall hazard on environment. We have find out 14 ways to affect human. The governing equations are distribution functions. Classical Semi-infinite Cloud Model has used to solve the functions of external dose from radioactive plume. Monte-Carlo Method has applied only to solve the function of Gamma dose kernel integral over the plume. Here, we gave priority to make the air dispersion factor correct. The sample results has simulated for 1 year time span for both short term release and long term release of a normal operational Accelerator Facilities. Numbers of trials have been run to assume the approximate value to estimate the accurate results.
[Atomic measurements, radioactive pollution, dosimetry, nuclear radiation, effective doses, Monte-Carlo method, classical semiinfinite cloud model, statistical model, water pollution measurement, Gamma dose kernel integral, radioactive water, effective dose, contaminant amount, accelerator facilities, Monte Carlo methods, nuclear hazard, aqueous releases, radioactive air, nuclear facilities, geophysical techniques, Blogs, air pollution measurement, radioactive plume, Rivers, stability class, Dispersion factor, computational model, exposure-Airborne pathways, integrated environmental contamination, air dispersion factor]
Optimization of error performance in a WiMAX transceiver using novel Adaptive Cyclic Prefix strategy
14th International Conference on Computer and Information Technology
None
2011
In this paper, a complete novel and unique concept of Adaptive Cyclic Prefix (ACP) is proposed for IEEE 802.16e-WiMAX Physical Layer (PHY) using a Simulink-VSA based simulation model. Implementation of ACP algorithm provides a better QoS in the form of lowest average error for low channel SNR condition, compared to the existing Fixed Cyclic Prefix (FCP) in WiMAX. MATLAB plays the crucial part for implementing ACP, while the WiMAX PHY Layer scenario is created within Simulink workspace. By measuring Error Vector Magnitude (EVM) and Relative Constellation Error (RCE) in Vector Signal Analyser (VSA), a significant improvement is obtained in average error performance with respect to varying Channel SNR for ACP compared to FCP. Simulation results also show that ACP is efficient enough to be used for each of the modulation and coding specified for a WiMAX system, making it an easy alternative to the standard Adaptive Modulation and Coding (AMC). Finally exhaustive simulation results have been included in support to the success of the algorithm.
[Algorithm design and analysis, adaptive modulation, OFDM, error vector magnitude, fixed cyclic prefix, IEEE 802.16e-WiMAX physical layer, MATLAB, Simulink-VSA based simulation model, error performance optimization, WiMAX system, adaptive modulation and coding, QoS, AMC, PHY, ACP algorithm, vector signal analyser, VSA, radio transceivers, adaptive cyclic prefix strategy, WiMAX, WiMax, WiMAX transceiver, relative constellation error, quality of service, adaptive codes, RCE, IEEE 802.16 Standards, Optical sensors, Adaptive Cyclic Prefix, EVM]
Topology partitioning in wireless sensor networks using multiple sinks
14th International Conference on Computer and Information Technology
None
2011
Energy consumption is the key design criterion for routing data in wireless sensor Network. Recently the use of multiple sinks instead of single is considered as a research topic for energy saving. The objective is to maximize the network lifetime by reducing distance from source node to sink during data transmission. This paper proposes a network partitioning algorithm based on selection of farthest node from k-Nearest Neighbour Graph. After partitioning, a restricted flooding-based routing protocol is used for data transmission. It is shown that the network lifetime significantly increases in a partitioned network in comparison with a non-partitioned network.
[wireless sensor networks, network topology partitioning algorithm, graph theory, multiple sink, k-nearest neighbour graph, Artificial neural networks, wireless sensor network, telecommunication network topology, Routing, network lifetime maximization, restricted flooding-based routing protocol, energy saving, Wireless sensor networks, routing data, Wireless Sensor Network, routing protocols, telecommunication network reliability, data transmission, k-NNG, energy consumption, Multiple sinks, Partitioning network]
Real-time video chatting in low bandwidth by Facial Action Coding System
14th International Conference on Computer and Information Technology
None
2011
The goal of this project is to design a user interface for the videophone system to enable real-time video chat. The system includes voice and 3D talking heads and able to communicate using very low bandwidth because no video data are transmitted through network. Instead of video, facial expressions of speakers are transmitted along with voice in the real time. At the other end, talking head will show those facial expressions and will render the voice. The facial structure of the talking head will resemble the speaker's face.
[videotelephony, Encoding, video coding, FACS, Synchronization, AAM, Mouth, real-time video chatting, Haar Classifier, FAUS, low bandwidth, video communication, facial action coding system, videophone system]
Minimizing time overhead to detect soft errors through preceding variable analysis
14th International Conference on Computer and Information Technology
None
2011
Soft error is a significant reliability concern for nanometer technologies. Shrinking feature sizes, lower voltage levels, reduced noise margins, and increased clock frequency improves the performance and lowers the power consumption of integrated circuit. But it causes the integrated circuit more susceptible to soft error that can corrupt data and make systems vulnerable. In computer systems, where the reliability is a great concern, the impact of soft errors may be very catastrophic. This paper proposes a new approach to detect soft errors through variable dependency analysis. The proposed method has lesser time overhead in comparison to existing dominant approach.
[Computers, Instruction sets, integrated circuit reliability, variable dependency analysis, Variable dependency, nanometer technologies, Preceding variable, power consumption, time overhead, Critical variable, Soft error, computer systems, variable analysis, nanoelectronics, integrated circuit, Reliability, soft errors, reliability concern, computers]
Analysis of an Impulse Radio Ultra Wideband over fiber scheme based on photonic pulse generation technique
14th International Conference on Computer and Information Technology
None
2011
A method to implement a radio over fiber system using optically generated Impulse Radio Ultra Wideband signal has been proposed and investigated. In this technique, a continuous lightwave from a laser diode is phase modulated by a pulse pattern generator and sent to a Fiber Bragg Grating which is used as a frequency discriminator. Depending on the location of the optical carrier at the linear or quadrature slope of reflection spectrum of FBG, a Gaussian monocycle or doublet would be generated. This optical signal is then transmitted through a SMF link to the remote base stations where photodetectors convert the signal and remote antenna units transmit the IR-UWB signals. Performance of the system is evaluated through simulation and different criteria are analyzed with the simulation result.
[radio-over-fibre, laser diode, quadrature slope, SMF link, ultra wideband communication, optically generated impulse radio ultrawideband signal, reflection spectrum, Frequency conversion, optical signal, Delay, continuous lightwave, photodetectors, photonic pulse generation technique, remote antenna units, frequency discriminator, remote base stations, signal antenna units, IR-UWB, Optical reflection, optical links, pulse pattern generator, Optical switches, Radio over Fiber, Fiber Bragg Grating, Gaussian monocycle, optical carrier, Optical pulses, Bragg gratings, Stimulated emission, impulse radio ultrawideband over fiber scheme, optical fibre networks, linear slope, semiconductor lasers, fiber Bragg grating, Laser applications, On Off Keying, IR-UWB signals]
Selection of computationally efficient mutation strategy of differential evolution algorithm for the design of multiplier-less low-pass FIR filter
14th International Conference on Computer and Information Technology
None
2011
Reduction of structural complexity of digital systems has been emerging as one of the major areas of concern to the system designers. The deployment of any physical circuit or system in any specific application is solely determined by the intricacy of the concerned circuit which restricts the use of sophisticated circuits in high speed operation. Thus the design of computationally efficient structure has drawn special attention to the modern researchers. Due to the immense development of evolutionary computation techniques over the last few years, they are being increasingly used for a number of signal processing applications. This paper deals with one such robust technique called Differential Evolution (DE) and the impact of its different mutation strategies for efficient design of multiplier-less low-pass Finite duration Impulse Response (FIR) filter. In order to find out the most favourable mutation scheme, the computational efficiency of various mutation schemes has been studied. For this purpose, the convergence behaviour and error histogram of DE algorithm has been presented. From the simulation results, the name of the computationally most efficient mutation strategy has been suggested. Finally, the superiority of the established mutation strategy has been reconfirmed in terms of the filter performance.
[Algorithm design and analysis, signal processing, Finite duration Impulse Response (FIR) filter, Error histogram, signal processing application, Differential Evolution (DE), computationally efficient mutation strategy, Convergence speed, evolutionary computation, FIR filters, differential evolution algorithm, Sum of power of two (SPT), multiplier less low pass FIR filter, evolutionary computation techniques, finite duration impulse response filter]
An efficient memory block selection strategy to improve the performance of cache memory subsystem
14th International Conference on Computer and Information Technology
None
2011
Although cache improves performance by reducing the speed-gap between the CPU and main memory, cache increases the timing unpredictability due to its dynamic nature. Cache also requires significant amount of power to be operated. Unpredictability and power consumption become even worse in multicore systems due the presence of multiple levels of caches. Recent studies indicate that predictability can be increased and total power consumption can be decreased without compromising performance by locking appropriate memory blocks. The success of cache locking depends on the accurate selection of blocks to be locked. In this work, we propose an easy but efficient memory block selection strategy to enhance cache locking and cache replacement enactment and overall cache memory subsystem performance. Proposed scheme determines the blocks that produce more cache misses if not locked and stores the block address and miss information (BAMI) at cache level. Cache locking technique should lock memory blocks with higher cache misses and cache replacement policy should select victim blocks with lower cache misses using BAMI. We simulate single-core and multi-core systems, both with two-level cache memory subsystem, to evaluate the proposed block selection scheme. Experimental results show that the predictability can be improved by increasing hit ratio up to 11% and total power consumption can be decreased up to 20% by using our memory block selection scheme.
[Real time systems, Power demand, multiprocessing systems, Multicore processing, memory block selection, Cache memory, Discrete Fourier transforms, cache memory subsystem performance, BAMI, single core system, total power consumption, cache storage, dynamic nature, cache replacement, memory block selection strategy, Delay, power consumption, cache locking, multicore systems, power aware computing, cache replacement policy, block address and miss information, Memory management, cache memory subsystem]
Code construction and performance evaluation of Space Time Trellis Code (STTC) over Rayleigh fading channel
14th International Conference on Computer and Information Technology
None
2011
Space-Time Trellis Codes (STTCs) combine modulation and trellis coding to transmit information over multiple transmit antennas and MIMO channels. In this paper, code construction and performance analysis of different STTCs for various modulation schemes (e.g. BPSK, QPSK) over Rayleigh flat fading channel are presented. Improved form of STTCs using generator matrix and delay diversity is also described. Comparison of the code performance of STTCs with Orthogonal Space-Time Block Codes (OSTBCs) and Quasi-Orthogonal Space-Time Block Codes (QOSTBCs) for two transmit antennas and one or more receive antennas are shown. The result shows that STTCs provide high coding gain and outperform OSTBCs and QOSTBCs. It is possible to improve the performance of STTCs by increasing the number of states of the trellis.
[Rayleigh fading channel, delay diversity, transmitting antennas, MIMO channels, Rayleigh channels, performance evaluation, Generators, code construction, Binary phase shift keying, trellis codes, OSTBC, receiving antennas, space-time block codes, STTC, QOSTBC, quasi-orthogonal space-time block codes, space time trellis code, Signal to noise ratio, generator matrix]
A new approach for LSB based image steganography using secret key
14th International Conference on Computer and Information Technology
None
2011
This paper introduces a best approach for Least Significant Bit (LSB) based on image steganography that enhances the existing LSB substitution techniques to improve the security level of hidden information. It is a new approach to substitute LSB of RGB true color image. The new security conception hides secret information within the LSB of image where a secret key encrypts the hidden information to protect it from unauthorized users. In general, in LSB methods, hidden information is stored into a specific position of LSB of image. For this reason, knowing the retrieval methods, anyone can extract the hidden information. In our paper, hidden information is stored into different position of LSB of image depending on the secret key. As a result, it is difficult to extract the hidden information knowing the retrieval methods. We have used the Peak Signal-to-Noise Ratio (PSNR) to measure the quality of the stego images. The value of PSNR gives better result because our proposed method changes very small number of bits of the image. The obtained results show that the proposed method results in LSB based image steganography using secret key which provides good security issue and PSNR value than general LSB based image steganography methods.
[hidden information extraction, stego image quality, cryptography, LSB based image steganography, retrieval methods, secret key, hidden information encryption, peak signal-to-noise ratio, stego-image, hidden information security level, cover-image, LSB substitution techniques, least significant bit, steganography, RGB true color image, LSB, image colour analysis, PSNR value]
An enhanced scheme for lossless compression of short text for resource constrained devices
14th International Conference on Computer and Information Technology
None
2011
Text compression is an elementary concern for data engineering and management. The rapid use of battery powered small memory smart devices especially; mobile phones and wireless sensors for communication and monitoring have turned short text compression into a more important and prevailing research arena than large scale text compression. In this paper, we present an effective approach of short English text compression for smart devices. This paper also provides a review on recent research on text data compression. The main target of this research is to provide a light-weight compression scheme which is computationally simple and the storage required for compressing the source text is also lower. The main contribution of this paper is the integration of statistical text ranking or statistical component categorization with static coding in obtaining the compression. We have also presented a mathematical model of our proposed scheme in terms of compression parameters. The obtained compression ratio indicates a better performance in terms of resource consumption including better compression ratio, lower compression and decompression time with reduced memory requirements and lower complexity. This paper also incorporates an extensive analysis on power consumption for presented scheme. In overall analysis, the simplicity of computational requirement encompasses the compression effective and efficient.
[text analysis, data management, resource constrained device, lower compression, lightweight compression scheme, power consumption, Data Compression, Argon, Text Ranking, data compression, compression parameter, reduced memory requirement, lossless compression, Data Management, Probability, Smart Device, short English text compression, data engineering, text data compression, static coding, compression ratio, statistical component categorization, decompression time, mobile phone, statistical text ranking, wireless sensor, statistical analysis, battery powered small memory, Type-to-Token Ratio (TTR)]
Road vehicle rollover avoidance using active steering controller
14th International Conference on Computer and Information Technology
None
2011
Aim of this paper is to develop yaw and roll vehicle model and active steering control preventing rollover. Selecting the center of gravity of the vehicle's sprung mass; an active control which is a function of certain parameters is designed. Stability augmentation control, emergency steering control and emergency braking control are three feedback loops. An auxiliary steering angle from stability augmentation control and emergency steering control is set by an actuator to the steering angle. Stability augmentation control is based on feedback of the roll rate and roll acceleration. Emergency steering control and emergency braking control are based on feedback of the rollover coefficient. The sensitivity function at different points and simulation represent the evaluation of the control concept. Absolute stability of the steering control concept is analyzed to illustrate the effective design technique of the rollover avoidance control.
[rollover coefficient, steering systems, active steering control, feedback, road vehicle rollover avoidance, actuators, roll-yaw coupled model, road vehicles, stability augmentation control, emergency steering control, active steering controller, braking, emergency braking control, stability, sensitivity function, feedback loops, roll rate, vehicle dynamics, vehicle center-of-gravity, emergency control, Equations, yaw-and-roll vehicle model, stability augmentation, rollover avoidance, roll acceleration, sprung mass, auxiliary steering angle]
An effective level-1 cache locking strategy for energy-efficient real-time multicore systems
14th International Conference on Computer and Information Technology
None
2011
Multicore architectures with multilevel caches are being used in both desktop and embedded processors for their improved performance. Caches increase execution time unpredictability and make it difficult to support real-time applications. Caches also challenge the power supply system by consuming a lot of power. Studies show that cache locking improves predictability and performance/power ratio for single-core systems. Recent studies also show that way cache locking can be applied in multicore systems. In this work, we propose a simple but effective level-1 way cache locking scheme for multicore systems. This scheme is based on the analysis of applications' worst case execution time (WCET) and it allows changing the locked cache size during runtime to achieve the optimal predictability and performance/power ratio for the running application. Using Heptane WCET analyzer, we study MPEG4, H.264/AVC, FFT, MI, and DFT codes and generate workloads. Workloads provide miss information for the memory blocks (without cache locking). Using VisualSim tool, we model and simulate a system with four cores and two levels of caches. We also simulate a random cache locking strategy. Experimental results show that our cache locking scheme significantly improves predictability by decreasing total misses more than 50%. Experimental results also show that our proposed cache locking strategy outperforms the random strategy by up to 22%.
[Real time systems, energy efficient real-time multicore systems, cache storage, worst case execution time, energy-effcient multicore system, multilevel caches, Delay, MPEG 4 Standard, power consumption, power supply system, execution time predictability, Program processors, power aware computing, WCET, DFT, FFT, single core systems, MI, desktop processors, Power demand, multiprocessing systems, Multicore processing, Discrete Fourier transforms, H.264/AVC, MPEG4, memory blocks, cache locking, Cache locking, embedded processors, multicore systems, VisualSim tool, effective level-1 cache locking strategy, real-time application, multicore architectures, real-time systems]
Cost based performance evaluation of M/G/1/K traffic model
14th International Conference on Computer and Information Technology
None
2011
In this paper, we derived analytical expression of traffic parameters of M/G/1/K queuing system to observe performance of a router with finite buffer length in context of cost analysis. Four different types of service time distributions (Exponential, K-Erlangian, Hyper Exponential and Long tailed distribution) are considered to evaluate mean queue length and probability of overflow of buffer with change in offered traffic intensity. Finally a cost function is evaluated considering cost of loosing calls, cost of customer waiting in queue, cost of service of processor and cost of buffer to observe relative cost of four different cases of service time distribution of M/G/1/K queuing system.
[traffic parameters, queueing theory, M/G/1/K traffic model, service time, M/G/l/K queuing system, finite buffer length, costing, performance evaluation, call blocking rate, cost based performance evaluation, carried traffic and probability states, M/G/1/K, router, telecommunication network routing, service time distributions, Poisson's process, telecommunication traffic]
High precision indoor positioning using lighting LED and image sensor
14th International Conference on Computer and Information Technology
None
2011
High power white LEDs are expected to replace the existing lighting technologies in near future which are also suggested for visible light communication (VLC). We propose an algorithm for high precision indoor positioning using lighting LEDs, VLC and image sensors. In the proposed algorithm, at least three LEDs transmit their three dimensional coordinate information which are received and demodulated by two image sensors at the unknown position. The unknown position is then calculated from the geometrical relations of the LED images created on the image sensor. This technique does not require any angular measurement. Results from the numerical simulations are explained to understand the effect of different system parameters on positioning accuracy. Based on numerical analyses, we show that the system is able to estimate the unknown position within the accuracy of about 10 centimeters. Positioning accuracy can be increased by using high resolution image sensors.
[Switches, lighting LED, image sensor, Indoor Positioning, visible light communication, light emitting diodes, image sensors, Atmospheric measurements, Image Sensor, Particle measurements, Three dimensional displays, Visible Light Communication, Sensors, indoor radio, optical communication, high precision indoor positioning, Arrays, high power white LED, position measurement, positioning accuracy, Lenses]
An adaptive channel estimation technique for OFDM based cognitive radio systems
14th International Conference on Computer and Information Technology
None
2011
The explosive growth of wireless services in recent years leads to a spectrum scarcity. Cognitive radio (CR) is a promising technology to resolve the inconsistency between spectrum scarcity and spectrum underutilization. Orthogonal frequency division multiplexing (OFDM) provides an efficient and flexible transmission scheme in physical layer for CR. It is convenient for cognitive devices to close tones corresponding to spectrum map and prevent the license users (LUs) from interference. In this paper, we proposed an adaptive channel estimation technique for OFDM in the context of CR systems. We develop a control fee variable step size (VSS)-least mean square (LMS) CE algorithm that can adapts the optimum weighting coefficient to the channel condition. This time-varying step size method is updated at each iteration to minimize the sum of the squares of the prior estimation errors up to the current time point. This method use negative exponential error function to decrease the estimation error in CE procedure. Such approaches can guarantee the convergence towards the true channel coefficient.
[least mean squares methods, orthogonal frequency division multiplexing, OFDM, negative exponential error function, Mobile communication, OFDM based cognitive radio systems, Wireless communication, cognitive devices, cognitive radio, spectrum underutilization, Channel estimation, OFDM modulation, time-varying step size method, spectrum map, license users, Context, spectrum scarcity, Wireless sensor networks, channel estimation, optimum weighting coefficient, variable step size-least mean square CE algorithm, spectrum sensing, Explosives, wireless services, interference, Mobile computing, adaptive channel estimation]
A Web-based land management system for Bangladesh
14th International Conference on Computer and Information Technology
None
2011
The emergence of digitized land system has been carried out for years. This sector has been spotted as the leading factor in the corruption cases of Bangladesh. In this paper we have designed a framework that will add both transparency and efficiency to the current land management system. This paper presents a new modeling technique that represents data for the current land management system through a user-friendly and digitized map based system. We are proposing to scan the current paper based maps and convert the images into scalable vector graphics (svg) format into database. It allows nice visual presentation of maps and easy searching facility. User friendliness, data transparency and cost efficiency were the prime focus in designing the framework for the new Web-based Land Management System.
[Terminology, Land Information Management System, Web-based land management system, paper based map, Geospatial analysis, cartography, data transparency, database management systems, land use planning, computer graphics, Bangladesh, database, user friendliness, digitized land system, Automated Web System, E-governance, digitized map based system, svg format, Internet, scalable vector graphics, cost efficiency, map visual presentation]
Orthogonal point-set embeddings of 3-connected and 4-connected planar graphs
14th International Conference on Computer and Information Technology
None
2011
An orthogonal point-set embedding of a planar graph G on a set S of points in Euclidean plane is a drawing of G where each vertex of G is placed on a point of S, each edge is drawn as a sequence of alternate horizontal and vertical line segments and any two edges do not cross except at their common end. In this paper, we devise an algorithm for orthogonal point-set embedding of 3-connected cubic planar graphs having a hamiltonian cycle with at most (5n over 2 + 2) bends, where n is the number of vertices in G. We also give an algorithm for finding an orthogonal point-set embedding of 4-connected planar graphs with at most 6n bends. Both the algorithms run in linear time. To the best of our knowledge this is the first work on orthogonal point-set embeddings with fewer bends.
[hamiltonian cycle, Fewer bends, graph theory, Orthogonal drawing, computational geometry, Euclidean plane, 3-connected planar graphs, Algorithm, 3 connected cubic planar graphs, 4-connected planar graphs, horizontal line segments, Point-set embedding, USA Councils, orthogonal point set embeddings, vertical line segments]
Neural network and regression based processor load prediction for efficient scaling of Grid and Cloud resources
14th International Conference on Computer and Information Technology
None
2011
An effective and efficient resource allocation policy could benefit the cloud environment by saving cost. To support the continuous load increase, the cloud platform needs to create new virtual machines. However, substantial amount of time is required for the creation and the setup of a virtual machine. Therefore, allocating resources in advance based on prediction models could improve the quality of the service of the cloud platform. In this paper we present time delay neural network and regression methods for predicting future workload in the Grid or Cloud platform. We use real world workload traces to test the performance of our algorithms. We also present an overall evaluation of this approach and its potential benefits for enabling efficient auto-scaling of Cloud user resources.
[cost savings, time delay neural network, resource scaling, grid computing, regression analysis, neural network, Spline, Grid Computing, resource allocation, Cloud Computing, cloud environment, cloud resource, Argon, cloud computing, cloud user resource, grid resource, prediction model, quality of service, Sun, resource allocation policy, virtual machines, regression based processor load prediction, Nickel, workload prediction, Integrated circuit modeling, neural nets, polynomial regression]
A very low power and high throughput AES processor
14th International Conference on Computer and Information Technology
None
2011
We present the design of a very low power and high throughput Advanced Encryption Standard (AES) processor. A sophisticated AES algorithm without sacrificing its security features, throughput and area is used to design the processor. Due to the optimization of the algorithm and a number of design considerations, the processor shows its superiority over other AES processors. The proposed processor is simulated on the FPGA platform and Quartus II development software of Altera device of family Stratix II GX is used to simulate the design. A Power Play Early Power Estimation Tool is used to approximate the power consumption of the proposed processor. Later on the more reliable power analysis tool named Power Play Power Analyzer is used to estimate the static and dynamic power dissipation in the Processor. The high level of system integration along with very low power consumption and high throughput makes the AES processor an ideal choice for a range of application including small computing devices, smart card readers and network applications like WLAN, WPAN, WSN etc.
[processor design, Quartus II development software, AES Algorithm, field programmable gate arrays, advanced encryption standard, FPGA, very low power AES processor, wireless sensor network, Throughput, power play power analyzer tool, wireless personal area network, power consumption, Program processors, power aware computing, AES algorithm, field programmable gate array, high throughput AES processor, Low Power, FPGA platform, smart card reader, Hardware, CMOS integrated circuits, Cryptography, PPP Analyzer Tool, Altera device, dynamic power dissipation, small computing device, static power dissipation, cryptography, WLAN, Synchronization, WPAN, wireless local area network, WSN, power play early power estimation tool, Reliability, Clocks]
Modeling the head related transfer function for sound localization in normal hearing persons and bilateral cochlear implant recipients
14th International Conference on Computer and Information Technology
None
2011
Mathematical models can be very useful for understanding complicated systems, and for testing algorithms through simulations that would otherwise be difficult or expensive to implement. A model has been devised that simulates the sound localization performance of normal hearing persons, and that is being further refined for simulation of performance of hearing-impaired persons wearing two cochlear implants (CIs). The model is intended to be a tool in understanding the relative contribution of various factors involved in acoustic localization, and in developing new signal processing algorithms for neural encoding strategies. This presentation overviews the development of and results for the normal hearing model, and discusses modifications that are needed to simulate sound processing with binaural cochlear implants. The human head related transfer function (HRTF) is a critical component of the model, and provides the characteristics of head shadow, torso and pinna effects. It defines the temporal, intensity and spectral cues reaching each ear that are important for good localization ability. The model has been validated against published literature on HRTFs and localization in normal listeners, and is being further developed to account for differences in the signal pathway and sound reaching the CI user due to sound processing and microphone location effects.
[Computational modeling, mathematical models, biomedical ultrasonics, normal hearing persons, signal processing algorithms, acoustic localization, Binaural Processing, bilateral cochlear implant recipients, medical signal processing, sound localization performance, Head Related Transfer Function, Auditory system, Implants, Interaural Sound Differences, head related transfer function modeling, hearing, hearing-impaired persons, Cochlear Implants, neurophysiology, cochlear implants, neural encoding strategies, neural nets, ultrasonic imaging, Acoustic Localization]
Performance evaluation of different apodization profiles of linearly chirped FBG for dispersion compensation
14th International Conference on Computer and Information Technology
None
2011
Fiber Bragg gratings (FBGs) are widely used to compensate dispersion in densely high bit rate optical communication system. Apodization of the FBG mainly minimizes the effect of spurious side spectrum and apodization function plays an important role in controlling the behaviour of mean dispersion, reflection spectrum, group delay etc. In this paper, we have investigated and compared the performances of various apodization functions in linearly chirped apodized FBG. Here, six different apodization profiles have been studied including their effects on the performances of the dispersion compensator. Comparison reveals that the sinc profile results best performance for chromatic dispersion compensation.
[sinc profile, apodization profiles, Chirp, optical fibre testing, chromatic dispersion compensation, compensation and apodization profile, Bragg gratings, linearly chirped FBG, Linearly chirped apodized fiber Bragg gratings, spurious side spectrum, chirp modulation, optical fibre dispersion, fiber Bragg gratings, compensation, dispersion]
Moving edge matching for moving object tacking
14th International Conference on Computer and Information Technology
None
2011
We propose an edge segment based moving object tracking algorithm using a static camera. The recognition of object from a sequence image is difficult due to the change in object's shape, orientation, motion and size between frames. Objects may contain several parts with motion variation. Moving objects show a wide range of color variation due to the angle of view, illumination change, and reflectance from neighbor objects. Thus, to overcome these limitations, we make efficient use of edge-segments utilizing a Canny edge detector. Moving edge-segments are grouped by means of a iterative k-means clustering algorithm and the group is used in the Generalized Hough Transform based shape matching algorithm due to its robustness to utilize partial information. A Kalman filter is then used to predict the location of each group in future frames. Experiments with outdoor and indoor image sequences show encouraging result under varying illumination conditions with partial occlusion.
[object recognition, iterative methods, Kalman filter, Shape Matching, edge segment, edge matching, neighbor objects, Edge Segment, object tracking, Robustness, shape recognition, edge detection, image colour analysis, Kalman filters, image sequences, object tracking algorithm, color variation, static camera, Vectors, image sequence, image matching, illumination change, Generalized Hough Transform, image sensors, Canny edge detector, Atmospheric measurements, Hough transforms, Object Tracking, Hough Transform, Particle measurements, shape matching algorithm, iterative k-means clustering algorithm]
An effective framework for implementing electronic governance in developing countries: Bangladesh perspective
14th International Conference on Computer and Information Technology
None
2011
In this paper, we propose an effective framework for implementing electronic governance (e-governance) and e-services in developing countries like Bangladesh. We also present a comparative analysis of present government architecture and the prospects and challenges of implementing e-governance in Bangladesh emphasizing on the usage and potential of facilitating e-services in various sectors of governance. Specific concerns of implementing electronic governance in a developing country like Bangladesh where there exists extreme shortage of resources, limitations in financing, absence of proper development planning, lack of skilled human resources, unavailability of stable and fair democracy and more importantly, a number of unavoidable circumstances including natural disasters. We especially present the adaptability of e-governance in the prime sectors of government and provide a methodical study on the strategies of involving mass people in the governance process improving information and service delivery with their participation in overall decision-making. The potential to ensure highest level of services in all the sectors of government with the implementation of e-governance is also presented in this paper. Moreover, we provide specific recommendations for implementing e-governance in the most feasible, cost-effective, and efficient manner. Analyzing the conducted survey result through statistical procedures also derives a couple of significant factors in implementing e-governance. This paper also aims to point the possible solutions in handling the barriers to implement electronic governance. The supporting framework for integrating the overall socio-economic activities under the information and communication technology framework is also conveyed in this paper.
[socio-economic activities, electronic governance, Finance, e-governance adaptability, statistical procedure, information and communication technology framework, E-Commerce, E-Learning, E-Administration, E-Governance Framework, E-Transparency, governance sector, government architecture, e-services, Bangladesh, Education, service delivery, information delivery, Software, Hardware, statistical analysis, ICT Infrastructure, government data processing, electronic services]
Detection of various diseases by using formant track extraction and pitch contour analysis
14th International Conference on Computer and Information Technology
None
2011
Pitch and formant are two of the most important characteristics of voice signals. These features can be analyzed to detect the condition of patients having Obstructive Sleep Apnea Syndrome (OSAS) and Unilateral Vocal Cord Paralysis (UVCP) before or after treatment. In this paper, we propose a method to determine whether the patients have healed from the disease or not, by using formants track extraction and pitch contour analysis. The formant frequencies are computed by using Linear Predictive Coding (LPC) method and the pitch frequencies are determined by the same LPC &amp; calculating the real cepstrum of the voice signals. A total of 2 patients, one having OSAS and one having UVCP, were analyzed. Significant differences are found in the variation of formant frequencies of both groups. The formant frequencies of patients are higher than the formant frequencies of normal subject and the pitch frequencies are inconsistently unstable than the normal subject. The results are commended from the view of pathophysiological aspect.
[voice signals, disease detection, Atmospheric modeling, Europe, unilateral vocal cord paralysis, formant track extraction, diseases, UVCP, Encoding, medical signal detection, pitch contour analysis, sleep, speech recognition, OSAS, Formant, speech, linear predictive coding method, pathophysiological aspect, obstructive sleep apnea syndrome, linear predictive coding, patient diagnosis, LPC]
One step predictor extended Kalman filter in heavily clamorous system: A strategic approach of noise reduction
14th International Conference on Computer and Information Technology
None
2011
Since its invention, Kalman filter hit the crux of numerous innovation and successful endeavor in unwanted signal reduction. However, it is still to reveal the conducts and performance of extended Kalman filter in heavily noisy wireless signal. Due to the state space concept it conveys superior performance in adaptability over any RLS (Recursive Least Square) filter. In this context, the constitution of extended Kaman filter and its behavior in a rigorous noisy acoustic echo environment is established. Considering several parameters, the result and observed data is also analyzed and compared with NLMS.
[least mean squares methods, recursive least square filter, state space concept, Predictor-corrector algorithm, echo suppression, noisy wireless signal, nonlinear filters, predictor extended Kalman filter, signal denoising, RLS filter, noise reduction, Kalman filters, unwanted signal reduction, rigorous noisy acoustic echo environment, near and far-end signal and Kalman equation]
An efficient approach to recognise fingerprints
14th International Conference on Computer and Information Technology
None
2011
Fingerprint analysis is typically based on the position and pattern of detected singular points in the fingerprint images. These singular points (cores and deltas) represent the characteristics of local ridge patterns, determine the topological structure (i.e., fingerprint type) and largely influence the orientation field. A core-delta relation is used as a global constraint for the final selection of singular points. This paper proposed an approach for singular points detection and then recognizes fingerprints based on singular points position and their relative distances. Experimental results show that the approach is efficient and robust, giving better results than existing dominant approaches.
[fingerprint identification, Irrigation, Poincare index, Image recognition, core delta relation, Singular points, Fingerprints recognition, Fingerprint recognition, fingerprint type, ridge patterns, fingerprint images, singular point detection, topological structure, Delta, fingerprint analysis, fingerprint recognition, Core]
A solution of address space overflow for large Multidimensional Arrays
14th International Conference on Computer and Information Technology
None
2011
We describe a novel implementation scheme of multidimensional array for handling large scale high dimensional datasets. The scheme implements a dynamic multidimensional extendible array employing a set of two dimensional extendible arrays. The multidimensional arrays provide many advantages but it has some problems as well. The Traditional Multidimensional array is not dynamic extendible. Again, if the length of dimension and number of dimension of a multidimensional array is large then the address space for the array overflows soon. In this paper, we propose a solution against the essential problem of address space overflow for handling large scale multidimensional datasets using our implementation model. We evaluate our proposed scheme by comparing with Traditional Multidimensional Array (TMA) for different operations and find a reasonable delay on address space overflow with no significant performance degradation.
[Address space overflow, dynamic multidimensional extendible array, Karnaugh Map, 2D extendible arrays, Dynamic Extension, Vectors, address space overflow, performance degradation, dimension length, high dimensional dataset handling, traditional multidimensional array, Extendible Array, large scale multidimensional dataset handling, data handling, dimension number, MOLAP, Argon, Multidimensional Array]
Cloud security assessment and identity management
14th International Conference on Computer and Information Technology
None
2011
Cloud Computing is the long dreamed vision of computing as a utility, to provide on-demand applications and services. Users can be relieved from the burden of local data storage and maintenance by hosting their data on the clouds. With all its inherent value, the cloud introduces significant security challenges for both the consumers and cloud service providers in all the public, private, and hybrid cloud configurations. In this paper, security challenges (specifically the identity breach) for both consumers and service providers is presented. Cloud security assessment with Quantitative risk and impact framework (QUIRC) and Wide-band Delphi method have been proposed as a suitable process for collecting input from expert consensus. Major security breach incidents of the past have been discussed along with the present day cloud challenges as per the Cloud Security Alliance. A Dymanic Identity Mapping Association N Discovery System (DIMANDS) as an identity solution for large scale heterogeneous network environments is also discussed.
[DIMANDS solution, security breach incidents, service provider, Security, cloud data hosting, private cloud configuration, quantitative risk and impact framework, data storage, Cloud Computing, heterogeneous network environment, cloud computing, wideband Delphi method, public cloud configuration, Dymanic Identity Mapping Association N Discovery System, Privacy QUIRC, identity breach, hybrid cloud configuration, cloud security assessment, Management, DIMANDS, security of data, Cloud Security Alliance, data privacy, data maintenance, cloud identity management]
Yet an efficient algorithm for computing reduced area VLSI channel routing solutions with floating terminals
14th International Conference on Computer and Information Technology
None
2011
The main objective of VLSI channel routing problem is to compute a feasible reduced area routing solution which reduces the height of the channel. A channel is a rectangular routing region with two open ends (left and right) and two sets of fixed terminals (top terminals and bottom terminals) are placed in the upper and lower sides of the channel. A net is a set of terminals that need to be electrically connected (usually using rectilinear wiring). Routing is a process to interconnect all nets within the channel considering all constraints (horizontal and vertical constraints) of that channel. The terminals along the left and right ends of the channel are not fixed, known as floating terminals. Generally, channel routing problem for area minimization is NP-complete. So developing a heuristic algorithm is really interesting. In this paper, we consider a general channel routing problem for channel instances with fixed and floating terminals, and develop an efficient graph based heuristic algorithm for reducing area in the reserved two-layer Manhattan channel routing model.
[network routing, graph theory, channel height, area minimization, Reserved layer model, rectilinear wiring, Area minimization, Algorithm, integrated circuit interconnections, two layer Manhattan channel routing model, Constraints and constraint graphs, VLSI Channel routing problem, electrical connection, Floating terminals, integrated circuit design, floating terminals, reduced area VLSI channel routing solutions, fixed terminal, Clique, Manhattan routing, graph based heuristic algorithm, No-dogleg]
Design a digital system for detection of abnormality condition of heart from ECG waveforms
14th International Conference on Computer and Information Technology
None
2011
Heart disease is the leading cause of death in both men and women claiming up to 900,000 lives every year. Worldwide. An irregular heartbeat is an arrhythmia (also called dysrhythmia).Up to half of these deaths occurs even before emergency services can step in to intervene. To achieve this goal in this paper we have detected the peak of the QRS signal by using QRS detection method of Murthy and Rangaraj which is derivative based method, describe in Verilog HDL. The generated source has been simulated for validation and tested on software Verilogger Pro6.5 demo version.To best of the authers' knowledge this is the first attempt propose design a digital system of the QRS detection used by Murthy and Rangaraj algorithm which can be further implemented by the manufacturer company. In test bench coding we have given some sample values as an input of the peak detecting equations of Murthy and Rangaraj method and have got the appropriate output values according to the equations. From the output values we got the key point of RR interval between the peaks of QRS signal.
[Random access memory, Verilog HDL, quasirandom signal source, dysrhythmia, ALU, derivative based method, electrocardiography, QRS detection method, QRS, MA filter, heart abnormality condition detection, heart disease, emergency service, hardware description language, test bench coding, peak detecting equation, ECG, encoding, medical signal processing, ROM, Murthy-Rangaraj algorithm, digital system design, arrhythmia, ECG waveform, RAM]
Implementation challenges of mobile commerce in developing countries- Bangladesh perspective
14th International Conference on Computer and Information Technology
None
2011
Mobile commerce (m-commerce) is the extension of e-commerce which works within a mobile device using a mobile network infrastructure. M-commerce is an emerging technology and, like e-commerce, it has numerous issues and concerns. This paper is an initiative to identify the concerns regarding the implementation of m-commerce in a developing country like Bangladesh. An online questionnaire is prepared and asked to the different m-commerce stakeholders of Bangladesh to identify the implementation challenges. Collected data are analyzed and the results are produced with appropriate graphs. The results indicate some constructive findings regarding the implementation challenges of m-commerce in a developing country. A set of recommendations are generated based on the survey to make the implementation process easier.
[implementation process, graph theory, online questionnaire, mobile network infrastructure, mobile commerce, Bangladesh perspective, Privacy, mobile computing, graphs, developing countries, Software, e-commerce, Argon, Business, electronic commerce, m-commerce]
A novel self-adaptive algorithm for cancer classification based on feature reduction of SELDI-TOF data using wavelet decomposition
14th International Conference on Computer and Information Technology
None
2011
Surface enhanced laser desorption/ionization time of flight mass spectrometry (SELDI-TOF-MS) is one of the most powerful tool of modern proteomic technology. This high throughput mass spectrometry technology provides large number of complex data with high clinical significance. The protein sequence obtained from this technology is largely used in disease prediction, especially in cancer classification. But the high dimensionality of SELDI-TOF data presents great analytical and computational challenges for such classification using machine learning techniques. In this paper, novel technique has been proposed for dimensionality reduction of the SELDI-TOF data using wavelet decomposition. This technique is self adaptive and independent of the properties of data set. For classification purpose Support Vector Machine (SVM) has been proposed. The performance of the proposed algorithm is evaluated on ovarian and pancreatic cancer data set. The data sets were collected from National Cancer Institute, Center for Cancer Research, USA. A comparative performance analysis with another recently reported algorithm in literature reveals that our method can reduce the dimensionality of the data set more effectively with improved classification accuracy, sensitivity and specificity.
[machine learning technique, classification sensitivity, SELDI-TOF, United States of America, pancreatic cancer data set, wavelet decomposition, classification accuracy, SVM, cancer classification, SELDI-TOF data, high throughput mass spectrometry technology, dimensionality reduction, ovarian cancer data set, time of flight mass spectrometry, National Cancer Institute, feature reduction, Copper, learning (artificial intelligence), pattern classification, USA, support vector machines, surface enhanced laser desorption, proteomic technology, classification specificity, t-test, surface enhanced laser ionization, comparative performance analysis, Center for Cancer Research, support vector machine, Tin, time of flight mass spectroscopy, self-adaptive algorithm, cancer, disease prediction, medical computing, protein sequence, Cancer]
Improved log domain decoding algorithm for LDPC codes over GF (q)
14th International Conference on Computer and Information Technology
None
2011
An improved log domain decoding algorithm of Low density parity check (LDPC) codes over GF (q) using permutation to simplify the parity check equation is presented in this paper. This approach is different from the conventional log domain decoding algorithm of Low Density Parity Check (LDPC) codes over GF (q). The difference between improved log domain decoding and conventional log domain is that in improved log domain decoding permutation is applied in check node process where permutation is applied in between check node process and variable node process for conventional log domain decoding. Improved Log domain is mathematically equivalent to the conventional log domain decoding, however improved log-domain has advantages in terms of implementation, computational complexity and numerical stability. The proposed algorithm and the conventional log domain decoding algorithm are compared both in terms of memory requirement and simulated BER performance of (1008, 504) regular LDPC codes over GF (4) having row weight 3 &amp; column weight 6, BPSK modulation.
[sum-product algorithm, BPSK modulation, parity check codes, LDPC, memory requirement, Log Domain Decoding, LDPC codes, numerical stability, decoding, GF (q), Iterative decoding, log domain decoding algorithm, simulated BER performance, phase shift keying, error statistics, GF, low density parity check codes, permutation, computational complexity]
Mining biomedical data from hypertext documents
14th International Conference on Computer and Information Technology
None
2011
Data mining is a process of discovering useful information from a database and analysis of extracted information. Text mining uses many techniques of data mining. It primarily deals with unstructured data. Web mining is an extension of text mining since it deals with unstructured data. Data mining relates to find data from &#x201C;static databases&#x201D; which contains &#x201C;structured&#x201D; data where as, web mining plays with data that are &#x201C;dynamic&#x201D; and &#x201C;unstructured&#x201D;. In this papers our goal is to mine biomedical data from hypertext documents (e.g., mining data from web contents) using text mining techniques with the help of &#x201C;biomedical ontology&#x201D;. Web data repositories are the hypertext documents. Texts in the Hypertext documents are unstructured and they contain Hypertext Markup Language (HTML) tags, scripting languages, images, audios, videos, URLs etc. We collect a number of documents using Google crawler and preprocess the hypertext documents and extract the text data. Next, we identify whether a word is a biomedical entity or not by using a biomedical database the &#x201C;Unified Medical Language System (UMLS) metathesaurus&#x201D;. The mapping of biomedical entity from the metathesaurus will be done based on keyword query. Then we apply the result to re-rank the web documents to find most relevant documents. We conclude that the more occurrence of a biomedical entity in a page, the more relevant the page is, and thus, we can re-rank the documents to find the most relevant documents by using text mining technique.
[text analysis, Communities, Unified modeling language, data mining, document clustering, hypertext markup language tags, Biomedical ontology, database management systems, Engines, URL, Web data repositories, query processing, keyword query, Google crawler, extracted information analysis, Web mining, Web documents, text mining, unified medical language system metathesaurus, scripting languages, hypermedia markup languages, medical information systems, Datamining, classification, biomedical data mining, hypertext documents, static databases, ontologies (artificial intelligence), Internet, biomedical ontology, performance analysis]
Tabular representation of schema mappings of a data exchange system
14th International Conference on Computer and Information Technology
None
2011
Data exchange is the problem of taking data structured under a source schema and translating them into data structured under a target schema. In data exchange, a target instance is materialized that conforms to the source data as much as possible. A schema mapping is a high-level specification that describes the relationship between two database schemas. Schema mappings constitute essential building blocks of data exchange and data integration systems. Global-and-local-as-view (GLAV) is one of the approaches for specifying the schema mappings. In our earlier work we showed that GLAV mappings of a data integration system can be expressed by tabular structures which we called schema mapping tableaux (SMT). SMT can be used as an operator on an instance of the source schema to produce an instance of the global schema of a data integration system. In this paper, we show that GLAV mappings of a data exchange system can also be represented by SMT. Moreover, SMT can be used to compute target instances of a data exchange system which are `minimal' and `most general' in nature.
[global-and-local-as-view, schema mapping tableaux, data exchange system, data structure, Universal Solutions, database management systems, Data Exchange, source schema, schema mapping, data integration systems, Schema Mapping, database schemas, data structures, tabular representation, GLAV, SMT, Core]
An enhanced decision based adaptive median filtering technique to remove Salt and Pepper noise in digital images
14th International Conference on Computer and Information Technology
None
2011
In this paper, we present an algorithm to remove Salt and Pepper noise from grayscale images. It is an enhanced adaptive median filtering algorithm which initially calculates median without considering noisy pixels in the processing window. If the noise-free median value is not available in the maximum processing window, the last processed pixel value is used as the replacement. Moreover, in extreme situations such as noise corrupted pure black and white images, a threshold value is used to determine the pixel value. Experimental results show that our algorithm can perform better than the other non-linear filters, suppressing noise level more than 90% while preserving visual quality and necessary details of the image.
[Algorithm design and analysis, noise level suppression, median filters, adaptive filters, decision theory, enhanced decision based adaptive median filtering technique, Noise, Adaptive Median Filter (AMF), nonlinear filters, noise corrupted pure black-and-white images, visual quality, image denoising, Flowcharts, salt-and-pepper noise removal, threshold value, pixel value, digital images, Salt and Pepper, image resolution, Manganese]
Propagation properties and stress sensitivity of square and hexagonal photonic crystal fiber
14th International Conference on Computer and Information Technology
None
2011
Propagation properties of square and hexagonal lattice photonic crystal fibers (PCFs) are analyzed here for stressed and unstressed conditions. The analysis has been carried out using finite element method (FEM). Both types of fibers are investigated here by varying number of air hole rings with same design and operating parameters. The results show that birefringence increases more as a function of stress for hexagonal PCFs than square. We also have found that external stress induced confinement loss is higher for hexagonal PCFs with lower number of air hole rings than square but it decreases very sharply with the increase of number of air hole rings.
[propagation properties, holey fibres, optical fiber communication system, Photonic crystal fibers, Photonic crystal fiber, finite element analysis, stress sensitivity, Optical fiber dispersion, Photonics, FEM, Optical fiber polarization, PCF, finite element method, hexagonal photonic crystal fiber, optical fibre networks, confinement loss and elasto-optic effect, photonic crystals, air hole rings]
A Multi-Agent negotiation based virtual conference decision making system
14th International Conference on Computer and Information Technology
None
2011
Multi-Agent negotiation system has become the core of the intelligent e-commerce. A Virtual conference is the process where several agents meet virtually using electronic devices such as computer, mobile etc. Decision making is one of the most fundamental issues in Multi-Agent negotiation system for the virtual conference of Electronic markets. A negotiation protocol is used for planning and decision making. In this paper, we present a distributed Multi-Agent decision making system for mobile devices. We present an architecture that supports virtual conference decision making system using mobile phones. We also provide an algorithm that helps for making decision regarding to a specific agenda proposed by an individual participating in the virtual conference. Finally, we implement our system in a Blackberry platform.
[Decision Making, intelligent e-commerce, Agent, Negotiation, multi-agent systems, virtual conference, Proposals, virtual conference decision making system, electronic devices, mobile computing, multiagent negotiation system, Organizations, decision making, electronic markets, Virtual Conference, Blackberry platform, Consumer electronics, electronic commerce, negotiation protocol]
Performance analysis of SFBC in MIMO-OFDM system over Nakagami-m fading channel
14th International Conference on Computer and Information Technology
None
2011
Multiple input and multiple output (MIMO) OFDM exploit spatial diversity to improve spectral efficiency. In this paper, we have derived an analytical model for signal to noise plus interference ratio incorporating inter carrier interference (ICI) and average probability of error in MIMO-OFDM using space frequency block code (SFBC) over Nakagami-m fading channel. Results show that average probability of error performance of MIMO-OFDM system in Nakagani fading channel decreases if fading parameter m is increased. At a constant fading parameter m, normalized frequency offset increases as the average probability of error increases.
[Inter carrier interference, MIMO-OFDM system, space frequency block code, Nakagami-m fading channel, analytical model, Nakagami channels, SFBC, Nakagami-m fading, block codes, spectral efficiency, inter carrier interference, spatial diversity, Average probability of error, Orthogonal frequency division multiplexing, OFDM modulation, Multiple input multiple output, signal to noise plus interference ratio, multiple input and multiple output OFDM, intercarrier interference, MIMO communication, performance analysis]
New variants of Ant Colony Optimization for Network Routing
14th International Conference on Computer and Information Technology
None
2011
This paper suggests new variants of Ant Colony Optimization(ACO)Techniques for Network Routing. There are three existing variants of ACO based on pheromone deposit calculation. In our earlier work we suggested three different heuristics for selecting the next node at each step of iteration. Incorporation of these heuristics in each of the above three variants result into nine variations. In this paper the performance of these nine variations have been studied. Moreover, we have modified the pheromone deposit calculation considering the transmission time of each successful packet(ant) and incorporated this new pheromone update formula in each of the nine variants. As a result, we have obtained nine new variants of ACO. The performance of these new nine variants have been compared with previous ones with respect to the speed of execution, throughput and the number of successful packets. In this paper it has been observed that the new variations of ACO have outperformed the previous. These new variants can perform efficient network routing in an environment having variable transmission time along the paths due to congestion or poor link quality.
[iterative methods, poor link quality, network routing, Artificial neural networks, transmission time pheromone, metaheuristics, ACO, Network Routing, pheromone deposit calculation, optimisation, ant colony optimization, successful packets, telecommunication network routing, iteration step, Reliability, congestion, variable transmission time]
Bilingual plagiarism detector
14th International Conference on Computer and Information Technology
None
2011
Internet has become primary medium for information access, commerce in today's globalized world and almost every information is available in the Internet either in the native language of the user or in a non-native language. Therefore, it becomes easier to use another author's contents from the Internet without proper citation or reference and this tendency is increasing day-by-day. Such use of another author's contents, thoughts, ideas, or expressions and the representation of them as one's own original work is known as plagiarism. Though plagiarism can be found in almost every field, it is a major problem in academic area as plagiarism destroys individual's creativity and originality and defeats the purpose of education. At present many commercial and noncommercial plagiarism detection software are available. However, most of them are unilingual in nature and none of them considers checking of Bangla documents for plagiarism. In this paper, we have introduced statistical method and method based on individual content for detecting plagiarism from English and Bangla electronic documents. The first method performs different statistical analysis of the documents for plagiarism detection whereas the second method is based on the analysis of individual contents of the documents. The system can perform plagiarism checking in a Bangla document from English documents and vice versa. It can also detect plagiarism from the documents of the same language. The system has been evaluated by real documents. We have found that our system can detect plagiarism from documents of two different languages efficiently.
[document handling, nonnative language, documents relevancy, query execution, natural language processing, individual originality, Generators, English electronic document, bilingual plagiarism detector, native language, individual creativity, Databases, security of data, Plagiarism, statistical method, information access, root detection, User interfaces, education purpose, Bangla electronic document, Internet, statistical analysis]
Ensembles of Neural Networks through crossover based pattern generation
14th International Conference on Computer and Information Technology
None
2011
The goal of an ensemble construction with several neural networks is to achieve better generalization than that of a single neural network. A Neural Network Ensemble (NNE) performs well when the component networks are diverse, so that failure of one is compensated for by others. Training data variation (i.e., different training sets for different networks) is a good source of diversity because the function that a network approximates is learned from its training data. We introduce a new approach to training data variation and propose the Ensemble based on Crossover based Pattern Generation (ECPG). ECPG generates some new training patterns for a particular network; a pair of pattern is generated interchanging some of input feature values in between a pair of selected original patterns. The effectiveness of ECPG was evaluated using several benchmark classification problems, and ECPG was found to achieve better or competitive performance with respect to related conventional methods. With several benefits over conventional methods, crossover based pattern generation appears to be a good technique for ensemble construction.
[pattern classification, Artificial neural networks, neural network ensembles, diversity, generalization, neural network ensemble, ensemble construction, Lead, crossover based pattern generation, classification problems, pattern generation, learning (artificial intelligence), training patterns, Bagging, neural nets, training data variation, Signal to noise ratio]
E-governance framework for higher education institutes using grid: Digital Bangladesh perspective
14th International Conference on Computer and Information Technology
None
2011
e-Governance provides government services through interactive website and portals. The strategic objective of e-Governance is to support and simplify governance for all the stakeholders: a) Government, b) People, and c) Businesses. It is a collection of ICT enabled applications to achieve efficiency, effectiveness, transparency and accountability in various forms of e-Governance such as G2G, G2E, G2C and G2B. With immergence of technology and growing demand of the society, if appropriately applied, Higher Education Institutes (HEIs) can also leverage the benefit of e-Governance by collaborative resource sharing through grid computing. The underlying goals for adopting e-Governance practices are to ensure improved quality in disseminating education and administration; establish dynamic and need-based communication with various internal, external and peer entities across the country/globe; and conforming to regulations. Grid computing can be an ideal framework for HEIs to effectively and efficiently handle need-based computational and on-demand query processing for enabling e-Governance, contributing in achieving digital Bangladesh in faster time frame. Implementing an e-Governance solution using grid can potentially lower the cost of developing, deploying, and managing governance solutions and providing better services to the stakeholders as well as the citizens (students, guardians). In this paper, the authors propose a framework of e-Governance-grid suitable for Bangladeshi HEIs; ensuring quality education to broader audience in building digital Bangladesh.
[grid computing, Grid, e-governance framework, governance solution deployment, quality education, query processing, e-Governance, Production, Fabrics, interactive website, Cryptography, governance solution development, further education, collaborative resource sharing, interactive portals, Digital Bangladesh perspective, Educational institutions, governance solution management, HEI, Higher Education Institute (HEI), government services, higher education institutes, computer aided instruction, government data processing, ICT collections]
Simulated annealing variants for solving resource Constrained Project Scheduling Problem: A comparative study
14th International Conference on Computer and Information Technology
None
2011
Now-a-days different meta-heuristic approaches are being applied for solving Combinatorial Optimization Problems (COP). In this paper Resource Constrained Project Scheduling Problem (RCPSP) has been presented as a COP. This is a common problem for many construction projects. It is highly constrained and is categorized as a NP-hard problem. In our earlier work Simulated Annealing (SA_RCP) outperformed other meta-heuristics, like, Genetic Algorithm, Tabu Search, Particle Swarm Optimization and its variant in solving benchmark instances of this problem. Having been inspired by this result we have further applied new variants of Simulated Annealing for RCPSP. In this work, we have taken three more SA variants and applied them for solving a benchmark instance of this problem. The results show that Simulated Annealing incorporated with Tabu List and Greedy Selection Heuristic (GTSA_RCP) outperforms other methods in getting optimal results with maximum hit and minimum fluctuations.
[Schedules, combinatorial mathematics, particle swarm optimisation, Metaheuristics, Simulated Annealing, simulated annealing variants, metaheuristic approaches, genetic algorithm, greedy selection heuristic, scheduling, resource constrained project scheduling problem, tabu search, Hybrid Methods, Local Search, search problems, Annealing, Fluctuations, project management, simulated annealing, genetic algorithms, particle swarm optimization, Resource Constrained Project Scheduling, construction industry, tabu list, construction projects, NP-hard problem, combinatorial optimization problems, computational complexity]
BER performance of MIMO-OFDM system over Nakagami fading channel
14th International Conference on Computer and Information Technology
None
2011
Orthogonal frequency division multiplexing (OFDM) is a well-liked technique for high data rate as well as high spectral efficiency. As a promising technology for next generation wireless communication, MIMO-OFDM has gained more wellbeing. In this paper, we have derived an analytical expression of signal to noise plus interference ratio (SNIR) and average probability of error in MIMO-OFDM system over Nakagami-m fading channel. It is found that the SNIR of MIMO-OFDM system decreases while increases normalized frequency offset. To increase the fading parameter m, the bit error rate (BER) performance for Nakagami-m fading channel has been reduced.
[SNIR, MIMO-OFDM system, orthogonal frequency division multiplexing, OFDM, Bit error rate, next generation networks, normalized frequency offset, Signal to noise plus interference, Nakagami fading channel, Nakagami channels, error average probability, bit error rate performance, Nakagami-m fading, BER performance, Orthogonal frequency division multiplexing, OFDM modulation, next generation wireless communication, Multiple input multiple output, signal to noise plus interference ratio, MIMO communication, error statistics]
Blocking artifact detection by analyzing the distortions of local properties in images
14th International Conference on Computer and Information Technology
None
2011
Now-a-days, recent trend is to represent high quality images or videos by using less bit representation. To represent high quality videos or images with low bit rate, an effective compression algorithm removes the redundancy because of statistical correlation and also the insignificant component of image signal. This paper represents a new algorithm to measure the blocking artifacts of videos by analyzing the distortions of local properties of image signals like dominant edge magnitude and direction. For this purpose sobel convolution mask is used rather than kirsch mask to make the detection process faster and to model video noises that occur in broadcasting systems. Extensive experiments on various videos show that the new algorithm is very much efficient to measure the blocking artifacts in real time video error detection applications.
[broadcasting systems, local properties, kirsch mask, Discrete Cosine Transform, Humans, blocking artifact detection, video quality, statistical correlation, video error detection, sobel convolution, Edge magnitude, image signal component, image quality, video noises, image signals, edge magnitude, Human Visual Sensitivity, Blockiness, edge detection, statistical analysis, video signal processing, image resolution, Biomedical imaging]
Translating unknown words using WordNet and IPA-based-transliteration
14th International Conference on Computer and Information Technology
None
2011
Due to small available English-Bangla parallel corpus, Example-Based Machine Translation (EBMT) system has high probability of handling unknown words. To improve translation quality for Bangla language, we propose a novel approach for EBMT using WordNet and International-Phonetic-Alphabet(IPA)-based transliteration. Proposed system first tries to find semantically related English words from WordNet for the unknown word. From these related words, we choose the semantically closest related word whose Bangla translation exists in English-Bangla dictionary. If no Bangla translation exists, the system uses IPA-based-transliteration. For proper nouns, the system uses Akkhor transliteration mechanism. We implemented the proposed approach in EBMT, which improved the quality of good translation by 16 points.
[unknown word translation, Gold, Dictionaries, example-based machine translation system, Transliteration, natural language processing, EBMT system, Akkhor transliteration mechanism, Machine Translation, Indexes, WordNet, English-Bangla dictionary, international-phonetic-alphabet-based transliteration, Example-Based Machine Translation, IPA-based-transliteration, Software, translation quality improvement, Bangla language, Marine vehicles, Bangla translation, English-Bangla parallel corpus, dictionaries, language translation]
Internet exchange points: A context of Bangladesh
14th International Conference on Computer and Information Technology
None
2011
The current internet infrastructure, as regulated by BTRC (Bangladesh Telecommunication Regulatory Commission) requires all ISPs (Inter Service Providers) to be connected to IIGs (International Internet Gateways) for all local and global routings. However, if there is a local exchange that can route all local traffic within the country without using international gateways or the expensive bandwidths that they provide, the local intranet would be a much stronger and efficient one, attracting hosting investments from local and foreign investors. It will provide superior speeds for local traffic due to the exclusion of redundant nodes, whereas the speed for international traffics will also be enhanced due to a reduction in load. We will be going through the ideas, different aspects and enhancements over the present infrastructure in our paper.
[IXP, internetworking, Promoting Local Hosting, Bangladesh telecommunication regulatory commission, Robustness, international Internet gateways, local routings, Mirrors, Monitoring, local intranet, Technological innovation, Peer to peer computing, Internet exchange points, global routings, local exchange, inter service providers, WiMAX, BDIX, Bangladesh, Internet infrastructure, telecommunication network routing, Logic gates, hosting investments, Reduced Latency, Internet]
A study of code cloning in server pages of web applications developed using classic ASP.NET and ASP.NET MVC framework
14th International Conference on Computer and Information Technology
None
2011
Frequently change in requirements, tight delivery deadline and complex application architecture slow down web applications development and encourage code cloning. Web application frameworks mainly support developers to speed up development by providing libraries for database access, session management, and they often promote code reuse. In this paper, we provide a systematic study of cloning in six (6) Web Applications of different sizes, developed using Classic ASP.NET and ASP.NET MVC framework to find out whether there is any relation between frameworks and code cloning. The contribution of our study is: 1) the study results shows which framework in .NET technology can be chosen to avoid cloning in development of web application; 2) the cloning metrics that we have calculated and applied in our study may be useful in other similar studies.
[Measurement, session management, Blogs, Cloning, Web applications, libraries, database management systems, code reuse, cloning metrics, server pages, open source application, code cloning, software reusability, database access, web application framework, ASP.NET, Internet, ASP.NET MVC framework, software metrics, complex application architecture]
Fuzzy logic controller for an inverted pendulum system using quantum genetic optimization
14th International Conference on Computer and Information Technology
None
2011
In this paper, we propose a new generalized design methodology of intelligent robust fuzzy control systems based on quantum genetic algorithm (QGA) called quantum fuzzy model that enhance robustness of fuzzy logic controllers. The QGA is adopted because of their capabilities of directed random search for global optimization to find the parameters of the shape and width of membership functions and rule set of the FLC to obtain the optimal fuzzy controller simultaneously. We test the optimal FLC obtained by the quantum computing applied on the control of dynamic balance and motion of cart-pole balancing system. We compare the proposed technique with existing mamdani fuzzy logic controller which is designed through conventional genetic algorithm. Simulation results reveal that QGA performs better than conventional GA in terms of running speed and optimizing capability.
[directed random search, Rule Base, Force, discrete systems, membership functions, Biological cells, Optimization, fuzzy control, Fuzzy control, cart-pole balancing system, motion control, Cart-pole Balancing Problem, robust control, inverted pendulum system, dynamic motion control, search problems, robustness enhancement, Fuzzy Logic Control, Quantum Computing, quantum genetic optimization, intelligent control, fuzzy controller, control system synthesis, optimal control, Encoding, genetic algorithms, dynamic balance control, global optimization, generalized design methodology, Mamdani fuzzy logic controller, quantum genetic algorithm, nonlinear control systems, intelligent robust fuzzy control systems, quantum computing, quantum fuzzy model]
Incorporating high frequency component matching motion estimation in H.264: A revolutionary improvement for real-time telemedicine applications
14th International Conference on Computer and Information Technology
None
2011
In Telemedicine applications, for diagnostic purposes, it is essential that the video compression process causes no tangible loss of detail and introduces no noticeable artifacts which could be otherwise misinterpreted as being pathological in nature. On the other hand, due to the limitation of storage and transmission and the huge amount of medical videos, high compression ratio is often required. Video coding exploits the high correlation between successive image frames to improve coding efficiency, which is achieved by motion estimation and motion compensation techniques. This paper demonstrates the feasibility of incorporating the high frequency component matching block motion estimation in H.264 in order to improve video compression performance in telemedicine applications. Implementation using the H.264 codec and thereby facilitating performance analysis, the experimental result indicates that high frequency component matching introduces a revolutionary improvement as a matching criterion for motion compensated predictive coding in real-time telemedicine applications.
[H.264 Motion Estimation, real-time telemedicine applications, Frequency estimation, diagnostic purposes, motion compensation, successive image frames, motion compensated predictive coding, high frequency component matching motion estimation, Telemedicine, Tomography, motion estimation, Weight measurement, telemedicine, Codecs, Video Compression, Computational modeling, High Frequency Component Matching, Estimation, H.264 codec, motion compensation techniques, video coding, image matching, video compression process, Matched filters, medical videos, patient diagnosis]
Implication of association rules employing FP-growth algorithm for knowledge discovery
14th International Conference on Computer and Information Technology
None
2011
Nowadays the database of an organization is increasing day by day. Sometimes it is necessary to know the behavior of that organization by retrieving the relationships among different attributes of their database. Implication of association rules provides an efficient way of data mining task which is used to find out the relationships among the items or the attributes of a database. This paper addresses on implication of association rules among the quantitative and categorical attributes of a database employing classical logic and Frequent Pattern (FP) - Growth algorithm. The system is based on generating association rules over binary or categorical attributes and is organized with splitting the quantitative attributes into two or more intervals to generate association rules when the domain of quantitative attribute increases. The effectiveness of the method has been justified over a sample database.
[Barium, database quantitative attributes, FP-Growth, data mining, association rules, Data Mining, knowledge discovery, KDD, Association Rule, Indexes, Remuneration, database management systems, database categorical attributes, Education, FP-Tree, tree data structures, frequent pattern-growth algorithm, FP-growth algorithm, classical logic]
Compression enhancement of HIBASE technique using HUFFMAN coding
14th International Conference on Computer and Information Technology
None
2011
HIBASE compression technique simply replaces the attribute values in a tuple with fixed length code-words. However, fixed length coding system is not an optimal compression technique because some redundancies occur in the compressed table. This redundancy can be avoided if we use Huffman code-words. Moreover, using Huffman code-word will ensure optimal compression as well as High performance operation. The objectives of the research are to i) develop a dictionary by applying the principle of Huffman coding, ii) further compress the relational storage of HIBASE by applying dynamic Huffman coding, iii) develop algorithm to perform query operation on the compressed storage, iv) analyze the performance of the proposed system in terms of both storage and queries.
[fixed length code-words, data compression, fixed length coding system, Huffman codes, Compression, optimal compression, high compression database system, Database Compression, Variable length Coding, HUFFMAN, database management systems, relational storage compression, HIBASE, Databases, dynamic Huffman coding, compression enhancement]
Effect of Subpath Protection in optical WDM Mesh Networks
14th International Conference on Computer and Information Technology
None
2011
Networking makes communication ease. But failure of any network element due to the node/nodes or the link/links or any other factor can cause a huge problem resulting in data loss. As such, many protection and restoration techniques have been developed. In our research, we worked with Optical Wavelength Division Multiplexing (WDM) Mesh Networks, assuming a hypothetical network of a larger area (Bangladesh). We have applied Shared-Path Protection and Subpath Protection schemes for a single link failure for a given set of lightpaths. Then, comparison of both the simulation results of the above mentioned protection schemes, showed that subpath protection gives much reduced fault-recovery time than that of shared-path protection. Same is concluded for survivability and scalability of our hypothetical network.
[wavelength division multiplexing, optical WDM mesh network, Scalability, data loss, Optical fiber networks, Wavelength division multiplexing, shared-path protection scheme, fault-recovery time reduction, Wavelength Division Multiplexing (WDM), protection scheme, lightpath, Mesh networks, Fault management, single link failure, telecommunication network reliability, optical wavelength division multiplexing mesh network, optical fibre networks, subpath protection scheme, partitioning]
Training neural network with damped oscillation and maximized gradient function
14th International Conference on Computer and Information Technology
None
2011
Constant learning rate (LR) which is most widely used for training neural networks (NNs) in back propagation (BP) but it is not usually preferable due to its slow convergence rate while using small learning rate and it also shows less accuracy while using higher learning rate. In this paper, we are proposing a faster and supervised algorithm which shows more accuracy in a few iterations while dealing with neural networks (NNs). Training of NNs with damped oscillation and maximized gradient function (DOMG) deals with the implementation of damped oscillation in learning rate called damped learning rate (DLR) by which we get more accuracy in a few iterations and maximized gradient function is used for fast weight updating. DOMG is significantly tested on eight real world benchmark classification problems such as heart disease, ionosphere, Australian credit card, time series, wine, horse, glass and soybean identification. The proposed DOMG outperforms the existing BP in terms of convergence rate and generalization ability.
[soybean identification, generalization ability, Glass, Classification algorithms, ionosphere, Australian credit card, damped learning rate, heart disease, Convergence rate, gradient methods, benchmark classification problems, pattern classification, glass, back propagation, Artificial neural networks, Neural Networks, time series, constant learning rate, damped oscillation and maximized gradient function, supervised algorithm, horse, Maximized gradient function, backpropagation, fast weight updating, neural network training, Damped learning rate, convergence rate, neural nets, wine]
Compensation of quadrature imbalance in an optical 16-QAM digital coherent receiver
14th International Conference on Computer and Information Technology
None
2011
In this paper, the impact of quadrature imbalance (QI) is analyzed for an optical 16-quadrature amplitude modulation (16-QAM) transmission system that employs digital coherent receivers. A compensation method for QI in digital domain is demonstrated using four real-valued filters connected in butterfly configurations and adapted by decision-directed least-mean-square (DD-LMS) algorithm. The effectiveness of the proposed QI compensation scheme is verified by numerical simulations.
[least mean squares methods, butterfly configurations, Quadrature amplitude modulation, filtering theory, quadrature amplitude modulation, decision-directed least mean square, optical 16-QAM, optical receivers, optical modulation, Phase shift keying, digital coherent receiver, real-valued filters, quadrature imbalance compensation]
Towards an adaptive resource management scheme for mobile cellular network
14th International Conference on Computer and Information Technology
None
2011
Due to the tremendous increase in both the size of the wireless mobile community and demand of high bandwidth by resource hungry applications, efficient radio resource management is one of the most important issues in wireless communication nowadays. A good resource management scheme should support as many mobile users as possible while maintaining the necessary quality of service. Flexibility to adapt the dynamic nature of the communication infrastructure depends on some factors other than available bandwidth, such as, density of users, percentage of reservation for handoff and new calls etc. In this paper, we present and analyze a flexible and dynamic resource management scheme, namely Dynamic Resource Allocation with Flexible Reservation (DRAFR), which is specially tailored to support the emerging traffic in a wireless and mobile network. Extensive analysis and graphical results are shown to demonstrate the efficacy of our scheme with regard to traditional Quality of Service (QoS) parameters when compared to other resource allocation schemes.
[user density, Handoff, radio resource management, mobility management (mobile radio), dynamic resource allocation-with-flexible reservation, wireless mobile community, wireless communication, Road transportation, resource allocation, Quality of service (QoS), resource hungry applications, Bandwidth, handoff reservation percentage, dynamic resource management scheme, high bandwidth demand, DRAFR, wireless network, quality-of-service, adaptive resource management scheme, Call admission Control, Educational institutions, Flexible channel allocation, quality of service, mobile cellular network, telecommunication traffic, cellular radio, mobile network]
A novel rate-based macroblock classification for fast mode decision in H.264 video coding
14th International Conference on Computer and Information Technology
None
2011
In this paper, a novel rate-based macroblock classification is proposed for fast mode decision in H.264 video coding standard. The main idea is to classify each macroblock into simple motion or complex motion contents based on the Inter16&#x00D7;16 mode's residue block bit-rate and then according to the classification different mode searching orders with distinct early termination schemes are employed. This new algorithm is very simple for both hardware and software implementations without extra computational module. To speedup the intra mode decision, a new fast Intra 4&#x00D7;4 mode selection algorithm is also proposed by choosing most likely modes using the low complexity SATD cost as screening function. It is demonstrated by experimental results that the proposed rate-based fast algorithm can reduce 47% to 65% of the H.264 total encoding time with negligible degradation in the rate-distortion performance. While the rate-based algorithm combined with the fast intra mode selection method could further speedup 5% to 10% of the encoding time with only little rate-distortion degradation.
[H.264 video coding, Macroblock, image classification, Mode Decision, rate based algorithm, video coding, block bitrate, H.264/AVC, simple motion, complex motion, novel rate based macroblock classification, fast mode decision, Video Coding, Early termination Scheme]
Simultaneous design of membership functions and rule sets for type-2 fuzzy controllers using genetic algorithms
14th International Conference on Computer and Information Technology
None
2011
A Type-2 Fuzzy logic controller adapted with genetic algorithm, called type-2 genetic fuzzy logic controller (T2GFLC), is presented in this paper to handle uncertainty with dynamic optimal learning. Genetic algorithm is employed to simultaneous design of type-2 membership functions and rule sets for type-2 fuzzy logic controllers. Traditional fuzzy logic controllers (FLCs), often termed as type-1 fuzzy logic systems using type-1 fuzzy sets, cannot handle large amount of uncertainties present in many real environments. Therefore, recently type-2 FLC has been proposed. The type-2 FLC can be considered as a collection of different embedded type-1 FLCs. However, the current design process of type-2 FLC is not automatic and relies on human experts. The purpose of our study is to make the design process automatic. The evolved type-2 FLCs can deal with large amount of uncertainties and exhibit better performance for the mobile robot. Furthermore, it has outperformed their type-1 counterparts as well as the traditionally designed type-2 FLCs.
[Mobile Robot, type-2 genetic fuzzy logic controller, rule sets, type-2 fuzzy controllers, MIMICs, fuzzy set theory, dynamic programming, control system synthesis, membership functions, genetic algorithms, Type-2 fuzzy sets, Genetic-Algorithms, T2GFLC, Biological cells, Optimization, fuzzy control, dynamic optimal learning, simultaneous design, Interval Type-2 FLC, learning (artificial intelligence)]
A flexible keypad reducing keystrokes and key jamming for cell phones
14th International Conference on Computer and Information Technology
None
2011
With the ever-increasing popularity of cell phone devices, text based services on such devices are becoming more and more popular. Problems with traditional keypads primarily lie with the placement of the letters alphabetically on the keys. This configuration is comparatively easy for the users to remember but can greatly limit the flexibility of finger movement, as well as require a higher number of keystrokes and key jamming. In attempting to resolve these issues, a novel innovative solution is proposed here, focusing on both the structure of human finger movements and ordering of letters on the keys based on their frequency of use. Simulations and performance measurement of our represented system have shown rapid reduction in key jamming by up to 57 percent, improvements in flexibility of finger movement by up to 11.5 percent and reduction in number of keystrokes by up to 34 percent.
[Algorithm design and analysis, Finger Movement, keystrokes, Thumb, ergonomics, Keypad and Key Jamming, Jamming, Frequency of Alphabets, Presses, text based services, Databases, flexible keypad, Cellular phones, Layout, key jamming, Cell Phone, Human Factors, mobile handsets, cell phones, finger movement]
Reducing packet loss in Mobile IPv6
14th International Conference on Computer and Information Technology
None
2011
In Mobile IPv6, a Mobile Node directly updates its current location information to a Correspondent Node using a route optimization method. Route optimization method helps to culminate the packet delay incurred due to suboptimal route traversed by the packets from a Correspondent Node to a Mobile Node in Mobile IPv4. However, it introduces packet losses during Mobile Node's transition from one foreign network to another foreign network. This packet loss increases as the mobility of the node increases. In this paper, we propose a scheme to reduce this packet loss. Our solution is based on the Mobile Node's mobility prediction at the Correspondent Node and a buffering mechanism at the Home Agent. Simulation results show the effectiveness of our scheme in case of reducing the packet loss problem besetting the Mobile IPv6.
[mobility, Mobile IP, packet loss reduction, buffering mechanism, MIPv6, MIPv4, mobile computing, optimisation, mobile node IPv6, home networks, Packet Loss, Route Optimization, route optimization method, IP networks, home agent, Buffering, Manganese]
Impact of corpus size and quality on English-Bangla statistical Machine Translation system
14th International Conference on Computer and Information Technology
None
2011
Statistical machine translation (SMT) evolves with the motivation of translating a text from source language to target language which employs the machine learning technique to a parallel corpus for producing a translation system exclusively automatic. We have developed Anubad[26], a phrase-based Bangla to English SMT on the top of the SMT model proposed in [1] which is publicly available on www.anubad.com. As the most challenging task for SMT system development is the designing of large parallel corpora as the translation quality significantly depends upon the corpus dimension and quality, Bangla parallel corpus suffers the same problem and fails to provide a standard translation till now. In this paper, through simulations, we provide a guideline for developing an English-Bangla bilingual corpus. Although in a phrase-based Statistical Machine Translation systems, more training data is generally better outcome, however, we deflect from this notion and according to our experimental results, we observed that quality of good corpus could significantly improve the Bangla to English translation quality. We have found better translation quality by employing our techniques and achieved effective improvements on NIST and BLEU scores.
[text translation, machine learning technique, natural language processing, Phrase-Based Machine Translation, Statistical machine translation, Bangla to English, Natural Language Processing, Parallel Corpus, Decoding, target language, parallel corpora, parallel corpus, language source, corpus dimension, NIST, SMT, statistical analysis, English-Bangla statistical machine translation system, language translation]
Robust facial expression recognition based on Local Monotonic Pattern (LMP)
14th International Conference on Computer and Information Technology
None
2011
Automatic facial expression recognition is a prominent and challenging research interest with usefulness in a variety of fields. It is playing increasingly important role in the fields of human computer interaction, data-driven animation etc. Success of most facial image analysis solutions depend on an effective facial feature representation. This paper presents one such new appearance-based facial feature, the Local Monotonic Pattern (LMP). LMP can extract robust facial feature from a face image that gives accurate and reliable recognition performance for expression recognition. The LMP operator applied on a pixel, finds the monotonic intensity transition of neighboring pixels at different radii. The micro patterns thus found is enhanced with spatial information by tiling the image and taking histogram of each tile. The final feature vector is a collation of these histograms. This feature vector is then employed to classify expressions with well known machine learning method: Support Vector Machine (SVM). Experimental results using Cohn-Kanade expression database show that the LMP descriptor yields improved recognition rate against other existing appearance-based feature descriptors.
[Facial expression recognition, LMP operator, robust facial feature extraction, SVM, machine learning method, emotion recognition, appearance-based feature descriptors, robust automatic facial expression recognition, appearance-based facial feature, feature extraction, face recognition, learning (artificial intelligence), monotonic intensity transition, data-driven animation, support vector machines, Cohn-Kanade expression database, facial image analysis solutions, local monotonic pattern, Ions, facial feature representation, neighboring pixels, appearance-based, LMP, feature vector, support vector machine, image representation, human computer interaction, LMP descriptor]
An unsupervised natural image segmentation algorithm using mean histogram features
14th International Conference on Computer and Information Technology
None
2011
A new integrated feature distributions based natural image segmentation algorithm has been proposed. The proposed scheme uses histogram based new color texture extraction method which inherently combines color texture features rather then explicitly extracting it. Use of non parametric Bayesean clustering makes the segmentation framework fully unsupervised where no a priori knowledge about the number color textures regions are required. The feasibility and effectiveness of the proposed method have been demonstrated by various experiments using images of natural scenes. The experimental results reveal that superior segmentation results can be obtained through the proposed unsupervised segmentation framework.
[mean histogram features, segmentation framework, Non parametric Bayesian clustering, color texture features, Natural scene, Bayesian clustering, image texture, unsupervised learning, Image segmentation, pattern clustering, Mean histogram, image segmentation, color texture extraction method, integrated feature distributions, Bayes methods, image colour analysis, unsupervised natural image segmentation algorithm, Color texture feature]
Performance evaluation of a 32-nm CNT-OPAMP: Design, characteristic optimization and comparison with CMOS technology
14th International Conference on Computer and Information Technology
None
2011
In this paper, we present a simplified design, characteristic performance evaluation and comparative analysis for a Carbon Nanotube-based Operational Amplifier (CNT-OPAMP), using a circuit-compatible compact model for the intrinsic channel region of the MOSFET-like single-walled Carbon Nanotube Field-Effect Transistors (CNFETs). The study makes a simulation-based assessment on the design and performance analysis of a CNFET-based CMOS-OPAMP and compares the result to that of the conventional Silicon-based OPAMP for 32nm technology. The comparative analysis from our study shows a promising increase in operation Bandwidth by 33.8% and in Gain-Bandwidth product (GBP) by 105.7% for the CNFET-OPAMP, with a switching speed faster by over 200% and a huge reduction in power consumption by over 600 times. It also shows a considerably good noise performance with an increment of CMRR by a factor of two. The results obtained in our study suggest that the CNT-OPAMP has a promising potential for low-power high-speed applications in nanoelectronic circuits.
[Solid modeling, MOSFET, carbon nanotube-based operational amplifier, nanoelectronic circuits, Semiconductor device modeling, comparative analysis, carbon nanotubes, characteristic optimization, MOSFET-like single-walled carbon nanotube field-effect transistors, integrated circuit design, intrinsic channel region, operational amplifiers, simplified design, Silicon, Carbon nanotube field-effect transistor (CNFET), CMOS integrated circuits, silicon-based OPAMP, characteristic performance evaluation, operation bandwidth, gain-bandwidth product, 32-nm CNT-OPAMP, CNFET compact model, Carbon, simulation-based assessment, Carbon nanotube operational amplifier (CNT-OPAMP), HSPICE, CMRR, Quantum mechanics, circuit-compatible compact model, CNFET-based CMOS-OPAMP, power consumption reduction, CMOS technology, Carbon nanotube (CNT), CNTFETs]
The challenges and prospect of OpenStreetMap in Bangladesh
14th International Conference on Computer and Information Technology
None
2011
OpenStreetMap (OSM) is a world-wide campaign for developing open source maps. With the ever increase in the rate of cell phone usage, the necessity of location based services is increasing day by day and the prospect of such open source maps is very bright in commercial, educational and political points of views. Starting from the 3G mobile applications to cheap SMS-based services, we need maps everywhere. Various applications of digital maps are also seen in different national and international issues. Hence, OSM has been growing at a rapid pace in different parts of the world. However, it is a bit challenging task to develop this sort of voluntary efforts in the developing third world countries like Bangladesh. In this paper, we discuss the present conditions and initiatives taken for making OpenStreetMap a success in Bangladesh. Then we discuss about the various sectors that can be directly benefitted by the use of OSM in the context of developing countries like us and finally, we present the shortcomings and challenges that we face while developing OSM for this region.
[OpenStreetMap, Public Health, location based services, public domain software, Communities, Government, OSM, Educational institutions, digital maps, cartography, open source map development, cell phone, Global Positioning System, Diseases, Bangladesh, Volunteerism, 3G mobile applications, developing countries, Location Based Services, SMS-based services, Cities and towns, Developing Countries, Disaster Response System]
Secrecy capacity analysis in a cooperative MIMO based wireless sensor network
14th International Conference on Computer and Information Technology
None
2011
Although conventional cryptographic security mechanisms are essential to the overall problem of security, the openness of wireless medium poses both threats and opportunities for secure transmission. Information theoretic security attracts researchers for its robust nature and multiple input multiple output (MIMO) channel in the presence of multiple eavesdropper offers an unveiled research area. In this paper, information theoretic security is analyzed in the form of ergodic capacity in a wireless sensor network (WSN) scenario. Analytical calculations are performed for different channel state information (CSI) and number of antennas. Simulation results show that MIMO configurations with both CSI for main channel and eavesdropper channel are known at the transmitter shows better performance where secrecy capacity is the measure of performance.
[telecommunication security, cooperative MIMO based wireless sensor network, information theoretic security, wireless sensor networks, secrecy capacity analysis, channel state information, Indexes, secure transmission, Wireless sensor networks, multiple input multiple output channel, secrecy capacity, Lead, ergodic capacity, CSI, cryptographic security, MIMO, information theory, MIMO communication, Cooperative technique, multiple eavesdropper]
A reliable structural health monitoring protocol using wireless sensor networks
14th International Conference on Computer and Information Technology
None
2011
In this paper, we present a distributed reliable structural health monitoring (SHM) protocol using hierarchical wireless sensor networks. We assume that the sensor network consists of (i) sensors capable of sensing the structural health conditions (e.g., strain, vibration, pressure, temperature, etc.), (ii) cluster-head nodes that collect and process the sensory measurements from the sensors and prepares a local report and (iii) a sink that collects the local reports. Sensors are organized into single hop clusters, each managed by a cluster-head node. A cluster head should take the sensor-faults into account when preparing the local report based on the measurements of its subordinate sensors. In majority decision-rule for combining measurements, the judgement about health status will follow the majority where a correct majority decision requires that a majority of observing nodes provide accurate measurements. Using Baysian approach to form a judgment is problamatic without additional information or assumptions (for example, the difficulty of knowing conditional probabilities). Dempster-Shafer theory of evidence based approach overcomes these limitations. Unlike the simple binary decision, it produces a judgment value between 0 and 1 that reflects the degree of belief in that judgment. It discounts the unreliable observer's measurements. Our results through extensive simulations show the efficacy of the proposed scheme. We believe that our design might lead to the development of commercial, cost-effective, yet efficient and reliable distributed SHM systems to effectively monitor and intelligently detect structural health under various loads.
[Protocols, structural engineering, wireless sensor networks, sensory measurements, Vibration measurement, uncertainty handling, vibration, Dempster-Shafer theory of evidence, Structural Health Monitoring, temperature, structural health conditions, Monitoring, strain, single hop clusters, cluster-head nodes, pressure, Wireless Sensor Networks, distributed reliable structural health monitoring protocol, Wireless sensor networks, Bayesian approach, Atmospheric measurements, Dempster-Shafer, majority decision, Tin, Particle measurements, condition monitoring, Bayes methods, Performance, sensor faults, hierarchical wireless sensor networks]
Using catadioptric sensor to obtain image of the inner surface of a pipe and detection and analysis of faults by image processing
14th International Conference on Computer and Information Technology
None
2011
Omnidirectional image acquisition sensor has become an integral part of robotic application. In this paper, a catadioptric sensor is developed for inspection of the interior wall of a cylindrical pipe. The developed system provides 360 degree field of view and can be used as an onboard sensor of a robotic system where human intervention is not convenient or impossible. Image processing is done to detect faults in the image obtained from catadioptric sensor. A suitable hardware setup is implemented to test the inspection ability of the catadioptric sensor.
[Computers, image processing, fault diagnosis, Catadioptric sensor, fault detection, contrast stretching, cylindrical pipe interior wall, mobile robots, optical sensors, catadioptric sensors, fault analysis, mechanical engineering computing, pipe surface detection, omnidirectional image acquisition sensor, robotic application, Virtual private networks, Omnidirectional image acquisition, pipes, inspection, onboard sensor, image unwarping]
Parallel commutation of sparse linear systems on many core processor
14th International Conference on Computer and Information Technology
None
2011
In this paper, the authors describe the parallel implementation of a conjugate gradient method in a many-core system specially for solving the sparse linear systems. The new version of algorithm implementation differs from the one applied earlier [1], because it uses a special method for storing sparse coefficient matrices: only non-zero elements are stored and taken into account during computations, so that the sparsity of the coefficient matrix is taken full advantage of. Finally, A speedup of the parallel algorithm has been examined for different coefficient sparse matrices resulting in solving different physical problems.
[Linear systems, conjugate gradient methods, parallel algorithm, Sparse matrices, Graphics processing unit, algorithm implementation, sparse coefficient matrices, manycore, sparse matrix, Conjugate gradient, conjugate gradient method, Mathematical model, parallel commutation, parallel algorithms, Symmetric matrices, multiprocessing systems, coefficient matrix sparsity, PCGA, Vectors, linear systems, Equations, linear system, many core processor, sparse linear systems, sparse matrices]
Phase specific optimal treatment for cancer using GA and swarm intelligence
14th International Conference on Computer and Information Technology
None
2011
This paper presents an investigation into the development of a multi-objective optimal chemotherapy control model to reduce the number of cancer cells after a number of fixed treatment cycles with minimum side effects. A phase specific drug scheduling method using a close-loop control method with multi-objective techniques is proposed in this paper. Genetic Algorithm (GA) and particle swarm optimisation algorithm (PSO) are used to optimise the control solution for trading-off between the cell killing and toxic side effects. A close-loop control method, namely Integral-Proportional-Derivative (I-PD) is designed to control the drug to be infused into the patient's body and multi-objective GA (MOGA) and multi-objective PSO (MOPSO) are used to find suitable parameters of the controller. The proposed algorithm is implemented, tested and verified through a set of experiments. Performances of the proposed methods demonstrated that both the MOGA and MOPSO approach can offer very efficient drug scheduling that trade-off between cell killing and toxic side effects and satisfy associated design goals. It is also noted that the MOGA based method offers better performance as compared to MOPSO and can reduce the number of proliferating and quiescent cells up to 72.2% and 60.4% respectively. Future research needs to evaluate the proposed scheduling with clinical data and experiments.
[Drugs, multiobjective optimal chemotherapy control model, particle swarm optimisation algorithm, Cancer chemotherapy, particle swarm optimisation, toxic side effects, drugs, phase specific drug scheduling method, Genetic Algorithm, optimal control, genetic algorithms, Feedback control, Multi-objective optimisation, Particle Swarm Algorithm, genetic algorithm, genetics, patient treatment, integral-proportional derivative, phase specific optimal cancer treatment, cancer, Phase specific scheduling, close-loop control method, cell killing]
Survey of oppositional algorithms
14th International Conference on Computer and Information Technology
None
2011
Evolutionary algorithms (EA) are gaining popularity due to their success in solving optimization problems. New methods are being introduced continuously in the literature. One such technique, biogeography-based optimization (BBO) is proving itself as an effective algorithm for real-world problems with large number of variables. Previous work shows that augmenting BBO and other EA with opposition-based learning increases their performance, specially for high-dimensional problems. Five different oppositional algorithms have already been separately introduced in the literature. This paper combines BBO with these five oppositional methods and tests their performance on 21 benchmark problems. Furthermore, a new oppositional algorithm, fitness-ranking-based central opposition (FCB), is introduced for the first time. FCB was able to solve two of the problems that other EA could not. FCB is also determined to be the oppositional algorithm with the highest success rate.
[FCB, Biogeography-based optimization, oppositional algorithm, evolutionary algorithms, fitness ranking-based central opposition, evolutionary algorithm, evolutionary computation, opposition, optimisation, BBO augmentation, opposition-based learning, USA Councils, biogeography-based optimization, Biomedical imaging]
Electrical and optical characteristics of infrared photodetectots based on InP nanowire [photodetectots read photodetectors]
14th International Conference on Computer and Information Technology
None
2011
High speed photodetectors are most sophisticated optoelectronic devices, because it has high photo sensitivity, and allow a large wavelength range of detection as a receiver from 750 nm to 1.3-1.55 μm in the optical communication system. Since the last decade, the electrical and optical characteristics of photodetectors have been investigated to improve their performance and price. We have worked on two different type of infrared photodetectors based on nanowire. One photodetector was p-n photodiode, and the other one was p-i-n structure. We investigated the detector performance at 77K-300K temperature corresponding with wavelength in darkness and under illumination as regarding breakdown voltage, sensitivity, and quantum efficiency. We have also compared the differences between the two photodetectors performance characteristics.
[p-i-n structure, Photonic band gap, infrared detectors, indium compounds, electrical characteristics, NWs, Tides, optical characteristics, optical communication system, Temperature sensors, photodetectors, nanowires, breakdown voltage, Lighting, high speed infrared photodetector performance characteristics, Lead, optoelectronic device, optical communication, wavelength 750 nm to 1.55 mum, Stationary state, temperature 77 K to 300 K, nanosensors, InP, optical sensors, IR Photodetector, high photo sensitivity, quantum efficiency, p-n photodiode, p-i-n photodiodes, Optical sensors, III-V semiconductors, nanowire]
Performance comparison of LMS and RLS channel estimation algorithms for 4G MIMO OFDM systems
14th International Conference on Computer and Information Technology
None
2011
Multiple input multiple output (MIMO) technology allows mobile networks to obtain higher signal to noise ratio to achieves considerable performance gain. It provides the significant performance improvement for the fourth generation (4G) communication systems. This paper, we compare the performance of least mean square (LMS) and recursive least square (RLS) channel estimation (CE) algorithm for MIMO orthogonal frequency division multiplexing (OFDM) systems. The simulation results show that the RLS has better mean square error (MSE) performance compared with LMS algorithm. The RLS CE algorithm has better anti-noise as well as tracking ability. But the RLS CE algorithm suffers from higher complexity than LMS CE algorithm. In addition, when the number of receiving antenna is greater than transmitting antenna then the performance is increase significantly in both algorithms and vice versa. Furthermore, as the SNR is increases from 5 to 15dB then MSE is deceases with each iteration. Therefore, the advantage in the MSE and convergence towards true channel coefficient may be significantly useful for future mobile communications which allow broadband multimedia Internet access and wireless connection anywhere, and any time.
[Irrigation, least mean squares methods, OFDM, convergence, true channel coefficient, LMS algorithms, orthogonal frequency division multiplexing systems, Mobile communication, antinoise, MIMO OFDM systems, broadband multimedia Internet access, transmitting antenna, recursive least square algorithm, antenna arrays, recursive estimation, RLS, OFDM modulation, MIMO, MIMO communication, least mean square algorithm, MSE, multiple input multiple output technology, fourth generation communication systems, mean square error performance, mobile networks, performance gain, RLS channel estimation algorithms, Radio access networks, wireless connection, 4G mobile communication, channel estimation, LMS, receiving antenna, tracking ability]
Combined normalized and offset extended min sum decoding algorithm for LDPC codes over GF (q)
14th International Conference on Computer and Information Technology
None
2011
In this paper the combined normalized and offset extended min sum (EMS) is proposed for efficient decoding of non-binary low density parity check (LDPC) codes. EMS is the approximation of the sum product algorithm for reducing the computation complexity. However there is a performance gap between EMS and SP algorithm due to this approximation. The approximate check node messages of the extended min sum are compensated by both normalized and offset factor. The normalized factor and offset factor is determined for one iteration and is kept constant for all SNRs and iterations. It will shown in bit error rate simulation of (1008, 504) LDPC code over GF (4) that combined normalized offset extended min sum has superior performance than normalized and offset extended min sum. However, it requires only extra one multiplication for each cycle.
[iterative methods, SP algorithm, parity check codes, performance gap, nonbinary LDPC codes, normalized factor, bit error rate simulation, SNR, computation complexity reduction, Log domain decoding, Parity check codes, Extended Min Sum, error statistics, GF, approximation theory, approximate check node messages, offset factor, Non binary LDPC, nonbinary low density parity check codes, decoding, iterations, iterative decoding, OMS, NMS, Reliability, sum product algorithm, combined offset extended min sum decoding algorithm, EMS algorithm, computational complexity]
Internet gateway discovery and selection scheme in Mobile Ad Hoc Network
14th International Conference on Computer and Information Technology
None
2011
Internet gateways are used to access the Internet services from the Mobile Ad Hoc Network (MANET). Internet gateways need to be discovered and selected in an appropriate way to deliver more packets to the Internet and reduce end-to-end delay. Existing gateway discovery schemes do not scale well with the number of nodes, traffic load, and speed of the nodes in MANET. To make it scalable, we proposed a new gateway discovery and selection scheme. In our scheme, the gateways advertise gateway advertisement messages only on-demand. Moreover, it contains the advertisements within a limit in order to make our scheme scalable. We considered the interface queue length and the total number of neighbors along a route in addition to the hop count to bypass the loaded and dense route to the gateways in order to reduce the delay and packet loss. Simulation results show that our scheme scales well compared to that of other schemes.
[Internet services, Internet Gateway, mobile ad hoc network, internetworking, Re-discovery, Ad hoc networks, Internet gateway discovery, Gateway Advertisement, Guidelines, Wireless communication, Wireless sensor networks, MANET, end-to-end delay, Triggered Broadcast, traffic load, Intermediate Nodes, mobile ad hoc networks, Logic gates, Lead, Internet, Mobile computing, telecommunication traffic]
An optical flow based approach for action recognition
14th International Conference on Computer and Information Technology
None
2011
A new approach for motion-based representation on the basis of optical flow analysis and random sample consensus (RANSAC) method is proposed in this paper. Optical flow is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer (an eye or a camera) and the scene. It is intuitive that an action can be characterized by the frequent movement of the optical flow points or interest points at different areas of the human figure. Additionally, RANSAC, an iterative method to estimate parameters of a mathematical model from a set of observed data which contains inliers and outliers, can be used to filter out any unwanted interested points all around the scene and keep only those which are related to the particular human's motion. By this manner, the area of the human body within the frame is estimated and this rectangular area is segmented into a number of smaller regions or blocks. The percentage of change of interest points in each block from frame to frame is then recorded. Similar procedure is repeated for different persons performing the same action and the corresponding values are averaged for respective blocks. A matrix constructed by this strategy is used as a feature vector for that particular action. Afterwards, for the purpose of recognition using the extracted feature vectors, a distance-based similarity measure and a support vector machine (SVM)-based classification technique have been exploited. From extensive experimentations upon a standard motion database, it is found that the proposed method offers not only a very high degree of accuracy but also computational savings.
[iterative methods, image classification, RANSAC method, Motion-based representation, motion-based representation, apparent motion pattern, SVM, SVM-based classification technique, feature vector extraction, feature extraction, image segmentation, parameter estimation, rectangular area segmentation, image sequences, Lifting equipment, support vector machines, standard motion database, filtering theory, optical flow points, visual scene, optical flow, iterative method, image motion analysis, matrix algebra, Support vector machines, RANSAC, support vector machine, image representation, action recognition, mathematical model, Feature extraction, distance-based similarity measure, random sample consensus method, optical flow analysis]
PAPR distribution analysis of OFDM signals with Partial Transmit Sequence
14th International Conference on Computer and Information Technology
None
2011
Third Generation Partnership Project (3GPP) LTE has adopted OFDMA as the uplink multiple access scheme. One of the major drawbacks is the high PAPR. Not only is the performance of PAPR with PTS technique influenced by the number of subblocks and the phase vector but also by the subblock partitioning. The Partial Transmit Sequence (PTS) technique suffers from the search complexity of finding the optimum set of phase vectors. We propose a suboptimal combination algorithm that reduces the search complexity. The number of commutations in the suboptimal combination algorithm is much lower than the required by the original PTS technique. In this paper, we propose a suboptimal combination algorithm to reduce the searching complexity of finding the optimum set of vectors to minimize PAPR. The performance of PAPR utilizing the PTS technique improves by the use of the proposed suboptimal combination algorithm.
[3G mobile communication, suboptimal combination algorithm, Peak to average power ratio, sequences, Long-term-evolution (LTE), partial transmit sequence, PTS technique, Lead, OFDM modulation, PAPR distribution analysis, OFDM signals, OFDMA, peak-to-average power ratio (PAPR), Long Term Evolution, Fading, phase vector, Partial Transmit Sequence (PTS), frequency division multiple access, 3G partnership project, 3GPP, Orthogonal frequency division multiplexing (OFDM), Frequency division multiaccess, Stimulated emission, Adaptive optics, uplink multiple access scheme, LTE, subblock partitioning]
Healthcare system's operational security
14th International Conference on Computer and Information Technology
None
2011
Computer Security is a concept of protection designated to information system to accommodate the fundamental security objectives for data, information and computer services [1]. The goal of this research paper is to implement a security plan for the Lawson healthcare organization to assist with protecting and providing a secure information transmission and exchange of public and private data. The current healthcare security system is outdated and it does not protect public or private data exchange across the network. There are IT personnel who are uneducated on the security mechanism to prevent security risks within the daily operations of the healthcare systems. The development of new operational security architecture is to reduce threats and educate the IT personnel of techniques to improve the outdated security mechanism being used in the Healthcare organization. We have used Lawson healthcare as a case study; however, this knowledge can be applied to any healthcare organization.
[data exchange, Medical services, computer service, private data, Lawson healthcare organization, database management, Databases, data service, security risks, Healthcare organization, health care, information system, public data, security risk, security objective, secure information transmission, cryptography, medical information systems, healthcare system operational security, PKI, security mechanism, computer security, security of data, Standards organizations, Public key, Organizations, operational security architecture, information service]
A novel technique of integrating the conventional algorithms for broadcasting protocols to reduce the redundancy in wireless ad-hoc networks
14th International Conference on Computer and Information Technology
None
2011
The broadcasting protocols for transmission from peer to peer ad-hoc networks are currently a topic of enormous interest. There are several traditional algorithms which deal with the transmission of information from one source to another in the wireless ad-hoc networks. However, these conventional algorithms are not capable of eliminating the data redundancy completely. This study proposes a new technique of merging the traditional algorithms to absolutely eradicate the data redundancy problem in such networks. The customary algorithms such as the counter, distance and location based schemes only provide with a generalized methods of transmitting and reboradcasting information from one node to another but do not address the redundancy problem. The proposed method integrates the counter and location based schemes with the distance based method respectively and compares their performances to get rid of the unnecessary re-transmission of information .
[algorithms, peer-to-peer computing, Radiation detectors, Peer to peer computing, broadcasting protocols, Redundancy, Merging, Receivers, Ad hoc networks, broadcast communication, ad-hoc networks, peer to peer ad-hoc networks, Broadcasting, wireless channels, conventional algorithms, wireless ad-hoc networks, redundancy, ad hoc networks, protocols, rebroadcast]
Signature hiding and recovery in a graph coloring solutions using modified genetic algorithm
14th International Conference on Computer and Information Technology
None
2011
Intellectual Property Protection or IPP is an act of protecting a digital product by embedding an author's signature into the object in the form of minute errors. This signature will be the proof of ownership. However, this technique cannot be directly applied to protect intellectual properties such as solutions to hard problems which must maintain the correct functionality. This paper proposes a constraint-based watermarking technique and lays out a theoretical framework to evaluate this type of technique. Based on this framework, we analyze previously proposed two watermarking techniques for the graph coloring problem because of its theoretical importance in complexity theory and numerous applications in real life. This newly proposed algorithm is the another way to protect graph coloring solution from an intruder by coloring a graph using Modified Genetic Algorithm. We used those colored partitions for the purpose of embedding signature into the graph, so that the developer of the solution can only unhide the signature message from the graph to claim his/her authorship.
[complexity theory, industrial property, MSPGCA, proof-of-ownership, IPP, genetic algorithms, graph colouring, constraint-based watermarking technique, watermarking, Chromatic Number, signature hiding, GCP, authorisation, graph coloring problem, intellectual property protection, digital product protection, digital signatures, GA, signature recovery, modified genetic algorithm, computational complexity]
Image steganography using 24-bit bitmap images
14th International Conference on Computer and Information Technology
None
2011
Steganography is a method of hiding data in a seemingly innocent medium, like an image, a sound or a video clip, which doesn't arise any suspicion as it travels from the sender to the receiver through public communication channels. In our proposed algorithm, we have tried to implement Steganography by hiding data in the pixel which occurs the most in a 24-bit bitmap image. First we calculate which pixel value is most frequent. We then modify only those pixels to hide the data using simple LSB modification and write them to the output file.
[data hiding, LSB modification, Steganography, Pixel Intensity, least significant bit, 24-bit Bitmap Image, Transform coding, steganography, image steganography, pixel value, image coding, 24-bit bitmap image, Biomedical imaging]
Improve the quality of supervised discretization of continuous valued attributes in data mining
14th International Conference on Computer and Information Technology
None
2011
Dealing with continuous-valued attributes is an important data mining problem that has effects on accuracy, complexity, and understandability of the mining algorithms. This paper presents a new approach for dealing with continuous attributes that improve the quality of discretization as a preprocessing step for decision tree and nai&#x0308;ve Bayesian classifier. The proposed approach focus on supervised discretization, however, unsupervised discretization can also be applied in the same way. It finds the possible cut points with the attribute values of continuous attribute that can separate the class distributions, and then consider the best cut point as an interval border with information gain heuristic and Bayesian classifier. The proposed approach has been tested by comparing with other discretization methods on a number of benchmark problems from UCI machine learning repository. The experimental results proved that the proposed approach for discretization of continuous attributes improves the quality of discretization.
[pattern classification, Cut Points, mining algorithm complexity, data mining, supervised discretization quality, continuous valued attributes, nai&#x0308;ve Bayesian classifier, mining algorithm accuracy, mining algorithm understandability, Interval Border, decision tree, class distribution separation, interval border, decision trees, UCI machine learning repository, information gain heuristic, Bayes methods, unsupervised discretization, Information Gain, Bayesian Classifier]
A simulation study on some parametric confidence intervals for the difference of means of two skewed populations
14th International Conference on Computer and Information Technology
None
2011
Skewed data are common in the Biosciences and in life testing modeling. In this paper, we evaluate several existing techniques and propose some new techniques for estimating the differences of means of two skewed populations. A simulation study has been made to compare the performance of the selected intervals and a real life example has been considered to illustrate the applications of the techniques. We believe the findings of this paper will be helpful for different health researchers those are interested to choose the proper techniques for estimating the differences of means for two skewed populations.
[Positively skewed, biology, Biological system modeling, means difference, simulation, skewed data, simulation study, parametric confidence interval, Primary hypertension patients, Analytical models, Biometric variable, Coverage probability, bioscience, life testing modeling, skewed population, Statistical inference, statistical analysis, Monte Carlo simulation, Polymorphism]
Fast algorithms for finding patterns in indeterminate and Arc-Annotated sequences
14th International Conference on Computer and Information Technology
None
2011
In this paper, we present efficient algorithms for finding indeterminate Arc-Annotated patterns in indeterminate Arc-Annotated references. Our algorithms run in O(m + mn/w) time where n and m are respectively the length of our reference and pattern strings and w is the size of our target machine word size. Here we have assumed the alphabet size to be constant, because, indeterminate Arc-Annotated sequences are used to model biological sequences. Clearly, for short patterns, our algorithms run in linear time and efficient algorithms for matching short patterns to reference genomes have huge applications in practical settings. We also perform some preliminary experiments that suggest that our algorithms run very fast in practice.
[pattern matching, String matching, Short pattern, indeterminate arc-annotated reference, indeterminate arc-annotated pattern, target machine word size, alphabet size, linear time algorithm, biological sequence model, biology computing, Degenerate sequence, DNA, arc-annotated sequence, genomics, string matching, Arc-Annotated sequence, reference genomes, computational complexity, pattern string, Indeterminate sequence]
New adaptive nonlinear compensator
14th International Conference on Computer and Information Technology
None
2011
A new adaptive compensator was proposed to be applied for compensating the nonlinear distortion of memoryless nonlinear systems with saturation characteristics. The proposed adaptive compensator can have faster convergence speed and better capability of compensating the nonlinear distortion than the conventional Volterra compensator, with nearly equal complexity of computation.
[complexity, adaptive nonlinear compensator, convergence, nonlinear compensator, adaptive control, Complexity theory, memoryless nonlinear systems, memoryless systems, saturation characteristics, Volterra compensator, adaptive, nonlinear distortion, nonlinear control systems, Three dimensional displays, nonlinear distortion compensation, computational complexity]
Dual iris based human identification
14th International Conference on Computer and Information Technology
None
2011
In this paper, a dual iris based human identification system that increases the accuracy and the performance of a typical human iris recognition system is proposed. The system detects and then isolates and extracts the iris region from eye image. It then sets the radial and angular resolution for the extracted iris region and maps the circular region into rectangular polar coordinates according to the resolutions. This polar image is then convolved with 1D log Gabor filter and phase of the response is quantized to four levels to generate the binary iris template and its corresponding mask. The Hamming distance between two iris templates is computed to find out if the templates are generated from the same iris or not. The conventional method measures this Hamming Distance using iris images from a single eye. However, the proposed method takes images from both eyes simultaneously for comparison process which shows an increased accuracy and performance. Using the proposed method CASIA Iris database V3, false positive rate and false negative rate were found to be 0% and 9.96% respectively while the overall accuracy was 99.92%.
[Biometrics, CASIA iris database, false positive rate, false negative rate, polar image, dual iris based human identification system, iris region extraction, feature extraction, binary iris mask, Gabor filters, rectangular polar coordinates, image resolution, human iris recognition system, Iris recognition, Image resolution, Hamming distance, ROC, radial resolution, angular resolution, Image segmentation, Log Gabor filter, eye image, circular region, 1D log Gabor filter, binary iris template, iris recognition]
An Efficient and cost effective maximum clique analysis based approximation in military application of Wireless Sensor Network
14th International Conference on Computer and Information Technology
None
2011
Military Applications of Wireless Sensor Network in domains of maximizing security and gaining maximum benefits from attacking the opponent is a challenging and prominent area of research now-a-days. A commander's goal in a battle field is not limited by securing his troops and the country but also to deliver proper commands to assault the enemies using the minimum number of resources. In this paper we propose, for the first time, an efficient and low cost approximation algorithm using the maximum clique analysis that finds the strategy of maximizing the destruction, in a battlefield, to defeat the opponents with minimum resources. Experimental results show the effectiveness of the proposed algorithm in the prescribed areas of applications and the impacts of different parameters used on its performance measures.
[approximation theory, wireless sensor networks, cost effective maximum clique analysis based approximation, low cost approximation algorithm, wireless sensor network, maximum destruction, Approximation methods, performance measures, Jamming, maximum clique analysis, Wireless communication, Temperature sensors, Wireless sensor networks, security, Wildlife, military application, military communication, Monitoring]
A novel night mode algorithm for image capturing devices using fuzzy logic
14th International Conference on Computer and Information Technology
None
2011
Nowadays most digital image capturing devices uses night mode for capturing image at night. In this paper we proposed an algorithm for night mode using fuzzy logic which is simple but effective. This algorithm enhances brightness of night color image, preserves hue and enhances saturation to bring more details. For further enhancement of an image the algorithm can be used recursively. Experimental results of numerous night images show that the algorithm works efficiently for night mode.
[image processing, Visualization, night mode, saturation enhancement, digital image capturing device, fuzzy logic, brightness enhancement, image processing equipment, Equations, membership function, image enhancement, night color imaging, night mode algorithm, image colour analysis, hue preservation]
A corner detection method using angle accumulation
14th International Conference on Computer and Information Technology
None
2011
In this paper a Contour based corner detector that provides robust corner locations against different image transformations has been proposed. The presented corner detector is the combination of one of the most outperformed corner detectors named CPDA and the fast and efficient high curvature point detector IPAN99. The combination has been done in such a way that the weaknesses from both of the algorithms are eliminated and thus a promising method has come out. A comprehensive performance evaluation of the proposed detector has been performed by using two metrics Average Repeatability and Localization Error. The experimental results demonstrate the effectiveness of the proposed approach among seven other existing contour based corner detectors.
[corner detection method, contour-based corner, IPAN99, image transformations, corner evaluation, computational geometry, performance evaluation, localization error metric, computer vision applications, object detection, Equations, corners, angle accumulation, CPDA, robust corner locations, computer vision, curvature point detector, contour based corner detector, edge detection, average repeatability metric]
Minimized reversible/quantum synthesis of non-reversible quinary logic function
14th International Conference on Computer and Information Technology
None
2011
Reversible/quantum multiple-valued logic circuit has several advantages over reversible/quantum binary logic circuit. Galois field sum of products (GFSOP) based synthesis of multiple-valued logic function is more promising and practical than other approaches. In this paper, we have developed 196 Galois field expansions (GFE) and have proposed a method of minimization of GFSOP expression for non-reversible quinary logic function using the application of these GFEs. We have also proposed a method of realization of quinary GFSOP expression as cascade of quinary reversible/quantum gates. Experimental results with 26 functions having up to six inputs and two outputs show that a significant minimization can be achieved using the proposed minimization method.
[minimization method, reversible logic, GFSOP expression, GFSOP minimization, Galois field expansion, GFSOP synthesis, GFSOP based synthesis, reversible binary logic circuit, Quantum computing, quinary logic, multivalued logic, nonreversible quinary logic function, multiple-valued logic circuit, multiple-valued logic function, quantum gates, reversible synthesis, quantum synthesis, quantum logic, Galois field sum of products, logic gates, multivalued logic circuits, quantum binary logic circuit, quinary reversible gate, minimisation, logic design]
Incorporation of dynamic parameters in hybrid feature-based Bangla phoneme recognition using multilayer Neural Networks
14th International Conference on Computer and Information Technology
None
2011
This paper presents a Neural Network-based Bangla phoneme recognition method for Automatic Speech Recognition (ASR). The method consists of three stages: at first stage, a multilayer neural network (MLN) converts acoustic features, mel frequency cepstral coefficients (MFCCs), into phoneme probabilities, where the second stage computes dynamic (velocity (&#x0394;) and acceleration (&#x0394;&#x0394;)) parameters from the phoneme probabilities by using three point linear regression (LR). Finally, the phoneme probabilities, dynamic parameters, &#x0394; and &#x0394;&#x0394;, and the input MFCCs, combined as hybrid features, are fed into a hidden Markov model (HMM) based classifier to obtain more accurate phoneme strings. From the experiments on Bangla speech corpus prepared by us, it is observed that the proposed method provides higher phoneme recognition performance than the existing method. Moreover, it requires a fewer mixture components in the HMMs.
[hidden Markov model, regression analysis, HMM based classifier, phoneme probabilities, Bangla speech corpus, automatic speech recognition, dynamic parameters, phoneme strings, hidden Markov models, Dynamic Parameters, speech recognition, acoustic feature conversion, cepstral analysis, acceleration parameters, Hidden Markov Model, Context, multilayer perceptrons, Automatic Speech Recognition, natural language processing, probability, signal classification, hybrid feature-based Bangla phoneme recognition, Mel Frequency Cepstral Coefficients, multilayer neural networks, mel frequency cepstral coefficients, three point linear regression, Multilayer Neural Network]
An edge-texture based moving object detection for video content based application
14th International Conference on Computer and Information Technology
None
2011
This paper presents a moving-object segmentation algorithm using texture information along the edge segment. The proposed method is developed to address challenges due to variations in ambient lighting and background contents. We investigated the suitability of the proposed algorithm in comparison with the traditional edge-pixel-based and edge-segment-based detection methods. In our method, edges are extracted from each frame of a video sequence and are represented as segments using an efficiently designed edge class. In addition we maintain the underlying texture information with a newly proposed texture descriptor Local Directional Pattern (LDP). LDP feature is generated by comparing a pixel's edge response in eight directions. LDP texture extracted along the edge region to identify moving edge segment using three most recent frames. This fusion of texture and edge segment information helps to obtain the geometric information of edge in the case of edge matching. Detected moving edges are utilized along with watershed algorithm for extracting video object plane with more accurate boundary. Experiment results with real image sequence reflect that the proposed method is suitable than other existing approaches.
[video content based application, local directional pattern feature, watershed algorithm, image fusion, Manuals, edge extraction, object detection, edge-texture based moving object detection, Moving object detection, edge-segment-based detection method, edge segment, edge matching, feature extraction, image segmentation, edge detection, background content, video object plane, video signal processing, image sequences, edge-pixel-based detection method, segment information fusion, texture information, moving-object segmentation algorithm, texture descriptor, video sequence, lighting, image matching, image texture, image motion analysis, ambient lighting, image representation, local directional pattern, segment representation, Kirsch edge detector, LDP feature, texture analysis]
Compact printed ultra-wideband antenna with dual band- notch characteristics
14th International Conference on Computer and Information Technology
None
2011
A compact printed planar ultra-wideband antenna with dual band-notched is proposed. The proposed antenna consists of a rectangular radiating patch and a modified partial ground plane. Two rectangular parasitic strips are placed below the substrate to achieve dual band-notched characteristics. The measured results show that the proposed antenna achieved a wide impedance bandwidth from 2.87 to more than 11 GHz, defined by -10 dB return loss with one notch frequency band at 3.3 to 3.8 GHz and the other at 5.1 to 5.6 GHz. Compared to the other designs, the proposed antenna has a simple structure to achieve the notch-band characteristics to reduce the potential interference between UWB and existing WiMAX and WLAN systems. A stable radiation pattern and flat gain except in the notched bands makes the proposed antenna suitable for being used in UWB applications.
[dual band-notch characteristics, wide impedance bandwidth, ultra wideband antennas, rectangular radiating patch, UWB applications, notched bands, compact printed planar ultra-wideband antenna, band notch, antenna radiation patterns, WLAN systems, substrate, flat gain, rectangular parasitic strips, stable radiation pattern, WiMAX, modified partial ground plane, WLAN, microstrip antennas, Radio access networks, Ultra-wideband, multifrequency antennas, compact printed ultra-wideband antenna, interference, Antennas, dual band-notched characteristics]
Hybrid neural network ensemble construction combining boosting and negative correlation learning
14th International Conference on Computer and Information Technology
None
2011
An ensemble of several neural networks is a convenient way to achieve better performance for a classification task. A number of methods on the basis of different techniques have been investigated for neural network ensemble (NNE) construction from early 1990s. To achieve better performance, a few hybrid NNE methods combining different individual methods are also investigated recently. This paper also presents a hybrid ensemble construction method combining boosting and negative correlation learning (NCL). The proposed method first produces a pool of predefined number of networks using standard boosting and NCL, and then genetic algorithm is used to the task of selecting an optimal subset of networks for an NNE from the pool. The proposed method builds problem-dependent adaptive NNE and shows consistently better performance with concise ensemble over the conventional methods when tested on a suite of 20 benchmark problems.
[pattern classification, Artificial neural networks, Boosting, boosting algorithm, genetic algorithms, classification, network selection, genetic algorithm, negative correlation learning, diversity, generalization, neural network ensemble, hybrid neural network ensemble construction, Genetics, learning (artificial intelligence), Bagging, neural nets]
An improved heuristic algorithm for sorting genomes with inverted block-interchanges
14th International Conference on Computer and Information Technology
None
2011
A classic problem in comparative genomics is finding sequence of evolutionary operations that transform one genome into another. Analysis of genome evolving by different types of genome rearrangement operators such as reversals, transpositions, translocations, block interchange, double cut and join(DCJ) etc. leads to a shortest sequence of different types of operations that sorts one genome into another. In this paper we consider reversals and block-interchanges simultaneously and incorporate inverted block-interchange, which inverts one or both of two swapped segments of a block-interchange. Experimental results show that using inverted block-interchange, previous heuristic algorithm finds better or equal sorting sequence.
[reversal operators, Barium, Heuristic algorithms, genome rearrangement operator, Genomics, genome sorting sequence, comparative genomics, heuristic algorithm, evolutionary computation, genome rearrangement genome sorting, biology computing, sorting permutations, block-interchange operators, sorting, inverted block-interchange, genomics, evolutionary operation sequence, inverted block interchange]
ThirdHand: A novel and cost-effective HCI for physically handicapped people
14th International Conference on Computer and Information Technology
None
2011
In this paper a HCI, namely ThirdHand, has been developed especially for physically handicapped people who can at least rote their heads and lips. This system focuses on the design of a head-operated headset that uses two visual light emitting diodes (LEDs) with different colors and a lips-operated push switch. A webcam attached to the computer captures the light of LEDs and translates them into the movement and clicking operations of the mouse pointer on the screen. One of its most important advantages is its accuracy of the user-friendliness. Another advantage is that it requires bellow $1 to produce the hardware parts - comparatively lower than many HCI devices developed for the same functionality.
[Computers, mouse pointer clicking operation, handicapped aids, head-operated headset, lips-operated push switch, Head, Switches, switches, Light emitting diodes, helmet mounted displays, LED, light emitting diodes, Web cam, user-friendliness, HCI, mouse controllers (computers), physically handicapped people, Heating, Lighting, ThirdHand HCI, human computer interaction, mouse pointer, handicapped people, clicking operations]
A single sign on mechanism for multiple grid manager on Alchemi .NET based grid framework
2012 15th International Conference on Computer and Information Technology
None
2012
Grid computing has emerged as an important field synonymous to high throughput computing. Grid computing is widely regarded as a technology of immense potential for accessing geographically distributed resources using computer networks. To ensure this new paradigm fully functional it demands several key concepts like authentication and authorization. The most commonly used security specification for grid computing is global security infrastructure or GSI. It specifies all the security aspect relating grid computing like authentication, authorization, single sign on and delegation [8]. Among all the required security features of grid computing &#x201C;single sign on (SSO)&#x201D; is a crucial one. But single sign on feature is missing in Alchemi .NET framework, a middleware to setup grid in windows based operating system. The architecture of Alchemi .NET framework does not possess single sign on feature which is a major security related architectural issue that exixts in current alchemi system. In this paper we have proposed a mechanism to implement &#x201C;single sign on&#x201D; integrating with Alchemi .NET framework. In current Alchemi .NET framework's architecture, an user connects with Alchemi grid manager directly by giving username &amp; password via Alchemi console. In our proposal a user has to go through the single sign on mechanism to be connected with grid manager via Alchemi console to submit a grid application.
[Alchemi.NET based grid framework, Windows based operating system, high throughput computing, grid computing, authentication concept, Grid Computing, computer network, Grid Security, Alchemi grid manager, Alchemi, authorization concept, authorisation, security specification, operating systems (computers), SSO security, Single Sign On, GSI, single sign on mechanism, middleware]
Gender effect cannonicalization for Bangla ASR
2012 15th International Conference on Computer and Information Technology
None
2012
This paper presents a Bangla (widely used as Bengali) automatic speech recognition system (ASR) by suppressing gender effects. Gender characteristic plays an important role on the performance of ASR. If there is a suppression process that represses the decrease of differences in acoustic-likelihood among categories resulted from gender factors, a robust ASR system can be realized. In the proposed method, we have designed a new ASR incorporating the Local Features (LFs) instead of standard mel frequency cepstral coefficients (MFCCs) as an acoustic feature for Bangla by suppressing the gender effects, which embeds three HMM-based classifiers for corresponding male, female and geneder-independent (GI) characteristics. In the experiments on Bangla speech database prepared by us, the proposed system has achieved a significant improvement of word correct rates (WCRs), word accuracies (WAs) and sentence correct rates (SCRs) in comparison with the method that incorporates Standard MFCCs.
[gender effect cannonicalization, SCR, hidden Markov model, WCR, gender characteristic, WA, acoustic likelihood, automatic speech recognition, Mel frequency cepstral coefficient, hidden Markov models, sentence correct rate, speech recognition, gender effect sppression, MFCC, acoustic feature, natural language processing, word correct rate, signal classification, Bangla speech database, HMM-based classifier, automatic speech recognition system, local feature, acoustic model, word accuracy, gender effects suppression, gender issues, Bangla ASR]
Scalability and performance analysis of CRUD matrix based fragmentation technique for distributed database
2012 15th International Conference on Computer and Information Technology
None
2012
Distributed processing is an efficient way to improve performance of a database management system significantly. Distribution of data involves fragmentation, replication and allocation process. Previous research works provided fragmentation solution based on empirical data which are not applicable at the initial stage of a distributed database. In this paper we have presented a fragmentation technique that can be applied at the initial stage when no experimental data are present as well as in later stages of a distributed database system for partitioning the relations. Scalability of our proposed technique also investigated for different situation those may arise in practical cases of a distributed database. Experimental results show that our technique can solve initial fragmentation problem of distributed database system properly also compete with other non initial fragmentation techniques quite good in later stages.
[distributed database, initial fragmentation, relation partitioning, database management system, allocation, data distribution, distributed processing, allocation process, replication process, CRUD matrix based fragmentation technique, fragmentation solution, matrix algebra, attribute locality precedence, scalability analysis, distributed databases, MCRUD matrix, performance analysis]
Determination of the effect of having energy drinks on respiratory and heart function analyzing blood perfusion signal
2012 15th International Conference on Computer and Information Technology
None
2012
In this work, we evaluate the effect of having energy drinks (ED) using laser doppler flowmetry (LDF) technique by analyzing blood perfusion and ECG signal after having energy drinks on different healthy human subjects. After having energy drinks, it is observed that the amplitude of blood perfusion signal increases around two fold. Further analyzing frequency spectrum of the blood perfusion signal using Fast Fourier transform, we have determined the effect of having energy drinks on respiratory and heart function. A significant change in heart activity after having energy drinks has been observed. The amplitude of frequency spectrum of LDF signal related to heart activity increase around three fold. The amplitude of ECG signal and amplitude of frequency spectrum also increase in response to having energy drinks. A little change is observed in respiratory activity as the amplitude of frequency spectrum of LDF signal corresponding to respiratory activity increases around 1.5 times after having energy drinks.
[fast Fourier transforms, heart function, respiratory function, energy drink, fast Fourier transform, electrocardiography, LDF, laser doppler flowmetry, medical signal processing, frequency spectrum amplitude, blood perfusion signal analysis, blood flow measurement, AcqKnowledge software, LDF technique, heart activity, ECG signal, Blood perfusion signal, Energy drinks]
An efficient grid algorithm for faster clustering using K medoids approach
2012 15th International Conference on Computer and Information Technology
None
2012
Clustering is the methodology to separate similar objects of data set in one cluster and dissimilar objects of data set in another cluster. K means and K medoids are most widely used Clustering algorithms for selecting group of objects for data sets. k means clustering has less time complexity than k medoids method, but k means clustering method suffers from extreme values. So, we have focused our view to k medoids clustering method. Conventional k-medoids clustering algorithm suffers from many limitations. We have done analysis on these limitations such as the problem of finding natural clusters, the dependency of output on the order of input data. In this paper we have proposed a new algorithm named Grid Multidimensional K medoids which is designed to overcome the above limitations and provide a faster clustering than K medoids.
[data mining, Dataset, Grid, time complexity, Outlier, dissimilar objects, grid algorithm, K medoids approach, pattern clustering, Partitioning, clustering algorithms, grid multidimensional K medoids, k-medoids clustering algorithm, k means clustering method, k medoids clustering method, Medoid, Time complexity, computational complexity]
A tracking based BEM algorithm for OFDMA channel estimation in high Doppler spread
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper, we investigate the Channel Impulse Response (CIR) estimation in an Orthogonal Frequency Division Multiple Access (OFDMA) uplink using a tracking based Basis Expansion Modeling (BEM) algorithm. By introducing a new tracking term in the BEM coefficients that gives the rate of change of the coefficients, the algorithm is particularly suitable for high mobility application. Specifically, the algorithm estimates the BEM coefficients for each OFDMA block in an iterative manner based on using a new objective function that takes into consideration first order variations in the coefficients of the current and adjacent blocks.
[radio links, mobility application, iterative methods, channel impulse response estimation, frequency division multiple access, Orthogonal Frequency Division Multiple Access (OFDMA), iterative manner, Basis Expansion Modeling (BEM), orthogonal frequency division multiple access uplink, mobility management (mobile radio), tracking based BEM algorithm, Doppler shift, tracking based basis expansion modeling algorithm, channel estimation, OFDMA channel estimation, Channel estimation, high Doppler spread, OFDMA block, OFDM modulation, CIR estimation, OFDMA uplink, objective function]
Performance analysis of an LDPC coded FSO communication system with different modulation technique under turbulent condition
2012 15th International Conference on Computer and Information Technology
None
2012
Free-space optical (FSO) communication is an attractive and cost-effective solution for high-rate image, voice, and data transmission than an RF channel. Atmospheric turbulence-induced fading is one of the main impairments affecting FSO communications. To design a high performance communication link for the atmospheric FSO channel, it is of great importance to characterize the link with proper model. Different modulation techniques have already been proposed for FSO communication in various publications. In this paper, an analytical approach is presented to evaluate the bit error rate performance of an LDPC coded FSO communication employing on-off keying (OOK), binary phase-shift keying (BPSK) and Q-ary pulse position (QPPM) as modulation technique. The performance results are evaluated in terms of bit error rate (BER). It is found that LDPC coded FSO system with QPPM provides significant coding gain over uncoded system compare than other modulation techniques.
[modulation, parity check codes, atmospheric turbulence induced fading, Bit error rate (BER), high performance communication link, phase shift keying, bit error rate performance, optical communication, binary phase shift keying, LDPC coded FSO communication system, free-space optics (FSO), error statistics, on off keying, optical links, Q-ary pulse position, binary phase-shift keying (BPSK), on-off keying (OOK), turbulent condition, amplitude shift keying, free space optical communication, Q-ary pulse position modulation (QPPM), low density parity check (LDPC) code, atmospheric FSO channel, coding gain, data transmission, performance analysis]
High performance decision based median filter for salt and pepper noise removal in images
2012 15th International Conference on Computer and Information Technology
None
2012
A high performance decision based median filter is proposed for removal of salt and pepper in images. It is an enhanced Adaptive Switching Median filter which initially detects noise pixels iteratively through several phases and replaces the noisy pixels with median value. It calculates median value without considering noisy pixels to improve the performance of median filter for high density noise. Detection of noise is done by expanding the mask until 7&#x00D7;7 to maintain local information extraction. Moreover, the processing pixel is replaced by last processed pixel if the algorithm fails to detect noise free pixel at 7&#x00D7;7. If the noise free median value is not available at 7&#x00D7;7 processing window, the last processed pixel take into consideration if it is noise free. If the last processed pixel is noisy, the algorithm select a window size with 15&#x00D7;15 dimension and calculate the number of 0's and 255's in the processing window. Then replace the processing pixel with 0 or 255 which is more in number in the selected window. Experiment result shows that it can provide very high quality restored images for images that are contaminated by &#x201C;salt &amp; pepper&#x201D; noise, especially when the noise density is large.
[iterative methods, median filters, processing pixel, processing window, local information extraction, noise free median value, high density noise, Adaptive Switching Median filter, high performance decision based median filter, image denoising, image noise removal, noise density, salt-and-pepper noise removal, image restoration, enhanced adaptive switching median filter, Salt and Pepper Noise, iterative noise pixel detection, image contamination]
Night mode face recognition using adaptively weighted sub-pattern PCA
2012 15th International Conference on Computer and Information Technology
None
2012
The face recognition problem is made difficult by the great variability in head rotation and tilt, lighting intensity and angle, facial expression, aging, partial occlusion (e.g. Wearing Hats, scarves, glasses etc.), etc. Principal components from the face space are used for face recognition to reduce dimensionality of database images. However, this paper discusses on adaptively weighted sub-pattern PCA (Aw-SpPCA) based face recognition system for dark images that have captured at night. It is really difficult to capture good quality picture at night for lacking of light source with traditional acquisition devices like camera or mobile phone. The computational photographic concepts have been applied to enhance the quality of the capture images at night automatically. Multi-scale retinex color restorations (MSRCR) technique has been applied for overcome this problem. Moreover, for recognition phase of this propose method, unlike PCA based on a whole image pattern, Aw-SpPCA operates directly on its sub patterns partitioned from an original whole pattern and separately extracts features from them. Aw-SpPCA can adaptively compute the contributions of each part and then endows them to a classification task in order to enhance the robustness to both expression and illumination variations. Experimental results show that the proposed method is competitive.
[Night mode face recognition, night mode face recognition, image classification, dark image, expression variation, FAUD, lighting, MSRCR technique, classification task, PCA, dimensionality reduction, multiscale retinex color restoration, illumination variation, face recognition, adaptively weighted subpattern PCA, database image, image colour analysis, Aw-SpPCA, principal component analysis, MSRCR]
A novel 3-Layer user authentication system for remote accessibility
2012 15th International Conference on Computer and Information Technology
None
2012
User authentication through password matching is an age-old issue. It has been popularly being used in the computing world for its simplicity, flexibility and remote accessibility. Although people later developed and deployed some other authentication systems like Biometrics Authentication and Token-Based Authentication; despite proving higher degree of security, they all suffer from an orthodox problem-remote accessibility to an Internet-Based System. Again, for remote access, the general trend of using textual passwords is not guaranteed to be highly secured and, most often, they are seen breached by the intruders using some common password breaking algorithms. Hence, a more reliable, robust, secured and allover simple authentication system for remote accessibility is yet needed in digital world. In this paper, we propose a 3-Layer user authentication system for remote access of Internet-based systems that is guaranteed to be more secured, robust and reliable as compared to its existing counterparts. Besides, the proposed system ensures flexibility, reduced complexity and simplicity as well.
[password breaking algorithm, textual password, Robust LogIn Mechanism, token-based authentication, system flexibility, remote accessibility, Graphical-Textual Based Authentication, User Authentication System, security, 3-layer user authentication system, system complexity, system simplicity, authorisation, Secured Remote Accessibility, Internet, password matching, orthodox problem, Internet-based system, Layerd Authentication Architecture, biometrics authentication]
Issues in implementing electronic governance: Bangladesh perspective
2012 15th International Conference on Computer and Information Technology
None
2012
This paper focuses on various approaches of implementing electronic governance in developing countries and explores the specific factors related to the challenges and opportunities in implementing electronic governance. Since, the models for implementation of electronic governance greatly varies because of certain technical and socio-economic aspects, it is essential to analyze the features related to implementing electronic governance not as a whole rather on a specific basis. As a developing country with lots of limitations like extreme shortage of resources, limitations in financing, absence of proper development planning, lack of skilled human resources, unavailability of stable and fair democracy and more importantly, a number of unavoidable circumstances including natural disasters, it is a must to analyze the concerns. In this paper, we especially present the adaptability of e-governance in the prime sectors of government for developing countries. Moreover, we provide specific recommendations for implementing e-governance in the most feasible, cost-effective, and efficient manner. Analyzing the conducted survey result through statistical procedures also derives a couple of significant factors in implementing e-governance. This paper also aims to point the possible solutions in handling the barriers to implement electronic governance. The supporting framework for integrating the overall socio-economic activities under the information and communication technology framework is also conveyed in this paper.
[financing limitation, electronic governance, statistical procedure, E-Administration, E-Governance Framework, Opportunities, prime sector, development planning, e-governance, Implementation of E-Governance Challenges, resource shortage, technical aspect, Bangladesh perspective, socio-economic aspect, public administration, natural disaster, socio-economic activity, information and communication technology, fair democracy, skilled human resource, socio-economic effects, statistical analysis, ICT Infrastructure]
A novel architecture for nanometer scale low power VLSI design
2012 15th International Conference on Computer and Information Technology
None
2012
Power consumption is one of the major threads in CMOS technology. International technology road-map for semiconductors (ITRS) [1] reports that leakage power dissipation may come to dominate total power consumption. Although Leakage power was negligible at 0.18μ technology and above, in nano scale technology, but when the technology is decreases these leakage powers are the top most concern for VLSI circuit designer. As the technology feature size shrink static power consumption dominant the dynamic power exponentially and this static power consumption is known as a sub-threshold leakage. Sub-threshold leakage is a leakage that is arises by creating a weak inversion channel between drain to source. However, tunneling current through gate oxide insulator, channel punch through current and gate current due to hot-carrier injection are also responsible for semiconductor power consumption. Although gate-oxide thickness will be reduced as the technology decreases in nano scale, but this reduction causes sub-threshold leakage. So, there were several method was proposed to tackle the leakage. However, every proposed method has some trade-offs between power, delay and area, in this paper novel common vdd and gnd technique is proposed to overcome the semiconductor leakage and this technique has excellent tradeoffs between power, delay and area, moreover this method will be new weapon for low power VLSI circuit designer.
[vdd technique, technology size, VLSI, very large scale integrated circuit, VLSI circuit design, gnd technique, leakage power dissipation, subthreshold leakage, power consumption, ITRS, nanometer scale low power VLSI design, complimentary metal oxide semiconductor, Sub-threshold leakage, nanometer, integrated circuit design, gate oxide insulator, CMOS technology, static power consumption, dynamic power consumption, CMOS integrated circuits, size 0.18 mum, international technology road-map for semiconductor]
Agent based framework for providing security to data storage in cloud
2012 15th International Conference on Computer and Information Technology
None
2012
Cloud system framework must ensure appropriate security for data storages along with satisfied timing constrain and performance. Usually, data in cloud are very dynamic and different types of data demand different level of security. Hence, if different cloud data storages can be provided with different level of security, managing data will be more flexible and efficient; the performance of system will increase as well. Therefore, in order to ensure adequate security for different data storages in cloud, we have proposed a three tier security framework. We have analyzed the performance with respect to overhead for different security services such as confidentiality, integrity and authenticity and showed that data classification according to different level of security enhances the performance of the system that provides security services to the data in cloud.
[framework and security service, cloud data storage security, data management, security service, timing constraint, cloud system framework, three tier security framework, data integrity, Data, data confidentiality, data authenticity, agent based framework, security level, Security, data demand, security of data, data classification, data privacy, cloud computing]
A biologically plausible neural network training algorithm with composite chaos
2012 15th International Conference on Computer and Information Technology
None
2012
Chaos appears in many real and artificial systems. Inspired from the presence of chaos in human brain, we attempt to formulate neural network (NN) training method. The method uses a composite chaotic learning rate (CCLR) to train a neural network. CCLR generates a composite chaotic time series consisting of three different chaotic sources such as Mackey Glass, Logistic Map and Lorenz Attractor and a rescaled version of the series is used as learning rate (LR) during NN training. It gives two advantages - similarity with biological phenomena and possibility of jumping from local minima. In addition, the weight update may be accelerated in the local minimum zone due to chaotic variation of LR. CCLR is extensively tested on five real world benchmark classification problems such as diabetes, time series, horse, glass and soybean. The proposed CCLR outperforms the existing BP and BPCL in terms of generalization ability and also convergence rate.
[neural network training method, generalization ability, neural network, human brain, Mackey glass, composite chaotic time series, Lorenz attractor, learning (artificial intelligence), soybean, pattern classification, chaos, BPCL, time series, Hurst exponent, generalisation (artificial intelligence), composite chaotic learning rate, biologically plausible neural network training algorithm, horse, biological phenomena, benchmark classification problem, backpropagation, CCLR, convergence rate, artificial system, diabetes, neural nets, logistic map]
Classical arithmetic logic unit embedded on reversible/quantum circuit
2012 15th International Conference on Computer and Information Technology
None
2012
Reversible circuit dissipates less heat than irreversible circuit. A promising use of reversible circuit may be embedding of reversible circuits in irreversible general purpose computers to allow low-power design. In this paper, we embed an n-bit classical ALU on reversible circuit, which can perform addition, subtraction, EXOR, EXNOR, AND, NAND, OR, NOR, and NOT operations on n-bit data. The quantum realization of our n-bit ALU requires 27n - 10 primitive quantum gates with quantum circuit width of 4n + 5. The known reversible n-bit ALU capable of performing only mod 2n addition, subtraction, negative subtraction, EXOR, and no-operation requires 22n - 10 primitive quantum gates with quantum circuit width of 2n + 5. With a marginal increase of quantum primitive gate count and nearly doubling the quantum circuit width, our ALU implements a larger set of operation needed for general purpose computing.
[multiplying circuits, reversible circuit, quantum gates, n-bit data, classical arithmetic logic unit, NOT operation, EXNOR operation, quantum primitive gate count, arithmetic logic unit, negative subtraction, general purpose computers, NAND operation, n-bit classical ALU, adders, quantum circuit, digital arithmetic, irreversible general purpose computer, low-power design, quantum circuit width, low-power electronics, addition]
New sufficient conditions for Hamiltonian paths
2012 15th International Conference on Computer and Information Technology
None
2012
A Hamiltonian path in a graph is a path involving all the vertices of the graph. In this paper, we revisit the famous Hamiltonian path problem and present new sufficient conditions for the existence of a Hamiltonian path in a graph.
[graph theory, Hamiltonian path problem, graph vertices, graph Hamiltonian path, sufficient condition]
BER performance analysis of a multi user Alamouti-MRC system on secured text message transmission
2012 15th International Conference on Computer and Information Technology
None
2012
This paper investigates the effect of antenna diversity for a double transmit and multiple receive antenna supported wireless communication system that employs multi user Alamouti's space time block coding (STBC) and maximal ratio combining (MRC) scheme on secured text message transmission. The FEC encoded Alamouti-MRC transmission system under investigation implements RSA cryptographic algorithm and deploys various multi-level digital modulations (16-PSK, 16-DPSK and 16-QAM) techniques over an Additive White Gaussian Noise (AWGN) and Rayleigh Fading Channels. It has been observed from the study that in case of without receive antenna diversity the system shows comparatively worst performance in 16-DPSK scheme and satisfactory performance in 16-QAM. It is noticeable that the system performance is improved with increase in number of receive antenna. The performance analysis shows that with implemented Alamouti-MRC scheme (6 receive antenna) under 16-QAM digital modulation, the system provides excellent performance over a significant low signal to noise ratio (SNR) values.
[telecommunication security, Rayleigh fading channel, differential phase shift keying, AWGN, STBC, secured text message transmission, electronic messaging, antenna diversity, quadrature amplitude modulation, Antenna Diversity, AWGN channels, multiuser channels, public key cryptography, 16-QAM technique, multilevel digital modulation, Bit Error Rate, RSA cryptographic algorithm, multiuser Alamouti space time block coding, MRC scheme, error statistics, diversity reception, Rayleigh channels, 16-DPSK technique, radio receivers, wireless communication system, maximal ratio combining scheme, 16-PSK technique, BER performance analysis, space-time block codes, multiple receive antenna, radio transmitters, radiocommunication, FEC encoded Alamouti-MRC transmission system, additive white Gaussian noise, SNR value, Rayleigh Fading, multiuser Alamouti-MRC system, double transmit antenna, signal to noise ratio, MRC]
Towards a schedulable fault tolerant HW-SW mapping for real time systems
2012 15th International Conference on Computer and Information Technology
None
2012
An important part in the design of real-time systems is the allocation and scheduling of the software tasks onto the hardware architecture. This faces the challenge of meeting deadlines, completion times, earliest start times and tolerating faults. Unfortunately, these processes are far from trivial due to the wide range of complex constraints that typically appear in real-time systems. The lack of edibility and expressive power in existing scheduling frameworks makes it difficult to model the system accurately. Moreover, the designed system must be cost-effective in terms of resource utilization which implies the need for an optimization approach. In order to tackle these deficiencies this paper proposes a schedulable fault-tolerant hardware-software mapping methodology for real time systems. The design optimization approach decides the mapping of jobs to nodes such that the timing constraints of the application are satisfied. Experimental studies show that the proposed method outperforms existing dominant work.
[job mapping, hardware-software codesign, Real-Time systems, Fault-Tolerance, schedulable fault tolerant HW-SW mapping, software fault tolerance, Complexity, complex constraints, optimisation, resource allocation, software task allocation, design optimization approach, timing constraints, real-time systems, software task scheduling, scheduling, HW-SW Mapping, hardware architecture, real time systems, resource utilization, Schedulability Analysis, schedulable fault-tolerant hardware-software mapping methodology]
Features extraction and classification for Ictal and Interictal EEG signals using EMD and DCT
2012 15th International Conference on Computer and Information Technology
None
2012
Electroencephalogram (EEG) is a record of electrical signal to represent the human brain activity. Many researchers are working on human brain as they are fascinated by the idea of secret, thought and feeling from the external and internal stimuli. Feature extraction, analysis, and classification of EEG signals are still challenging issues for researchers due to the variations of the brain signals. Different features are used to identify epilepsy, coma, encephalopathies, and brain death, etc. However, we have observed that extracted features from same kinds of signal transformations are not effective to differentiate the epilepsy periods including Ictal (active seizure period) and Interictal (interval between seizures) of EEG signals. In this paper we present a new approach for feature extraction using high frequency components from DCT transformation. We also combine the new feature with the bandwidth feature extracted from the empirical mode decomposition (EMD). These features are then used as an input to least squares support vector machine (LS-SVM) to classify Ictal and Interictal period of epileptic EEG signals from different brain locations. Experimental results show that the proposed method outperforms the existing state-of-the-art method for better classification of Ictal and Interictal period of epilepsy for benchmark dataset.
[empirical mode decomposition, Epilepsy, brain death, least squares support vector machine, encephalopathy, signal transformation, external stimuli, LS-SVM, interictal period, feature extraction, brain location, epilepsy period, Seizure, EMD, discrete cosine transforms, electroencephalography, biology, EEG, interictal EEG signal, electroencephalogram, brain signal, coma, epileptic EEG signal, DCT transformation, human brain activity, DCT, internal stimuli, features extraction, electrical signal]
Synthesizing fault tolerant safety critical systems
2012 15th International Conference on Computer and Information Technology
None
2012
To keep pace with today's nano-technology, safety critical embedded systems are becoming less tolerant to errors. Research into techniques to cope with errors in these systems has mostly focused on transformational approach, replication of hardware devices, parallel program design, component based design and/or information redundancy. It would be better to tackle the issue early in the design process that a safety critical system never fails to satisfy its strict dependability requirements. A novel method is outlined in this paper that proposes an efficient approach to synthesize safety critical systems. The proposed method outperforms dominant existing work by introducing the technique of run time detection and completion of proper execution of the system in presence of faults.
[object-oriented programming, transformational approach, safety critical embedded system, run time detection, Program, parallel program design, safety-critical software, dependability requirement, parallel programming, software fault tolerance, hardware device replication, component based design, information redundancy, Detector, embedded systems, fault tolerant safety critical system, nanotechnology, design process, Safety Critical System, system fault, Fault Tolerance]
Performance analysis of a coherent chaos-shift keying technique
2012 15th International Conference on Computer and Information Technology
None
2012
The transmission of binary information by using chaotic switching named as chaos-shift keying (CSK) with the generation of chaos using cubic map is presented. A system has been proposed for calculating the analytical bit-error rate (BER) of a coherent chaos-shift keying (CSK) digital communication system under an additive white Gaussian noise environment with the consideration of ideal synchronization at the receivers. The bit-error rate of the CSK system is compared with the conventional modulation techniques like ASK, FSK, and PSK. Generating the chaos with the simple cubic map, it is shown that the performance of coherent CSK system can be improved with the variations of the initial conditions of chaos generation. The bit-error rate of the CSK system is improved for both single-user and multiple-user environment.
[chaos-shift keying (CSK), AWGN, single-user environment, binary information transmission, phase shift keying, receivers, multiple-user environment, digital communication, ASK, coherent chaos-shift keying technique, CSK digital communication system, additive white Gaussian noise environment, error statistics, FSK, chaotic switching, chaos, conventional modulation techniques, PSK, amplitude shift keying, frequency shift keying, modulation techniques, BER, chaos communication, synchronisation, telecommunication switching, chaotic communication, Bit-error rate, analytical bit-error rate, cubic map, synchronization, chaos generation]
Multischeme Spray and Wait routing in Delay Tolerant networks exploiting nodes delivery predictability
2012 15th International Conference on Computer and Information Technology
None
2012
Delivering message in Delay Tolerant Networks is challenging due to its sparse nature and intermittent connectivity. Therefore efficient routing is very important for these networks. Here, we focus on the Spray and Wait routing schemes for Delay Tolerant Networks to avoid identical spraying technique and blind forwarding by mobile nodes. To avoid those problems, we first define an adaptive spraying scheme based on the delivery predictability of nodes. We then formulate an equation based on the number of remaining copies of messages to select a specific spraying technique each time. Thus, we propose to employ multiple spraying techniques during the spray phase of our scheme. Simulation results show that our Multischeme Spray and Wait routing performs better in different network scenarios.
[node delivery predictability, adaptive spraying scheme, Hybrid Routing Scheme, Multischeme Spray and Wait Routing, multischeme spray and wait routing scheme, identical spraying technique, delay tolerant networks, Delivery Predictability, blind forwarding, telecommunication network routing, Adaptive Spraying, spray phase, mobile nodes, Delay Tolerant Networks]
Adaptation of spray phase to improve the binary spray and Wait routing in Delay Tolerant Networks
2012 15th International Conference on Computer and Information Technology
None
2012
Delivering messages in Delay Tolerant Networks is a nontrivial challenge due to its sparse nature and intermittent connectivity. Therefore efficient routing of messages is vital for these networks. In this work, we aim to adapt the spray phase of the Binary Spray and Wait routing scheme in Delay Tolerant Networks to avoid large communication overhead suffered by the scheme. We define an adaptive spraying scheme to allow nodes to switch to the wait phase without lingering the spray phase unnecessarily. Binary Spray and Wait routing permits nodes to enter the wait phase when the remaining number of copies of a message reaches one. However, we permit nodes to consider the time to live value of a message instead of the number of copies to enter the wait phase. We formulate an equation to obtain a switch value to compare against the time to live value of a message to migrate to the wait phase. Furthermore, we compare our scheme with that of other schemes through simulations. Simulation results show that our proposed adaptive spraying scheme performs better than that of others in different network scenarios.
[adaptive spraying scheme, wait phase, switch value, Communication Overhead, delay tolerant networks, communication overhead, Switch Value, time to live value, telecommunication network routing, Adaptive Spraying, Time to Live, spray phase, binary spray and wait routing, Delay Tolerant Networks]
Performance analysis of RLS and VSS-LMS channel estimation techniques for 4G MIMO OFDM systems
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper recursive least (RLS) square and variable step size least mean (VSS-LMS) square adaptive channel estimator are described for multiple input multiple output (MIMO) orthogonal frequency division multiplexing (OFDM) system. Both the techniques uses adaptive estimator which are capable of updating the parameters of estimator continuously and therefore knowledge of channel and noise statistics are not necessary, only knowledge of receive signal is required. From the simulation result it is observed that RLS CE method works better in terms of quick convergence rate than VSS-LMS CE for MIMO OFDM system. In addition, the utilization of more multiple antennas at the transmitter and/or receiver provides a much higher performance compared with fewer antennas.
[least mean squares methods, noise statistics, OFDM, orthogonal frequency division multiplexing system, antenna arrays, adaptive estimator, RLS, recursive least square technique, OFDM modulation, MIMO, multiple antennas, MIMO communication, VSS-LMS channel estimation techniques, 4G MIMO OFDM systems, transmitter, adaptive estimation, receiver, receive signal, RLS CE method, VSS-LMS CE, variable step size least mean square adaptive channel estimator, 4G mobile communication, channel estimation, multiple input multiple output system, VSS-LMS, RLS techniques]
Obtaining business process from value process in blended value based sustainable e-business modelling
2012 15th International Conference on Computer and Information Technology
None
2012
E-business modelling is already an established term as it converts technology into economic value. Sustainability is another global contemporary issue. While modelling e-business for sustainability it is essential to know the `blended value process' of the proposed value based on which `business process' is derived. The ability to incorporate between the blended value process and the business process is one of the imperative factors that play very significant roles for the companies to be competitive in today's exigent market. A number of research works exist on sustainability, e-business modelling, and value creation but none of them clearly explains the importance of incorporation between these two processes or how business process can be obtained from value process in e-business modelling. We, therefore, demonstrate in this article the process of how business process can be derived from blended value process in sustainable e-business modelling using process algebra. We also provide an illustrative example of our approach for enhanced understanding.
[Business process, Value process, process algebra, blended value process, Sustainability, sustainable e-business modelling, value creation, electronic commerce, E-Business model]
Device control by using GSM network
2012 15th International Conference on Computer and Information Technology
None
2012
In the age of electronic systems it is important to acquire and control information from everywhere. Here we present a novel secure real time method to control device. We summarizes a study conducted to examine the feasibility of implementing Global System for Mobile Communications (GSM), using Duel Tone Multi-Frequency (DTMF) as an alternative means of communication to Radio Frequency (RF) to control electrical appliances. In remote communication, DTMF signal can replace RF signal for the advantage of simplicity and audibility. A user can send DTMF tone to control home appliances or machines in industries. Any GSM mobile phone can be employed to send DTMF tone. The advantage of this system is the use of GSM technology for communication and at a time control of many devices. This paper deals with the design and experimentation, which can be useful for the people who are working in research laboratory, industry &amp; education in the field of Communication, Control, wireless technology etc.
[remote communication, GSM technology, DTMF signal, control information, DTMF, domestic appliances, device control, RF communication, Remote communication, GSM, time control, Global System for Mobile Communications, RF signal, radio frequency communication, security of data, electronic system, telecontrol, DTMF tone, GSM network, electrical appliance control, secure real time method, dual tone multifrequency, GSM mobile phone, research laboratory, Electrical appliances, cellular radio]
Color image segmentation using visible color difference and canny edge detector
2012 15th International Conference on Computer and Information Technology
None
2012
This paper presents an improved version for the JSEG color image segmentation algorithm integrating visible color difference and canny edge detector to measure the boundary of the region. This method resolves the major inherent problem in JSEG caused by varying illumination using visible color difference and canny edge detector with JSEG segmented image. Experiments on natural color of Berkeley Segmentation Dataset are presented which show improved results in comparison with the classical JSEG and Fractal-JSEG algorithm.
[visible color difference (VCD), fractal-JSEG algorithm, region boundary measurement, visible color difference, JSEG color image segmentation algorithm, Canny edge detector, Color image segmentation, J value segmentation (JSEG), image segmentation, illumination variation, J value segmentation, edge detection, image colour analysis, Berkeley segmentation dataset, classical JSEG algorithm]
Stochastic Kronecker graph revisited
2012 15th International Conference on Computer and Information Technology
None
2012
Here we calculate the expected number of isolated vertices, edges, self loops and triangles in a random realization of stochastic Kronecker graph. We then establish some bounds on the values of the parameters of the stochastic Kronecker graph which are sufficient to generate large random graph with no isolated vertices, edges, self loops and triangles. Finally we show two phase transitions: one for the emergence of edges and the other for the emergence of self loops under stochastic Kronecker model of graph generation.
[random realization, random graph, graph theory, random processes, stochastic Kronecker graph revisited, Isolated vertex, Phase transitions, Self loops, triangles, graph generation, Edge count, stochastic processes, Stochastic Kronecker Grap]
Multiband low profile Modified Inverted-FL strip antenna for 5.2/5.8 GHz WLAN and 5.5 GHz WiMAX applications in laptop computer
2012 15th International Conference on Computer and Information Technology
None
2012
This paper contains a multiband low profile Modified Inverted-FL (IFL) strip antenna for 5.2/5.8 GHz WLAN and 5.5 GHz WiMAX operations in a laptop computer. The simulation results are analyzed by means of Numerical Electromagnetic Code-2 (4NEC2) software. This antenna covers -10 dB return loss bandwidth of 900 MHz (5150 MHz ~6050 MHz). The antenna provides very high peak gains with lower gain variation in its operating bands. The antenna has suitable omnidirectional radiation characteristics. The dimension of the antenna is 16&#x00D7;18.5 mm2, which is very compact in size. So, this low profile antenna is promising to be embedded within the different portable devices employing WiMAX and WLAN applications.
[frequency 5150 MHz to 6050 MHz, Numerical Electromagnetic Code-2 software, WiMax, WiMAX, multiband low profile modified inverted-FL strip antenna, WLAN, frequency 5.5 GHz, laptop computer, omnidirectional radiation characteristics, Inverted-FL antenna (IFL Antenna), IFL strip antenna, 4NEC2 software, Low profile antenna, omnidirectional antennas, frequency 5.2 GHz, wireless LAN, frequency 5.8 GHz]
Multiband fractal square Koch antenna design for UHF/SHF application
2012 15th International Conference on Computer and Information Technology
None
2012
There exists various types of antennas for various purposes, but the interest in this area is increasing day by day. There have been ever growing demands for antenna designs that possess the highly desirable properties: compact size, low profile, multi-band, wide bandwidth, high gain, improved SWR etc. Fractal antennas can be used to meet these demands. This paper represents the analysis and design of multiband square Koch fractal dipole antenna, where it is shown that as the iterations are increased, the band of frequencies also increase. The designed antenna has operating frequencies for first iteration are of 496 MHz and 1430 MHz, and for second iteration are of 460 MHz, 1248 MHz, 1926 MHz and 4390 MHz with acceptable bandwidth, which has useful applications in UHF/SHF. The radiation characteristics, SWR, reflection-coefficient, input impedance and gain of the proposed antenna are described with 4NEC2 Software package. Here, the antenna is placed in the XY-plane for the first iteration and in YZ-plane for the second iteration.
[antenna designs, reflection-coefficient, XY-plane, Multiband, YZ-plane, frequency 460 MHz, antenna radiation characteristics, Fractal, antenna radiation patterns, UHF antennas, Square Koch, Antenna, frequency 496 MHz, 4NEC2 software package, UHF-SHF application, Wire Antenna, dipole antennas, frequency 4390 MHz, SWR, fractal antennas, frequency 1926 MHz, multifrequency antennas, multiband square Koch fractal dipole antenna design, frequency 1430 MHz, frequency 1248 MHz]
A multilayer network-supporting universal electronic cash transaction framework
2012 15th International Conference on Computer and Information Technology
None
2012
Bangladesh has achieved a tremendous growth in the telecommunication sector recently in spite of various deficiencies regarding socio-economic infrastructure. Following a number of years of huge development in this sector, each and every parts of Bangladesh is now under cellular coverage and the teledensity is expected to reach at maximum satisfactory level soon. This success motivates us towards a more reliable &amp; transparent economic infrastructure through the use of cellular services and intelligent software systems. This paper proposes multilayer network-supported framework which completely eliminates the need and use of paper notes for all kinds of economic transactions. However, the proposed framework is not a threat at all to the present financial institutions and their fundamental mechanisms. Rather, it would be more transparent and free of corruptions as reliable and efficient monitoring of transactions will be possible by respective authorities. This proposed framework will handle all kinds of transactions by electronic means e.g. Push Pull services offering Short Message Service(SMS), road-side booths containing a web interface. Under this distributed system, each and every entity of the entire economic infrastructure will makes it flexible, robust, secure and lawful. The satisfactory experimental results on a small case scenario supports its potential possibilities in real-world implementation.
[Web interface, financial institution, electronic messaging, distributed processing, distributed system, cellular service, short message service, reliable economic infrastructure, electronic money, road-side booth, Push Pull, multilayer network-supported framework, transparent economic infrastructure, transaction monitoring, telecommunication sector, teledensity, push pull service, intelligent software system, economic transaction, Bangladesh, multilayer network-supporting universal electronic cash transaction framework, SMS, Internet, cellular coverage, electronic transaction, socio-economic infrastructure, Multilayer network]
Design and performance analysis of ultra wideband Inverted-F antenna for Wi-Fi, WiMAX, WLAN and military applications
2012 15th International Conference on Computer and Information Technology
None
2012
In this manuscript a modified Inverted-F antenna (IFA) is designed to serve the purposes of mobile WiMAX, Wi-Fi, WLAN as well as military applications. The main intension of this work is to propose an ultra wideband antenna for multi-serving purposes with good return loss characteristics, high gain as well as impedance matched and good radiation characteristics. An ultra wideband antenna (modified double IFL antenna) is proposed which bandwidth is 2.16 GHz (3.34-5.5 GHz) serving for WiMAX (3.4-3.6 GHz), Wi-Fi (5.15-5.35 GHz), WLAN (5.15-5.35 GHz) and military (4.4-4.8 GHz) applications. The proposed antenna is of good gain and good radiation characteristics. As the dimension of the proposed antenna is very small, the antenna is promising to be embedded within the different portable devices employing Wi-Fi/ mobile WiMAX applications.
[ultra wideband antennas, IFA, Inverted F Antenna, Double Inverted F Antenna, bandwidth 5.15 GHz to 5.35 GHz, ultra wideband inverted-F antenna, antenna radiation patterns, portable device, bandwidth 4.4 GHz to 4.8 GHz, bandwidth 2.16 GHz, planar inverted-F antennas, Wi-Fi, WiMax, WiMAX, double IFL antenna, WLAN, bandwidth 3.4 GHz to 3.6 GHz, mobile WiMAX, radiation characteristic, military application, military communication, return loss characteristic, bandwidth 3.34 GHz to 5.5 GHz, wireless LAN, Military Applications]
New efficient algorithms for the merged LCS problem with and without block constraints using sparse dynamic programming
2012 15th International Conference on Computer and Information Technology
None
2012
The longest common subsequence problem has been widely studied and used to find out the relationship between sequences. In this paper, we study the interleaving relationship between sequences. Given a target sequence T and two merging sequences A and B, we need to find out the LCS between M(A, B) and T, where M(A, B) denotes the merging sequence of A and B. We first present a O((Rr + Pm)log log r) time algorithm where |T| = n, |A| = m, |B| = r, R is the total number of ordered pairs of positions at which the two strings A and T match and P denotes the total number of ordered pairs of positions at which the two strings B and T match. We also propose an algorithm to solve a variation of the problem where block constraint arises. The running time of the blocked version is O(max{R&#x03B2; log log r, P&#x03B1;log log r}), where &#x03B1; denotes the number of blocks in A and &#x03B2; denotes the number of blocks in B.
[Merged Sequence, Longest Common Subsequence, Block Constraint, merged LCS problem, Dynamic Programming, longest common subsequence problem, bioinformatics, dynamic programming, string matching, sparse dynamic programming, Bioinformatics, block constraints]
Cooperative secondary user selection as a relay for the primary system in underlay cognitive radio networks
2012 15th International Conference on Computer and Information Technology
None
2012
We consider an underlay cognitive radio network consisting of a single pair of primary transmitter-receiver PTx-PRx and a group of M secondary transmitter-receiver pairs STx-SRx within the transmission range of the primary system. Secondary transmitters are partitioned into two groups K, K&#x03B5;M active secondary transmitters ST<sub>i</sub>, i &#x03B5; {1, 2,..., K} which may transmit data with the co-existence of PTx below a certain interference threshold to the primary receiver and (M-K) inactive secondary transmitters ST<sub>j</sub>, j&#x03B5; {1, 2,..., (M-K)} which are in idle state, ready to assist the primary system when the data rate between PTx and PRx over a direct link falls below the target rate R<sub>target</sub>. In this paper, we propose a cooperative relay selection scheme where one of the inactive ST<sub>j</sub> which achieves the primary target R<sub>target</sub> will be selected as a best relay to forward the primary information. Simulation results show the effectiveness of our proposed system.
[radiofrequency interference, cooperative secondary user selection, cooperative relay selection scheme, cognitive radio, radio transmitters, underlay cognitive radio networks, cooperative secondary selection, transmitter-receiver pairs, radio receivers, Cognitive radio, cooperative communication, interference threshold]
A high embedding capacity image steganography using stream builder and parity checker
2012 15th International Conference on Computer and Information Technology
None
2012
Steganography is one of the arts and sciences of securing or hiding secret information in some cover media. In the present study, a new steganography technique is developed to hide large data in Bitmap image using stream builder and parity checker. This method uses the concept of odd and even parity for embedding and extracting of secret message. This method is an improvement of Least Significant Bit (LSB) method for hiding information in images. The proposed method can hide large data in a single Bitmap image retaining the advantages and discarding the disadvantages of the LSB method.
[cover media, embedding capacity image steganography, odd parity, parity check codes, secret message embedding, hiding secret information, Bitmap Image, stream builder, Bitmap image, Stream Builder, Steganography, secret message extracting, steganography technique, steganography, parity checker, single bitmap image, least significant bit method, LSB, even parity, Even-Odd Parity, image coding, securing secret information, LSB method]
Longest common subsequence problem for run-length-encoded strings
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper, we present a new and efficient algorithm for solving the longest common subsequence problem between two run-length-encoded (RLE) strings. Suppose Y&#x0302; and X are two RLE strings having length k and &#x2113;, respectively. Also, assume that Y and X are the two uncompressed versions of the two RLE strings Y&#x0302; and X having length k and &#x2113; respectively. Then, our algorithm runs in O((k + &#x2113;)+R loglog(k&#x2113;) + R log log &#x03C9;) time, where &#x03C9; = k + &#x2113; and R is the total number of ordered pairs of positions at which the two RLE strings match. Our algorithm outperforms the best algorithms for the same problem in the literature.
[run-length-encoded string, RLE string, Matched Block Calculation, longest common subsequence problem, vEB Tree, RLE, string matching, Bounded Heap, string length, LCS, computational complexity]
Solar power enhancement using concentration method, tracking system and fuzzy based control system
2012 15th International Conference on Computer and Information Technology
None
2012
This research paper carries out a realistic experimental approach to enhance the solar output power to a significant level with the application of solar concentrator and tracking system and also proposes a fuzzy based control system for the utilization of the duration for which solar power is available. By using only a solar panel, the theoretical value is supposed to be around 40% more. The method is arranged using `Double Sun Technology' and `Solar Tracker'. In addition to `Double Sun Technology', four flat mirrors have been used to increase the output power. A remarkable enhancement in the output power is achieved with this method. Rather than using only a solar panel, the output power increases by 58.32% by using a photovoltaic (PV) panel with four flat mirrors with solar tracker system. The concentrators are designed to collect the incident light from the sun and concentrate them on the PV panel so that the solar irradiance is more. The experiment is conducted in four different setups. In the first setup, the output power is calculated without the use of Double Sun and solar tracker. In the second setup, power is measured with Double Sun but without solar tracker. Both Double Sun and solar tracker are used in the third setup. Then four flat mirrors and solar tracker are considered. Output power has been calculated using open circuit voltage and short circuit current. Finally, the proposed system improves the total output by maximum utilization of available photovoltaic energy and maximum utilization of the duration for which solar power is available based on fuzzy logic.
[PV panel, incident light collection, passive device, solar output power enhancement, solar tracker system, maximum available photovoltaic energy utilization, short circuit current, open circuit voltage, short-circuit currents, double Sun technology, tracking, fuzzy control, fuzzy based control system, solar concentrator, flat mirror, photovoltaic panel, concentrator, tracking system, solar panel, solar power, mirrors, solar energy concentrators, fuzzy logic, solar tracker, solar power availability, solar irradiance, Photovoltaic (PV) panel, concentration method, Double Sun Technology, power control, solar cells]
Maximal path based conflict resolution approach in multiple homologous gene list alignment
2012 15th International Conference on Computer and Information Technology
None
2012
This paper deals with the alignment of multiple homologous gene lists, which is a specific problem of multiple sequence alignment. In this case, the smallest unit of the input sequences is comprised of complete genes. An efficient graph based algorithm for the alignment of multiple homologous gene lists is presented in this paper. The fundamental concept to multiple sequence alignment and the graphical structure of the genomic segments are provided. Two heuristics are developed for the resolution of local alignment conflicts which reduces the search space while searching for the candidate link for deletion. The performance of the proposed algorithm is assessed by comparing the alignment results of homologous genomic segments in Arabidopsis thaliana to those obtained by using other alignment methods.
[multiple homologous gene list alignment, graph theory, gene list sequencing, multiple sequence alignment, homologous gene list, local alignment conflict resolution, search space reduction, Arabidopsis thaliana, genomic segment graphical structure, biology computing, maximal path based conflict resolution approach, genomics, graph based algorithm, Bioinformatics]
iClassroom: Toward a low cost interactive classroom
2012 15th International Conference on Computer and Information Technology
None
2012
Use of computer technology in modern student centred leaning is playing a vital role. Works are going on in the developed countries in building interactive teaching tools which will give more degrees of freedom to both the teachers and the students in the learning process. In this process, classroom itself can play an important role. So we have developed an interactive classroom. iClassroom is a low-cost classroom system with an interactive whiteboard and an automatic attendance taking system mainly for developing regions. The classroom additionally provides the control panel for electrical devices present in the classroom.
[iClassroom, [interactive classroom, technology for teaching, learning process, interactive teaching tools, teaching, computer technology, low cost teaching tools, RFID based system, electrical devices, low cost interactive classroom, automatic attendance taking system, student centred leaning, interactive devices, Appliance Control, computer aided instruction, interactive classroom, interactive whiteboard]
Pattern classification of deep brain local field potentials for brain computer interfaces
2012 15th International Conference on Computer and Information Technology
None
2012
The trend of current brain computer interfaces (BCI) seek to establish bi-directional communication with the brain, for instance, recovering motor functions by externally controlling devices and directly stimulating the brain. This will greatly assist paralyzed individuals through bypassing the damaged brain region. The key process of this communication interface is to decode movements from neural signals and encode information into neural activity. The majority of decoding or pattern classification studies have focused on cortical areas for BCIs, but deep brain structures have also been involved in motor control. The subthalamic nucleus (STN) in the basal ganglia is involved in the preparation, execution and imagining of movements, and may be an alternative source for driving BCIs. This study therefore aimed to classify patterns of deep brain local field potentials (LFPs) related to execution of visually cued movements. LFPs were recorded bilaterally from the STN through deep brain stimulation electrodes implanted in patients with Parkinson's disease. The frequency dependent components of the LFPs were extracted using the wavelet packet transform. In each frequency component, signal features were extracted using an alternative approach called neural synchronization by analyzing Granger causality between the STN. Based on these extracted features, a new feature selection strategy, namely weighted sequential feature selection (WSFS) was developed to efficiently select the optimal feature subset. A support vector machine (SVM) classifier was implemented alongside this novel feature extraction and selection strategy, and evaluated using a cross-validation procedure. Using this optimised feature subset, average correct pattern classification accuracy of movement (left or right) reached 76.0&#x00B1;3.1%. The results obtained in this study are encouraging and suggest that the neural activity in the deep neural circuit (basal ganglia) can be used for controlling BCIs.
[WSFS, Support vector machine (SVM), Parkinson's disease, brain-computer interfaces, wavelet transforms, signal feature extraction, support vector machine classifier, frequency dependent components, paralyzed individuals, STN, motor functions, neural activity, neural synchronization, weighted sequential feature selection, LFP, Wavelet packet transform, Local field potentials (LFPs), cortical areas, SVM classifier, brain computer interfaces, support vector machines, diseases, motor control, decoding, synchronisation, medical signal processing, deep brain stimulation electrodes, optimised feature subset, subthalamic nucleus, neurophysiology, neural signals, visually cued movements, cross-validation procedure, pattern classification accuracy, basal ganglia, bidirectional communication, wavelet packet transform, communication interface, neural chips, encode information, Weighted sequential feature selection (WSFS), feature extraction, deep neural circuit, deep brain structures, damaged brain region, deep brain local field potentials, pattern classification, feature selection strategy, frequency component, controlling BCI, causality, Brain computer interfaces (BCI), Granger causality, optimal feature subset]
Affective mapping of EEG during executive function tasks
2012 15th International Conference on Computer and Information Technology
None
2012
Executive function is a set of mental processes commonly linked with the activation of the brain's prefrontal cortex. While many studies have focused on EF in adulthood, the development of EF in children is yet to be understood. This paper proposes a new approach for understanding children's reactions during EF tasks by mapping their EEG signals onto the 2D valence-arousal affective space model. Brain signals of ten pre-school children aged between 4-6 years (male: 5; female: 5) were collected while they play the standardized version of the Dimensional Change Card Sort. Behavioral results in terms of percentage of correct responses and response time did not vary significantly across gender. Emotion mapping using the valence-arousal model showed that boys tend to be consistent in their emotion during pre-switch and post-switch tasks. The emotion of girls, however, tends to shift towards neutral state during the post-switch test regardless of their initial emotions in the pre-switch phase.
[electroencephalography, mental process, dimensional change card sort, emotion mapping, preswitch task, brain models, prefrontal cortex, postswitch task, EEG, EEG mapping, brain signal, computational intelligence, medical signal processing, emotion model, card-sorting, children, neutral state, executive function task, 2D valence-arousal affective space model]
BER performance analysis of M-ary modulation techniques in an outdoor environment with MLSE equalizer over fading channels
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper, we consider a communication system that uses M-ary phase shift keying (MPSK), M-ary quadrature amplitude modulation (MQAM), M-ary differential phase shift keying (MDPSK) and M-ary pulse amplitude modulation (MPAM) to transmit information over a Rayleigh and Ricean fading channel perturbed by additive white Gaussian noise (AWGN). We model the fading channel using Jakes Doppler power spectrum and AJakes Doppler power spectrum. The performance of these different modulation techniques is analyzed in terms of bit error rate (BER) for single and multipath Rayleigh fading channel in outdoor environment by MATLAB simulation. For the simulation of fading channel the different property values e.g. path delays, average path gains, maximum Doppler shifts, Doppler spectrum parameters etc are chosen properly to a reflect realistic fading channels. In all the cases, our analysis shows that MQAM performs better and the system performance increases with the use of maximum likelihood sequence estimation (MLSE) equalizer over Rayleigh and Ricean Fading channels.
[differential phase shift keying, AWGN, quadrature amplitude modulation, Doppler effect, Doppler power spectral density, AWGN channels, outdoor environment, maximum likelihood sequence estimation, equalisers, maximum Doppler shift, Rician fading channel, information transmission, error statistics, MPSK, average path gain, path delay, pulse amplitude modulation, communication system, M-ary modulation technique, Rayleigh channels, Doppler spectrum parameter, M-ary phase shift keying, MPAM, MDPSK, bit error rate, M-ary modulation, MLSE equalizer, MQAM, M-ary quadrature amplitude modulation, M-ary differential phase shift keying, delays, Rician channels, BER performance analysis, data communication, M-ary pulse amplitude modulation, additive white Gaussian noise, multipath Rayleigh fading channel, MATLAB simulation, AJakes Doppler power spectrum, Rayleigh fading, Ricean Fading]
Modeling business processes for PLM services integration in PLM system
2012 15th International Conference on Computer and Information Technology
None
2012
Business Process modeling plays the central role in the business management through systems engineering from the perspective of enterprise modeling. Every organization prior to adapting an enterprise solution for productivity and efficiency needs to analyze the current processes within the organization and identify the required process efficiency and quality along with the transformation needed. The Object Management Group (OMG) Product Life Cycle Management (PLM) Services 2.0 prescribes specifications based on information and computational model for engineering objects that facilitate life cycle management. Besides, this specification also provides high level scenarios for the independent model rather than specific domain. For an efficient enterprise solution using PLM system, through PLM services, there is an emergent need for business process modelling since PLM system does not cover all the business processes and which converge towards the OMG PLM Services 2.0 specification with the required business change managements. This paper highlights key modeling issues for the business processes from the perspective of OMG PLM Services 2.0 using a case study in industrial context, and proposes a methodology for the transformation of BPM towards PLM Services. The proposed methodology satisfies the business process constraints for the PLM services.
[business process modeling, OMG PLM Services 2.0 specification, product life cycle management, business management, business change management, enterprise modeling, Object Management Group, OMG, PLM services integration, product life cycle management service 2.0, PLM-Services, formal specification, PLM, Business Process Model &#x2014; BPM, management of change, systems engineering, business process constraint, Systems Engineering, business data processing]
Trust model simulation for supply chain management
2012 15th International Conference on Computer and Information Technology
None
2012
A supply chain management (SCM) is the management of network having interconnected business nodes which spans all movements of services and goods from the point of origin to the point of consumption through chaining the services within the network. In the context of SCM within PLM, Trust modeling is an important and crucial aspect from the perspective of sustainability of the supply chain and to gain efficient performance in management of product life cycle. In the supply chain, the more we trust, the more we exchange information on demand and on forecast of the last customer so as with the level of stock and on the forecast of the suppliers. In this work, we attempted to model the Trust in SCM for using Agent Modeling Language (AML) and proposed a Multi Agent System (MAS) SCM model of trust in supply chain management. The proposed model is implemented using Java Agent Development Environment (JADE) and the simulation results demonstrated the impact of trust in supply chain along with the evolution of trust.
[interconnected business nodes, AML, Java, multi-agent systems, product life cycle management, supply chain sustainability, digital simulation, Trust Model, Supply Chain, agent modeling language, trust model simulation, network management, multiagent system, sustainable development, PLM, security of data, information exchange, supply chain management, Java agent development environment, simulation languages, JADE, SCM, MAS]
Dhaka stock market timing decisions by hybrid machine learning technique
2012 15th International Conference on Computer and Information Technology
None
2012
Stock market prediction has been a challenging task due to the nature of the data which is very noisy and time varying. However, this theory has been faced by many empirical studies and a number of researchers have successfully applied machine learning approaches to predict stock market. The problem studied here is about stock prediction for the use of investors. It is true investors usually get loss because of unclear investment objective and blind investment. This paper proposes to investigate the rough set model, the artificial neural network model and the hybrid artificial neural network model and the rough set model for determining the optimal buy and sell of a share on a Dhaka stock exchange. Confusion matrix is used to evaluate the performance of the observed and predicted classes for selected models. Our experimental result shows that the proposed hybrid model has higher accuracy than the single rough set model and the artificial neural network model. We believe this paper will be useful to stock investors to determine the optimal buy and sell time on Dhaka Stock Exchange.
[confusion matrix, Technical indicators, optimal buy and sell, Dhaka stock exchange, Hybrid machine learning, investment, hybrid machine learning, Neural network, rough set model, hybrid artificial neural network model, Dhaka stock market timing decision, Stock market prediction, Rough set, stock market prediction, decision making, Confusion matrix, learning (artificial intelligence), rough set theory, stock markets, neural nets, blind investment, stock investors]
Crosstalk analysis in digital subscriber line by evaluating power spectral density function
2012 15th International Conference on Computer and Information Technology
None
2012
Digital Subscriber Line (DSL) combines the advantages of providing fast means of internet access with the usage of an existent infrastructure known as the plain old telephone service (POTS). Transmissions that use higher frequencies are more subject to attenuation and to crosstalk. Crosstalk is the undesired radiation of energy among wires. It is more likely to occur among/between similar frequencies and results in a received wave form that differs from the original wave form that was sent. In this article we analysis the crosstalk in various DSL systems by evaluating power spectral density function, i.e. functional principles of DSL and impairment caused by crosstalk. It is performed by the analysis of the results obtained from the Matlab program.
[Matlab program, digital subscriber line, Crosstalk, crosstalk analysis, Internet access, crosstalk, DSL systems, NEXT, POTS, FEXT, digital subscriber lines, power spectral density function evaluation, DSL, plain old telephone service, PSD function]
Recognition of hand gesture using hidden Markov model
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper we proposed a recognition system for hand gesture in 3D environment by using only a single camera. For calculating the relative motion towards the camera, generally a depth sensing device is needed. In order to remove that, we proposed an approach of using the change of the area of the hand in input image. Using skin color; we detect the hand from the input image sequences and then we process the data for feature extraction. Three features are proposed for effectively recognize the gesture by our system. These are orientation, area and angle of the palm. As we proposed our system for dynamic gesture, Hidden Markov Model is utilized to recognize the gesture. In our lab environment our proposed system shows very promising result and we were able to achieve about 80.67% recognition rate on average. The system that we proposed will not only help to recognize the gesture of hand accurately but also lessen the cost for implementing this kind of system because of using minimal number of hardware.
[dynamic gesture, Computer vision, skin color, hidden Markov model, relative motion calculation, depth sensing device, image sequence, Pattern recognition, image motion analysis, Human computer interaction, hidden Markov models, gesture recognition, hand detection, feature extraction, Hidden Markov models, image colour analysis, image sequences, hand gesture recognition]
Secure dynamic flow policy for content delivery networks
2012 15th International Conference on Computer and Information Technology
None
2012
Content Delivery Networks (CDNs) have evolved to overcome the inherent limitations of the Internet in terms of user perceived Quality of Service (QoS) when accessing Web content. A CDN replicates content from the origin server to cache servers (also called surrogates), scattered over the globe, in order to deliver content to end-users in a reliable and timely manner from nearby optimal surrogates. Like many other network systems and services, securing data is becoming a crucial need for CDNs. Deploying and broadcasting contents across the geographically distributed servers require reliably secured flow and access control policies. In this article, we have developed a security mechanism for CDN based on dynamic source routing protocol where flow control can be supported with dynamic flow scenario.
[DSR, user perceived quality of service, dynamic source routing protocol, cache storage, quality of service, CDN, Web content, content delivery networks, computer network security, content management, surrogates, access control policy, security, QoS, file servers, routing protocols, authorisation, cache servers, Internet, origin server, dynamic flow scenario, SDFP, dynamic flow policy security]
Basic HPSG structure for Bangla grammar
2012 15th International Conference on Computer and Information Technology
None
2012
This paper proposes a structure for detecting Bangla grammar which recognizes correct Bangla sentences and rejects incorrect ones based on Head-Driven Phrase Structure Grammar(HPSG). Context-free grammars (CFG) provide a powerful mechanism for language computation but it cannot detect the semantic errors. In case of CFG, generating huge number of grammar rules for a correct sentence is very complex. Lexical Functional Grammar (LFG) have been widely used to formally specify the syntax of natural language. In this case the LFG cannot recognize the semantic correctness of a sentence. To solve these kinds of problems we have used HPSG structure for recognizing Bangla grammar. We have used the LKB system for implementing our designed Bangla Grammar. Our designed Grammar can detect the correctness of a Bangla sentences for both syntax and semantics.
[text analysis, natural language processing, Bangla sentence recognition, Bangla grammar, computational linguistics, CFG, Bangla grammar recognition, LKB system, lexical functional grammar, head-driven phrase structure grammar, language computation, HPSG structure, LFG, context-free grammar, grammar rule, context-free grammars, syntax, Language Computation, semantic correctness, semantic error, HPSG, natural language]
Design and optimization of a low cost multi band microstrip patch antenna for K-band, Ku-band and X-band applications
2012 15th International Conference on Computer and Information Technology
None
2012
The main motto of this paper is to design the effective shape of a microstrip patch antenna which can provide lower return losses, better gain and performance for K-band (18 GHz to 26 GHz), Ku-band (12 GHz to 18 GHz) and X-band (2 GHz to 12 GHz) applications. Attempts have been made to optimize the antenna performance by using a single slot in different position in patch, by increasing the number of slots in vertical and horizontal direction and by using array technique. The simulation is performed by using GEMS simulator which is commercially available antenna simulator. The antenna is designed by using Taconic TLY-5 dielectric substrate with permittivity &#x25A1;<sub>r</sub> =2.2 and height, h=1.588 mm. Without using array technique we have got the return losses in the range of -20 db to -25 db at the frequencies around 19.5 GHz. The series feed array offers -13db return loss around 21GHz, -17.5 db return loss around 16 GHz and -7.25 db return loss around 11.5 GHz. Therefore, this antenna is suitable for K-band, Ku-band and X-band applications.
[return losses, series feed array technique, Ku-band applications, loss -20 dB to -25 dB, loss -7.25 dB, X-band applications, K-band, GEMS simulator, frequency 18 GHz to 26 GHz, UHF antennas, Taconic TLY-5 dielectric substrate, X-band, single slot, antenna feeds, Ku-band, permittivity, microstrip antenna arrays, K-band applications, low cost multiband microstrip patch antenna design, antenna simulator, frequency 2 GHz to 12 GHz, loss -17.5 dB, loss -13 dB, multifrequency antennas, slot antenna arrays, Microstrip patch antenna, Series feed array antenna]
On acyclic colorings of graphs
2012 15th International Conference on Computer and Information Technology
None
2012
An acyclic coloring of a graph G is a coloring of the vertices of G, where no two adjacent vertices of G receive the same color and no cycle of G contains vertices of only two colors. An acyclic k-coloring of a graph G is an acyclic coloring of G using k colors. In this paper we show the necessary and sufficient condition of acyclic coloring of a complete k-partite graph. Then we derive the minimum number of colors for acyclic coloring of such graphs. We also show that a complete k-partite graph G having n<sub>1</sub>, n<sub>2</sub>,..., n<sub>k</sub> vertices in its P<sub>1</sub>, P<sub>2</sub>,..., P<sub>k</sub> partition respectively is acyclically (2k - 1)-colorable using &#x03A3;<sub>i&#x2260;j, i, j&#x2264;k</sub> n<sub>i</sub>n<sub>j</sub> + n<sub>max</sub> + (k-1) - &#x03A3;<sub>i=0</sub>k-1 (k-i)n<sub>i+1</sub> division vertices, where n<sub>max</sub> = max(n<sub>1</sub>, n<sub>2</sub>,..., n<sub>k</sub>). Finally we show that there is an infinite number of cubic planar graphs which are acyclically 3-colorable.
[adjacent vertices, division vertices, Acyclic Chromatic Number, acyclic graph coloring, Cubic Planar Graph, Graph Subdivision, graph colouring, infinite number, Acyclic Coloring, complete k partite graph, cubic planar graphs, acyclic k coloring, fc-partite Graph]
Consistency analysis of RSSI measurement for distance estimation of Wireless Sensor nodes
2012 15th International Conference on Computer and Information Technology
None
2012
Wireless Sensor Devices are becoming more attractive as a sensor platform compared to wired sensor device because of the ease of installation and freedom in sensor placement. The field of Wireless Sensor Network (WSN) has been increasing with the advancement of modern science and technology. In order to determine the location of sensor nodes, lots of localization algorithm have been established and proposed by the researchers. Since the Received Signal Strength Indicator (RSSI) is assumed to be a strong parameter, most of researches have been worked out for localization algorithm using RSSI. But it is found that lots of irregular values of RSSI are produced at different conditions. Thus it is so much troublesome to get the actual position of sensor nodes using RSSI. In our experiments, we tried to investigate the reasons of deviation in RSSI values in different conditions using TelosB sensor nodes. We showed the effects of different obstacles between the sensor nodes and their different orientations as well. The results show that, the distances, obstacles and sensor node orientations play a significant role in changing the RSSI values.
[wireless sensor nodes, wireless sensor networks, sensor node orientations, received signal strength indicator, RSSI measurement consistency analysis, localization algorithm, TelosB sensor nodes, RSSI, WSN, distance measurement, sensor placement, distance estimation, wired sensor device, wireless sensor devices, Localization]
Android mobile application: Remote monitoring of blood pressure
2012 15th International Conference on Computer and Information Technology
None
2012
There has been an exponential increase in health care costs in the last decade. Seniors have to make frequent visits to their doctor to get their vital signs measured. There is a huge market for non-invasive methods of measurement of these vital signs. The objective of this paper is to design and implement a reliable, cheap, low powered, non-intrusive, and accurate system that can be worn on a regular basis and monitors the vital signs and displays the output to the user's cell phone. This paper specifically deals with the signal conditioning and data acquisition of vital sign: blood pressure. Blood pressure combines the methodologies of Electrocardiography to continuously monitor the systolic and diastolic blood pressure. Here remote monitoring of a patient's blood pressure (BP) is described. The data is transferred to a central monitoring station using a wireless sensor network for displaying and storing.
[blood pressure remote monitoring, Bluetooth, wireless sensor networks, central monitoring station, wireless sensor network, patient monitoring, systolic blood pressure, electrocardiography, Android mobile application, noninvasive method, mobile computing, signal conditioning, diastolic blood pressure, blood pressure measurement, data acquisition, reliable cheap low powered nonintrusive accurate system, health care, blood pressure, vital sign monitoring, Android cell phones, vital sign measurement, user cell phone, patient blood pressure, medical signal processing, cellular radio]
Free-space optical communication with M-ary pulse position modulation under gamma-gamma and negative exponential atmospheric turbulence model
2012 15th International Conference on Computer and Information Technology
None
2012
Free space optical (FSO) communication offers an attractive alternative to the radio frequency (RF) channel for the purpose of transmitting data at very high rates. Atmospheric turbulence can degrade the performance of FSO links, particularly over ranges of the order of 1 km or longer. The performance of the FSO link can be improved by employing an appropriate modulation scheme and channel model which makes a good compromise between complexity and performance. In this view, we consider in this paper the M-ary pulse position modulation (PPM) which has the interesting advantage of being average-energy efficient with gamma-gamma and negative exponential atmospheric turbulence model under turbulent condition. The performance results are evaluated in terms of symbol error probability (SEP) for different type of channel model.
[radiofrequency channel, Free Space Optics (FSO), optical links, distance 1 km, gamma-gamma atmospheric turbulence model, symbol error probability (SEP), negative exponential atmospheric turbulence model, symbol energy, atmospheric turbulence, PPM, FSO communication, optical modulation, M-ary pulse position modulation, symbol error probability, channel model, pulse position modulation (PPM), FSO links, free-space optical communication, RF channel, probability of density function (PDF), pulse position modulation, error statistics, SEP]
An automated design methodology for FPGA-based Multi-Gbps LDPC decoders
2012 15th International Conference on Computer and Information Technology
None
2012
Low density parity check (LDPC) codes are error-correcting codes that offer huge advantages in terms of coding gain, throughput and power dissipation. Error correction algorithms are often implemented in hardware for fast processing to meet the real-time needs of communication systems. However hardware implementation of LDPC decoders using traditional hardware description language (HDL) based approach is a complex and time consuming task. This paper presents an efficient automated high level approach to designing LDPC decoders using a collection of high level modelling tools. High data rate Multi-Gbps LDPC decoders have been developed and implemented on FPGA using the proposed methodology. These Multi-Gbps LDPC decoders can be utilized in the latest generation of high data rate wireless communication such as WLAN, WiMAX and DVB-S2.
[error correction codes, field programmable gate arrays, FPGA, parity check codes, automated design methodology, WiMAX, digital system, WLAN, data rate wireless communication, decoding, wireless communication, design automation, HDL, multiGbps LDPC decoder, hardware description language, DVB-S2, digital communication, Error correction coding, low density parity check code, LDPC code, error-correcting code, power dissipation]
Stock market prediction model using TPWS and association rules mining
2012 15th International Conference on Computer and Information Technology
None
2012
The objective of this research is to classify or forecast the stock market from the general investor's point of view. There are three parts in this research. In the first part we performed a survey on most of the well known data mining indicators, implemented the algorithms and calculated the accuracy by applying them on historical data. Then we presented an indicator algorithm which has higher accuracy compare to existing algorithms and it also provides a decision point that helps the investor to understand the significance of the result of the indicator. Finally we applied association rules mining to group the selected (based on precision) indicator algorithms to come up with a model to increase the overall accuracy. However motivating fact is we achieved far better results from our suggested model than other comparable indicator algorithms or strategy. For our research we used the data of Dhaka Stock Exchange (DSE), capital market of Bangladesh.
[data mining indicator, Technical analysis, Stock market forecastin, Dhaka stock exchange, data mining, stock market prediction model, DSE, stock markets, Association rules mining, association rules mining, TPWS, Rule extraction]
Distributed k-dominant skyline queries
2012 15th International Conference on Computer and Information Technology
None
2012
Skyline query function is one of promising information filtering methods. Skyline queries return a set of interesting data objects that are not dominated by any other object on all dimensions. Therefore in this paper, we consider k-dominant skyline computation when the underlying dataset is partitioned into geographically distant computing core that are connected to the coordinator (server). The existing solutions are not suitable for our problem, because they are restricted to centralized query processors, limiting scalability and imposing a single point of failure. In this paper, we developed a distributed k-dominant skyline queries (DKSQ) computation algorithm. Where the coordinator iteratively transmits data to each computing core. Computing core is able to prune a large amount of local data, which otherwise would need to be sent to the coordinator. Extensive performance study shows that proposed algorithm is efficient and robust to different data distributions.
[server, Skyline, coordinator, data distribution, k-dominant skyline, distributed processing, information filtering, Load Balancing, skyline query function, query processing, query processor, distributed k-dominant skyline query, information filtering method, data object, data transmission, Distributed, DKSQ computation algorithm, data handling, k-dominant skyline computation, dataset, geographically distant computing core]
A CLONALG-based approach for the set covering problem
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper, we have proposed a CLONALG-based simple heuristics, which is one of the most popular artificial immune system (AIS) models, for the non-unicost set covering problem (SCP). It is well known that SCP is NP-hard problem that can model several real world situations such as crew scheduling in airlines, facility location problem, production planning in industry etc. In real cases, the problem instances can reach huge sizes, making the use of exact algorithms impracticable. So, for finding practically efficient approaches for solving SCP, different kind of heuristic approaches have been applied in the literature. To the best of our knowledge our work here, is the first attempt to solve SCP using Artificial Immune System. We have evaluated the performance of our algorithm on a number of benchmark instances. Computational results have shown that it is capable of producing high-quality solutions.
[SCP, AIS models, NP-hard problem, CLONALG-based simple heuristics, CLONALG-based approach, artificial immune system model, nonunicost set covering problem, artificial immune systems, computational complexity]
An approach to identify myopathy disease using different signal processing features with comparison
2012 15th International Conference on Computer and Information Technology
None
2012
Myopathy, one of the most frequent inherited musculoskeletal diseases resulting in muscular weakness. Muscle cramps, tautness &amp; spasm are also associated with myopathy. The electromyography (EMG) signals are biomedical signals that examine the muscle function through the inquiry of the electrical signal the muscles emanate. As the nervous system controls the muscle activity, the EMG signals can be viewed and analyzed in order to recognize the indispensable features of myopathy disease in individuals. The aim of this work is to dissociate the myopathic signals by studying the time &amp; frequency domain features of the EMG signals. In this paper, autocorrelation (ACR), zero crossing rate (ZCR) as time domain features, mean frequency as frequency domain feature and Short Time Fourier Transform (STFT) as Time-frequency feature; are extensively analyzed on EMG signals of both the normal persons and the patients to successfully distinguish the patients from normal group. In order to comprehend this aim, EMG signal database was obtained from a normal control group consisted of 6 healthy persons &amp; a group of patients with myopathy consisted of 6 patients. The analytical results show that myopathic signals have lower autocorrelation peak then the healthy ones and zero crossing rate and mean frequency of the affected signals are higher than the normal persons. In addition to that the power level of the spectrogram of the myopathy patients is considerably lower than that of the normal group and frequency shifting to higher frequency region for peak values.
[muscle function, musculoskeletal diseases, frequency shifting, signal processing feature, muscle cramps, myopathy patients, myopathy disease, muscular weakness, short time fourier transform (STFT), time frequency domain feature, myopathy, EMG signal database, diseases, electromyography signal, zero crossing rate, zero crossing Rate (ZCR), autocorrelation, short time Fourier transform, medical signal processing, myopathic signal, nervous system, electromyography (EMG), spectrogram, autocorrletion (ACR), electromyography, mean frequency, muscle activity, electrical signal, biomedical signal]
Speckle noise reduction from ultrasound images using principal component analysis with bit plane slicing and nonlinear diffusion method
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper we present and evaluate a novel method for an efficient speckle denoising by using principal component analysis (PCA) with bit plane slicing and nonlinear diffusion. We use PCA transformation for generating de-correlated dataset from a noisy image. Then we apply bit plane slicing on the de-correlated dataset and nonlinear diffusion is applied on each bit plane. For nonlinear diffusion in each bit plane level, a gradient threshold is automatically estimated. Add up all bit plane slice after nonlinear diffusion execution and then we implement inverse principal component analysis for making denoised images. The proposed speckle reduction method could improve image quality and the visibility of small structures and fine details in medical ultrasound imaging compared with state-of-the-art speckle denoising algorithms.
[bit plane slice, diffusion, Speckle noise, denoised images, state-of-the-art speckle denoising algorithms, biomedical ultrasonics, medical ultrasound imaging, Denoising, noisy image, nonlinear diffusion execution, PCA transformation, gradient methods, ultrasound images, medical image processing, speckle noise reduction, nonlinear diffusion method, image denoising, inverse principal component analysis, PCA, image quality, Bit Plane Slicing, bit plane slicing, Ultrasound image, decorrelated dataset, gradient threshold, Nonlinear diffusion, speckle reduction method, principal component analysis, ultrasonic imaging]
Development of a remote digital system laboratory
2012 15th International Conference on Computer and Information Technology
None
2012
This paper presents a new technique that allows an instructor/student to bring the hardware construction and the laboratory experiments into the Digital System Lab environment through the Internet without being near the actual equipments. It offers relax time constraints, a cost effective way of opening a Laboratory for 24 hours for students locally in a Laboratory. It also enhances sharing of knowledge, expertise and experience. An experimental unit, called e-Lab, is prototyped and tested for variety of design examples. It consists of a PLD/FPGA Board interfaced with a PC using RS232 interface via customized hardware. The custom hardware uses low cost data acquisition components. A Graphical User Interface (GUI) has been designed for communication between the local PC terminal and the remote e-Lab system. This allows the user to take over the entire control of the system. Students at the local PC can access the Lab PC via internet using Microsoft's `Remote Desktop Connection' facility. The prototype uses a Web Cam to provide a view of the FPGA system operation for the remote user. It ensures the remote student's about correctness of the system operation and enhances confidence through the visual access of the experimental results.
[field programmable gate arrays, graphical user interfaces, knowledge sharing, remote digital system laboratory development, FPGA, customized hardware, local PC terminal, PLD, remote student, Embedded System, experimental unit, remote user, GUI, FPGA system operation, microcontrollers, hardware construction, PLD/FPGA board, Microcontroller, graphical user interface, digital system lab environment, lab PC, Microsoft remote desktop connection, visual access, distance learning, laboratory experiment, low cost data acquisition component, Web cam, microcontroller, RS232 interface, Remote laboratory, student experiments, peripheral interfaces, Reconfigurable Computing, Internet, relax time constraint, remote e-lab system]
Local Gabor directional pattern for facial expression recognition
2012 15th International Conference on Computer and Information Technology
None
2012
Humans display emotions through facial expressions. Detecting and recognizing these expressions comes naturally to us. But recognizing expressions is not straight forward for machines. In recent years a lot of literatures surged concerning automatic facial expression recognition. While some of these efforts have generated very good results, room for improvement still remains. There are several ways facial expression recognition can be performed. In this work we are proposing an appearance based method for extracting a new type of image descriptor based on Gabor wavelet which can be used for creating a feature vector of an image. To achieve this goal we applied Gabor filter over the image and recorded the edge response at each pixel. For each pixel we take several responses on several orientations. Finally we encode the responses and create a feature vector from the generated codes. We also showed experimental results on standard database for facial expression recognition using our proposed method.
[response encoding, Gabor filter, automatic facial expression recognition, facial expression, image descriptor extraction, wavelet transforms, appearance based method, emotion recognition, gabor filter, Gabor wavelet, local Gabor directional pattern, expression detection, feature extraction, local directional pattern, face recognition, image feature vector, Gabor filters, human emotion, edge response]
Electrical load forecasting using echo state network
2012 15th International Conference on Computer and Information Technology
None
2012
An algorithm for half hourly electrical load forecasting based on echo state neural networks (ESN) is proposed in this paper. Electrical load forecasting is one of the most challenging real life time series prediction problems. This demands a dynamic network. ESN is a new epitome for using recurrent neural networks (RNNs) with a simpler training method. Several versions of ESN are discussed. The load profile is treated as time series signal. The forecasting performance of ESN is analysed on the basis of its key parameters. ESN is compared with feed forward neural network (FNN) and Bagged Regression trees. Simulation results demonstrate that the proposed ESN algorithms can obtain more accurate forecasting results than the FNN and Bagged Regression trees.
[Echo State Network (ESN), ESN, time series prediction, recurrent neural nets, trees (mathematics), regression analysis, FNN, time series, Bagged Regression trees, echo state network, power engineering computing, dynamic network, load profile, feed forward neural network, Feed forward neural network (FNN), RNN, Recurrent Neural Network (RNN), recurrent neural networks, load forecasting, electrical load forecasting, bagged regression trees, Electrical load forecasting, feedforward neural nets, time 0.5 hour]
A comprehensive tool for text categorization and text summarization in bioinformatics
2012 15th International Conference on Computer and Information Technology
None
2012
The work focuses on the integration of text categorization and text summarization tasks based on some existing algorithms. We primarily employ the method for bioinformatics literatures to categorize them in relevant domains of bioinformatics and then get a summarized overview of each of the documents in the domain. For text categorization we have chosen three different and core domains of bioinformatics: Protein-Protein Interaction, Disease-Drug Relevance and Pathway-Process Involvement. The method uses TF-IDF based technology for the categorization task and then after categorization it summarizes the key contents of each document using some existing features. The system plays important role in automatically reducing review spaces for the researchers as they do not need to manually select their relevant texts. It also saves time by providing ranked and significantly relevant lines of the documents. Our method outperforms other existing summarization tools in the sense that it optimizes summarization by first categorizing the documents on the basis of TF-IDF technology and then avoids redundant information by properly ranking the sentences using existing score.
[automatic review space reduction, text analysis, disease-drug relevance, drugs, summarization tool, Pathway, bioinformatics literature, diseases, text categorization, Text Summarization, Text Categorization, pathway-process involvement, SumBasic score, sentence ranking, text summarization task, proteins, bioinformatics, TF-IDF, TF-IDF based technology, protein-protein interaction, redundant information]
Design of a compact multiband antenna on Al2O3 ceramic material substrate
2012 15th International Conference on Computer and Information Technology
None
2012
A compact electrically small multiband rectangular shape microstrip patch antenna is designed and analyzed in this paper. The antenna designed with 13 &#x00D7; 10 mm2 radiating copper patch on Aluminium oxide high dielectric ceramic material substrate with ground plane for three different frequency application which is smaller in size compared to other available multiband antennas. The proposed antenna is designed and analyzed by using finite element method 3-D full wave electromagnetic high frequency structure simulator. From the result it is observed that, in 4.7 GHz, 9.05 GHz and 13.2 GHz resonance frequency the impedance bandwidth (VSWR &#x2264; 2) below -10 dB return loss obtained are 400 MHz, 200 MHz, 800 Mhz and achieved gain are 2.53 dBi, 6.72 dBi, 1.95 dBi respectively.
[ceramics, 3D full wave electromagnetic high frequency structure simulator, dielectric ceramic, frequency 4.7 GHz, frequency 13.2 GHz, microstrip patch antenna, dielectric materials, microstrip antennas, finite element analysis, compact electrically small multiband rectangular shape microstrip patch antenna, bandwidth 800 MHz, bandwidth 400 MHz, alumina, finite element method, radiating copper patch, compact multiband antenna design, Alunimium oxide, multifrequency antennas, ground plane, multiband, aluminium oxide high dielectric ceramic material substrate, bandwidth 200 MHz, frequency 9.05 GHz, Al<sub>2</sub>O<sub>3</sub>]
An adaptive resource management scheme for heterogeneous wireless networks
2012 15th International Conference on Computer and Information Technology
None
2012
The demand for mobile radio communications is ever increasing and is expected to be high bandwidth. However, there is a limit to current radio frequency resources. A good resource management scheme in integrated wireless LAN and cellular networks should support as many mobile &amp; laptop users as possible while maintaining the necessary quality of service. It is widely accepted that connection dropping is more annoying than connection blocking and more important in determining user satisfaction. In this paper, we present a flexible and dynamic resource management scheme, namely modified Dynamic Resource Allocation with Flexible Reservation (mDRAFR), for heterogeneous wireless networks. The proposed resource allocation scheme considers two different handoff schemes (Horizontal &amp; Vertical) and allocation of channels can be flexible to adapt to the different scenarios. We can change the portion of reserved channels according to different area and traffic, like rural area, urban area and highway.
[radio frequency resources, heterogeneous networks, adaptive resource management scheme, Resource allocation, connection dropping, modified dynamic resource allocation with flexible reservation, mobility management (mobile radio), quality of service, laptop, resource allocation scheme, mobile radio communications, resource allocation, horizontal and vertical handoff, integrated wireless LAN, mDRAFR, heterogeneous wireless networks, dynamic resource management scheme, call admission control, channel allocation, wireless LAN, cellular networks, connection blocking, cellular radio, flexible resource management scheme]
Novel approaches for detecting fabric fault using Artificial Neural Network with K-fold validation
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper we have proposed a novel method to detect the defects in woven fabric based on the abrupt changes in the intensity of fabric image due to the defects and have constructed a classification model to properly identify the defects. We have also improved an existing method based on histogram processing for the classifier. In classification model we have implemented Artificial Neural Network (ANN). Both of our newly proposed method and improved technique have outperformed the existing methods. We have implemented K-validation to estimate the performance of our classification model. Additionally we have analyzed the performance of our classification model for different experimental parameters. Finally we have presented a comparative analysis of these techniques.
[woven composites, fabric image, artificial neural network, ANN, K-Validation, image classification, production engineering computing, object detection, woven fabric, classification model, K-fold validation, fabrics, Roberts Operator, Adaptive Median Filter, Artificial Neural Network, histogram processing, neural nets, fabric fault detection]
An efficient error correction coding approach to tolerate soft error
2012 15th International Conference on Computer and Information Technology
None
2012
Soft errors are significant concern for circuit reliability. Among various adopted techniques to mitigate soft errors effects on system functionality, Error Correcting Code (ECC) is one of the dominant approaches since it reduces cost overhead significantly. This paper deals with a new error correcting coding approach to tolerate soft errors at processor, data path, and memory devices of a computer system. To detect soft errors, the proposed approach introduces `modular checking' which in turns, is working (decoding and comparing) with even and/or odd numbers of row and column of a two-dimensional array organization of pre-encoded data set rather than the whole matrix. Hence, it reduces the cost by lessen the circuitry needed to implement the scheme and the detection time is reduced as well if the soft error is detected at early modular checking. The proposed approach is able to detect soft errors at any of bit positions and correct those efficiently with reduced cost as well as detection time in a great context.
[cost overhead reduction, soft error, data path, two-dimensional array organization, parity, integrated circuit reliability, modular checking, precoding, modular-checking, coding, circuit reliability, detection time, soft errors, error correction codes, ECC, memory device, radiation hardening (electronics), computer system, bit position, encoding, processor, decoding, odd number, system functionality, preencoded data set, error correcting code]
Wireless power transfer: An application to cell phone battery recharging
2012 15th International Conference on Computer and Information Technology
None
2012
A simple prototype wireless power transfer (WPT) system has been proposed and practically implemented to explore the possibility of cell phone battery recharging using WPT technique. In the proposed system, radio frequency approach has been utilized employing a FM transmitter and receiver. Our preliminary experimental results demonstrate that the cell phone battery can be recharged wirelessly by FM transmission and reception technique for which at least 1.25 mW signal strength is required at the receiver for the proposed system.
[cell phone battery recharging, WPT system, FM transmitter, radiofrequency approach, Cell phone battery recharging, FM receiver, FM transmission-reception technique, signal strength, Wireless power transfer, wireless power transfer, WPT, inductive power transmission, FM transmitter and receiver, mobile handsets]
A block based data hiding method in images using pixel value differencing and LSB substitution method
2012 15th International Conference on Computer and Information Technology
None
2012
Information hiding is a historical but on demand fascinating research area. Now, in today's world, the availability of information has become so easy that the security and insecurity of information goes side by side. In order to provide security of information a science called, Steganography has emerged. Steganography conceals the existence of information into images to formulate a secure communication. In this paper, a Steganographic method based on Pixel Value Differencing (PVD) and LSB Substitution method is proposed. To meet the increasing demand for privacy and secrecy the method focused mainly on making it a robust, secured technique of information hiding. An efficient and dynamic embedding algorithm is proposed in this paper that not only hides the secret data with an imperceptible visual quality and increased capacity but also make secret code breaking a good annoyance for the attacker. This method also represents an extraction algorithm that effectively extracts the entire secret message without any loss of a single data. Finally, experimental results show that, the proposed method achieves an increased embedding capacity and lower image degradation with improved security as compared to LSB substitution method and some recent existing methods of data hiding.
[image, secret message extraction algorithm, information hiding, steganographic method, information security, embedding capacity, block based data hiding method, Stego Image, LSB substitution method, dynamic embedding algorithm, Steganography, LSB Substitution Method, steganography, pixel value differencing, image degradation, image coding, PVD, Steganalysis PVD, Cover Image]
Human authentication process using finger knuckle surface with artificial Neural Networks based on a hybrid feature selection method
2012 15th International Conference on Computer and Information Technology
None
2012
An improved human authentication process using knuckle surface for personal identification has shown promising results. The texture pattern produced by the finger knuckle bending is highly unique and makes the surface a distinctive biometric identifier. In this paper we proposed a new approach for efficient and more secure personal identification using knuckle surface. A specific data acquisition device is constructed to capture the finger knuckle surface images, and then an efficient finger knuckle print algorithm is presented with trained neural network. The finger back surface images from each of the users are normalized to minimize the scale, translation and rotational variations in the knuckle images. The main attraction of this proposed method is that a hybrid feature selection method of Lempel-Ziv Feature Selection and Principle Component Analysis is used for feature extraction and an artificial Neural Network based on Scaled Conjugate Gradient is used for the recognition. The experimental results from the proposed approach are promising and confirm. Compared with the other existing finger-back surface based biometric systems, the proposed system is more efficient and can achieve higher recognition rate in real time.
[conjugate gradient methods, finger knuckle print algorithm, artificial neural network, human authentication process, secure personal identification, texture pattern, translation variation, biometrics (access control), finger knuckle surface, scale variation, rotational variation, data acquisition device, feature extraction, finger knuckle bending, principle component analysis, hybrid feature selection method, data acquisition, data compression, finger knuckle surface image capture, scaled conjugate gradient, finger geometry, finger-back surface based biometric system, image texture, security of data, Human detection, Lempel-Ziv feature selection, hybrid feature selection, distinctive biometric identifier, image recognition, neural nets, principal component analysis, finger back surface image normalization]
Performance analysis of short channel GaAs MESFET fabricated by SAINT method
2012 15th International Conference on Computer and Information Technology
None
2012
In nanoelectronics the speed of the device is the main concern along with their size. The device integrated in nanospace, needs to work faster. The traditional silicon field effect transistors have low sensitivity and low mobility which limit device speed compare to compound semiconductors devices. On the other hand compound metal field effect transistor (MESFET) shows very high cutoff frequency, around 15GHz, for 1 μm channel whereas NMOS shows only 2GHz. The high mobility of GaAs plays the pivotal roles for this enhanced speed of MESFET. Speed of the device also increases for shorter channel length. However fabrication of short channel MESFET needs very complicated lithography process. Self aligned implantation for N+ layer technology (SAINT) is a promising method for fabricating short channel MESFET. To achieve short channel MESFET SAINT method has been opted in this work. In this paper we simulated SAINT short channel MESFET using TCAD tool then evaluate the characteristics of voltage and current for different channel length.
[gallium arsenide, self aligned implantation for N+ layer technology, MESFET, frequency 2 GHz, SAINT method, size 1 mum, short channel, silicon field effect transistors, compound metal field effect transistor, lithography, Schottky gate field effect transistors, short channel MESFET, GaAs, technology CAD (electronics), silicon, channel length, TCAD, nanoelectronics, TCAD tool, performance analysis, III-V semiconductors]
Noise reduction algorithm for LS channel estimation in OFDM system
2012 15th International Conference on Computer and Information Technology
None
2012
Wireless communication system incorporating coherent OFDM requires the estimation and tracking of the channel Impulse Response (CIR) for accurate demodulation of data at the receiver. In pilot-symbol-aided OFDM system, Minimum Mean Square Error (MMSE) estimator performs better than Least Square (LS) estimator; however, computational complexity associated with the MMSE method is relatively higher than the LS. Although, the LS estimator has lower complexity and requires minimum knowledge the channel state information, the estimator suffers from inherent additive Gaussian noise and Inter Carrier Interference (ICI). Following that, in this study an efficient and improved channel estimation technique is proposed based on the LS algorithm. Simulation results show that the proposed method performs considerably better than the conventional LS method for a range of Signal to Noise Ratios (SNRs). In addition, the performance of the proposed method is found to be almost equal, if compared with the MMSE estimator. Despite the proposed method experiences relatively higher computational complexity than the LS, the complexity is yet to be achieved about 40% lower than the MMSE.
[OFDM, transient response, pilot-symbol-aided OFDM system, minimum mean square error estimator, LS, LS channel estimation, demodulation, communication complexity, MMSE estimator, CIR, ICI, channel impulse response, Complexity, AWGN channels, wireless communication system, additive Gaussian noise, Channel Estimation, radiocommunication, noise reduction algorithm, mean square error methods, signal to noise ratio, intercarrier interference, data demodulation, computational complexity]
Remote-touch: Augmented reality based marker tracking for smart home control
2012 15th International Conference on Computer and Information Technology
None
2012
People of physical limitations like old age people, physically disabled people, and autistic people face a lot of challenges to accomplish their daily tasks. It is hard for them to operate regular appliances (like fan, light, opening or closing the window) which may seem very casual for us who are physically blessed. Now a days touch based controlling has greater psychological impact on human mind. Touch uses the user's imagination power more. Driven by these key thinking, we have thought of a home appliance controlling system - touch based smart home which augments the users real life experience. In this paper we describe the development of a prototype that uses the augmented reality based on touch controlling system to control the daily home appliances in a smart home. We have used a smart phone to design the user interface, QR code to track appliances, web service to communicate between the central controlling system and a client app (the smart phone), through X10 to control the appliances physically.
[handicapped aids, human mind, autistic people, tactile sensors, Web service, assisted living, augmented reality, touch based smart home, Remote-touch, smart home, real life experience, user interfaces, augmented reality based marker tracking, physically disabled people, domestic appliances, psychological impact, marker tracking, home appliance controlling system, old age people, central controlling system, SOA, daily home appliances, smart phones, touch controlling system, user interface, elderly care, home automation, Web services, touch based controlling, user imagination power, QR code, remote-touch, smart home control]
A survey of mobility management in wireless mesh networks
2012 15th International Conference on Computer and Information Technology
None
2012
Wireless mesh networking (WMN) has received unprecedented attention from the research community due to its cost effectiveness for providing high bandwidth internet access. For realizing large scale wireless mesh network with seamless communication, mobility management is a vital issue. Although remarkable contribution has been made on mobility management in WMNs, it needs more attention for improvement of real time communication. In this article, an investigation on IP-based mobility management schemes in WMNs is conducted. By analyzing the existing and current research works on mobility management in WMNs, this paper proposes the open challenges at the end.
[Mobility Management, Pointer Forwarding, high bandwidth Internet access, mobility management (mobile radio), large scale wireless mesh network, wireless mesh networks, IP-based mobility management, Wireless mesh networks, cost effectiveness, Channel splitting, WMN, seamless communication, Hybrid Routing, Internet, IP networks, Mobility Prediction]
Single fed orthogonal linear polarization switchable microstrip planar antenna
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper, a concept and investigation of orthogonal linear polarization switchable planar antenna is proposed. In order to switch the polarization axis, four switching diodes are loaded on the upper plane of the antenna. The boundary condition of the planar antenna is controlled by the ON/OFF condition of the diodes. The antenna can switch the polarization at &#x00B1;45 deg. &#x25A1; and 0 &amp; 90 deg., depending on the polarity of the bias voltage and the position of the diodes. The FDTD method introduced by Yee is used to simulate the antenna which is followed by the experimental investigation.
[semiconductor diodes, Microstrip planar antenna, polarization axis, switching diodes, Orthogonal linear polarization, Polarization Switching, antenna feeds, microstrip antennas, finite difference time-domain analysis, planar antennas, single fed orthogonal linear polarization switchable microstrip planar antenna, FDTD method]
Performance of decode and forward MIMO relaying for wireless uplink
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper, performance of relay assisted wireless link is evaluated in the presence of rayleigh fading where mobile handset is equipped with single transmit antenna, relay is equipped with multiple transmit and receive antennas, and destination has multiple receive antennas. Data are modulated using QPSK or 16 QAM or 64 QAM modulator at handset and send to relay which combined and decode the incoming signal using Maximum Likelihood decoding and further encode the symbols using STBC, and the encoded data are split into n streams which are simultaneously transmitted using n transmit antennas of relay. It is observed that relay with 2 transmit antennas and 2,3,4,5 and 6 receive antennas provides 13 dB, 11 dB, 9 dB, 6 dB and 5 dB gains respectively compared to direct link at 10-5. And there are around 1.5 dB to 6 dB gains for increasing number of receiving antennas of relay from 2 to 3 or 3 to 4 but there are very little gains for increasing number of receiving antennas of relay from 4 to 5 or 5 to 6.
[radio networks, Relay, transmitting antennas, STBC, Space Time Block Code, Decode and Forward, relay assisted wireless link, quadrature amplitude modulation, quadrature phase shift keying, decode and forward communication, QAM modulator, QPSK, receive antennas, Wireless Communication, MIMO, MIMO relaying, wireless uplink, Uplink, MIMO communication, relay networks (telecommunication), Rayleigh channels, maximum likelihood decoding, transmit antennas, space-time block codes, Rayleigh fading]
Resolving scalability issue to ontology instance matching in Semantic Web
2012 15th International Conference on Computer and Information Technology
None
2012
Ontology instance matching is a key interoperability enabler across heterogeneous data sources in the Semantic Web and a useful maneuver in some classical data integration tasks dealing with the semantic heterogeneous assignments. Though most of the research has been conducted on ontology schema level matching so far, with the introduction of Linked Open Data (LOD) and social networks, research on ontology matching is shifting from ontology schema or concept level to instance level. Since heterogeneous sources of massive ontology instances grow sharply day-by-day, scalability has become a major research issue in ontology instance matching of semantic knowledge bases. In this paper, we propose an efficient method by grouping instances of knowledge base into several sub-groups to address the scalability issue. Then, our instance matcher, which considers the semantic specification of properties associated to instances in the matching strategy, works by comparing an instance within a classification group of one knowledge base against the instances of same sub-group of other knowledge base to achieve interoperability. A novel approach for measuring the influence of properties in the matching process is also presented. The experiment and evaluation depicts satisfactory results in terms of effectiveness and scalability over baseline methods.
[heterogeneous data sources, pattern matching, Record Linkage, open systems, scalability issue resolving, ontology schema level matching, Ontology alignment, formal specification, semantic knowledge base, Ontology Population, property semantic specification, knowledge based systems, Semantic Knowledge base, semantic heterogeneous assignments, Linked Open Data, classical data integration tasks, LOD, pattern classification, Ontology Instance Matching, semantic Web, social networks, classification group, interoperability, instance level, Anchor-flood Algorithm, Linked Data, ontology instance matching, Identity Recognition, ontologies (artificial intelligence), social networking (online), concept level, instance matcher, data integration]
Automated recharge of prepaid mobile phones
2012 15th International Conference on Computer and Information Technology
None
2012
This paper presents a dynamic approach for online mobile phone recharge. Over the past several years, percentage of overall prepaid users has increased rapidly. However, the recharging process is still somehow manual. The system proposed in this paper shows an automated way to recharge the prepaid account. This approach will have a great business impact both in local and global aspects. The effectiveness has been proved with a SMSC server and a recharge SIM.
[mobile radio, prepaid user, Immigrant, dynamic approach, business impact, recharging process, Online Recharge, File Watcher, Parser, SMSC Server, automated recharge, recharge SIM, prepaid mobile phone, online mobile phone recharge, prepaid account, SMSC server]
A digital video watermarking technique based on identical frame extraction in 3-Level DWT
2012 15th International Conference on Computer and Information Technology
None
2012
Digital video watermarking was introduced at the end of the last century to provide means of enforcing video copyright protection. Video watermarking involves embedding a secret information in the video. In this paper, we proposed a digital video watermarking technique based on identical frame extraction in 3-Level Discrete Wavelet Transform (DWT). In the proposed method, first the host video is divided into video shots. Then from each video shot one video frame called identical frame is selected for watermark embedding. Each identical frame is decomposed into 3-level DWT, then select the higher subband coefficients to embed the watermark and the watermark are adaptively embedded to these coefficients and thus guarantee the perceptual invisibility of the watermark. For watermark detection, the correlation between the watermark signal and the watermarked video is compared with a threshold value obtained from embedded watermark signal. The experimental results demonstrate that the watermarking method has strong robustness against some common attacks such as cropping, Gaussian noise adding, Salt &amp; pepper noise adding, frame dropping and frame adding.
[watermark signal, 3-level DWT, Video Shot, discrete wavelet transforms, Gaussian noise adding attack, discrete wavelet transform, digital video watermarking technique, identical frame extraction, video watermarking, Identical Frame, salt and pepper noise adding attack, frame dropping attack, Gaussian noise, feature extraction, watermarked video, frame adding attack, watermark detection, cropping attack, secret information embedding, Video Watermarking, Discrete Wavelet Transform, video copyright protection, subband coefficient]
Implementation of an efficient bangla soft keyboard with text-to-image replacement support
2012 15th International Conference on Computer and Information Technology
None
2012
In present world, Bangla is one of the most spoken languages. But, in the arena of electronic devices, tools for Bangla writing are inadequate. Also, there are limitations for typing Bangla faster in an efficient way because of the lack of proper layout. After forty-one years of the liberation war, an optimal and widely accepted Bangla keyboard layout has not yet been obtained. And in present days, smart-phones and tabs are being used profoundly where soft keyboard is the most essential input method. In Bangla soft keyboard, the arrangement of letters should be in such a way that paves the way towards an easy and efficient text input method. Our primary evaluation shows a significant improvement in typing speed, ease of learning and reduction in error rates. Hence we propose an efficient Fixed Bangla Keyboard Layout where Bangla alphabets are mapped phonetically on the standard QWERTY keyboard and a Bangla text-to-image replacement technique for non-Bangla supported smartphones. In this paper we present a soft keyboard implemented on Android platform using this layout with text-to-image conversion support.
[text analysis, Soft keyboard, keyboards, QWERTY keyboard, Keyboard layout, Text to image replacement (Bangla), smart-phone, Bangla, document image processing, text-to-image replacement, smart phones, text-to-image conversion support, Bangla writing, typing speed, Android, Bangla alphabet, text input method, electronic device, Android platform, natural languages, Bangla soft keyboard, fixed Bangla keyboard layout]
Obstacles invariant navigation of an autonomous robot based on GPS
2012 15th International Conference on Computer and Information Technology
None
2012
Robotics has a momentous features and application in our daily life. It can make our life faster easier and comfortable. Many researchers around the world are trying to develop an effective robotic system for household or industrial equipment management purposes. Global positioning System (GPS) is good for reaching high accuracy techniques to track the equipment current position. Motivating from this works in this paper we proposed the design and development of a prototype system which is based on GPS navigation and IR sensor for detecting and avoiding obstacle in dynamic context. As per the initial experimentation, we found out that the proposed approach of obstacle avoiding mechanism is appealing and useful to the user. Some of the potential applications of the proposed system include household appliances or hospital apparatus movement and children guidance.
[collision avoidance, obstacle avoidance, Autonomous Robot, GPS navigation, robotic system, infrared detectors, obstacle detection, children guidance, mobile robots, autonomous robot, domestic appliances, Micro-controller, household appliance, Infrared sensors, hospital apparatus movement, industrial equipment management purpose, Global Positioning System, home automation, GPS Navigation, equipment current position, IR sensor, obstacle invariant navigation, household equipment management purpose, prototype system development, prototype system design]
Development of a device for remote monitoring of heart rate and body temperature
2012 15th International Conference on Computer and Information Technology
None
2012
We present a new integrated, portable device to provide a convenient solution for remote monitoring heart rate at the fingertip and body temperature using Ethernet technology and widely spreading internet. Now a day, heart related disease is rising. Most of the times in these cases, patients may not realize their actual conditions and even it is a common fact that there are no doctors by their side, especially in rural areas, but now a day's most of the diseases are curable if detected in time. We have tried to make a system which may give information about one's physical condition and help him/her to detect these deadly but curable diseases. The system gives information of heart rate and body temperature simultaneously acquired on the portable side in realtime and transmits results to web. In this system, the condition of heart and body temperature can be monitored from remote places. Eventually, this device provides a low-cost, easily accessible human health monitor solution bridging the gaps between patients and doctors.
[integrated portable device, Infrared Receiver, biomedical equipment, fingertip, local area networks, rural area, Infrared Transmitter, patient monitoring, biothermics, heart rate information, physical condition, Ethernet technology, human health monitor solution, health care, telemedicine, Heart Rate, remote monitoring device, Microcontroller, Body Temperature, cardiology, body temperature information, remote place, heart related disease, Ethernet, patient condition, Internet, medical computing, haemodynamics]
Composite pattern matching in time series
2012 15th International Conference on Computer and Information Technology
None
2012
For last few years many research have been taken place to recognize various meaningful patterns from time series data. These researches are based on recognizing basic time series patterns. Most of these works used template based, rule based and neural network based techniques to recognize basic patterns. But in time series there exist many composite patterns comprise of simple basic patterns. In this paper we propose two novel approaches of recognizing composite patterns from time series data. In our proposed approach we use combination of template based and rule based approaches and neural network and rule based approaches to recognize these composite patterns.
[pattern matching, Rule based approach, time series, Pattern recognition, Neural network, rule based technique, Composite pattern, Template based approach, template based technique, knowledge based systems, neural network based technique, time series data, composite pattern matching, neural nets, time series pattern recognition]
Investigation of performance parameters of different wearable narrowband antennas in close proximity to the human body
2012 15th International Conference on Computer and Information Technology
None
2012
This paper investigates and compares the on-body performance parameters of five different narrowband antennas at 2.45 GHz. Parametrical analysis addressing the human body effects on the performance parameters of different types of narrowband antennas have been presented. The study was performed by combination of numerical simulation and experimental investigation. The antennas used in this study are microstrip fed patch, wire monopole, printed monopole, inverted L and circular loop.
[frequency detuing, circular loop antennas, inverted L-antennas, microstrip antennas, human body effects, printed monopole antennas, biological effects of microwaves, close proximity, wearable narrowband antennas, microstrip fed patch antenna, frequency 2.45 GHz, body-centric wireless communications, wire antennas, Narrowband antennas, loop antennas, numerical analysis, UHF antennas, on-body performance parameters, wire monopole antennas, antenna feeds, numerical simulation, wearable antennas]
Adaptive array antenna for WLAN: A smart approach to beam switching through phase shifting in feed network
2012 15th International Conference on Computer and Information Technology
None
2012
The WLAN is operated in some high traffic area where the radiation beam produced by the antenna is to be utilized with optimum efficiency. The aim of those antennas is to produce some directive and switched beams rather than producing Omni-directional pattern since these beam patterns are considered to be wastage in some non-used or low traffic areas. Hence, the antenna systems producing the directive and switched beams should be highly preferred for the WLAN application. A smart antenna array system performs the adaptive beam forming by focusing the beam in the desired direction and creating nulls in other directions to avoid the interference and the wasting of the beams. This is performed by the operating the phase shifting in its feed network and different ports. This paper investigates the performance of the different array patterns with different phase shifting for the optimum performance of the WLAN applications in a high congested indoor environment.
[interference suppression, phase shifting, radiation beam switching, adaptive beamforming, WLAN, beam patterns, high congested indoor environment, Beam forming, adaptive array antenna, feed network, smart approach, omnidirectional pattern, antenna radiation patterns, array, antenna feeds, antenna systems, wireless LAN, adaptive antenna arrays, smart antenna array system, interference avoidance, array signal processing, adaptive antenna]
Chondrobot-2: A simple and efficient semi-autonomous tele-robotic lunar excavator
2012 15th International Conference on Computer and Information Technology
None
2012
A simple, light-weight and efficient excavation system to excavate and collect lunar regolith has been developed at BRAC university. The excavation system has a dimension of 1.21m &#x00D7; 0.66m &#x00D7; 0.74m with a total weight of 65 kg. This four wheel drive robot consists of a bucket-ladder system for excavation and is capable of collecting 8kg of regolith per minutes. It also includes a collector bucket which can contain 18kg regolith and deposit in a bin which is installed on 50 cm height. The excavation hardware can be operated both manually and remotely through a web browser by logging in from any computer without direct visual and auditory access to the hardware. A KINECT sensor based semi-auto navigation is incorporated to avoid obstacle while motion.
[collision avoidance, obstacle avoidance, BRAC university, KINECT sensor based semiauto navigation, direct visual access, telerobotics, obstacle detection, semi-autonomous, mobile robots, excavation hardware, four wheel drive robot, Tele-robot, collector bucket, control engineering computing, lunar regolith, Chondrobot-2, bucket-ladder, aerospace robotics, excavation system, Kinect sensor, bucket-ladder system, auditory access, semiautonomous telerobotic lunar excavator, Web browser, online front-ends, Lunar excavator, lunar surface, aerospace instrumentation, excavators]
TCP performance enhancement over IEEE 802.11
2012 15th International Conference on Computer and Information Technology
None
2012
Transmission Control protocol (TCP) establishes a connection between two host computers in a virtual network and exchanges streams of data. TCP ensures that transfer of data and packets will be delivered in the same order as they were sent. The network topology is designed in a wireless network environment which is IEEE 802.11. TCP is a connection-oriented, end-to-end reliable protocol which is existing in our network topology. In this simulation the total number of nodes is 10. Reno, New Reno are generally used in a network topologies to control congestion. But in these cases Packet Delivery Fraction (PDF) is not sufficient, delay is high, packet loss is also high as it takes longer time to detect and recover packet loss. In order to improve retransmission scheme we propose an updated version of New Reno. It does not have to wait for 3 duplicate acknowledgements, thus it can retransmit faster and doesn't reduce the window size too much prematurely. For evaluating this performance we use NS-2.34 which is the most suitable network simulator in wired and wireless network environment.
[IEEE 802.11, TCP, TCP performance enhancement, telecommunication network topology, virtual network, congestion control, packet delivery fraction, packet loss, transmission control protocol, Congestion control, host computers, network topology, connection-oriented end-to-end reliable protocol, PDF, transport protocols, network simulator, wired network environment, wireless network environment, wireless LAN, NS-2.34, New Reno]
A secure routing scheme in MANET with CRT based secret sharing
2012 15th International Conference on Computer and Information Technology
None
2012
Sharing of secret values in MANET is inconsistent and computationally insecure due to its dynamic nature. Over the years, key generation, encryption and decryption have evolved as important techniques for providing secure routing in MANETs. Many researchers are involved in solving the secret sharing problem. Shamir's proposal is one of the eminent secret sharing schemes. However, this scheme does not include secure key generation. This paper proposes the use of the mathematical theory of Chinese remainder theorem coupled with the commonly used RSA for generating keys. Computational complexity of CRT is less than RSA modular exponentiation scheme. This helps to reduce the computational overhead. Once a secure key has been generated, it has been used for finding a secure route between source and destination. The final route is selected based on the highest weighted trust value of the routes. The three phased approach is aimed to increase the overall performance.
[telecommunication security, secure routing scheme, secure key, Chinese remainder theorem, Chinese Remainder Theorem, CRT based secret sharing, Safety key, Lagranges Interpolation, mathematical theory, Encryption, Decryption, MANET, public key cryptography, encryption, mobile ad hoc networks, telecommunication network routing, RSA modular exponentiation, decryption]
Local feature based gender independent bangla ASR
2012 15th International Conference on Computer and Information Technology
None
2012
This paper presents automatic speech recognition (ASR) for Bangla (widely used as Bengali) by suppressing the speaker gender types based on local features extracted from an input speech. Speaker-specific characteristics play an important role on the performance of Bangla automatic speech recognition (ASR). Gender factor shows adverse effect in the classifier while recognizing a speech by an opposite gender, such as, training a classifier by male but testing is done by female or vice-versa. To obtain a robust ASR system in practice it is necessary to invent a system that incorporates gender independent effect for particular gender. In this paper, we have proposed a Gender-Independent technique for ASR that focused on a gender factor. The proposed method trains the classifier with the both types of gender, male and female, and evaluates the classifier for the male and female. For the experiments, we have designed a medium size Bangla (widely known as Bengali) speech corpus for both the male and female. The proposed system has showed a significant improvement of word correct rates, word accuracies and sentence correct rates in comparison with the method that suffers from gender effects using. Moreover, it provides the highest level recognition performance by taking a fewer mixture component in hidden Markov model (HMMs).
[classifier training, natural language processing, hidden Markov model, local featues, word correct rates, HMM, local feature extraction, gender factor, automatic speech recognition, gender independent Bangla ASR, speaker gender type suppression, word correct rate, signal classification, word accuracies, hidden Markov models, sentence correct rate, speech recognition, feature extraction, Bengali speech corpus, word accuracy, learning (artificial intelligence), sentence correct rates, gender issues]
Design an intelligent robotic head to interacting with humans
2012 15th International Conference on Computer and Information Technology
None
2012
Attracting people attention to initiate an interaction is one of the most fundamental social capabilities both for the human and the robot. If the robot would like to communicate a particular person, it should turn its gaze to that person and make eye contact with him/her. However, only such a turning action is not enough to set up an eye contact phenomenon in all cases specially, when the robot and the target person are in greater distance and they are not facing each other. Therefore, the robot should perform some actions so that it can attract the target person before meeting his/her gaze. In this paper, we propose three actions (head turning, head shaking, and reference terms) for the robot as its attention attraction capabilities corresponding to the three viewing situations (near peripheral field of view, far peripheral field of view, and out of field of view) of human. A preliminary experiment using twelve participants confirms the effectiveness of the robot actions to attract human attention.
[human-robot interaction, target person, robot action, Human-Robot Interaction, turning action, reference term, intelligent robots, head turning, head shaking, human attention attraction, intelligent robotic head, peripheral field of view, motion control, Attention Attraction, robot communication, robot gaze, Direction of Attention, social capability, eye contact, attention attraction capability, Viewing Situation]
An optimal resource distribution method for arsenic mitigation in Bangladesh
2012 15th International Conference on Computer and Information Technology
None
2012
The discovery of arsenic contamination of groundwater in many nations including Bangladesh shows that this is a global problem. Because of the delayed health effects, poor reporting, and low levels of awareness in some communities, the extent of the adverse health problems caused by arsenic in drinking-water is at alarming level in Bangladesh. Also, allocating resources such as tube wells efficiently and effectively to mitigate arsenic hazard is a challenging task in Bangladesh. To allocate resources based on different arsenic hazard parameters, we have developed a Decision Support System that enables the user to observe the effect of allocation policy both in tabular and spatial format using statistical models. We have also developed an algorithm for optimal allocation of resources. Finally, we have analyzed and demonstrated the efficacy of our algorithm graphically.
[arsenic hazard mitigation, water pollution control, Decision Support System, groundwater, statistical model, arsenic, resource distribution method, Vulnerability Index, decision support systems, arsenic mitigation, Geographic Information System, contamination, arsenic hazard parameter, Bangladesh, resource allocation, environmental science computing, arsenic contamination, decision support system, Arsenic hazard, drinking water, statistical analysis, health hazards]
Time and space efficient algorithm for consumer's priority product management
2012 15th International Conference on Computer and Information Technology
None
2012
In this highly competitive free market economy, the test or interest of a certain consumer plays a momentous role for the business organizations to select appropriate products to be advertised to him through the online store or their website. In other words, business organizations need to be proficient in advertising their products to attract the potential consumers. To achieve the goal, organizations have to keep track of the consumers' buying habit to figure out the priority products. Thus the consumers' priority product management is a candidate for high degree of attention. In this paper, we have developed and analyzed the algorithm for consumers' priority product management with minimum time and space complexity. We use the concept of balanced binary search tree in this case to attain efficient time complexity. For reducing space requirement, we have proposed merging algorithm that incorporates with the balanced binary search tree, to achieve the optimal performances. We have simulated for 1 million test cases and our results show that the algorithm achieves a high memory saving index with logarithmic time complexity in each working round.
[product advertising, online store, merging algorithm, consumer behaviour, logarithmic time complexity, highly competitive free market economy, Red Black Tree, balanced binary search tree, business organization, time efficient algorithm, Information Management, memory saving index, advertising data processing, consumer buying habit, optimal performance, consumer priority product management, Binary Search Tree, tree searching, Consumers' Priority Product Management, product selection, Web sites, space efficient algorithm, Web site, potential consumer, space complexity, space requirement, computational complexity, Digital Store]
UMTS and WiMAX handover performance comparison
2012 15th International Conference on Computer and Information Technology
None
2012
Convergence of telecommunications, Internet, entertainment, and information technologies is an important driver for the seamless provisioning of multimedia services across different network types. The emergence of various radio access technologies and wireless data communication networks has revolutionized the entire telecommunications industry. By and large, 3G UMTS has become a major wireless cellular technology and IEEE 802.16 WiMAX has been widely accepted as a convenient alternative to the conventional mobile communication networks. In mobile communication systems, handoff/handover controls are critical to ensure a seamless migration as devices move from place-to-place. This paper analyzes the performance of the existing types of handover applied in the third generation mobile networks and compares them with handover used in WiMAX.
[mobile communication networks, IEEE 802.16 WiMAX, third generation mobile networks, information technology, radio access technology, IEEE 802.16, 3G mobile communication, WiMax, handoff-handover controls, WiMAX, mobility management (mobile radio), mobile communication systems, wireless data communication networks, radio access networks, telecommunications industry, UMTS, multimedia services, 3G UMTS, 3G, Handover, wireless cellular technology, Internet, WiMAX handover performance]
Analysis of Proxy Mobile IPv6: A network-based mobility solution
2012 15th International Conference on Computer and Information Technology
None
2012
Terminal-based mobility protocols require mobile devices to participate in mobility signaling that consumes lots of processing power and memory. Network-based mobility protocol, such as, Proxy Mobile IPv6, solves this problem by excluding low-end mobile devices from signaling requirement. In this paper, we have explained Proxy Mobile IPv6 architecture along with its detailed signaling diagram. We have identified the major advantages and limitations of this protocol. We have also performed the signaling cost analysis of its key mobility entities to obtain the amount of overhead on them. Results show interesting relationships among various network parameters, such as, network size, mobility rate, traffic rate. Our critical analysis can help researchers better understand the strengths and weaknesses of this protocol and our signaling analysis can be used by network engineers to estimate the resource requirements of its entities in actual deployment.
[Proxy Mobile IPv6, network-based mobility protocol, mobility rate, wireless networks, mobility management (mobile radio), mobility signaling, network size, signalling protocols, mobile computing, proxy mobile IPv6 architecture analysis, signaling diagram, localized mobility protocol, signaling cost analysis, low-end mobile devices, IP networks, cost analysis, telecommunication traffic, Mobility management, handover protocol, terminal-based mobility protocols, traffic rate]
Diabetes diagnosis decision support system based on symptoms, signs and risk factor using special computational algorithm by rule base
2012 15th International Conference on Computer and Information Technology
None
2012
Diabetes is one of the major type of disease. It is the very common disease in the modern world. Diabetes is a serious disease that affects almost every organ in the body like heart, eyes, kidney, skin, nerves, blood vassals, foot etc. If left the disease unchecked it will make serious complications including death. However, a proper diagnosis at an early stage can result in significant life saving. Unfortunately, all the physicians are not equally skilled, which can cause for time delay, inaccuracy of the diagnosis. In the present paper, a Decision Support System has been proposed for Diabetes diagnosis. The proposed system is designed and developed by using Netbean7.1's GUI and MySQL server feature with the implementation of The dataset used in this study are the signs, symptoms, risk factors associated with diabetes and the results of physical evaluation of a patient. This system provides a user interactive, menu driven environment. It will ask a bunch of questions about the signs, symptoms and risk factors to the system user and user should give yes or no answer. According to the answer the system will make Decision about the possibility of illness, how much severe it is like slight chance, moderate chance, high chance, very high chance, diabetic or not.
[graphical user interfaces, MySQL server feature, sign, Decision Support System, user interactive environment, diabetes diagnosis decision support system, moderate chance, special computational algorithm, very high chance, knowledge based systems, interactive systems, Netbean7.1 GUI, Special Computational Algorithm by Rule Base(SCARB), high chance, risk factor and symptoms, menu driven environment, disease, illness possibility, diseases, slight chance, risk factor, decision support systems, SQL, symptoms, Signs, rule base, Diabetes, medical computing, patient diagnosis]
Group Search Optimization to solve Traveling Salesman Problem
2012 15th International Conference on Computer and Information Technology
None
2012
The goal of Traveling Salesman Problem (TSP) is to find the shortest circular tour visiting every city exactly once. TSP has many real world applications and a number of methods have been investigated to solve TSP. Recently, nature inspired algorithms are also attracted to solve it. Here we studied Group Search Optimizer(GSO), the recently proposed nature inspired algorithm, to solve TSP. GSO is a population based optimization technique on the metaphor of producer-scrounger based social behavior of animals where producer searches for finding foods and scrounger searches for joining opportunities. GSO has found as an efficient method for solving function optimization problems for which it modeled. In this study we employ the concept of Swap Operator (SO) and Swap Sequence (SS) to modify GSO for TSP. The modified GSO (mGSO) was tested on a number of benchmark TSPs and results compared with some existing approaches. mGSO has shown best results (best tour cost) for some problems and competitive performance in other cases.
[swap operator, GSO, traveling salesman problem, travelling salesman problems, TSP, optimisation, producer-scrounger based social behavior, Group Search Optimizer, population based optimization technique, Swap Sequence and Swap Operator, group search optimization, swap sequence, Traveling Salesman Problem, search problems]
A Greedy Approach in Path Selection for DFS Based Maze-map Discovery Algorithm for an autonomous robot
2012 15th International Conference on Computer and Information Technology
None
2012
This research addresses the map discovery issue for an autonomous robot in an unknown maze. Breadth First Search (BFS) and Depth First Search (DFS) algorithm can be used to do it. Here, we proposed three variants of DFS to discover the map of unknown maze. We implemented Greedy Approach in Path Selection for DFS Based Maze-map Discovery Algorithm which is one of the variants of DFS based approaches. Performance analysis with standard DFS shows that this algorithm produces better result in terms of number of movements and number of rotations. However, we also found that this performance improvement is dependent on certain parameters like the starting position of the robot, presence of boxed pattern in the maze, number of walls, etc.
[BFS algorithm, Map Discovery, graph theory, DFS algorithm, mobile robots, Depth First Search, boxed pattern, autonomous robot, performance improvement, Maze Map Discovery, motion control, Greedy Approach, performance index, greedy algorithms, path selection, robot starting position, tree searching, SLAM (robots), unknown maze, greedy approach, DFS based maze-map discovery algorithm, Maze, depth first search algorithm, performance analysis, breadth first search algorithm]
Usability analysis of e-Governance services in Bangladesh &#x2014; A survey and future directions
2012 15th International Conference on Computer and Information Technology
None
2012
This research addresses the issues affecting e-Governance implementation in Bangladesh in consideration with a detailed usability analysis and directions for future development. Implementing e-Governance has always been a challenge either it is social, economical, or political. Besides this, there are many technological problems, which should be understood and met, so that a user-accepted e-Governance system emerges. This research provides a quantitative investigation of e-Governance implementation problems with emphasis on analyzing quantitative data gathered in a survey using structured questionnaires that was generated on the basis of our theoretical study. Based on these results we provide a clear conception about those problems, which should be considered at the time of implementing e-Governance services in developing countries like Bangladesh.
[Bangladesh, e-Governance, usability, e-Governance service, awareness, survey, government data processing, usability analysis]
60 GHz array antenna with new method of beam forming
2012 15th International Conference on Computer and Information Technology
None
2012
The spectrum below 10 GHz frequency will likely be congested and the spreading of millimeter wave technology in different emerging wireless applications as well as associated increase in energy consumption will be witnessed in the near future. Array antenna technology with higher beam efficiency (BE) and lower side lobe level (SLL) is required in order to increase the coverage area of wireless communication, transmission bit rate and at the same time decrease the energy consumption and interference. It is possible to achieve higher BE and lower SLL of array antenna by implementing different power distributions for different array elements. A new and technically better method of beam forming by implementing the concept of Staircase Power Distribution (SPD) of antenna elements has been investigated and compared with other edge tapering methods for 60 GHz signal. It is found that the Maximum Side Lobe Level (MSLL) is the lowest in SPD among the compared methods and the BE is also equivalent to those of other edge tapering methods. Fabrication of antenna elements with SPD technology is also easier since the number of different power distribution in SPD is least and stepwise uniform. The use of antenna elements in SPD in terms of power distribution is also the best among the compared edge tapering methods.
[Antenna Theory, Adaptive arrays, Antenna Radiation Pattern, Beam Steering, Antenna Tapering]
A Location Based Advertisement scheme using OpenStreetMap
2012 15th International Conference on Computer and Information Technology
None
2012
Location Based Advertisement (LBA) has become today's most personal and direct marketing channel that provides customers more relevant information, personalized message, targeted offer about products and allows marketers to reach a specific target audience by creating campaigns. Location-based advertising (LBA) is a new form of advertising that integrates mobile advertising with location based services (LBS) to provide location-specific advertisements on consumer's devices. With the help of LBA, it is possible to target population at the right place and the right time. By taking advantage of a consumer's real world position, location based advertising delivers relevant ads for products and services. In this paper the features and usability of the application, &#x201C;Location Based Intelligent Advertisement using OpenStreetMap&#x201D; are explained and how this type of OpenStreetMap (OSM) based LBS application is effective for Bangladesh is discussed. The application helps the registered shop owners to introduce the offers to a consumer who is in close proximity to make them take those final steps to enter his store and let the consumer know what is around him with audio and map support. Additional feature of this application is for visually impaired people so that they can shop easily. We also focus on the benefits of using an open source map over a commercial one in this respect.
[visually impaired people, OpenStreetMap, advertising data processing, location based service, mobile advertising, location based advertisement scheme, LBA, location-specific advertisement, location based intelligent advertisement, Android, mobile computing, Visibly Impaired People, direct marketing channel, open source map, LBS]
A Fuzy-ANP extent analysis to assess and select location for wind power plant
2012 15th International Conference on Computer and Information Technology
None
2012
This paper presents an approach to select an appropriate location for wind power plant. Three main criteria: wind potential, transportation, and cost are considered. The purpose of this study is to present a comprehensive pairwise comparison of criteria and subcriteria for selecting the best wind power plant location. A multi-criteria decision making (MCDM) methodology-Fuzzy-Analytical Network Process (F-ANP) has been utilized to solve the problem. In fact, an analytical network structure of the criteria and subcriteria is developed according to their dependences on each other and triangular fuzzy numbers are employed for pairwise comparison among them. The extent analysis of fuzzy pairwise comparison is done to get the weight vectors. Finally from the limiting global supermatrix of weights the priorities of the locations are found. A real life numerical example is demonstrated to illustrate the methodology.
[fuzzy pairwise comparison, ANP, multicriteria decision making, fuzzy set theory, comprehensive pairwise, fuzzy analytical network process, Extent analysis, facility location, Location selection, Fuzzy sets, triangular fuzzy numbers, fuzzy ANP extent analysis, global supermatrix, Multi criteria decision making (MCDM), best wind power plant location, wind power plants]
Low power design of Johnson Counter using clock gating
2012 15th International Conference on Computer and Information Technology
None
2012
Power dissipation minimization is one of the prime concerns in recent VLSI design. As chip size is shrinking and many other micro-electronics reliabilities are developing gradually, low power design of any system has become priority. Computer system consists of sequential circuits mostly and that is why efficient low power design of various sequential circuits is very important. In this paper, we have proposed a low power design scheme of Johnson Counter using clock gating system. Doing some power analysis in SPICE, it is considered that our proposed system has lower power dissipation and simpler interconnections compared to the conventional design.
[very high speed integrated circuits, chip size, counting circuits, very large scale integrated circuit, low power design, clock gating, circuit analysis computing, VLSI design, power dissipation minimization, clock gating system, low power VLSI design, integrated circuit design, sequential circuits, SPICE, microelectronics reliability, Johnson Counter, Johnson counter, sequential circuit, power dissipation]
Multi-view video compression using dynamic background frame and 3D motion estimation
2012 15th International Conference on Computer and Information Technology
None
2012
The H.264/MVC multi-view video coding standard provides a better compression rate compared to the simulcast coding technique (i.e., H.264/AVC) by exploiting inter- and intraview redundancy. However, this technique imposes random access frame delay as well as requires huge computational time. In this paper three novel techniques are proposed to overcome the above mentioned problems. Firstly, a simulcast video coding technique is proposed where each view is encoded individually using two reference frames-immediate previous frame and a dynamic background frame (popularly known as McFIS- the most common frame in a scene) of the corresponding view. Secondly a novel technique is proposed using 3D motion estimation (3D-ME) where a 3D frame is formed using the same temporal frames of all views and ME is carried out for the current 3D frame using the immediate previous 3D frame as a reference frame. Thereafter, a fractional ME refinement is also conducted on individual frames of 3D current frame using individual reference frames. Finally, a modification of the 3D-ME technique is proposed where an extra reference frame namely 3D McFIS is used for 3D-ME. As the correlation among the intra-view images is higher compared to the correlation among the inter-view images, the proposed 3D-ME techniques reduce the overall computational time and eliminate the frame delay with comparable rate-distortion (RD) performance compared to H.264/MVC. Experimental results reveal that the proposed techniques outperform the H.264/MVC in terms of improved RD performance by reducing computational time and by eliminating the random access frame delay.
[intraview redundancy, 3D McFIS, dynamic background frame, uncovered background, random access frame delay elimination, interview image, 3D Video Coding, rate-distortion performance, 3D frame, McFIS, RD performance, motion estimation, H.264-MVC multiview video coding standard, multiview video compression, immediate previous frame, fractional ME refinement, computational time reduction, data compression, 3D-ME technique, most common frame in a scene, video coding, 3D motion estimation, interview redundancy, intraview image, MRFs, simulcast video coding technique, 3D Motion Estimation, hierarchical B-picture]
Reconfigurable encryption system: Encrypt digital data
2012 15th International Conference on Computer and Information Technology
None
2012
This paper presents a reconfigurable system that can encrypt digital data. The system provides the option of choosing one of familiar encryption methods DES, 3 DES and AES to the user. All these methods are symmetric type block cipher cryptography. DES takes 64 bit key to encrypt each 64 bits block of the entire message. AES on the contrary takes 128 bit key to encrypt each 128 bits block. Providing reconfigurability, the architecture enables the user to choose one of the existing techniques according to the level of security required. So the designed architecture is both flexible and reliable enough for the user to secure their privacy of conversation or e-commerce transaction. The architecture is designed using Verilog hardware description language, synthesized in Xilinx Synthesis Tool (XST) and Simulated by Verilogger Pro 6.5. It may be implemented in commercially available FPGAs.
[digital data, RTL schematic, FPGA, cryptography, hardware description languages, Encryption, reconfigurable encryption system, AES, symmetric type block cipher cryptography, word length 64 bit, Xilinx synthesis tool, DES, timing diagram, reconfigurable architectures, Symmetric key cryptography, Verilogger Pro 6.5, XST, 3 DES, Verilog hardware description language, word length 128 bit]
On-body radio channel measurements for three different human body sizes
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper, the effects of different body sizes and shapes on the narrowband (2.45 GHz) on-body radio propagation channels are investigated. Three different human body sizes (thin, medium build and fatty/larger size) are investigated. Experimental investigation is performed using Printed Monopole antennas in the indoor environments. Thirty four different receiver locations on the front part of the body are considered for calculating the path loss. Results show that due to three different human body shapes maximum of 20.3% variation in path loss exponent is observed.
[monopole antennas, body size effects, UHF radio propagation, human body, microstrip antennas, printed monopole antennas, frequency 2.45 GHz, Narrowband, body-centric wireless communications, on-body radio channel measurement, indoor environments, wireless channels, narrowband on body radio propagation channels, path loss, on-body radio channel]
Bi-lingual audio assistance supported screen reading software for the people with visual impairments
2012 15th International Conference on Computer and Information Technology
None
2012
Accessibility of computer for visually impaired people is a Challenging task. This paper presents software named Mongol Dip, which provides audio-based interactive interfaces to help people with visual impairments to explore and navigate windows operating system. In this software we have used text to speech technology (TTS) to echo every operation done by the user. Mongol Dip provides the easiest interface that helps the visually impaired people to access computer. Visually impaired people cannot read out Bangla text document via computer because existing software they use to operate computer do not support Bangla text. Bi-lingual (English &amp; Bangla Language) support is the main significant part of our software. Hence the main goal of this software is to assist the people with visual impairments to operate computers and connecting with the digital world using minimum operations and maximum usability using bi-lingual TTS support.
[handicapped aids, text analysis, linguistics, English language, echo, digital world, Bangla text document, Windows operating system, TTS, audio-based interactive interface, text to speech technology, interactive systems, visual impairment, Bangla language, visually impaired people, computer accessibility, speech synthesis, natural language processing, Subachan, Visual impairments, bilingual TTS support, audio user interfaces, Mongol Dip, operating systems (computers), bilingual audio assistance supported screen reading software]
Design and FPGA-based implementation of a high performance 32-bit DSP processor
2012 15th International Conference on Computer and Information Technology
None
2012
To meet the faster processing demand in consumer electronics, performance efficient DSP processor design is important. This paper presents a novel design and FPGA-based implementation of a 32 bit DSP processor to achieve high performance gain for reduced instruction set DSP processors. The proposed design includes a hazard-optimized pipelined architecture and a dedicated single cycle integer MAC to enhance the processing speed. Performance of the designed processor is evaluated against existing similar reduced instruction set DSP processor (MUN DSP-2000). Synthesis results and performance analysis of each system building component confirmed a significant performance improvement in the proposed DSP processor over the compared one.
[consumer electronics, field programmable gate arrays, reduced instruction set computing, FPGA, dedicated single cycle integer MAC, processing demand, FPGA-based implementation, system building component, 32 bit DSP processor, MUN DSP-2000, DSP processor design, Pipelined, DSP processor, high performance gain, reduced instruction set DSP processors, Hazard Handling, Single cycle MAC, digital signal processing chips, high performance 32-bit DSP processor, hazard-optimized pipelined architecture]
Neural network &amp; genetic algorithm based approach to network intrusion detection &amp; comparative analysis of performance
2012 15th International Conference on Computer and Information Technology
None
2012
In this paper backpropagation learning algorithm and genetic algorithm is applied for network intrusion detection and also to classify the detected attacks into proper types. During the training process of the backpropagation algorithm two possible set of features in the rule sets are used separately to determine proper rule set features for better performance. Then the performance of genetic algorithm is compared to the performance of both of the backpropagation approach. The process is tested on training dataset as well as test dataset to analyze the performance. It is found that in detecting the attack connections backpropagation algorithm shows better performance but in classifying the detected attacks into proper types the genetic algorithm approach is more successful.
[genetic algorithms, Security, neural network, genetic algorithm, backpropagation learning algorithm, Backpropagation algorithm, security of data, network intrusion detection, Genetic algorithm, Intrusion detection, backpropagation, attack detection, neural nets]
Bringing DSpace-based digital libraries into mobile devices
2012 15th International Conference on Computer and Information Technology
None
2012
Digital libraries (DLs) have revolutionized the access to library materials. Very often they are even the best alternative. However, in some regions where computer and Internet access are a real issue, the usage of mobile devices for accessing such libraries seem to be a good alternative. This paper presents a design of an alternative mechanism for accessing DSpace-based digital libraries on mobile phones. Such mechanism can be particularly relevant for developing countries, where there are serious problems with the traditional publishing and distribution mechanisms and where computer penetration rate is very low.
[mobile device, digital library, mobile computing, Internet access, developing country, distribution mechanism, DSpace-based digital library, library material access, mobile phone, digital libraries, publishing mechanism, DSpace]
Analysis of PAPR reduction of DFT-SCFDMA system using different sub-carrier mapping schemes
2014 17th International Conference on Computer and Information Technology
None
2014
This paper is concerned with the reduction of peak to average power ratio (PAPR) of 3rd generation partnership project (3GPP) LTE standard using the single carrier-frequency division multiple access (SCFDMA) for mobile uplink transmission. The PAPR reduction is done based on the discrete Fourier transform (DFT) spreading technique which is well known as DFT - SCFDMA and in case of LTE which can reduce the PAPR of OFDM signal to a level of single carrier transmission. Note that the needed power of the signals for uplink transmission of LTE is varied depending on the different sub-carrier mapping and modulation techniques. This paper discusses different sub-carrier mapping techniques such as-localized FDMA (IFDMA), distributed FDMA (DFDMA) and interleaved FDMA (IFDMA). To show the effectiveness of DFT-SCFDMA in the reduction of PAPR with different sub-carrier mapping and modulation techniques the results are compared with the conventional OFDMA system. This paper also shows the comparative results of different sub-carrier mapping techniques in terms of symbol error rate (SER). The comparative results show that the DFT-SCFDMA greatly enhances the PAPR reduction for LTE uplink transmission over the conventional OFDMA.
[radio links, SER, Quadrature amplitude modulation, 3G mobile communication, Peak to average power ratio, 3GPP LTE standard, modulation technique, Time-domain analysis, 3rd generation partnership project, PAPR reduction, LFDMA, SCFDMA, DFT-SCFDMA system, single carrier frequency division multiple access, peak to average power ratio, OFDM modulation, single carrier transmission, OFDMA, Long Term Evolution, error statistics, interference suppression, discrete Fourier transforms, frequency division multiple access, OFDM signal, Discrete Fourier transforms, subcarrier mapping technique, discrete Fourier transform, Frequency division multiaccess, symbol error rate, mobile uplink transmission, LTE uplink transmission, IFDMA]
Impact analysis of input and output block size of DCT-SCFDMA system
2014 17th International Conference on Computer and Information Technology
None
2014
This paper aims to analyze the bit error rate (BER) and peak to average power ratio (PAPR) for discrete cosine transform (DCT) based single carrier frequency division multiple access (SCFDMA) system. Note that BER and PAPR are the major concerns for any wireless communication and for having lower PAPR till now discrete Fourier transform (DFT) based SCFDMA is implemented for the up-link communication for long term evolution (LTE). This paper focuses to analyze the effect of the input and output block size of DCT based SCFDMA system which is a supplementary of the DFT-SCFDMA system. Using the conventional raised cosine (RC) and square root raised cosine (RRC) pulse shaping filters with different sub-carrier mapping schemes (e.g. interleaved frequency division multiple access (IFDMA), localized frequency division multiple access (LFDMA) and distributed frequency division multiple access (DFDMA)) the paper analyze the PAPR for the DCT based SCFDMA system. The suitable block size of DCT-SCFDMA is very important to improve the BER and PAPR performances of the system because if the block sizes vary then the PAPR also changes. Therefore, this paper also analyzes the impact of the input and output block size of the system. Finally, the numerical evaluation is done using the pulse shaping filter for different sub-carrier mapping schemes based on the different block sizes. From the numerical analysis and evaluation this paper shows the way to chose the input and output block size to have a significantly low PAPR and BER.
[RRC, discrete cosine transform based single carrier frequency division multiple access system, Bit error rate, pulse shaping, DFT based SCFDMA, Peak to average power ratio, DFDMA, Time-domain analysis, wireless communication, LFDMA, DFT, discrete Fourier transform based SCFDMA, Modulation, PAPR, numerical analysis, peak to average power ratio, Discrete cosine transforms, Long Term Evolution, error statistics, discrete cosine transforms, discrete Fourier transforms, frequency division multiple access, Discrete Fourier transforms, DCT-SCFDMA system block size analysis, up-link communication, filtering theory, conventional raised cosine, bit error rate, BER, RC, RRC pulse shaping filter, DCT, square root raised cosine pulse shaping filter, LTE, Pulse shaping methods, subcarrier mapping scheme, IFDMA]
SSTF: A novel automated test generation framework using software semantics and syntax
2014 17th International Conference on Computer and Information Technology
None
2014
Test Automation saves time and cost by digitizing the process of test generation and execution. The automated test generation techniques in the literature do not always produce effective and compilable test cases. A test generation framework is proposed in this paper which uses the information extracted from UML diagrams and source code. The three layer architecture of the framework is responsible for this generation task. The first layer processes the user inputs i.e the UMLs as XMLs and the source code as source classes, which are used by the second layer. This layer identifies the application semantic from XMLs and extracts syntax from source. It combines this extracted information together and generates unit and integration test cases. This incorporation of syntax and semantics should make the generated tests less erroneous as it creates a better understanding of the application before the test construction. These two directional information collection should also mitigate the negative effects of inconsistent UMLs or source code on the test suites. Moreover, the generation of both unit and integration test case may increase the test coverage. A case study, conducted on a sample Java project, assessed the framework competence and has been successful to construct test scripts.
[source code (software), program testing, Java project, Unified modeling language, UML diagrams, source classes, unit testing, test scripts, automatic test generation, test execution process digitization, Data mining, application semantic, test suites, two-directional information collection, Semantics, automated test generation framework, software semantics, unit test case, Testing, Java, Unified Modeling Language, software testing, source code, integration test case, programming language semantics, information extraction, SSTF, integration testing, XML, Syntactics, test coverage, Software, three-layer architecture, test generation process digitization, software syntax]
Semantic modelling of unshaped object: An efficient approach in content based image retrieval
2014 17th International Conference on Computer and Information Technology
None
2014
This paper presents an efficient image exploration scheme for the unshaped object using semantic modelling. The local regions of an image have been classified with respect to the frequency of occurrences. The semantic concept is evaluated using RGB histogram dissimilarity factor, overall dissimilarity factor and regional dissimilarity factor. The dissimilarities determine the local concept with accuracy up to 89.86% which is much higher than the existing techniques. The proposed algorithm also allows to ranks the unshaped objects according to their semantic similarity.
[Computers, Histogram, local image region classification, Conferences, image classification, semantic similarity, occurrence frequency, Content based image retrieval, Histograms, Accuracy, Image color analysis, Semantics, image colour analysis, overall dissimilarity factor, Dissimilarity factors, semantic modelling, Computational modeling, unshaped object ranking, content-based retrieval, Semantic modeling, image exploration scheme, local concept, Unshaped object, image retrieval, content based image retrieval, RGB histogram dissimilarity factor, regional dissimilarity factor]
Performance analysis of LDPC coded wireless ad-hoc network for emergency response communications
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper, we present the performance evaluation of coded wireless ad-hoc network for emergency response communication. Due to the limited transmission range, a number of intermediate relaying nodes may exist between source and destination and these convey source transmission using hybrid Amplify-and-forward (AF)/Decode-and-forward (DF) protocol. All nodes contain single antenna and OFDM based Low Density Parity Check (LDPC) coded transmission is considered over Rician fading channel. The closed form bit-error-rate (BER) expression has been deduced for the proposed system. Performance evaluation reveals that BER of the LDPC coded ad-hoc network is better than that of non-coded ad-hoc network.
[DF, Bit error rate, parity check codes, Emergency response, LDPC, amplify-and-forward protocol, Wireless ad-hoc network, Relays, decode-and-forward protocol, decode and forward communication, Wireless communication, fading channels, Parity check codes, DF protocol, Mathematical model, Rician fading channel, bit-error-rate, BER expression, error statistics, intermediate relaying nodes, AF, emergency response communications, OFDM based low density parity check, LDPC coded transmission, Ad hoc networks, non coded ad-hoc network, Equations, convey source transmission, limited transmission range, Rician channels, LDPC coded wireless ad-hoc network, ad hoc networks]
RDCC: An effective test case prioritization framework using software requirements, design and source code collaboration
2014 17th International Conference on Computer and Information Technology
None
2014
Test case prioritization is a technique for selecting those test cases, which are expected to outperform for determining faulty modules earlier. Different phases of software development lifecycle represent the total software from different point of views, where priority module may vary from phase to phase. However, information from different phases of software development lifecycle is rarely introduced and no one integrates that information to prioritize test cases. This paper presents an effective test case prioritization framework, which takes software requirements specification, design diagrams, source codes and test cases as input and provides a prioritized order of test cases using their collaborative information as output. Requirement IDs are split into words or terms excluding stop words to calculate requirements relativity. Design diagrams are extracted as readable XML format to calculate the degree of interconnectivity among the activities. Source codes are parsed as call graphs where vertices and edges represent classes, and calls between two classes respectively. Requirements relativity, design interconnectivity and class dependencies are multiplied by their assigned weight to calculate final weight and select test cases by mapping the customers' requirements and test cases using that weight. The proposed framework is validated with an academic project and the results show that use of collaborative information during prioritization process can be beneficial.
[Computers, source code (software), customer requirements mapping, program testing, Unified modeling language, graph theory, test case selection, call graphs, test case prioritization, UML and Code Collaboration, test case mapping, formal specification, weight assignment, graph edges, design diagram extraction, information integration, final weight calculation, graph vertices, source code collaboration, Testing, requirement ID, software priority module, class dependencies, Unified Modeling Language, software requirements specification, Software Testing, requirements relativity, readable XML format, software development lifecycle phases, Information technology, Test Case Prioritization, test case prioritization framework, design interconnectivity degree, RDCC, Software Engineering, Collaboration, XML, Software, collaborative information]
Normal inverse Gaussian parameters in the empirical mode decomposition domain for the detection of epilepsy and seizure
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper, a comprehensive analysis of electroencephalogram (EEG) signals is carried out in the empirical mode decomposition (EMD) domain using a publicly available benchmark EEG database. First, the intrinsic mode functions (IMF) are extracted in the EMD domain. Next, normal inverse Gaussian (NIG) probability density function (pdf) is introduced and it is investigated whether the NIG pdf can suitably model the IMFs extracted in EMD domain of the EEG signals. It is shown that the NIG pdf is a suitable prior to model the first five IMFs extracted from various types of EEG recordings. It is further shown that the NIG parameters can distinguish among the EEG signals at the five IMF levels quite well. The analysis is further confirmed through the p-values obtained by one way ANOVA analysis. Thus, the NIG parameters in the EMD domain may be used to characterize EEG signals and help the researchers in developing fast, effective and improved classifiers for the detection of epilepsy and seizure.
[Computers, Empirical mode decomposition, electroencephalogram signals, normal inverse Gaussian parameters, Electroencephalography, Epileptic seizure, NIG pdf, Databases, inverse problems, epilepsy detection, one way ANOVA analysis, Probability density function, normal inverse Gaussian probability density function, EEG signals, IMF levels, Electroencephalogram(EEG), electroencephalography, empirical mode decomposition domain, EMD domain, probability, intrinsic mode function, improved classifiers, Normal Inverse Gaussian(NIG), medical signal detection, Information technology, benchmark EEG database, signal classification, medical disorders, EEG recordings, seizure detection, Empirical Mode Decomposition(EMD), Gaussian processes, Brain modeling, statistical analysis, NIG parameters, p-values]
Centroidal voronoi tessellation based energy efficient clustering protocol for heterogeneous wireless sensor and robot networks
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper, we propose a novel clustering protocol for wireless sensor and robot networks (WSRN) to ensure efficient energy usage and maintain maximum connectivity among the sensors. Our algorithm mainly works in two phases: (i) creates optimal number of clusters in the region of interest via three-point centroidal Voronoi diagram. (ii) aggregates data from the members of each cluster and transfers to base station. This protocol is designed using double layered adaptive clustering and unknown region exploration which can be changed when needed for specific application. Additionally, this protocol works without the prior knowledge of the deployment region. We compared the results with LEACH and LEACH-C as a proof of concept. Specifically, simulation results exploit higher level of performance improvement in terms of energy dissipation, node failure, transmission overhead, and data aggregation among large number of mobile sensors. Finally, we analyzed the protocol with different settings which manifest the viability of our design.
[unknown region exploration, energy efficient clustering protocol, Protocols, wireless sensor networks, wireless sensor network, computational geometry, LEACH-C, Mobile communication, mobile robots, data aggregation, performance improvement level, Clustering algorithms, telecommunication network reliability, energy efficiency, Robot sensing systems, Mathematical model, protocols, Base stations, region exploration, voronoi diagram, mobile radio, telecommunication power management, heterogeneous wireless sensor-and-robot network, centroidal voronoi tessellation, WSRN, Wireless sensor networks, energy dissipation, pattern clustering, region of interest, energy conservation, three-point centroidal Voronoi diagram, clustering, node failure, double layered adaptive clustering]
Design of a compact fault tolerant adder/subtractor circuits using parity preserving reversible gates
2014 17th International Conference on Computer and Information Technology
None
2014
Reversible logic has drawn great attention in recent years due to its emerging propagation in diverse range of areas. In this paper, we present a novel approach to unite addition and subtraction operations; circuits that perform addition/subtraction operations using fault tolerant reversible gates with fault detection capability. Adder and subtractor are basic building blocks of any Arithmetic Logic Unit; in this manner we first present the concept of merging those two circuits into one logical block. Then we introduce all possible approaches to construct fault tolerant united addition-subtraction circuit for not only reducing the number of gate but also minimizing quantum cost and garbages of circuit at a meaningful level. We demonstrate three types of half-adder/subtractor circuits and four types of full-adder/subtractor circuits. Again, we depict an algorithm based on our novel concept and we also present simulations on our proposed circuits. Besides, the comparative analysis of our proposed compact method shows our proposed circuit outperform than existing circuit as highest improvements of proposed circuits are 33.33% for garbage output, 26.66% for quantum cost and 50% for gate count. Finally, overall significance of our proposed designs is presented in conclusion.
[half-adder/subtractor circuits, fault tolerant gates, gate reduction, quantum cost minimization, circuit improvement, compact fault tolerant adder/subtractor circuit design, fault detection capability, full-adder/subtractor circuits, Fault tolerance, storage management, fault tolerant reversible gates, adders, circuit garbage minimization, Fault tolerant systems, parity-preserving reversible gates, garbage output, reversible logic block, Adders, gate count, Reversible logic, quantum gates, fault tolerant adder/subtractor circuit, Vectors, arithmetic logic unit, subtraction operation, Equations, addition operation, fault tolerant reversible circuits, Simulation, fault tolerant united addition-subtraction circuit, Logic gates, fault tolerant computing, logic design]
Enhancing wireless capacity in heterogeneous networks with interference power
2014 17th International Conference on Computer and Information Technology
None
2014
We consider the problem of wireless multicasting through multiple-input multiple-output (MIMO) heterogeneous networks in which macro and pico base stations communicate with multiple macro and pico users, respectively. At first, we calculate the multicast capacity of macro users (MUs) and pico users (PUs), and investigate the impact of interferences on it. Then, we use zero-forcing (ZF) precoding at a relay to enhance the multicast capacity of MUs and PUs removing the impact of interferences. Finally, we propose a scheme to improve the multicast capacity of MUs and PUs making use of interference energy that already exists in the communication medium employing selective precoding at the relay. In the proposed scheme, interference provides an additional source of energy that enhances multicast capacity of MUs and PUs without the need to increase the transmitted power. Our results show that exploiting free interference power, the performance of wireless multicasting through the heterogeneous networks can be improved without affecting the performance of any base station by the other base station.
[PU multicast capacity enhancement, radio networks, pico base station, MU multicast capacity enhancement, multiple input multiple output heterogeneous network, selective precoding and zero-forcing precoding, precoding, Relays, Wireless communication, radiofrequency interference, MIMO heterogeneous network, multicast communication, pico user multicast capacity, MIMO, macro user multicast capacity, MIMO communication, pico user, relay networks (telecommunication), Base stations, macro base station, Interference, zero-forcing selective precoding, ZF precoding, wireless multicasting capacity enhancement, picocellular radio, Receiving antennas, Macro user, Signal to noise ratio]
Performance of Multiuser MIMO uplink wireless communication using Space Time Block Coding for rayleigh fading channel
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper, performance of Multiuser Multiple-Input Multiple-Output (MU-MIMO) uplink wireless communication is investigated in the presence of rayleigh fading. Information are modulated using QPSK or 16 QAM or 64 QAM modulator and modulated data are further encoded using STBC and then split into n streams which are simultaneously transmitted through M<sub>T</sub> transmit antennas of each user. Space Time Block Coding (STBC) is used for encoding the information at user's handset because the channel from handset to base station is bad-state channel and STBC is more suitable for bad-state channel to obtain low BER. Capacity is analyzed and simulated to show the performance of the proposed system. Simulation results obtained show that there is around 6 to 11 bits/s/Hz improvement in capacity at 10 dB for increasing number of users from two to four, and there is around 6 to 8 bits/s/Hz improvement in capacity at 10 dB for increasing number of users from single to two. BER Performance is also simulated and it is shown that performance declines around 10 to 11 dB and 2.5 to 3 dB for increasing number of users from single to two and two to four respectively at 10-5.
[Rayleigh fading channel, channel coding, transmitting antennas, STBC, Transmitting antennas, channel capacity, quadrature amplitude modulation, quadrature phase shift keying, information encoding, wireless communication, Wireless communication, multiuser channels, QPSK, Wireless Communication, MIMO, multiuser MIMO uplink wireless communication, Uplink, base station, MIMO communication, error statistics, Base stations, Rayleigh channels, MU-MIMO, user handset, BER, Slot antennas, transmit antennas, multiuser multiple input multiple output, Multiuser, Receiving antennas, space-time block codes, space time block coding, QAM, mobile handsets]
Design and performance analysis of a Parabolic-Inverted Helix antenna and link budget optimization
2014 17th International Conference on Computer and Information Technology
None
2014
In the modern communication technology, the development of very small size, low-cost, low-profile, high-gain and high directivity antennas is a must. In this paper, the design and performance of a Parabolic-Inverted Helix along with the link budget optimization have been analyzed for the C-band applications. The proposed antenna system has a gain of 20.14dB, directivity of 20.30dBi; return loss value of -23.707989dB, Voltage Standing Wave Ratio (VSWR) of 1.1396164, bandwidth of 61.6MHz, antenna efficiency of 96.36% and 3dB angular beamwidth i.e., the Half Power Beamwidth (HPBW) of 65.1deg. The resonant frequency of the antenna is 5.8755 GHz. This antenna can be used for C-band applications like satellite communications transmissions, Wi-Fi, cordless telephones, weather radar systems and other wireless systems. This antenna system is designed and simulated in the CST Microwave Studio. Link budget optimization is performed in order to analyze the critical factors in the transmission chain and to optimize the performance characteristics. The link budget determines what size antenna i s to use, power requirements and in general, the overall customer satisfaction.
[radio links, CST microwave studio, bandwidth 61.6 MHz, Helical antennas, c-band, Mobile communication, resonant frequency, VHF antennas, mobile satellite communication, satellite communications, Optimization, gain, Satellite antennas, directive antennas, C-band applications, antenna radiation patterns, half power beamwidth, frequency 5.8755 GHz, mobile antennas, Antenna radiation patterns, transmission power, voltage standing wave ratio, microwave antennas, gain 20.14 dB, parabolic inverted helix antenna design, mobile satellite communication technology, VSWR, satellite antennas, loss 23.707989 dB, angular beamwidth, high directivity antenna, link budget optimization, HPBW, parabolic inverted helix antenna performance analysis, Satellite communication, Gain]
Introducing active learning on Text to Emotion Analyzer
2014 17th International Conference on Computer and Information Technology
None
2014
Now-a-days, online interpersonal communications have become more preferable than face-to-face interactions. However, emotions play a significant role in online communication. Automatic extraction of emotions from the text is a hot research issue because it minimizes the communication gap and misunderstanding between users. To become emotionally more intelligent, our previous text to emotion analyzing system should communicate with experts for suggestions of possible emotional state if it fails to analyze the text. In this research, we augment our previous system by introducing active learning approach which allows to query experts for emotional label of the given text. It makes our training dataset enriched. To build a classification model by analyzing the training dataset, we employ Naive Bayes classification technique. Our classifier updates the emotional database automatically. We also develop a prototype of our system named TEA: Text-to-Emotion-Analyzer. Our experiment and evaluation section exhibits satisfactory results in terms of recall-precision over our previous system as well as other method namely Vector Space Model (VSM).
[Computers, text analysis, emotion analyzing system, Intelligent Chat Messenger, Naive Bayes classification technique, training dataset, Sentiment Analysis, database management systems, emotion recognition, Machine Learning, emotional database, Databases, Computer architecture, online interpersonal communications, learning (artificial intelligence), pattern classification, text-to-emotion analyzer, active learning approach, Automatic emotions extraction, Vectors, Information technology, Emotion Extraction, TEA: Text-to-Emotion-Analyzer, classification model, Affective Computing, XML, Speech, Bayes methods]
Performance analysis of MIMO link under fading channels
2014 17th International Conference on Computer and Information Technology
None
2014
In modern wireless communication, multiple-input multiple-output (MIMO) system plays an important role as it improves channel capacity, range and reliability without requiring any additional bandwidth or transmit power. This research presents a detailed analysis of capacity performance of MIMO systems under four fading cases i.e. Gaussian, Weibull, Rayleigh and Nakagami-m fading in the low signal-to-noise ratio (SNR) regime. We first derive analytical expressions for the expectation of the trace of the complex channel matrix. Then we measure the low-SNR performance of MIMO system under four fading conditions with respect to minimum normalized energy per information bit and wideband slope. Also a comparative analysis has been performed between the channel capacity of spatial-multiplexing (SM) MIMO and Orthogonal Space Time block coded (OSTBC) MIMO system. Simulation results show that performance of the system depends on different criteria of MIMO fading channels. According to the capacity performance of MIMO system, four fading channels can be ordered as Gaussian, Nakagami-m, Rayleigh and Weibull fading channels respectively.
[radio links, Rayleigh fading channel, Multiple Input Multiple Output, space division multiplexing, Shape, channel coding, channel capacity, SM multiple input multiple output system reliability, Nakagami-m fading channel, low signal-to-noise ratio, Nakagami channels, orthogonal codes, orthogonal space time block coded MIMO system, telecommunication network reliability, MIMO, fading channel capacity improvement, Gaussian channels, MIMO communication, Weibull fading channel, MIMO link performance analysis, Gaussian fading channel, Fading, low SNR performance, Channel capacity, spatial multiplexing MIMO system, Rayleigh channels, Weibull fading channels, OSTBC MIMO system, OSTBC, low-SNR, space-time block codes, complex channel matrix, Signal to noise ratio]
Application of data mining tools for rice yield prediction on clustered regions of Bangladesh
2014 17th International Conference on Computer and Information Technology
None
2014
Reservation of adequate food is a major concern for many developing countries worldwide. Governments of those countries also want to meet the demand of food in the long term, especially during the period of natural calamity. For low lying region such as Bangladesh, predicting the supply of food is critical. Besides, rice productivity of Bangladesh has also changed due to varying climatic over the last couple of decades. In this paper, a comprehensive analysis has been made to forecast the rice yield of Bangladesh.
[Computers, regression tree, rice productivity, data mining, agricultural products, linear regression, food demand, neural network, Training, k-means clustering, decision tree, productivity, Agriculture, data mining tools, Regression tree analysis, Testing, Meteorology, Bangladesh clustered regions, rice yield, Self organizing maps(SOM), Linear regression, food products, food reservation, ensemble learning, pattern clustering, developing countries, rice yield prediction]
Analysis of Bangla-2-Braille machine translator
2014 17th International Conference on Computer and Information Technology
None
2014
Bangla-2-Braille machine translator is implemented using rule based Discrete Finite Automata (DFA) [1]. Rule based DFA directs the necessity of generating regular expression from the DFA in order to validate the language model used for Braille conversion and extend the translator for better usability by the visually impaired people. In this work, we conducted experiments using structured and state elimination method to generate, and validate regular expressions from the DFA designed for Bangla-2-Braille machine translator. The generated expressions were tested for Braille language rules and the results were satisfactory.
[Computers, visually impaired people, handicapped aids, Braille, regular expression generation, Conferences, Computational modeling, natural language processing, Formal languages, Braille language rule, Bangla, finite state machines, Information technology, Bangla-2-Braille machine translator, discrete finite automata, Regular Expression, Automata, rule-based DFA, Machine Translator, Braille conversion, DFA, language translation, state elimination method]
A Belief Rule Based clinical decision support system framework
2014 17th International Conference on Computer and Information Technology
None
2014
Taking into account, the need for handling clinical information under uncertainty from clinical domain knowledge and clinical data in clinical decision making, we proposed a new Belief Rule Based (BRB) Architectural Framework to design a clinical decision support system (CDSS). It can handle uncertain information to support the clinical decision making process. This paper describes how to design a clinical knowledge base and inference process of a CDSS by using belief rule-based system with an evidential reasoning approach. A case study demonstrates to employ this BRB Architectural Framework in developing CDSS is a reliable novel approach.
[Decision support systems, Computers, clinical decision making, Uncertainty, clinical decision making process, clinical data, medical expert systems, evidential reasoning approach, Computer architecture, BRB architectural framework, belief rule-based system, CDSS, Decision support System, Architecture, medical information systems, clinical domain knowledge base design, inference mechanisms, Information technology, decision support systems, decision-making, uncertain clinical information handling, Belief rule base (BRB), belief rule based clinical decision support system framework, inference process, Expert systems, Medical diagnostic imaging]
Towards a standard Bangla PhotoOCR: Text detection and localization
2014 17th International Conference on Computer and Information Technology
None
2014
A complete Bangla PhotoOCR requires a series of carefully chosen algorithms. Text extraction from images is a long-standing active research area. It is even more attractive today due to the availability of low-cost mobile image acquisition devices. Many researchers have addressed this problem using different approaches. Often times, the first step towards text extraction from images is detection of text areas. Bangla texts, specially in images, pose a unique set of challenges than texts in other languages. In this paper, we experiment with two established approaches, available for other languages, to automatically localize Bangla texts in complex natural scene images towards developing a complete Bangla PhotoOCR system. In our approach, features are extracted from an image using wavelets based decomposition and histogram calculation techniques. We use 56 features to train two different types of classifiers (ANN based and SVM based) to localize Bangla texts in natural scene images. Our experimental results show that ANN is a good classifier for identifying Bangla texts.
[text extraction, Roads, image classification, wavelet transforms, PhotoOCR, automatic Bangla text localization, optical character recognition, Text recognition, feature extraction, text area detection, SVM-based classifier, support vector machines, Image edge detection, Artificial neural networks, ANN-based classifier, Character recognition, Bangla Text Extraction, Support vector machines, Pattern Recognition, complex natural scene images, standard Bangla PhotoOCR, wavelet-based decomposition, Feature extraction, histogram calculation techniques, Computer Vision, mobile image acquisition devices, neural nets, text detection]
Robot learning using Symbol Grounding
2014 17th International Conference on Computer and Information Technology
None
2014
Solving the Symbol Grounding Problem (SGP) is crucial for the autonomous robot learning. The failure of classical AI in delivering smart robots has forced many researchers to address the problem earnestly. In this paper we present a methodology to solve this problem by facilitating a human instructor to interact with a robot using a Microsoft Kinect&#x2122; sensor so as to ground symbols. The instructor exhibits the physical objects and also provides the corresponding symbols using voice commands to a robot. The robot captures the physical attributes of different objects for the symbol to elaborate its own semantics autonomously and grounds the same within. Grounding is achieved by toiling on sensory information to acquire the commonalities from the attributes while symbol theft enables the grounding of meta-level symbols or aliases. Apart from the formal description of the inherent grounding mechanism, the paper also discusses the results obtained from actual experiments that eventually make the robot identify and point to the concerned physical objects.
[voice commands, dexterous manipulators, human instructor, meta-level symbol grounding, Speech Processing, Training, Image color analysis, physical objects, Image Processing, Robot sensing systems, speech processing, Microsoft Kinect&#x2122;, human robot interaction, smart robots, Robot Learning, human-robot interaction, Grounding, ground symbols, robot vision, formal description, intelligent robots, autonomous robot learning, meta-level alias grounding, image sensors, symbol grounding problem, Symbol Grounding, Robot kinematics, Microsoft Kinect sensor, sensory information, symbol theft, Speech, SGP, physical attributes]
Fast normalized cross-correlation based retinal recognition
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper, a simple biometric scheme based on RGB retinal fundus images is proposed. First, prominent vasculature energy based feature vectors are constructed from RGB retinal fundus images to utilize the unique pattern of retinal vasculature. Next, fast normalized cross-correlation based feature matching is employed for person identification on publicly available DRIVE and STARE databases. This method excels recently published literatures with perfect recognition accuracy of 100% on STARE and accuracy of 99.77% on DRIVE databases. Its smaller dimension of feature vector and better recognition result make this method eligible for real-time person identification scheme.
[Wavelet transforms, fast-normalized cross-correlation based retinal recognition, vasculature energy based feature vectors, fast-normalized cross-correlation based feature matching, Retina, Vectors, real-time person identification scheme, RGB retinal fundus images, image matching, retinal recognition, biometric scheme, publicly available STARE database, publicly available DRIVE database, Accuracy, Databases, feature extraction, Feature extraction, retinal vasculature pattern, Biomedical imaging]
Improvement of the read range of a chipless RFID for MPSK UWB system in outdoor and farm NLOS environment using receiver diversity with maximal ratio combining
2014 17th International Conference on Computer and Information Technology
None
2014
An analytic approach is presented to evaluate the Bit Error Rate (BER) and read range of a UWB chipless RFID network using diversity technique at the reader receiver for Rayleigh fading over the channels. The analysis is carried out with multiresonator based chipless RFID tags for a UWB system of frequency range from 3GHz - 6GHz and bandwidth of 500MHz in an outdoor and farm non line of sight (NLOS) environment. SISO configuration is used to communicate from reader to tag and SIMO configuration is used to communicate from tag to reader. Maximal ratio combining technique is used in the reader for diversity reception of the tag. BPSK, QPSK and M-ary PSK are considered with coherent demodulation to evaluate the BER performance and a read range of 548.00m at the BER of 10-06 could be achieved. The analyzed approach not only gives an improved result compared to the previous research works but also gives a clear insight regarding the interrelationship between BER, detection ranges, reader received power, number of receiving antenna and read range improvement.
[chipless RFID network, non line of sight, radiofrequency identification, maximal ratio combining, Bit error rate, Read range, ultra wideband communication, quadrature phase shift keying, receiver diversity, QPSK, diversity, M-ary PSK, frequency 3 GHz to 6 GHz, error statistics, MPSK, diversity reception, SISO configuration, reader receiver, Rayleigh channels, demodulation, Binary phase shift keying, radio receivers, bit error rate, farm NLOS environment, BER, SIMO configuration, coherent demodulation, MPSK UWB system, Diversity reception, Receiving antennas, detection range, reader received power, bandwidth 500 MHz, read range improvement, BPSK, receiving antenna, Rayleigh fading, multiresonator based chipless RFID tags, BER performance evaluation, Radiofrequency identification, MRC]
Android academic assistant
2014 17th International Conference on Computer and Information Technology
None
2014
Android phones are now becoming available to more and more people with the advancement of electronics and communication technology. They are getting wide acceptability because of their user friendly applications. This paper deals with such an android application made for academic aid of students, teachers and staffs of educational institution. Its features are- providing class and laboratory schedule, notice board, teacher's update, notification for newly included updates, CGPA (cumulative grade point average) calculation. Its goal is to provide assistance in academic works by making communication easier, provide easier and faster access to information. Though it has been developed for a specific institution, this application has the potential flexibility to include more assisting function and have extended version for wider range of users.
[Schedules, server, codes, android, Programming, educational administrative data processing, Servers, academic news updates, Android (operating system), mobile computing, electronics and communication technology, teacher update, Databases, information access, user friendly applications, CGPA calculation, mobile application, information analysis, smart phones, notice board, cumulative grade point average calculation, laboratory schedule, Android academic assistant, Android phones, Internet, educational institutions, Androids, educational institution, Smart phones, academic aid]
Performance analysis of Nakagami-m fading massive MIMO channels with linear receivers
2014 17th International Conference on Computer and Information Technology
None
2014
This paper estimates bounds on the capacity of Nakagami-m fading massive multiple-input multiple-output (MIMO) channels with large number of antennas at the transmitter and receiver. We consider two models namely conventional co-located MIMO (C-MIMO) and distributed MIMO (D-MIMO) models. At first, we derive the achievable rate for the C-MIMO and D-MIMO systems. Secondly, we use maximum ratio combining (MRC), zero-forcing (ZF) and minimum mean square error (MMSE) detectors in the receiver and derive the expression of achievable rate for both the C-MIMO and D-MIMO systems. Finally, we analyze the asymptotic behavior of the ergodic capacity when the number of antennas at one or both sides goes to infinity. Our investigation concludes that the ergodic capacity of both the co-located and distributed MIMO channels is intuitive as the increasing number of antennas helps to eliminate the effect of fading. MMSE detector shows significant performance in enhancing the ergodic capacity of Nakagami-m fading channels compared to ZF and MRC detectors.
[antenna, least mean squares methods, Transmitting antennas, minimum mean square error, channel capacity, multiple input multiple output, Nakagami channels, asymptotic behavior, Detectors, ergodic capacity, MRC detector, MIMO, MIMO communication, transmitter, Fading, diversity reception, signal detection, MMSE detector, Asymptotic behavior, maximum ratio combining, ZF detectors, linear receiver, co-located MIMO, distributed MIMO, radio receivers, D-MIMO system, C-MIMO system, Nakagami-m fading massive MIMO channels capacity, zero forcing detector, Receiving antennas, radio transmitters]
Development of electronic voting machine with the inclusion of Near Field Communication ID cards and biometric fingerprint identifier
2014 17th International Conference on Computer and Information Technology
None
2014
The basis of this project is to create an electronic voting machine that will help to eradicate defrauding of the manual voting systems by multiple votes cast by the same user. With the inclusion of a Near Field Communication ID card reader and biometric fingerprint device, each voter will be entered into the system through a swift process only after being recognized and checked to the given database of enlisted voters. Once the corresponding fingerprint is matched with the information provided by the identification card, the voter will be allowed to vote for their preferred candidate through a panel of buttons. The respective card will then be marked for further referencing, and the voter will not be allowed to take part in multiple votes. The proposed project also carries the unique feature of being autonomous during the course of operation, which helps to diminish the issue of hacking occurring in previous attempts of electronic voting machines.
[Computers, fingerprint identification, Electronic voting systems, public domain software, electronic voting machine, biometric fingerprint device, Fingerprint recognition, fingerprint matching, enlisted voters, NFC, database, hacking, Databases, near field communication ID card reader, computer crime, Electronic voting, vote, microcontrollers, Microcontrollers, Nominations and elections, biometric fingerprint identifier, defrauding, near-field communication, fingerprinter, EVM, government data processing]
On the optimum secrecy capacity of multiple relay networks
2014 17th International Conference on Computer and Information Technology
None
2014
We consider a confidential communication system in which a source sends a confidential information to the destination in the presence of an eavesdropper. Multiple relays are used to provide cooperative diversity to the destination. The destination and the eavesdropper are equipped with multiple antennas while each relay and source are equipped with single antenna. We are interested to protect the transmitted information from eavesdropping and to find the effect of receive diversity on the secrecy capacity of the proposed model. We consider the maximal ratio combining (MRC) technique at the destination and derive the expression of combiner output signal-to-noise ratio (SNR) using optimum weighting vector so that the eavesdropper is unable to decode a single bit from the original massage. Finally, we derive the expression of optimum secrecy capacity in terms of the number of relays and the number of antennas at the destination. Our results show that the optimization of weighting vector at the receiver enhances secrecy capacity of a wiretap channel.
[eavesdropper, diversity reception, Conferences, maximal ratio combining, Receivers, Vectors, multiple relay networks, Relays, cooperative communication, Wireless communication, maximal ratio combining technique, cooperative diversity, Confidential communication, combiner output signal-to-noise ratio, single antenna, secrecy capacity, optimum secrecy capacity, wiretap channel, multiple antennas, optimum weighting vector, Antennas, Signal to noise ratio, relay networks (telecommunication)]
I-bin: The intelligent and entertaining bin
2014 17th International Conference on Computer and Information Technology
None
2014
The objective of this project is to develop a program on android device to remotely control a robot to collect discarded material by using human voice command and can detect hindrance autonomously. In this recent time, android device as a user and as developer is become more popular, so we have selected android mobile operating system based device which have Google voice command and we have developed an application to control the robot to get a desirable output. We can communicate with the robot from the android device by a Bluetooth module which is connected to an arduino installed in the robot. This robot is also can detect obstacle and avoidance using sonar which transmits phonetic pulses in its surroundings and for the echoes from the objects nearby that lie within its working range, hence detects obstacles. The robot is installed with Omni wheel which is a special function of this robot. This Omni wheel that provides easy 360 degree movement propel with rotational and sideways maneuverability. The outcome of the project is a combination of embedded computing and programming.
[collision avoidance, intelligent bin, obstacle avoidance, Bluetooth, human voice command, telerobotics, obstacle detection, embedded computing, Wheels, android, echo, mobile robots, sonar, Mobile robots, Google voice command, Android (operating system), speech recognition, phonetic pulses, entertaining bin, Robot sensing systems, remote robot control, embedded programming, human-robot interaction, Omni wheel, Obstacle detection, DC motors, smart phones, android mobile operating system based device, service robots, Bluetooth module, I-Bin, voice recognition]
An energy aware heuristic-based routing protocol in Wireless Sensor Networks
2014 17th International Conference on Computer and Information Technology
None
2014
Wireless Sensor Networks (WSNs) consist of small nodes equipped with sensing, data processing and radio transmission units. A number of clustering, routing, power management and data aggregation protocols are specially designed for WSNs where energy awareness is an essential design issue. Due to the resource constrained nature of sensor nodes, innovative techniques are required to extend the network lifetime in WSNs. This paper proposes an energy efficient routing protocol to find the forwarding path between source and destination node using heuristic function and A* search algorithm. Simulation results with OMNET++ show that our proposed protocol is efficient in terms of network lifetime, total energy dissipation and message throughput.
[data processing unit, OMNET++ simulation, WSN energy efficient routing protocol, wireless sensor networks, sensing unit, Heuristic algorithms, telecommunication power management, wireless sensor network power management, electronic messaging, energy aware heuristic-based routing protocol, Routing, Batteries, A* search algorithm, Wireless sensor networks, energy dissipation, data aggregation protocol, routing protocols, radio transmission unit, data communication, Routing protocols, Energy states, message throughput]
An entertainment recommendation system using the dynamics of user behavior over time
2014 17th International Conference on Computer and Information Technology
None
2014
Personalized entertainment items recommendation is required to help millions of people narrow the universe of potential items to fit their unique tastes. These services usually depend on a machine-learning algorithm, which breaks down items into long lists of attributes and matches these elements to a user's preferences. A set of such algorithms have been proposed. Most of these, such as collaborative filtering, works by finding a group of users based on the item he/she buys or provides feedback and then recommend popular items in the group. In this paper we argue that relying only on such acts are not sufficient for an effective recommendation system, we need to consider other things, such as, an entertainment item enjoying time since it could change over the course time. In this paper we proposed a novel algorithm to find a group of user that usage not only the ratings but also the time of the given ratings. Additionally, we propose algorithms for recommending items to the producers such that they can entertain us more. We perform experiments to validate the performance of our system. We show that our system outperforms in comparison with the existing algorithms.
[Computers, collaborative filtering, entertainment item enjoying time, personalized entertainment item recommendation, Filtering, entertainment recommendation system, entertainment, Time measurement, User preference, Recommendation system, Accuracy, recommender systems, user behavior dynamics, Item swarm, Collaboration, Entertainment industry, interesting item, popular item, Motion pictures]
Data mining approaches to predict final grade by overcoming class imbalance problem
2014 17th International Conference on Computer and Information Technology
None
2014
Data mining approaches have been used in business purposes since its inception; however, at present it is used successfully in new and emerging areas like education systems. Government of Bangladesh emphasizes the need to improve the education system. In this research, we use data mining approaches to predict students' final outcome, i.e., final grade in a particular course by overcoming the problem of imbalanced dataset. We implement several re-sampling techniques to balance the dataset so that could get better performance. Re-sampling techniques include SMOTE (Synthetic Minority Over-sampling Technique), ROS (Random over Sampling), RUS (Random under Sampling). Experimental results show that re-sampling techniques enhance the performance of the classification models that are developed to predict students' final grade in a particular course.
[final grade prediction, class imbalance problem, data mining, data mining approach, synthetic minority over-sampling technique, educational administrative data processing, Classification algorithms, Data mining, resampling techniques, Accuracy, SMOTE, Educational Data Mining (EDM), Decision trees, Imbalanced dataset, pattern classification, sampling methods, Decision Tree, Computational modeling, random processes, random over sampling, classification, Bangladesh Government, RUS, random under sampling, Neural Network, Naive Bayes, Neural networks, student final outcome prediction, ROS, education systems, Data models]
Classification of seizure and nonseizure EEG signals exploiting higher order statistics of the dominant Intrinsic mode function
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper, a method of seizure and non-seizure classification has been proposed based on the higher order statistics of the dominant Intrinsic mode function (IMF) resulting from the Empirical Mode Decomposition(EMD) of the EEG signals. Analyzing the temporal energy contents of different IMFs, it is found reasonable to determine the dominant IMF. In order to reduce the dimensionality, higher order statistics of the dominant IMF are employed to form the feature vector. The reduced feature vector thus formed is found effective for distinguishing seizure and non-seizure EEG signals when fed to a k-nearest neighborhood(k-NN) classifier. Extensive simulations are carried out using a benchmark EEG dataset. It is shown that the proposed method is capable of producing greater sensitivity, specificity, and accuracy in comparison to that obtained by using a state-of-the-art method employing the same EEG dataset and classifier.
[seizure, empirical mode decomposition, Intrinsic Mode Function, nonseizure EEG signal classification, benchmark EEG dataset, temporal energy contents, Electroencephalography, k-nearest neighborhood classifier, Training, Accuracy, k-NN classifier, dominant intrinsic mode function, EMD, Higher order statistics, Testing, electroencephalography, EEG Signal Analysis, IMF, Vectors, higher order statistics, Electroenchephalogram, signal classification, feature vector, medical signal processing, Sensitivity, k-Nearest Neighbors]
Recognition of human affection in Smartphone perspective based on accelerometer and user's sitting position
2014 17th International Conference on Computer and Information Technology
None
2014
Diverse applications are being developed for Smartphone considering several aspects of human computer interaction. The technology which is competent enough of understanding human affection or emotional state leads to future context aware systems and recommendation services. Emotion aware applications, intelligent user interfaces and personalized services will be generated based on precise human mood. There is an emerging market for the technology that can assist businesses exhibiting higher respect for customer feelings. The crucial first step is to identify whether the user is in a mental state of being bored, stressed, excited or neutral. In this paper we have discussed about state-of-the-art methodologies to detect human emotional states. We proposed a system by which three different emotional states (neutral, stressed and excited) will be distinguished using smartphone's built-in accelerometer sensor based on the sitting position of the user. Interaction log of the user was in consideration as well in order to predict accurate result.
[Computers, excited mental state, stressed emotional state, user sitting position, neutral mental state, emotion recognition, stressed mental state, smart phone perspective, Training, neutral emotional state, intelligent user interfaces, Accelerometer, Robot sensing systems, recommendation services, Affective-Computing, Emotion, customer feelings, human emotional state, Accelerometers, Emotion recognition, personalized services, user Interaction log, bored mental state, excited emotional state, context aware systems, smart phones, human mood, accelerometer sensor, human affection recognition, Mood, Keyboards, Mood Detection, accelerometers, human computer interaction, human emotional state detection, Smartphone, Sensor]
Design of a fuzzy logic based automated shading and irrigation system
2014 17th International Conference on Computer and Information Technology
None
2014
This study is about the design of an automated fuzzy system for monitoring and controlling the shading and irrigation process. We incorporate an integrated web system which is designed and developed by Trono et al. (2014) with our designed fuzzy system. The integrated Web system provides information about different rain rates of different rain events. These rainfall data are considered as input to our fuzzy system and by using our developed knowledge base; it generates decisions to maintain the shading and irrigation process in the crop field for different rain events. Use of shading system in the crop field and application of fuzzy logic to control and maintain the shading system with irrigation process is a novel idea. To save the crops from harmful effect of unwanted rainfall, our designed system can be applied in the real life scenario to get outstanding result in the field of agriculture.
[Computers, knowledge base, Irrigation, fuzzy logic based automated shading system design, Integrated web system, crops, fuzzy reasoning, crop field, fuzzy control, Rain, irrigation, fuzzy logic based automated irrigation system design, Monitoring, Fuzzy systems, rain, irrigation process control, rainfall data, fuzzy system design, irrigation process monitoring, shading process monitoring, rain events, Shading, process monitoring, Fuzzy logic, shading process control, process control, integrated Web system, Internet, rain rates]
Effect of node number in range based node localization technique for underwater communications network
2014 17th International Conference on Computer and Information Technology
None
2014
In underwater network, it is very important to know the location of unmanned underwater vehicle (UUV). The unknown position of UUV considered as an unknown node that can be determined from the known node locations through underwater acoustic network. The known node locations considered here as fixed positioned node. The range data is measured between unknown node and known nodes. By exploiting these measured range data and combination among fixed positioned nodes on this network, it is possible to perform underwater positioning of unknown node (UUV), similar to that of the satellite-based GPS program. The aim of this paper is to determine the position of UUV in an underwater acoustic network using combination method with different number of nodes, whose locations are known, to analyze the effect of node number in position estimation.
[Underwater network, Error analysis, underwater acoustic network, combination method, range estimation, Equations, range based node localization technique, Global Positioning System, Vehicles, underwater acoustic communication, underwater communications network, sensor placement, latitude and longitude, autonomous underwater vehicles, unmanned underwater vehicle, underwater node positioning, Mathematical model, Underwater acoustics, UUV position estimation, positioning system]
An automatic bleeding detection scheme in wireless capsule endoscopy based on statistical features in hue space
2014 17th International Conference on Computer and Information Technology
None
2014
Wireless capsule endoscopy (WCE) is a recently developed video technology to detect small intestine diseases, such as bleeding. For analyzing WCE video frames, instead of using the most common RGB (red, green, blue) color scheme, in this paper, HSV (hue, saturation, intensity value) color scheme is used, which corresponds better to human perception system. The HSV color scheme exhibits less sensitivity to illumination changes, which helps in handling the problem of illumination variation in WCE videos due to the weakening of battery. Different statistical features computed from H, S, and V spaces of WCE images are investigated and it is found that hue provides a useful feature as it captures intrinsic information about the color of objects or surfaces in a scene. Hence in this paper, an automatic bleeding detection scheme from WCE video is proposed utilizing the hue space. Among different statistical measures, mean, standard deviation, variance and moment exhibit significantly distinguishable characteristics for bleeding and non-bleeding images. For the purpose of classification, K-nearest neighbor (KNN) classifier is employed. From extensive experimentation on several WCE videos collected from a publicly available database, it is observed that the bleeding detection performance of the proposed method in terms of accuracy, sensitivity and specificity is quite satisfactory in comparison to that obtained by some of the existing methods.
[RGB color scheme, KNN classifier, image classification, Wireless capsule endoscopy, K- nearest neighbor classifier, WCE video frame analysis, nonbleeding image, WCE images, Wireless communication, Histograms, Image color analysis, Endoscopes, feature extraction, hue-saturation-intensity value color scheme, intrinsic information, endoscopes, video signal processing, medical image processing, bleeding detection, HSV color domain, HSV color scheme, diseases, statistical measures, human perception system, Wireless sensor networks, hue space, red-green-blue color scheme, automatic bleeding detection scheme, Feature extraction, wireless capsule endoscopy, statistical analysis, small intestine diseases, Hemorrhaging, biomedical optical imaging, standard deviation]
Bengali character based digital clock using 13 segment LED display
2014 17th International Conference on Computer and Information Technology
None
2014
Digital clock is a very common and useful device to show the time and date. 7-segment, Alphanumeric, Liquid Crystal and Graphic displays are commonly used in digital clock. They show time in English character. But, we were motivated to design this useful device showing time in Bengali to respect our national heroes, who sacrificed their lives for our language in 21th February, 1952 as well as to introduce our local language in technology besides English. In this paper, a Bengali character based digital clock is designed which shows time in 13-segment LED display. Here, a real time clock (DS1307) is used to generate precise timing pulse, a microcontroller (PIC 16F877A) is used to adjust the time and to interface the 13-segment displays. The time of the clock can be adjusted by pressing simple push switches. The design is simple, cost effective and user friendly.
[Computers, PIC 16F877A microcontroller, timing pulse, Bengali character based digital clock, DS1307, real time clock, 13 segment LED display, Real-time systems, buffer, microcontrollers, digital clock, Microcontrollers, LED segment display, timing, push switches, Crystals, Light emitting diodes, LED displays, Information technology, clocks, microcontroller, English character, natural languages, local language, Bengali clock, Clocks]
Uniform band thinning call admission control for QoS provisioning in wireless networks
2014 17th International Conference on Computer and Information Technology
None
2014
Utilization of limited resources and quality of service (QoS) improvement are most important challenges for wireless networks. Excessive call blocking of lower priority traffic is very often event at higher traffic rate in multiple services. Considering these factors, we proposed a uniform band thinning call admission control (CAC) scheme that reduces the call blocking probability of lower priority traffic classes with approximately steady call blocking probabilities for the higher priority traffic classes with ensuring the satisfied QoS. Our proposed scheme introduces the acceptance factor in specific bands where calls get access according to the predefined uniform acceptance factor throughout the bands independent of channel occupancy. Else, analytically we characterize that the performance of our proposed scheme in the aspect of call blocking probability (CBP), channel utilization, and overall call blocking probability are better than the conventional fixed guard band (FGB) scheme.
[Computers, uniform band thinning call admission control, radio networks, Call admission control, telecommunication congestion control, quality of service (QoS), Quality of service, Telecommunication traffic, wireless networks, call blocking probability, acceptance factor, quality of service, Equations, Call admission control (CAC), CAC scheme, CBP, uniform thinning technique (UTT), Wireless networks, QoS, uniform band thinning (UBT) scheme, channel utilization, telecommunication traffic]
A Conceptual Model for effective email marketing
2014 17th International Conference on Computer and Information Technology
None
2014
Email marketing is broadcasting commercial messages to a group of people using email. Currently, email marketing is consistently delivering relatively high return on investment (ROI) in a marketing field. However, collecting subscriber information and sending the email only to interested consumers is a major research issue in email marketing. This paper introduces a conceptual model for an effective email marketing system clustering and segmenting subscribers based on their activity throughout a marketing campaign. The model consists of two main components: Collecting subscriber activity data and Clustering and Segmenting subscribers. We performed a marketing experiment based on our model and analyzed subscriber activity data. Using our model, the overall performance of subscriber activity was improved after sending out email campaigns to segmented groups of subscribers based on their individual interest.
[Computers, electronic mail, subscriber activity data analysis, Electronic mail, subscriber information, History, Subscriber Analysis, email marketing campaign, Databases, cost-benefit analysis, email marketing, data analysis, Computational modeling, investment, email marketing system clustering, Email Marketing, marketing data processing, Information technology, subscriber segmentation, subscriber activity data collection, pattern clustering, commercial messages, Web pages, return on investment, K-means Clustering]
Semantic information integration of Health Care Network for Physical-Cyber-Social computing approach
2014 17th International Conference on Computer and Information Technology
None
2014
The Health Care Network (HCN ) related information of a country is one of the most influential fields of data sharing and integrating with disparate data sources of physical world and compatible health care organizations on the exponentially growing World Wide Web (WWW ) for improving and maintaining health related services of mass people. Although these accessible HCN related unstructured or semi-structured data in multiple heterogeneous repositories on the web originate challenges to develop a smarter method in finding specific information using traditional keyword-based search engines. Moreover, it is also a formidable task to harmonize spatial data with our HCN for exploring implicit knowledge that assist people in making their decisions regarding of health services and it is getting researchers' attention at a rapid pace. In this regard, we applied the Physical-Cyber-Social (PCS) computing approach on HCN that demonstrates the feasibility of semantic web technologies to represent domain information in the machine understandable format and we also integrate data from accessible Cyber Space Data Sources (CSDSs) to address these challenges. We published our HCN related data using Resource Description Framework (RDF ) to build a knowledge-base. We call this repository as HCN - BD, which is HCN related machine understandable data source of Bangladesh. This yields a huge knowledge graph with 0.25 million nodes, i.e. RDF triples to facilitate reasoning important information pattern by a graph based query language, SPARQL. Furthermore, we integrate this semantic repository with our generic spatial knowledge-base, Geo - Bangladesh, a machine understandable geographic data source related to the administrative structure of Bangladesh, to examine health care locations using SPARQL and therefore, we achieved the semantic interoperability in our Linked Open Data (LOD) application. Moreover, our research effectively addresses nearest health care centers using real time spatial data from Global Positioning System (GPS) and maps these locality to increase searching ability by mass people.
[Electronic publishing, generic spatial knowledge-base, health related service maintenance, Communities, semantic Web technologies, health related service improvement, GPS, Semantic Interoperability, ARQL, PCS computing approach, accessible cyber space data sources, spatial data harmonization, health care centers, Resource Description Framework, Linked Open Data, data sources, semantic repository, Health Care Network, semantic Web, linked open data, knowledge graph, Global Positioning System, SQL, HCN, Hospitals, Information services, semantic networks, implicit knowledge, health care organizations, Internet, graph based query language, locality maps, data integration, Semantic Knowledge Representation, data sharing, semantic interoperability, real time spatial data, open systems, accessible HCN-related unstructured data, Ontology, Cyberspace, World Wide Web, health care network, searching ability improvement, administrative structure, HCN-BD repository, physical world, semantic information integration, physical-cyber-social computing approach, HCN related data, domain information representation, health care, CSDS, multiple heterogeneous repositories, Semantic Information Integration, LOD application, accessible HCN-related semistructured data, WWW, keyword-based search engines, Organizations, Physical-Cyber-Social Computing, ontologies (artificial intelligence), HCN related machine understandable data source, information pattern reasoning, RDF triples, Geo-Bangladesh]
Achieving the proportional fairness in MU-MIMO uplink cellular networks
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper, the optimal scheduling problem is studied to maximize network utility according to proportional fairness policy in multi-user multiple-input and multiple-output (MU-MIMO) uplink cellular networks. An optimization framework is developed to maximize the aggregate utility and to compute the optimal scheduling parameters in a single cell cellular network. The optimization problem is a very complex non linear program (NLP). The problem is solved numerically for several medium size network scenarios using an optimization tool. The numerical results show that MU-MIMO technique provides a significant utility gain over single user MIMO (SU-MIMO) technique. Further, the optimal scheduling parameters are determined analytically for the both SU-MIMO and MU-MIMO schemes. Interestingly, the optimal scheduling is found to be the same as the round-robin scheduling for the both MIMO schemes which is also validated by numerical computation. In this work, it is demonstrated that the simple round robin scheduling can achieve the proportional fairness in a SU-MIMO as well as a MU-MIMO uplink cellular network.
[optimization problem, nonlinear programming, network utility, Transmitting antennas, SU-MIMO, Optimal scheduling, Throughput, telecommunication scheduling, optimal scheduling problem, NLP, numerical computation, optimization, proportional fairness, scheduling, MIMO, throughput, Round robin, MIMO communication, round robin, proportional fairness policy, MU-MIMO uplink cellular network, round-robin scheduling, Aggregates, multiuser multiple input multiple output, Multi-user MIMO, network utility maximization, complex nonlinear program, cellular radio]
Visualization and queuing analysis of spatio-temporal traffic data
2014 17th International Conference on Computer and Information Technology
None
2014
Traffic jams have become a major issue in Dhaka city which results in delay of numerous trips resulting in late arrivals, monetary loss, and tiredness. Traffic data was analyzed to model a visualization of the flow of traffic in the road. The model performs a fast analysis of large volumes of data efficiently. The funneling of the road at the horizon was solved to spread out the traffic showing top view of the road. The model reveals a track of the movement of the different types of traffic. The vehicle congregation hotspots can be analyzed to identify some features on the road like bus stops. Lane changing frequencies of the different types of vehicles have been compared. The M/M/c queuing model revealed that increasing the number of lanes, exponentially decreases queue length, waiting time in queue, length of system, and the waiting time in the system.
[Queuing model, Roads, road traffic control, M/M/c queuing model, Traffic, queue length, late arrivals, Dhaka city, Vehicles, monetary loss, traffic flow visualization, lane changing frequencies, Sociology, road funneling, traffic movement, queuing analysis, Cities and towns, traffic jams, tiredness, queueing theory, waiting time, automobiles, Statistics, traffic monitoring, Data visualization, Traffic data analysis, traffic data analysis, Data Visualization, Data models, spatio-temporal traffic data, system length, bus stops, vehicle tracking]
Knowledge discovery from academic data using Association Rule Mining
2014 17th International Conference on Computer and Information Technology
None
2014
Discovering the hidden knowledge from large volume of educational data and applying it properly for decision making is essential for ensuring high quality education in any academic institution. This knowledge is extractable through data mining techniques. Association Rule Mining technique aims at discovering implicative tendencies that can provide valuable information for the decision maker. In this paper, we present an applied research on mining Association Rule using academic data of a university. We have discovered knowledge regarding the academic performance and personal statistics of students. Here we have developed a technique to transform the existing relational database for students' academic performance into a universal database format using academic and personal data of a student. After that we have transformed the universal format into a modified format for suitability of using Association Rule mining algorithm. We have used Apriori algorithm for finding interested association rules from the transformed database which can be useful to extract knowledge of students' academic progress, decay in their potentiality, abandonment as well as retention of students. The impact of courses and curriculum and teaching methodologies are also found from the extracted knowledge which is beneficial for any institution of higher education.
[Computers, academic data, universal database format, data mining, Association Rules, educational administrative data processing, Classification algorithms, database management systems, Academic Performance, academic institution, Educational Data Mining, Databases, Education, educational data, data mining techniques, hidden knowledge discovery, Apriori algorithm, further education, high quality education, knowledge acquisition, higher education institution, Knowledge Discovery, Student Retention, relational database, Association rules, Information technology, association rule mining, personal data, Student Abandonment, knowledge extraction, decision making, educational institutions, personal statistics, student academic performance]
An evolutionary fuzzy genetic and Na&#x00EF;ve Bayesian approach for multivariate data classification
2014 17th International Conference on Computer and Information Technology
None
2014
Over past few decades, statistical and soft-computing techniques have become an emerging research area for machine learning problems. Fuzzy logic with better generalization capability and rapport with reality is being used in classification problems immensely. In this paper a fuzzy rule based classification system is modeled as a combinatorial optimization problem. Thus the optimization power of Genetic Algorithm has been applied to select a small number of significant fuzzy rules for a compact classification system applied to multivariate dataset classification. The hybrid algorithm is used here to predict cellular localization sites of proteins in yeast. Two different fitness measures were taken to evaluate the rules generated by the algorithm during evolution process. Than the performance of Fuzzy Genetic algorithm with better fitness measure is compared to the performance of Nai&#x0308;ve Bayesian approach to accurately detect and classify patterns. The performance analysis includes testing on train as well as test dataset, created from original dataset. It is found that the fuzzy genetic algorithm with the fitness measure based on data mining support-confidence framework performs better compared to other fitness measure but in terms of classification Nai&#x0308;ve Bayesian technique is much more reliable.
[data mining support-confidence framework, combinatorial mathematics, statistical techniques, data mining, fuzzy set theory, combinatorial optimization problem, Genetic Algorithm, machine learning problems, fuzzy reasoning, cellular localization sites, Niobium, Genetic algorithms, Nai&#x0308;ve Bayesian approach, Training, soft-computing techniques, Accuracy, biology computing, Sociology, proteins, Classification, Genetics, generalization capability, learning (artificial intelligence), pattern detection, Na&#x00EF;ve Bayes, evolution process, pattern classification, evolutionary fuzzy genetic algorithm, fuzzy logic, fuzzy rule based classification system, genetic algorithms, generalisation (artificial intelligence), multivariate dataset classification, yeast, Pragmatics, Machine learning, Bayes methods, Fuzzy, performance analysis]
Piezoelectric field-dependent optical properties of InGaAs/GaAs quantum well architecture in arbitrary crystal orientation
2014 17th International Conference on Computer and Information Technology
None
2014
A numerical approach is introduced to study the optical properties of compressively strained InGaAs/GaAs quantum well (QW) architecture considering the piezoelectric (PZ) effect by solving one dimensional Schro&#x0308;dinger equation using finite difference method. Unitary transformation is used to modify the wave vector and strain matrix in conventional [100] crystal orientation. The simulation is carried out for conventional crystal orientations [110] and [111] as well as non-conventional orientations [113] and [131]. It is found that there is a substantial correlation between piezoelectric field and conduction and valence band energy dispersion profile. Using the shift in energy bands due to PZ field, the optical gain profile for compressively strained QW is evaluated and compared with the optical gain profile neglecting PZ field. From the MATLAB simulation results, the regular optical gains are inspected as 2630, 1720, 2720, and 2020 cm-1 in [1 1 0], [1 1 1], [1 1 3], and [1 3 1] crystal orientations which indicates an average decrease of about 13% of peak gain due to presence of PZ field.
[Integrated optics, valence bands, Crystal orientation, Gallium arsenide, mathematics computing, Piezoelectric field, indium compounds, finite difference methods, one-dimensional Schrodinger equation, 131, InGaAs, 111, Schrodinger equation, 113, semiconductor quantum wells, numerical approach, unitary transformation, electro-optical effects, Optical amplifiers, Optical Gain, Optical fibers, gallium arsenide, conventional [100, strain matrix, Crystals, crystal orientation, Optical pumping, compressively strained quantum well architecture, wave vector, finite difference method, arbitrary crystal orientation, 100, piezoelectric field-dependent optical properties, piezoelectricity, two-dimensional spectra, Strained Quantum Well, conduction bands, III-V semiconductors]
A novel node mobility(NoM) approach of cognitive Radio Networks (CRNs) communication in beyond 4g spectrum management (SM)
2014 17th International Conference on Computer and Information Technology
None
2014
The amount of signaling needed to maintain the CRNs topology has been estimated with regard to different radius of clusters size management scenarios. The node mobility for the recommended network model has been simulated according to a Random Waypoint mobility model (RWP). The simulation results show that only a portion of the network hosts simultaneously traverse their respective cluster boundaries. Additionally, this amount is heavily impacted by the cluster sizes, which must be properly dimensioned based on factors like, local, geographical or regulatory considerations. The sides (vertices) of a large cluster become longer, which further increases the probability of nodes traversing them. Consequently, this signifies the need for a more flexible and dynamic allocation of the spectrum resources, which requires a new approach to cognitive radio network management. This can be achieved with the help of mobile devices capable of accurately sensing the spectrum occupancy, learning about temporarily unused frequency bands and able to reconfigure their transmission parameters in such a way that the spectral opportunities can be effectively exploited.
[Computers, recommended network model, cluster, mobility, signaling, CRN topology, transmission parameter reconfiguration, Mobile communication, mobility management (mobile radio), Network topology, cognitive radio, 4G, Sociology, clusters size management scenario, SM, radio spectrum management, random waypoint mobility model, probability, telecommunication network topology, RWP, Cognitive radio, Statistics, mobile device, 4G mobile communication, NoM Approach, cognitive radio network management, node mobility approach, pattern clustering, spectrum resource dynamic allocation, nodes traversing probability, spectrum, spectrum sensing, Tin, 4G spectrum management, CRN]
Effects of unequal bit costs on classical Huffman codes
2014 17th International Conference on Computer and Information Technology
None
2014
Classical Huffman codes have a very good compression performance over traditional systems. Yet, more efficient encoding is possible by considering and applying techniques that treat the binary bits differently considering requirement of storage space, energy consumption, speed of execution and so on. Future transmission systems are likely to be more efficient in many aspects. These systems will consume fewer resources to transmit or store one of the binary bits. Hence, an unequal bit cost would necessitate a different approach to producing an optimal encoding scheme. This work proposes an algorithm, which considers unequal bit-cost contribution to a message. Our experiment yields that the proposed algorithm reduces overall communication cost and improves compression ratio considerably in comparison to classical Huffman codes. This unequal bit cost technique produces a variant of Huffman Code that reduces total cost of the compressed message.
[Computers, Coding and information theory, data compression, binary codes, Huffman codes, binary bit storage space, Electronic mail, Information technology, optimal encoding scheme, Data Compression, Computer science, communication cost reduction, unequal bit cost effect, classical Huffman code, Huffman Coding, Huffman coding, energy consumption, data compression ratio improvement, Source Coding]
Intelligent windshield for automotive vehicles
2014 17th International Conference on Computer and Information Technology
None
2014
Windshield control is a vital operation of driver during driving. The mountings fitted in the windscreen or also called windshield are essential to use for smooth driving. These can be automated by using sensors and microcontroller. A complete windshield controlling system has been developed here to increase human comfort and flexibility. The wiper has been controlled by a water level sensor which regulate the wiper motor through sensing the level of water or rain. A dust sensors has been integrated to spill some water in the windscreen and then wipe it. It senses when a certain level of dust get accumulated in the screen. The sun visor which is mounted inside the car to shade the driver's eye from sun would be easier to control by a servo motor. Here an automatic sun visor has been designed to be controlled through a light sensor which is used to measure the light intensity and send the signal to the main control unit. This project focuses on improving human comfort in the existing system so that the driver can pay full attention in driving at all weather even in dusty, rainy or summer.
[Smart windscreen control, ergonomics, Vehicles, Wiper, human comfort, servomotors, automotive vehicles, Sensors, intelligent windshield, Servomotors, Automotive components, automatic sun visor, automotive components, microcontrollers, cleaning, Automation, Microcontrollers, automobiles, Sun visor control, windshield controlling system, optical sensors, wiper motor, light sensor, Sun, driver eye, Intelligent windshield, microcontroller, water level sensor, optical windows, servomotor, light intensity measurement, windscreen]
Improved protein disorder predictor by smoothing output
2014 17th International Conference on Computer and Information Technology
None
2014
Intrinsically disorder regions (IDRs) or, proteins (IDPs) are associated with important biological functions, while lacking stable structure in their native state. The phenomena of disordered proteins or residues are abundant in nature and are extensively involved in critical human diseases and hence impacting drug discovery. Thus, the study using disorder prediction is becoming crucial in the proteomic research. The large scale growth of genome database demands high performance computational methods for identification of protein disorder. We developed a canonical support vector machine based disorder predictor, DisPredict by integrating RBF kernel. It employs novel feature set for accurate characterization of disorder which outperformed two leading predictors: the neural network based SPINE-D and Meta predictor MFDp based on ten-fold cross validation. We propose a post processing of probabilities to further improve the accuracy, named DisPredict1.1 which yields outstanding performance further both in binary annotation and real valued probability prediction per residue in both short and long disordered regions. It provides highest Mathews Correlation Coefficient (MCC), competitive Area Under receiver operating characteristic Curve (AUC) and lowest Mean Absolute Error (MAE) when compared with twenty existing predictors of several kinds on independent benchmark dataset. DisPredict is available online.
[critical human diseases, canonical support vector machine based disorder predictor, intrinsically disorder proteins, AUC, Proteins, Training, Mathews correlation coefficient, high-performance computational methods, long-disordered regions, IDP, drug discovery, real valued probability prediction, IDR, Kernel, support vector machines, probability postprocessing, probability, Bigram, DisPredict1.1, genome database, Protein prediction, Pattern recognition, MAE, bioinformatics, intrinsically disorder regions, biological functions, Computers, feature set, Cross validation, proteomic research, SVM, binary annotation, Accuracy, output smoothing, accuracy improvement, area-under receiver operating characteristic curve, Databases, Monogram, proteins, disorder characterization, radial basis function networks, RBF kernel, mean absolute error, proteomics, Probability smoothing, short-disordered regions, MCC, Support vector machines, Intrinsic disorder, improved protein disorder predictor, disordered residues]
Connected component based approach for text extraction from color image
2014 17th International Conference on Computer and Information Technology
None
2014
Image based text extraction is one of the fastest growing research areas in the field of multimedia technology. The extraction of text from a complex or more colorful images is a challenging problem. Text data present in images contains useful information for habitual explanation, indexing, and structuring of images. Extraction of this information involves detection, localization, tracking, extraction, enhancement, and recognition of the text from a given image. For fast extracting text from images, we have proposed a connected component based approach which identifies more accurately for small or large texts in the image. The text extraction process starts with conversion of the color image to gray scale image and then it converts the gray scale image into a binary image. Then each text region is marked and the text is extracted from the image. Finally, the extracted text is written into another gray scale image. The experimental results demonstrate that the performance of the proposed method is superior compared to some recent approaches.
[Computers, text extraction, gray scale image, object detection, image-based text extraction, connected component, Data mining, Image, Text recognition, Image color analysis, image enhancement, feature extraction, multimedia technology, complex images, image colour analysis, image processing, text enhancement, indexing, Color, habitual explanation, Information technology, information extraction, text tracking, connected component based approach, text localization, text recognition, color image, Feature extraction, text detection, image structuring]
Towards a novel sensorless dual axis solar tracking algorithm for Bangladeshi PV modules
2014 17th International Conference on Computer and Information Technology
None
2014
There have been different types of solar trackers employing different algorithms and techniques. Nevertheless most of them adopt the use of different types of solar radiation sensors to detect the position of the sun over the sky. Consequently these algorithms introduce the use of advanced mathematics and complex hardware. In this paper, the authors propose a much simpler algorithm which uses no sensor and no advanced mathematics. The approximate positions of the sun over the sky in Bangladesh from 9.00 AM to 3.00 PM have been calculated for 365 days. Then linear quantization technique has been applied to define the optimum altitude and azimuths of the sun at each hour considering a step size of 10 degrees. These optimum altitude and azimuth angles can be fed to a simple low cost microcontroller for controlling the movement of the motors used in the tracker.
[Computers, sensorless solar tracker, Tracking, linear quantization technique, solar radiation sensors, optical sensors, Sun, Bangladeshi PV modules, automatic solar tracking system, Earth, sunlight, Azimuth, sun path diagram, Sensors, solar cells, Mathematical model, sensorless dual axis solar tracking algorithm]
A hybrid approach for decision making to detect breast cancer using data mining and autonomous agent based on human agent teamwork
2014 17th International Conference on Computer and Information Technology
None
2014
Every year many women die of breast cancer and the saddest part is that most of them die due to late diagnosis. A number of studies have been carried out to explore the basic reasons of breast cancer. Unfortunately, most of them failed to detect the main causes and the disease itself at a primary stage. On the other hand, it has already been proved that early detection of cancer can give the patient a chance of longer survival. Therefore, early detection of breast cancer is very crucial to save a patients' life. To address this problem, we have introduced a hybrid model to identify breast cancer at primary stage. In this model, the first part includes data mining using decision tree algorithm. The second part includes an autonomous agent that takes decision based on predefined rules to detect breast cancer at the very beginning stage. These rules are deduced through a data mining tool (i.e. Weka).The autonomous agent has been developed using Java, which works in collaboration with human. In this research work, we mostly focused on creating an adjustable autonomous agent and setting its rules and behaviors effectively. The performance of the proposed hybrid model has been tested on breast cancer dataset collected from UCI (University of California, Irvine) machine learning repository. The study reveals that autonomous agent works better when it collaborates with human as a team member. In addition, this hybrid model can be used to assist medical practitioners to provide better treatment to the patients.
[breast cancer detection, data mining, Data mining, primary stage, Autonomous Agent, autonomous agent, decision tree, patient treatment, UCI machine learning repository, Autonomous agents, Decision trees, Mathematical model, learning (artificial intelligence), hybrid approach, Java, adjustable autonomy, disease detection, decision tree algorithm, University of California-Irvine machine learning repository, Breast cancer, human-agent teamwork, software agents, medical practitioner assistance, bioinformatics, decision trees, decision making, cancer, Teamwork, Weka data mining tool]
A Package Based Clustering for enhancing software defect prediction accuracy
2014 17th International Conference on Computer and Information Technology
None
2014
Software defect prediction models considering clustering are to combine related features to enhance the probability of predicting defects. Aggregating related and similar classes is the main challenge in software clustering. An efficient clustering approach named as Package Based Clustering has been proposed to group the software for predicting defects. It uses Object Oriented classes' relationships and similarities to group the software into multiple clusters. To segregate a software project into multiple clusters, it performs textual analysis to identify all Object Oriented classes from the software project. Then it uses package information of each class to divide those into clusters. To analyze the proposed clustering algorithm, the linear regression model is used which learns from clusters of related and similar classes. The experiment has been conducted on JEdit 3.2 and results show that the prediction model using Package Based Clustering is 54%, 71%, 90% better than the prediction models built on BorderFlow clustering, k-means clustering and the entire system respectively.
[Measurement, object oriented classes, Java, project management, program testing, Object oriented modeling, package information, Software algorithms, software reliability, regression analysis, Predictive models, package based clustering, software project, JEdit 3.2, software defect prediction models, linear regression model, pattern clustering, software defect prediction accuracy, Clustering algorithms, software packages, textual analysis, Prediction algorithms, Software, object-oriented methods]
Employment of pulse shaping techniques for efficient PAPR reduction in OFDM system
2014 17th International Conference on Computer and Information Technology
None
2014
This paper aims at contributing to find out the best pulse shaping technique to reduce peak to average power ratio (PAPR) for OFDM system efficiently. At first different narrowband pulses (Sine, Tukey window and Kaiser window) as well as broadband pulses (Raised cosine and Square root raised cosine pulse) have been employed in the transmitter section. In order to carry out the performance analysis, a mathematical model is derived for each of the above employed pulse shaping techniques which includes equations for PAPR and cross correlation. The proposed technique needs only one IFFT/FFT in the transceiver and does not require any side information. Based on this model, a performance comparison is carried out between all the mentioned narrowband and broadband pulses. Numerical result shows that the square root raised cosine pulse reduces the PAPR more significantly than any other pulse and the existing system as well. Furthermore cross correlation function is also examined for both narrowband and broadband pulses. Finally in order to show the efficacy of the proposed efficient pulse shaping technique (Square root raised cosine pulse), bit error rate (BER) is calculated and compared with the conventional system. Simulation result shows that the proposed scheme can reduce the BER more effectively than the conventional system. Therefore, square root raised cosine pulse can be used not only to reduce the PAPR of OFDM system effectively but also to improve the BER performance of the system.
[fast Fourier transforms, Square root raised cosine, Correlation, OFDM, pulse shaping, transceiver, Peak to average power ratio, IFFT, Sine, inverse transforms, Narrowband, PAPR reduction, Raised cosine, FFT, PAPR, OFDM modulation, Mathematical model, pulse shaping technique, OFDM system, transmitter, error statistics, radio transceivers, Broadband communication, cross correlation function, bit error rate, BER, Tukey window, Kaiser window, square root raised cosine pulse, Pulse shaping methods, correlation methods]
Acoustic localization of unknown sources with wireless sensor nodes
2014 17th International Conference on Computer and Information Technology
None
2014
Accuracy of unknown sound source localization and lifetime of sensor nodes is one of the important and critical issue and challenge in wireless sensor network. Localization plays major role when there is an uncertainty of the exact source location of some fixed or mobile devices. The main problem is source localization in an environment where they try to localize and focus on a source of unknown sound and noisy distance. This paper presents a new approach on the indoor localization of an acoustic sensor source. In this localization system, the environment will be scattered with a swarm of acoustic motes. The motes used in this localization system are the LOTUS motes from MEMSIC. In order to get the system working. We utilize a modular sensor board to plug on the mote. This sensor board captures acoustic signals and then digitalizes them. This localization algorithm uses the incoming Time Difference of Arrival (TDOA) values of a swarm ZigBee modules. The location of the source of the acoustic signal is determined by the multilateration technique. A sensor network is used in order to collect the signals. The evident advantage of the algorithm is that it requires synchronization between the motes mutually and not between the motes and acoustic source. The algorithm for these methods is written in MATLAB. The motes communicate in a ZigBee mesh network.
[ZigBee mesh network, localization, wireless sensor nodes, ZigBee, Correlation, Time difference of arrival, wireless sensor networks, TDOA, Zigbee, wireless sensor network, Acoustics, indoor localization, acoustic localization, MATLAB, time difference of arrival, Microphones, Wireless communication, Wireless sensor networks, WSN, sensorboard, indoor radio, modular sensor board, sound source localization]
Stress detection of computer user in office like working environment using neural network
2014 17th International Conference on Computer and Information Technology
None
2014
Detecting the stress of computer user in an office like environment will enable more development of computer and make it intelligent enough where it can interact with its user, taking users effective state in to account; known as affective computing. In this research work, we have analyzed physical and mental stress of a computer user in all day long working environment by analyzing variations in physiological signals. Physiological data sets of 12 subjects were collected where all the subjects were went through a specific sequence of computer using session which includes different computer mediated task. To determine the stress level accurately and for detailed analysis of stress condition a three layer back propagation (BP) neural network were constructed. Research shows that stress level of a subject varies with computer using time and subject's effort towards work. And induced stress is mainly due to intense eye work and mental strain.
[Computers, Electrooculography, Time-frequency analysis, electro-oculography, affective computing, human factors, computer using time, Electroencephalography, computer mediated task, BP, electrocardiography, office-like working environment, EOG signal, BP neural network, computer sequence, physical stress analysis, EMG signal, computer user stress detection, Electromyography, mental stress analysis, ECG signal, stress detection, physiological signal, stress condition, physiological signal variation analysis, three-layer backpropagation neural network, Stress, induced stress, medical signal processing, Affective computing, user work effort, intense eye work, Artificial Neural Network, occupational stress, electromyography, backpropagation, Feature extraction, user effective state, Physiological data sets, neurophysiology, neural nets]
Image spatial resolution enhancement: A novel wavelet approach
2014 17th International Conference on Computer and Information Technology
None
2014
Enhancement of image spatial resolution is a widespread topic in the field of image processing and there exists various solutions with diverse approaches. But in the last few years wavelet based approaches show good results in image resolution enhancement. In this paper, we present another new wavelet based image resolution enhancement technique where discrete wavelet transformation (DWT) is used to decompose the low resolution (LR) image into frequency subbands (horizontal, vertical and diagonal). All these frequency subbands are binarized and then interpolated using lanczos interpolation. At the same time Laplacian filter, Sobel operators and lanczos interpolation are used to estimate another set of binarized horizontal and vertical subbands. Finally these pairs of binarized horizontal and vertical subbands are added (bitwise OR) and used into the inverse DWT process for generating a new high resolution (HR) image. This new technique has been tested on four famous test images Lena, Baboon, Elaine and Peppers. Experimental results show that the proposed technique outperforms conventional and state-of-the-art techniques in terms of Peak Signal to Noise Ratio (PSNR), Root Mean Square Error (RMSE), entropy and Universal Quality Index (UQI) value. In addition behavior of the proposed technique for different wavelets are simulated and discussed.
[discrete wavelet transformation, peak signal to noise ratio, Entropy, entropy, image enhancement, frequency subband binarization, UQI value, high resolution image, mean square error methods, image resolution, binarized horizontal subbands, image processing, DWT, PSNR, discrete wavelet transforms, root mean square error, binarized vertical subbands, Laplacian filter, Discrete wavelet transforms, RMSE, low resolution image, DWT process, HR image, universal quality index value, Sobel operators, LR image, Interpolation, interpolation, lanczos interpolation, image spatial resolution enhancement, Spatial resolution]
Hardware-based DLAS: Achieving geo-location guarantees for cloud data using TPM and Provable Data Possession
2014 17th International Conference on Computer and Information Technology
None
2014
Recently the lack of geo-location assurance of data in cloud storage has been identified as one of the main reasons why organizations that deal with sensitive data (e.g., financial data, health related data) cannot adopt a cloud storage solution even if they want to. In this paper, we present a Hardware-based Data geo-Location Assurance Solution (HDLAS), which is suitable for almost all cloud storage applications available today. Trusted Platform Module (TPM) and a cryptographic scheme called Provable Data Possession (PDP) are the basis of our solution. We define a new attack model for HDLAS which seems to be a realistic attack model for the existing cloud storage applications. With the combination of a GPS receiver and TPM, HDLAS is able to offer its clients not only the accurate geo-location of their data but also a hardware-based root of trust for that. Unlike many existing solutions, HDLAS works even if a piece of data is replicated into different storage servers. Furthermore we also illustrate how easily HDLAS can be adopted in existing Cloud Storage Providers such as Microsoft Azure.
[Cloud computing, trusted platform module, cloud storage applications, Cloud security, geolocation guarantees, Memory, Servers, cloud storage providers, TPM, Geo-location of data, storage management, cryptographic scheme, HDLAS, Microsoft Azure, hardware-based DLAS, cloud computing, PDP, Secure cloud storage, GPS receiver, Hardware-based DLAS, cryptography, radio receivers, Global Positioning System, Data geo-location problem, storage servers, Secure cloud computing, Public key, hardware-based data geolocation assurance solution, provable data possession, Data models, trusted computing, Accountable cloud]
Image enhancement in spatial domain: A comprehensive study
2014 17th International Conference on Computer and Information Technology
None
2014
With the advancement of imaging science, image enhancement has become an important aspect of image processing domain. It is necessary to gather a comprehensive knowledge regarding the existing enhancement technologies to identify and solve their problems and thus to elevate the current image enhancement methodologies. This paper provides the underlying concept of contrast enhancement, brightness preservation as well as brightness enhancement techniques. Besides this, we provide a short description of the existing renowned enhancement methods with their mathematical description and application area. Moreover, experimental results are provided to make a comparative analysis where both qualitative and quantitative measurements are performed. Different enhancement methods are run on same images to examine the qualitative performance. Peak signal to noise ratio (PSNR), normalized cross-correlation (NCC), execution time (ET) and discrete entropy (DE) are quantitative measurement metrics used for quantitative assessment. Most of the cases, it is found that Histogram Equalization has the highest degree of deviation from the input image which basically generates more visual artifacts. Contextual and Variational Contrast enhancement technique takes long time for execution with respect to other enhancement techniques. From our quantitative and qualitative evaluation, we find that Layered Difference Representation performs comparatively produces better enhancement result in all aspect than other existing methods.
[Visualization, Brightness, peak signal to noise ratio, histogram equalization, Entropy, Histograms, variational contrast enhancement technique, entropy, image enhancement, image enhancement methodologies, contrast enhancement, brightness enhancement techniques, normalized cross-correlation, discrete entropy, PSNR, brightness preservation, quantitative measurement metrics, Information technology, NCC, imaging science, execution time, image representation, visual artifacts, mathematical description, contextual contrast enhancement technique, layered difference representation, Image enhancement, correlation methods, image processing domain]
An improved algorithm for solving helix generation of RNA secondary structure prediction
2014 17th International Conference on Computer and Information Technology
None
2014
This paper presents an efficient O(n2) time algorithm for solving the helix generation problem of RNA to predict the predict the secondary structure of that RNA. It encodes the RNA secondary structures as an integer permutation of helices. The helices are pre-computed by the helix generation algorithm and each integer corresponds to a candidate helix. In this paper, a helix is formed only when three or more adjacent base pairs are formed and the loop connecting the helix must be at least three nucleotides in length. From this algorithm we find all possible helices that can form in a structure. After that we predict secondary structure of RNA by SARNA-Predict based on Simulated Annealing (SA). SARNA-Predict use a permutation-based representation to the RNA secondary structure and percentage swap translocating mutation operator to find a solution with a lower free energy [1]. Calculating the minimum free energy, we find the stable secondary structure of the RNA.
[Algorithm design and analysis, Computers, O(n2) time algorithm, ribonucleic acid, candidate helix, RNA, SARNA-Predict, RNA secondary structure prediction, Minimum Free Energy, permutation-based representation, Prediction algorithms, Silicon, percentage swap translocating mutation operator, integer permutation, simulated annealing, helix generation algorithm, mathematical operators, Secondary Structure Prediction, Information technology, base pairs, Helix, molecular biophysics, Time complexity, nucleotides, computational complexity]
Incremental aggregation scheme based on Extendible Karnaugh Arrays
2014 17th International Conference on Computer and Information Technology
None
2014
Data is increasing so rapidly that new data warehousing approaches are required to process and analyze data. Aggregation of data incrementally is needed to fast access of data and compute aggregation functions. Multidimensional arrays are generally used for this purpose. But some disadvantages such as address space requirement is large and processing time is comparatively slow in case of aggregation. For this purpose we use Extendible Karnaugh Array (EKA). EKA is an efficient scheme which has better performance than other data structures that we have tested in our research. In this research work we use EKA as basic structure for implementing incremental aggregation of data and evaluate its performance over other approaches. We use Multidimensional Online Analytical Processing (MOLAP) which stores data in optimized multi-dimensional array storage, rather than in a relational database. We create MOLAP data cube using Traditional Multidimensional Array (TMA) and EKA scheme and compare incremental aggregation with Relational Online Analytical Processing (ROLAP). The effective outcome of EKA structure for incremental aggregation on MOLAP structure is shown by some experimental results.
[Computers, Karnaugh Map, data mining, Dynamic Extension, data aggregation functions, MOLAP data cube, History, extendible Karnaugh array, Data Cube, storage management, Databases, incremental data aggregation, multidimensional online analytical processing, EKA structure, MOLAP structure, data structures, EKA, data analysis, EKA scheme, TMA, OLAP, Information technology, traditional multidimensional array, Aggregates, Extendible Array, ROLAP, MOLAP, Arrays, multidimensional array storage, data warehouses, data warehousing approaches, Multidimensional Array, space requirement, data processing time]
Wireless sensor network for information based domestic power management
2014 17th International Conference on Computer and Information Technology
None
2014
Residential power consumption and the increasing environmental pollution emitted from the electric generators have lead to the energy management focus. To minimize the power consumption, the overall efficiency of electrical networks must have to be improved. Smart Grid concept has played an important role in moving towards better energy management. The current technologies and projects fail to address the energy waste issue and has given less afford on standby power management and energy monitoring. In this paper, we focus on the energy waste issue and proposed a wireless sensor network for gathering contextual information and controlling the domestic power consumption. Our proposed wireless sensor network consists of multi-modal multi-sensor nodes which is used to monitor different activities and movements, environmental factors and the power consumption of the appliances. Then it controls the operation of the appliances and manages their power consumption according to the gathered contextual information from the nodes thereby it prevents power consumption on unused or unnecessary services.
[information based domestic power management, Power demand, smart grid, wireless sensor networks, environmental factors, wireless sensor network, smart power grids, multimodal multisensor nodes, domestic appliances, Vehicles, Temperature sensors, Temperature measurement, Home appliances, Wireless sensor networks, energy management systems, Domestic Power Management, Wireless Sensor Network, domestic power consumption control, Domestic Power Saving, energy waste issue, energy management, Monitoring]
A mobility model for MANET in large Scale disaster scenarios
2014 17th International Conference on Computer and Information Technology
None
2014
Modeling mobility in a large scale unorganized disasters using existing mobility models is a tedious and cumbersome task because they are designed for small scale and well organized disasters. In large scale unexpected disasters obstacles play a key role in determining the path of the nodes. Moreover, complete path from source to destination cannot be determined at the initial step. It is to be determined dynamically step by step instead. We use ant colony optimization to select the path dynamically. We propose a mobility model which facilitates simple and realistic placement of obstacles. The disaster area is divided into small cells to model the obstacles in a realistic way. As there are no mobility traces available for the disaster scenarios, we generate synthetic traces and compare them with the existing mobility models for disaster scenarios. We observed significant differences in the mobility parameters. This can be due to the dynamic path selection in our model.
[Computers, mobile ad hoc network, Computational modeling, disasters, Gaussian distribution, dynamic programming, Probabilistic logic, Disaster Recovery scenario, mobility management (mobile radio), Information technology, Mobile ad hoc networks, dynamic path selection, Large Scale Disaster Mobility Model(LSDMM), MANET mobility model, Obstacles, large scale unorganized disasters, MANET, Performance Evaluation, ant colony optimization, mobile ad hoc networks, ant colony optimisation, Mobility model]
A comprehensive method for attribute space extension for Random Forest
2014 17th International Conference on Computer and Information Technology
None
2014
Attribute space extension for decision forests often contribute to improving the ensemble accuracy. In this paper we suggest the use of a recent method for attribute space extension where the newly generated attributes that have high classification capacity are chosen for extension. In literature, it is shown that the inclusion of these new attributes in the attribute space increases the prediction capacity of the decision trees. Random Forest is a state-of-the-art popular forest building algorithm that generates quite diverse decision trees. To increase the ensemble accuracy of Random Forest we consider the inclusion of more attributes with high classification capacity and employ the attribute extension technique that guarantees inclusion of newly generated attributes with higher classification capacity. We conduct an elaborate experimentation on ten different data sets from the UCI Machine Learning Repository. The experimental results show that ensemble accuracy for Random Forest increases when it is supplied with the aforementioned attribute extension technique.
[pattern classification, Decision Tree, Attribute Space Extension, Buildings, classification capacity, random forest, Random Forest, Radio frequency, prediction capacity, Ensemble Accuracy, ensemble accuracy, Accuracy, forest building algorithm, Training data, Vegetation, decision trees, data sets, UCI machine learning repository, attribute space extension technique, decision forests, Decision trees, learning (artificial intelligence), Bagging]
LDA based paper currency recognition system using edge histogram descriptor
2014 17th International Conference on Computer and Information Technology
None
2014
Paper currency recognition is an important concern for automation to improve our daily monetary activities. Such recognition system uses the banknote images to train a classifier for identification of unknown input notes. One of the basic problems of such system is high dimensional representation of the feature vector (more than 100 dimensions) of note images. Moreover, most of the traditional approaches do not consider to minimize intra-class scatter and maximize inter-class scatter. To get rid of these basic shortcomings, in this paper, we propose an LDA based paper currency recognition method using edge histogram descriptor (EHD). Applying this method, we succeed to represent a note image by a very low dimensional feature vector (around only 15 dimensions). Besides adjusting the scatter of different classes, this method has the ability to tolerate noise of a certain level. We have performed different experiments to support all attractive features of the proposed system. For those experiments, we have used banknotes of different countries and achieved high accuracy with low dimensional feature vector.
[EHD, Paper Currency Recognition, image classification, Noise, banknote images, Training, Histograms, Edge Histogram Descriptor (EHD), feature extraction, edge detection, Texture Descriptor, high-dimensional feature vector representation, edge histogram descriptor, Image edge detection, interclass scatter maximization, daily monetary activity improvement, Vectors, image classifier training, Noise measurement, unknown input note identification, image denoising, note image representation, LDA-based paper currency recognition system, noise tolerance ability, Linear Discriminant Analysis (LDA), image representation, Feature extraction, low-dimensional feature vector, intraclass scatter minimization]
A probabilistic approach to support Self-Organizing Map (SOM) driven facial expression recognition
2014 17th International Conference on Computer and Information Technology
None
2014
Automated understanding of human facial expression is an active and concerning research topic. It is expected that in near future full-fledged understanding of human facial expression will enable machines to behave more intelligently. In this paper we proposed a system for automatic facial expression recognition. A consistent combination of Self-Organizing Map (SOM), Learning Vector Quantization (LVQ) and Nai&#x0308;ve Bayes classifier is developed to recognize facial expression from Cohn Kanade (CK) and Japanese Female Facial Expression (JAFFE) database. Satisfactory experimental results yield the possibility of using this system in real world applications. Proposed methodology shows an accuracy rate of 81.5% for CK dataset and 87.2% accuracy rate for JAFFE dataset.
[Local Binary Pattern (LBP), image classification, LVQ, Classification algorithms, emotion recognition, probabilistic approach, automated human facial expression understanding, Na&#x00EF;ve Bayes Classifier, Cohn Kanade database, JAFFE database, nai&#x0308;ve Bayes classifier, self-organising feature maps, face recognition, Face, learning (artificial intelligence), Testing, Gold, automatic facial expression recognition, Face recognition, support self-organizing map driven facial expression recognition, CK database, SOM-driven facial expression recognition, Self-organizing Map (SOM), vector quantisation, Support vector machine classification, learning vector quantization, Facial Expression Recognition, Feature extraction, Bayes methods, Japanese Female Facial Expression database, Learning Vector Quantization (LVQ)]
Designing an efficient photovoltaic system with maximum power point tracking technique by comparing different converter topologies
2014 17th International Conference on Computer and Information Technology
None
2014
Photovoltaic modules show nonlinear output characteristics because of which different system loss occurs. Maximum power point tracking (MPPT) is an intelligent technique for reducing these losses by driving the system at its optimal operating point. In this paper two well established MPPT techniques: Perturb and Observe (P&amp;O) method and Incremental conductance (INC) method is explained and verified with simulated results. Due to some drawbacks in P&amp;O method, a comprehensive optimized photovoltaic system is designed implementing the INC method in Matlab/Simulink. DC/DC converter is an essential part of a MPPT controlled photovoltaic (PV) system which functions as an interface between PV system and load. There are many converter topologies whose are implemented according to their required applications. A detailed comparative study among buck, boost and buck-boost converters is presented here. Our study shows that among these three converters buck-boost delivers the maximum power to the load. Finally an optimized PV system implementing INC method interfaced with buck-boost converter is designed and simulated which is robust and compatible to all other techniques.
[intelligent technique, Temperature, solar energy, MPPT, INC method, Capacitors, MPPT techniques, buck-boost converter, DC/DC converter, photovoltaic modules, perturb and observe method, nonlinear output characteristics, buck-boost converters, Photovoltaic systems, Radiation effects, Voltage measurement, PV system, efficient photovoltaic system, PV power system, photovoltaic power systems, MPPT efficiency, Topology, incremental conductance method, Maximum power point trackers, maximum power point trackers, maximum power point tracking technique, P&amp;O method]
Wavelet based watermarking approach of hiding patient information in medical image for medical image authentication
2014 17th International Conference on Computer and Information Technology
None
2014
Watermarking scheme is required to secure data to protect digital contents from unauthorized modifications. Image watermarking scheme can effectively be used in medical image processing to authenticate or investigate the integrity on medical images. In this paper, discrete wavelet transform (DWT) domain and chaotic system based medical image watermarking scheme has been proposed for hiding patient information in medical image to authenticate, in other words, to trace the origin of the image. The patient information has been embedded into the medical images as watermark, without affecting the image quality. DWT has been applied to the host medical image to decompose the image into four sub-bands of low and high frequency components. Chaotic system shows complex behavior and improves the security level of watermarking scheme by providing secret keys. In this paper, logistic map has been used through which chaotic watermark has been obtained, which has been embedded into the low frequency sub-band of medical image. The low frequency sub-band has been divided into 3&#x00D7;3 non overlapping blocks. The chaotic watermark has been embedded into each block by observing neighbor pixels conditions and modifying one of neighbor pixels. Experimental results of the proposed method have been compared with other existing medical image watermarking scheme. From the experimental results, it can be seen that the proposed watermarking scheme outperforms the existing methods. The proposed method achieves high values of peak signal to noise ratio (PSNR) of watermarked image and high values of normalized correlation (NC) of the extracted watermark.
[discrete wavelet transformation, DWT domain, image watermarking, digital content protection, Watermarking, medical image authentication, peak signal-to-noise ratio, normalized correlation, discrete wavelet transform domain, medical administrative data processing, chaotic system, medical image processing, wavelet-based watermarking approach, PSNR, discrete wavelet transforms, data security, Discrete wavelet transforms, image quality, patient information, medical image watermarking scheme, private key cryptography, chaotic watermark, Medical diagnostic imaging, Logistics, logistic map]
LED matrix based digital learning display for children with wireless control
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper a digital learning display has designed especially for the children in rural areas from which they can learn Bengali and English alphabet and numbers. The display is formed cascading twelve 8&#x00D7;8 dot matrix displays so that the designed character patterns are attractive to the children. It can be controlled by an IR remote. Only a 12V DC battery is enough to drive the display. The power consumption of the display is very low. So, schools at remote area where electricity is not available can easily use this display to make the learning more enjoyable to the children. An 8-bit PIC microcontroller is used to control the display and generating different character patterns. This is our first attempt to make the learning interesting for the children. In future, we can add some additional features with this display such as games, music, animation etc.
[Computers, linguistics, Shift registers, English number learning, remote area schools, rural area children, matrix display, power consumption, Bengali number learning, digital learning, LED matrix based digital learning display, wireless control, microcontrollers, Bengali alphabet learning, Microcontrollers, dot matrix displays, English alphabet learning, character patterns, Receivers, Light emitting diodes, LED displays, IR, IR remote, Information technology, light emitting diodes, PIC microcontroller, microcontroller, natural languages, computer aided instruction, educational institutions, Transistors, DC battery]
Optimized hardware architecture for implementing IEEE 754 standard double precision floating point adder/subtractor
2014 17th International Conference on Computer and Information Technology
None
2014
IEEE 754 standard double precision (64-bit) binary floating point arithmetic unit is often necessary in complex digital signal processing applications. The basic operations, floating point addition and subtraction, need to be optimized to efficiently compute floating point multiplier, divider and square root. However, the main challenge is to design the floating point arithmetic unit hardware that uses fewer logical resources of FPGA and ASIC and has a maximum operating frequency with a fewer number of clock cycles. This paper proposes a new, efficient hardware design methodology for implementing double precision floating point addition and subtraction. The pipeline hardware design is implemented on Virtex-6 and Virtex-5 Xilinx FPGA. As per the synthesis result, the maximum operating frequency achieved for the proposed hardware design for clock latency of 8 cycles is significantly higher than the previous hardware designs. Furthermore, area overhead is 50 percent fewer than that of the earlier proposed hardware designs for computing IEEE 754 compliant double precision floating point addition and subtraction.
[Algorithm design and analysis, IEEE 754 standard double precision binary floating point arithmetic unit, field programmable gate arrays, Virtex-6 Xilinx FPGA, FPGA, floating point divider, Virtex-5 Xilinx FPGA, clock latency, adder, floating point arithmetic, ASIC, adders, Computer architecture, IEEE 754 standard double precision floating point adder-subtractor, logical resources, double precision, Hardware, complex digital signal processing applications, IEEE standards, Leading Zero Counter, Adders, Virtex-6, optimized hardware architecture, Barrel Shifter, Standards, IEEE 754, subtractor, clocks, floating point multiplier, hardware design methodology, floating point square root, Field programmable gate arrays, application specific integrated circuits, floating point, Clocks, pipeline hardware design]
ICT based market information system: An effective approach for market price monitoring and supervision in developing countries
2014 17th International Conference on Computer and Information Technology
None
2014
This paper presents an effective framework for market monitoring, supervision and price management in developing countries using Information and Communication Technology (ICT). This research also illustrates the possible ways to handle the price hike and also derives solutions to resist unethical and ill-motivated hoarding by integrating ICT. In this paper, we have presented an overview on the application of ICT in disseminating market price information to the consumers and also existent mechanisms in adopting ICT as a tool to stabilize the market price from the point of view of consumers' right. Rather than paying the price of any commodity in cash to the seller, the consumer is charged through the identification of his mobile phone within particular fair price margin by sending SMS. The seller also demands the price using SMS and confirmation is also made by the buyer through the same. This fair price margin should be market specific and be defined by the government or some legitimate agencies appointed to monitor the market as currently done to some extent for displaying the market price on the entrance of the markets in large cities. Since, the price margin is supposed to be defined balancing mutual interests of the producer, seller and buyer along with considerations of all other related phenomena like transport costs to the market, labor costs in that area etc, there may be legal bindings to come to an agreement. Illustrating ICT based promising and easiest framework of making the people aware of price of any product and thus maintaining a reasonable price for markets in developing country like Bangladesh is the main contribution of the paper.
[Computers, legitimate agencies, market research, Security, Information systems, Electronic Commerce, information and communication technology, Production, Market Information System, Agriculture, Information and Communication Technology, Monitoring, price hike, ICT based framework, information dissemination, Government, market price information dissemination, marketing data processing, Market Monitoring, Electronic Governance, Bangladesh, developing countries, price margin, market price monitoring, market price supervision, price management, mobile phone, consumer right, pricing, mobile handsets, ICT based market information system]
Impact of underwater bandwidth on cross-correlation based node estimation technique
2014 17th International Conference on Computer and Information Technology
None
2014
Bandwidth is the most monumental topic both for the terrestrial and underwater communications. For proper network operation the estimation of number of signal sources (N) is very important. Most of the protocol based techniques are failed to give the desired results of node estimation due to underwater properties (long propagation delay, high absorptions and dispersion). Node estimation technique using cross-correlation is not based on protocol, where infinite band Gaussian signals are used. But the underwater bandwidth is finite (1-15 kHz). This limited bandwidth can encumber the signal bandwidth which might invade the estimation process. In this paper, the number of estimated nodes is being obtained considering the effect of underwater bandwidth using three sensors.
[node estimation, Protocols, wireless sensor networks, cross-correlation, Conferences, bandwidth, Estimation, infinite band Gaussian signal, terrestrial communication, underwater acoustic communication, underwater communication, underwater acoustic sensor networks (UASN), Bandwidth, underwater bandwidth Impact, sensor, underwater acoustic sensor network, Communication networks, signal sources number estimation, Radiofrequency identification, correlation methods, cross-correlation based node estimation technique]
An optimized algorithm to find maximum parsimonious tree using PrimeNucleotide based approach
2014 17th International Conference on Computer and Information Technology
None
2014
Phylogenetic inference methods like Maximum-parsimony perform exhaustive search strategy to extract evolutionary information from genomic sequences. Complexity arises when we increase the number of sequences involved, as the number of possible solutions increase exponentially alongside. In this paper, we have proposed an algorithm which identifies the highest repeating nucleotide (PrimeNucleotide) from the informative site efficiently to fix one ParentNode with the best fitted nucleotide using a predefined WeightMatrix to find the most parsimonious phylogenetic tree in linear time. The algorithm has been applied on the genome sequences of different bacteria and viruses to ensure its efficiency and universality. The results obtained were similar to the traditional Transverse-parsimony method and a significant improvement in both time consumption and memory usage rate were achieved.
[Algorithm design and analysis, genomic sequences, phylogenetic tree, phylogenetic inference methods, bacteria, Genomics, Transeverse-parsimony, Phylogeny, highest-repeating nucleotide, Biological cells, evolutionary information extraction, memory usage rate improvement, genetics, ParentNode, parsimonious phylogenetic tree, genomics, maximum-parsimony, Bioinformatics, search strategy, time consumption improvement, viruses, evolution (biological), Maximum-parsimony, optimized algorithm, Topology, weight matrix, inference mechanisms, parent node, microorganisms, prime nucleotide based approach, PrimeNucleotide, Vegetation, bioinformatics, WeightMatrix, exhaustive search, linear time]
Cloud based healthcare application architecture and electronic medical record mining: An integrated approach to improve healthcare system
2014 17th International Conference on Computer and Information Technology
None
2014
Healthcare system can be enhanced vastly with the use of modern information technology. Still now in underdeveloped and developing countries, traditional paper based system is being used in healthcare. Although very few organizations use computer based system, they could not establish a ubiquitous network among patients, physicians and government. Cloud computing is the emerging technology which can be used to develop a heterogeneous network to improve the system. In this article, a three tier cloud based application &#x201C;eHealth Cloud&#x201D; has been developed which will involve different parties to improve old-fashioned healthcare system. RIA (Rich Internet Application) based client, SimpleDB based server and a logic layer have been designed to build an easily accessible network. By using the &#x201C;eHealth Cloud&#x201D;, enormous electronic medical record (EMR) will be stored everyday. This huge size of data can lead us with new research opportunities. Data mining from the large amount of EMR has been proposed. The process of data mining, a standard for exchanging data and a mining model is described. Finally, the challenges and future research options are directed.
[computer based system, Cloud computing, RIA-based client, data exchange, data mining, Medical services, Rich Internet Application based client, ubiquitous network, three-tier cloud based application, Data mining, Servers, healthcare system improvement, cloud-based healthcare application architecture, healthcare, Databases, cloud computing, health care, service-oriented architecture, client-server systems, Government, data size, SimpleDB based server, electronic health records, e-health cloud, EMR, electronic medical record mining, electronic data interchange, logic layer, developing countries, integrated approach, heterogeneous network development, Medical diagnostic imaging]
Investigation of electric field intensity and degree of uniformity between electrodes under high voltage by Charge Simulation Method
2014 17th International Conference on Computer and Information Technology
None
2014
This paper describes degree of uniformity and electric field intensity in the gap between two electrodes subjected to high voltage which is necessary to study the behavior of insulation for design of high voltage equipments. Here, 3-D arrangements of fictitious charges and contour points in Cartesian co-ordinates are considered to find out the desired outputs for Sphere-Sphere and Sphere-Plane electrodes by using well known Charge Simulation Method (CSM), using MATLAB. Both the symmetrical and asymmetrical applied voltages are considered for the investigation. From this investigation it is revealed that more uniform electric field having degree of uniformity of &#x03B7;=0.5507 and 0.4394 has been obtained for symmetrical applied voltage of Sphare-Sphare and Sphare-Plane respectively, compared to &#x03B7;=0.3699 and 0.3428 for asymmetrically applied voltage. It is also found that the probability of insulation breakdown is greater near the High Voltage electrode compared to other portion of the dielectric. The developed computer program can be used in designing HV equipments.
[probability, MATLAB, Electric field intensity, Charge Simulation Method (CSM), insulation, Geometry, electric fields, electric field intensity, sphere-plane electrodes, Schwaiger factor, insulation breakdown probability, High voltage, electric field degree, high voltage equipments, sphere-sphere electrodes, charge simulation method, insulation behavior, CSM, Cartesian coordinates]
Denoising of ECG signals using dual tree complex wavelet transform
2014 17th International Conference on Computer and Information Technology
None
2014
Electrocardiogram (ECG) signals usually corrupted by different types of noise like power line interference, baseline drift due to respiration, electromyogram interference, abrupt baseline shift and their composite noise. Denoising of noisy signal has great clinical importance for the diagnosis of cardiac abnormalities. In this paper, dual tree complex wavelet transform (DTCWT) has been used to denoise noisy ECG signals. DTCWT generates complex coefficient to obtain their real and imaginary part by using wavelet filters of dual tree. Noisy ECG signal has been simulated applying five different noise models to ECG records of MIT-BIH Arrhythmia database. The calculated SNR and MSE after denoising using DTCWT showed better performance for all types of noise than the discrete wavelet transforms (DWT). Dual tree complex wavelet transforms has performed significantly for composite of different noises.
[electromyogram interference, baseline drift, Noise reduction, power line interference, electrocardiography, ECG records, signal denoising, dual tree complex wavelet transform, denoising, SNR, Electrocardiography, MIT-BIH Arrhythmia database, noise, mean square error methods, MSE, DWT, noisy signal denoising, cardiac abnormalities, electrocardiogram signals, discrete wavelet transforms, trees (mathematics), ECG, Discrete wavelet transforms, wavelet filters, noisy ECG signal denoising, medical signal processing, DTCWT, composite noise, Signal to noise ratio]
Ultrasound strain imaging based on information theoretic delay estimation
2014 17th International Conference on Computer and Information Technology
None
2014
Ultrasound strain imaging has great importance as tumor or cancerous tissues are much stiffer than normal tissues. Estimations of strain calculating the time delays between consecutive ultrasound echo signals are common approaches in quasi-static elastography. Because of the ill posed nature of strain images, the ultrasound strain estimation still remains challenging. In this paper, we present an iterative strain estimator that uses mutual information between stochastic pre and post compressed radio frequency (RF) signals to define time delays. For each RF signal, a past and future is defined by cutting sample sequences into two parts. The time delay is considered to be the time shift between the cutting moments for which the mutual information between both past vectors and future vectors reaches a minimum. Displacement calculation with this time delay provides effective strain estimation for small strains. A synthetic phantom of tissue with tumor is modeled using the finite element method (FEM) and the pre-/post-compressed RF signals are generated using ultrasound simulation package Field II. Using these simulated RF data displacements and strain of deformation are calculated in MATLAB. The Contrast to Noise Ratio (CNR) and Signal to Noise Ratio (SNR) are found better compared to conventional cross-correlation method. This algorithm can detect inclusions having echogenicity very close to that of the surrounding tissues and hence can be useful for detection of tumors or cancers.
[information theoretic delay estimation, mathematics computing, biomedical ultrasonics, conventional cross-correlation method, echogenicity, MATLAB, normal tissues, Ultrasonic imaging, finite element method, linear ultrasound, deformation, stochastic processes, strain estimation, tumours, Estimation, time delay, cancerous tissues, Strain, medical signal processing, sample sequences, cancers, consecutive ultrasound echo signals, Finite element analysis, Delays, ultrasonic imaging, iterative strain estimator, synthetic phantom, iterative methods, time shift, ultrasound simulation package Field II, delay estimation, cutting moments, contrast-to-noise ratio, ultrasound strain imaging, biomechanics, simulated RF data displacements, displacement calculation, Radio frequency, phantoms, signal denoising, stochastic precompressed radiofrequency signals, mutual information, FIELD II, stochastic post-compressed radiofrequency signals, time delays, finite element analysis, quasistatic elastography, FEM, tumor, signal-to-noise ratio, cancer, Mutual information]
Gaussian noise reduction in digital images using a modified fuzzy filter
2014 17th International Conference on Computer and Information Technology
None
2014
Noise can be easily induced in images during acquisition and transmission. Therefore, it is a basic requirement to remove noise from an image while keeping its features intact for better understanding and recognition. Gaussian and impulse are two very common types of noise. Tremendous research initiatives are being taken for removing these noises. Previously, we developed a fuzzy filter that effectively removes high density impulse noise in images. In this paper, we proposed a modified fuzzy filter for reduction of Gaussian noise. Experimental result confirms the superiority of the proposed method compared to the conventional filters in terms of both denoising and details preservation.
[Wiener filters, PSNR, fuzzy set theory, image transmission, fuzzy filter, Nonlinear filters, image filtering, fuzzy membership function, image acquisition, peak signal-to-noise ratio, image denoising, image noise removal, Gaussian noise reduction, Gaussian noise, modified fuzzy filter, impulse noise, Information filters, digital images, image recognition]
Detection and Localization of objects within a volume conductor using electrical impedance measurements: phantom study
2014 17th International Conference on Computer and Information Technology
None
2014
A uniform volume conductor has symmetrical distribution of electrical impedance. Electrical impedance will change in the presence of a substance (conductor or insulator or any biological tissue) whose dielectric properties differ from that uniform volume conductor. This impedance change could be measured by surface electrodes. Electrical impedance tomography (EIT) is a technique by which impedance measurements on surface of an object are reconstructed into impedance images. It is fast, inexpensive and non-invasive but has relatively low spatial resolution. This paper describes a simple but effective method of detection and localization of objects within a volume conductor using electrical impedance measurements. Sixteen electrodes were placed equidistantly on the surface of a cylindrical tank filled with saline and electrical transfer impedances from 6 pair of electrodes were measured. Similar impedance profiles were also measured by placing in homogeneity (iron rod and glass sphere) at an arbitrary position. Absolute impedance variation from the uniform volume conductor impedance for both objects is calculated and plotted. For both cases, the absolute impedance variation showed maximum value for the electrode near the object which confirms that detection and localization of objects within volume conductor is possible by the proposed method. With the data recorded EIT image can be constructed in future.
[electrical impedance change measurement, conductors (electric), Object Detection, electric impedance imaging, saline, electrical transfer impedance, object detection, absolute impedance variation calculation, absolute impedance variation plotting, Electrodes, phantoms, noninvasive method, Impedance measurement, conductor dielectric properties, tomography, EIT image recording, Tomography, symmetrical electrical impedance distribution, spatial resolution, electrical impedance measurement, image resolution, medical image processing, Fe, glass, impedance profile, Conductivity, Conductors, image reconstruction, Electrical Impedance, cylindrical tank, phantom study, EIT, surface electrode, biological tissue dielectric properties, uniform volume conductor impedance, Object Localization, object localization, electrical impedance tomography, glass sphere position, bioelectric phenomena, iron, insulator dielectric properties, impedance image reconstruction, Impedance, biomedical electrodes, maximum absolute impedance variation value, iron rod position, Volume Conductor]
Detection of heart condition by time and frequency based template
2014 17th International Conference on Computer and Information Technology
None
2014
The phonocardiogram (PCG) is an easy and costless yet powerful tool to detect the heart condition. While the cardiovascular disease is an increasing global threat to humanity, there is a decrease in doctors' capability for diagnosing these diseases by auscultation. In this work, a neural network based automated system is proposed to aid early detection of these diseases. Two template-based feature representations are developed to effectively represent the characteristics of heart sound. These features, extracted from sound files of known cases, are then used to train a neural network. Our experimental results corroborate that the proposed method can efficiently detect the heart condition with good overall classification accuracy. In addition, a graphical user interface and a low cost device is developed which is user-friendly and can be used without much relevant knowledge.
[Heart, Frequency Domain Template, user-friendly system, Frequency-domain analysis, graphical user interfaces, frequency-based template, Time Domain Template, Valves, classification accuracy, Time-domain analysis, time-based template, electrocardiography, sound files, feature extraction, phonocardiography, learning (artificial intelligence), cardiovascular disease, disease detection, neural network based automated system, low cost device, graphical user interface, Classification of Heart diseases, signal representation, Diseases, signal classification, phonocardiogram, heart sound characteristics, medical signal processing, Neural Network, PCG, Feature extraction, human computer interaction, neural network training, template-based feature representation, Phonocardiography, neural nets, heart condition detection]
A countermeasure for compromising electromagnetic emanations of wired keyboards
2014 17th International Conference on Computer and Information Technology
None
2014
Confidential data are often transmitted by computer keyboards. Electromagnetic waves are emitted as they contain electronic components. Very sensitive information like keystrokes can be revealed by these emanations. Most of the modern computer keyboards generate compromising emanations. Best practical attack can fully recover up to 95% of the keystrokes of a PS/2 keyboard at a distance up to 20 meters, even through walls. Some counter measures have been indicated in some previous work. But, those are not cost effective and convenient. So, we propose a model of a device which can be cost effective and convenient. It can jam the emanation. Compromising emanations before and after jamming are also shown and compared.
[Computers, electronic components, signal detection, Image edge detection, keyboards, keystrokes, Electromagnetic Emanation, jamming, Receivers, Keyboard, computer keyboards, Jamming, Electromagnetics, electromagnetic waves, PS/2 keyboard, electromagnetic emanations, Signal Jammer, electromagnetic interference, Keyboards, Keystroke Detection, data privacy, confidential data, wired keyboards, Clocks]
Efficient de Bruijn graph construction for genome assembly using a hash table and auxiliary vector data structures
2014 17th International Conference on Computer and Information Technology
None
2014
Modern next-generation sequencing technologies can generate huge volumes of data. One popular and useful tool to analyze these huge amount of data is the so called de Bruijn graph. Because of the huge number of nodes, in de Bruijn Graph based genome assembly the main barrier is the memory and runtime. And, this area has been the focus of significant attention in the contemporary literature. We present an algorithm that makes a balance between memory and runtime. Our approach stores the de Bruijn graph in a hash table with an auxiliary data structure which improves the total memory usage and runtime with no false positives. In the whole assembly process, generally the graph construction procedure takes the major share of the time. Our approach presents significant advancement in this aspect. All the data files (in FASTA format) along with the program code are available for downloaded at the following link: https://drive.google.com/folderview?id=0B3D-hZtRZ933V1dMOVBHUkNJM00&amp;usp=sharing.
[graph theory, Genomics, data files, storage management, Runtime, biology computing, Computer Science, genomics, data structures, auxiliary vector data structures, Bioinformatics, Vector, Assembly, hash table, Hashtable, Data structures, Vectors, memory usage, Indexes, program code, genome assembly, FASTA format, de Bruijn graph, next-generation sequencing technologies, de Bruijn graph construction procedure]
Design of adaptive electrical energy management systems for cost minimization
2014 17th International Conference on Computer and Information Technology
None
2014
The objective of this paper is to model the power consumption trends of industrial or residential facilities, and design smart energy management mechanisms in order to reduce the energy cost. For a given set of components, and with minimal impact on the subjective human intervention, a substantial reduction of the energy cost can be achieved through smart energy management schemes that react to real-time power consumption patterns of the elements, instantaneous energy costs, and the availability of renewable energy. A complex system containing electrical elements with a wide array of functionalities (e.g. pre-loading, post-loading, time-flexible load distribution) and variables (physical factors, user intervention, variable energy prices) requires a rigorous formulation of the energy management scheme. The optimum combination of the time-balancing of loads is derived, and an adaptive energy management scheme is designed that can both sense an imminent change in the trend, and adjust the loading mechanism accordingly.
[cost reduction, load time-balancing, Power demand, adaptive electrical energy management system design, residential facilities, industrial facilities, dynamic programming, power consumption trends, instantaneous energy costs, electrical elements, complex system, Adaptive control, power consumption, renewable energy, energy cost reduction, energy management systems, Loading, Market research, Load management, energy management, smart energy management mechanisms, cost minimization, Erbium, Load modeling]
Outage capacity analysis of a secondary network in interference limited cognitive radio spectrum sharing system
2014 17th International Conference on Computer and Information Technology
None
2014
In this paper, we investigate the outage capacity of a secondary network in a cognitive radio (CR) cooperative spectrum sharing system consisting of a primary transmitter (PTx)-receiver (PRx) pair as well as multiple secondary transmitter (STx)-receiver (SRx) pairs. We evaluate the outage capacity of the secondary network in slow fading channel by considering three different scenarios based on the primary user (PU) activity. The scenarios are: spectrum access by a secondary user (SU) when the data rate of the PTx to PRx over the direct link achieves primary request target rate R<sub>PT</sub>, spectrum access by a SU when the data rate of the PTx to PRx falls below R<sub>PT</sub> over the direct link but achieves R<sub>PT</sub> on the relaying link (PTx-relay-PRx) and spectrum access by two SUs at the same time when PTx-PRx pair is idle or primary system is in outage. In this paper, a closed-form expression of the outage capacity is derived. Theoretical results show that outage capacity improves when the active secondary transmitter is located farther away from the PRx.
[outage capacity analysis, multiple secondary transmitter-receiver pairs, Closed-form solutions, fading channel, Radio transmitters, outage capacity, Interference, Rayleigh channels, secondary network, radiofrequency interference, relaying link, primary user activity, cognitive radio, fading channels, secondary network in interference, cognitive radio spectrum sharing system, primary transmitter-receiver pair, Silicon, Capacity planning, spectrum sharing]
Real-time computer vision-based Bengali Sign Language recognition
2014 17th International Conference on Computer and Information Technology
None
2014
This paper presents a real-time computer vision-based Bengali Sign Language (BdSL) recognition system. The system detects the probable hand from the captured image. The system uses Haar-like feature-based cascaded classifiers to detect the hand in each frame. From the detected hand area, the system extracts the hand sign based on Hue and Saturation value corresponding to human skin color. After normalization the system converts the hand sign to binary image. Then the binary images are classified by comparing with pre-trained binary images of hand sign using K-Nearest Neighbors (KNN) Classifier. The system is able to recognize 6 Bengali Vowels and 30 Bengali Consonants. The system is trained using 3600 (36&#x00D7;10&#x00D7;10) training images where each of 10 signers performed 10 signs for each corresponding Bengali alphabet and the system is tested using another 3600 (36&#x00D7;10&#x00D7;10) images of 10 signers. The system is achieved recognition accuracy of 98.17% for Vowels and 94.75% for Consonants.
[real-time computer vision-based Bengali sign language recognition system, KNN classifier, image classification, Skin color segmentation, BdSL recognition system, Training, Bengali vowels, Accuracy, Image color analysis, K-Nearest Neighbors (KNN) algorithm, pretrained binary images, Haar-like feature-based cascaded classifiers, hue and saturation value, image colour analysis, sign language recognition, k-nearest neighbor classifier, Bengali Sign Language Recognition, Assistive technology, natural language processing, Bengali consonants, Gesture recognition, image capture, Bengali alphabet, Hand detection, probable hand detection, computer vision, binary image classification, Haar transforms, Feature extraction, Skin, human skin color, recognition accuracy]
Sentiment detection from Bangla text using contextual valency analysis
2014 17th International Conference on Computer and Information Technology
None
2014
Sentiment Analysis or opinion mining is an area of important research over the last decade. The basic task in sentiment analysis is classifying the polarity of a given text whether the expressed opinion in the text is positive, negative, or neutral. This paper presents an approach to sentiment assessment from Bangla text using contextual valence analysis. In linguistics valence of a verb is the number of satellite noun phrases with which a verb combines. We have used the WorldNet to get the senses of each word according to its parts of speech and SentiWordNet to get the prior valence (i.e. polarity) of each word. We calculate the total positivity, negativity and neutrality of sentence or document with respect to total sense. We developed our own methodology to calculate the sentiment from Bangla text using valency analysis. Sufficient examples and experiments are presented to describe the methodology.
[Computers, opinion mining, Sentiment analysis, text analysis, data mining, linguistics valence, polarity classification, WorldNet, Bangla text, sentiment assessment, SentiWordNet, pattern classification, sentence negativity, natural language processing, sentence neutrality, Blogs, sentiment analysis, sentence positivity, Information technology, Pragmatics, XML, satellite noun phrases, Machine learning, contextual valence analysis, Sentiment detection, Speech, Bangla verb, Valency analysis]
Hybrid single electron transistor based low power consuming 4-bit parallel adder/subtractor circuit in 65 nanometer technology
2014 17th International Conference on Computer and Information Technology
None
2014
Hybridization between CMOS logic and single electron transistor has already revolutionized our present nano technological aspects. Ultra low power consumption as well as ultra dense circuit formation is now possible with the help of mutual integration between the two mentioned above. These benefits have drawn the attraction of the future researchers in this hybrid SET-CMOS technology for future nano-scale low power VLSI design. In this paper, we have designed a room temperature operable 4-bit adder/subtractor circuit in hybrid SET-CMOS logic with considerably low power consumption. Power-delay product has also been calculated numerically and graphically. XOR gates are used as controlled inverter for the selection of add or subtract operation. All the simulations are performed in Tanner environment and for the simulation purpose two separate model files are used. MIB model for SET operation and BSIM4.6.1 for the operation of PMOS. It is notable that the hybrid structure provides far better performance in respect to the conventional MOSFET structure.
[parallel subtractor circuit, size 65 nm, XOR gate, low power consumption, very large scale integration, Single Electron Transistor, SET-CMOS technology, power consumption, BSIM4.6.1, 4-bit parallel adder/subtractor circuit, complementary metal oxide semiconductor, PMOS operation, adders, CMOS logic circuits, ultradense circuit formation, nanotechnology, mutual integration, MIB model, CMOS integrated circuits, Junctions, Adders, controlled inverter, Power demand, VLSI, MIB, Hybrid SET-CMOS, hybrid single electron transistor, power-delay product, single electron transistors, logic gates, Tanner environment, nanoscale low power VLSI design, word length 4 bit, Logic gates, parallel adder circuit, Integrated circuit modeling, CMOS logic, MOSFET structure, Single electron transistors]
Screen reading with Bangla &amp; English audio assistance bi-lingual supported software &#x2018;Mongol Dip&#x2019; for visually impaired people
2014 17th International Conference on Computer and Information Technology
None
2014
In the modern science and information technology everyone wants to attach with latest technology and services. Although visually impaired people do not get proper light from the benefits of technology revolution due to lack of scope. Mongol Dip will give visually impaired population a great opportunity to connect with and contribute to the digital world. In this paper we present software named `Mongol Dip' that helps people with visual impairments to operate windows operating system for computational tasks. In this software we have tracked user activity by using core windows DLL (Dynamic-link Library) hooking and used text to speech technology to echo every operation done by the user. Mongol Dip provides the easiest interface that helps the visually impaired people to read out the contents of the document in both the Bengali and English language by using Microsoft TTS and Bengali Text to Speech application &#x201C;Subachan&#x201D;. Hence the main goal is to assist the people with visual impairments to work with the computers like a normal human being by our software Mongol Dip.
[Computers, Visualization, handicapped aids, English language, information technology, DLL, digital world, dynamic-link library, user interfaces, Bengali language, Postal services, visual impairments, Microsoft TTS, text to speech technology, Libraries, windows operating system, visually impaired people, speech synthesis, natural language processing, core windows, Subachan, Browsers, Visual impairments, Information technology, Mongol Dip, user activity, operating systems (computers), Bengali text to speech application, Software]
Graceful labeling of trees: Methods and applications
2014 17th International Conference on Computer and Information Technology
None
2014
Any tree with n vertices is conjectured to be graceful if its vertices can be labeled using integers 0, 1, ..., n - 1 such that each vertex label as well as the corresponding edge label is distinct throughout the tree. There has been multiple attempts with different approaches to prove this conjecture but it remains the same. Here we will discuss the methods used to solve this problem along with two new classes of graceful tree and some applications of it.
[Computers, tree vertices, trees (mathematics), network theory (graphs), vertex label, Linear programming, Electronic mail, integer labeling, edge label, Information technology, Computer science, graceful tree labeling, Databases, Labeling]
Towards empirical study based mathematical modeling for energy consumption and end-to-end delay of MANETs
2014 17th International Conference on Computer and Information Technology
None
2014
Mathematical modeling for energy consumption of MANETs considering the impact of different layers in the protocol stack in addition to that of different network parameters remains unexplored till now even though such modeling is considered as the fastest and the most cost-effective tool for evaluating the performance of a network. Therefore, in this paper, we attempt to develop a mathematical model for energy consumption of MANETs considering both of the aspects. In addition, we also focus on developing mathematical models for end-to-end delay, this metric limits the maximum throughput of a network. In our analysis, we perform rigorous simulation utilizing ns-2 to capture the performance of MANETs under diversified settings. Our rigorous empirical study reveals that we need to develop cross-layer mathematical models for energy consumption and delay to represent the performance of MANETs and such mathematical models need to resolve higher-order polynomial equations. Consequently, our study uncovers a key finding that mathematical modeling of MANETs considering variations in all parameters is not feasible.
[Measurement, network parameters, Energy consumption, higher order polynomial equation, delay estimation, polynomials, telecommunication power management, network performance evaluation, NS-2 simulation, Ad hoc networks, Equations, end-to-end delay, MANET, mobile ad hoc networks, mathematical modeling, Market research, Mathematical model, energy consumption, Mobile computing]
Detection of semantic errors from simple Bangla sentences
2014 17th International Conference on Computer and Information Technology
None
2014
We describe a methodology to detect semantic errors from Bangla sentences. According to Bangla grammar, a single verb can have many forms depending on its tense and person of its subject. The subject of a sentence can be noun or pronoun, may indicate human, animal, or any non-living entity. There is a fixed semantic relation between every verb and subject and object of a sentence. For example, a non-living entity can never feel hungry but living entity feels. This semantic difference checking for its correctness in a language is very important for the purpose of machine learning study and intelligent agent development for human computer interaction. Semantic error detection for Bangla language is an important research problem because of the variety that Bangla language offers in its grammatical, structural and semantic diversity. In this paper, we have established the relationship between subject and verb as well as object and verb of Bangla sentence. Hence we have proposed an algorithm for semantic correctness of simple Bangle sentences. The algorithm can easily be extended for other forms such as complex and compound sentences.
[Computers, Algorithm design and analysis, Bangla Language Processing, Conferences, natural language processing, Bangla grammar, Semantic analysis, noun, Bangla Simple structure, Grammar, Compounds, Information technology, machine learning study, semantic difference checking, Semantics, pronoun, Bangla Grammer, Bangla verb, human computer interaction, Bangla language, learning (artificial intelligence), Bangla sentences, intelligent agent development, semantic errors detect]
Application of dominance-based Rough Set theory for knowledge discovery in cooperative learning
2014 17th International Conference on Computer and Information Technology
None
2014
This paper illustrates a rule induction approach to discover decision patterns by using Dominance-based Rough Set Approach (DRSA) to unravel the intrinsic factors for learning successes and failures in a cooperative group-learning environment. It employs an innovative group quiz method in a university Management Information Systems (MIS) course which induces students into a unique learning process. The survey results enumerate the conditioning factors in terms of multiple criteria ranked in ordinal scale or preference-ordered. The DRSA model is uniquely appropriate when data includes multiple criteria whose domains are preference-ordered. The assessment of the learning success shows how a preference-centric group decision model can be integrated with the dominance-based principle of rough set to derive minimal covering &#x201C;if.., then..&#x201D; decision rules. The paper demonstrates that it is possible to develop a formal approximate reasoning scheme to develop group dynamics and discover inherent patterns of the heuristics for group decisions.
[Computers, MIS course, formal approximate reasoning scheme, Psychology, data mining, cooperative learning, knowledge discovery, Approximation methods, dominance-based rough set theory, Information systems, multiple criteria, innovative group quiz method, preference-ordered domains, Education, DRSA model, learning failures, cooperative group-learning environment, management information systems, inference mechanisms, dominance-based principle, Information technology, university management information systems course, preference-centric group decision model, educational courses, Set theory, multi-criteria sorting, rough set theory, learning success assessment, management education]
User satisfaction with e-government websites: An Australian experience
2014 17th International Conference on Computer and Information Technology
None
2014
Many nations worldwide are continuing to invest in electronic government (e-government) initiatives that now demand an investigation into how well users perceive the success of these initiatives. Currently, there exists a dearth of empirical evaluation of e-government success in particular for the Australian context. Our research project, reported in this paper, is concerned with developing a conceptual model on user satisfaction with e-government websites and reports and initial empirical evaluation of the model with regard to the e-government website developed by the Australian Department of Immigration and Citizenship. The findings suggest that user satisfaction incorporates four factors and is positively related to users' overall trust in e-government and negatively related to their anxiety towards using e-government website. These findings have implications for theory and practice alike.
[Context, Computers, Correlation, e-government website, electronic government initiatives, Information technology, Australian Department of Immigration and Citizenship, E-governmen, e-government success, e-government Web sites, Australia, Web sites, government data processing, Electronic government, user satisfaction]
Effect of signal strength on different parameters of cross-correlation function in underwater network cardinality estimation
2014 17th International Conference on Computer and Information Technology
None
2014
Cardinality estimation of underwater network is somewhat troublesome using terrestrial node estimation methods due to unique characteristics of underwater environment such as large propagation latency, node mobility, non-negligible capture effect, and high error rate. For this reason, a cardinality estimation method based on cross-correlation of Gaussian signals is proposed for underwater network, in which different estimation parameters can be formulated from the cross-correlation function (CCF). Effect of signal strength on these estimation parameters is analyzed in this paper in order to find out the suitable estimation parameter for cross-correlation based technique.
[Computers, cross-correlation function, terrestrial node estimation method, Protocols, propagation latency, Gaussian signals, Conferences, marine communication, Estimation, underwater network, cardinality estimation, estimation parameter, Information technology, effect of signal strength, Standards, node mobility, signal strength effect, nonnegligible capture effect, Cross-correlation function, Gaussian processes, high error rate, parameter estimation, underwater network cardinality estimation, Sensors]
Change detection-aided single linear prediction of multi-temporal satellite images
2014 17th International Conference on Computer and Information Technology
None
2014
Significant temporal correlation of multi-temporal remote sensed images presents an opportunity to use historical images for predicting the recent image. This paper investigates the possibilities of predicting the current image from a reference previous image using linear regression after change detection. Single linear regression model for temporal prediction is usually used for the images that are mainly affected by system or environmental noise. When the images do experience small portions of real changes, a PCA based outlier removal technique for generating a prediction model to best fit the majority of unchanged data is investigated. The PCA based change detection method first excludes the changed pixels and then the prediction method is applied on rest of the pixels. The potential of this sequential data compression is dependent mainly on multi-temporal image analysis for prediction or forecasting purposes. The transmission load can be substantially reduced by properly exploiting the temporal correlation. But if the amount of real land-cover change is not limited then the degraded temporal dependency can affect the model parameters of the prediction. The proposed change detection-aided prediction method will substantially improve the accuracy as the changed pixels have been removed.
[Correlation, multitemporal image analysis, regression analysis, PCA based outlier removal technique, Predictive models, environmental noise, Remote sensing, historical images, change detection-aided single linear prediction method, land cover, data compression, sequential data compression, degraded temporal dependency, Linear regression, single linear regression model, land-cover change, temporal correlation, geophysical image processing, remote sensing, multitemporal remote sensed images, PCA based change detection method, PCA, multitemporal satellite images, multi-temporal, Satellites, artificial satellites, regression, change detection, Data models, principal component analysis, Principal component analysis]
Design of a novel dual-band microstrip patch antenna operating at 2.45 GHz and 2.84 GHz with practical implementation
16th Int'l Conf. Computer and Information Technology
None
2014
This paper presents the design and analytical studies of a novel dual-band microstrip patch antenna operating at two frequencies of ISM bands: 2.45 GHz and 2.84 GHz. The necessity for multiband antennas is increasing with the demand of wireless communications and as a result, light-weight, low-volume and low-profile microstrip patch antennas have been nourished by the researchers which initiated our design of this proposed antenna. This antenna is characterized well with its parameters i.e. Reflection coefficient (below -10 decibels), voltage standing wave ratio (VSWR) (about 1.5), forward gain (above 8.07 decibel with compared to isotropic antenna), radiation pattern etc. which are shown in this paper. The antenna was designed in Numerical Electromagnetic Code (4nec2) design environment and simulated for the antenna parameters. Then it was practically implemented and tested with WATS-2002 (wave and antenna training system). The simulated and tested results show that this antenna is eligible for short range communication like Bluetooth, WLAN, Wi-Fi and so on. This antenna is highly directive, matched with 50 ohms feeding line impedance and has uniform radiation over all the directions, but has a low bandwidth for each of the two bands which can be improved with a modification in the structure and the frequency bands can be changed by adding some parasitic elements.
[wave and antenna training system, Bluetooth, dual-band microstrip patch antenna, numerical electromagnetic code, Microstrip, Broadband antennas, frequency 2.84 GHz, frequency 2.45 GHz, Wireless communication, reflection coefficient, ISM bands, Microstrip patch, antenna radiation patterns, Microstrip antennas, UHF antennas, multiband antennas, Antenna radiation patterns, forward gain, WATS, voltage standing wave ratio, isotropic antenna, ISM band, Wi-Fi, WLAN, microstrip antennas, VSWR, Patch antennas, radiation pattern, multifrequency antennas, multiband antenna]
Performance of Turbo Coded Vertical Bell Laboratories Layered Space Time Multiple Input Multiple Output system
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper, performance of a Turbo Code (TC) and Vertical Bell Laboratories Layered Space Time (VBLAST) encoded wireless link is evaluated in the presence of Rayleigh fading for Multiple-Input Multiple-Output (MIMO) system. Turbo encoded bits are modulated using QPSK or 16 QAM or 64 QAM modulator and modulated data are further encoded using VBLAST and then split into n streams which are simultaneously transmitted through n transmit antennas. Simulation results obtained show that coded MIMO system provides 17 to 25 dB coding gain for different combination of transmit and receive antennas at a BER of 10-3 compared to uncoded MIMO system. And there is around 2 to 8 dB gain for increasing number of transmitting antennas from 2 to 3 or 3 to 4. On the other hand, capacities for (2 T<sub>x</sub>, 2 R<sub>x</sub>), (3 T<sub>x</sub>, 3 R<sub>x</sub>) and (4 T<sub>x</sub>, 4 R<sub>x</sub>) are almost same from 0 dB to 10 dB but change significantly with increasing SNR from 10 dB.
[MIMO system, VBLSAT, layered space time encoded wireless link, VBLAST, transmitting antennas, Transmitting antennas, Rayleigh channels, quadrature amplitude modulation, multiple-input multiple-output system, quadrature phase shift keying, layered space time multiple input multiple output system, Wireless communication, turbo codes, QPSK, Slot antennas, Receiving antennas, 16-QAM modulator, Turbo codes, 64 QAM modulator, MIMO, Turbo Code, Rayleigh fading, turbo coded Vertical Bell Laboratories, MIMO communication]
Design and performance analysis of a 4-element planar inverted-F antenna (PIFA) array including human interaction
16th Int'l Conf. Computer and Information Technology
None
2014
A 4-elements PIFA array antenna is designed for the laptop Wi-Fi application. The proposed array antenna elements are optimized for 2.45 GHz. It is a wide band antenna. It is embedded on the back side of the laptop screen. These array elements are excited simultaneously. The performance of this antenna is analyzed and the interaction between this antenna array and user (phantom) of the laptop is studied. The VSWR and the return loss values are excellent. The SAR value lies within the acceptable limit of Federal Communications Commission (FCC). All the simulation results are performed using CST Microwave Studio 2011.
[VSWR value, Federal Communications Commission, Portable computers, Ports (Computers), return loss values, design engineering, frequency 2.45 GHz, human interaction, antenna arrays, UHF antennas, 4-element planar inverted-F antenna array, MIMO communication, Antenna radiation patterns, planar inverted-F antennas, MIMO antenna, CST Microwave Studio, Specific Absorption Rate (SAR), Patch antennas, SAR value, SAM Phantom, 4-elements PIFA array antenna, specific absorption rate, laptop screen, screens (display), Arrays, MIMO Antenna, Antenna arrays, CST Microwave Studio 2011]
Monitoring the performance of computer user by analyzing physiological signals
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper, we have analyzed the physical and mental performance of computer user due to the long term use of computer by analyzing the variations in physiological signals. Performances of computer user are monitored by recording and analyzing the variations in Electrocardiogram (ECG), Electromyogram (EMG), Electrooculogram (EOG), and Electroencephalogram (EEG). Detecting the performance of computer user throughout the computing session will enable more development of computer where it can interacts with its user taking users effective state in to account; known as affective computing. This research work has analyzed the variations in physiological parameters of a computer user which indicates the physical and mental condition of the user. All the signals are taken by using BIOPAC system and analyzed by using BIOPAC AcqKnowledge software and MATLAB. The different data sets of 12 subjects were collected where all the subjects were undergo a specific session of computer using which includes different computer mediated task. Research shows that there are significant variations in physiological signals throughout the session. This variation indicates differences in user physical and mental performance and can be used for affective computing purpose.
[Computers, Electrooculography, electro-oculography, BIOPAC, affective computing, Mental state, Electroencephalography, MATLAB, electromyogram, electrocardiography, electrocardiogram, Acqknowledge software, Electrocardiography, Electromyography, EOG, EMG, physiological signal, EE, electroencephalography, Smoothing methods, electrooculogram, EEG, ECG, computer user, BIOPAC system, electroencephalogram, medical signal processing, Affective computing, BIOPAC AcqKnowledge software, human computer interaction, Biomedical monitoring]
A novel audio watermarking algorithm based on neighboring samples
16th Int'l Conf. Computer and Information Technology
None
2014
A simple and effective audio watermarking algorithm based in time domain is proposed in this paper for controlling the unauthorized distribution of copyright audio. The algorithm makes use of the advantages of both embedding a watermark bit to an individual sample and embedding a bit to a group or frame of samples. Here, a frame of three positive valued samples is used to define the amount of distortion to be added but only the middle sample is distorted based on the decision. Based on the watermark bit either &#x201C;0&#x201D; or &#x201C;1&#x201D;, the middle sample of a frame is made equal to one of the neighboring samples. Thus, the watermarked audio has a very good perceptual quality. Experimental results show that our new audio watermarking algorithm is not only imperceptible, but also robust against various common signal processing attacks such as amplitude modification, re-sampling, low-pass filtering, re-quantization, MP3 compression, etc. Though proposed in time domain but the algorithm can be implemented in any transform domains with little or no modification.
[MP3 compression, music industries, copyright, time domain, Frame, Neighboring samples, Watermarking, Transforms, transforms, Distortion, Imperceptible, Quantization (signal), music, transform domains, quantisation (signal), signal sampling, Robustness, low-pass filters, low-pass filtering, neighboring samples, data compression, amplitude modification, Middle sample, filtering theory, requantization, Information technology, common signal processing attacks, resampling, audio watermarking, Signal processing algorithms, Cepstrum, unauthorized copyright audio distribution, audio watermarking algorithm, perceptual quality]
Mobile phone SIM card: A security concern in the perspective of Bangladesh
16th Int'l Conf. Computer and Information Technology
None
2014
At present time the unregistered SIM cards (here unregistered means the SIM cards which are registered with false information provided by dealer) are widely used for committing crimes in the perspective of Bangladesh. Due to the complexity of the present SIM card registration process, the subscribers are reluctant to register themselves. In this paper we propose a cloud based system model which demonstrates an online method of SIM card purchasing and registration. This system model conquers the constraints of the conventional system of mobile purchasing and registration which is lengthy and stalking. It enables the subscriber to buy the SIM card and to be registered in an uncomplicated way. Here we mainly emphasize on the security issue through mandatory mobile SIM card registration. The purpose of our system model is lessening the rate of crimes which can be taken place by the use of mobile. We also show that how this system will encourage the people to be a registered user and at the same time discourage the criminals to commit crime with the use of mobile call and SMS.
[Computers, telecommunication security, cloud based system model, mobile purchasing, online SIM card registration, smart cards, Mobile communication, Mobile handsets, Security, Databases, criminals, crime rate, cloud computing, security concern, mobile phone SIM card, central database, mandatory mobile SIM card registration, mobile radio, Government, subscribers, PIN code, mobile registration, mobile call, Bangladesh, unregistered SIM cards, security of data, Cellular phones, Authentication, SIM, SMS, online SIM card purchasing, security issue, mobile handsets]
Adoption of e-banking in Bangladesh: Evolution, status and prospects
16th Int'l Conf. Computer and Information Technology
None
2014
Electronic banking (e-banking) is the next-generation banking that electronically delivers a wide range of banking services where the physical presence of the customer at the bank premises is not required. With the advent of Internet connectivity and a number of security tools of e-commerce, the penetration of e-banking is increasing most of the developing countries, e.g., Bangladesh. Although there are different forms of e-banking, including Internet banking, telephone banking, mobile phone or Short Message Service (SMS) banking and Automatic Teller Machine (ATM) banking; Internet banking is the most cost-effective form of e-banking. The number of Internet users is geometrically increasing in Bangladesh. Moreover, the banks have started to invest in e-banking, specially in Internet banking. Although, Internet banking has immense potential in Bangladesh, it fails to attract the customers in general. In this paper, by carrying out an on-line survey, we study the existing e-banking facilities of Bangladesh, the use of this e-banking services and the challenges that need to be addressed for full-scale deployment of e-banking. By analyzing survey data we have identified the key reasons behind the slow spread out of e-banking services, and also prepared a number of recommendations to overcome the present hindrances.
[Computers, Online banking, data analysis, SMS banking, electronic banking, e-banking services, e-banking adoption, Banking, Servers, Security, short message service, Information technology, survey data analysis, ATM banking, telephone banking, E-banking, Bangladesh, mobile phone banking, automatic teller machine, customer satisfaction, e-commerce, Internet, Internet banking, bank data processing]
Experimental framework for mel-scaled LP based Bangla speech recognition
16th Int'l Conf. Computer and Information Technology
None
2014
This paper deals with the recognition process of Bangla speech. The used database consists of two sets of data - one is for training containing 3824 utterances of Bangla digit sequences of 25 male and 25 female speakers and the other one is test dataset containing 1985 utterances of 26 male and 26 female speakers. The test set is subdivided into four groups such as clean1, clean2, clean3 and clean4. Mel-LPC based front-end has been used to design the front-end, since it incorporate auditory-like frequency resolution. The Mel-LPC is a time-domain feature and computationally efficient. The Mel-LPC based cepstral coefficients are obtained directly from the input speech by using generalized autocorrelation function. In this estimation process bilinear transformation is not required, and frequency warping is obtained by using a first-order all-pass filter instead of unit delay. A detail experimental framework both for front-end and HMM based back-end have been presented in this paper. The final recognition experiments show the satisfactory performance of the developed system. The recognition accuracy are found to be 98.11%, 98.05%, 97.94%, and 97.63%, for test sets clean1, clean2, clean3 and clean4, respectively.
[first-order all-pass filter, Mel-LPC, Mel-scaled LP based Bangla speech recognition, generalized autocorrelation function, Mel-LPC based cepstral coefficients, Accuracy, Databases, speech recognition, speech coding, Bangla speech recognition, Bangla digit sequences, speaker utterance, bilinear transformation, Mel-LPC based front-end, auditory-like frequency resolution, Computational modeling, filtering theory, HMM, Bilinear transformation, Information technology, Hidden Markov models, Speech recognition, linear prediction coding, Speech, speaker recognition]
The beneficial effect of filtering in current controlled inverter
16th Int'l Conf. Computer and Information Technology
None
2014
An approach of harmonics component analysis, portraying the presence of certain amount of harmonics at the inverter output terminal, has been indicated in this paper. Some unwanted impacts are caused by the harmonics such as unbalance and excessive neutral currents, interference in nearby communication networks and disturbance to other consumer, torque pulsations in electric motor drives are and so on. Thus, the minimization of the harmonics contained in the output of a single phase current controlled inverter undergoing hysteresis modulation technique is important so as to get rid of these detrimental effects. With a view to reducing the assumed 3rd order harmonics, an LC low pass filter is used in this paper that blocks the harmonics and undeniably passes almost a sinusoidal output at the output terminal. It has been found from the simulation that the Total Harmonic Distortion (THD) is just above 33% during the existence of up to 3rd order harmonics, although it is predicted to be 0% in ideal case, and more importantly, that can be slumped to 0.00003% by the application of an LC low pass filter.
[Frequency modulation, Frequency-domain analysis, LC low pass Filter, power harmonic filters, Harmonic analysis, Inverters, Time-domain analysis, Harmonics Impact, Total Harmonic Distortion (THD), hysteresis modulation technique, THD, invertors, IGBT, harmonics minimization, low-pass filters, total harmonic distortion, LC low pass filter, Hysteresis, harmonics component analysis, single phase current controlled inverter, Single Phase Inverter, Fast Fourier Transform (FFT), harmonic distortion, Power harmonic filters, inverter output terminal, Hysteresis Modulation]
Effect of Neural Network based phonetic feature segmentation in ASR
16th Int'l Conf. Computer and Information Technology
None
2014
This paper describes a system for phone segmentation using phonetic features, where context information influences the performance of Automatic Speech Recognition (ASR). Current Hidden Markov Model (HMM) based ASR systems have solved this problem by using context-sensitive triphone models. However, these models need a large number of speech parameters and a large volume of speech corpus. In this paper, we propose a technique to model a dynamic process of co-articulation and embed it to ASR systems. Recurrent Neural Network (RNN) is expected to realize this dynamic process. But main problem is the slowness of RNN for training the network of large size. We introduce Distinctive Phonetic Feature (DPF) based feature extraction using a two-stage system consists of a Multi-Layer Neural Network (MLN) in the first stage and another MLN with longer context window in the second stage where the first MLN is expected to reduce the dynamics of acoustic feature pattern and the second MLN to suppress the fluctuation caused by DPF context. The experiments are carried out using Japanese triphthong and Japanese Newspaper Article Sentences (JNAS) data. The proposed DPF based feature extractor provides better segmentation performance with a reduced mixture-set of HMMs. Better context effect is achieved with less computation using MLN instead of RNN.
[hidden Markov model, speech corpus, fluctuation suppression, automatic speech recognition, multilayer neural network, Mel frequency cepstral coefficient, MLN, DPF context, multi-layer neural networ, hidden Markov models, speech recognition, RNN, speech parameter, acoustic feature pattern, feature extraction, speech processing, Context, multilayer perceptrons, local features, distinctive phonetic feature, recurrent neural network, natural language processing, recurrent neural nets, mixture-set, Vectors, DPF based feature extractor, neural network based phonetic feature segmentation, phone segmentation, context-sensitive triphone model, DPF based feature extraction, Japanese triphthong, Japanese newspaper article sentences, segmentation performance, context window, Hidden Markov models, HMM based ASR system, JNAS data, Feature extraction, Speech]
A framework for Bangla text to speech synthesis
16th Int'l Conf. Computer and Information Technology
None
2014
We describe a basic framework and methodology to convert Bangla Text to Speech. Articulated words are automatically produced from Bangla input text by the methodology from the basic pronunciation of the Bangla words. The single tone syllables are considered as the fundamental units for analysis. The methodology selects phonetic units from uttered vocabulary and then combined the appropriate diphones to get the final output. The uttered syllables are selected on the basis of synthesized input text. The input text is analyzed according to the very basic grammar rules of Bangla pronunciation. Using the methodology, we developed a prototype system and the output generated from the system has been analyzed with the natural human voice.
[Computers, text analysis, Bangla text to speech synthesis, natural human voice, Speech synthesis, single tone syllables, uttered vocabulary, Bangla input text, Bangla Text to Speech, speech synthesis, natural language processing, Bangla pronunciation, Text Normalization, Educational institutions, Bangla word pronunciation, articulated words, grammar rules, Grammar, Information technology, diphone preparation Syllable Parser, Bangla text to speech conversion, phonetic unit selection, Speech, input text analysis, diphone]
A Wideband 6.9&#x2013;17.7 GHz CMOS low-noise amplifier with a gain flattening technique
16th Int'l Conf. Computer and Information Technology
None
2014
A flat-gain design of an ultra wide-band CMOS amplifier is proposed and simulated in 90 nm CMOS process. Inductive degeneration is applied to reduce noise figure without significantly raising the architecture's power requirement. Additionally, a resistive shunt feedback technique is applied with an RL peaking load to flatten the gain throughout design band. This topology allows the amplifier to have a very large bandwidth of 6.9-17.7 GHz with a low minimum noise figure of 2.26 dB (at 10 GHz). NF also remains below 5.2 dB across the 10.8 GHz bandwidth. The design exhibits gain which peaks to 11.1 dB and has a low power demand of 15.7 mW (from a 1.2 V supply). Comparing the circuit performance to previously published amplifiers shows that it achieves reduction in NF and power dissipation while maintaining a flatter gain in X-K band.
[Noise figure, size 90 nm, noise figure 2.26 dB, flat-gain design, voltage 1.2 V, CMOS process, RL peaking load, Inductors, ultra wide-band CMOS amplifier, integrated circuit design, CMOS analogue integrated circuits, CMOS integrated circuits, power dissipation, circuit performance, resistive shunt feedback technique, power 15.7 mW, noise figure, microwave amplifiers, gain flattening technique, bandwidth 6.9 GHz to 17.7 GHz, wideband amplifiers, power requirement, low noise amplifiers, Shunts (electrical), X-K band, inductive degeneration, CMOS amplifier, wideband CMOS low-noise amplifier, gain throughout design band, Flat-gain, Gain, Wideband, Resistive feedback]
A robust RGB channel based image steganography technique using a secret key
16th Int'l Conf. Computer and Information Technology
None
2014
The paper proposes a RGB channel based steganographic technique for images imparting better information security. This technique inserts the information into deeper layers of the selected RGB channel and the position is determined depending on the status of channel and value of the secret key. Pixels of the cover image are selected depending on the environment of the channels and hidden information. The ambiguity of pixel, channel and position selection process increases robustness of the steganographic system. The technique is also less vulnerable to unintentional attacks like image manipulation as data hides in the deeper layer of the pixels. The system uses the RGB channels of the stego-image and the secret key to extract the hidden information. The use of secret key gives another way to secure the information from malicious user. The experiment shows that on average 77.00% pixels of the cover image are used for hiding secret information and produces high PSNR value which indicates the capacity and imperceptibility of the technique respectively.
[Computers, data hiding, secret key, RGB channel based steganographic technique, hidden information, steganographic system, robust RGB channel based image steganography technique, Image color analysis, public key cryptography, RGB channels, image manipulation, Robustness, image colour analysis, Cryptography, PSNR value, PSNR, Image steganography, Stego-image, information security, stego-image, Imperceptibility, Green products, cover image, steganography, RGB image, Secret key, Arrays, image coding, position selection process]
An approach for enhancing message security in audio steganography
16th Int'l Conf. Computer and Information Technology
None
2014
Concealing a message and ensuring its security is inevitable in data transmission. Among various concepts, one approach is steganography that encodes secret message in indiscernible way. In this paper, we present an audio steganographic technique and propose a novel approach to hide data in the least significant bit (LSB) of the stereo-audio samples with CD-quality. Here, on the basis of stego-key and its parity, message bits are encoded into cover audio samples. In terms of security and imperceptibility, this method is a significant improvement of LSB method for hiding information in audio.
[stereo-audio samples, audio steganography, information hiding, Receivers, Encoding, Decoding, Security, stego-key, message security, audio coding, message security enhancement, least significant bit, parity bit, steganography, data transmission, Bismuth, Silicon, Signal to noise ratio]
A novel automatic property weight generator for semantic data integration
16th Int'l Conf. Computer and Information Technology
None
2014
The dramatic increase of heterogeneous data resources, whether they are semantic knowledge bases or databases, demands for an automatic data integration technique, which is directly affected by the weight of property associated to instances or data. Choosing a suitable metric for generating weight for a property automatically is nevertheless a formidable task. In this study, we analyze different metrics for generating weights and formulate mathematical reasoning to select an appropriate one. Our observation suggests that the weight of a property is highly influenced by the number of instances contain the property, the number of instances contain the distinct value for the property and the total number of instances in a training dataset. The experiments and evaluations illustrate the fact and shows the strength of automatic weight generator for properties in a data integration technique.
[Measurement, Record Linkage, semantic Web, Knowledge based systems, Instance Matching, Ontologies, semantic data integration, Semantic data Integration, Equations, heterogeneous data resources, database, Semantics, Data integration, automatic property weight generator, Identity Recognition, Benchmark testing, semantic knowledge bases, automatic data integration technique, data integration, mathematical reasoning]
Evaluating Q-learning based stateful round trip time estimation over high-data-rate wireless sensor networks
16th Int'l Conf. Computer and Information Technology
None
2014
Diversified applications of wireless sensor networks demand reliable data transmission, which generally experiences limited performance owing to the inherent resource constraints of sensor nodes. Estimated round trip time is one of the prominent aspects of reliable transmissions that possesses a potential to enhance the performance sustaining the resource constraints. Therefore, we have already proposed an efficient technique for estimating round trip time over wireless sensor networks. We have evaluated the proposed technique over low-data-rate networks, consisting low-bandwidth radios, considering classical deployments of sensor networks. However, recent deployments of wireless sensor networks have started to exploit high-bandwidth radios as they are now experiencing high-data-rate transmission. Therefore, in this paper, we evaluate the proposed estimation technique over high-data-rate wireless sensor networks consisting high-bandwidth radios. Our evaluation through both real testbed experiments and ns-2 simulation reveals significant performance improvement using the proposed technique compared to that using other existing techniques.
[wireless sensor networks, resource constraints, Estimation, ns-2 simulation, high-data-rate transmission, Throughput, Routing, low-data-rate networks, low-bandwidth radios, high-data-rate wireless sensor networks, Jacobian matrices, Q-learning based stateful round trip time estimation, Wireless sensor networks, sensor nodes, data communication, Data communication, Reliability, learning (artificial intelligence), reliable data transmission]
Design and implementation of a RF controlled robotic environmental survey assistant system
16th Int'l Conf. Computer and Information Technology
None
2014
Survey operation to collect information about environmental parameters is an inevitable part in various cases. But manual data logging by human is dangerous in perilous places. This paper deals with design and implementation of a RF Controlled Robotic Environmental Survey Assistant System for remote survey operations, which can assist in remote data acquisition of environmental parameters like temperature, humidity and the presence and level of LPG gas in the air. Technically, two individual units make up the complete survey assistant system. First one is the wireless control unit for controlling and monitoring the survey process, whereas the second one is a remote controlled data collector robot unit which executes the survey process. The survey robot is equipped with sensor units required for measurement of temperature, humidity and combustible gas level as environmental parameters of the survey zone, ultrasonic sensor for obstacle detection in the path of the robot and GPS receiver for collecting information about position of the robot. A Radio Frequency (RF) Amplitude Shift Keying (ASK) transceiver channel enables the communication between the robot and control panel. The robot can be operated in two different modes. In autonomous mode, the robot travels a random path depending on obstacle avoidance technique and stores or collects and broadcasts live data stream to the control panel. Each time the robot is set to autonomous mode, after completion of a limited time survey operation, the robot comes back to it's starting position where autonomous mode was started. Obstacle sensor units installed in the robot helps it to detect and avoid obstacles in its path. The real-time data visualization is achieved on a Liquid Crystal Display (LCD) in the control panel and the mode of operation can be select from the control panel by operator.
[remote data acquisition, collision avoidance, radio frequency, telerobotics, Environmental parameters, mobile robots, Temperature sensors, Wireless communication, remote controlled data collector robot unit, Control unit, RF controlled robotic environmental survey assistant system, Humidity, wireless control unit, Robot sensing systems, surveying, obstacle avoidance technique, data acquisition, wireless channels, remote survey operations, Manual and autonomous mode, amplitude shift keying, real-time data visualization, LCD, liquid crystal display, Survey unit, control panel, Temperature measurement, Wireless sensor networks, RF ASK transceiver]
Maze solving algorithm for line following robot and derivation of linear path distance from nonlinear path
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper we have discussed a unique general algorithm for exploring and solving any kind of line maze with another simple one for simple mazes without loops or with loops having highest two branches none of which are inward. For the general algorithm, we need a method to map the whole maze, which is required if the maze is complex. The proposed maze mapping system is based on coordinate system and after mapping the whole maze as a graph in standard `Adjacency-list representation' method, shortest path and shortest time path was extracted using Dijkstra's algorithm. In order to find the coordinates of the turning points and junctions, linear distances between the points are needed, for which wheel encoder was used. However, due to non-linear movement of robot, the directly measured distance from the encoder has some error and to remove this error an idea is built up which ended by deriving equations that give us almost exact linear distance between two points from the reading of wheel encoder of the robot moving in a non-linear path.
[Maze solving, shortest time path extraction, graph theory, Wheels, mobile robots, Mobile robots, linear path distance derivation, robot nonlinear movement, line following robot, standard adjacency-list representation method, nonlinear path, unique general algorithm, linear distance, maze mapping system, Junctions, coordinate system, mapping, linearisation techniques, maze solving algorithm, turning point coordinates, wheel encoder, path planning, Equations, Dijkstra algorithm, Robot kinematics, junction coordinates, Arrays, linear path distance]
EEG signal classification using frequency band analysis towards epileptic seizure prediction
16th Int'l Conf. Computer and Information Technology
None
2014
Epilepsy is one of the most common and diverse set of chronic neurological disorders characterized by an abnormal excessive or synchronous neuronal activity in the brain that is termed &#x201C;seizure&#x201D;, affecting about 50 million individuals worldwide. Electroencephalogram (EEG) signal processing technique plays a significant role in detection and prediction of epileptic seizure. Recently, many research works have been devoted to detect/predict of epileptic seizure based on analysis of EEG signals. Even though remarkable works have been conducted on seizure detection/prediction, experimental results are not mature enough in terms of sensitivity, specificity, and accuracy. In this paper we present a new approach for seizure detection to analysis preictal (before seizure onset) and interictal (period between seizures) EEG signals by extracting different features from gamma frequency band by decomposing the signals using discrete wavelet transformation. Note that the detection of preictal and interictal EEG signals leads to predict the epileptic seizure. Experimental results demonstrate that the propose method outperforms the state-of-the-art method in terms of sensitivity, specificity and accuracy to classify seizure by analyzing EEG signals to the benchmark dataset in different brain locations.
[discrete wavelet transformation, Epilepsy, gamma frequency band, Electroencephalography, electroencephalogram signal processing, chronic neurological disorders, Accuracy, epileptic seizure prediction, LS-SVM, feature extraction, synchronous neuronal activity, Seizure, abnormal excessive neuronal activity, DWT, Temporal lobe, electroencephalography, discrete wavelet transforms, frequency band analysis, EEG, brain locations, Discrete wavelet transforms, signal classification, medical disorders, seizure detection-prediction, medical signal processing, Sensitivity, benchmark dataset, Feature extraction, benchmark testing, neurophysiology, preictal EEG signals, EEG signal classification, interictal EEG signals]
An enhanced compression-based energy efficient data communication scheme for wireless sensor networks
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper, we propose an effective data communication scheme for wireless sensor networks. The data communication approach makes efficient use of data compression to achieve energy efficiency. The main contribution of the proposed approach is, it makes use of the simple but efficient representation of value pairs to attain the compression. It is also novelty of our approach that, proposed scheme employs cluster coordinators for balancing energy consumption by assigning the inter cluster communications to the cluster heads and the intra cluster communications to cluster coordinators. Since, providing flexible service using wireless sensor network is challenging because they usually contain a large number of resource constraint sensing nodes with small memory, lower processing capability and extremely limited battery life, the scheme is considered to be simple, computationally feasible and overall resource requirement is minimum. It is also a mentionable feature of our research that, besides consideration of resource constraints of individual sensor nodes into concern, the proposed compression based compression scheme takes the overall routing scheme into consideration which ensures efficient and minimum energy consumption of WSNs.
[Energy consumption, Dictionaries, wireless sensor networks, enhanced compression-based energy efficient data communication scheme, Data compression, resource constraint sensing nodes, inter cluster communications, Resource Constraints, Cluster Coordinators, Data Compression, routing scheme, Sensors, Data communication, Data Communication, Base stations, data compression, intra cluster communications, telecommunication power management, cluster heads, Routing, Power Consumptions, Wireless Sensor Networks, Wireless sensor networks, telecommunication network routing, value pair representation, cluster coordinators, energy consumption balancing]
Irregular total labellings of Ladder
16th Int'l Conf. Computer and Information Technology
None
2014
The total edge irregularity strength tes(G) and total vertex irregularity strength tvs(G) are invariants analogous to irregular strength s(G) of a graph G for total labellings. Bac&#x030C;a et al. [Discrete Mathematics, 307: 1378-1388, (2007)] determined the bounds and precise values for some families of graphs concerning these parameters. In this paper, we show the exact values of the total edge irregularity strength tes (L<sub>n</sub>)= n total vertex irregularity strength and tvs(L<sub>n)</sub> = &#x230A;n/2 &#x230B; + 1 for the Ladder L<sub>n</sub>.
[Computers, irregular total labellings, graph theory, Ladder Graphs, Educational institutions, Graph theory, Information technology, total edge irregularity strength tes, graph, Computer science, total vertex irregularity strength tvs, ladder, Total labeling, Irregular total labelling, Labeling]
Performance of some parametric test statistics for testing the difference of means of two skewed populations
16th Int'l Conf. Computer and Information Technology
None
2014
Right skewed data is available in the field of health science, biological science, epidemiological science and others. This paper assessed some existing procedures for testing the difference of means of two right skewed populations. A Monte Carlo simulation study has been conducted to compare the performance of the test statistics in the sense of empirical size and power of the selected tests. Based on the simulation study results, some good test statistics are recommended for practitioners. A real life health related example has been considered to illustrate the application of the techniques.
[Computers, health science, difference of means testing, right skewed data, right skewed population, statistical distributions, epidemiological science, Monte Carlo methods, Sociology, Statistical Inference, statistical testing, health care, Testing, Monte Carlo simulation study, data analysis, Right skewed, Biological system modeling, Statistics, Information technology, Standards, Primary hypertension patients, Simulation, Size of the test, biological science, parametric test statistics, Power, Polymorphism]
Diagnostics based principal component analysis for robust plane fitting in laser data
16th Int'l Conf. Computer and Information Technology
None
2014
Plane fitting and obtaining characteristics (e.g., normal) from the estimated plane are fundamental tasks in many applications in which laser scanner 3D data is used. Unfortunately, laser data are not free from outliers. Principal Component Analysis (PCA) is a popular method for plane fitting, but it is known that PCA is very sensitive to outliers and gives misleading non-robust results. We present a robust plane fitting algorithm based on PCA coupled with an outlier detecting diagnostic statistical approach. In this method, the recently introduced robust scatter matrix is used to calculate robust statistical distance for finding outliers. After excluding outliers, PCA is performed on the outlier free data which is used for fitting planar surfaces and to estimate robust normal and other parameters. Demonstration of the new algorithm through several synthetic and vehicle based laser scanning data show that the proposed method is efficient, and gives robust estimates. Results outperform Least Squares (LS), PCA and are significantly better than the well-known RANSAC in terms of time, accuracy and robustness. This method has great potential for robust segmentation, surface reconstruction, and other point cloud processing tasks.
[surface reconstruction, Surface reconstruction, robust scatter matrix, Three-dimensional displays, segmentation, Robustness, least squares algorithm, robust plane fitting algorithm, least squares approximations, Image edge detection, Fitting, robust normal, outlier, point cloud, robust statistical distance, plane fitting characteristics, matrix algebra, PCA, RANSAC, Feature extraction, Surface fitting, data handling, diagnostics based principal component analysis, principal component analysis, Principal component analysis, laser data]
Optimizing software design migration from structured programming to object oriented paradigm
16th Int'l Conf. Computer and Information Technology
None
2014
Several industries are using legacy softwares, developed with Structured Programming (SP) approach, that should be migrated to Object Oriented Paradigm (OOP) for ensuring better software quality parameters like modularity, manageability and extendability. Automating SP to OOP migration is pivotal as it could reduce time that take in the manual process. Given this potential benefit, the issue is yet to be addressed by researchers. This paper addresses the scenario by modeling this problem as a graph clustering problem where SP functions and function calls are vertices and edges respectively. The challenge evolving the problem is to find optimized clusters from graphs. To aid this problem, certain heuristic algorithms based on Monte Carlo and Greedy approaches are being developed. The proposed algorithms have been tested against a collection of real and synthetic data. The numerical results show that greedy algorithms are faster and produced better results than the average performance of Monte Carlo based approaches.
[Greedy algorithms, OOP migration, graph theory, structured programming, SP function, heuristic algorithm, Monte Carlo methods, Clustering algorithms, Monte Carlo approach, Graph Clustering, software design migration, Mathematical model, greedy algorithm, Software Design, DSM, object-oriented programming, greedy algorithms, software quality parameter, function calls, legacy softwares, software maintenance, Information technology, Equations, object oriented paradigm, graph clustering problem, Call Graph, pattern clustering, Software, greedy approach, Legacy Code]
Joint routing, scheduling, and physical network coding in fixed wireless multihop networks
16th Int'l Conf. Computer and Information Technology
None
2014
We study joint routing, scheduling, and physical network coding (PNC) in fixed wireless multihop networks under a physical interference model. The objective is to quantify the throughput gain provided by PNC in wireless mesh networks. We formulate a cross-layer optimization framework to determine the optimal max-min throughput of the flows and the optimal configuration of the routing and scheduling parameters in multi-hop wireless networks with PNC. We solve the problem for several small to medium size wireless mesh networks. Our numerical results show that PNC provides a significant throughput gain in wireless mesh networks at low transmission power. This throughput gain is significantly higher than the throughput gain of the XOR-like network coding.
[XOR-like network coding, PNC, Throughput, minimax techniques, wireless mesh networks, Optimization, radiofrequency interference, routing, joint routing parameter, Wireless mesh networks, scheduling, throughput, optimal maxmin throughput determination, Joints, transmission power, network coding, physical network coding, throughput gain quantification, Interference, physical interference model, Routing, scheduling parameter, link layer network coding, telecommunication network routing, Cross-layer optimization, Network coding, small-to-medium size wireless mesh networks, fixed wireless multihop networks, cross-layer optimization framework]
Unfairness problem in WLANs due to asymmetric co-channel interference and its mitigation
16th Int'l Conf. Computer and Information Technology
None
2014
Co-channel interference exists in IEEE 802.11 based wireless local area networks (WLANs) due to the limited number of non-overlapping channels in the current standards. In this paper, we investigate the impact of co-channel interference in carrier sense multiple access with collision avoidance (CSMA/CA) based WLANs via simulation. We find that the throughput unfairness among the users is severe in WLANs due to asymmetric co-channel interference among the users. We propose a centralized algorithm based on analytical analysis to configure the minimum contention window of the nodes to mitigate the throughput unfairness problem in WLANs. The performance of the proposed algorithm is evaluated by simulation and found to be very effective.
[IEEE 802.11, Wireless LAN, collision avoidance, telecommunication congestion control, Wireless local area networks, co-channel interference, Throughput, nonoverlapping channel, Multiaccess communication, minimum contention window, Sensors, CSMA, throughput, CA, asymmetric cochannel interference mitigation, fairness, interference suppression, Receivers, WLAN, centralized algorithm, cochannel interference, throughput unfairness problem mitigation, wireless local area network, analytical analysis, CSMA/CA, wireless LAN, Interchannel interference, carrier sense multiple access, contention window]
Performance analysis of MC-DS-CDMA in the presence of carrier frequency offset and timing jitter over Rayleigh Fading Channels
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper an analytical approach is presented to determine the impact of frequency offset, timing jitter and additive white Gaussian noise (AWGN) on the bit error rate (BER) performance of a multi-carrier direct-sequence code division multiple access (MC-DS-CDMA) system over a Rayleigh Fading Channel. The analysis developed the pdf (probability density function) at the receiver considering combined influence of fading, timing jitter and Doppler frequency offset etc with maximal ratio combining (MRC) scheme. The expression for the conditional BER conditioned on a given timing error and fading is derived and the average BER is evaluated in the presence of Multiple Access Interference (MAI) and Inter-Carrier Interference (ICI). The performance results are evaluated numerically in terms of SINR and BER considering system parameters like number of users, number of sub-carriers. The result shows significant deterioration in SINR and BER performance due to fading along with the changes in parameters.
[AWGN, OFDM, MC-DS-CDMA, Bit error rate, spread spectrum communication, Multiaccess communication, AWGN channels, multiple access interference, carrier frequency offset, intercarrier interference, error statistics, Fading, Timing jitter, timing jitter, MC-DS-CDMA system, Timing Jitter, Interference, Rayleigh channels, Rayleigh fading channels, code division multiple access, bit error rate, BER, ICI, jitter, MAI, additive white Gaussian noise, multicarrier direct-sequence code division multiple access system, Rayleigh Fading, SINR, Signal to noise ratio]
Intrusion detection learning algorithm through network mining
16th Int'l Conf. Computer and Information Technology
None
2014
This paper presents a learning algorithm for adaptive network intrusion detection based on clustering and nai&#x0308;ve Bayesian classifier, which induces a hybridization of unsupervised and supervised learning processes. The proposed approach scales up the balance detection rates for different types of network intrusions, and keeps the false positives at acceptable level in network intrusion detection. The algorithm first clusters the network logs into several groups based on similarity of network logs, and then calculates the prior and class conditional probabilities for each cluster. In classifying a new network log, the algorithm calculates the similarity of attribute values of network data with each cluster and initialize a weight value for each cluster. Then each cluster classifies the network data with its priori and conditional probabilities that multiply with respective cluster's weight value. Finally, voting techniques applied for classifying the new network data based on each cluster's classification result. The performance of the proposed algorithm tested by employing KDD99 benchmark network intrusion detection dataset, and the experimental results proved that it improves the detection rates as well as reduces the false positives for different types of network intrusions.
[Computers, pattern classification, boosting, unsupervised learning process, data mining, supervised learning process, intrusion detection, Classification algorithms, na&#x00EF;ve Bayesian classifier, intrusion detection learning algorithm, Niobium, false positive reduction, security of data, pattern clustering, Intrusion detection, Clustering algorithms, KDD99 dataset, network logs similarity, conditional probability, Bayes methods, learning (artificial intelligence), network mining, naive Bayesian classifier, adaptive network intrusion detection]
Two stage fuzzy logic based clustering approach wireless sensor network LEACH protocol
16th Int'l Conf. Computer and Information Technology
None
2014
Energy proficiency is an essential feature to intend an extensive existence wireless sensor network. More than a few methods have been proposed to boost the battery lifetime of sensor nodes. One of the methods is clustering approach which consists of two ways: selecting cluster heads with more residual energy and rotating cluster heads periodically from cluster to cluster thus extends the network lifetime. In this paper we present the wireless sensor networks by means of TSFL-LEACH (Two Stage Fuzzy Logic based Low Energy Adaptive Clustering Hierarchy) where completely two separate cluster levels are defined depending on the distance from the cluster heads and the cluster nodes. We also applied fuzzy asymmetrical clustering mechanism to prolong the lifetime of WSNs. We compare our algorithm with some popular algorithms namely LEACH, LEACH-C and EEUC. The simulation result shows that our bespoken edition improves the energy efficiency and enhances the life length of a wireless sensor network (WSN).
[Algorithm design and analysis, cluster head rotation, Protocols, wireless sensor networks, WSN lifetime, fuzzy set theory, wireless sensor network LEACH protocol, two stage fuzzy logic based clustering approach, TSFL-LEACH, sensor nodes, Clustering algorithms, cluster nodes, rotating cluster heads, protocols, LEACH-C algorithm, Base stations, energy efficiency improvement, fuzzy asymmetrical clustering mechanism, residual energy, two stage clustering, cluster head selection, Fuzzy logic, EEUC algorithm, Wireless sensor networks, pattern clustering, energy proficiency, battery lifetime, Energy efficiency]
Performance analysis of DS-CDMA wireless communication system with and without diversity
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper, performance of a novel analytic model of direct sequence code division multiple accesses (DS-CDMA) using diversity with maximal ratio combining (MRC) scheme is observed under the influence of multipath Rayleigh fading and multi-access interference (MAI) varying different system parameters and properties of PN sequences. The analysis presents a mathematical expression for bit error rate (BER) of the novel DS-CDMA model using diversity with MRC when signal to interference plus noise ratio (SINR) per bit follows a chi-square distribution. Performance results are evaluated in terms of average bit error rate (BER) at the output of MRC receiver by averaging the conditional BER over the probability density function (pdf) of signal to interference plus noise ratio (SINR). In addition to that the receiver sensitivity of the model is observed for different transmit and receive diversity in a proposed multiple input multiple output (MIMO) system model varying Rayleigh fading co-efficient. Variants of simulation result at various cases are also analyzed.
[DS-CDMA model, Bit error rate, DS-CDMA wireless communication system performance analysis, Diversity, MIMO system model, spread spectrum communication, MRC receiver sensitivity, Multiaccess communication, statistical distributions, DS-CDMA, PDF, multiple input multiple output system model, multiaccess interference, chi-square distribution, MIMO, MIMO communication, error statistics, probability density function, Fading, diversity reception, Receivers, Rayleigh channels, code division multiple access, BER mathematical expression, radio receivers, bit error rate, BER, PN sequence properties, maximal ratio combining scheme, direct sequence code division multiple accesses, MAI, Diversity reception, diversity transmission, diversity receival, signal to interference plus noise ratio, multipath Rayleigh fading coefficient, SINR, Signal to noise ratio, MRC]
Designing a press and swipe type single layered Bangla soft keyboard for Android devices
16th Int'l Conf. Computer and Information Technology
None
2014
Development of text entry speed and accuracy is the key concern for designing and evaluating a soft keyboard. Bangla language has large collection of characters and it is hard to design a single layered fixed layout keyboard to provide highperformance in today's small mobile device. The traditional soft keyboards available in the market are multi-layered and menu based hierarchical keyboard which offer poor text entry speed. This paper describes the design, implementation and evaluation of a fixed layout single layered press and swipe type soft keyboard. Analysis of the comfort zone and movement of fingers, placement of characters and keys based on monograph, implementation on android platform and evaluation comparing with QWERTY keyboard have been described.
[soft keyboard, text analysis, QWERTY keyboard, Humanoid robots, fingers movement, Presses, menu based hierarchical keyboard, single layered fixed layout keyboard, Fingers, Pressing, press and swipe keyboard, Bangla language, text entry speed, Bangla keyboard, multilayered based hierarchical keyboard, single layered Bangla soft keyboard, Android devices, keyboards, natural language processing, Keyboard layout, Single layered, smart phones, fixed layout single layered press and swipe type soft keyboard, mobile device, Layout, Keyboards, monograph, text entry accuracy, Androids, comfort zone, Android keyboard]
E-governance using social network: A model for strong democratic environment in Bangladesh
16th Int'l Conf. Computer and Information Technology
None
2014
In recent years, investigating national incidents of Bangladesh it has been seen that social network played a vital role. Though social network sites like Facebook, Twitter, and Google+ are mainly used for connecting people all over the world but they are now being used for advertisements, news updates and for various important purposes of day to day happenings. Microblogging is getting more and more popular among educated and conscious people of Bangladesh. It is becoming another popular media which can provide entertainment and knowledge sharing about anything. Social Networking Sites (SNS) has become rendezvous of different people, different beliefs and interests to share and discuss. In this circumstance here we propose a model of e-governance using SNS for democratic Bangladesh perspective. Direct interaction between citizens and government can strengthen democracy. Accountability and transparency of politicians and government-workers can be ensured by flow of information to citizens of various developing activities. This paper proposes a model concerning these matters, in order to reduce corruption using a government controlled SNS.
[Computers, Transparency, knowledge sharing, entertainment, social network sites, Twitter, democratic environment, information flow, advertisements, microblogging, E-governance, SNS, Facebook, accountability, news updates, Google+, SNS (Social Network Sites), Government, Nominations and elections, popular media, Media, politicians, e-governance, Information technology, G2C (Government to Citizens), Bangladesh, ICT (Information and Communication Technology), transparency, E-Democracy, Democracy, social networking (online), government-workers, government data processing, national incidents]
On the impact of virtual environment in trust building: E-commerce perspective
16th Int'l Conf. Computer and Information Technology
None
2014
Virtual Reality is an important aspect of the state-of-art technologies. The emerging trend to integrate the highest level of interactivity in the computer-oriented activities like web pages has introduced a new dimension in various aspects of computer-oriented services. Electronic commerce (e-commerce) is such a sector, which may be greatly facilitated with the touch of interactive virtual reality or virtual environment. This research mainly aims to develop a trust establishment model for electronic commerce activities. This paper focuses on the usability of virtual reality and virtual environments in establishing trust in electronic commerce and related issues. We perform an extensive analysis on the current electronic commerce infrastructure and evaluate the effectiveness of virtual environment from the point of view of customer relationship management, trust-worthiness and customer satisfaction. An effective framework of integrating virtual reality with e-commerce is also presented in this paper. This paper also investigates the applicability of virtual reality and virtual environment for trust management in online shopping. The prime contribution of this paper is, it proposes an effective trust-building model to enhance the customer-seller relationship in e-commerce and provides a robust framework for the same. A survey has also been conducted evaluating the acceptability of proposed framework. Comprehensive recommendations on e-commerce-trust formation model in developing countries and its prospects and constraints are provided in this paper too.
[Computers, Solid modeling, virtual reality, online shopping, Buildings, Virtual environments, Virtual Environments, Electronic commerce, customer relationship management, Electronic Commerce, trust-worthiness, customer satisfaction, E-Commerce Trust, Customer Trust Management.), Web pages, Customer Relationship Management, e-commerce, Internet, electronic commerce activities, Virtual Reality, electronic commerce, retail data processing]
Radius particle swarm optimization for resource constrained project scheduling problem
16th Int'l Conf. Computer and Information Technology
None
2014
The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical well-known and NP-hard problem which includes the resource and precedence constraints that has been applied to many applications. This paper proposes the Radius Particle Swarm Optimization (RPSO) to solve the RCPSP. It extends the Particle Swarm Optimization (PSO) by regrouping the agent particles within the appropriate radius of the circle. It initializes the group of particles, calculate the fitness function, and find the best particle in that group. It is then applied to the RCPSP to find the best scheduling. The proposed method is tested against the PSPLIB standard datasets. The results show that the R-PSO gives better average makespan and standard deviation than traditional PSO.
[Computers, Schedules, project management, particle swarm optimisation, fitness function, precedence constraint, resource constraint, Topology, radius particle swarm optimization, Particle swarm optimization, Statistics, Genetic algorithms, NP-hard problem, Sociology, Resource constrained Project Scheduling Problem (RCPSP), Radius Particle Swarm Optimization (R-PSO), Particle Swarm Optimization (PSO), resource constrained project scheduling problem, RPSO, computational complexity, RCPSP]
A compact W-shaped 2.45 GHz RFID tag antenna design for UHF RFID applications
16th Int'l Conf. Computer and Information Technology
None
2014
This paper represents design, analysis &amp; investigation of an radiator for UHF RFID applications. This antenna covered the frequency spectrum 2392.86-2457.14MHz with bandwidth 14.28MHz which is sufficient for RFID tag applications. We have designed as well as simulated this antenna using Numerical Electromagnetic Code (NEC) antenna modeling software package which is based on the method of moments solution and after simulation we have got SWR exactly 1.097 having reflection coefficient -26.68 without filtering and after passing through a low-pass L-network the SWR value is sharply 1.00054 with reflection coefficient -71.386 at 2.45 GHz band. The total antenna gain is excellent and it is approximately 22.3 dBi. We have achieved the decent impedance that is 48.36-j4.95 ohm before filtering and 49.99-j0.029 ohm after filtering. Afterward we have fabricated this designed antenna on a low-cost printed circuit board with ground plane including dielectric constant 4.4 &amp; thickness of the metal 1.58mm. The efficiency of the proposed antenna is 99.96%. The measurement shows that the SWR value is 1.05404 at 0 dB attenuation.
[standing wave ratio, radiofrequency identification, UHF RFID application, low-pass L-network, radio frequency identification, compact W-shaped RFID tag antenna design, dielectric constant, frequency 2.45 GHz, numerical electromagnetic code antenna fabrication, reflection coefficient, antenna radiation patterns, Low-pass L-network, Bandwidth, UHF antennas, low-cost printed circuit board, low-pass filters, method of moments, ground plane, Electrically Small Antenna (ESA), Antenna radiation patterns, Antenna measurements, Filtering, radiator analysis, NEC antenna modeling software package, UHF filters, Standing Wave Ratio (SWR), RFID Tag Antenna, SWR, size 1.58 mm, frequency 2392.86 MHz to 2457.14 MHz, Numerical Electromagnetic Code (NEC), Impedance, frequency spectrum, Radiofrequency identification, bandwidth 14.28 MHz]
Restoration of non-uniformly warped noisy images based on coarse-to-fine optical flow estimation
16th Int'l Conf. Computer and Information Technology
None
2014
This paper proposes a high accuracy and fast image restoration approach to restore a sequence of atmospheric turbulence degraded frames of a remote object or scene. A coarse-to-fine optical flow technique is employed to estimate the dense motion fields of the frames against a reference frame. The First Register Then Average And Subtract (FRTAAS) method is used to correct the geometric distortions and restore a high quality sequence. Finally, a non-local means filter is applied to extract noise from each frame of the sequence. A performance comparison is presented between the proposed restoration method and an earlier method in terms of computational time and accuracy. The effectiveness of the proposed approach is demonstrated on both synthetic and real-life videos.
[Integrated optics, Noise, dense motion fields estimation, image registration, synthetic videos, image filtering, reference frame, remote object, real-life videos, image restoration, feature extraction, motion estimation, first register then average and subtract method, high quality sequence restoration, image sequences, geometric distortions, nonuniformly warped noisy images restoration, Estimation, Optical distortion, Optical imaging, optical flow, Image restoration, atmospheric turbulence, FRTAAS method, image denoising, coarse-to-fine optical flow estimation, nonlocal means filter, remote scene, noise extraction, Atmospheric turbulence, Adaptive optics, atmospheric turbulence degraded frames]
Modified CMA based blind equalization and carrier-phase recovery in PDM-QPSK coherent optical receivers
16th Int'l Conf. Computer and Information Technology
None
2014
Digital coherent optical receivers enable equalization of fiber transmission impairments and carrier-phase estimation in digital domain. In this paper, a novel equalization and carrier-phase recovery algorithm, both based on modified constant-modulus algorithm (CMA), is proposed for polarization-division multiplexed quadrature phase-shift keying (PDM-QPSK) systems. The proposed scheme is validated with 40-Gbits/s PDM-QPSK transmission experiments.
[Blind equalizers, equalization, PDM-QPSK coherent optical receivers, modified CMA based blind equalization, modified constant-modulus algorithm, Estimation, optical fibre communication, digital coherent optical receivers, Optical receivers, fiber transmission impairments, optical receivers, polarization-division multiplexed quadrature phase-shift keying system, Demultiplexing, phase shift keying, Signal processing algorithms, CMA, blind equalisers, carrier-phase estimation, optical fiber communication, Optical fiber communication, carrier-phase recovery, digital signal processing]
A gaming approach in physical therapy for facial nerve paralysis patient
16th Int'l Conf. Computer and Information Technology
None
2014
This work is based on developing a system for the Bell's palsy, a type of facial nerve paralysis, patients to help them in passive exercise, which is one of the major ways to recovery. This exercise method will be provided in the form of computer game so the user will be able to perform exercise without the hardship that comes with active exercise. It will also show how much this system helps the patients and how its performance is analyzed by the users' i. e. the patients. We will also show that how passive exercise is preferable to current active one. As per initial experimentation, we found out that the proposed gaming approach for Bell's palsy recovery is useful to the patient.
[Computers, passive exercise, facial nerve paralysis, computer game, gaming approach, Bell palsy, Length measurement, diseases, patient rehabilitation, patient recovery, Image color analysis, patient treatment, active exercise, Prototypes, computer games, Games, Cameras, human computer interaction, physical therapy, Face, medical computing, facial nerve paralysis patient, passive exercise method, Bell's palsy]
DBF-MLT: NP-completeness of specialized voice service in networked virtual environment a peer-to-peer approach
16th Int'l Conf. Computer and Information Technology
None
2014
Several researchers have successfully developed realistic technique for specialized voice service in networked virtual enthronement (NVE) like Second Life. Specialized voice service in virtual environment enables users to speak to each other naturally and seamlessly. The spatial 3D avatar location and the voice direction in the virtual environment are used to generate realistic sound. Although with this specialized audio or voice streaming in virtual environment the real user can more easily identify who is talking, if several people are taking simultaneously. But this is a challenging task to stream the voice or audio data to the users' machine by considering the user bandwidth limit and the tight latency constraint and this can be formalized as a NP-complete problem. In this paper, we presented the proof of NP-completeness of the specialized audio streaming problem called DBF-MLT.
[latency constraint, NP-Completeness, Avatars, audio streaming, peer-to-peer approach, spatial 3D avatar location, user bandwidth limit, Bandwidth, specialized voice service, voice direction, realistic sound, Networked Virtual Environment, speech processing, networked virtual environment, Second Life, Robots, avatars, voice streaming, peer-to-peer computing, Specialized Voice Service, Virtual environments, Receivers, DBF-MLT, NP-complete problem, NP-completeness, Peer-to-peer computing, NVE, acoustic signal processing, computational complexity, specialized audio streaming]
Neural net based complete character recognition scheme for Bangla printed text books
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper we propose a neural net based characters recognition scheme for Bangla printed text books. There are a lot of scientific literature, novels, magazines and books etc that are written in Bangla language. More than 400 million people use Bangla language. Most of the library and educational institutions want to keep copy of the books in a digital format. For storing those books in digital text format we need a good character recognition schemes by which we can convert the scanned text book images to editable texts. The key contribution of our research highlights this issue. We propose four main stages namely preprocessing, segmentation, training-recognition and post-processing. In the beginning the input book images preprocessed by rotation, scaling, binarization and noise elimination. The binarized image is then segmented and extracted into individual characters that are trained and recognized by an artificial neural network. Finally, the process ends by reconstructing the text in the post processing stage.
[Computers, text analysis, artificial neural network, digital printing, Noise, Training, optical character recognition, feature extraction, image segmentation, Bangla printed text books, image preprocessing, image training-recognition, learning (artificial intelligence), image post-processing, digital text format, optical character recognition scheme, Character recognition, Optical character recognition software, Boundary Fill, Information technology, Image segmentation, text reconstruction, Neural Network, image extraction, Character Recognition, Binary Image, neural nets, OCR]
PIssGA: An ultra fast meta-heuristic approach to solve protein inference problem
16th Int'l Conf. Computer and Information Technology
None
2014
The protein inference problem represents a major challenge in proteomics research. In this paper, we propose a novel meta-heuristic approach i.e. PIssGA to infer proteins from a given peptide sequence identified from tandem mass spectrometry in shotgun proteomics. Although most of the works available in the literature mainly adopts parsimonious pressure i.e. prefer least number of inferred proteins, our model is neither strictly parsimonious nor strictly optimistic, rather a tunable model that provides the flexibility to infer proteins either parsimoniously or optimistically or somewhere in between. Another important feature of PIssGA is its efficiency in search that makes the approach ultra first. We used Sigma49 dataset to test our method and found the proposed algorithm extremely fast and moderately accurate.
[peptide sequence, Peptides, genetic algorithms, parsimonious pressure, Statistics, mass spectroscopy, PIssGA, ultrafast metaheuristic approach, Genetic algorithms, shotgun proteomics, Proteins, tandem mass spectrometry, Sociology, proteins, Proteomics, bioinformatics, protein inference problem, Inference algorithms, Sigma49, steady state genetic algorithm]
Categorization of copies to improve the performance of routing schemes in delay tolerant networks
16th Int'l Conf. Computer and Information Technology
None
2014
Optimizing the three most important network performance parameters i.e. Delivery probability, Overhead ratio, and Delay is challenging in Delay Tolerant Networks (DTN) due to high occurrences of network partitioning in these networks. Owing to the partitioning nature efficient routing or dissemination of messages is important in DTN. In this paper, we focused to improve the spray phase of Binary Spray and Wait routing scheme, an efficient routing scheme in DTN, by dividing the number of copies of a message in different categories. The proposed scheme provides higher delivery probability and lower delay for messages while keeping the overhead ratio low. Here, an adaptive spraying scheme is used combining the regular binary spraying and performance based spraying of a message in each node. Thus, each message is forwarded starting with the binary followed by an adaptive spraying technique depending on the time to live (TTL) value of that message. We also formulate an equation to permit a node to calculate a switch value for each message it carries. Later, the switch value is compared to the TTL of the message to toggle to the wait phase for that particular message instead of the number of copies of the message reaching to one. Simulation results show that our adaptive scheme has better delivery and delay performance than that of other schemes.
[Computers, binary spray phase, electronic messaging, Switches, message adaptive spraying scheme, Delivery Predictability, Relays, wait routing scheme performance improvement, TTL message dissemination, Performance Copy, mobile ad hoc networks, regular binary spraying, network parameters, mobile ad hoc network, Regular Copy, Spraying, probability, delivery probability, Routing, delay tolerant networks, message forwarding, Information technology, message copy categorization, Switch Value, telecommunication network routing, time to live, Adaptive Spraying, Delays, delay tolerant network partitioning, overhead ratio, DTN, Visit List]
The effects of non-inhibitory serpin maspin on cell migration using an artificial neural network
16th Int'l Conf. Computer and Information Technology
None
2014
Maspin (Mammary Serine Protease Inhibitor) is a non-inhibitory member of the serpin family of protease inhibitors that influences multiple cellular functions including adhesion, migration, and invasion in tumour malignancy. In this paper, we describe a computational model of the effects of exogenous maspin on cellular proliferation, migration and invasion. To date, the whole cellular mechanisms of maspin impact on cellular behaviors have not been clearly defined in any computational model. In this paper, we have used a feedforward artificial neural network to define cell signaling and cellular automata approach to define the each and every cell behavior. Results show that maspin reduces migration and invasion by 10-40% and 15-30% respectively; confirmed by published in vitro data. This is the first attempt to model the effects of maspin using a computational model to verify in vitro data. This will provide new insights into the tumour suppressive properties of maspin and inform the development of novel cancer therapies.
[exogenous maspin, inhibitors, artificial neural network, cellular biophysics, noninhibitory member, cellular automata approach, cell behavior, tumour suppressive property, cellular automata, feedforward artificial neural network, cancer therapy, biology computing, tumour malignancy, multiple cellular function, cell migration, maspin, tumours, noninhibitory serpin maspin, maspin impact, cellular behavior, Computational modeling, Biological system modeling, adhesion, mammary serine protease inhibitor, cellular mechanism, In vitro, serpin family, Adhesives, invasion, Neural networks, cell signaling, computational model, cancer, tumour growth, Tumors, Cancer, feedforward neural nets, cellular proliferation]
A belief rule based (BRB) system to assess asthma suspicion
16th Int'l Conf. Computer and Information Technology
None
2014
Asthma is a common chronic inflammatory disease. A belief rule based Clinical Decision Support System (CDSS) of asthma suspicion is enhancing the accuracy of suspicion. This research paper presents out the development of a Belief rule based (BRB) system to assess Asthma suspicion by using signs and symptoms. The recently developed generic belief rule-based inference methodology by using the evidential reasoning approach (RIMER) has been considered to develop this BRB system. This system can deal with various types of uncertainties, found in clinical sings, symptoms, and clinical domain knowledge. The knowledge base of this system has been constructed by taking account of real patient data and consultation with specialists. The practical case studies provided to test this system. It has been observed that the proposed model is effective and can generate better prediction than from manual system (usually carried out by specialist) in terms of accuracy.
[knowledge base, real patient data, Belief Rule Base (BRB), Uncertainty, RIMER, signs and symptoms, Manuals, Medical services, BRB system, uncertainity, Prototypes, knowledge based systems, clinical symptoms, Computer architecture, asthma suspicion, belief networks, belief rule based clinical decision support system, Erbium, CDSS, clinical signs, generic belief rule-based inference methodology, evidential reasoning, diseases, medical information systems, belief rule based system, inference mechanisms, decision support systems, Asthma, chronic inflammatory disease, Suspicion, Expert systems, clinical domain knowledge]
A new hierarchical interconnection network for future generation parallel computer
16th Int'l Conf. Computer and Information Technology
None
2014
A Midimew connected Mesh Network (MMN) is a MInimal DIstance MEsh with Wrap-around links network of multiple basic modules, in which the basic modules are 2D-mesh networks that are hierarchically interconnected for higher-level networks. In this paper, we present the architecture of the MMN, addressing of node, routing of message, and evaluate the static network performance of MMN, TESH, mesh, and torus networks. It is shown that the MMN possesses several attractive features, including constant degree, small diameter, low cost, small average distance, moderate bisection width, and high fault tolerant performance than that of other conventional and hierarchical interconnection networks. It is also shown that with the same node degree, arc connectivity, bisection width, and wiring complexity, the diameter and average distance of the MMN is lower than that of the TESH network.
[Computers, Static Network Performance, parallel architectures, Ports (Computers), multiprocessor interconnection networks, arc connectivity, fault tolerant performance, hierarchical interconnection network, Complexity theory, communication complexity, future generation parallel computer, higher-level networks, basic modules, wiring complexity, Wiring, Midimew connected mesh network, TESH network, 2D-mesh networks, Multiprocessor interconnection, Network topology, torus networks, telecommunication network reliability, MMN architecture, Routing, Interconnection Network, message routing, mInimal dIstance mesh, MMN, static network performance, node degree, wrap-around links network, Massively Parallel Computers, telecommunication network routing, bisection width]
A CSMA based intra cluster communication technique for saving cluster head energy
16th Int'l Conf. Computer and Information Technology
None
2014
Wireless sensor network is one of the most promising technology in wireless network. To design a sensor network, improving the lifetime of sensor node is critical issue. For these reason researchers in these field pay great attention to Medium Access Control (MAC) protocol. As most of the nodes are placed in remote and hazardous area, so when the power is down it is difficult to recharge them or it is not possible to replace them. For these reason the main concern of researchers is how to utilize the medium in a power effective manner. With advance of these various MAC protocols are introduced and lots of recent works are going on. LEACH protocols are one of these protocols which introduce clustering technique. In this paper, we propose a method for intra cluster communication based on CSMA and an extended cluster head selection algorithm. Also figure out mathematical comparison between LEACH protocol and our proposed method. The protocol selection depends on the application requirements and hardware characteristics. LEACH is suitable for low traffic and small area. Here we also consider low traffic.
[Computers, Schedules, wireless sensor networks, telecommunication power management, medium access control protocol, clustering technique, access protocols, Multiaccess communication, cluster head energy saving, Wireless sensor network, Wireless sensor networks, CSMA based intracluster communication technique, Wireless Sensor Network, pattern clustering, sensor node lifetime improvement, LEACH protocol, Clustering algorithms, MAC protocol, Media Access Protocol, Enhancing Lifetime, telecommunication traffic, carrier sense multiple access]
Color feature based video content extraction and its application for poster generation with relevance feedback
16th Int'l Conf. Computer and Information Technology
None
2014
Poster that represents a video has many applications in internet and media world. In this paper, a method is proposed to generate poster based on color feature and relevance feedback. Color feature is used because it is more closely to human perception and it is easy to extract, match and also effective for indexing. In our method as color feature of a frame in a video, HSV color space and Color Moment are used. To calculate similar frames i.e. to detect shot, Euclidean distance between the features of frames are compared with a predefined threshold value. One frame is selected from each shot and stored with Temporally Occurrence Value (TOV) of the Frame. As a video can have similar shots in it but in different times, so the similar frames from the similar shots of different times are detected and one frame is selected from them. Then this selected frame is stored with Permanent Occurrence Value (POV) of the Frame by replacing its TOV and discarding others of them from storage. Finally, five frames with maximum POV are selected to generate poster layout and ranked the posters by their relevance feedback. The experiments show the robust result than existing methods.
[Computers, human perception, Color Moment, Color Space, Image color analysis, color feature based video content extraction, permanent occurrence value, poster layout, image colour analysis, Relevance Feedback, video signal processing, HSV color space, Color, temporally occurrence value, HSV, Information technology, Layout, relevance feedback, POV, Euclidean distance, poster generation, Feature extraction, TOV, Internet, color moment]
Cloud-Niagara: A high availability and low overhead fault tolerance middleware for the cloud
16th Int'l Conf. Computer and Information Technology
None
2014
Fault tolerance is the ability to a system to continue its functionality despite the presence of faults in the architecture. For a dynamic system such as the cloud, fault tolerance is required to ensure business continuity. This paper proposes a high availability middleware that ensures fault tolerance for cloud based applications. Effective Descriptive Set Theory is used to determine the model of fault detection for real life applications running on the open source cloud. A deterministic algorithm of the middleware is provided that achieves automatic allocation of backup nodes to the system based on the faults. After detection of faults, the middleware directs the system to add new nodes as replicas of the failed nodes, ensuring continuity of the cloud applications. Next, a case study including seven real life applications such as PostGreSQL Database, etc are described and fault tolerance is ensured through the proposed middleware. Empirical performance analysis of the algorithm is carried out and results are compared to traditional systems. Results show that in the presence of faults induced during experimentation, the middleware can be effectively used to introduce replica and ensure fault tolerance of bottleneck resources for executing 700 to 1000 processes per unit time.
[Availability, open source cloud, Random access memory, cloud based applications, set theory, deterministic algorithm, Middleware, fault tolerance middleware, Fault tolerance, effective descriptive set theory, Fault tolerant systems, Computer architecture, fault tolerant computing, Central Processing Unit, cloud computing, Cloud-Niagara middleware, middleware]
A fast recognition scheme for off-line Bangla numerals
16th Int'l Conf. Computer and Information Technology
None
2014
The requirement of Bangla character recognition has become one of the prime attentions among the current researchers due to the increase of automated systems and usage of hand held devices. This paper presents a novel approach to recognize handwritten bangla numerals and addresses a robust feature extraction scheme that spawns 23- dimensional features based on the numeral's structure and topology. The recognition scheme is organized with a proposed decision tree that has been justified using entropy calculation. Considering inconsistency in individuals' writing style and the presence of significant curves and loops, the proposed feature extraction method restricts to less dimensional features of each numeral from 14600 data samples. The recognition time of this scheme is much lower than the existing procedures, since the preprocessing tasks have been performed during down sampling operation. The quick response time 13.04 ms and the higher accuracy (96.82%) explore a new era for the proposed scheme of being implemented in Bangla numeral LeadPad toys.
[fast off-line Bangla numeral recognition scheme, feature extraction method, quick response time, Feature Extraction, Entropy, hand held devices, automated systems, down sampling operation, image sampling, Accuracy, decision tree, 23-dimensional features, Bangla Numeral, feature extraction, Decision trees, numeral topology, robust feature extraction scheme, natural language processing, character recognition, Character recognition, Information technology, individual writing style, Bangla character recognition, Handwriting recognition, numeral structure, Character Recognition, decision trees, entropy calculation, Bangla numeral LeadPad toys, Feature extraction, Time factors, Down Sampling]
Study of different forecasting models on Google cluster trace
16th Int'l Conf. Computer and Information Technology
None
2014
Workload prediction in cloud system is an important task and it helps in efficient resource allocation by minimizing cost and thus maximizing the profit. In this paper we analyze a large scale production workload trace (version 2) which is recently made publicly available by Google. The main objective of our research is to design and compare different forecasting models. We develop models through Adaptive Neuro-Fuzzy Inference System (ANFIS), Non-linear Autoregressive Network with Exogenous inputs (NARX), and Autoregressive Integrated Moving Average (ARIMA). Finally, we compare these three prediction models to find out the best one. Performance of forecasting techniques is measured by two popular statistical metrics, i.e., Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). The experimental result demonstrates that NARX model outperforms other models, e.g., ANFIS and ARIMA.
[autoregressive moving average processes, Predictive models, fuzzy reasoning, resource allocation, ANFIS, Mathematical model, cloud computing, cloud system, mean square error methods, NARX, root mean squared error, Testing, mean absolute error, Computational modeling, Biological system modeling, Time series analysis, Google Cluster Trace, RMSE, Forecasting, Workload, adaptive neurofuzzy inference system, MAE, Neural Network, nonlinear autoregressive network with exogenous inputs, ARIMA, autoregressive integrated moving average, forecasting models, Google cluster trace, workload prediction]
Developing an automated Bangla parts of speech tagged dictionary
16th Int'l Conf. Computer and Information Technology
None
2014
This paper develops an algorithm for making an automated Bangla Parts Of Speech (POS) tagged dictionary. Natural Language Processing is one of the most vigorous research areas of computer science. It enables to communicate and retrieve information form computer based system more effectively and efficiently. Researches on Bangla language processing have started long back. However, this research area still suffers from resource scarcity. A POS tagged corpus is a cardinal element for language processing. POS tagging is the process of categorizing a particular word to a particular part of speech or syntactic category. In Bangla, we do not have any large POS tagged dictionary. In this paper we develop an automated way to make a POS tagged dictionary of Noun, Verb and Adjective. Initially, a suffix (or postfix) list is created manually for Bangla language. Based on this suffix list the POS tagged dictionary is developed. The proposed algorithm is evaluated using a paragraph consisting of manually tagged 10,000 words with 11 tags. We found that POS tagging is obtained more accurately for Verb than Noun and Adjective.
[Computers, computer based system, Bangla Language Processing, Dictionaries, Bangla language processing, automated Bangla parts of speech tagged dictionary, Machine Learning, Parts Of Speech Tagging, automated Bangla POS tagged dictionary, computer science, Verb, Natural language processing, learning (artificial intelligence), Bangla Corpus, Noun, POS tagged corpus, natural language processing, Adjective, information retrieval, Information technology, Hidden Markov models, Tagging, Speech, syntactic category, information communication, dictionaries]
Structured web search in small domain
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper our aim is to bridging the gap between machine interpretation of search queries in search engines and actual user queries. Our primary concentration is to find a way to represent diverse search result pages in a structured way to reduce search query complexity and satisfy the need of diverse people. We first propose a traditional small scale search engine, called BuetSearch, for Bangladesh University of Engineering and Technology (BUET) domain, which has been designed by combining the PageRank algorithm and a proposed ranking and weighting scheme for Information Retrieval scoring. Then we propose a K-nearest neighbor algorithm based document classification algorithm for structured representation of search results. The results show that web search in small domain has merit over traditional large scale search engines and can be optimized to provide excellent results. With the application of structured approach, web search can be significantly simplified and the inconvenience of the users can be reduced.
[Computers, document handling, user queries, BuetSearch search engine, Google, pattern classification, structured Web search, information retrieval scoring, k-nearest neighbor algorithm, search engines, PageRank algorithm, document classification algorithm, BUET domain, ranking-and-weighting scheme, query processing, Bangladesh University of Engineering and Technology, Query processing, Web pages, Prototypes, Search engines, structured search results representation, Web search]
A new effective part selection approach for part-based gait recognition
16th Int'l Conf. Computer and Information Technology
None
2014
In part-based human gait recognition, choosing the appropriate body parts is the most challenging problem. The various cofactors such as carrying conditions (backpack or side bag or hand bag), cloths (long coat or jacket or gown) affect various body parts. Here, we proposed a method for detecting the various cofactors in early stage. We have taken some pixel points which are marked as a boundary for each cofactor. The weight of these points is calculated and used to detect the particular cofactors in early stage. We divide the human body into seven body parts based on anatomical studies of gait. Discarding the body parts where the cofactor is present, the remaining parts are used in classification. We have achieved better result compared with other classical methods.
[Computers, object recognition, Image edge detection, pixel point, Clothing, part-based human gait recognition, object detection, Pattern recognition, Cofactors, Part-based, Training, Gait Recognition, part selection approach, Probes, Gait recognition]
A switching mechanism for grid providers
16th Int'l Conf. Computer and Information Technology
None
2014
Economic-based resource management models are found to be efficient for distributed resource collaboration in the Grid. Due to the wide range of applications and dynamic nature of Grid resources, constant performance by a specific economic model is therefore subjective. This work identifies the potential of different models in different scenarios in the Grid. This identification motivates us to design an optimization framework that couples suitable economic models and is able to switch from one model to another based on the models' domains of strength. The research further describes the roles played by an agent in dynamically deciding which model to be used when and for what purpose. It shows the effectiveness of the switching framework compared to any other individual models in a dynamic computing environment.
[switching, Adaptation models, agent, Biological system modeling, Computational modeling, grid computing, Switches, Grid, grid resources, switching framework, Optimization, telecommunication switching, economic-based resource management models, Supply and demand, resource allocation, domains of strengths, economic models, optimization framework, grid providers, dynamic, switching mechanism, distributed resource collaboration]
iDMI: A novel technique for missing value imputation using a decision tree and expectation-maximization algorithm
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper we present a novel technique called iDMI that imputes missing values of a data set by combining a decision tree algorithm (DT) and an expectation-maximization (EMI) algorithm. We first divide a data set into horizontal segments through applying a DT algorithm such as C4.5, and then apply an EMI algorithm on each segment in order to impute the missing values belong to the segment. If all numerical attribute values of a record are missing then we impute them by the mean values of the attributes of the records belong to a segment where the record falls in, and thereby reduce the computational time complexity of iDMI compare to an existing technique called DMI which calculate the mean value of an attribute by using all records of a data set. We evaluate the performance of iDMI over three high quality existing techniques on two real data sets in terms of four evaluation criteria. Our initial experimental results, including several statistical significance analysis, indicate the superiority of iDMI over the existing techniques.
[Computers, Correlation, expectation-maximization algorithm, data mining, DT algorithm, C4.5 algorithm, Remuneration, Information technology, data cleansing, Accuracy, decision tree, Electromagnetic interference, iDMI technique, decision trees, expectation-maximisation algorithm, statistical significance analysis, Data pre-processing, Decision Trees, Decision trees, EM algorithm, missing value imputation]
Heart function determination by analyzing sympathetic and vagal activity of ECG signal after having energy drinks
16th Int'l Conf. Computer and Information Technology
None
2014
In this work, sympathetic and vagal activity are the two component of heart rate variability (HRV) measurements of ECG signal were analyzed to determine the effect of having energy drinks on heart function by observing 12 healthy human subjects. After 6 min observation in both conditions it was found that after having energy drinks sympathetic activity was increased and vagal activity was decreased. From the previous research it is shown that heart rate is increased by slow acting sympathetic activity or decreased by fast acting parasympathetic (vagal) activity. Significant differences of sympathetic and vegal activities in both condition i.e., before and after having drinks are same i.e., 0.0045. So that to determine the effect of having energy drinks on heart function sympathetic-vagal balance was analyzed by the two component of sympathetic and vagal activity. It was observed that sympathetic-vagal balance was increased that indicates R-R interval increased as well as decreases the heart rate. Raw RR techogram also used to analyze the interval of RR. After having energy drinks a significant change also observed in sympathetic-vagal balance.
[Computers, R-R interval, heart function determination, HRV measurement, heart rate reduction, raw RR techogram, sympathetic-vagal balance analysis, electrocardiography, heart rate variability measurement, parasympathetic activity, heart rate variability, Electrocardiography, ECG signal, energy drink effect, energy drinks, sympathetic activity analysis, time 6 min, Indexes, RR tachogram, Information technology, sympathetic-vagal balance, vagal activity analysis, AcqKnowledge software, neurophysiology, Heart rate variability, RR interval analysis]
Handwritten Bangla digit recognition employing hybrid neural network approach
16th Int'l Conf. Computer and Information Technology
None
2014
Handwritten Bangla digit recognition is one of the most attractive area for researchers who have interest in image processing and pattern recognition field. In our everyday activities like bank check identification, passport and document analysis, number plate identification and especially in our postal automation service, recognition of handwritten digits plays a significant role. That's why a rich body of literature already has been published in this area. But most traditional techniques are generally based on complex feature extraction approach that introduces a great overhead in recognition tasks. Recently a novel approach Back-propagation algorithm is used for recognition which simplifies the recognition process but the main drawback is, the network takes lots of iterations to converge. This paper addresses a faster and efficient Hybrid Neural Network Solution called (BAM+BPNN) which is a combination of Bidirectional Associative Memory and Back-Propagation Neural Network. BAM is used for dimensional reduction and BPNN is used to train the neural network with the set of input patterns for acquiring separate knowledge of each digit. This research can take a decision that Hybrid Neural Network algorithm (BPNN with BAM) takes less iteration to train and less time to recognize digits than Back-propagation algorithm (BPNN). Experimental study shows the effectiveness of our proposed technique.
[image processing, Image recognition, handwritten character recognition, Neurons, Back-propagation, bidirectional associative memory, backpropagation neural network, complex feature extraction, Biological neural networks, BAM, Training, Handwritten Bangla Digit, Handwriting recognition, BPNN, feature extraction, handwritten Bangla digit recognition, backpropagation, hybrid neural network, Hybrid Neural Network, natural languages, dimensional reduction, neural nets, content-addressable storage, pattern recognition]
A distributed SCTP scheme for bandwidth aggregation
16th Int'l Conf. Computer and Information Technology
None
2014
Bandwidth aggregation is a popular research topic that enables users to use different kind of network interfaces connected with Internet together. This is very useful especially in modern smart phones, where multiple interfaces can be used not only to aggregate bandwidth, but also to use an uninterrupted network connection. For different reasons, service providers may not be able to provide service all places according to customers' need. Also, Line may be dropped for mobility. Considering all those factors, we have developed an algorithm that extends basic Stream Control Transmission Protocol (SCTP) to aggregate bandwidths of multiple interfaces. In this paper, we have simulated the effects of different related parameters using C programming language and measured and demonstrated performance of the suggested algorithm graphically.
[Computers, Protocols, SCTP protocol, multipath data transfer, Throughput, distributed SCTP scheme, SCTP, C language, Information technology, network interface, Aggregates, transport protocols, Bandwidth, C programming language, multihoming, Data transfer, bandwidth aggregation, multistreaming, Internet]
University course advising: Overcoming the challenges using decision support system
16th Int'l Conf. Computer and Information Technology
None
2014
Universities require a fast and reliable system to provide academic advising to its students, register them into different courses, and to manage change requests. Currently in Bangladesh, academic advising is done mostly on paper, as seen in many of the public and private universities. In this paper, we propose a unique online-based system that would make the university course advising and registration process easier and intelligent. The proposed online tool will work very closely and interdependently with each educational institution's own course curriculum, grading system, and university policies. The intention of this paper is to find key challenges in the system and to evaluate the efficiency of an easy-to-read web-based course advising and decision support tool. The tool expedites the academic advising process. It acts as an online advisor for the students, eases the task and save time of the advisors by automation of redundant tasks and reducing paper work. The recommended advising software through its decision support system aids both students and teachers with unique facilities. It also provides additional time for advisors to focus on students' development.
[Decision support systems, Computers, Schedules, grading system, university registration process, educational administrative data processing, Registers, Servers, easy-to-read Web-based course advising tool, university policies, Educational institutions, online-based system, decision support systems, advising software, online academic advising, university course advising, course curriculum, educational courses, decision support system, Software, educational institutions, Internet, course registration automation, educational institution, academic advising process]
Bangladeshi style: A way of facial artistic stylization in visual effects
16th Int'l Conf. Computer and Information Technology
None
2014
In computer graphics field, stroke-based painterly rendering (SBPR) is the program to convert photographs into paintings, such as oil painting, watercolor and sketches. Beyond specific techniques to render real photographs, major challenge is how to understand real photographs as human visual perception and cognition according to artists' painting motivation, which parts should be drawn and how to draw strokes. Before starting to draw with pigments, real artists commonly draw a sketch using simple lines to mark out final stroke layouts which navigate to control stroke drawing trajectories and poses. This can be treated as a blue print in their mind. In SBPR, it is also a powerful tool to guide strokes configuration. In this paper, we propose a image-based method to extract this blue print called guidelines (GL). We generate GL by combining color gradient and morphological structure of real photographs. In the experiment, we apply our GL into Bangladeshi traditional mask automatic animation generation with webcam video. The results show that our method is sufficient and efficient to assist SBPR generation.
[Computers, stroke drawing trajectory, photograph conversion, Ink, color gradient, image-based method, stroke layout, computer animation, Bangladeshi traditional mask automatic animation generation webcam video, facial artistic stylization, artist painting motivation, guidelines, Trajectory, image colour analysis, Face, oil painting, rendering (computer graphics), human visual perception, stroke-based painterly rendering, blue print extraction, photograph rendering, human cognition, Painting, Bangladeshi style, watercolor, stroke configuration, GL extraction, visual effects, computer graphics field, Rendering (computer graphics), Feature extraction, artistic stylization, sketch, Stroke-based painterly rendering, morphological structure, SBPR generation]
Analyzing QRS complex, ST segment and QT interval of ECG signal to determine the effect of having energy drinks on hypercalcaemia
16th Int'l Conf. Computer and Information Technology
None
2014
In this work, the components of ECG signal are QRS complex, QT interval and ST segment were analyzed to determine the effect of having energy drinks on hypercalcaemia by observing 12 healthy human subjects. The reasons for occurring hypercalcaemia which are analyzed here are: absence of the ST segment, wider QRS complex and shortened of QT interval. The normal interval for QRS complex is 0.04 to 0.12 seconds. After having energy drinks it is observed that there is no change happen at the interval level of QRS complex. It is cleared by the significant different which shows &#x201C;ns&#x201D; that means significant different does not occur at these conditions i.e., before and after having energy drinks. Also the average value of QRS complex is also smaller than 0.1 s. From this observation it is shown that one of the condition for hypercalcaema is the absence of the ST segment which is not present here i.e., ST segment is present. A little change occurs in the QT interval but it does not give any effect on the abnormality of Hypercalcaemia.
[Heart, Computers, QT interval, beverages, energy drinks, Calcium, Information technology, Blood, electrocardiography, QRS complex analysis, hypercalcaemia, Temperature measurement, medical disorders, AcqKnowledge software, Electrocardiography, ST segment, ECG signal components, QRS complex]
Security risk modelling using SecureUML
16th Int'l Conf. Computer and Information Technology
None
2014
Several security modelling languages (e.g., Misuse case, Secure Tropos) help dealing with security risk management at the system requirements stage. But no design level modelling language has been explored to model security risk. In this paper, we are focusing on SecureUML which is a design level modelling language to represent security risk. More specifically we investigate how SecureUML supports information systems security risks management (ISSRM). The outcome of this work is an alignment table between SecureUML language constructs to the constructs of the ISSRM domain model. We ground our analysis on the number of illustrative examples. We hope that our results will help developers to understand how they can consider security risks at the system design stage. It also indentifies the shortcomings of SecureUML to model security risk and provides recommendations for improvement.
[design level modelling language, Unified modeling language, Security risk modelling, information system security risks management, security risk representation, Security, formal specification, Authorization, SecureUML, Computer hacking, SecureUML language construct, security risk modelling, security modelling language, security risk management, system requirements, risk management, Unified Modeling Language, Computational modeling, alignment table, ISSRM domain model, Misuse case, system design, Secure Tropos, security of data, Risk management]
Machine understandable information representation of geographic related data to the administrative structure of Bangladesh
16th Int'l Conf. Computer and Information Technology
None
2014
The heterogeneous data on the web related to the geographic information of a country is an increasingly important field of data sharing and integrating with diverse sources for retrieving suitable information using search engines. These heterogeneous geographic data is mostly available in unstructured or semi-structured format. Converting and integrating these data into a machine understandable representation is a challenging task and is getting researchers' attention at a rapid pace. In this regard, we described geographic data for administrative structure of Bangladesh which integrated all resources and instances by comprising of their concepts and relations, coined a repository name Geo-Bangladesh by rich semantic Resource Description Framework (RDF). Then we utilized our Geo-Bangladesh with one more knowledge repository Bangladeshi-Citizen to achieve the semantic interoperability in our application. Furthermore, we performed SPARQL query operations on our proposed semantic knowledge repository to retrieve and inference specific information that shows its usability and effectiveness through the adequate quantitative results, both in term of concepts and geographical entities of the administrative structure of Bangladesh.
[Vocabulary, semantic interoperability, search engines, open systems, information inference, Ontologies, semistructured format data, geographic information systems, Resource description framework, SPARQL query operations, knowledge management, Interoperability, heterogeneous geographic data, semantic resource description framework, Databases, semantic knowledge repository, Semantics, geographic information, RDF, Bangladeshi-Citizen, information retrieval, geographical entities, public administration, Internet, Bangladesh administrative structure, machine understandable information representation, data sharing, Geo-Bangladesh]
Breast cancer classification from ultrasonic images based on sparse representation by exploiting redundancy
16th Int'l Conf. Computer and Information Technology
None
2014
We present a Sparse Representation-based Classifier (SRC) that provides superior performance in terms of high Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) in classifying benign and malignant breast lesions captured in ultrasound images. Although such a classifier was proposed for face recognition, it has been proposed in medical diagnosis from ultrasonic images in this work for the first time. The classifier is based on &#x2113;<sub>1</sub>-norm based sparse representation of a patient's test data in terms of linear combination of the features of the benign and malignant test lesions available in the training set. The proposed classifier introduces an index called Sparsity Rank (SR) for the classification obtained from the normalized energy of the weights as a linear combination of the global sparse representation of the ultrasound images of the training set. The performance of the classifier is further enhanced to a great extent by two ways: first, by intelligently combining the features extracted from the multiple ultrasound scan of the same mass, and the second, by using the optimal feature set obtained by a suboptimal strategy that avoids the time exhaustive brute force approach that has a combinatorial search space. With all the enhancements an AUC of 0.9802 has been achieved, when training and testing sets are chosen by leave-one-out approach from the data set.
[global sparse representation, ultrasound scan, image classification, Noise, sensitivity analysis, SRC, biomedical ultrasonics, normalized energy, sparsity rank, suboptimal strategy, area under the ROC curve, AUC, Training, Ultrasonic imaging, benign breast lesions classification, feature extraction, ultrasonic images, redundancy, Lesions, receiver operating characteristic, ultrasound images, medical image processing, breast cancer classification, patient test data, malignant breast lesions classification, &#x2113;1-norm based sparse representation, medical diagnosis, image representation, Breast, sparse representation-based classifier, cancer, optimal feature set, ultrasonic imaging, Cancer]
Automatic Slice Growing Method based 3D reconstruction of liver with its vessels
16th Int'l Conf. Computer and Information Technology
None
2014
In the recent years, reconstructing 3D liver and its vessels from abdominal CT volume images becomes an inevitable and necessary research field. In this paper, a method of 3D reconstruction of liver with its vessels has been implemented, which involves volume preprocessing, de-noising, segmentation, contouring, and combination of different modalities. An advanced liver segmentation algorithms have been proposed: the first one is a 2.5D method that utilizes automatic Slice Growing Method (SGM) to segment liver part of each slice of a data set. It takes advantage of curvature control of level set segmentation method to distinguish liver and adjacent organs. It is proved that the result of this proposed method is much better than simple 3D level set method in liver segmentation. In the case of liver vessel segmentation, we have proposed an improved smoothing method dedicate to 3D vascular volume which results from region growing segmentation method. The cooperation of region growing method and proposed smoothing method has been demonstrated the possibility of efficient vessel segmentation with very accurate results. And the results indicate that our method is suitable for anatomical studying and surgical planning.
[Shape, Level set, level set segmentation method, MRI, Liver, abdominal CT volume image, image contouring, 3D liver vessels, smoothing method, Liver Segmentation, Three-dimensional displays, 3D reconstruction, 2.5D method, Computed tomography, image segmentation, 3D level set, surgical planning, liver segmentation algorithm, curvature control, medical image processing, Biomedical imaging, liver, 3D vascular volume, Vessel Segmentation, automatic slice growing method, blood vessels, image reconstruction, image denoising, volume preprocessing, Image segmentation, CT, computerised tomography, anatomical studying, region growing segmentation, SGM]
Information piracy and data incongruity prevention in WCMS to WCMS conversion procedure using precise information hiding and data integrity manager
16th Int'l Conf. Computer and Information Technology
None
2014
This paper addresses the rapidly increasing issue of sensitive user data theft and the data incongruity due to Web Content Management System (WCMS) structure and very complex WCMS to WCMS conversion procedure. After retaining the full picture of the situation, the paper approaches a solution for securely accessing, modifying and migrating WCMS database(DB) using a precise information hiding and data integrity manager named Data Security Processor Software Unit (DSPSU) without compromising the sensitive data to fraudsters.
[Encryption, Servers, content management, WCMS structure, End User Right, Databases, Web content management system, Database, WCMS, secure migration, Open-Source, secure access, secure modification, Content management, data integrity manager, DSPSU, Data Security Processor Software Unit, data integrity, WCMS-to-WCMS conversion procedure, Information technology, information piracy prevention, security of data, data incongruity prevention, precise information hiding, fraudsters, sensitive user data theft, sensitive data, data privacy, Internet, WCMS database, data encapsulation, Web Security]
Linked open data representation of historical heritage of Bangladesh
16th Int'l Conf. Computer and Information Technology
None
2014
The historical heritage is a vast domain that generates distributed and heterogeneous data repositories in the existing World Wide Web (WWW). The integration of these heterogeneous data repositories and retrieving data from WWW using traditional search engine is a challenging task. In this regard, our research is to demonstrate the feasibility of semantic web technology for creating machine understandable description of entities associated with the creation and maintenance of historical heritage of Bangladesh. Moreover, our proposed approach uses the essence of semantic knowledge base composed of real historical heritage data that are interlinked with data from the Geo-Bangladesh, a machine understandable Linked Open Data (LOD) repository of geographic related data to the administrative structure of Bangladesh and retrieve specific information using semantic search tools like SPARQL. We perform a number of experiments with SPARQL to retrieve and inference specific information that shows usefulness of our proposed Linked Open Data on historical heritage of Bangladesh.
[Vocabulary, semantic Web, search engine, Knowledge based systems, historical heritage, information retrieval, Ontologies, Educational institutions, World Wide Web, Resource description framework, history, heterogeneous data repository, linked open data representation, semantic knowledge base, Bangladesh, semantic Web technology, SPARQL tool, Semantics, WWW, Search engines, LOD repository, data handling, Geo-Bangladesh]
Numerical study of a loaded U-antenna for 3.5 GHz mobile WiMAX and 7.5 GHz military satellite communication applications
16th Int'l Conf. Computer and Information Technology
None
2014
This paper presents the numerical simulation of a low-profile dual frequency loaded U-antenna for 3.5 GHz WiMAX and 7.5 GHz military satellite communication applications. The antenna has a very compact size of 23.5&#x00D7;14 mm2 and provides a bandwidth of 340 MHz (3.4 GHz-3.74 GHz) and 570 MHz (7.23 GHz-7.8 GHz) in the two different resonant modes. The antenna provides maximum gain of 7.51 and 3.14 dBi respectively at S<sub>11</sub> &lt;; -10 dB. It also provides almost omnidirectional radiation characteristics and excellent impedance matching in the operating bandwidth.
[microwave antennas, bandwidth 340 MHz, WiMAX, Mobile antennas, WiMax, low-profile dual frequency loaded U-antenna, Broadband antennas, mobile satellite communication, military satellite communication application, Loaded antennas, impedance matching, omnidirectional radiation characteristic, Loaded-U antenna, mobile WiMAX, antenna radiation patterns, omnidirectional antennas, Numerical Electromagnetic Code (NEC), military communication, multifrequency antennas, bandwidth 570 MHz, frequency 3.5 GHz, Gain, antenna gain, frequency 7.5 GHz]
Automated detection of optic disc and blood vessel in retinal image using morphological, edge detection and feature extraction technique
16th Int'l Conf. Computer and Information Technology
None
2014
Reliable, fast and efficient optic disc localization and blood-vessel detection are the primary tasks in computer analyses of retinal image. Most of the existing algorithms suffer due to inconsistent image contrast, varying individual condition, noises and computational complexity. This paper presents an algorithm to automatically detect landmark features of retinal image, such as optic disc and blood vessel. First, optic disc and blood vessel pixels are detected from blue plane of the image. Then, using OD location the vessel pixels are connected. The detection scheme utilizes basic operations like edge detection, binary thresholding and morphological operation. This method was evaluated on standard retinal image databases, such as STARE and DRIVE. Experimental results demonstrate that the high accuracy achieved by the proposed method is comparable to that reported by the most accurate methods in literature, alongside a substantial reduction of execution time. Thus the method may provide a reliable solution in automatic mass screening and diagnosis of the retinal diseases.
[blood-vessel detection, blood vessel, landmark feature, binary thresholding, Retina, STARE, retinal recognition, DRIVE, retinal disease diagnosis, feature extraction technique, OD location, standard retinal image database, feature extraction, retina, morphological technique, edge detection, automated detection, automatic mass screening, fundus image, medical image processing, Biomedical imaging, optic disc localization, Image edge detection, blue plane detection, Blood vessels, blood vessels, Optical imaging, diseases, optic disc, morphological operation, diabetic retinopathy, blood vessel pixel, Image segmentation, image contrast, edge detection technique, Feature extraction, computational complexity]
Selection of proper frequency band and compatible features for left and right hand movement from EEG signal analysis
16th Int'l Conf. Computer and Information Technology
None
2014
Electroencephalogram (EEG) signal plays an important role in the field of brain-computer interface (BCI) which has diverse applications ranging from medicine to entertainment. BCI acquires brain signals, extracts informative features and translates these features into a control signal for an external device. The Purpose of this work is to select proper frequency band and to extract suitable features for left and right hand movements from EEG signals analysis. The Discrete Wavelet Transform (DWT) is used to extract different significant features, which separates Alpha, Beta and Theta band of frequencies of the EEG signal. Extracted EEG features of different bands are classified using an artificial neural network (ANN) trained with the back propagation (BP) algorithm. The classification rate shows that Beta band (97.5%) has higher mapping precision and better convergence rate than the other bands, alpha (93.2%) and theta (87.8%). Finally an ANN self-organizing feature mapping (SOFM) is used to find the compatible feature for EEG bands related to hand movement. SOFM analysis shows that approximate entropy (ApEn) for theta band and scale variance for alpha and beta band can be used as compatible feature. The results of this study are expected to be helpful in brain computer interfacing.
[artificial neural network, ANN, alpha band, theta band, brain-computer interfaces, Electroencephalogram, Feature Extraction, Electroencephalography, feature extraction, EEG signal analysis, Discrete Wavelet Transform, DWT, electroencephalography, backpropagation algorithm, discrete wavelet transforms, brain-computer interface, Artificial neural networks, BCI, discrete wavelet transform, Discrete wavelet transforms, Biological neural networks, electroencephalogram, medical signal processing, mapping precision, Self-Organizing Feature Mapping, left-and-right hand movement, Artificial Neural Network, beta band, backpropagation, Feature extraction, convergence rate, neural nets, frequency band]
A formal approach to verify software scalability requirements using set theory and Hoare triple
16th Int'l Conf. Computer and Information Technology
None
2014
Scalability is the ability of a system to handle variation in execution environment and continuing to function in order to meet user needs. For ensuring scalability, it is important to verify that programmers are writing code that can scale. However, verifying scalability from code level has its own limitations as it did not receive adequate attention from researchers. This paper proposes a formal approach to verify scalability from the code level using set theory and Hoare triple. The method denotes variables and functions involving scalability through set notations. Hoare triple is used to measure the performance fulfillment with varying workload by a code segment given that certain code quality measure like caching or data compression is applied. The methodology is presented by means of an algorithm which strictly inhibits to passover a specific scalability requirement and requires to re-apply a quality measure until a specific requirement is being satisfied. The approach is applied for developing a real life online ticketing system and results show that it provides stable response time over a wide range of user requests. This indicates that the proposed approach is capable of ensuring scalability by verifying it from system's code.
[Computers, user requests, program verification, Scalability, performance fulfillment measurement, Data compression, set theory, software quality, code level, set notations, Servers, user needs, caching, formal approach, execution environment, code segment, workload variation, code quality measurement, data compression, real life online ticketing system, Hoare triple, Formal Method, Verification, variation handling, Hoare Triple, Information technology, Software Scalability, software scalability requirements verification, response time, Set theory, Time factors, Set Theory]
A helper initiated distributed cooperative Medium Access Control (MAC) protocol for wireless networks
16th Int'l Conf. Computer and Information Technology
None
2014
Spatial diversity for wireless transmission requires more than one antenna at the transmitter. However, mobile devices are usually limited by size, so installation of multiple antennas increases the hardware complexity significantly. Due to the omnidirectional nature of wireless signal, a data transmission between a source node and a destination node can be overheard by many other neighbor nodes. By exploiting this characteristic, a number of recent research activities on cooperative Medium Access Control (MAC) have been devised where low data rate stations are assisted by the high data rate stations in forwarding data traffics. Therefore, wireless devices with a single antenna can effectively form a virtual array of antennas by sharing each other's' antennas in a multiuser environment. To facilitate cooperative communication in the data link layer requires a great deal of attention. In designing a cooperative MAC, selection of relay stations is the most important criterion. In this paper, we propose a distributed cooperative MAC protocol where a potential relay node initiates itself to participate in the cooperation by calculating supported data transmission rate between source to relay and relay to destination links. A mathematical analysis of our proposed scheme is derived and throughput of the proposed scheme is then compared with that of the existing IEEE 802.11 DCF MAC. Numerical results show that our proposed scheme can increase the throughput of any IEEE 802.11 low data rate station comprehensively.
[IEEE 802.11 DCF, telecommunication links, wireless signal omnidirectional nature, CTS, Helper Initiated Medium Access Control (MAC) Protocol, Throughput, access protocols, Relays, IEEE 802.11 DCF MAC protocol, wireless transmission, Wireless communication, RTS, relay node, antenna arrays, CoopMAC, Data communication, wireless network, multiple antenna array, transmitter, multiuser environment, relay networks (telecommunication), high data rate station, relay station selection, IEEE 802.11 Standards, Receivers, helper initiated distributed cooperative medium access control protocol, data link layer, mathematical analysis, Cooperative Networking, MAC, cooperative communication, mobile device, data traffic forwarding, omnidirectional antennas, data transmission, spatial diversity, Media Access Protocol, data communication, radio transmitters, wireless LAN, telecommunication traffic]
Bangla news classification using naive Bayes classifier
16th Int'l Conf. Computer and Information Technology
None
2014
Web is gigantic and being constantly update. Bangla news in web are rapidly grown in the era of information age where each news site has its own different layout and categorization for grouping news. These heterogeneity of layout and categorization can not always satisfy individual user's need. Removing these heterogeneity and classifying the news articles according to user preference is a formidable task. In this paper, we propose an approach that provides a user to find out news articles which are related to a specific classification. We use our own developed web crawler to extract useful text from HTML pages of news article contents to construct a Full-Text-RSS. Each news article contents is tokenized with a modified light-weight Bangla Stemmer. In order to achieve better classification result, we remove the less significant words i.e. stop - word from the document. We apply the naive Bayes classifier for classification of Bangla news article contents based on news code of IPTC. Our experimental result shows the effectiveness of our classification system.
[Computers, Bangla news article content classification, pattern classification, text analysis, Dictionaries, Web crawler, Taxonomy, IPTC news code, information retrieval, Full-Text-RSS, Vectors, useful text extraction, Bangla Stemmer, HTML pages, Information technology, news site, Training, naive Bayes classifier, Layout, user preference, Bayes methods, Internet, Web sites, news grouping]
A smart user interface for efficient resource allocation for arsenic mitigation in Bangladesh
16th Int'l Conf. Computer and Information Technology
None
2014
Arsenic contamination of groundwater in many nations including Bangladesh shows that this is a global problem. Because of the delayed health effects, poor reporting, and low levels of awareness in some communities, the extent of the adverse health problems caused by arsenic in drinking-water is at alarming level in Bangladesh. Also, allocating resources such as tube wells efficiently and effectively to mitigate arsenic hazard is a challenging task in Bangladesh. To allocate resources based on different arsenic hazard parameters, we have developed a Decision Support System that enables the user to observe the effect of allocation policy both in tabular and spatial format using statistical models. We have also developed an algorithm for optimal allocation of resources. A Smart User Interface is designed for the users so that they will find an interactive, user-friendly, intelligible, logical, clear, and sound environment to work with. Finally, we have analyzed and demonstrated the efficacy of our algorithm graphically.
[Decision support systems, smart user interface, Decision Support System, Hazards, groundwater, Electron tubes, user interfaces, Indexes, Vulnerability Index, decision support systems, arsenic mitigation, Geographic Information System, contamination, Bangladesh, resource allocation, Sociology, environmental science computing, arsenic contamination, decision support system, statistical models, User interfaces, Arsenic hazard, Water pollution, statistical analysis]
Proactive approach of making eye contact with the target human in multi-party settings
16th Int'l Conf. Computer and Information Technology
None
2014
Establishing eye contact with the target agent plays a central role both in human-human and human-robot communications. However, it is not so easy task for the robot to meet eye contact with a particular human. Especially, when the robot and the target human are facing each other initially or the target human is intensely involved his/her task. In this paper, we proposed a conceptual model of eye contact for social robots consisting of three parts: attracting attention, establishing gaze crossing, and displaying gaze awareness. Evaluation experiments with 36 participants reveal the effectiveness of the proposed model in three viewing situations (near peripheral field of view, far peripheral field of view, and out of field of view).
[Computers, human-robot interaction, human-robot communication, proactive approach, target human, social robots, gaze crossing, Magnetic heads, far peripheral field of view, gaze awareness, gaze tracking, near peripheral field of view, Human-robot interaction, human-human communication, Robot sensing systems, Cameras, humanoid robots, out of field of view, Face, eye contact, multiparty settings]
Design and implementation of an FPGA-based realtime face recognition system using spatial correlation function
16th Int'l Conf. Computer and Information Technology
None
2014
This paper presents a simple and efficient design of a face recognition system, where feature extraction algorithm is employed based on the principle of spatial cross-correlation. In the feature extraction process, instead of processing the entire image at a time, only a pair of rows or columns of an image is considered which makes the algorithm very efficient and low-cost. Considering the cross-correlations between the pairs, a unique 1-D signature of a 2-D face image is obtained which represents the variation in the face geometry along the vertical or horizontal direction. It is shown that the resulting vertical and horizontal features provide high compactness within the class and separation between the classes. It is found that the proposed design can provide a satisfactory recognition performance for different standard databases. The results of hardware implementation in terms of resources used and processing speed have also been presented.
[Spatial feature extraction, Face recognition, field programmable gate arrays, FPGA, Vectors, spatial cross-correlation function, feature extraction algorithm, Cross-Correlation, Face geometry, Accuracy, face geometry variation, feature extraction, Support vector machine classification, Classification, Verilog, face recognition, Feature extraction, Hardware, FPGA-based realtime face recognition system, Face]
An improved MaxProp based on neighborhood contact history for Delay Tolerant Networks
16th Int'l Conf. Computer and Information Technology
None
2014
Delay Tolerant Networks (DTNs) are intermittently connected mobile networks, in which a fully connected path from source to destination does not exist. Therefore in these networks, message delivery relies on opportunistic routing where nodes use store-carry-and-forward paradigm to route the messages. However, effective forwarding based on a limited knowledge of contact behavior of nodes is challenging. In some routing algorithms, (such as Prophet [1], MaxProp [2]) the messages are forwarded to nodes which has higher chance to meet the destination. The MaxProp was designed for vehicle based DTN. It has some drawbacks. For instance, during normalization step of updating delivery probability, it drastically decrements the delivery probability for the nodes corresponding to older encounters. On the other hand, it increments one node's delivery probability, corresponding to the latest encounter to a great extent. This procedure of updating delivery probability can suffer from non-optimal routing decisions. In this paper we propose a new technique for updating delivery likelihood based on neighborhood contact history. For the simulation, we have used Opportunistic Network Environment (ONE) Simulator. The simulation shows that our proposed technique performs much better than original MaxProp.
[mobile radio, MaxProp algorithm, Prophet algorithm, Routing, Mobile communication, delay tolerant networks, mobile networks, Indexes, nonoptimal routing decision, store-carry-and-forward paradigm, opportunistic network environment simulator, delivery likelihood, routing algorithm, telecommunication network routing, neighborhood contact history, Routing protocols, Peer-to-peer computing, Delay-tolerant network, Mobile computing, DTN, The ONE Simulator]
The mCard approach for Bangladesh: A smart phone based Credit/Debit/ATM card
16th Int'l Conf. Computer and Information Technology
None
2014
Mobile payments are a natural evolution of e-payment schemes that will facilitate mobile commerce. A mobile payment or m-payment may be defined, as any payment where a mobile device is used to initiate, authorize and confirm an exchange of financial value in return for goods and services. Mobile devices may include mobile phones, PDAs, wireless tablets and any other device that connect to mobile telecommunication network and make it possible for payments. In Bangladesh, several private banks (Dutch-Bangla Ltd, Islami Bank Ltd, Trust Bank Ltd and Others) introduced mobile banking. The BRAC Bank-initiated mobile banking service, &#x201C;bKash&#x201D;, is at present the country's leading service-provider in mobile banking. This paper present a new m-payment system called &#x201C;mCard&#x201D; replacing existing Credit or Debit or ATM card and includes some feature of mobile banking (except money transfer to remote area).
[Mobile Payment, mCard, mobile phones, smart cards, mCard approach, Mobile communication, wireless tablets, Security, PDA, Dutch-Bangla Ltd, Apps, banking, electronic money, NPS, BRAC bank-initiated mobile banking service, mobile computing, e-payment scheme, Islami Bank Ltd, private banks, m-payment system, mobile telecommunication network, bkash, money transfer, financial value, Online banking, mobile payments, bKash, Banking, smart phone, smart phones, credit transactions, mobile commerce, Android, debit card, Bangladesh, Smart Phone, Trust Bank Ltd, POS, Credit/Debit Card, ATM card, debit transactions, ATM, Internet, credit card, Smart phones]
Structural similarity based disparity estimation for stereoscopic images
16th Int'l Conf. Computer and Information Technology
None
2014
Depth estimation of a scene from stereo images is an active area of research in computer vision applications. In this paper, a simpler and faster technique for depth estimation, based on structural similarity index (SSIM) method, is proposed. The technique provides a simple block based similarity measure that considers the structural similarity between the corresponding blocks of the left and right views. It avoids explicit calculations in depth or disparity estimation. The method shows that the estimation performance is acceptable and it can be used in autonomous navigation and 3D media applications.
[Computers, similarity measure, SSIM, Stereo image processing, Estimation, depth estimation, Indexes, Information technology, structural similarity index, stereoscopic images, Image coding, disparity, computer vision, quality assessment, stereo image processing, block based similarity measure, SSIM method, Depth estimation, disparity estimation]
Bangla phonetic feature table construction for automatic speech recognition
16th Int'l Conf. Computer and Information Technology
None
2014
This This research constructs a phonetic feature (PF) table for all the phonemes pronounced in Bangla (widely known as Bengali) language where the whole study is divided into two parts. In the first part, a PF table is constructed, while the second part deals with Bangla automatic speech recognition (ASR) using PFs. For Bangla language, fifty three phonemes including both vowels and consonants are considered in which the phones, k (/s/) and m (/s/), and, Y (/n/) and b (/n/) contain approximately same spectrum and hence, they share same PFs. In the PF table, twenty two PFs (Silence, Short Silence, Stop, ...) are required for representing all the Bangla phonemes. On the other hand, the second part comprised of three stages: i) first stage deals with acoustic features, mel frequency cepstral coefficients (MFCCs) extraction, ii) second stage embeds PFs extraction procedure using a multilayer neural network (MLN) and iii) the final stage integrates a triphone-based hidden Markov model (HMM) for generating the output text strings by inputting log values of twenty two dimensional PFs. In the experiments on Bangla Newspaper Article Sentences, it is observed that the PF-based ASR system provides higher word correct rate, word accuracy and sentence correct rate in comparison with the standard MFCC-based method.
[Computers, embeds PFs extraction procedure, hidden Markov model, multilayer neural network, output text string generation, automatic speech recognition, Bengali language, MLN, hidden Markov models, Bangla newspaper article sentences, sentence correct rate, speech recognition, PF-based ASR system, cepstral analysis, PF table, Bangla language, phonemes, multilayer perceptrons, MFCC extraction, natural language processing, mel frequency cepstral coefficient, Bangla automatic speech recognition, HMM, Information technology, Standards, Bangla phonetic feature table construction, word correct rate, mel frequency cepstral coefficient extraction, triphone-based hidden Markov model, acoustic features, Hidden Markov models, Speech recognition, phonetic feature, Speech, Feature extraction, word accuracy]
A faster approach to periodic data flipping of SRAM array for NBTI recovery
16th Int'l Conf. Computer and Information Technology
None
2014
Negative Bias Temperature Instability (NBTI) is a prime reliability issue for micro and nano-scale semi-conductor devices. Due to continuous device scaling, NBTI effect has become more severe than before and affected device life-time. Memory devices like SRAMs are tremendously affected by NBTI as the PMOS transistor gate is connected at `0' logic for a long time. Several device-level and architecture-level solutions have been proposed to improve device life-time by interrupting NBTI degradation. Such an architectural level solution is to flip data in a particular SRAM cell after a certain time, causing periodic stress and relaxation. SRAM data flipping techniques proposed so far are not so time friendly as it is needed to access each memory cell individually and flip the data stored. It makes the process more time consuming and inconvenient for present ultra-fast system with high activity factor. In this paper, we proposed a new 7-T SRAM cell to allow flipping data of more than one memory cells at same clock pulse, hence decreasing the flipping time of entire memory array and concluded that with the new proposed cell and flipping procedure, data flipping of the entire memory array will become much faster which will ensure convenient NBTI recovery.
[7-T SRAM cell, MOSFET, device lifetime, Fast Flipping Time, periodic data flipping, Switches, NBTI degradation, periodic stress, negative bias temperature instability, SRAM array, NBTI recovery, SRAM, stress relaxation, Cell Data Flipping, 7-T SRAM Cell, Microprocessors, SRAM chips, SRAM data flipping techniques, SRAM cells, Arrays, NBTI Degradation]
Face recognition using eyes, nostrils and mouth features
16th Int'l Conf. Computer and Information Technology
None
2014
This paper describes a face recognition algorithm that extracts the eyes, nostrils and mouth features from cumulative distribution function (CDF) by applying Otsu thresholding. The algorithm, which is inspired by the probability of white pixels of binary facial image, has been tested using the BioID frontal face large database in different illuminations, expressions and lighting conditions. Illumination and lighting variations are addressed using a selective equalization technique. The experimental results have confirmed an average recognition rate of 93.55%.
[selective equalization technique, CDF, probability of white pixels, expression conditions, statistical distributions, face recognition algorithm, facial features, Cumulative distribution function, Databases, feature extraction, image segmentation, Lighting, Mouth, face recognition, eye feature extraction, Otsu thresholding, lighting conditions, Face, Facial features, image resolution, illumination conditions, mouth feature extraction, BioID frontal face, Face recognition, Otsu thresolding, binary facial image white pixel probability, cumulative distribution function, Feature extraction, nostril feature extraction]
Performance comparison of constant coefficient and variable coefficient tracking filters in radar detection
16th Int'l Conf. Computer and Information Technology
None
2014
In this paper comparison between constant coefficient and variable coefficient tracking filters for radar has been presented. &#x03B1;&#x03B2;&#x03B3; filter and Kalman filter are discussed as instances of constant coefficient and variable coefficient filters respectively. Both of the filters are sophisticated smoothing and prediction tracking filters which are used in sampled data target trackers. As polynomial predictor linear recursive filters they can construct future position based on present position measurements. For the same target model the mean residual error from both of the filters are presented to compare their relative performance. Noise variance was varied to observe its effect on both filters. Performance of &#x03B1;&#x03B2;&#x03B3; filter becomes deteriorated with increased noise whereas Kalman filter can track a target with high accuracy even with increased noise variance. These clearly illustrate the advantage of using a variable coefficient Kalman filter over a fixed coefficient &#x03B1;&#x03B2;&#x03B3; filter.
[radar detection, Noise, Kalman filter, Radar tracking, sampled data target tracker, tracking filters, Radar tracker, Kalman filters, variable coefficient Kalman filter, radar tracking, recursive filters, Target tracking, Smoothing methods, polynomials, variable coefficient tracking filter, Variable coefficient, polynomial predictor linear recursive filter, residual error, noise variance, Alpha-beta-gamma (&#x03B1;&#x03B2;&#x03B3;) filter, target tracking, Constant coefficient, Transition matrix (&#x03A6;), Acceleration, constant coefficient tracking filter, fixed coefficient &#x03B1;&#x03B2;&#x03B3; filter]
Primitive quantum gate realizations of multiple-controlled Toffoli gates
16th Int'l Conf. Computer and Information Technology
None
2014
Multiple-controlled Toffoli gates are extensively used in quantum and reversible circuits. However, very limited attempt has been made for primitive quantum gate realizations of these gates. In this paper, we present realization of three-qubit Toffoli gate requiring five primitive quantum gates. We then propose realization of four-qubit Toffoli gate requiring 13 primitive quantum gates and one ancilla input. Finally, we propose realizations of (n + 1)-qubit (n &gt; 3 control lines) Toffoli gates using (&#x2308;n/2&#x2309; + 1)-qubit and (&#x230A;n/2&#x230B; + 1)-qubit Toffoli gates and ancilla inputs, which make a trade off between quantum cost and number of ancilla inputs. Our proposed method outperforms the best result so far presented in the literature in terms of quantum cost and number of ancilla inputs.
[Computers, reversible circuits, ancilla inputs, quantum circuits, reversible circuit, Multiple-controlled Toffoli gate, quantum gates, three-qubit Toffoli gate, quantum cost, Information technology, realization of Toffoli gate, quantum gate, primitive quantum gate realizations, Quantum computing, quantum circuit, Logic gates, Nuclear magnetic resonance, Mirrors, multiple-controlled Toffoli gates, Artificial intelligence]
Mobile phone based telemedicine service for rural Bangladesh: ECG
16th Int'l Conf. Computer and Information Technology
None
2014
Cardiovascular diseases such as; heart attack (coronary thrombosis, myocardial infarction), hypertension, stroke, coronary artery disease (CAD), congestive heart failure, peripheral artery disease (PAD) and diabetes create a new concern worldwide due to its harmness. In Bangladesh where majority live in rural areas that lack specialist care, we envision the need for much larger Internet-based telemedicine systems that would enable a large pool of doctors and hospitals to collectively provide healthcare services to entire populations. We propose a scalable, Internet-based architecture for telemedicine (ECG) to integrating multiple hospitals and mobile medical specialists focusing the rural health centers. This system based on the push model. For the necessity of the research at first we conduct a short survey among different health professional to justify the study more effectively.
[hospitals, push model, Internet-based architecture, Mobile communication, Servers, mobile medical specialists, electrocardiography, mobile computing, rural health centers, Telemedicine, Database server, health professional, Electrocardiography, health care, telemedicine, mobile phone based telemedicine service, ECG, cardiovascular diseases, Diseases, EKG shield, healthcare services, ECG measuring units, rural Bangladesh, Internet, Push model, medical computing, Medical diagnostic imaging]
Budget allocation model for the academic library acquisition using data mining technique
16th Int'l Conf. Computer and Information Technology
None
2014
Allocation of budget for acquisition of an academic library is an important initiative to enhance the efficiency of management, to accomplish the objectives ensuring the availability of materials in a library. Processing the circulation data of library, two important dimensions, namely, concentration and connection [1] could be explored among departments and library members. This can make the analysis easier by calculating weights in those two important dimensions to make the decision about budget allocation. This paper analyses the circulation data of North South University and suggests for efficient management and budget allocation of the North South University Library by using the above mentioned metrics.
[library circulation data processing, library members, academic library acquisition, budgeting data processing, data mining, Materials, Circulation Data, North South University Library, library departments, Data mining, Databases, Semantics, concentration dimension, management efficiency enhancement, Knowledge discovery, Libraries, Budget Acquisitions, connection dimension, Computational modeling, material availability, academic libraries, Utilization, budget allocation model, educational institutions, data mining technique, Resource management]
Low complexity offline character recognition of license plate using perspective view
16th Int'l Conf. Computer and Information Technology
None
2014
License plate detection and character recognition have unveiled new possibilities and challenges in the field of intelligent transport system. Numerous algorithms have been proposed regarding license plate localization and tracking, character segmentation and character recognition. License plate character recognition is still an active area of research specially in terms of processing complexity. A handful of feature extraction algorithms including vertical and horizontal histogram, center of gravity, structural characteristics, modified edge maps, image projection, multi zoning, adaptive multi zoning, concavities measurement, gradient directional features, median gradient features, longest-run features and momentum features have been proposed. Neural network followed by ensemble classifier shows recognition rate in the high nineties. However, ensemble classifier based on multi-feature is burdened with computational intricacy and lethargic processing. This paper presents a perspective view based license plate character recognition algorithm that copes with some of the inherent problems regarding character recognition. The proposed algorithm is resilient against slant, skewness and obscurity of image while limiting processing time and memory usage. Our algorithm tracks the principal curvature of the segmented image in terms of magnitude and inter-impulse distance. A simple back propagation neural network offers recognition rate of 93% with faster training and validation time. A thorough analysis of the algorithm and meticulous details of implementation are presented throughout this paper.
[back propagation neural network, image classification, license plate localization, Licenses, low complexity offline character recognition, horizontal histogram, Classification algorithms, gradient directional features, neural network, Training, momentum features, object tracking, concavities measurement, license plate, perspective view, principal curvature, longest-run features, intelligent transportation systems, license plate tracking, vertical histogram, median gradient features, Image segmentation, structural characteristics, multifeature, Feature extraction, intelligent transport system, object detection, adaptive multizoning, optical character recognition, center of gravity, offline character recognition, feature extraction, image segmentation, license plate detection, computational intricacy, perspective view based license plate character recognition algorithm, modified edge maps, ensemble classifier, image colour analysis, feature extraction algorithms, Character recognition, lethargic processing, character segmentation, Neural networks, image projection, backpropagation, curve fitting, neural nets, recognition rate, OCR]
Towards ubiquitous learning tools for computer aided classroom in developing regions
16th Int'l Conf. Computer and Information Technology
None
2014
Education is recognized as one of the most powerful aspects for human development. Scope of education includes dispersal of knowledge, logical thinking, flourishing skills, refining habits, and sharing values. Accordingly, basic or primary education is a fundamental right for every citizen of a country. Studies provide sufficient evidence that individuals lacking primary education are incapable of making sensible choice about their own benefits in personal, social, and economic life. To ensure technology enhanced primary education for every citizen is yet a significant challenge for developing countries. A number of obstacles in the financial, technological, social, and political sector impede the way to exploit the blessings of modern technology in this sector. Keeping all these in mind, we have worked on a cost efficient tool to improve the learning experience in classrooms in developing regions. We have also designed some learning materials that can make learning exciting and enjoyable to the young learners. In this paper we focus on the existing problems in Primary Education system and the causes behind them. Later we present our cost effective equipment; application of which is believed to be effective in improving the scenario.
[Computers, Materials, primary education system, ubiquitous computing, flourishing skill, raspberry pi, learning material, developing country, cost effective equipment, ubiquitous learning tool, Government, computer aided classroom, Educational institutions, developing regions, Information technology, refining habit, logical thinking, sharing value, human development, Games, dispersal of knowledge, computer aided instruction, technology enhanced primary education, learning tool, basic education]
Interplanetary network: A brief introduction
16th Int'l Conf. Computer and Information Technology
None
2014
Interplanetary Network defines the architecture and protocols necessary to permit inter-operation of the Internet residents or systems on earth/spacecrafts with other remotely located Internet resident/systems on other systems/spacecrafts in transit in the hostile and unpredictable environment of space. Inter-networking in such environment requires new techniques other than traditional communication protocols. This paper focuses on the understanding of the current system and protocols used in Interplanetary communication. This paper is mainly based on the study of different layers of CCSDS (Consultative Committee for Space Data Systems) protocol, which is the recommended protocol used by all the Space Communication Organizations including NASA. The resources used for this paper are mainly reports from CCSDS, NASA and JPL. This paper will pave the easier way of studying and understanding of Interplanetary Network. In the CCSDS protocol Stack, Security is shown as a protocol layer. But, the reality is, every layer has its own security concerns. In this paper, we have tried to reflect this idea too.
[spacecrafts, CCSDS protocol, Space Communication Organizations, Protocols, NASA, internetworking, communication protocols, JPL, Aerospace electronics, Consultative Committee for Space Data Systems protocol, space communication links, Security, interplanetary network, Interplanetary Network, Standards, interplanetary communication, Space vehicles, security, protocol, security of data, Space missions, transport protocols, Internet]
Power pre-emphasised OFDM for RSOA modulation in high speed WDM-PONs
16th Int'l Conf. Computer and Information Technology
None
2014
Orthogonal frequency division multiplexing (OFDM) technique is useful for limited bandwidth reflective semiconductor optical amplifier (RSOA) modulation to design high-speed passive optical networks (PONs). However, the capacity improvement of using conventional OFDM format is severely degraded over the fiber transmission due to the presence of RSOA chirp induced RF power fading. Therefore, we proposed a pre-emphasis OFDM for RSOA modulation to enhance the system performance against both RSOA low bandwidth and high chirp parameter. Experimental results showed that the transmission speed is reached up to 17.1 Gb/s at the bit error rate (BER) within forward error correction (FEC) limit over 25 Km fiber span by using commercially available RSOA of bandwidth &lt;; 1 GHz.
[Optical fibers, RSOA modulation, wavelength division multiplexing, Passive optical networks (PONs), Chirp, OFDM, Optical fiber communications, high-speed passive optical networks, forward error correction, Passive optical networks, high speed WDM-PON, bit error rate, BER, power preemphasised OFDM, Orthogonal frequency division multiplexing (OFDM), Optical attenuators, Modulation, Reflective semiconductor optical amplifiers (RSOAs), passive optical networks, orthogonal frequency division multiplexing technique, bandwidth reflective semiconductor optical amplifier modulation, OFDM modulation, RSOA chirp induced RF power fading, error statistics]
Evaluation of different SVM kernels for predicting customer churn
2015 18th International Conference on Computer and Information Technology
None
2015
Churn prediction has been emerging as an essential research area in the commercial sector, specially in telecommunication business. Predicting future churners and retaining them through campaigning is a crucial job in this competitive telecommunication market. Although, several classical procedures exist for predicting churn, but recently Support Vector Machines (SVMs) are gaining popularity for providing excellent out-of-sample generalization. This paper aims to evaluate various SVM kernels to predict churners on an imbalanced distribution of churners and non-churners data set. The experiment was carried out on a telecommunication data taken by random sampling. Our study shows that linear kernel outperforms commonly used Radial Basis Function(RBF), sigmoid and polynomial kernels.
[random sampling, customer churn prediction, SVM, SVM kernel evaluation, nonchurners data set, telecommunication computing, AUC, RBF, radial basis function networks, Communications technology, ROC curve, Kernel, churners data set, Evaluation metrics, linear kernel, Laplace equations, sampling methods, support vector machines, polynomials, polynomial kernels, telecommunication market, Standards, Support vector machines, out-of-sample generalization, Kernels, radial basis function, sigmoid, Imbalanced distribution, Churn Prediction, Data models, Analysis of variance]
Comparison of position sensorless control based back-EMF estimators in PMSM
2015 18th International Conference on Computer and Information Technology
None
2015
This research approach a study with comparison of position sensorless control schemes based on back-electromotive force (back-EMF) estimation in permanent magnet synchronous motors (PMSM). The characteristics of the estimated back-EMF signals are analyzed using various mathematical models of a PMSM. The transfer functions of the estimators, based on the extended EMF model in the rotor reference frame, are derived to show their similarity. They are then used for the analysis of the effects of both the motor parameter variations and the voltage errors due to inverter nonlinearity on the accuracy of the back-EMF estimation. The differences between a phase-locked-loop (PLL) type estimator and a Luenberger observer type estimator, generally used for extracting rotor speed and position information from estimated back-EMF signals, are also examined. An experimental study with a 250-W interior-permanent-magnet machine has been performed to validate the analyses.
[rotor reference frame, rotor speed extraction, voltage error effect analysis, mathematical models, position information, motor parameter variation effect analysis, phase locked loops, PLL, Control, Stators, position control, Mathematical model, Estimators, back-electromotive force estimation, Sensor less, position sensorless control based back-EMF estimators, rotors, error analysis, sensorless machine control, Rotors, Transfer functions, Observers, Permanent magnet motors, Back-EMF, Phase locked loops, electric potential, synchronous motors, machine theory, phase-locked-loop type estimator, power 250 W, PMSM, permanent magnet synchronous motors, transfer functions, permanent magnet motors, observers, Luenberger observer type estimator]
Custom MPPT design of solar power switching network for racing car
2015 18th International Conference on Computer and Information Technology
None
2015
This paper approach to design and build a highly efficient solar-powered race car for Solar Jackets Racing Team, will focus on developing a system to charge the car batteries given the energy received from solar panels throughout the day. It will utilize a switching power supply circuit to handle the variable amount of input power. The circuit will include an inductor, a switch, and a diode to transfer the appropriate energy from the source (solar cells) to the load (car battery). The switch will be controlled by a microcontroller that employs maximum power point tracking (MPPT) technology to determine how much current and voltage should be sent to the batteries to produce maximum power.
[Algorithm design and analysis, MPPT, inductor, PV, Switches, Batteries, solar powered vehicles, Solar Jackets Racing Team, Switching circuits, car batteries, Photovoltaic cells, racing car, microcontrollers, Microcontrollers, solar power switching network, Solar, Microcontroller, Switching Network, Racing Car, switched mode power supplies, switching power supply circuit, Maximum power point trackers, maximum power point trackers, microcontroller, custom MPPT design, maximum power point tracking, solar panels, solar cells, solar-powered race car]
Map matching for error prone GPS data on a sparse road network and predicting travel time of a route
2015 18th International Conference on Computer and Information Technology
None
2015
Optimal ambulance route detection in an urban area is a challenging problem especially in the presence of difficulties such as sparse road network, error-prone GPS data and irregular traffic conditions. We have worked on solutions robust to these challenges and for the first time ever in the context of Dhaka, Bangladesh. This paper presents our newly designed map-matching algorithms as well as an intelligent route prediction scheme. We have implemented our algorithms on real traffic data and discuss the results here.
[Algorithm design and analysis, map matching algorithms, Roads, urban area, Urban areas, map-matching, error-prone GPS data, optimal ambulance route detection, travel time prediction, traffic engineering computing, intelligent transportation system, image matching, emergency vehicle management, Global Positioning System, Vehicles, intelligent route prediction scheme, sparse road network, real traffic data, error prone GPS data, Data collection, Optimal route detection, Prediction algorithms, travel time, driver assistive technology]
PoE (Power over Ethernet) switch based remote power control system for the better performance of ISPs in Bangladesh
2015 18th International Conference on Computer and Information Technology
None
2015
There are Forty ISPs (Internet Service Provider) operate in Bangladesh which provide services like Basic Internet Connection, IP-VPN Services, MPLS, Mail Server, Intranet etc., to both Corporate &amp; SOHO (Small Office/Home Office) users, along with individual users by various delivery method like ADSL, DSL, FTTH, Wi-Fi, WiMAX, Radio or simple Ethernet connection. To cover, vast geographic area &amp; to provide last mile connectivity to their users, ISPs setup numerous PoPs or Point of Presence in strategic locations, equipped with required hardware and connected by backhauling of telecommunication lines or FTTH connections or Radio. Usually the strength of an ISP is measured by its number of PoPs and the number of subscribers these PoPs can handle. Nowadays PoPs are usually resided inside the rented space of telecommunication switching room or BTS (Base Transceiver Station). These PoPs are usually equipped with network equipment like Network Switch, Gateway, Router, E1 Converters, and Media Converters etc. Most of the time, these PoPs are not manned. And because these are rented, the original owners (Telecom Operators) keep those locked in. Problem arises when the mentioned equipment (E1 converter, Media Converter) becomes unresponsive, or in tech slang, gets hanged. To resolve the issue, one has to go to the remote site just to flip the power switch to perform a simple reboot of the unresponsive device. A common phenomenon in this situation is, often there aren't any technically trained personnel at the site who can perform maintenance and resets on equipment. Even if it is a manned station, the risk of wrong equipment getting rebooted is high. To encounter such problems, lower the travel time expense-time, eliminate the man-hour misused and specially minimize downtime, we developed a PoE (Power over Ethernet) switch based remote power control system by which a network administrator can control power of network equipments from anywhere and anytime. Power over Ethernet switch offers advance switching controls and provides power to network devices using Ethernet cables. These are the sole reasons which propelled the motivation to research in this particular issue. We believe this would help this competitive industry (ISP) to minimize cost and maximize service efficiency.
[Power over Ethernet (PoE) Splitter, power over Ethernet, remote power control system, point of presence, Ports (Computers), Internet service provider, Switches, Companies, PoE, local area networks, Media converter, PoP, Home appliances, power aware computing, network performance, network equipment power control, Power over Ethernet (PoE) switch, telecommunication control, ISP, Media, computer network performance evaluation, telecommunication switching, E1 converter, Bangladesh, power control, Internet, Network performance]
A study on low cost solar powered wheel chair for disabled people of Bangladesh
2015 18th International Conference on Computer and Information Technology
None
2015
This paper presents a study on low cost solar powered wheelchair for disabled people of Bangladesh. The main components are: the wheelchair structure, the solar panels, the DC motors, the controlling circuits, microcontroller and joystick. The proposed model is very useful for the physically challenged people of the rural areas. This proposed model is affordable to the low income people of countries like Bangladesh. The wheelchair is cost effective and user friendly. The proposed model is self-driven and independent. For the proposed model the Life Cycle Cost analysis is done and compared with the power wheelchair that takes the grid power for charging and the proposed model is proven to be lower in the cost. The solar powered wheelchair will help the physically challenged people a great deal in their day to day movements. The proposed model will be very effective in the rural areas as well as in the urban areas.
[wheelchair, mobility, life cycle costing, Wheels, disability, low cost, assistive device, Wheelchairs, controlling circuit, Sociology, interactive devices, wheelchairs, solar panel, spinal cord, life cycle cost analysis, solar cell arrays, microcontrollers, solar power, DC motors, joystick, Solar panels, Statistics, low cost solar powered wheel chair structure, microcontroller, cost effective wheelchair, DC motor, Solar energy, Bangladesh disabled people, user friendly wheelchair]
Cohesion based personalized community recommendation system
2015 18th International Conference on Computer and Information Technology
None
2015
The importance of social networking sites (SNS) in our life is increasing day by day as they are attracting millions of users by their interesting features and activities. Joining different communities is one of the most common activities of users in social network. However, information overloading has troubled many users as thousands of communities are being created each day. To solve this problem, we have introduced a cohesion based community recommendation system where cohesion means high degree of connection among SNS users. Our proposed framework consists of the steps like, extracting sub-network (e.g. Facebook), measuring the friendship factors (both offline and online), measuring user preference factor, calculating threshold from present communities of any user, and finally recommending community based on automatically derived threshold.
[user preference factor, Cohesion, user activities, friendship factors, Social Networking, User factor, subnetwork extraction, Friendship factor, recommender systems, SNS users, Employment, Community recommendation, Collaboration, information overloading, Motion pictures, social networking (online), cohesion based personalized community recommendation system, Software, Facebook, Recommender systems, social networking sites]
An effective approach for relevant paragraph retrieval in Question Answering systems
2015 18th International Conference on Computer and Information Technology
None
2015
Paragraph retrieval is a substantial task in Question Answering (QA) systems. It represents the extraction of data from a huge data collection, which have the probability to contain an answer to a question and is a significantly important intermediary step between a user of a QA system and the answers. It is barely impossible to analyse a large collection of data extensively in quick time and because of the vast nature of the background data, it is very important to narrow down the search space from where an answer can be looked for. In this paper, we address the information extraction step and present an effectively designed model for the relevant paragraph retrieval, which is efficient in terms of execution time as well. The model deals with the structure and the organization of information, performs an in-depth analysis of user's question, and presents a priority based searching capability for retrieving paragraphs according to the necessity which will be both effective and time efficient. Experiments were carried out and compared against similar systems based on the data of the document retrieval task of Text REtrieval Conference (TREC) 2005. We also tested our methodology against the data set from TREC 2007. The system performance was measured in terms of various parameters such as R-precision, Recall, and Mean Average Precision. A satisfactory result achieved by our approach establishes its competency for getting integrated into any realtime QA systems.
[Context, text analysis, natural language processing, QA system, Information retrieval, Indexes, data extraction, information extraction, Semantics, question answering system, Organizations, Syntactics, Knowledge discovery, question answering (information retrieval), paragraph retrieval]
Trust based D2D communications for accessing services in Internet of Things
2015 18th International Conference on Computer and Information Technology
None
2015
Internet of Things (IoT) is a vision of connecting everything for providing better services efficiently. IoT consists of enormous number of heterogeneous computing devices with different capabilities to provide a diverse range of services situated around the globe. Increasing number of IoT devices and availability of different services in the edge of the networks makes it inevitable to interact among devices. As a result, this open, non-homogeneous and distributed environment also breaches the integrity of secure and reliable device to device (D2D) communication. Traditional access control mechanisms are not prolific to the itinerant, decentralized and dynamic scenarios in the IoT. Trust management is a proven technology for applications like P2P, Grid, and ad hoc network. Hence, this technology can also be used to increase the user reliability in IoT. In this paper, we propose a trust based D2D communication mechanism for accessing different services to meet the growing transactions and successful operations of IoT. We also analyze the effect of adaptive trust parameters for IoT device to device communication in order to access the services.
[telecommunication security, Adaptation models, trust based D2D communication, heterogeneous computing device, wireless sensor networks, user reliability, Trust Management, Computational modeling, Quality of service, wireless sensor network, Security, Internet of Things, device to device communication, Service Management, IoT, Internet of things, telecommunication network reliability, D2D, Logic gates, trust management, access control, Reliability, trusted computing]
Study of ICU patient condition alert generation process using D2D under LTE network
2015 18th International Conference on Computer and Information Technology
None
2015
An ICU patient is vulnerable at any moment and requires constant monitoring and fast response from the doctors and nurses at critical moments. To do so, the doctors and nurses must know when the critical situation arises to attend the patient right on time. In this paper, we propose a heterogeneous network merged with an automated system to generate alert signal if the patient is in critical condition and pass the alert mode to the nurses and doctor for fast response from them. Another issue is to inform the patient's worried visitor/attendant waiting at the hospital by sending a report periodically with selective data stating his condition. Since D2D communication under LTE network works well in close proximity, we can deploy this technique to alert the doctor about his patient's critical condition and also to transfer the report of the ICU patient's attendants waiting outside or nearby. Simulations showed that the time required for transmitting alert signal to the doctors is only few seconds indicating critical condition of the patient. This proposal needs 4G LTE network enabled smart phones as it uses D2D communication from server point to nurse station, doctor and patient attendants. Still, employing D2D will be very cost effective as we emphasis on saving human lives in time.
[Medical services, D2D, Mobile handsets, ICU, Sensors, Servers, LTE network, Biomedical monitoring, Long Term Evolution, Monitoring]
Cough detection using speech analysis
2015 18th International Conference on Computer and Information Technology
None
2015
Common cold is a common disease now-a-days. Due to common cold patient faces cough, sore throat, sneezing and runny nose problem. Most of the time patients' speech sounds different due to cough. In this paper, analyzing speech recording of cough and normal state of a person, we have derived two sets of representative features. These features are used for classifying normal and cough state of the patient. The classification algorithms we have used are Support Vector Machine, Bayesian Classifier and Neural Network. On the generated real life dataset, we have applied the features and classifiers. We have listed the performance statistics of the exhaustive experiment. The performance measures reveal that the classifiers with the second feature set provide very good accuracy (greater than 70% for all the classifiers). Among the three classifiers Bayesian provides the best accuracy (86.31%).
[classification algorithms, pattern classification, Harmonic analysis, Entropy, runny nose problem, cough state, neural network, Bayesian classifier, Training, cough detection, support vector machine, sneezing, Nose, sore throat, Speech, speech processing, common disease, Bayes methods, belief networks, neural nets, Testing, speech analysis, common cold]
On the downlink SINR and outage probability of stochastic geometry based LTE cellular networks with multi-class services
2015 18th International Conference on Computer and Information Technology
None
2015
Due to the increasing number of irregularly spaced base stations (BSs) as well as the intrinsic random channel environment, modeling and analysis of cellular networks using classical hexagonal cell shapes is becoming ever more impractical. Therefore, stochastic geometry models having the ability to picture near realistic situations is gaining a wide acceptability for evaluating cellular network performance. In light of this, this paper presents a simulation-based investigation on the downlink signal-to-interference-noise-ratio (SINR) and the outage probability of orthogonal frequency division multiple access (OFDMA)-based long term evolution (LTE) cellular systems using stochastic geometry with multi-class services. Locations of BSs are modeled using both Poisson point process (PPP) as well as hard-core Poisson process (HCPP). Moreover, a computationally efficient method is proposed for capturing the effect of inter-cell interference in such stochastic geometry based cellular networks. Network performance including the outage probability of various multi-class services under varying shadow fading scenario and BS density is evaluated using Monte Carlo simulations, and compared with that of the traditional hexagonal models. Simulation results clearly demonstrates the over optimistic network performance of hexagonal model, while the most realistic HCPP model provides a compromise between the hexagonal and the PPP models.
[cellular network analysis, signal-to-interference-noise-ratio, Stochastic processes, irregularly spaced BS, Downlink, LTE cellular networks, radiofrequency interference, Monte Carlo methods, fading channels, outage probability, hard-core Poisson process, telecommunication network reliability, OFDM modulation, Stochastic Geometry, PPP model, stochastic processes, Poisson Point Process, Long Term Evolution, irregularly spaced base stations, OFDMA-based long term evolution cellular systems, frequency division multiple access, Computational modeling, downlink SINR, orthogonal frequency division multiple access systems, intrinsic random channel environment, Interference, cellular network performance evaluation, Hard-core Poisson Process, stochastic geometry models, intercell interference effect, multiclass services, Geometry, cellular network modeling, Monte Carlo simulations, Poisson point process, HCPP model, shadow fading scenario, LTE, Signal to noise ratio, cellular radio]
Better user recommendations using enhancing software development process repository
2015 18th International Conference on Computer and Information Technology
None
2015
Reusing previously completed software repository to enhance the development process is a common phenomenon. If developers get suggestions from the existing projects they might be benefited a lot what they eventually expect while coding. The strategies available in this field have been rapidly changing day by day. There are a number of efforts that have been focusing on mining process and constructing repository. Some of them have emphasized on the web based code searching while others have integrated web based code searching in their customized tool. But web based approaches have inefficiency especially in building repository on which they apply mining technologies. To search the code snippets in response to the user query we need an enriched repository with a better representation and abstraction. To ensure that repository before mining process we have developed a concept based on Enhancing Software Development Process (ESDP). In ESDP approach multiple sources of codes from both online and offline storages are considered to construct the central repository with XML representation and applied mining techniques in the client side. The respective evaluation shows that ESDP approach works much better in response time and performance than many other existing approaches available today.
[user recommendations, offline storages, Buildings, Software algorithms, data mining, user query, constructing repository, Data mining, Servers, data mining process, development process, XML representation, Mining, recommender systems, online storages, Repository, Searching, XML, Search engines, code searching, enhancing software development process repository, Software, software engineering, ESDP]
NEDindex: A new metric for community structure in networks
2015 18th International Conference on Computer and Information Technology
None
2015
There are several metrics (Modularity, Mutual Information, Conductance, etc.) to evaluate the strength of graph clustering in large graphs. These metrics have great significance to measure the effectiveness and they are often used to find the strongly connected clusters with respect to the whole graph. In this paper, we propose a new metric to evaluate the strength of graph clustering and also study its applications. We show that our proposed metric has great consistency which is similar to other metrics and easy to calculate. Our proposed metric also shows consistency where other metrics fail in some special cases. We demonstrate that our metric has reasonable strength while extracting strongly connected communities in both simulated (in silico) data and real data networks. We also show some comparative results of our proposed metric with other popular metric(s) for Online Social Networks (OSN) and Gene Regulatory Networks (GRN).
[Graph clustering, Community structure, GRN, Social network services, graph theory, online social networks, complex network, Social network, Graph theory, NEDindex, OSN, Level measurement, graph clustering, gene regulatory networks, pattern clustering, community structure, Clustering algorithms, Complex networks, Complex network, social networking (online), Mathematical model]
Smart taxi control system: Bangladesh perspective
2015 18th International Conference on Computer and Information Technology
None
2015
To reduce the passengers' adversity and elevate the social security, consumer rights, an effective and smart taxi control system is proposed in this paper. The system may help to reduce the different problems mass people face while travelling on a CNG driven taxi. Certitude of reasonable fare charging can be easily uphold by the proper implementation of this smart system. Forgery of driving license and fitness license of vehicle will be reduced because of the highly secured registration process of the system. Proposed system also provides the proper opportunity to the vehicle owner to monitor the owned vehicle as well as an income statement of daily income on behalf of each vehicle. Taking legal steps will become a matter of mouse clicks to Law enforcement authorities due to the enhanced tracking facilities of the system. Smartly designed meter device, indicator light and siren surely a suitable solution for socio-economic perspective of Bangladesh. Though the system is proposed for CNG driven taxies, but it can be implemented for other vehicles with little modification.
[driving license forgery, indicator light, CNG driven taxi, law enforcement authorities, siren, Tracking, road traffic control, passenger adversity, Licenses, Control systems, income statement, Security, tracking, Vehicles, meter device, smart meters, Law enforcement, road vehicles, smart taxi control system, social security, vehicle fitness license forgery, Electronic meter, Fare charging, socio-economic effects, tariffs, fare charging, Siren, intelligent transportation systems, Public transportation, Standards, Bangladesh, Indicator light, Handheld computers, consumer rights, socio-economic perspective]
Handover management in heterogeneous network
2015 18th International Conference on Computer and Information Technology
None
2015
Femtocells and picocells are becoming popular in the telecommunication industry in order to provide high data rate communication with superlative quality of service. The concept of dividing a large cell into numerous small cells is to reduce the blocking and dropping of calls in a cellular network. Integrating femtocell and macrocell will be the medium of transferring a large volume of calls from the macrocell to the femtocells by means of efficient handling. Hierarchical Cellular Networks (HCN) offer an improved utilization of the internal networks by allocating the high and low speed users to the picocell and femtocells respectively. In this paper, we present an integrated network and discuss the major issues of mobility management. We propose a Call Admission Control (CAC) scheme with optimized SINR in the femtocellular/macrocellular network which effectively handles various calls. We also propose a M/M/C Markov model for the two layers of HCN with a FIFO queue. Both of these schemes lowers the call dropping probability and the differences between them are also observed. Our proposed schemes will be very valuable for the people working in this type of research and industry to implement.
[Macrocell networks, Call Admission Control, Hierarchical Cellular Network, Base stations, telecommunication congestion control, telecommunication industry, Mobile handsets, femtocells, hierarchical cellular networks, mobility management (mobile radio), quality of service, Markov model, picocells, handover management, Heterogeneous network, femtocellular radio, heterogeneous network, Handover, Markov processes, call dropping probability, call admission control, FIFO queue]
Applying the technology acceptance model in examining Bangladeshi consumers' behavioral intention to use mobile wallet: PLS-SEM approach
2015 18th International Conference on Computer and Information Technology
None
2015
Mobile based cash transaction services, also known as mobile wallet was first introduced by the BRAC bank in Bangladesh. Since its inception, it has been widely accepted by unbanked people of Bangladesh. This study examines consumers' behavioral intention to use mobile wallet services in Bangladesh in terms of applying, and analyzing TAM. Total 104 respondents who were chosen randomly within the sample extent and sample frame completed the structured survey questionnaires to know the perception about the services, and the motivation to use mobile wallet. Instead of using Covariance-Based tool of Structural Equation Modeling, Partial Least Squares Structural Equation modeling (PLS-SEM) tool was employed in order to analyze data which provides evidence of the reliability and validity of the Technology Acceptance Model (TAM) in this study. The result shows that all variables of the model, except perceived ease of use, significantly affect users' behavioral intention to use mobile wallet.
[TAM, behavioral intention, Correlation, Predictive models, Mobile communication, consumer behaviour, Structural Equation Modeling (SEM), banking, electronic money, Analytical models, mobile computing, PLS-SEM approach, BRAC bank, Mathematical model, technology acceptance model, Context, least squares approximations, mobile wallet, Mobile wallet, partial least squares structural equation modeling tool, Bangladeshi consumer behavioral intention, mobile based cash transaction services, Reliability, statistical analysis, Technology Acceptance Model (TAM), PLS-SEM tool]
Secure outage performance analysis for multicasting with linear equalization
2015 18th International Conference on Computer and Information Technology
None
2015
We consider a confidential communication through Rayleigh fading channel in which a source transmits a common stream of information to the multiple client receivers via multiple relays in the presence of an eavesdropper. We assume that there is no direct path from source to client receivers and eavesdropper, and communication occurs only through the relays. In order to increase the spectral efficiency, we consider a linear equalization, e.g., zero-forcing (ZF) in cooperative spatial multiplexing system. Considering the effect of linear equalization, at first, we derive the exact closed-form expression for the ergodic secrecy multicast capacity. Then, we derive the closed-form expressions for the secure outage probability and the complementary cumulative distribution function (CCDF) of secrecy multicast capacity to analyze the outage performance of proposed model.
[Multiplexing, telecommunication security, Rayleigh fading channel, radio networks, eavesdropper, Closed-form solutions, space division multiplexing, source transmits, secure outage performance analysis, Multicast communication, confidential communication, complementary cumulative distribution function, Relays, cooperative spatial multiplexing system, fading channels, Probability density function, ergodic secrecy multicast capacity, multiple client receivers, Receivers, Rayleigh channels, radio receivers, ZF, Cooperative spatial multiplexing, zero-forcing, linear equalization, secure outage probability, CCDF, Capacity planning]
Extremely randomized trees for Wi-Fi fingerprint-based indoor positioning
2015 18th International Conference on Computer and Information Technology
None
2015
Wi-Fi fingerprint-based position estimation becomes one of the key component of many location based services since the existing Wi-Fi infrastructures in indoor environment can be utilized for user's position estimation in order to reduce the deployment cost. However, the low positioning accuracy is still a key challenge for indoor positioning system due to the environmental dynamics and noisy characteristics of the RF signal. This paper presents a robust indoor localization approach based on Wi-Fi fingerprints using extremely randomized trees as the location estimation algorithm. In this approach, the collected raw fingerprints data are preprocessed, and then fed to the proposed localization algorithm, given their capability to handle high dimensional and unbalanced data, to localize users. The evaluation results of the experiments conducted on the first publicly available multi-building multi-floor indoor localization database indicate that the proposed technique performs much better than the traditional systems in terms of localization accuracy and calibration effort. The proposed approach yielded the maximum localization rate of 91.44%.
[Wireless LAN, Computational modeling, environmental dynamics, Buildings, Estimation, trees (mathematics), extremely randomized trees, Wi-Fi infrastructures, position estimation, indoor environment, RF signal, user position estimation, Training, indoor positioning system, robust indoor localization, Databases, Wi-Fi fingerprint, indoor positioning, wireless LAN, IEEE 802.11 Standard]
Clustered and smarter web mining using semantic web
2015 18th International Conference on Computer and Information Technology
None
2015
Semantic Web which is an extension to the current web 2.0 presents information more magnificently for humans and automated tools as well. It offers an intelligent web service that synchronizes and arranges pre&#x0301;cised data over web in a well-organized modus. The exactitude of fetching relevant data as per the user demand has been a real challenge for traditional web over years in the field of data mining. In this paper, we propose an approach for mapping data over web 3.0 through web ontology and retrieve the pre&#x0301;cised information through an intelligent agent. The agent provides all the searched data related to the user query from where desired information is found. If the user does not have enough search parameter, mining can be carried out by the information appended by the agent. Our proposed model shortlists the results fetched by traditional search engines such as Google which are semantically related to users' search parameter. A case study of Institutions of Barisal Division is considered to examine the helpfulness of the model.
[search engines, Web 2.0, semantic Web, data mining, user query, Ontologies, Resource description framework, intelligent data mining, Web ontology, query processing, Institutions of Barisal Division, Semantic web, intelligent agent, Web services, search parameter, clustered Web mining, Semantics, Web mining, user demand, Search engines, ontologies (artificial intelligence), web 3.0, intelligent Web service, information retrieve]
Analysis of real time frequency transients and inertia estimation of Bangladesh power system
2015 18th International Conference on Computer and Information Technology
None
2015
This paper presents observations of real time frequency transients of Bangladesh Power System (BPS) including last country-wide blackout on 1st November, 2014 after HVDC station tripped with respect to frequency control and primary focus on Inertial Frequency Response. The inertia constant of a system describes the initial, transient, frequency behavior of a system when subjected to a real power disturbance. Therefore, the inertia constant of a system can be a useful tool when investigating the frequency stability of a system. The frequency transients of power system is a significant index for power system operation and research on the dynamic frequency characteristic of power system is the basis of many important design works such as the design of under frequency load shedding scheme. Therefore, it is of great urgency and necessity to analysis the dynamic frequency characteristic of BPS before countrywide blackout after disturbance. This paper first analysis two real time frequency transients those have almost similar pre-disturbance network condition and secondly shows that maintaining minimum level of Inertial reserve in real-time operations with unit commitment can be crucial to ensure stable, secure and reliable operation of BPS. Beside this under frequency scheme should be revised and controlled BPS network disintegration along with rate of change of frequency should be adopted for adaptive as well as self-healing of BPS. This paper also focus on adaptive central under frequency and under voltage load shed scheme (UFVLS) rather than discrete under frequency load shed (UFLS).
[load shedding, inertia constant, Time-frequency analysis, inertia estimation, inertial frequency response, country-wide blackout, Power system stability, power system transient stability, Frequency control, frequency load shedding scheme, Under frequency Load shed (UFLS), unit commitment, Bangladesh power system, under frequency and under voltage load shed scheme, Transient analysis, HVDC (High Voltage Direct Current), power disturbance, UFVLS, BPS network disintegration, HVDC transmission, HVDC station, df / dt, Generators, frequency control, BPS (Bangladesh Power System), BIPTC (Bangladesh India Power Transmission Center), system frequency stability, frequency transients, Inertial Reserve, real time frequency transient analysis, transient analysis, discrete under frequency load shed]
Transient anode voltage modeling of IGBT and its base doping profile investigation
2015 18th International Conference on Computer and Information Technology
None
2015
In many power converter applications, study of doping concentration in the carrier storage region of IGBT is considered desirable. This paper introduces an estimation technique for base doping concentration through investigation into transient anode voltage modeling of Non-punch through (NPT) Insulated Gate Bipolar Transistor (IGBT). Parabolic profile has been used for derivation of minority carrier concentration within the base. With the derived expression, an analytical model has been developed for turn-off anode voltage of IGBT in all doping profile conditions. Better agreements with the experimental results have been found compared to the previously used linear model. Finally, the implications of base doping dependence on the anode voltage are discussed, including implementation of such a doping concentration estimation technique.
[Minority Carrier Lifetime, insulated gate bipolar transistors, Doping profiles, parabolic profile, Parabolic Approximation, analytical model, transient anode voltage modeling, minority carrier concentration, nonpunch through insulated gate bipolar transistor, power converter applications, IGBT, doping concentration estimation technique, carrier storage region, base doping profile investigation, Semiconductor process modeling, semiconductor device models, Mathematical model, doping profiles, Transient analysis, transients, Insulated gate bipolar transistors, power convertors, turn-off anode voltage, minority carriers, Transient Anode Voltage, Carrier Storage Region, base doping concentration, anodes, Base Doping Concentration, Anodes]
Proposal of a simple structure photonic crystal fiber for lower indexed chemical sensing
2015 18th International Conference on Computer and Information Technology
None
2015
In this paper, a simple two layer photonic crystal fiber has been proposed. The proposed PCF shows higher sensitivity as a chemical sensor for lower refractive index chemicals Ethanol and Methanol. The proposed structure also reduces confinement loss. Micro structured array of elliptical air holes in the core has been used for better sensitivity. A full vectorial finite element method (FEM) was used for the investigation of the proposed chemical sensor for its different geometrical properties. The investigation depicts the impact of various geometrical properties like inner and outer layer diameters and pitch values on sensitivity and confinement loss. The simplicity and the higher results of the proposed PCF have been used for lower refractive index chemical sensor. The proposed PCF shows a higher sensitivity 40.32% for Ethanol and 34.90% for Methanol at the wavelength 1.33 μm. Besides that, the proposed two layers PCF sense the chemicals in a wide range of wavelength from 0.5 μm to 1.5 μm.
[holey fibres, optical fibre losses, wavelength 0.5 mum to 1.5 mum, Photonic crystal fibers, Refractive index, microsensors, refractive index chemical, ethanol, sensor arrays, Finite element method, finite element method, fibre optic sensors, methanol, Sensors, two layer photonic crystal fiber, confinement loss, Methanol, confinement loss reduction, Ethanol, refractive index measurement, microstructured array, lower indexed chemical sensor, finite element analysis, chemical sensors, Evanescent field, FEM, core, Sensitivity, PCF, elliptical air hole array, chemical sensor, Finite element analysis, organic compounds, photonic crystals, Relative sensitivity]
An approach to empirical Optical Character Recognition paradigm using Multi-Layer Perceptorn Neural Network
2015 18th International Conference on Computer and Information Technology
None
2015
In this paper we are represent the architecture of Optical Character Recognition that converting from visual character to the machine readable format. To present this architecture, several stages are associate like take the character input image, preprocessing the image, feature extraction of the image and at last take a decision by the artificial computational model same as biological neuron network. Decision making system by the Artificial Neural Network associated with two steps; first is adapted the artificial neural network throughout the Multi-Layer Perceptron learning algorithm and second is recognition or classification process for the character image to comprehensible for the machine in a way that what character is it. Our proposal architecture achieved 91.53% accuracy to recognize the isolated character image and 80.65% accuracy for the sentential case character image.
[artificial neural network, ANN, Biomedical optical imaging, image classification, character input image, sentential case character image, MLP, optical character recognition, Image color analysis, feature extraction, artificial computational model, image preprocessing, learning (artificial intelligence), multilayer perceptrons, Optical Character Recognition, Multi-Layer Perceptron, Optical imaging, biological neuron network, Optical character recognition software, Character recognition, machine readable format, optical character recognition paradigm, character image classification, Pattern Recognition, Artificial Neural Network, decision making system, decision making, isolated character image recognition, Feature extraction, Adaptive optics, multilayer perceptron neural network, visual character, OCR]
Joint texture and depth coding using cuboid data compression
2015 18th International Conference on Computer and Information Technology
None
2015
The latest multiview video coding (MVC) standards such as 3D-HEVC and H.264/MVC normally encodes texture and depth videos separately. Significant amount of rate-distortion performance and computational performance are sacrificed due to separate encoding due to the lack of exploitation of joint information. Obviously, separate encoding also creates synchronization issue for 3D scene formation in the decoder. Moreover, the hierarchical frame referencing architecture in the MVC creates random access frame delay. In this paper we develop an encoder and decoder framework where we can encode texture and depth video jointly by forming and encoding 3D cuboid using high dimensional entropy coding. The results from our experiments show that our proposed framework outperforms the 3D-HEVC in rate-distortion performance and reduces the computational time significantly by reducing random access frame delay.
[Video coding, cuboid data compression, 3D-HEVC, dynamic background, rate-distortion performance, McFIS, Three-dimensional displays, Quantization (signal), random access frame delay reduction, entropy, cuboid, Computer architecture, depth coding, data compression, decoder framework, multiview video coding, Encoding, video coding, mutiview video, Standards, 3D cuboid encoding, encoder framework, texture coding, high dimensional entropy coding, MVC standards, Delays]
Reliability analysis of automated pond oxygen management system
2015 18th International Conference on Computer and Information Technology
None
2015
Dependability of safety-critical systems are a prime concern of the modern society due to the growing dependence on those systems. Safety of systems is a fundamental requirement for system reliability. Fault tree analysis (FTA) is powerful, well-established, and widely used tool for evaluating system safety. Fault trees provide a graphical and logical framework for analysing the dependability of systems. They have been successfully employed in studying the failure behaviour of a variety of real-world systems. In aquaculture, the volume of oxygen contained in water is considered as a critical parameter for the health and well-being of fishes. If oxygen levels drop below 4 mg/L then fishes may stop feeding, stressed and begin to die. Automated Pond Oxygen Management System (APOMS) is a critical component in aquaculture to maintain the proper oxygen level in water. In summer months, when oxygen level starts dropping due to increased temperature, then the APOMS can balance the oxygen level in water by generating oxygen artificially. Therefore, the failure of this system may result in a disastrous outcome. In this paper, we have used fault tree analysis to evaluate the reliability of an automated fault tolerant pond oxygen management system.
[graph theory, fault tree analysis, Batteries, Safety Analysis, graphical framework, Fault Tree Analysis, Reliability Analysis, Safety, oxygen, Fault trees, APOMS, Oxygen, fault tolerance, Risk Assessment, Safety Critical Systems, fault trees, Generators, automated pond oxygen management system, aquaculture, reliability analysis, environmental science computing, safety-critical system, Logic gates, Reliability, logical framework, oxygen level]
Speech based text correction tool for the visually impaired
2015 18th International Conference on Computer and Information Technology
None
2015
In this modern age of revolutionary smart devices, technology reigns supreme. Visually impaired users are a part of this modern world. They deserve to taste the beauty of this technology. However, they require assistive technology which is the concern of HCI (Human Computer Interaction). In this paper, we present a text correction tool that is entirely designed for the visually impaired. Instead of having to use traditional keyboard or mouse, they can write or edit text using speech only. The text can be read to the user using speech synthesizer. The user will be notified of different events through audio feedback and the user will be able to write text entirely using speech. Different voice commands have been designed to interact with this tool. Also, important modes can be activated using a single click on a mouse button. Since clicking either of the mouse button may not be very difficult for the visually impaired, we can use this option for flexibility. Existing speech based tools offer fast writing method, higher accuracy, but they are not optimized for the people with low or no vision. Text editing is a basic way to communicate with the computer. Providing a clear and efficient solution regarding this matter will obviously open a new door for the disabled which is in this case visual impairment. Five participants evaluated the system and the feedback was more than satisfactory.
[Computers, Visualization, Google, handicapped aids, text analysis, Speech based interaction, speech synthesis, Assistive technology, Google Speech, HCI, feedback, visually impaired user, Keyboards, audio feedback, Speech recognition, Writing, Speech, speech processing, speech based text correction tool, Text Editor, human computer interaction, Visually Impaired, speech synthesizer]
Automatic Bengali news documents summarization by introducing sentence frequency and clustering
2015 18th International Conference on Computer and Information Technology
None
2015
A method has been proposed in this paper for Bengali news documents summarization which extracts significant sentences using the four major steps (a) preprocessing, (b) sentence ranking, (c) sentence clustering, and (d) summary generation. The noticeable feature of this method is the incorporation of the sentence frequency where redundancy elimination is a consequence. Another one remarkable aspect is sentence clustering on the basis of similarity ratio among sentences. The summary sentence selection is done from all the clusters so that there will be maximum coverage of information in summary even if information is found scattered in input document. Two sets of human generated summary have been utilized where one is to train the system and another is for performance evaluation. The proposed method has been found better while turning comparison with the latest state-of-the art method of Bengali news documents summarization. The results of performance evaluation show that the average Precision, Recall and F-measure values are 0.608, 0.664 and 0.632 respectively.
[Performance evaluation, information resources, Art, Redundancy, preprocessing, F-measure values, summary generation, Electronic mail, Information technology, sentence ranking, word processing, Computer science, redundancy elimination, sentence clustering, similarity ratio, automatic Bengali news documents summarization, Documents summarization, Internet, sentence frequency, summary sentence selection]
A conceptual framework for design of mobile governance in developing countries: The case of Bangladesh
2015 18th International Conference on Computer and Information Technology
None
2015
Mobile governance (m-governance), which is considered as an extension of electronic governance (e-governance), has enormous opportunity to flourish in the developing countries due to the rapid increase of mobile phone subscribers. There are also a number of challenges accompanying the opportunities for design and implementation of mobile governance in developing countries. In this paper a conceptual framework is proposed based on secondary research to justify the current mobile governance situation of developing countries, with Bangladesh being selected as the case study, by explaining the essential components of a mobile governance system and the surrounding environmental factors that have influence upon it.
[Government, electronic governance, environmental factors, Mobile communication, Mobile handsets, mobile governance system, Digital divide, e-governance, m-governance, mobile computing, Bangladesh, developing countries, mobile governance, policy framework, Mobile computing, government data processing]
Crystal orientation dependent performance of cubic InGaN QW blue-violet laser
2015 18th International Conference on Computer and Information Technology
None
2015
In this paper, the crystal orientation-dependent electronic and optical performance of a compressively strained quantum well (QW) blue-violet Vertical Cavity Surface Emitting Laser (VCSEL) is numerically simulated using MATLAB. It is found that there is a strong correlation between the crystal orientation and the energy band dispersion and hence the optical emission spectrum of the laser. The peak gain and peak emission wavelength depend on the orientation-dependent fundamental energy gap, energy separation between the HH1 and LH1 subbands, and momentum matrix element. It is observed that the maximum and minimum gain of the VCSEL is 1150 and 920 cm-1 corresponding to the (112) and (110) crystal orientations, respectively, and also the peak emission wavelength can be shifted from 454 nm to 462 nm with the change in crystal orientation from (001) to (110).
[surface emitting lasers, energy band dispersion, Photonic band gap, compressively strained quantum well blue-violet VCSEL, energy gap, Crystal orientation, Lattices, indium compounds, Electron optics, crystal orientation-dependent electronic performance, Gallium nitride, optical emission spectrum, optical correlation, momentum matrix element, crystal orientation-dependent fundamental energy gap, quantum well, blue-violet laser, numerical analysis, crystal orientation-dependent optical performance, wavelength 454 nm to 462 nm, InGaN, cubic InGaN QW blue-violet laser, numerical simulation, gallium compounds, vertical cavity surface emitting laser, Crystals, crystal orientation, quantum well lasers, valance band, Strain, optical dispersion, wide band gap semiconductors, Vertical cavity surface emitting lasers, III-V semiconductors]
A new approach in pattern matching: Codon Detection in DNA and RNA using hash function (CDDRHF)
2015 18th International Conference on Computer and Information Technology
None
2015
Detection of different known patterns in any newly found sequence is one of the most important works in the field of Bioinformatics. Given a specific pattern, a long string has to be searched for finding that pattern. That means pattern matching is used for checking the sequence of tokens and the match has to be exact, obviously. These patterns usually have the form of either sequences or tree structures. In DNA and RNA sequences, this type of pattern searching is often needed to detect codons (both "met" and "stop" sequences) in it. This paper presents the concept and implementation of a new algorithm "codon detection in DNA and RNA using hash function (CDDRHF)" for detection of codons in long DNA and RNA sequences, using the concepts of hash function along with metaheuristic approaches. Then the gene between "met" and "stop" codons will easily be extracted to be used for proper alignment. This new algorithm works so fast that any DNA or RNA sequence of any length can be used to perfectly search for codons withinseconds.
[Algorithm design and analysis, RNA sequences, pattern matching, CODON, RNA, DNA sequences, Sequence alignment, codon detection, Amino acids, pattern matching:, metaheuristic approach, SSAADR algorithm, Proteins, hash function, Hash function, DNA, bioinformatics, Sugar]
Adaptive beamforming with a Microphone Array
2015 18th International Conference on Computer and Information Technology
None
2015
A robust and optimized system architecture has been developed and designed for adaptive beamformer with a Microphone Array. The system includes following subsystems - MMSE STSA Estimator, DOI (Direction of Interest) Estimator and an Adaptive Beamformer. The system architecture has been implemented and tested for Xtensa Processor which was configured for HiFi-2 DSP Standard for audio processing.
[Adaptive Beamformer, least mean squares methods, Correlation, Xtensa processor, adaptive beamforming, DOI Estimation, Microphone arrays, HiFi-2 DSP standard, MMSE, robust system architecture, array signal processing, microphone arrays, Estimation, DOI estimator, audio processing, Microphone Array, microphone array, Interpolation, direction of interest estimator, minimum mean square error short time spectrum amplitude estimator, optimized system architecture, Speech, MMSE STSA estimator, Delays, acoustic signal processing, digital signal processing chips]
SAL: An effective method for software defect prediction
2015 18th International Conference on Computer and Information Technology
None
2015
For software quality assurance, software defect prediction (SDP) has drawn a great deal of attention in recent years. Its goal is to reduce verification cost, time and effort by predicting the defective modules efficiently. In SDP, proper attribute selection plays a significant role. However, selection of proper attributes and their representation in an efficient way are very challenging due to the lacking of standard set of attributes. To address these issues, we introduce Selection of Attribute with Log filtering (SAL) to select a proper set of attributes. Our proposed attribute selection process can effectively select the best set of attributes, which are relevant for the discrimination of defected and non-defected software modules. Further, we adopt log filtering to pre-process the input data. We have evaluated the proposed attribute selection method using several widely used publicly available datasets. The simulation results demonstrate that our method is more effective to improve the accuracy of SDP than the existing state-of-the-art methods.
[Measurement, Software Defect Prediction, NASA, Log Filtering, Predictive models, Electronic mail, software quality, introduce selection of attribute with log filtering, software modules, Attribute Selection, selection process, Niobium, Simulation, Software, software quality assurance, defective modules, Balance, software defect prediction, SDP, SAL]
An improved bug localization using structured information retrieval and version history
2015 18th International Conference on Computer and Information Technology
None
2015
Locating buggy files is a time consuming and challenging task because defects can deflate from a large variety of sources. So, researchers proposed several automated bug localization techniques where the accuracy can be improved. In this paper, an information retrieval based bug localization technique has been proposed, where buggy files are identified by measuring the similarity between bug report and source code. Besides this, source code structure and frequently changed files are also incorporated to produce a better rank for buggy files. To evaluate the proposed approach, a large-scale experiment on three open source projects, namely SWT, ZXing and Guava has been conducted. The result shows that the proposed approach improves 7% in terms of Mean Reciprocal Rank (MRR) and about 8% for Mean Average Precision (MAP) compared to existing techniques.
[source code (software), program debugging, program testing, source code structure, Electronic mail, History, MRR, Accuracy, improved bug localization, Semantics, Guava project, Mathematical model, mean average precision, open source projects, Bug Localization, structured information retrieval, ZXing project, information retrieval, automated bug file localization techniques, buggy file ranking, Information retrieval, SWT project, Vector Space Model, configuration management, version history, bug report, Computer bugs, similarity measurement, mean reciprocal rank, Software, Information Retrieval, MAP]
Self organized sensor deployment with Brownian motion in wireless sensor and robot networks
2015 18th International Conference on Computer and Information Technology
None
2015
In this paper, we propose a novel self organized sensor deployment protocol to ensure proper surveillance through better coverage in urban search and rescue (USAR) operations using customized Brownian motion scheme. Specifically, we use density control over Brownian Motion (DCBM) to make an adaptive self deployment protocol which can be used on heterogeneous wireless sensor and robot networks. Generally, Brownian Motion scatters particles in a random manner with disjoint behaviour which is not appropriate to be used as a mobility model for wireless network. Additionally, the model has higher collision rate among the moving particles. To avoid the previous two issues, every node in the network computes node density and decides particular direction at any timestamp. The nodes may take pause in their movement and start over from that location instead of starting every time from the initial starting point to make the model behave continuously. We simulated DCBM scheme on a series of different topologies and compared with Random waypoint model that shows promising insights.
[Protocols, wireless sensor networks, DCBM scheme, disjoint behaviour, multi-robot systems, self deployment, Wireless communication, adaptive self deployment protocol, heterogeneous wireless sensor, Robot sensing systems, urban search and rescue, USAR operations, self organized sensor deployment protocol, surveillance, Mathematical model, wireless network, random waypoint model, robot networks, node density, Brownian motion, Wireless sensor network, Wireless sensor networks, customized Brownian motion scheme, mobility model, rescue robots, density control]
Optimized designs of reversible fault tolerant BCD adder and fault tolerant reversible carry skip BCD adder
2015 18th International Conference on Computer and Information Technology
None
2015
In recent years, reversible logic has become one of the most important areas of researches because of its applications in several technologies; such as low-power CMOS, Nano-computing and optical computing. In this paper, we have presented designs of a compact and efficient fault tolerant reversible Binary Coded Decimal (BCD) adder as well as a fault tolerant reversible Carry Skip BCD adder. We have proposed new reversible fault tolerant gates and heuristic algorithms to design compact BCD Adders. The proposed reversible fault tolerant BCD adder achieves the improvement as reducing cost of 23.07% on the number of gates, 52.67% on quantum cost, 31.03% on garbage outputs, 29.16% on the number of constant inputs and 23.07% on unit delay over the existing best one. Similarly, the proposed reversible fault tolerant carry skip BCD adder achieves the improvement as reducing cost of 34.72% on the number of gates, 43.24% on quantum cost, 37.5% on garbage outputs, 37.14% on the number of constant inputs and 34.72% on unit delay over the existing best one.
[reversible logic, optimized design, Overflow detection logic, reversible fault tolerant gates, carry logic, Fault tolerance, Quantum computing, adders, Fault tolerant systems, unit delay, Adders, cost reduction, Reversible, reversible fault tolerant carry skip BCD adder, UFT, heuristic algorithms, quantum gates, compact BCD adder design, Circuit faults, quantum cost, garbage outputs, Logic gates, fault tolerant computing, Delays, fault tolerant reversible binary coded decimal adder, Fault Tolerance and BAFTA etc, logic design]
Rain attenuation prediction analysis and contour map design over Bangladesh
2015 18th International Conference on Computer and Information Technology
None
2015
Rain fall causes drastic attenuation in wireless communication signal including mobile, satellite communication etc. Consequently, there is a substantial requirement to consider rain attenuation to design system for various terrestrial links. This paper contains annual R0.01% rain rate of 30 stations for 31 years around Bangladesh. Using ITU-R prediction model, total rain attenuation is obtained utilizing different time, distance and frequency. Vertical polarization is considered to measure the attenuation due to its less attenuation than horizantal. Moreover, the contour map is designed for different bands (C, Ka and Ku band) for vertical polarization. Furthermore, from the entire contour map, it is also observed that, rain attenuation is the most severe in Teknaf and least in Rajshahi.
[rain, Computational modeling, rain attenuation prediction analysis, terrestrial links, Contour Map, rain fall, Rain Rate, Predictive models, geophysics computing, vertical polarization, drastic attenuation, Earth, wireless communication signal, Rain, Bangladesh, satellite communication, Rain Attenuation, Attenuation, contour map design, ITU-R prediction model, Mathematical model, Satellite communication]
Optimal design of a low power UWB LNA for 5&#x2013;10 GHz application
2015 18th International Conference on Computer and Information Technology
None
2015
This paper introduces low power Ultra wideband LNA designed with lower supply voltage in 130nm CMOS RF technology for 5-10 GHz band application. The effect of parasitic and nonlinear relationship among performance parameters has been considered in the design process. Considering the maximum gain, low supply voltage has been used to reduce power consumption. Two designs have been proposed to distinguish the trade off among power, noise figure and linearity. Simulated results were compared with several published results on UWB LNA at simulation level.
[Noise figure, MOSFET, Ultra Wideband, CMOS RF technology, size 130 nm, power consumption, ultra wideband LNA, CMOS, Impedance matching, integrated circuit design, CMOS analogue integrated circuits, Power demand, low power UWB LNA, FCC, ultra wideband technology, noise figure, frequency 5 GHz to 10 GHz, microwave amplifiers, low noise amplifiers, RF, Logic gates, Impedance, low-power electronics, Gain, LNA]
Resource allocation for sum-power minimization in multiuser OFDM with user rate constraints
2015 18th International Conference on Computer and Information Technology
None
2015
This paper studies the resource allocation optimization problem under user rate constraints in a single cell multiuser orthogonal frequency division multiplexing system. Since the optimization problem does not have a convexity structure, it is numerically very difficult to solve and the optimal solution is computationally very expensive. We propose two efficient suboptimal solutions, i) based on Lagrange dual decomposition (LDD), and ii) based on separating the subcarrier and power allocation. The former solution employs subgradient method to optimize the dual variables. When the number of subcarriers is very large, it is shown that the solution obtained from LDD is optimal. However, because of non-convexity of the original optimization problem, the optimality is not guaranteed. The latter approach disjoints the subcarrier and power allocation. The subcarrier allocation is performed based on proportional data rate requirements, and finally optimal power allocation is performed under given subcarrier allocation. For both of the solutions, the achieved objective values are very close to the optimal solution, and have lower computational complexity.
[OFDM, Resource allocation, multiuser OFDM, subgradient method, multiuser orthogonal frequency division multiplexing system, LDD, Optimization, Ellipsoids, optimisation, resource allocation, user rate constraints, resource allocation optimization problem, sum power minimization, multi-access systems, OFDM modulation, Iterative methods, gradient methods, Multiuser OFDM, Minimization, power allocation, User rate constraints, Computational complexity, Sum-power minimization, Lagrange dual decomposition, Resource management, subcarrier allocation]
Clustering and detection of good and bad rail line anchors from images
2015 18th International Conference on Computer and Information Technology
None
2015
Absence of railway anchors/fasteners is a serious concern as it might lead to severe consequences such as train derailments. Hence regular inspection is an obligation to ensure safety. The third world countries choose the inspection process to be non-automatic where a trained operator moves along the rail line boarding a motor trolley checking for visual anomalies. In the previous research [1], an automatic system was proposed to overcome the cons of the running manual technique by using image processing. Two feature detection algorithms - Shi Tomasi and Harris Stephen - were used and an accuracy of 83.55% was achieved. This research presents an upgraded version of the previous work by introducing Neural Network. The addition of NN has not only speeded up the detection process but increased the accuracy significantly to approximately 93.86%.
[inspection process, NN, Railway accidents, visual anomalies, train derailments, neural network, Training, Feature detection, feature extraction, Rail transportation, rail line detection, railway anchors-fasteners, image processing, feature detection algorithms, mechanical engineering, automatic optical inspection, railway engineering, Rails, Neural Network, trolleys, Neural networks, motor trolley checking, rail line clustering, Feature extraction, Detection algorithms, neural nets]
An empirical study on constructing the Bangla Dirukto Shobdo for Universal Networking Language (UNL)
2015 18th International Conference on Computer and Information Technology
None
2015
Now a day the World Wide Web (WWW) is a most triumphant communication medium. This represents revolutionary tools to communicate and access information, which enables to access innumerable documents on a huge variety of topics. Though the profusion of information, languages frequently cause troubles. Conversion from another language to Bangla language is instantly a highly demand for Bangladeshi peoples because they are inducing so rapidly on digital contracting and web resources. The Universal Networking Language (UNL)[5] deals to minimize above problems. This helps to conquer the language blockade among citizens of different nations to solve problems rising from current globalization trends. In this paper we propose the psychiatry of Bangla Dirukta Shobdo and translation of Bangla sentences to UNL expression. Here we actually are paying attention on a concept in bangla "Dirukta Shobdo" from Bangla Grammar and what are the principle of converting dirukto shobdo to UNL language and the process of implementation.
[Computers, document handling, Bangla Language Processing, Dictionaries, natural language processing, Bangladeshi peoples, Knowledge based systems, Dirukta Shobdo, World Wide Web, Grammar, language blockade, UNL, Bangla Dirukto Shobdo, universal networking language, WWW, digital contracting, Speech, Internet, Bangla language, Web sites, Web resources, language translation]
A novel approach to obtain trajectories of targets from laser scanned datasets
2015 18th International Conference on Computer and Information Technology
None
2015
Laser scanner has several bons when compared with video camera. It does not record real world videos except scanned points. As a result, processing of data becomes faster and easier. Over and above, it takes away the problem of private life conservation. This paper proposes a new and competent computer vision based approach for detecting and tracking targets (e.g., pedestrians and vehicles) from laser scanned datasets. Laser scanned data points from each scan have been deemed as a video frame. Blobs are extracted and then computer vision techniques (e.g., Kalman filter, Hungarian algorithms, and etc.) are applied to recognize and track the kind of targets. Scanned datasets, collected from two kinds of laser scanners, were used to conduct experiments. Full trajectories of pedestrians, vehicles, and noises were resulted in three dimensional spaces. Experimental results give evidence of the efficacy of our proposed framework.
[Legged locomotion, Target tracking, Estimation, video cameras, video camera, optical scanners, Vehicles, target detection, laser scanned datasets, video frame, computer vision, target tracking, computer vision-based approach, Trajectory, laser scanner, Kalman filters, video signal processing]
An efficient method for video summarization using moving object information
2015 18th International Conference on Computer and Information Technology
None
2015
Video surveillance system captures continuous video for the purpose of security, monitoring, investigating and so on. It requires huge memory space to store as well as enormous time to retrieve important information manually from this high volume of videos. In this paper, we propose a novel video summarization scheme using moving object information by considering area of moving objects extracted from dynamic background modeling and frame-to-frame object motion. Through this scheme we rank all frames according to the importance of being key frame by combining moving object features through a fusion method so that users can select desired length of videos for summary. The experimental results show that the proposed method provides better video summary compared to the state-of-the-art method using a publicly available benchmark BL-7F video surveillance dataset.
[continuous video capture, image fusion, fusion method, frame difference, Security, dynamic background modeling, Dynamics, feature extraction, moving object information, video signal processing, video surveillance, video summarization, frame-to-frame object motion, video surveillance system, image capture, moving object area, memory space, image motion analysis, BL-7F video surveillance dataset, Feature extraction, Cameras, Video surveillance, moving object features, video length, Background modelling]
Performance analysis of a multi-antenna based cooperative relaying scheme over fading channels in spectrum sharing environment
2015 18th International Conference on Computer and Information Technology
None
2015
This paper presents an outage analysis of a cooperative relaying scheme over flat Rayleigh and Nakagami-m fading channels. In the proposed scheme, secondary transmitters cooperatively relay the primary traffic. Each secondary transmitter, equipped with multiple antennas, is divided into one of the two clusters: a cooperative cluster (CC) and a non-cooperative cluster (NCC). Cluster head (CH) of the CC will be selected as a best decode-and-forward (DF) relay and will forward the primary information. Results show that the proposed scheme outperforms both non-cooperative, conventional single-antenna systems and random relay selection schemes in terms of outage probability. In each case, theoretical results are verified with Monte-Carlo simulation results.
[Rayleigh and Nakagami-m fading, spectrum sharing environment, noncooperative cluster, multi-antenna, Nakagami-m fading channels, Relays, relay selection schemes, decode and forward communication, Nakagami channels, cluster head, cooperative relaying, Monte Carlo methods, decode-and-forward relay, fading channels, outage probability, cognitive radio, relay networks (telecommunication), CC, radio spectrum management, antennas, Radio transmitters, CH, DF relay, Rayleigh channels, multiantenna performance analysis, Monte-Carlo simulation, secondary transmitters, NCC, Nakagami distribution, cooperative relaying scheme, radio transmitters, cooperative cluster, Antennas, Signal to noise ratio]
Watermarking approach of embedding patient facial information into RONI of Brain CT scan image
2015 18th International Conference on Computer and Information Technology
None
2015
This paper presents a watermarking approach which has been proposed for embedding patient facial information into region of non-information (RONI) part of Brain CT (computed tomography) scan image for image authentication and integrity verification. As the doctors diagnose from region of information (ROI), modification of even one bit is not tolerated in ROI. Therefore, the integrity of ROI must be strictly controlled. In the proposed approach, the information of ROI has been embedded into patient face image, which has been embedded into RONI of Brain CT scan image for authentication. The proposed method is based on two level discrete wavelet transform (DWT). Two dimensional Arnold's cat map and encoding technique have been used in proposed scheme to scramble and encode the patient face image before embedding. The scrambled and encoded patient face image with information of ROI has been embedded into the coefficients of RONI of Brain CT scan image by technically using additive algorithm. As the facial information has been technically embedded using additive algorithm, no original document is required in extraction stage except secret key, which means the proposed approach focused on blind watermarking. Experimental results obtained from proposed method have been compared with other existing methods by using several performance parameters including mean square error, peak signal to noise ratio and structural similarity index. From experimental results, it can be seen that the proposed method outperforms the other methods by confirming good quality of watermarked image.
[Additives, image watermarking, peak signal to noise ratio, Watermarking, additive algorithm, watermarked image, patient face image, brain CT scan image, region of information, structural similarity index, Arnold's cat map, Computed tomography, face recognition, patient facial information, blind watermarking, region of noninformation, encoding technique, Face, medical image processing, RONI, computed tomography, DWT, discrete wavelet transforms, discrete wavelet transform, mean square error, Discrete wavelet transforms, ROI, image authentication, watermarking, computerised tomography, Authentication, integrity verification]
Regulatory requirements compliance in e-Government service development
2015 18th International Conference on Computer and Information Technology
None
2015
Electronic Government has been gathering momentum in research and practices since last decade. It is the contribution and benefit of ICTs to improve the government services to citizen and business organizations through the efficient intra-government operations. However, the introduction of e-Government services have become challenging nowadays because of the growing number of policies and regulations that needs to be compliant in the e-Government system development. The non-compliance of the regulatory requirements from policy and regulation may cause a significant consequence in the e-Government project development. This paper investigates and analyzes various concepts of e-Government regulatory requirements compliance from published literatures employing Systematic Literature Review (SLR). The results and analysis of this paper can serve the e-Government researchers and practitioners as a means of better understanding the ambiguous issues of regulatory requirements in e-Government system development to be compliant with the enacted policies and regulations.
[Regulatory requiremetns, government policies, e-Government, Compliance, Ontologies, e-Government regulatory requirements compliance, e-Government project development, business organizations, government service improvement, e-Government system development, Systematics, Bibliographies, Collaboration, SLR, ICT, electronic government, intra-government operations, e-Government regulations and policies, government data processing, systematic literature review, Electronic government]
Analytical evaluation of BER performance of a power line communication system using symmetrical alpha stable (S&#x03B1;S) impulsive noise model
2015 18th International Conference on Computer and Information Technology
None
2015
An Analytical approach is presented to evaluate the effect of impulsive noise on the bit error rate performance of a OFDM power line communication system in the presence of background non-white Gaussian noise. The symmetric alpha-stable (S&#x03B1;S) model of impulsive noise is considered to evaluate the BER. The results are evaluated numerically for several power line noise bandwidth and background Gaussian noise to impulsive noise ratio. The results show that there is significant deterioration of BER performance due to impulsive noise and system suffers penalty in receiver sensitivity which is about 50dB, 45dB, 39dB and 35dB for the ratio of Gaussian noise to impulsive noise variance, &#x0393;= 0.1, 0.05, 0.025 and 0.01 respectively at a skewness and heaviness of the tails of the density function, &#x03B1; = 0.1. The analytical results are found to be in good agreement with simulation results reported earlier.
[OFDM power line communication system, background Gaussian noise to impulsive noise ratio, attenuation, OFDM, Bit error rate, receiver sensitivity, carrier transmission on power lines, background nonwhite Gaussian noise, power line noise bandwidth to impulsive noise ratio, Gaussian noise, receivers, bit error rate performance, impulse noise, PSD (Power Spectral Density), OFDM modulation, skewness, Mathematical model, error statistics, Computational modeling, Power line communications, STBC (Space Time Block Code), BER performance analytical evaluation, Noise measurement, S&#x03B1;S impulsive noise model, symmetrical alpha stable impulsive noise model, SαS (Symmetric α-Stable)]
Android assistant EyeMate for blind and blind tracker
2015 18th International Conference on Computer and Information Technology
None
2015
At present many blind assistive systems have been implemented but there is no such kind of good system to navigate a blind person and also to track the movement of a blind person and rescue him/her if he/she is lost. In this paper, we have presented a blind assistive and tracking embedded system. In this system the blind person is navigated through a spectacle interfaced with an android application. The blind person is guided through Bengali/English voice commands generated by the application according to the obstacle position. Using voice command a blind person can establish voice call to a predefined number without touching the phone just by pressing the headset button. The blind assistive application gets the latitude and longitude using GPS and then sends them to a server. The movement of the blind person is tracked through another android application that points out the current position in Google map. We took distances from several surfaces like concrete and tiles floor in our experiment where the error rate is 5%.
[handicapped aids, Bluetooth, Server, Bengali/English voice command, Acoustics, JSON, GPS, Servers, Android (operating system), Fingers, blind tracking embedded system, Modulation, Android assistant EyeMate, blind tracker, blind assistive system, GSM, Google, mobile radio, natural language processing, Microcontroller, obstacle position, Pulse, RFID, Google map, Global Positioning System, Android, Analog, Arduino, Smart phones, Ultrasonic]
A motion detection algorithm for video-polysomnography to diagnose sleep disorder
2015 18th International Conference on Computer and Information Technology
None
2015
Sleep is a vital biological function. Polysomnography is usually used to determine sleep disorder but it is a partly invasive and interfering method due to attachment of many electrodes on to the body of the patient involved in monitoring process. Therefore patients already having some sort of sleep abnormality may face greater difficulty in sleeping resulting in improper diagnosis. But our proposed video-polysomnography method can overcome this problem where the whole night sleep is recorded and analyzed later. In this paper it is proposed that we can create a shortened motion video out of the whole night sleep video, containing only the movement related portions and make a motion index graph from full length sleep video. From the shortened motion video, a sleep analyst can diagnose for sleep disorder by analyzing that video in a short time. Along with this the motion index graph shows the frequency, level and timing of motion during sleep. For primary detection of sleep disorder it can be a low cost, simply implemented and noninvasive method of polysomnography.
[motion timing, biological function, Motion Detection, motion index graph, Image and video processing, patient monitoring, sleep abnormality, shortened motion video, Electrodes, sleep, Webcams, Sleep Disorder, Motion index graph, motion estimation, Motion detection, whole night sleep video recording, video signal processing, medical image processing, Shortened motion video, video recording, monitoring process, motion level, Video-polysomnography, full length sleep video, Indexes, Polysomnography, medical disorders, Sleep, Software packages, motion detection algorithm, sleep disorder diagnosis, video-polysomnography method, motion frequency]
Performance limitations of a WDM coherent optical transmission system due to Raman amplifier induced crosstalk
2015 18th International Conference on Computer and Information Technology
None
2015
Analysis is presented for a wavelength division multiplexing (WDM) transmission system with optical on-off-keying (OOK) modulation and heterodyne detection taking into account the effect of Raman amplifier induced crosstalk and beat noise components arising out of beating of pump power, signal power and crosstalk during the photodetection process. The results are numerically evaluated at a bit rate of 10 Gbps for several WDM channel, interchannel spacing, fiber length and local oscillator power in the receiver. The results are presented in terms of signal to crosstalk plus beat noise ratio and bit error rate (BER). It is found that BER deteriorates due to crosstalk induced by Raman amplifier at higher number of WDM channel and higher transmission distance. The allowable transmission distance for a given BER of 10-9 and number of WDM channel are also evaluated. For example, the allowable transmission distance is approximately 92 Km for the number of channel is 4 whereas it reduces to almost 7 Km when the number of channel is increased to 16, at a BER of 10-9 with pump power of 10 mW and interchannel spacing 12.5 GHz.
[Local oscillators, light coherence, wavelength division multiplexing, WDM channel, noise component, Bit error rate, Crosstalk, pump power beating, frequency 12.5 GHz, light transmission, Raman amplifier, fiber length, optical crosstalk, WDM coherent optical transmission system, error statistics, coherent detection, photodetection process, power 10 mW, optical on-off keying modulation, interchannel spacing, transmission distance, Wavelength division multiplexing, OOK modulation, amplitude shift keying, bit error rate, bit rate 10 Gbit/s, bit error rate (BER), BER, Optical fiber amplifiers, optical modulation, Stimulated emission, crosstalk, signal power, heterodyne detection, wavelength division multiplexing (WDM), Optical crosstalk, performance limitation, local oscillator power, Raman amplifier induced crosstalk]
Forecasting US NASDAQ stock index values using hybrid forecasting systems
2015 18th International Conference on Computer and Information Technology
None
2015
Capability to predict precise future stock values is the most important factor in financial market to make profit. Because of virtual trading, now a day this market has turn into one of the hot targets where any person can earn profit. Thus, predicting the correct future value of a stock has become an area of hot interest. This paper attempt to forecast NASDAQ stock index values using novel hybrid forecasting models based on widely used soft computing models and time series models. The daily historical US NASDAQ closing stock index for the periods of 08 February 1971 to 24 July 2015 is used and is applied our proposed hybrid forecasting models to see whether considered forecasting models can closely forecast daily NASDAQ stock index values. Mean absolute error and root mean square error between observed and predicted NASDAQ stock index are considered as evaluation criterions. The result is compared on the basis of selected individual forecasting time series model and individual soft computing forecasting models and the proposed hybrid forecasting models. Our experimental evidences show that the proposed hybrid back-propagation artificial neural network and genetic algorithm forecasting model has outperformed as compare to other considered forecasting models for forecasting daily US NASDAQ stock index. We trust that daily US NASDAQ stock index forecasts will be notice for a number of spectators who wish to construct strategies about this index.
[financial market, Predictive models, Soft computing forecasting model, hybrid forecasting systems, Genetic programming, Statistical evaluation measures, stock markets, mean absolute error, soft computing models, genetic algorithm forecasting model, Computational modeling, Biological system modeling, Time series analysis, root mean square error, hybrid back-propagation artificial neural network, Time series, Time series forecasting model, time series, genetic algorithms, Indexes, Forecasting, time series models, Fuzzy logic, stock value prediction, Hidden Markov models, backpropagation, Forecast, US NASDAQ stock index value forecasting, virtual trading, neural nets]
Fast inter-mode decision strategy for HEVC on depth videos
2015 18th International Conference on Computer and Information Technology
None
2015
Multiview video employs the utilization of both texture and depth video information from different angles to create a 3D video for more realistic view of a scene. Unlike texture, depth video is a gray scale map that represents the distance between the camera and 3D points in a scene. Existing multiview video coding (MVC) techniques including 3D-High Efficiency Video Coding (HEVC) standard encode both texture and depth videos jointly by exploiting texture video information for the corresponding depth video coding (DVC) to reduce computational time as the texture and depth videos have motion similarity in representing the same scene. This strategy has two limitations: (i) more bits and computational time might be required due to the large residuals for the misalignment between depth and texture edges and (ii) switching between different views may require more times due to the increased dependency between texture and depth. In this paper, we propose an independent DVC technique using HEVC (a video coding standard for single view) so that we can improve the rate distortion (RD) performance and reduce computational time by improving switching speed. For this, we use motion features to reduce a number of motion estimation (ME) and motion compensation (MC) modes in HEVC. As we use motion feature which is the underlying criteria for selecting different modes in the standard and then we select a subset of modes which can provide almost the same RD performance. Experimental outcomes reveal a reduction of 48% encoding time of HEVC encoder with similar RD performance and better interactivity.
[Video coding, realistic view, Correlation, depth videos, 3D video, video coding standard, 3D high efficiency video coding, Intermode selection, HEVC, motion compensation, Videos, scene, depth video coding, Three-dimensional displays, RD performance, 3D points, motion estimation, camera, computational time, rate distortion performance, Motion features, HEVC standard, HEVC encoder, depth video information, multiview video coding, Encoding, MVC techniques, motion similarity, video coding, realistic images, gray scale map, MC modes, Standards, image texture, inter-mode decision strategy, Feature extraction, texture video information, texture edges, Depth video, DVC technique, ME modes]
Optimization of 2&#x00D7;2 MZI electro-optic switch and its application as logic gate
2015 18th International Conference on Computer and Information Technology
None
2015
Electro-optic Mach-Zehnder interferometer (MZI) switches are well known devices in optical systems for their high speed of operation. In next generation optical network, such switches have wide applications due to their high switching capability and great reliability. In this paper we have investigated the enhanced performance of an electro-optic switch made by titanium diffused lithium niobate (Ti:LiNbO<sub>3</sub>) functioned as waveguide medium based on integrated Mach-Zehnder interferometer (MZI) at 1.55 μm wavelength. The performance of the switch is optimized for low switching voltage, low insertion loss, high extinction ratio. This is done by varying the strip thickness of titanium, interferometer arm length, arm gap, waveguide width, gap in the coupling region of the 3 dB couplers, and center position of second electrode deposited on integrated Mach-zehnder Interferometer arm with respect to the original axis of the switch. The designed switch gives lowest insertion loss of 0.00881dB and highest extinction ratio of 33.581 dB at 8.8 V.
[Optical interferometry, interferometer arm length, Mach-Zehnder interferometer, waveguide width, waveguide medium, arm gap, Insertion loss (IL), Extinction ratio (Ex.R), low insertion loss, optimisation, titanium diffused lithium niobate, Titanium, Extinction ratio, Nonlinear optics, logic gate, Optical switches, electro-optical switches, OptiBPM12.2, Electro-optic switch, Electrooptical waveguides, Mach-Zehnder interferometers, low switching voltage, logic gates, MZI electrooptic switch, electrooptic Mach-Zehnder interferometer switches, Insertion loss, titanium strip thickness, Lithium Niobate, Directional couplers]
Implementation of remote wireless communication laboratory for enhancing engineering education in developing countries
2015 18th International Conference on Computer and Information Technology
None
2015
At present a large number of skilled engineers are required in the wireless communication sector with a deeper understanding of all sorts of wireless technology both practical and theoretical. However, in most of the developing countries such as Bangladesh it is not possible to provide the latest and relevant practical experiences to each of the students due to limited resources. So an appropriate method for improving and enriching engineering education in developing countries is essential. This paper focuses on the design and implementation of web accessible remote laboratory for wireless communication to utilize limited resources for enhancing engineering student's practical knowledge as well as self learning ability. This lab allows the student to perform experiment from their home or any preferred location using the internet connection. Several analog and digital modulation based experiments have been designed for this remote wireless communication lab. Various kinds of performance analysis parameter for wireless communication system at the receiver are also analyzed. This remote laboratory system has been designed using LabVIEW application software, MATLAB, USRP, Ethernet switch and HD web camera.
[web accessible remote laboratory implementation, Bit error rate, remote wireless communication lab, MATLAB, engineering education enhancement, LabVIEW application software, Wireless communication, cameras, remote wireless communication laboratory implementation, Modulation, Ethernet switch, telecommunication engineering education, Remote laboratories, HD web camera, Instruments, self learning ability, Eye diagram, G-server, radio receivers, Virtual instruments, BER, virtual instrumentation, Pulse shaping filter, Constellation diagram, developing countries, digital modulation, analog modulation, computer aided instruction, Internet, USRP, EVM, internet connection]
Acoustic modeling using deep belief network for Bangla speech recognition
2015 18th International Conference on Computer and Information Technology
None
2015
Most of the Speech Recognition (SR) systems use Hidden Markov Model (HMM) for acoustic modeling and Gaussian Mixture Model (GMM) for state modeling. Artificial Neural Network (ANN) based methods are also found as a good replacement of GMMs in SR system development. This paper presents a method for Bangla SR using Deep Belief Network (DBN) which is probabilistic generative ANN composed by multiple layers of restricted Boltzmann machine along with HMM. At first Mel Frequency Cepstral Coefficients is used extract features from the speech data. Then DBN is trained with these feature vectors to calculate each of the phoneme states. After that Viterbi decoder is used to determine the resulting hidden state sequence that generates the word. The training of DBN is performed in two steps. At first generative pre-training is used to train the network layer by layer. In the second step, enhanced gradient is used to slightly adjust the model parameters to make it more accurate. Total 840 utterances (20 utterances for each of 42 speakers) of the words are used in this study. The proposed method is shown satisfactory recognition accuracy and outperformed other prominent existing methods.
[hidden Markov model, phoneme states, probabilistic generative ANN, generative pretraining, Boltzmann machines, Gaussian mixture model, ANN based methods, artificial neural network based methods, hidden Markov models, DBN, speech recognition, Bangla speech recognition, feature extraction, cepstral analysis, Bangla SR, deep belief network, Restricted Boltzmann Machine, state modeling, Hidden Markov Model, belief networks, Speech Recognition, Gaussian Mixture Model, GMM, Viterbi decoder, HMM, acoustic modeling, Deep Belief Network, hidden state sequence, enhanced gradient, feature vectors, Gaussian processes, restricted Boltzmann machine, mixture models, acoustic signal processing, Mel frequency cepstral coefficients]
Context likelihood of relatedness with maximal information coefficient for Gene Regulatory Network inference
2015 18th International Conference on Computer and Information Technology
None
2015
In this study, we have investigated the recently proposed association detector method Maximal Information Coefficient (MIC) instead of Mutual Information (MI) in inferring Gene Regulatory Network (GRN). GRN plays an important role to understand the interactions and dependencies of genes in different conditions from gene expression data. An information theoretic GRN method first computes dependency matrix from the given gene expression dataset using an entropy estimator and then infer network using individual inference method. A number of prominent methods use MI because it is an efficient approach to detect nonlinear dependencies. But MI does not work well for continuous multivariate variables. In this paper, MIC incorporated into the prominent MI based GRN method Context Likelihood of Relatedness (CLR) and proposed CLR-MIC. To understand the effectiveness of MIC in GRN inference, SynTReN generated synthetic data and SOS E. Coli real gene expression data were considered. The experimental results revealed that proposed CLR-MIC outperformed its counter standard CLR and identified the proficiency of MIC in GRN inference.
[gene expression data, MI based GRN method, SOS E. Coli real gene expression, information theoretic GRN method, Mutual Information, Maximal Information Coefficient, maximal information coefficient, dependency matrix, maximum likelihood estimation, matrix algebra, genetics, biology computing, gene regulatory network inference, entropy estimator, context likelihood of relatedness, Gene Regulatory Network, Nonlinear Dependence, SynTReN]
FP-ANK: An improvised intrusion detection system with hybridization of neural network and K-means clustering over feature selection by PCA
2015 18th International Conference on Computer and Information Technology
None
2015
Intrusion Detection System (IDS) predominantly works for detecting malicious attacks. Many researchers have proposed the IDS with different techniques to achieve the best accuracy with the consolidation of Clustering and Artificial Neural Network (ANN). Clustering and ANN based models give better precision rate with better accuracy where attack records are low. Nevertheless, all the features of dataset are not relevant for classifying different attacks. So, feature selection can improve the stability and accuracy of IDS. In this paper, it is proposed that IDS with the amalgamation of best efficient features selected by Principal Component Analysis (PCA) can reduce the computational complexity of the system. It has been combined with the K-means clustering technique to cluster the specific groups of attacks and Artificial Neural Network to get a preeminent output by training the formulation of different base models. The model name has been defined by FP-ANK model. Investigational results have been reported on the NSL-KDD dataset where the accuracy rate associating with other models is distinct to validate the proposed system.
[artificial neural network, ANN, Artificial neural networks, NSL-KDD, Intrusion Detection System, PCA, Training, intrusion detection system, k-means clustering, security of data, pattern clustering, Artificial Neural Network, Intrusion detection, FP-ANK model, Feature extraction, IDS, Data models, Principal Component Analysis, feature selection, neural nets, principal component analysis, Principal component analysis, K-means Clustering]
Design of multi-objective UPFC employing backtracking search algorithm for enhancement of power system stability
2015 18th International Conference on Computer and Information Technology
None
2015
In the arena of power system, small signal stability is one of the most important issue to be settled down. In relatively weak tie line interconnections, these low frequency oscillations are observed. Use of Flexible AC Transmission System (FACTS) devices is playing most important roles in damping out of those oscillations. Unified Power Flow Controller (UPFC), one of the key members of FACTS family is being used in long and high voltage transmission networks of modern power system. Tuning of the UPFC parameters in real time power system application is a multi-objective optimization problem. In this paper the parameters of UPFC has been optimized employing a novel optimization approach, Backtracking Search Algorithm (BSA). The efficacy of the proposed BSA tuned UPFC has been investigated by comparing the time domain simulation results with fixed gain conventional UPFC. Additionally, the Eigenvalues of optimized and conventional UPFC of the power system has also been compared.
[Damping, eigenvalues, Power System Stability, real time power system application, small signal stability, Power system stability, Eigenvalue, flexible AC transmission systems, Optimization, modern power system, multiobjective optimization problem, eigenvalues and eigenfunctions, backtracking search algorithm, optimisation, FACTS devices, Sociology, Multi-objective Optimization, power system interconnection, Mathematical model, low frequency oscillations, time domain simulation results, FACTS, power system stability, multiobjective UPFC, Backtracking Search Algorithm, Stability analysis, Statistics, load flow control, relatively weak tie line interconnections, backtracking, unified power flow controller, flexible AC transmission system, high voltage transmission networks, UPFC]
A wearable sensor based elderly home care system in a smart environment
2015 18th International Conference on Computer and Information Technology
None
2015
Elderly people need 24-hours care and support for their physical disability and mental weakness. With the ever growth of the elderly people this section needs some importance. But 24-hours caring and monitoring can't be provided by the family members and caregivers because of their daily affairs. So the question arises here how they gives supportive care to their elderly. The answer is related to the using technology. In every sphere of our life technology is used for making the life easier and comfortable. So the caregivers monitor and care their elderly people with the help of technology. These kind of technologies introduced smart environment. The smart environment informs the caregivers about the current position and status of the elderly people and their living environment. In this paper we introduced a wearable sensor based elderly home care system in a smart environment. We have used personal computers as monitoring system, sensors for sensing data, webservice to communicate between monitoring systems. From our user study we can see that our system is not only useful but also would be helpful for further research in this domain.
[handicapped aids, telemedicine, wearable sensor, Web service, Sensor systems, mental weakness, elderly care, Postal services, physical disability, personal computer, wearable computers, geriatrics, Web services, sensors, elderly home care system, monitoring system, Senior citizens, smart environment, wearable sensors, Biomedical monitoring, Monitoring, health care]
Computer vision based Bengali sign words recognition using contour analysis
2015 18th International Conference on Computer and Information Technology
None
2015
This paper presents a computer vision based Bengali sign words recognition system using contour analysis. Haar-like feature based cascaded classifier is used to locate the predefined hand posture (Opened Hand and followed by Closed Hand postures) from the captured image, and bounded by a rectangular box that is initialized as region of interest (ROI). The system follows this ROI, crops it and normalizes into predefined size. The system segments skin-like area based on Hue and Saturation value from the normalized image. Then the system employs morphological operations and Gaussian smoothing to remove noises, and then converts it into gray image. The system extracts contours using Canny edge detector and encodes extracted contours into Vector Contours (VC). After scaling VC into predefined size, the system generates feature space based on equalized VC, value of normalized Auto-Correlation Function (ACF) and ACF descriptors for each sign word that will be used for training and/or testing process. The system recognizes sign words based on maximum similarity between tests and predefined training contour templates using Inter-Correlation Function (ICF). The system is trained and tested using 1800 (18&#x00D7;10&#x00D7;10) contour templates separately for 18 Bengali sign words from 10 signers achieving recognition accuracy of 90.11% with computational cost of 26.063 milliseconds per frame.
[Auto-Correlation Function (ACF), Bengali sign word recognition, normalized auto-correlation function, Bengali Sign Language (BdSL), contour analysis, image segmentation, edge detection, hue-and-saturation value, morphological operations, sign language recognition, Skin Color Based Segmentation, Cannv edge detector, training contour templates, gray image, inter-correlation function, Inter-Correlation Function (ICF), Haar, ICF, rectangular box, Gaussian smoothing, normalized image, normalized ACF, scaling VC, Contour Analysis, computer vision, Gaussian processes, Haar transforms, Vector Contour (VC), vector contours, image coding]
Performance analysis of an FSO link in presence of pointing error using multiple PIN photodetectors with equal gain combiner
2015 18th International Conference on Computer and Information Technology
None
2015
An analysis is carried out to evaluate the conditional bit error rate conditioned on a given value of pointing error for a Free Space Optical (FSO) link with multiple PIN photodetector receivers using Equal Gain Combining (EGC). The probability density function (pdf) of output signal to noise ratio (SNR) with receive diversity is also derived in presence of pointing error using EGC. The average BER of a SIMO FSO link is analytically evaluated by averaging the conditional BER over the pdf of the output SNR to find out the numerical results. Number of approaches is presented to find the average BER for SIMO FSO link to evaluate the best approach. The BER performance results are evaluated for several values of pointing error with OOK modulation and direct detection. The numerical evaluation show that, the SIMO FSO system suffers significant power penalty due to pointing error and can be reduced by receive diversity. The improvement of receiver sensitivity over a single receiver is about 6 dB and 9 dB when the number of photodetector is 4 and 8 respectively at a BER of 10-10. It is also noticed that, FSO system with receive diversity can tolerate higher values of pointing error at a given BER and transmit power.
[SIMO FSO link, output SNR, Bit error rate, Equal Gain Combining (EGC), receiver sensitivity improvement, optical receivers, Bit Error Rate (BER), photodetectors, conditional bit error rate evaluation, multiple PIN photodetector receivers, equal gain combiner, direct detection, free space optical link, free-space optical communication, On-Off Keying (OOK), Photodetector etc, error statistics, probability density function, Diversity Technique, Free-Space Optical (FSO), EGC, optical links, diversity reception, Pointing Error, probability, Receivers, Photodetectors, OOK modulation, Optical transmitters, amplitude shift keying, BER, pointing error, pdf, Diversity reception, numerical evaluation, Adaptive optics, signal to noise ratio, performance analysis, power penalty, Signal to noise ratio]
A simple approach to count and track underwater fishes from videos
2015 18th International Conference on Computer and Information Technology
None
2015
Fishes are of great importance to the ecosystem. Behavior of fishes is interesting. Counting and tracking of fishes can provide good knowledge about the behavior of fishes. Counting and behavior quantifying of fishes within a turbulence or trawl environment are challenging tasks. The traditional methods are not only inefficient but also expensive. Thus counting and tracking under water fishes from videos are emerging topic for ichthyologists. This paper addresses a simple method to count and track underwater fishes from videos. It is a hybrid of background subtraction, Hungarian algorithm, and Kalman filter. It enables tracking of fishes whose number can vary over time. Theoretical runtime of the tracking algorithm is O(n3) with problem size n. Experimental results demonstrate its effectiveness.
[tracking algorithm, Target tracking, ecosystem, Estimation, Kalman filter, videos, turbulence, trawl environment, Videos, Image segmentation, aquaculture, Hungarian algorithm, fish tracking, fish behavior, Kalman filters, background subtraction, Marine animals, video signal processing, underwater fishes, ichthyologists, fish counting]
An efficient REDCap based data collection platform for the Primary Immune Thrombocytopenia and its analysis over the conventional approaches
2015 18th International Conference on Computer and Information Technology
None
2015
Primary Immune Thrombocytopenia (ITP) is a rare chronic disease. The registry of ITP contains all the necessary information for its research and analysis. The registry is thoroughly dependent on its underneath data collection platform. Generally, data collection for the ITP conducted on a routine basis and over a long period of time. However, most of the existing web based data collection approaches compromise the data security, data backup, flexibility and scalability. The REDCap based system can overcome these weaknesses as it has easier migration, validation, extraction and evolution processes. Thus, this paper develops a REDCap based data collection platform for the ITP registry. Then a detail comparative analysis between the two systems has been presented.
[REDCap based data collection platform, data extraction process, Research, Servers, primary immune thrombocytopenia, data migration process, Databases, data validation process, Immune system, Primary Immune Thrombocytopenia, Data Collection Platform, ITP registry, diseases, medical information systems, Registry, research electronic data capture tool, Diseases, SQL, chronic disease, Data collection, REDCap, Software, data evolution process, Hemorrhaging, Chronic Disease]
An improved position based power aware routing algorithm in mobile ad-hoc networks
2015 18th International Conference on Computer and Information Technology
None
2015
In this paper we introduced an improved scheme based on a weighted metric of remaining battery power, speed and distance of nodes for determining routes in wireless Mobile Ad hoc Networks (MANET). For the cases where significant difference in the velocities of the communicating nodes or the battery power of the intermediate nodes is low, traditional schemes fail to establish the communication among nodes with reliable QoS. We proposed a new algorithm that uses weighted combination of metrics of distance, velocity and battery power in selecting the route over earlier MFR (Most Forward within Radius) method. The proposed scheme encompasses the load balancing issues and eventually it increases the network lifetime and network performance. Simulation experiment showed that the proposed algorithm reduces the packet loss than that of existing MFR algorithm. Experimental results also revealed that besides packet loss, the proposed strategy achieves higher throughput (14.35%) rate than that of existing MFR. Furthermore, usages of these new metrics ensure the higher mean time to node failure.
[battery power, weighted combination, Packet loss, Power Efficient and Weight based Algorithm, packet loss, Routing, Throughput, Batteries, quality of service, Mobile ad hoc networks, weighted metric, MANET, mobile ad-hoc networks, Position based Routing, mobile ad hoc networks, telecommunication network routing, telecommunication network reliability, communicating nodes, MFR, Routing protocols, reliable QoS, power aware routing algorithm, most forward within radius method]
An efficient and blind audio watermarking technique in DCT domain
2015 18th International Conference on Computer and Information Technology
None
2015
A blind audio watermarking algorithm in Discrete Cosine Transform domain using the relationship among the consecutive groups of samples is proposed in this work. Here, we divided the DCT represented audio signal into equal-sized non-overlapping segments which in turn divided into four non-overlapping consecutive frames. Here, an informative relation between the frames in a segment is maintained to indicate the successful embedding of a watermark i.e. in case of embedding `1' in a segment, make the value of the `difference between the first two frames' higher than the value of the `difference between the last two frames' at a distance of threshold value and we do the opposite if the watermark bit is `0'. Embedding a single watermark bit in multiple segments makes the scheme robust. In this work, the size of frame and the segment entirely depend on the number of watermark bit which helps the proposed scheme to increase the capacity of watermarking. Additionally, in order to maintain security against hackers a secret key permutation is proposed to apply on the watermark message. The experimental result shows that the method provides high robustness against common signal manipulation as well as high capacity without significant perceptual distortion.
[Algorithm design and analysis, secret key permutation, DCT domain, Watermarking, discrete cosine transform domain, Time-domain analysis, security, equal-sized nonoverlapping segments, audio signal representation, segment, Robustness, Discrete cosine transforms, perceptual distortion, permutation, blind audio watermarking technique, watermark message, discrete cosine transforms, nonoverlapping consecutive frames, blind audio watermarking, signal representation, signal manipulation, divide DCT, audio signal processing, audio watermarking, watermark bit, private key cryptography, Signal to noise ratio, frame]
Electrical pole climbing robot: For wiring and repairing distribution lines
2015 18th International Conference on Computer and Information Technology
None
2015
The basis of this project is to create an electrical pole climbing robot which can be used to reduce the risk of an electrician to connect the distribution lines for supplying purposes. Pole climbing robot, nowadays, is a very common and interesting idea, which mainly works by connecting the distribution lines according to the directions given to it. In this modern era robots are being developed for various purposes to accomplish many tasks which seem to be too complex and life endangering for humans. Benefits of using robots have been immense in terms of risk-free, speed and efficiency of doing required tasks compared to that of humans. The main objective of this work is to to save human lives as numbers of people are died from electrical injuries almost every year in Bangladesh. Considering on that issue, a pole climbing robot has been designed, built and simulated. However, further modifications of this work might be able to perform the wiring and repairing tasks instead of an electrician. The developed robot works on the principle of linear motor, which is partially autonomous. With the installation of this project, risk of human injuries and death can be minimized while working in the distribution lines which is the main consideration of this paper.
[Actuators, risk reduction, Bluetooth, power distribution lines, FSR, injuries, electrical pole climbing robot, distribution line wiring, Manipulators, linear motors, Partially-autonomous, distribution line repairing, mobile robots, Grippers, linear motor, Robot sensing systems, Climbing robots, human injuries, Bluetooth shield, electrical injuries, Actuator]
Development of national health data warehouse Bangladesh: Privacy issues and a practical solution
2015 18th International Conference on Computer and Information Technology
None
2015
Healthcare organizations in Bangladesh own a large amount of data in diverse health information systems. Potential and useful hidden knowledge can be discovered if integration of this huge medical data is performed in national level. The integration process requires linkage of patients' records among different heterogeneous sources. To facilitate effective data mining, it is essential to preserve record linkage in health data warehouse by retaining identifiable attributes. On the other hand, identifiable health data have high risk to patient privacy and also increase the chance of attacks by cyber criminals. In this paper, we have provided a practical solution of privacy and security problems for developing national health data warehouse of Bangladesh. Our developed technique can anonymize identifiable private data of the patients while maintaining record linkage in national warehouse to facilitate knowledge discovery process. For this purpose, we have used encrypted mobile number, gender and name-value of patients to produce Patient Identification Key. Our system is being implemented to protect privacy of sensitive health data in health data warehouse.
[Data privacy, Record Linkage, record linkage, data mining, Medical services, Health Data, Data Mining, gender encryption, Security, Health Data Warehouse, patient identification key, name encryption, Data warehouses, cryptography, national health data warehouse development, electronic health records, security problems, Couplings, Bangladesh, privacy issues, patients data privacy, mobile number encryption, Organizations, data privacy, sensitive health data privacy protection, data warehouses, knowledge discovery process, Medical diagnostic imaging, Data Privacy]
A review on diabetes patient lifestyle management using mobile application
2015 18th International Conference on Computer and Information Technology
None
2015
Diabetes is a silent-killer disease that highly demands proper patient care and sound self-management which is a major challenge for a patient. The advancement in ICT and the rapid increase of mobile phone users are showing that lifestyle management of a diabetes patient can be guided using mobile application. The objective of this review is to analyze the articles and the existing mobile apps that are currently available to support diabetes patient and based on the analysis to describe the opportunities for developing a mobile app integrating most of the common features required for the self-management of a diabetic patient. The review covers journal databases for articles and online markets for mobile app. Initially we found 273 articles from the journal databases and 1004 apps from the online markets out of which 29 articles and 43 apps satisfied our selection criteria. We successfully brought out five primary and seven secondary features which can efficiently handle the self-management system. The major features include blood glucose monitor, medication, diet plan, physical activity, automated transfer of blood glucose data, SMS based notification, weight management, communication etc. No app in our review is found satisfying all the features. Since all the primary and secondary features as a whole are very essential for an effective diabetes patient lifestyle management, but not available in a single app, so there is a potential scope to develop such an app integrating all the features very efficiently.
[diet plan, smartphone, diabetes patient self-management system, Mobile communication, Mobile handsets, Blood, mobile computing, SMS based notification, information and communication technology, diabetes self-management, silent-killer disease, Sugar, Monitoring, blood glucose data automated transfer, mobile application, weight management, diseases, patient care, blood, diabetes medication, blood glucose monitor, Diabetes, medical computing, sugar, diabetes patient lifestyle management, diabetes]
Performance analysis of dispersion and nonlinear characteristics of a photonic crystal fiber (PCF) with GeO2 core and defected cladding
2015 18th International Conference on Computer and Information Technology
None
2015
An innovative Nearly Zero Dispersion Fiber (NZDF) at 1.6μm wavelength is designed having GeO<sub>2</sub> elliptical core, GeO<sub>2</sub> doped SiO<sub>2</sub> at gladding region as well as varying diameter of nearest air hole of core with the others air-hole and investigated Finite Element Method (FEM) using COMSOL. The suggested PCF shows encouraging dispersion characteristics (-1.954 &#x00D7; 10-14ps/km-nm) with an effective area (2.11 &#x00D7; 10-11 μm2) and a value of non-linear parameter (0.000641W-1km-1) at 1.6μm making it suitable candidate for nonlinear optical application and chromatic dispersion controller.
[Chromatic dispersion, holey fibres, Photonic crystal fibers, Doping, Optical fiber dispersion, GeO<sub>2</sub> doped SiO<sub>2</sub>, chromatic dispersion controller, finite element method, germanium compounds, Optical fiber communication, GeO<sub>2</sub> elliptical core, SiO<sub>2</sub>:GeO<sub>2</sub>, photonic crystal fiber, Optical fibers, nonlinear characteristics, optical fibre cladding, nearly zero dispersion fiber, NZDF, air hole, finite element analysis, Dispersion, FEM, cladding region, nonlinear optics, PCF, wavelength 1.6 mum, silicon compounds, nonlinear parameter, nonlinear optical application, defected cladding, dispersion characteristics, optical fibre dispersion, Nonlinearity, photonic crystals, COMSOL, performance analysis]
Noise adaptive binary pattern for face image analysis
2015 18th International Conference on Computer and Information Technology
None
2015
This paper proposes a Noise Adaptive Binary Pattern (NABP) for facial image analysis such as face recognition, expression recognition and gender classification. NABP encodes the face microstructures using an adaptive threshold and generates more discriminative patterns than other existing local feature descriptors. Rigorous experiments on two well-known datasets, LFW and CK+, for three different aforementioned applications demonstrate the excellence of NABP as compared to the current state of the art methods.
[discriminative patterns, CK+, image classification, face image analysis, Electronic mail, noise adaptive binary pattern, adaptive threshold, Histograms, feature extraction, image segmentation, Noise adaptive binary pattern, face recognition, Face, LFW, expression recognition, gender classification, Face recognition, NABP, image denoising, Gender classification, Support vector machines, Image analysis, local feature descriptors, face microstructures, Adaptive threshold, Expression recognition, Feature extraction]
An efficient security architecture for Wireless Sensor Networks using pseudo-inverse matrix
2015 18th International Conference on Computer and Information Technology
None
2015
Public Key Cryptography (PKC) based schemes in Wireless Sensor Networks (WSNs) are very difficult to employ due to the resource constraint characteristics of the tiny sensors. Addressing the features of sensor nodes, in this paper, we propose a PKC based architecture, where singular value decomposition (SVD) of pseudo-inverse matrix is used for the key generation. Main attention of our paper is devoted to the security level and power consumption as the Asymmetric Key-Based Architecture is usually considered incompetent in terms of cost efficacy. Our experimental results demonstrate that the proposed scheme ensures better performance in terms of security and power than other prevailing asymmetric schemes considering the MICA2DOT motes of wireless sensor nodes. In addition, it can help to realize the networks accurately, utilizing computational ability, storage and energy level of the current generation sensor nodes.
[telecommunication security, SVD, wireless sensor networks, public key, resource constraint characteristics, wireless sensor network, pseudoinverse matrix, power consumption, pseudo-inverse matrix, MICA2DOT, sensor nodes, key generation, public key cryptography, Computer architecture, Sensors, singular value decomposition, singular value decompositiont, PKC-based architecture, Base stations, wireless sensor network security architecture, asymmetric key-based architecture, matrix inversion, private key, Wireless sensor networks, WSN, Public key]
An intelligent framework for text-to-emotion analyzer
2015 18th International Conference on Computer and Information Technology
None
2015
The expansiveness of the internet encourages people to express their personal feelings via textual medium in terms of virtual communication. People are being influenced to move into this type of communication with the remarkable growth of social sites, messengers, blogs, micro blogs etc. Automatic derivation of the emotion from text is a challenge as it minimizes the misunderstanding by conveying the internal state of the users. Here we propose an intelligent framework to detect the emotion of a text. We divide the framework into two modules, namely Training Module and Emotion Extraction Module. We utilize the concept of Exploratory Data Warehouse (DW) technology to train our system. Therefore, DW relies not only on internal data but also on external (Web) data. The DW is used by the Emotion Extraction Module to detect the emotion of a given text. A comprehensive experimental evaluation, comparing our framework to a solution made with existing systems on a concrete dataset, shows that the proposed framework outperforms the existing approaches in terms of accuracy.
[data mining, Twitter, Data Mining, knowledge discovery, Sentiment Analysis, Data mining, Machine Learning, intelligent framework, data warehouse technology, Training, Databases, DW technology, feature extraction, Text-to-Emotion Analyzer, training module, emotion extraction module, learning (artificial intelligence), Web data, Extraction-Transformation-Load, text-to-emotion analyzer, Blogs, sentiment analysis, Data warehouses, machine learning, Emotion Extraction, Affective Computing, Feature extraction, Internet, Data Warehousing, data warehouses]
Cross-breed type Bayesian network based intrusion detection system (CBNIDS)
2015 18th International Conference on Computer and Information Technology
None
2015
Modern day internet is victimizer of the cynical network attacks due to excessive usage and massive connectivity demands. Machine learning is an efficient approach to prevent the intrusion and classify the network attacks. This study highlights the combined power of filter approaches in intrusion detection framework. Feature selection technique removes the redundant features and builds a time consuming better-performed intrusion detector framework. This study presents a cross-breed type feature selection approach using duo filter schemes for intrusion detection. In this framework feature selection technique eliminate the irrelevant features to reduce the time complexity and build a better model to predict the result with a greater accuracy and Bayesian network based classification model has been built up to predict the types of attacks. The experiment shows that the proposed framework exhibits a superior overall performance in terms of accuracy which is 97.2746% and keeps the false positive rate at a lower rate of 0.008. The model shows better performance in terms of accuracy than other leading state-of-the-arts frameworks like Boosted DT, Hidden NB, KNN and Markov chain. The NSL-KDD is used as benchmark data set with Weka library functions in the experimental setup.
[massive connectivity demands, KNN, Classification algorithms, Markov chain, weka, hidden NB, Intrusion detection, Filtering algorithms, cross-breed type feature selection approach, belief networks, learning (artificial intelligence), Bayesian network, feature selection, time complexity, boosted DT, NSL-KDD, machine learning, Nsl-kdd, duo filter schemes, CBNIDS, intrusion detection system, security of data, cynical network attacks, cross-breed type Bayesian network based intrusion detection system, Markov processes, Weka library functions, Feature extraction, Data models, Filtering theory, Bayes methods, Internet, computational complexity]
A belief rule based expert system to assess Lung Cancer under uncertainty
2015 18th International Conference on Computer and Information Technology
None
2015
Lung Cancer which is also known as carcinoma of the lung or pulmonary carcinoma is one kind of fatal lung tumor described by uncontrolled cell growth in the lung tissues. If this tumor left untreated this growth will be spread beyond the lung in the process of metastasis into the nearby tissues or any other parts or organs of the body. Worldwide Lung Cancer is considered as one of the most leading cause of cancer related death in the present time. So, the assessment of lung cancer is a crucial issue. Lung cancer is generally assessed from its signs, symptoms and risk factors by the physicians. However, assessing lung cancer is complex due to the presence of various types of uncertainties such as vagueness, ignorance, imprecision, incompleteness associated with these signs, symptoms and risk factors. The recently developed generic belief rule-based inference methodology by using the evidential reasoning approach (RIMER) has been considered to develop an expert system to assess this disease. The system can deal with various types of uncertainties found in the clinical signs, symptoms and risk factors. The knowledge base of this system has been constructed by taking account of the real patient data as well as with the consultation of the specialists. The practical case studies are provided to test this system. It has been observed that the proposed system is more reliable than from manual system as well as than from fuzzy rule based expert system.
[knowledge base, expert system, Uncertainty, lung carcinoma, RIMER, uncertainty handling, Cognition, lung tumor, uncertainty, lung cancer, medical expert systems, evidential reasoning approach, lung cancer assessment, tumours, clinical signs, belief rule based expert system, risk factors, generic belief rule-based inference methodology, Knowledge representation, metastasis process, inference mechanisms, lung tissues, Diseases, lung, Lungs, uncontrolled cell growth, disease symptoms, cancer, Belief rule base, pulmonary carcinoma, Expert systems, Cancer]
Espionage: A voice guided surveillance robot with DTMF control and web based control
2015 18th International Conference on Computer and Information Technology
None
2015
This paper proposes a research that was taken up in the wake of increasing awareness of security crisis all over the world. An effort has been made to briefly demonstrate the proof of concept of using a mobile surveillance system based on the Raspberry Pi, capable of streaming live video feed and being controlled through voice. In addition to that the robot can also be controlled through backup means should the main control system fails. Currently, the option of using a mobile phone to ring the bot and control it using the keypad via DTMF exists. Also the robot can be controlled from the internet using a web interface. The viability of the use of remote control technology in surveillance systems has been addressed in our endeavor.
[Web interface, Web based control, DTMF control, Raspberry Pi, web interface, voice guided surveillance robot, DTMF, mobile robots, Security, mobile computing, remote control technology, Espionage, surveillance system, control engineering computing, Robots, security crisis, Google, Navigation, mobile surveillance system, Surveillance, telecontrol, mobile phone, live video feed, Internet, Feeds, IEEE 802.11 Standard]
MAYA: A fully functional rover designed for the mars surface
2015 18th International Conference on Computer and Information Technology
None
2015
In this paper, we have discussed about the Maya-a standalone, fully functional mobile platform rover which can come to aid for human in many cases. This rover can be controlled from 1 kilometer radius. It had successfully participated in 9th Annual University Rover Challenge 2015 organized by the Mars Society and achieved 9th position among the 44 teams all over the world. In this paper we have tried to give a brief idea about Maya, its architecture, facts and features, used components and techniques used to build up the whole rover. Finally the improvements have also been reported in this paper with it's controlling system software.
[Wheels, Equipment Servicing, mobile robots, GPS, Voltage control, Mars, Rover, planetary rovers, Human assistant, Blind Station, motion control, Soil sample, Soil, aerospace computing, Cameras, rover MAYA, fully functional mobile platform rover, Software, Mars surface, Sensors, control engineering computing]
Spatial data visualization methodologies in ICT4D research
2015 18th International Conference on Computer and Information Technology
None
2015
This paper attempts to visualize spatial data to promote information and communication technologies in developing countries. Mass people of developing countries are not well educated. They do not have proper knowledge of data and their representation. Consequently, Government is not capable of successfully making the data available for common people. Various types of data for example, climate, agriculture, transportation are publicly available to obtain meaningful pattern from the data so that they can be utilized in efficient ways. In this research, we have reported some visualization techniques to get better understanding of spatial data. Rainfall data has been used as test data. Our research shows that, utilizing programming tools, interesting visuals can be generated to promoted information and communication technology research.
[spatial data, Correlation, visualization, rainfall data, Monsoons, spatial data visualization methodologies, Spatial databases, programming tools, ICT4D, transportation, Sociology, Data visualization, data visualisation, Agriculture, information technology research, ICT4D research, communication technology research, visualization techniques]
Minimizing the interference leakage using a novel IA algorithm to provide perfect alignment solutions
2015 18th International Conference on Computer and Information Technology
None
2015
The goal of this paper is to design a new interference alignment (IA) precoders and decoders algorithm capable of finding perfect alignment solutions which is similar to the AltMin-IA in terms of convergence speed and reliability. Finally, extended the AltMin-IA algorithm to incorporate the rank constraint in the form of a minimum eigenvalue condition, and a transmit power constraint, which results in a generalized eigenvalue problem at each step of the alternating optimization procedure. In the interference channel (IC), K transmitter-receiver pairs exploit the channel state information (CSI) to jointly design their transmit strategies. Specifically here, consider orthogonal frequency-division multiplexing (OFDM) transmissions, where the application of existing IA algorithms require an additional level of cooperation; time synchronization. To avoid such demand, apply the precoders and decoders at the sample level in the time domain, which allows the users to transmit asynchronously.
[OFDM transmission, time domain, OFDM, interference channel, IA decoder algorithm, channel state information, precoding, degrees-of-freedom, transmit power constraint, Optimization, eigenvalues and eigenfunctions, Integrated circuits, radiofrequency interference, eigenvalue condition, interference alignment, telecommunication network reliability, CSI, rank constraint, Eigenvalues and eigenfunctions, MIMO, OFDM modulation, IC, wireless channels, interference suppression, time synchronization, Interference, K transmitter-receiver pair, time-domain analysis, convergence speed, Decoding, perfect alignment solution, radio receivers, decoding, synchronisation, orthogonal frequency division multiplexing transmission, alternating optimization procedure, radio transmitters, AltMin-IA algorithm, interference leakage minimization, MIMO testbed, minimisation, IA precoder algorithm, Signal to noise ratio, eigenvalue problem]
Bangla Parts-of-Speech tagging using Bangla stemmer and rule based analyzer
2015 18th International Conference on Computer and Information Technology
None
2015
Parts-of-Speech (POS) tagging plays vital roles in the field of Natural Language Processing (NLP), such as - machine translation, spell checker, information retrieval, speech processing, emotion analysis and so on. Bangla is a very inflectional language that induces many variants from a single word. Although there is a few POS Tagger in Bangla language, very small of them address the essence of suffices to identify tag of the words. In this regard, we propose an automated POS Tagging system for Bangla language based on word-suffixes. In our system, we use our own stemming technique to retrieve a possible minimum root words and apply rules according to different forms of suffixes. Moreover, we incorporate a Bangla vocabulary that contains more than 45,000 words with their default tag and a patterned based verb-data-set. These facilitate to improve tagging efficiency of Bangla POS Tagger. We experiment our proposed system on a Bangla text corpus. The result shows that our proposed Bangla POS Tagger has outperformed the known related tagging systems.
[automated POS tagging system, Vocabulary, Bangla Parts-of-Speech (POS) Tagger, Dictionaries, Bangla stemmer, natural language processing, computational linguistics, rule based analyzer, Natural Language Processing (NLP), Morphology Bangla Stemmer, Bangla vocabulary, Bangla text corpus, patterned based verb-data-set, vocabulary, NLP, Hidden Markov models, Training data, knowledge based systems, Tagging, Speech, word-suffixes, Natural language processing, Bangla parts-of-speech tagging, inflectional language]
Many-core accelerated local outlier factor based classifier in bearing fault diagnosis
2015 18th International Conference on Computer and Information Technology
None
2015
This paper proposes a feature extraction, selection, and classification based bearing fault diagnosis methodologies using acoustic emission (AE) signal. First, a set of statistical, time-domain, and frequency domain features are extraction from AE signal. Of these features, some features having more informative data to distinguish faults are selected. Finally, a classifier based on local outlier factor (LOF) is used to detect faults of bearing. LOF consists of calculating distances of each input will all the training features, thus having a lot of computations. To reduce execution time, this paper implemented the LOF on a data parallel many-core architecture. Experimental results showed that, many-core implementation of LOF algorithm is more than 923&#x00D7; faster than sequential implementation.
[Time-frequency analysis, fault diagnosis, many-core accelerated local outlier factor, parallel architectures, reliability, data parallel many-core architecture, Fault diagnosis, time-domain feature, machine bearings, feature extraction, Computer architecture, classifier, classification based bearing fault diagnosis, feature selection, local outlier factor, Induction motors, multiprocessing systems, frequency domain feature, acoustic emission testing, parallel architecture, statistical feature, Fault detection, mechanical engineering computing, Feature extraction, condition monitoring, statistical analysis, acoustic emission]
Exploring the intuitiveness of iconic, textual and icon with texts signs for designing user-intuitive web interfaces
2015 18th International Conference on Computer and Information Technology
None
2015
Web interface signs such as navigation links, command buttons, thumbnails, small images, and icons are crucial elements of web user interfaces, which act as a communication artifact between the computer and the human (user). In other words, users interact with a web interface via interface signs. Designing intuitive interface sign become essential to make an effective human-computer interaction (HCI). But, a very limited HCI research has been focused on interface sign to design them intuitive for end users. In this paper, an extensive empirical user study was carried out through semi-structured interviews. The study found a set of underlying features for HCI practitioners to design intuitive interface signs in order to construct web interfaces intuitive for end users.
[Computers, intuitive interface sign design, web sign ontology, icon sign, graphical user interfaces, interface sign, Web interface signs, Semiotics, Electronic mail, semistructured interviews, Guidelines, HCI practitioners, Human computer interaction, user-intuitive Web interfaces, human-computer interaction, textual sign, web usability, human computer interaction, user interface design, Internet, Usability, text detection, text sign]
Fuzzy membership function generation using DMS-PSO for the diagnosis of heart disease
2015 18th International Conference on Computer and Information Technology
None
2015
Fuzzy decision support systems (FDDSs) have demonstrated their ability to solve different kinds of problems in various application domains. Currently, there is an increasing interest to generate FDDSs with learning and adaption capabilities. In this paper, DMS-PSO can be merged with FDDS for giving the learning and adaptive capability of FDDSs. Here, DMS-PSO used for optimizing membership functions to design efficient and effective fuzzy DSSs. The proposed model is works as follows: Firstly, the datasets are preprocessed so that important effective attributes are selected to handle noisy data. Secondly, fuzzy rules are learned from example(s) and the optimize membership functions of fuzzy DSSs and adapting the DMS-PSO is done. Finally, to establish the efficiency of the adaptive FDSSs the presentation of the FDDSs is evaluated with quantitative, qualitative and comparative analysis. From the experimental results outcome, adaptive FDSSs obtained better accuracy when compared to the existing systems.
[Heart, Decision support systems, dynamic multiswarm particle swarm optimization, Adaptation models, particle swarm optimisation, Data Preprocessing, fuzzy set theory, effective attribute selection, Decision Support System, learning capabilities, comparative analysis, noisy data handling, Spread spectrum communication, fuzzy decision support systems, Rule Learning, learning (artificial intelligence), quantitative analysis, Fuzzy Logic, Membership Function Tuning, medical diagnostic computing, fuzzy rules, heart disease diagnosis, diseases, Attribute Selection, FDDS, Tuning, cardiology, decision support systems, Heart Disease, Diseases, Pragmatics, adaption capabilities, qualitative analysis, data handling, DMS-PSO, fuzzy membership function generation]
Towards real-time 3D geometric nonlinear diffusion filter and its application to CT and MR imaging
2015 18th International Conference on Computer and Information Technology
None
2015
We propose two near real-time nonlinear anisotropic diffusion filtering (NADF) methods for the 2D and 3D X-ray computed tomography (CT) and magnetic resonance (MR) image denoising. Typically, NADFs are preferred for the medical image denoising due to its edge preserving feature though they are computationally expensive. Recently, a computation-time efficient 2D NADF has been proposed which uses local pixel intensity-based geometric parameters for diffusion. But it has limitations resulting from (i) its assumption that the neighboring pixels are non-noisy while deciding on an interrogated pixel being noisy or not, and (ii) its confinement of working only on a 2D image. Motivated from this, we propose an improved 2D NADF method that uses additional neighboring pixels in an effective way to lower the noise impact on the estimated geometric parameters. We also extend our 2D method into 3D that considers all the three directions for information diffusion. The performance of the proposed methods is evaluated using a 3D synthetic phantom, and in vivo CT and MR data which demonstrates an average signal-to-noise-ratio-gain improvement of approximately 58% in 2D and 96% in 3D phantom data, and approximately 79% in 2D and 127% in 3D in vivo data, compared to the state-of-the-art method.
[biomedical MRI, magnetic resonance imaging, MR imaging, signal-to-noise-ratio-gain improvement, Diffusion filter, X-ray imaging, medical image denoising, interrogated pixel, Three-dimensional displays, Computed tomography, X-ray computed tomography, Phantoms, information diffusion, medical image processing, real-time 3D geometric nonlinear diffusion filter, computed tomography, CT imaging, computation-time efficient 2D NADF, Image edge detection, pixel intensity-based geometric parameters, neighboring pixels, image denoising, 3D synthetic phantom, magnetic resonance image denoising, computerised tomography, nonlinear anisotropic diffusion filtering, Signal to noise ratio, Image denoising]
Near optimal resource allocation algorithm for users with proportional data rate fairness in MU-OFDM systems
2015 18th International Conference on Computer and Information Technology
None
2015
We optimize the throughput of a single cell multiuser orthogonal frequency division multiplexing system with proportional data rate fairness requirements. The concept is to support mobile users with different levels of service. The optimization problem is a mixed binary integer nonlinear programming (MINLP) problem, which is computationally very expensive. We propose a computationally efficient near-optimal solution. The original MINLP problem is broken down to two subproblems by introducing auxiliary optimization variables and employing integer relaxation. A stochastic multi-start approach is invoked to facilitate escape from possible local minima. The simulation results show that the proposed solution exhibits very strong adherence to the desired proportional data rate fairness while achieving higher system throughput compared to the other existing solutions.
[Base stations, integer programming, nonlinear programming, Resource allocation, Programming, Throughput, mixed binary integer nonlinear programming, Computational complexity, Optimization, MU-OFDM, resource allocation, Proportional fairness, Bandwidth, multi-access systems, OFDM modulation, Resource management, MU-OFDM systems, resource allocation algorithm, single cell multiuser orthogonal frequency division multiplexing system]
A construction method and data migration strategy for hybrid cloud storage
2015 18th International Conference on Computer and Information Technology
None
2015
With more and more enterprises constructed their own private cloud storage platforms, certain limitations and challenges come for small and medium enterprises. So hybrid cloud storage is imperative with public cloud as a complement. In this paper, we propose a fast and efficient method to build a hybrid cloud storage framework: base on open source software OpenStack Swift to build the private cloud storage, use Baidu Object Storage as the public cloud provider, and do a second development of Cyberduck as the cloud storage client. This framework can realize the seamless integration of private and public cloud. To tackle the data migration problem in hybrid cloud storage, we present a part-migration-part-retention self-adaptive periodic migration strategy, and the specific implementation of migration process is given. The strategy not only improve the utilization rate of local cloud storage, but also make reasonable use of public cloud storage.
[Computers, hybrid cloud storage framework, Cloud computing, OpenStack Swift, Companies, Self-Adaptive, private cloud storage platform, Data Migration, Computer architecture, part-migration-part-retention self-adaptive periodic migration strategy, cloud computing, public cloud, open source software, BOS, small-to-medium enterprises, local cloud storage utilization rate improvement, Standards, public cloud provider, small and medium enterprises, cloud storage client, hybrid cloud, Baidu Object Storage, construction method, cloud storage, data migration strategy, Cyberduck]
Phonon spectrum and group velocity calculation of cubic semiconductor quantum dot
2015 18th International Conference on Computer and Information Technology
None
2015
Phonons play an important role in determining the electrical and thermal properties of semiconductors and other material systems. A mathematical model based on elastic continuum approach has been used for calculating the phonon dispersion of semiconductor cubic quantum dot. With this model acoustic phonon dispersions of an individual GaAs quantum dot are calculated for three different sizes to investigate the size effect on phonon spectra. Phonon group velocity has also been calculated for each phonon branch by numerical differentiation.
[elasticity, phonon dispersion relations, Acoustics, phonon group velocity, Nanostructures, elastic continuum approach, thermal properties, acoustic phonon, semiconductor quantum dots, group velocity, acoustic dispersion, Mathematical model, phonon spectrum, cubic semiconductor quantum dots, gallium arsenide, phonon spectra, Quantum dots, electrical properties, Phonons, Boundary conditions, acoustic phonon dispersion, size effect, GaAs, Dispersion, numerical differentiation, mathematical model, quantum dot, material system, III-V semiconductors]
A fuzzy features based online handwritten Bangla word recognition framework
2015 18th International Conference on Computer and Information Technology
None
2015
Handwriting recognition is one of the most important ways to ease the handling of information between man and machine. Online handwriting recognition can be a very attractive method when people feel inconvenient using keyboards to handle information with computing devices. The most complicated task associated with online Bangla handwritten recognition is to separate the adjacent characters and vowel signs from one another within a Bangla word. This problem becomes more complicated due to the variations of writing style of individuals. In this paper, we propose a framework to recognize handwritten Bangla words in real time considering different writing styles. We used fuzzy linguistic rules in order to recognize Bangla handwritten words. Evaluation result for various writing styles reveals that the propose framework can recognize Bangla handwritten words with 77% accuracy.
[handwritten character recognition, fuzzy features, computational linguistics, fuzzy set theory, fuzzy feature-based online handwritten Bangla word recognition framework, fuzzy linguistic rules, aggregation, Character recognition, Pragmatics, Handwriting recognition, Image segmentation, vowel signs, online recognition, information handling, Handwritten recognition, feature extraction, information handle, adjacent characters, Writing, Feature extraction, computing devices, segmentation, Real-time systems, writing style variations]
Securely outsourcing large scale Eigen value problem to public cloud
2015 18th International Conference on Computer and Information Technology
None
2015
Cloud computing enables clients with limited computational power to economically outsource their large scale computations to a public cloud with huge computational power. Cloud has the massive storage, computational power and software which can be used by clients for reducing their computational overhead and storage limitation. But in case of outsourcing, privacy of client's confidential data must be maintained. We have designed a protocol for outsourcing large scale Eigen value problem to a malicious cloud which provides input/output data security, result verifiability and client's efficiency. As the direct computation method to find all eigenvectors is computationally expensive for large dimensionality, we have used power iterative method for finding the largest Eigen value and the corresponding Eigen vector of a matrix. For protecting the privacy, some transformations are applied to the input matrix to get encrypted matrix which is sent to the cloud and then decrypting the result that is returned from the cloud for getting the correct solution of Eigen value problem. We have also proposed result verification mechanism for detecting robust cheating and provided theoretical analysis and experimental result that describes high-efficiency, correctness, security and robust cheating resistance of the proposed protocol.
[Cloud computing, iterative methods, power method, computational overhead reduction, Eigen value problem, computation outsourcing, Encryption, Servers, eigenvalues and eigenfunctions, client confidential data privacy maintenance, encrypted matrix, data protection, Computational efficiency, Paillier cryptosystem, cloud computing, public cloud, result verification mechanism, massive storage, direct computation method, eigen vector, data security, malicious cloud, cryptography, matrix algebra, input matrix, vectors, storage limitation reduction, outsourcing, eigenvectors, input/output data security, client efficiency, computational power, Outsourcing, securely-outsourced large-scale eigen value problem, power iterative method, decryption]
An efficient design of adder/subtractor circuit using quantum dot cellular automata
2015 18th International Conference on Computer and Information Technology
None
2015
In this paper, we propose an efficient adder/subtractor circuit using QCA 3-dot cell. This model is highly proficient to compute both addition and subtraction operations. We also showed here a strong subtractor circuit. Moreover, our new and novel schemes has least number of QCA cells till now. The proposed designs carry out more beneficiary than the present ones, e.g., the proposed 32-bit subtractor circuit improves 73% on QCA cell, 99% on area and the proposed 32-bit adder/subtractor circuit improves 90% on QCA cell, 99% on area than the existing best known one.
[adder-subtractor circuit, subtraction operations, addition operations, n-Bit Ripple Borrow Subtractor, 3-Dot, n-Bit Adder/Subtractor, QCA, Adder/Subtractor, cellular automata, quantum dots, adders, quantum dot cellular automata, Microprocessors, word length 32 bit, Computer architecture, DH-HEMTs, Logic gates, CMOS technology, QCA 3-dot cell, CMOS integrated circuits, Full Subtractor, Adders, logic design]
A new stroke matching based approach to recognize Bangla handwritten text
2015 18th International Conference on Computer and Information Technology
None
2015
This paper proposes stroke matching based text recognizer (SMTR), a new approach to recognize Bangla handwritten text (BHT) that mainly focuses on off-line Bangla handwritten characters (BHCs) as well as words. For recognition, at first SMTR simplifies any font, size and shape of BHT into a set of strokes. Then it matches these strokes with its own set of strokes of Bangla characters those are already stored in the database. Here the structural properties of lots of these stored strokes are proposed by SMTR. Usually handwritten character (HC) recognition means optical character recognition and herein the accuracy of recognition is not so high because of divergences, variations and characteristics of HCs. Although up to now, lots of recognizers are available only to recognize BHCs, a recognizer that can recognize BHT is almost unavailable. Syntactic method (SM) is one of the most popular methods to recognize BHCs. SMTR proposed in this paper modifies existing strokes of SM and exploits it to recognize BHCs as well as BHT. The experimental results show that SMTR can recognize BHCs and also BHT (i.e. words) even in a noisy environment profoundly.
[stroke matching based text recognizer, Syntactic method, visual databases, Modified syntactic method, BHT size, optical character recognition, Text recognition, Databases, Bangla text recognition, SM, Bangla characters, handwritten character recognition, syntactic method, natural language processing, Bangla handwritten text recognition, BHC recognition, Artificial neural networks, BHT shape, Pattern recognition, Character recognition, image matching, Bangla character recognition, Handwriting recognition, Image segmentation, structural properties, BHT font, SMTR, text detection]
A novel design of a robotic vehicle for rescue operation
2015 18th International Conference on Computer and Information Technology
None
2015
This paper describes the design and implementation of a robotic vehicle that uses a new control system based on Bluetooth communication with an Android Phone as the controller. For this project, an Arduino Microcontroller, a Bluetooth module, DC motors, motor drivers, a distance sensor, cameras, temperature and gas sensors are used. It is named a rescue robot because it can be used as a helping hand to the human rescue team by providing general information about the situation of the affected area or in mines. This robotic vehicle can also be used in other fields. The designed system is flexible enough so it can easily be upgraded or modified to be used in a variety of situations according to the need.
[Bluetooth, rescue robot, human rescue team, Android phone, Temperature sensors, cameras, Bluetooth communication, distance sensor, temperature, Robot sensing systems, motor driver, rescue operation, control system, rescue, microcontrollers, robotic vehicle, DC motors, smart phones, control, Arduino microcontroller, mines, Bluetooth module, sonar sensor, gas sensors, Cameras, Arduino, Smart phones, motor drivers, rescue robots]
Light following & obstacle avoiding robot using autonomous & android based manual controller
2015 18th International Conference on Computer and Information Technology
None
2015
In this work, we have presented a robot, which is compact, autonomous and fully functional. It is a proposed model which can be used in such an environment, which may be vulnerable and risky to human being. It has three types of functions. Those are light following, obstacle detection and controlling from an android device through bluetooth module. A mobile robot having a control unit integrating the processing and the main sensor functionalities into an android device is described and demonstrated in this paper.
[collision avoidance, Light Following, Bluetooth, obstacle detection, light following robot, Humanoid robots, Android Controller, DC motors, Motor Driver, smart phones, mobile robots, control unit, Mobile robots, android based manual controller, obstacle avoiding robot, sensors, autonomous based manual controller, bluetooth module, Robot sensing systems, Obstacle Avoidance, Androids, android device control, sensor functionalities, Smart phones]
Free-space optical communication with BPSK subcarrier intensity modulation in presence of atmospheric turbulence and pointing error
2015 18th International Conference on Computer and Information Technology
None
2015
An analysis has been carried out to evaluate the Signal to Noise Ratio (SNR) and average bit error rate (BER) in presence of pointing error and atmospheric turbulence. The probability density function (pdf) of atmospheric turbulence and pointing error has been combined to find out the joint effect on the BER performance on Free-Space Optical (FSO) system. The average BER is evaluated over the combined pdf of atmospheric turbulence and pointing error. The BER performance results are numerically evaluated for several values of pointing error and different link length using Binary Phase Shift Keying (BPSK) subcarrier intensity modulation. For strong turbulence power penalty is around 20dB, while for strong jitter power penalty is around 30dB at a BER of 10-9 and bit rate of 1 Gbps.
[Bit error rate, Gamma-Gamma turbulence, Jitter, average bit error rate, FSO system, Misalignmnet Fading, phase shift keying, SNR, Subcarrier Intensity Modulation (SIM), intensity modulation, free-space optical communication, BPSK subcarrier intensity modulation, error statistics, probability density function, Atmospheric modeling, Pointing error etc, binary phase shift keying subcarrier intensity modulation, probability, Receivers, free-space optical system, Binary phase shift keying, Optical transmitters, atmospheric turbulence, Bit Error Rate (BER) Free-Space Optical (FSO) communications, BER, pointing error, pdf, bit rate 1 Gbit/s, link length, BPSK, Adaptive optics, signal to noise ratio, power penalty]
TYPEHEX keyboard: A virtual keyboard for faster typing in Smartphone
2015 18th International Conference on Computer and Information Technology
None
2015
We present a soft keyboard layout and a novel text entry system for smart phones. The major problem In designing a soft keyboard is to fit a large number of buttons within a very tiny touchscreen area. This often leads towards user discomfort and typing mistakes. Our proposed layout, TYPEHEX keyboard, presents full text entry facilities with minimum required finger movement. This name implies our vision towards achieving faster typing speed with less error occurrence using hexagonal buttons. While comparing with soft QWERTY layout, which contains 27 buttons, our proposed keyboard has only 10 functional buttons. The main 5 buttons are given hexagonal shape. It allows us to place 7 characters in a single button (1 in the center and other 6 are placed in six remaining corners of the hexagon). During placing each letter (a-z), we considered letter-frequency data, bigrams and trigrams in order to ensure that frequently used characters, character pairs and character trios are placed as close as possible. We evaluate our design by combining the movement time model which is an application of Fitts' law with the linguistic model. The calculated word per minute (wpm) values for both index finger and thumb shows that our design approach gives improved wpm values than soft QWERTY and T9 input method.
[soft keyboard, Time-frequency analysis, smartphone, Fitts law, user interfaces, soft keyboard layout, TYPEHEX keyboard, character frequency, virtual keyboard, text entry system, hexagonal buttons, keyboards, bigrams, Thumb, smart phones, Indexes, trigrams, linguistic model, Layout, Keyboards, movement time model, letter-frequency data, hexagonal shape, Interface design, fitts' law, Smart phones]
PMD monitoring of polarization division multiplex transmission system using RF tone harmonics
2015 18th International Conference on Computer and Information Technology
None
2015
We propose and demonstrate a novel technique for monitoring the first order polarization-mode-dispersion (PMD) effects on polarization division multiplex (PDM) quadrature phase shift keying (QPSK) signals. This technique uses the radio frequency (RF) tone at first harmonic and at half the data rate (half tone) of the detected electric spectrum. The electric spectrum is generated in a tap-line using square detection. The RF tones are usually absent after coherent detection of the PDM-QPSK signals. The technique uses fiber Brag grating (FBG) optical notch filter in the tap-line to remove one of the sidebands of the optical spectrum, and thus preventing the beating the sideband terms which normally cancels the RF clock tones. We focus on monitoring the effect of PMD on the regenerated RF tones by comparing their relative amplitude slopes. The simulated technique is simple and do not require modification at the transmitter end. It is shown that for PDM-QPSK data, the monitoring window is increased covering the whole bit period with improved accuracy.
[polarization division multiplex quadrature phase shift keying signals, Harmonic analysis, monitoring window, quadrature phase shift keying, notch filters, square detection, tap-line, Radio frequency, optical filters, FBG optical notch filter, PDM-QPSK signals, sideband terms, PMD effects, optical communication, Monitoring, regenerated RF tones, Optical fibers, first order polarization-mode-dispersion effects, DGD, PDM-QPSK, coherent detection, Polarization mode dispersion monitoring, Power harmonic filters, RF clock tones, Bragg gratings, fiber Brag grating optical notch filter, multiplexing, half tone, detected electric spectrum, radio frequency tone, relative amplitude slopes]
Phoneme based Bangla text to speech conversion
2015 18th International Conference on Computer and Information Technology
None
2015
This paper presents a phoneme based Bangla Text to Speech (TTS) Synthesis framework which uses a new approach for recording Bangla phoneme to improve the speech quality. Main objective of our method is to produce more natural speech sound during Bangla text to speech conversion. In this approach, the size of the dictionary remains small, but produces more smooth and natural sounds than any other phoneme, syllable or diphone based approaches. In the proposed framework, the voice sound of each Bangla alphabet is recorded. Afterwards, the recorded voice sound is separated into their constituent phonemes by a voice cutter. This dataset can be used for future conversion of input text into its corresponding natural sounding speech from a sequence of phonemes. Before generating the phonemes sequence for each word, we normalized the text by considering numbers, abbreviations, acronyms, currency, dates and URLs. Our implemented system supports UNICODE input for Bangla text.
[Decision support systems, voice sound, phoneme-based Bangla text-to-speech conversion, phoneme sequence, speech synthesis, UNICODE input, natural sounding speech, input text, Speech synthesis, Bangla alphabet, speech processing, Bangla phoneme recording, Silicon, voice cutter, Text normalization, phoneme-based Bangla TTS synthesis framework, Phoneme, speech quality improvement, natural speech sound, phonemes]
Detection and classification of diabetic retinopathy using K-means clustering and fuzzy logic
2015 18th International Conference on Computer and Information Technology
None
2015
Diabetic retinopathy is a common vision threatening disease which occurs due to the abnormalities in retina of diabetic patients. Detection of diabetic retinopathy is crucial for prevention of loss of vision and diagnosis of diabetic. As fundus are sensitive to vascular syndromes, efficient detection of diabetic retinopathy related exudates and hemorrhages in fundus images of the retina can be a helpful for effective screening. This study presents an effective approach to detect exudates and hemorrhages in retinal fundus image and classify the diabetic retinopathy with reasonable cost. A k-means color compression technique is used to cluster the fundus image in different region of interest reducing color dimension. The different parts of diabetic fundus then segmented out and analyzed through the region properties attributes. Finally, the recognition of diabetic retinopathy was done by the knowledge based fuzzy inference system (FIS) with these effective attributes through experiment and trail basis. The sensitivity and accuracy of the detector is found as 98.2% and 92.3% respectively. The HRF retinal fundus image database images are successfully classified by fuzzy logic classifier with accuracy up to 96.67%.
[retina abnormalities, Retinopathy, image classification, diabetic retinopathy detection, knowledge-based fuzzy inference system, detector sensitivity, visual databases, Retina, detector accuracy, diabetic patients, fuzzy reasoning, diabetic retinopathy classification, High-Resolution Fundus, vision loss prevention, HRF retinal fundus image database, Image coding, Image color analysis, K-Means Color Compression, knowledge based systems, vision threatening disease, image colour analysis, Fuzzy Logic, medical image processing, fuzzy logic classifier, fuzzy logic, vision defects, knowledge-based FIS, diseases, vascular syndromes, region properties attributes, K-means clustering, eye, Image segmentation, color dimension reduction, pattern clustering, hemorrhage detection, k-means color compression technique, exudate detection, Diabetes, Hemorrhaging, Diabetic Retinopathy]
Friend recommendation framework for social networking sites using user's online behavior
2015 18th International Conference on Computer and Information Technology
None
2015
Social network sites (SNS's) have connected millions of users creating the social revolution. Users' social behavior influences them to connect with others with same mentality. Social networks are constituted because of its user or organizations common interest in some social emerging issues. The popular social networking sites are Facebook, Twitter, MySpace, Orkut, LinkedIn, Google plus etc. which are actually online social networking (OSN) sites. However, the large amount of online users and their diverse and dynamic interests possess great challenges to support recommendation of friends on SNS's for each of the users. In this paper, we proposed a novel friend recommendation framework (FRF) based on the behavior of users on particular SNS's. The proposed method is consisted of the following stages: measuring the frequency of the activities done by the users and updating the dataset according to the activities, applying FP-Growth algorithm to classify the user behavior with some criteria, then apply multilayer thresholding for friend recommendation. The proposed framework shows good accuracy for social graphs used as model dataset.
[Social Entities, Multilayer Thresholding, FP-Growth Algorithm, users activity frequency measurement, Nonhomogeneous media, Twitter, online users, friend recommendation framework, dataset update, Postal services, online social networking sites, SNS, MySpace, Facebook, multilayer thresholding, user social behavior, information retrieval, Social Networking Sites (SNS's), Orkut, Friend Recommendation Framework (FRF), OSN sites, LinkedIn, recommender systems, Games, user online behavior, social networking (online), FRF, behavioural sciences computing, Google plus, user behavior, FP-growth algorithm]
A proactive approach for context-aware self-adaptive mobile applications to ensure Quality of Service
2015 18th International Conference on Computer and Information Technology
None
2015
Mobile Applications are rapidly emerging as a convenient medium for using a variety of services. With the high penetration of smart phones in society, self-adaptation has become an essential capability required by mobile application users. In an ideal scenario, an application is required to adjust its behavior according to the current context of its use. This raises the challenge in mobile computing towards the design and development of applications that sense and react to contextual changes to provide a value-added user experience. In its general sense, context information can relate to the environment, the user, or the device status. In this paper, a framework is proposed for building context aware and adaptive mobile applications. Based on the concepts of software requirement specification and probabilistic modeling, this framework guides the modeling of adaptability at design time and supports context awareness and adaptability at runtime to tackle potential issues that can hamper Quality of Service (QoS). In the core of the approach, a probabilistic model has been derived from the software requirement model using the quantitative terms of context dependability and then, it is continuously verified against the runtime operations of a mobile application. Finally, the proposed approach has been used in a mobile application development use case to observe how proactive adaptability can be built into mobile application development to ensure QoS.
[Context, Adaptation models, probability, Quality of service, smart phone, smart phones, Mobile applications, Batteries, quality of service, Markov model, proactive adaptability, mobile application development, formal specification, context-aware self-adaptive mobile application, value-added user experience, Runtime, mobile computing, formal verification, QoS, systems analysis, Markov processes, probabilistic modelling, Software, software requirement specification]
Tunable bandgap and wavelength range of zinc blende indium gallium nitride quantum dots
2015 18th International Conference on Computer and Information Technology
None
2015
Indium Gallium Nitride is a material much known for its wide range of tunable bandgap. This feature makes it a viable candidate for optoelectronics and photovoltaic devices. Quantum dots made out of Indium Gallium Nitride have found numerous applications. However, it is the wurtzite Indium Gallium Nitride quantum dots which have been much studied. But recent advancements show that zinc blende Indium Gallium Nitride structure can also be utilized for multipurpose devices. This work has focused on finding the tunable bandgap and wavelength of zinc blende Indium Gallium Nitride quantum dots to evaluate its possibility of being used in optoelectronics and photovoltaics.
[wavelength, Photonic band gap, tuning, bandgap, Brus Equation, Quantum dots, photovoltaic devices, optoelectronics devices, Zinc, tunable bandgap, quantum dots, Excitons, semiconductor quantum dots, InGaN, zinc compounds, wide band gap semiconductors, Effective mass, Mathematical model, III-V semiconductors]
Determination of appropriate substrate for microstrip low-pass filter employing square designed tschebycheff distribution
2015 18th International Conference on Computer and Information Technology
None
2015
In modern microwave design domain, Electromagnetic Band Gap (EBG) structures which is a unique innovation in consideration of high potential and low complication in designing with microwave integrated circuits. This article inspects the relationship between Tschebycheff polynomial distribution and operating behaviour of different substrates. A number of methods, Tschebycheff polynomial distribution for designing the conventional square pattern with constant volumetric ratio and method of moments (MOM) for numerical analysis are used. To begin with, the parametric comparison has been demonstrated reviewing the selectivity, 20-dB insertion loss bandwidth (20-dB IL BW) and 10-dB return loss bandwidth (10-dB RL BW). Consequently, two substrates RT/Duroid 5880 and FR4 prevail with negligible ripple less than 0.7 dB, well impedance matching in the passband and wider, deeper stopband upto 6-7.5 GHz. Moreover, the 10-dB RL BW fluctuates very slowly while frequency shifts 2-3% from resonant frequency. Miniaturization of the circuit has been improved along with high efficiency with the help of Tschebycheff polynomial distribution.
[10-dB RL BW, RT/Duroid 5880, Tschebycheff polynomial distribution, Selectivity, EBG Structures, 20-dB IL BW, Bandwidth, numerical analysis, low-pass filters, method of moments, Microwave filters, loss 10 dB, frequency 6 GHz to 7.5 GHz, Dielectric constant, polynomials, microstrip low-pass filter, microstrip filters, photonic band gap, electromagnetic band gap structures, FR4, Metamaterials, microwave design domain, MOM, insertion loss bandwidth, Substrates, loss 20 dB, Tschebycheff Polynomial Distribution, microwave integrated circuits, microwave filters, Passband, Periodic structures]
Isolated Bangla word recognition and speaker detection by semantic modular time delay neural network (MTDNN)
2015 18th International Conference on Computer and Information Technology
None
2015
Speaker recognition is the identification of a person from characteristics of his/her voices and speech recognition concerns the recognizing of what is being said by the speaker. This paper presents a framework to recognize the isolated Bangla words and the corresponding speaker by proposing a semantic modular time delay neural network (MTDNN). Underlying acoustic fuzziness of human utterance and fluctuations of data due to environmental disturbance are managed by well-known Fuzzy C Means clustering technique. We have used MFCC features to recognize Bangla words and speaker detection. Experimental result with different individuals show that the proposed framework is functioning quite satisfactory with average accuracy of 82.66%.
[semantic modular time delay, human utterance, recurrent neural network, Delay effects, natural language processing, speaker detection, fuzzy set theory, modular architecture, Mel frequency cepstral coefficient, neural network, Biological neural networks, MFCC feature, Training, fuzzy c means clustering technique, environmental disturbance, delays, Speech recognition, Speech, neural nets, isolated Bangla word recognition, speaker recognition, acoustic fuzziness]
Semantic annotation of Bangla news stream to record history
2015 18th International Conference on Computer and Information Technology
None
2015
Every day thousands of news articles are published in Bangla from several different sources on the web and this number is even increasing rapidly. On the contrary, the readers are often selective to read their desired news only. In this connection, classical Information Extraction (IE) techniques are used to query with keywords from unstructured or semi-structured news contents to fulfill partial requirements. However, they cannot interpret sequences of events, relation among entities, inference some unveiled facts to facilitate further human analysis. To achieve this goal, semantic technology adds formal structure and semantics to the news stream. In this paper, we propose a system to analyze Bangla news content to annotate especially things, people and places with semantic technology automatically by extracting what happened, when, where and who being involved in the news with the help of classical Natural Language Processing (NLP) techniques. Furthermore, we relate news of today with the previous news to accumulate information over time. We present our proposed system of annotating Bangla news semantically and experiment with SPARQL to inference integrated news from different sources over time and shows its effectiveness in querying specific information.
[semantic technology, SPARQL, electronic publishing, keywords, semantic annotation, Metadata, Ontologies, HTML, History, information querying, query processing, formal semantics, Semantics, natural language processing techniques, NLP techniques, natural language processing, formal structure, Media, unstructured news contents, Standards, SQL, Bangla news stream, information extraction techniques, history recording, semistructured news contents]
Design of high performance and ultra-thin CdTe solar cells with SnTe BSF from numerical analysis
2015 18th International Conference on Computer and Information Technology
None
2015
Polycrystalline Cadmium Telluride (CdTe) is one of the leading solar cell materials for its efficiency, cost-effective and thermal stability. In this research work, numerical analysis is done by AMPS (Analysis of Microelectronic and Photonic Structures) simulator to investigate the cell performances (Jsc, FF, Voc, efficiency and temperature stability) of ultra-thin CdTe solar cell. Reduction of absorber layer was done and observed that 1 μm absorber layer is enough for acceptable range of cell conversion efficiency in the proposed cell. The possibility of this ultra-thin CdTe absorber layer was investigated, together with 100 nm SnTe back surface field (BSF) layer to reduce the barrier height in the valence band and to minimize the recombination losses at the back contact of the CdTe PV cell. From the investigation, it was found that the proposed ultra-thin cell have conversion efficiency of 18.68% (Jsc = 21.47 mA/cm2, FF = 0.85, Voc = 1.02 V) without BSF and with 100 nm SnTe BSF conversion efficiency increased to 22.61% (Jsc = 24.27 mA/cm2, FF = 0.876, Voc = 1.06 V) with only 0.7 μm of CdTe absorber layer. Moreover, without BSF and with SnTe BSF, the normalized efficiency of the proposed cell was linearly decreased with the increasing operating temperature at the gradient of -0.18%/&#x00B0;C and -0.16%/&#x00B0;C found in this analysis respectively, which indicated better stability of the proposed CdTe solar cell.
[cell conversion efficiency, valence bands, voltage 1.02 V, CdTe cell, Analysis of Microelectronic and Photonic Structures, back surface field layer, CdTe, Thermal stability, Cadmium compounds, voltage 1.06 V, Photovoltaic cells, BSF, barrier height, numerical analysis, ultra-thin solar cells, Numerical stability, cell performances, back contact, size 100 nm, solar cell materials, selenium compounds, SnTe BSF, recombination losses, Stability analysis, thermal stability, cadmium compounds, AMPS simulator, SnTe, absorber layer, polycrystalline cadmium telluride, Solar cell, AMPS, II-VI semiconductor materials, valence band, II-VI semiconductors, solar cells, temperature stability, Ultra-thin]
Dielectric gratings for efficient light trapping in thin-film silicon solar cell
2015 18th International Conference on Computer and Information Technology
None
2015
Efficient light trapping structure is needed for thin film silicon solar cell. We have investigated that dielectric diffraction grating structures have the potential to increase the efficiency by excellent light absorption. In order to boost up the efficiency of the solar cell optimized diffraction grating is designed which cause the optical path length enhancement within the silicon (Si) absorber layer. In our simulation Si square grating and titanium oxide (TiO<sub>2</sub>) pyramid grating is integrated separately on the illuminated surface of a 3.0 μm - thick-solar cell. Numerical simulation is performed with Finite Difference Time Domain (FDTD) method over a wavelength range of 400-1100 nm. The resultant potential short circuit current density is compared with the planar and ideal Lambertian case. Solar cell with TiO<sub>2</sub> pyramid diffraction grating on the front surface results better light trapping than the cell with Si square grating. Simulation result shows that TiO<sub>2</sub> pyramid grating provides a short circuit current density, Jsc, of 24.7mAcm-2 which is 67.2% of ideal Lambertian case.
[radiation pressure, dielectric diffraction grating structures, Diffraction gratings, Light trapping, optical path length, wavelength 400 nm to 1100 nm, FDTD method, light trapping, Lambertian case, Absorption, size 3 mum, Silicon, Gratings, finite difference time-domain analysis, finite difference time domain method, titanium compounds, light absorption, optical path length enhancement, current density, thin film solar cell, diffraction gratings, elemental semiconductors, Diffraction, silicon, TiO<sub>2</sub>, Si, silicon absorber layer, solar cells, short circuit current density, Dielectric gratings, thin-film silicon solar cell]
PCA and back-propagation neural network based face recognition system
2015 18th International Conference on Computer and Information Technology
None
2015
It's very difficult for an intelligent control system to recognize a person accurately as like humans. Though face recognition has some grand difficulties because of facial expression, illumination, lighting condition, pose etc., but the recognition of faces is very important for many applications such as: video surveillance, retrieval of an identity from a database for criminal investigations, home and office security, facebook and forensic applications. Within the last decade, the technological revolution has taken place all over the world based on computer vision. Face recognition has proved to be a challenging problem in computer vision. So, researches of computer science, biometrics, pattern recognition and neuroscientists have shown their deep interest in developing a consistent and proficient face recognition system. In this paper, we have proposed a method for face recognition by combining PCA and Back-propagation neural network which efficiently works in a constrained environment. Experiments demonstrate good results for face recognition on AT&amp;T(ORL) database.
[facial expression, Eigen face, visual databases, illumination condition, AT&amp;T-ORL database, Covariance matrices, Principal component analysis (PCA), Training, feature extraction, Lighting, pose estimation, Classification, face recognition, forensic applications, home security, Back-propagation Neural Network(BPNN), Face, video surveillance, Face recognition, person recognition, Eigenvector, facebook, Biometric identification, PCA, back-propagation neural network-based face recognition system, criminal investigations, intelligent control system, backpropagation, computer vision, identity retrieval, Feature extraction, office security, neural nets, principal component analysis, Principal component analysis, lighting condition]
A wideband stacked patch-ring antenna with corner perturbations for circular polarization
2015 18th International Conference on Computer and Information Technology
None
2015
This paper presents a method of wideband circular polarization (CP) generation using a stacked patch-ring microstrip antenna with a single probe feed. An enhanced axial ratio (AR) bandwidth is obtained when a combination of a square ring and a square patch with negative perturbations is used as parasitic and driven elements, respectively. The axial ratio bandwidth obtained is significantly larger compared to the case when square rings are used as parasitic and driven rings with a single feed. A simulated impedance bandwidth (S<sub>11</sub>&lt;; -10dB) of 16%, 3dB AR bandwidth of 8%and a peak gain of 8.65 dBic are obtained from this antenna.
[probe feed, electromagnetic wave polarisation, wideband circular polarization generation, stacked patch-ring microstrip antenna, Circularly polarized (CP) antennas, square patch, broadband antennas, microstrip antennas, square ring, Broadband antennas, Microstrip, impedance bandwidth, stacked patch antenna, Patch antennas, CP generation, Bandwidth, Microstrip antennas, corner perturbations, antenna feeds, Impedance, AR bandwidth, axial ratio bandwidth, wideband stacked patch-ring antenna]
Measuring the variability of CAIDA internet traffic traces
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper, we studied the variability of 17 CAIDA Internet traffic traces which were collected in 2013, 2014, 2015 and 2016. The variability of these traces was measured by using the Index of Variability. Based on the results, we outlined several important observations. In particular, the Index of Variability has the ability to reveal significant differences between traffic traces. It is dynamic and its behavior depends on several factors, such as network protocol dynamics and link speeds. In addition, traffic source link speeds have a major impact on network traffic variability (burstiness). Also, results show that there is a significant reduction in the variability for the 2015 and 2016 traces.
[Computers, Smoothing methods, Protocols, traffic source link speeds, index-of-variability, CAIDA Internet traffic trace variability measurement, link speeds, Information technology, Interpolation, network protocol dynamics, Internet, Splines (mathematics), telecommunication traffic]
TTL based MaxProp routing protocol
2016 19th International Conference on Computer and Information Technology
None
2016
Routing messages in sparse and intermittently connected networks is one of the major challenging issues in Delay Tolerant Networking as an end-to-end path between the source and the destination is absent most of the time. To deal with this problem a wide range of routing protocols have emerged over time. One such protocol is MaxProp-which prioritizes both the schedule of packets to be transmitted (based on their hop count) and the schedule of packets to be dropped (based on their delivery likelihood) by splitting the buffer into two portions. In this paper, we propose a Time-To-Live (TTL)-based routing scheme that prioritizes the schedule of packets to be transmitted not only based on their hop count but also based on their TTL values. In our scheme, messages with high TTL values are taken into special consideration which saves them from being dropped earlier. A series of simulation has been carried out and the results show that our TTL based routing scheme significantly improves the number of delivered messages reducing the overhead ratio and delay, consuming the same amount of network resources as MaxProp.
[Computers, MaxProp, Schedules, ONE, telecommunication scheduling, Packet Prioritization, delay tolerant networking, end-to-end path, mobile wireless network, overhead ratio reduction, routing message, Routing protocols, packet scheduling, Time-To-Live, mobile radio, time-to-live based routing scheme, TTL based MaxProp routing protocol, Ranked List, Routing, delay tolerant networks, Information technology, Resource fairness, Delivery Likelihood, delay reduction, routing protocols, Peer-to-peer computing]
Efficient techniques for improving coexistence problem in wireless body area network
2016 19th International Conference on Computer and Information Technology
None
2016
Wireless Body Area Network (WBAN) is a time critical remote network where various physiological information of patients are sent through satisfying stringent QoS requirements. However, performance of WBAN may degrade severely due to the coexistence problem, a well known phenomenon where different wireless channel access technologies share the overlapped frequency bands. In this paper, the coexistence problem of WBAN is addressed in terms of mutual interference mitigation and cross interference mitigation. A fuzzy logic based algorithm is proposed to detect the mutual interference and to mitigate it. Furthermore, a channel hopping technique is investigated to mitigate the cross interference problem. Simulation results show that our proposed techniques effectively mitigate the coexistence problem in WBAN scenarios.
[GTS, Body area networks, wireless body area network, Wireless communication, cross interference mitigation, stringent QoS requirements, fuzzy logic-based algorithm, wireless channels, wireless channel access technology, patient physiological information sending, time critical remote network, WBAN performance, coexistence problem improvement, interference suppression, Interference, mutual interference, channel hopping technique, body area networks, quality of service, mutual interference mitigation, Fuzzy logic, Wireless sensor networks, cross interference, Received signal strength indicator, Signal to noise ratio]
Optimization of ethernet ring protection switching for adaptive modulation in carrier access network
2016 19th International Conference on Computer and Information Technology
None
2016
The ability of Ethernet Ring Protection Switching (ERPS) to provide carrier grade switchover has made Ethernet technology viable in telecommunication carrier networks enabling network operators to replace SONET/SDH/PDH. Adaptive modulation (AM) is another evolution in technology which may effectively double the capacity of microwave links in favorable weather condition. In this paper, we propose an enhancement of ERPS protocol to support adaptive modulation in ERPS protected microwave access network for improved throughput, reliability and energy efficiency. We present a mathematical model to verify the effectiveness of our proposed modification. Simulation results show that the proposed enhancement achieves significantly higher throughput compared to existing ITU-T G.8032 protocol in varying channel conditions.
[adaptive modulation, Ethernet ring protection switching optimization, Ports (Computers), reliability, Microwave, Adaptive Modulation, Degradation, ERPS, microwave links, microwave access network, Network topology, Modulation, telecommunication access network, Access Network, Bandwidth, telecommunication network reliability, energy efficiency, ERPS protocol enhancement, wireless channels, throughput, microwave link capacity, ITU-T G.8032, Transport Network, Topology, radio access networks, telecommunication switching, Microwave technology, mathematical model, wireless LAN]
PNGP : A social relationship based routing algorithm for pocket switched network
2016 19th International Conference on Computer and Information Technology
None
2016
Transferring messages or packets through a network using human mobility as a means, requires intelligent routing decisions. Throughout an opportunistic network, nodes travel with a few messages and searches for an opportunity to deliver the message. A good delivery rate of such messages in these sort of networks is a challenge which might require complex calculations and several intricate decision making processes. With a view to improve efficiency and to deliver a simpler way of routing, this paper will present a new routing algorithm called Popular Node Gateway Protocol (PNGP). This new algorithm is tested against 3 other algorithms using our own built simulator and proves to be better in terms of delivery ratio delay and transmission cost. Unlike other algorithms this algorithm provides a simpler way of routing thus making this faster and much more efficient.
[Algorithm design and analysis, delay tolerant network, PNGP, packet switching, game theory, Routing, delay tolerant networks, social relationship based routing algorithm, PSN, Floods, Human mobility, human mobility, nodes, packet forwarding, telecommunication network routing, decision making, pocket switched network, Logic gates, popular node gateway protocol, Routing protocols, Delays, protocols]
A hybrid framework using Markov decision process for mobile code offloading
2016 19th International Conference on Computer and Information Technology
None
2016
With the emergence of a promising technology, Cloud Computing is spreading its usage everywhere. Mobile Cloud Computing (MCC) integrates cloud computing with mobile devices which holds a promise to run powerful and resource-intensive applications on mobile devices irrespective of its performance limitations (e.g., battery life, memory utilization and computation). Overcoming of these limitations is realized through a prominent solution, computation offloading: bypassing heavy computation to resourceful servers and receiving the outcome from these servers. A lot of research works have been done for mobile code offloading. Most of the frameworks either requires full virtual machine migration which results in coarsegrained and costly solutions, or implement fine-grained offloading requiring costly synchronization between local and offloaded codes. In both approaches, offloading of native methods are restricted only to local execution. In this paper, we have proposed a conceptual models blending the positive outcomes of both approaches using Markov Decision Process (MDP) for decision making and provides conceptual analysis with related research works.
[Cloud computing, full virtual machine migration, Mobile communication, Mobile handsets, conceptual models, Batteries, Servers, computation offloading, mobile computing, Cloud Computing, Markov Decision Process (MDP), cloud computing, resource-intensive applications, distributed programming, resourceful servers, Decision making, Mobile cloud computing, mobile code offloading, mobile cloud computing, decision making, virtual machines, Markov processes, mobile devices, Markov decision process, synchronization, Mobile Device, fine-grained offloading]
A power-utilization based virtual machine allocation algorithm
2016 19th International Conference on Computer and Information Technology
None
2016
Cloud computing becomes a buzzword now a day because of its simplicity, reliability, portability and security in maintaining and manipulating services from remote locations in the world. As the uses of cloud computing increase, more and more data centers are deploying throughout the world for providing high quality services to the consumers. The incremental performance of these data centers deserves a vast amount of energy to keep breathing and to response on demand immediately. Hence, development of efficient solutions is necessary that can reduce energy consumption without violating the performance much. Utilizing the physical resources at their best can be an answer to this problem. In this work, we have used the Simulated Annealing algorithm where it uses both of the power and the CPU utilization to select the best power saving host for allocating a VM. We have developed the algorithm in such way that as much workloads as possible can be served using least number of servers in a data center. We also cared about the SLA violation, performance degradation, VM migration etc. which is clearly clarified in the outcomes. We have performed exhaustive simulation study using our algorithm and the outcome of simulation results confirm the efficient resource utilization of our algorithm.
[Algorithm design and analysis, Cloud computing, Energy consumption, power-utilization based virtual machine allocation algorithm, Simulated Annealing, data centers, SLA violation, Energy Consumption, power saving host, Servers, Data Center, portability, SLA Violation, power aware computing, security, resource allocation, Cloud Computing, VM Selection, VM Allocation, CloudSim, cloud computing, resource utilization, energy consumption, Power demand, Host Selection, Computational modeling, performance evaluation, exhaustive simulation, performance degradation, Power-Utilization Awareness, computer centres, Global Warming, security of data, CPU utilization, simulated annealing algorithm, virtual machines, Resource management, VM migration]
Understanding foursquare venue popularity in Taiwan and a comparative study with the USA
2016 19th International Conference on Computer and Information Technology
None
2016
Location-based social network (LBSN) services are being popular at recent times. These are being manifested as service tools for different purpose of our daily lives. One very important aspect of these LBSNs services is to promote the business and help users to find their desirable location to visit. In this project, our aim is to find out the Foursquare venue popularity in Taiwan and compare its result with the USA. This includes: what venues are popular in Taiwan and what kinds of venues are popular here. we will also show the temporal effect on venue popularity. Then we will present a comparative study of venue popularity between Taiwan and the USA. In the end, we will have some interesting observations based on this venue popularity. This investigation will do a great help to the business owners as well as to the general users to select the appropriate venues.
[Computers, USA, Social network services, Venue Popularity, Airports, Electronic mail, venue popularity, Information technology, location-based social network, Public transportation, Location-based Social Network, mobile computing, LBSN services, Taiwan, social networking (online), Venue Category, Foursquare, Business]
GIS based heuristic solution of the vehicle routing problem to optimize the school bus routing and scheduling
2016 19th International Conference on Computer and Information Technology
None
2016
The School bus routing and scheduling can be treated as a multi-objective Vehicle Routing Problem (VRP) that helps to transport the students in the safest, most economical and convenient way. The objective of this paper is to optimize the overall transportation cost by minimizing the fleet of vehicles and time spent in the journey. In this research, a Geographical Information System (GIS) based solution for School Bus Routing and Scheduling (SBRS) with time window is proposed using Clarke &amp; Wright's algorithm. The result of this research helps the Transport Management to design a dynamic shortest and fastest route for the vehicles. The GIS helps to identify the real-world pickup stops according to the concentration of the students in a particular area as well as to visualize the current optimum route and pickup points. The proposed methodology is applied and tested to the Scholastica School in Dhaka city of Bangladesh with geo referenced data of school and pickup stops. A better result is obtained by the proposed method comparing to the existing semi-manual system.
[Computers, Heuristic algorithms, Clarke &amp; Wright's savings algorithm, Vehicle routing, Transportation, geographic information systems, GIS based solution, multiobjective vehicle routing problem, VRP, transport management, scheduling, geo referenced data, pickup stops, Geographic information systems, semimanual system, school bus routing, Scholastica School, Routing, Information technology, time window, GIS, Vehicle Routing Problem, School Bus Routing Problem, vehicle routing, SBRS, geographical information system, GIS based heuristic solution, overall transportation cost, Time Window]
Design of a split P-shaped multiband microstrip patch antenna for modern communication system
2016 19th International Conference on Computer and Information Technology
None
2016
A fictional design of a compact microstrip patch antenna for multiband frequency has been depicted in this paper. The proposed design presents a slit P-shaped antenna that operates at 3.5 GHz, 5.9 GHz, 9.2 GHz and 13.1 GHz. FR-4 (lossy) is used as substrate to design the recommended antenna which has a firm dimension of 18 &#x00D7; 20 mm2. This antenna operates at S, C, X and Ku band with moderate bandwidth because of its design and feedline. This P-shaped multiband antenna has directive gain of 3.3dBi, 4.9 dBi, 6.5 dBi and 7.1 dBi at resonating frequencies and is suitable for a modern communication system. Regarding this antenna, the results are obtained in terms of Return Loss, Voltage Standing Wave ratio, Gain and Radiation Pattern which have admissible values of return loss less than -10 dB, VSWR less than 2 at each resonant frequencies and Gain more than 3 dB.
[Ku band, Multiband, Microstrip, frequency 5.9 GHz, gain, Directivity, FR-4, antenna radiation patterns, Microstrip antennas, resonating frequencies, directive gain, modern communication system, Return Loss, multiband frequency, frequency 3.5 GHz, frequency 9.2 GHz, substrate, voltage standing wave ratio, compact microstrip patch antenna, X band, Split Microstrip Antenna, microstrip antennas, Substrates, VSWR, S band, Patch antennas, Resonant frequency, radiation pattern, C band, multifrequency antennas, split P-shaped multiband microstrip patch antenna design, return loss, Gain, frequency 13.1 GHz]
A compact slotted patch antenna for ultrawideband application
2016 19th International Conference on Computer and Information Technology
None
2016
A compact antenna with ultra wideband characteristics is proposed which covers the dimension of 23 &#x00D7; 21 mm2. It has a rectangular slotted patch with a circular slot, microstrip feed line and a tapered slot ground with a thickness of 1.6 mm which covers the inexpensive FR4 dielectric substrate. The proposed antenna covers a wider bandwidth of 10.1 GHz (2.90-12 GHz) with stable radiation pattern and constant gain which is ideal for UWB applications. The simulation is done with CST and HFSS simulator. The proposed antenna is low cost, compact in size and have good specifications which make it suitable for using UWB applications.
[Microwave antennas, compact, ultra wideband antennas, UWB applications, HFSS simulator, CST simulator, patch antenna, low cost, microstrip feed line, antenna radiation patterns, circular slot, Bandwidth, UHF antennas, ultra wideband application, antenna feeds, tapered slot ground, Antenna radiation patterns, stable radiation pattern, compact slotted patch antenna, tapered ground UWB, Ultra wideband antennas, microstrip antennas, frequency 2.9 GHz to 12 GHz, rectangular slotted patch, Slot antennas, slot antennas, inexpensive FR4 dielectric substrate, size 1.6 mm, microstrip lines]
Effect of exponential correlation model on channel estimation for massive MIMO
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper the impact of exponential correlation model on massive multiple input multiple output (MIMO) channel estimation is analyzed using a large number of antennas in the base station (BS) and a user terminal. We consider pilot-based channel estimation in the UL channel. The Linear minimum mean square error (LMMSE) estimator is used to investigate the effect of correlation factor on the average mean square error (MSE). Different number of antennas and several signal to noise ratio (SNR) values are implemented to see the estimation accuracy in each case. The effect of pilot length on the LMMSE channel estimator and its relationship with the exponential correlation model is investigated. It is shown that for low SNR values, the exponential correlation model has high impact on channel estimation.
[least mean squares methods, Correlation, Time Division Duplix, linear minimum mean square error channel estimator, massive multiple input multiple output channel estimation, Channel State Information, Covariance matrices, Signal to Noise Ratio, Channel estimation, antenna arrays, SNR, Massive MIMO, Mean Square Error, LMMSE channel estimator, Channel Estimation, signal-to-noise ratio values, Base Station, MIMO, base station, MIMO communication, UL channel, pilot-based channel estimation, exponential correlation model, Estimation, massive MIMO channel estimation, average mean square error, Correlation Factor, correlation factor effect, user terminal, channel estimation, Pilot Length, Antennas, Signal to noise ratio, correlation methods]
Impact of angular spread on massive MIMO channel estimation
2016 19th International Conference on Computer and Information Technology
None
2016
Large scale antenna arrays technology has the potential of bringing many advantages to future wireless systems. Energy and spectral efficiency are going to be the most important features. Hence, accurate estimate of channel state information (CSI) makes these advantages achievable. This paper investigates the effects of angular spread on the accuracy of channel estimation for massive multiple input multiple output (MIMO) wireless communication systems. The model we consider consists of user equipment (UE) and a base station with large antenna array. Linear minimum mean square error (LMMSE) is used to estimate the uplink channel of a massive MIMO system using a pilot signal. It is shown that higher spatial correlation (SC) positively affects the accuracy of channel estimation when the signal to noise ratio is kept constant.
[least mean squares methods, channel state information estimation, user equipment, channel state information, Covariance matrices, angular spread impact, large-scale antenna array technology, Channel estimation, antenna arrays, spectral efficiency, energy efficiency, Massive MIMO, angular spread, MIMO, wireless channels, linear minimum mean square error estimation, LMMSE estimation, Uplink, base station, MIMO communication, Estimation error, pilot signal, telecommunication power management, massive MIMO channel estimation, massive MIMO wireless communication system, spatial correlation, LMMSE, CSI estimation, channel estimation, energy conservation, signal to noise ratio, Antenna arrays]
Multiple hexagonal split-ring resonators based negative index material for multi band applications
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper, a new multiband metamaterial structure designed and simulated of unit-cell that has multiple hexagonal concentric metal rings one inside the other. The each ring of metamaterial structure delivers a magnetic resonance at different frequencies. The number of concentric ring resonator has created number of resonances. In this study, different analysis has been performed to verify distinct resonance frequencies, effective medium ratio and electromagnetic transmission properties simply by altering the design parameters. Computer Simulation Technology (CST) electromagnetic simulator which is commercially available is utilized to investigate the design of the metamaterial. The multiband response has been displayed of the metamaterial in conjunction with negative index material property over the certain span in the microwave regime. The proposed structure has become suitable for S-band, C-band, X-band and Ku-band applications and has achieved efficient electrically small microwave structure.
[frequency 2 GHz to 18 GHz, magnetic resonance, Computer Simulation Technology, Ku-band applications, X-band applications, C-band applications, microwave metamaterials, Magnetic materials, Permittivity, hexagonal concentric metal rings, CST electromagnetic simulator, multiband metamaterial structure, electromagnetic transmission properties, microwave resonators, S-band applications, Magnetic resonance, split-ring resonators, Metamaterials, Permeability, multiband response, Substrates, negative index material, multiple hexagonal split-ring resonators, UHF resonators, effective medium ratio, multiband, concentric ring resonator]
Energy efficient BS Cooperation in DPS CoMP based cellular networks with hybrid power supply
2016 19th International Conference on Computer and Information Technology
None
2016
Energy efficient cellular networking has recently drawn increasing attention for reducing network operation cost without sacrificing the quality of service (QoS). This paper proposes a framework for energy cooperation among base stations (BSs) in coordinated multi-point (CoMP) transmission based cellular networks, where the BSs are powered by hybrid power supplies including both the conventional grid and renewable energy sources. The considered network deploys BSs having independent energy storages, which are assumed interconnected by resistive power lines for energy sharing. The network also integrates dynamic point selection (DPS) CoMP technique for selecting the best serving BSs for an user equipment. The objective of the proposed cooperation is to maximize the usage of renewable solar energy leading to reduced on-grid power consumption. The proposed energy cooperation among BSs exploits the tempo-spatial diversities of both the renewable energy generation and the traffic demand. Monte Carlo based simulations are carried out for analyzing the energy efficiency (EE) performance of the proposed network. Simulation results validate the proposed inter-BS cooperation demonstrating substantial energy savings.
[Energy consumption, renewable solar energy, on-grid power consumption reduction, renewable energy generation, energy efficiency performance, hybrid power supply, energy efficient BS cooperation, traffic demand, Monte Carlo methods, QoS, energy storage, DPS CoMP-based cellular network, power grids, base station, dynamic point selection technique, solar power, coordinated multipoint transmission, telecommunication power management, operation cost reduction, conventional grid, quality of service, 3GPP, energy saving, Cellular networks, Green products, Solar energy, energy conservation, telecommunication traffic, renewable energy source, Monte Carlo simulation, Energy storage, cellular radio]
Multi band negative index metamaterial based on split square shape resonators
2016 19th International Conference on Computer and Information Technology
None
2016
A multi band negative index metamaterial based on split square shape resonators has been discussed in this paper. The proposed metamaterial unit cell structure is a combination of split square shape ring resonators. Commercially available electromagnetic simulator CST Microwave Studio has been used to investigate the calculation of the proposed design. The proposed structure shows resonance at 3.36 GHz, 5.98 GHz, 9.83 GHz, 13.10 GHz and exhibits negative index characteristics at 8.25 GHz. The effective medium ratio of the designed unit cell is 8.90. However, a performances comparison analysis has been done between Rogers RT 5880 and Teflon substrate materials.
[Multi Band, Shape, Refractive index, electromagnetic simulator CST Microwave Studio, MNG, Rogers RT 5880, SNG, resonators, Teflon substrate materials, frequency 3.36 GHz, microwave metamaterials, metamaterial unit cell structure, multiband negative index metamaterial, Permittivity, frequency 9.83 GHz, Resonator, split square shape resonators, frequency 8.25 GHz, Metamaterials, Permeability, Substrates, frequency 13.10 GHz, Resonant frequency, DNG, frequency 5.98 GHz]
Effects of human body on antenna performance: A quantitative study
2016 19th International Conference on Computer and Information Technology
None
2016
A quantitative study has been performed in order to investigate the effects of human body on antenna performance. The analysis has been carried out at 900, 1800 and 2400 MHz using a half-wave dipole antenna. The antenna is positioned perpendicularly to the body. At each frequency, the antenna has been moved towards the human body from a distance of one-wavelength to zero. A mathematical equation has been proposed relating the antenna gain with the distance from the body. A reasonable match has been obtained between the simulated and calculated gain. For simplification, a flat slab of muscle has been used to represent the human torso. Results show that, the gain of antennas varies exponentially from one-fourth of a wavelength to one-wavelength. Coefficients of the equations at the concerned frequencies are also presented. CST microwave studio and MATLAB are used for the numerical analysis.
[Computers, CST microwave studio, Dipole antennas, Muscles, Half-Wave Dipole Antenna, body area networks, human body, dipole antennas, MATLAB, half-wave dipole antenna, Body Area Network (BAN), Mathematical model, Gain, Antenna radiation patterns, antenna gain]
Characterization of low index Si waveguides
2016 19th International Conference on Computer and Information Technology
None
2016
In recent years, silicon photonics has attracted attention as an emerging technology for optical telecommunications and for optical interconnects in microelectronics. The refractive index contrast plays a fundamental role in determining the characteristics of an optical dielectric waveguide and a higher index contrast permits to move towards larger scales of integration and to access new devices and functionalities. But high contrast waveguides are more difficult to realize and more critical but there are no serious impediments in their use. The aim of this contribution is to investigate the dependence of the waveguide characteristics with respect to the index contrast and explore the difficulties. In this paper, the impact of the index contrast on the characteristics of dielectric waveguides such as single mode regime, losses, fiber to waveguide coupling, technological constraints and available materials are investigated. The investigation shows that the dependence of the sensitivity in optical waveguide sensors on the polarization of guided modes is less in small refractive index difference waveguide compare than the silicon on silica waveguides but it provide flexibility to the design engineers.
[optical fibre losses, single mode regime, integrated optoelectronics, Transverse Electric (TE) mode, Silica, Optical waveguides, optical fibre polarisation, low index silicon waveguides, Optical polarization, fibre optic sensors, refractive index contrast, optical dielectric waveguide, Optical variables control, Silicon, fiber-waveguide coupling, Core, guided mode polarization, optical waveguide sensors, Low index, Indexes, Cladding, elemental semiconductors, silicon, Planar waveguide, Si, refractive index, optical losses, Optical sensors, Optical refraction]
Design of a highly nonlinear single mode hexagonal photonic crystal fiber for high negative dispersion
2016 19th International Conference on Computer and Information Technology
None
2016
A simple hexagonal photonic crystal fiber is presented, that exhibits simultaneously high negative dispersion coefficient and high nonlinearity. The finite element method with perfectly matched absorbing layers boundary condition is used to investigate the guiding properties. The designed fiber offers large negative dispersion coefficient of about -3,471 ps/(nm&#x00B7;km) and nonlinear coefficient of 60.09 W-1km-1 at the excitation wavelength of 1.55 μm. Confinement loss of the proposed fiber is also reported which is 0.6753 dB/km. Due to its superior optical characteristics the proposed fiber can be considered as a promising candidate for dispersion compensation and super continuum generation.
[holey fibres, optical fibre losses, highly nonlinear single mode hexagonal photonic crystal fiber design, guiding properties, Photonic crystal fibers, perfectly matched absorbing layer boundary condition, optical characteristics, Optical fiber dispersion, Fabrication, wavelength 1.55 mum, finite element method, Optical fiber communication, Nonlinear optics, confinement loss, photonic crystal fiber, Optical fibers, finite element analysis, optical design techniques, excitation wavelength, dispersion coefficient, nonlinear coefficient, Confinement loss, nonlilearity, dispersion compensation, optical fibre dispersion, supercontinuum generation, photonic crystals, negative dispersion coefficient]
Numerical study of usefulness of parabolic index profile for the design of erbium doped fiber lasers and amplifiers
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper, the usefulness of parabolic index profile with single clad as well as double clad fiber structure has been investigated for the design of erbium doped fiber amplifier (EDFA) and laser (EDFL). The characteristics of parabolic profile fiber lasers in terms of couple pump power and cavity length have been simulated for 980 nm pump and 1550 nm signal. At pump wavelength, the power confinement of double clad (94.55%) is higher than single clad (93.30%). At signal wavelength, the power confinement of double clad (83.80%) is much higher than single clad (67.88%). We found that, EDFL exhibits very high gain. Output power of 68.68 mw (18.36dBm) for single clad and 68.92 mw (18.38 dBm) for double clad is obtained at 140 mW pump power and 50 cm fiber length. It is observed that EDFA provides very high small signal gain in expense of low pump power and small fiber length. We obtained 40 dB small signal gain with 120 mW pump and 1m fiber length. Therefore, parabolic index profile may be useful to design EDFA and EDFL. In the case of EDFL, both geometry offer almost same performance but double clad shows very well result for EDFA.
[laser cavity resonators, erbium, EDFL, Erbium-doped fiber lasers, Optical fiber dispersion, gain, power 120 mW, optical fibre couplers, single clad fiber structure, power confinement, numerical analysis, optical pumping, Erbium-doped fiber amplifiers, Erbium, size 50 cm, optical pump power, optical fibre testing, double clad fiber structure, Laser excitation, numerical study, Indexes, optical design techniques, wavelength 1550 nm, Parabolic profile, wavelength 980 nm, Optical fiber amplifiers, parabolic index profile, erbium doped fiber amplifier design, erbium doped fiber laser design, optical fibre amplifiers, EDFA, refractive index, cavity length, power 140 mW, size 1 m]
Modeling of solar photovoltaic system using MATLAB/Simulink
2016 19th International Conference on Computer and Information Technology
None
2016
This work presents a Simulink-based model of a photovoltaic (PV) system using a single-diode and two-diode model of solar cell. A comparison between the two-diode and single-diode model of PV cell has been illustrated. In addition, the output of series-parallel connection of PV cells has been examined. In the model, series and shunt resistances are calculated by an efficient iteration method based on open-circuit voltage, short-circuit current and irradiance values. The PV module implemented in Simulink/MATLAB considers five parameters. The parameters are series and shunt resistance, reverse saturation current, photocurrent and ideality factor. Approximate parameters are obtained from the manufacturers datasheets. The model includes light intensity and ambient temperature as input. Power, cell temperature and voltage as well as any measurements of interests are the outputs. For the grid connection of solar cell, inverter, filter and a step-up transformer is utilized. The performance of the model is found satisfactory.
[photoconductivity, iterative methods, Temperature, PV cells, mathematics computing, ambient temperature, Inverters, digital simulation, Irradiance, shunt resistances, MATLAB, irradiance values, short-circuit current, photocurrent, Solar PV, Photovoltaic cells, series resistances, light intensity, Series-parallel, two-diode model, Mathematical model, power grids, PV module, MATLAB-Simulink, ideality factor, iteration method, Computational modeling, open-circuit voltage, photovoltaic power systems, manufacturer datasheets, power engineering computing, single-diode model, Resistance, grid connection, solar photovoltaic system, step-up transformer, reverse saturation current, solar cell, series-parallel connection]
Reduced-order modeling of index-1 vibrational systems using interpolatory projections
2016 19th International Conference on Computer and Information Technology
None
2016
Large sparse second order index-1 descriptor systems arise in various disciplines of science and engineering, such as constraint mechanics or multibody dynamics, mechatronics (where mechanical and electrical elements are coupled), but also RLC circuit design. Simulation, controller design and design optimization are only some applications of such models. Either of these tasks, just like any other many-query situation becomes unfeasible when the system is high dimensional. This paper discusses an algorithm to obtain a reduced state space model of a large sparse second order index-1 system using an interpolatory projection method based on the iterative rational Krylov algorithm (IRKA). In each iteration of this algorithm, we need to solve a number of linear systems. The main contribution of this paper is to solve these linear systems by exploiting the sparsity of the original model, which reduces the computational cost drastically. The algorithm is applied to a micro-mechanical piezo-actuated structural FEM model of a certain building block of a machine tool. Numerical experiments with a complex 3d model of an adaptive spindle support (a piezo-mechanical multiphysics system) show the effectivity and efficiency of the techniques.
[Adaptation models, iterative methods, simulation, controller design, sparsity, Sparse matrices, optimisation, micro-mechanical piezo-actuated structural FEM model, reduced order systems, iterative rational Krylov algorithm, Reduced order systems, Numerical models, Mathematical model, IRKA, vibrations, RLC circuit design, design optimization, second order index-1 systems, Computational modeling, RLC circuits, index-1 vibrational systems, interpolatory projections, model reduction, control system synthesis, large sparse second order index-1 descriptor systems, finite element analysis, reduced-order modeling, Interpolation, interpolation]
Simulation and visualization of carrier trajectories in distributed MIGFET devices a simulation of newtonian particles at small dimensions
2016 19th International Conference on Computer and Information Technology
None
2016
Distributed MIGFET (Multiple Independent Gate Field Effect Transistor) based &#x201C;Equivalent Circuit&#x201D; design requires multiple channels adjacent to one another for optimization. To test a particular behavior of carriers in nMOS, a particle-based simulation is carried out. Due to the limited resources, the scale and complexity of the simulation is kept at a minimum. It has been found in the simulation that depending on the energy, carriers can travel to different parts of the device and adding a potential barrier reduces this problem.
[Computers, Electric potential, MOSFET, carrier trajectory visualization, distributed MIGFET devices, classical mechanics, equivalent circuits, multiple independent gate field effect transistor, optimization, Newtonian particles, semiconductor device models, Electric fields, carrier trajectories, MIGFET, Computational modeling, simulation complexity, molecular dynamics, carrier trajectory simulation, nMOS, particle-based simulation, Logic gates, carrier mobility, Transistors, Integrated circuit modeling, equivalent circuit design]
Stability analysis of forced duffing oscillator (FDO)
2016 19th International Conference on Computer and Information Technology
None
2016
The Duffing oscillator is one of the prototypes of nonlinear dynamics. Jump phenomenon can be induced by either varying the amplitude or the frequency of excitation. A comprehensive study of jump phenomenon of the duffing oscillator in two parameters (amplitude and frequency of excitation) is performed here. The analysis is based on a method of multiple time scale(MMS). In this research the stability conditions as well as unstable regimes on the frequency response curve are determined. The research also focuses on the stability of the dynamic motion of FDO. The amplitude response displays hysteresis in the frequency domain and jump phenomenon is observed in the case of primary resonance of duffing oscillator. The response of a vander Pol oscillator with sinusoidally varying amplitude is also discussed for non linear resonance. Here a method is devised to control the amplitude of oscillations. Hence Multiple harmonics are obtained as well as spring behaviour is also controlled by tweaking the cubic term of forced duffing oscillator.
[Damping, Computers, forced duffing oscillator, chaotic behavior, stability analysis, Duffing oscillator, frequency domain, stability conditions, FDO, vander Pol oscillator, nonlinear resonance, Modulation, multiple harmonics, Frequency response, jump phenomenon using method of multiple scales(MMS), Nonlinear systems, oscillations, nonlinear dynamics, stability, jump phenomenon, dynamic motion, multiple time scale, Bifurcation, relaxation oscillators, frequency response curve, frequency-domain analysis, primary resonance, Oscillators, MMS, amplitude response displays hysteresis, spring behaviour]
Parasite element materials and their effect on blade antenna performance
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper, method for increasing the performance of a blade antenna by changing the materials of the parasite grounded element is introduced. Return loss, reflection coefficient, directionality depends of a blade antenna depends on the parasite element. Changing the materials of the parasite element shows different results. The whole work is done in search of the suitable material for the parasite element. The antenna is designed in Antenna Magus Software and EM simulation is done in the CST Microwave studio.
[Microwave antennas, blade antenna, Blades, parasite element, Aluminum, EM simulation, CST Microwave studio, Antenna Magus software, VSWR, reflection coefficient, directive antennas, antenna radiation patterns, Lead, parasite grounded element material, directional antenna, Finite element analysis, blade antenna performance, return loss, Antenna radiation patterns]
Statistical approach based dynamic route selection support system
2016 19th International Conference on Computer and Information Technology
None
2016
A proper route selection process enables human beings to reach faster at their destination. This work introduces a dynamic route selection support system to find the best suitable path for a source-destination pair. It considers three individual road traffic delays which are considered as signalized delay, cross section delay and path delay. All of them are used to calculate total delay time which is followed by an algorithm that reflects the proposed system model. The term, path delay and cross section delay use general delay calculation methods whereas, based on the traffic environment, signalized delay requires special delay calculation method. For signalized delay calculation, heterogeneous traffic environment considers an empirical term which is integrated with classical delay model. As the capital of Bangladesh, Dhaka city upholds all characteristics of heterogeneous traffic environment, this work finds it more suitable in using modified webstar delay model. To verify signalized delay calculation, the term variance, derived from taylor series has been used. Performance analysis shows that the adjustment term that is considered for signalized delay calculation is suitable for Dhaka city. Thus, using statistical approach, the proposed model identifies best suitable path among all available paths between source-destination points.
[Adaptation models, cross section delay, Roads, heterogeneous traffic environment, Delay time calculation, Dhaka city, statistical approach based dynamic route selection support system, classical delay model, Variance, road traffic delays, Signalized delay, traffic information systems, Mathematical model, Junctions, total delay time calculation, road traffic, path delay, Urban areas, signalized delay calculation, Taylor series, taylor series, general delay calculation, Bangladesh, Modified webster formula, source-destination pair, modified webstar delay model, Delays, traffic environment, statistical analysis, performance analysis]
Gate dielectric strength dependent performance of CNT TFET: A tight binding study
2016 19th International Conference on Computer and Information Technology
None
2016
This article presents a detail study of the performance of CNT TFET taking into account of different dielectric strength of gate oxide materials. Here we have investigated the transfer characteristics, on/off current ratio (I<sub>ON</sub>/I<sub>OFF</sub>), subthreshold slope of the device using Non Equilibrium Greens Function (NEGF) formalism in tight binding frameworks. The results are obtained by solving the NEGF and Poisson's equation self-consistently in NanoTCAD ViDES environment and found to have an interesting dependency of gate oxide dielectric strength over the performance of CNT TFET.
[Performance evaluation, electric strength, carbon nanotube, carbon nanotube field effect transistors, transfer characteristics, NanoTCAD ViDES environment, CNT TFET, gate dielectric strength dependent performance, Barrier width, Tunneling, non equilibrium Greens function formalism, Oxide strength, gate oxide materials, Electric fields, Tight Binding, Dielectric constant, tunneling field effect transistor, Poisson equation, technology CAD (electronics), subthreshold slope, TFETs, tight binding study, Logic gates, on-off current ratio, Energy Band, Dielectric breakdown, Green's function methods, NEGF formalism, subthreshold slop]
An analytical approach for modelling of a top gated graphene based MOSFET
2016 19th International Conference on Computer and Information Technology
None
2016
In the neoteric years, Graphene is one of the predominant resplendence in the horizon of fabrication technology owing to some of its tremendous electronic properties like zero band gap, high saturation velocity, higher electrical conductivity and so on. Graphene based devices are now being deliberated as one of possible options for post Si based electronics era. In this paper, we have analyzed the miscellaneous idiosyncratics of top gated graphene metal oxide semiconductor field effect transistor (MOSFET). Surface potential dependent quantum capacitance is obtained self-consistently along with linear and square root approximation. Gate voltage dependence of surface potential has been shown. Output characteristics, transfer characteristics of the proposed device model have been investigated. Considering all of the graphical illustrations, we strongly recommend Graphene to be a fellow replacement of silicon based devices in future MOSFET applications.
[Electric potential, MOSFET, saturation velocity, Output Characteristics, square root approximation, metal oxide semiconductor field effect transistor, top gated graphene modelling, transfer characteristics, linear root approximation, miscellaneous idiosyncratics, zero band gap, Mathematical model, device model, graphene devices, fabrication technology, surface potential dependent quantum capacitance, gate voltage dependence, electronic properties, Surface Potential, analytical approach, Transfer Characteristics, Graphene, Logic gates, surface potential, Quantum capacitance, electrical conductivity, MOSFET applications]
Optimal route selection in complex multi-stage supply chain networks using SARSA(&#x03BB;)
2016 19th International Conference on Computer and Information Technology
None
2016
Success of any supply chain highly depends on the appropriate use of transportation. It is always a tough decision to select an optimal route in the stages of a supply chain that will balance the reverse condition of cost and time. Q-learning is already getting implied to optimize routes, but it greatly lacks in designing a complete Markov Decision Process (MDP) with enough number of states and actions sets. In this paper, we propose an MDP that can fit the design of a proper multi-stage supply chain network and intend to solve the MDP with SARSA (&#x03BB;) algorithm. The algorithm was simulated in a java based environment and experimental results show that Q-learning produced higher chunks of rewards but SARSA (&#x03BB;) was faster in case of convergence speed.
[Algorithm design and analysis, complex multistage supply chain networks, Q-learning, Supply chains, Transportation, Optimization, Genetic algorithms, goods dispatch data processing, reinforcement learning, reverse condition, SARSA algorithm, MDP, optimal route selection, learning (artificial intelligence), Java, state-action-reward-state-action algorithm, supply chain, Java based environment, convergence speed, transportation, Rails, supply chain management, vehicle routing, Learning (artificial intelligence), Markov processes, Markov decision process]
Modified van der pol (Vdp) oscillator based cardiac pacemakers
2016 19th International Conference on Computer and Information Technology
None
2016
The paper involves the study of nonlinear dynamics oscillator and numerical methodology to analyze and resolute the nonlinear dynamics model. Based on either the classical Vander Pol oscillator or other nonlinear oscillators, these models were interesting rather because of the physical phenomena that could be obtained (chaos and synchronization). Here, we can simulate many important physiological features of true physiological action potentials in practical systems by adjusting the parameter. We also show different ways to change pacemaker actions. As van der pol oscillator can model heart beat phenomenon so here these oscillator models are modified so that they can match well with the results obtained by actual pacemakers. We model electrical activity of cardiac electric system including a trial and ventricular muscles solving a set of coupled nonlinear oscillator equations. A new mathematical model for the electrical activity of the heart is proposed. In this paper a modified Vander Pol oscillator model was designed in order to reproduce the time series of the action potential generated by a natural pacemaker of the heart (i.e., the SA or the AV node). The model represents a special singularly perturbed three-dimensional system of ordinary differential equations with one fast and two slow variables.
[pacemakers, heartbeat, cardiac electric system, nonlinear dynamics model, History, atrial muscles, nonlinear dynamical systems, muscle, numerical analysis, ventricular muscles, Physiology, coupled nonlinear oscillator equations, Mathematical model, physiological action potentials, cardiac pacemaker, chaos, singularly perturbed three-dimensional system, Computational modeling, blood vessels, relaxation oscillators, time series, van der pol oscillator, heart beat phenomenon, action potential, Oscillators, cardiology, nonlinear differential equations, classical Van der Pol oscillator, ordinary differential equations, Pacemakers, nonlinear dynamics oscillator, Brain modeling, synchronization, modified Van der Pol oscillator-based cardiac pacemakers]
Usability evaluation of a video conferencing system in a university's classroom
2016 19th International Conference on Computer and Information Technology
None
2016
The integration of video conferencing systems (VCS) have increased significantly in the classrooms and administrative practices of higher education institutions. The VCSs discussed in the existing literature can be broadly categorized as desktop systems (e.g. Scopia), WebRTC or Real-Time Communications (e.g. Google Hangout, Adobe Connect, Cisco WebEx, and appear.in), and dedicated (e.g. Polycom). There is a lack of empirical study on usability evaluation of the interactive systems in educational contexts. This study identifies usability errors and measures user satisfaction of a dedicated VCS in a Danish university's classrooms. This work contributes (1) to the methodological approach that uses mixed methods to collect and analyze data from users as part of a summative evaluation, (2) to demonstrate the methods applied by independent usability evaluator using field study approach, (3) to the usability evaluation literature dealing with empirical evaluation methods. PACT (people, activity, context, and technology) analysis of participant observation and interview data shows a lack of user guide, training, IT support, and vendor coordination. Software usability measurement inventory (SUMI) analysis of 12 user responses results below average score. Post-study system test by the vendor has identified cabling and setup error. Applying SUMI followed by qualitative methods might enrich evaluation outcomes.
[Computers, human factors, VCS, teleconferencing, usability errors, appear.in, summative evaluation, empirical evaluation, Polycom, usability evaluation, Education, video conferencing system, interactive systems, SUMI, video communication, Context, Scopia, Danish university classrooms, data analysis, PACT analysis, software usability measurement inventory, field study approach, video conferencing system usability evaluation, Information technology, data collection, Standards, real-time communications, methodological approach, independent usability evaluator, higher education institutions, desktop systems, Cisco WebEx, Google Hangout, empirical evaluation methods, computer aided instruction, educational institutions, Adobe Connect, Usability, people-activity-context-and-technology analysis, WebRTC, user satisfaction]
Emotion recognition using the shapes of the wrinkles
2016 19th International Conference on Computer and Information Technology
None
2016
The emotion recognition has become a hot research topic in different domains: Human-Machine-Interaction, Natural Language processing etc. Recent research in the domain of Human Computer Interaction aims at recognizing the user's emotional state to give a smooth interface between humans and computers and to improve their interaction. In this paper we propose an emotion recognition system based on the analysis of the shapes of the wrinkles. The system contains four steps. The first one is detection of face's elements which is realized by the Viola and Jones detector. The second is localization of the wrinkles which is achieved automatically. Then, the information extraction. Finally, the classification using the wavelet network.
[Computers, Wavelet transforms, Emotion recognition, Conferences, wrinkle shapes, Viola detector, classification, emotion recognition, face element detection, Jones detector, the shapes of the wrinkles, wavelet network, component, user emotional state recognition, Mouth, Speech recognition, face recognition, human computer interaction, Face]
EXPRESit: Interactive messaging via gestures
2016 19th International Conference on Computer and Information Technology
None
2016
Text communication or messaging is devoid of emotional expression, gesture and intonation. As a result text communication has become a robotic conversation between human beings. People uses various applications like, Facebook Messenger, Viber, WhatsApp etc. for the purpose of messaging, where they follow the traditional system of writing messages and sending to the receivers. This paper, presents a new idea of messaging through gestures which is one of the primary elements of interaction. The message will be produced through the gestures that the sender will make. If, the sender shows a `thumbs up' gesture, a text or emoticon is generated and sent to the receiver. One of the best features of our system is, it can generate Bengali messages. The system can recognize `thumbs up' gesture which is converted to, means `Well Done' and sent to the receiver. We have evaluated our designed prototype and got satisfactory results.
[Computers, Thumb, electronic messaging, interactive messaging, Gesture recognition, Receivers, Interactive messaging, hand gestures, Hand gestures, text communication, emotion recognition, Kinect, EXPRESit, Bengali messages, interactive systems, Cameras, emotional expression, human computer interaction, Sensors, Human Computer Interactions]
A novel health support system with biometric data acquisition device
2016 19th International Conference on Computer and Information Technology
None
2016
At present, health related mobile applications have drawn considerable attention from researchers around the world. However, these applications only take disease symptoms as input from the users, but do not take vital signs of body into consideration. To provide vital signs, users need to collect data from external devices like blood pressure machine, blood glucose monitor etc. which may not be affordable for low-to-moderate income people. Besides, services that collects vital data from the user and provides directly into the system need internet connection for checking health status, which may be unavailable for people from less developed areas. The objective of our project is to present an effective health support system to the people from developing and underdeveloped countries in physical troublesome situations in a low-cost and user-friendly way. This system directly collects vital signs of body as biometric inputs with a health kit device. Using these inputs along with additional symptoms directly taken from the users, a mobile application checks their current health status and identifies chances of possible diseases with accurate severity level and provides suggestions accordingly. Moreover, it also provides first-aid counsels to handle accidents on emergency basis. The test results on the prototype show that it has encouraging potential as a tool for regular health checkups and emergency conditions.
[accident handling, biometric inputs, disease severity level, emergency conditions, health kit device, health support, Mobile communication, biometric data collector, biometrics (access control), biometric data acquisition device, healthcare, mobile computing, data acquisition, body vital signs, Bioinformatics, health care, current health status, health support system, Diseases, health checkups, human-computer interaction, Hospitals, medical support, first-aid counsels, mobile health application, health related mobile applications, Internet, Biomedical monitoring, Medical diagnostic imaging]
Movement related events classification from functional near infrared spectroscopic signal
2016 19th International Conference on Computer and Information Technology
None
2016
This study investigates left hand (LH) and right hand (RH) movements related events with respect to resting state (RS) on the basis of hemodynamic response of dorsolateral prefrontal cortex (DLPFC). The signals of hemodynamic responses are acquired by 16 channels functional near infrared (fNIR) spectroscopy. From these multiple channel data, it is difficult to classify the events. To solve this difficulty, statistically the most effective channels are identified. For identifying most effective channels, at first the raw fNIR signal is filtered and separated into three classes (RS, LH, and RH movements) based on the events. The most effective channels are found out by t-test hypothesis and effect size (ES) statistics. Furthermore, for classifying purpose, the time domain features are extracted from oxygenated hemoglobin (HbO<sub>2</sub>) signal of the most effective channels. From these features, artificial neural network (ANN) is used to classify the events. The classifying accuracy is achieved 79.5% in average. This study is helpful to estimate the voluntary movement from frontal cortex neural activity.
[left hand movement related events, artificial neural network, ANN, Effect Size (ES), right hand movement related events, functional near infrared spectroscopic signal, ES statistics, t-test hypothesis, dorsolateral prefrontal cortex, time domain feature extraction, 16 channel functional near infrared spectroscopy, event classification, Finite impulse response filters, feature extraction, effect size statistics, hemodynamic response, t-test Hypothesis, statistical testing, Head, DLPFC, raw fNIR signal, oxygenated hemoglobin signal, Data acquisition, Neurons, fNIR spectroscopy, Artificial neural networks, hemodynamic responses, LH movements, frontal cortex neural activity, RS movements, Artificial Neural Network (ANN), signal classification, RH movements, Functional Near Infrared Spectroscopy (fNIRS), medical signal processing, resting state, movement related event classification, infrared spectra, Feature extraction, Hemodynamics, Hemodynamic Response, haemodynamics, neural nets]
Activity recognition of a badminton game through accelerometer and gyroscope
2016 19th International Conference on Computer and Information Technology
None
2016
The scope for doing physical exercises in daily life is declining day by day. But, the importance of human physical exercise for a healthy life, remains the same. It is necessary to generate a solution to simulate the outdoor experience of physical exercises and sports inside our home. In this paper, we propose an idea of recognizing the activities of a badminton game which has the potential to be useful in simulating the Badminton Sport. We have used motion sensors (e.g. Accelerometer, Gyroscope) to recognize different activities like, serve, smash, backhand, forehand, return etc. We have collected data from a large set of users and labeled their data over several instances. We have applied the K-Nearest Neighbors (k-NN) and Support Vector Machines (SVM) classifiers to recognize those activities. Existing approaches (e.g. Microsoft Xbox 360) used vision based techniques to recognize activities and use it in simulated games but we are using sensor based approach. Vision based approaches have some limitations such as slow rate of data, illumination constraints, occluded backgrounds etc. Our approach gives a low cost solution with a classification technique which is faster. The experimental result shows a decent recognition rate.
[Computers, vision based approaches, human physical exercise, occluded backgrounds, SVM, badminton game, k-NN classifiers, k-NN, accelerometer, Activity recognition, Accelerometer, Gyroscopes, illumination constraints, vision based techniques, Accelerometers, pattern classification, Activity Recognition, support vector machines, support vector machines classifiers, Badminton Sport, daily life, gyroscopes, image motion analysis, gyroscope, k-nearest neighbors, simulated games, classification technique, motion sensors, badminton sport, Games, computer vision, accelerometers, Gyroscope, Acceleration, medical computing, sport, activity recognition]
Cross platform interactive programming learning environment for kids with edutainment and gamification
2016 19th International Conference on Computer and Information Technology
None
2016
Traditional way of learning programming is often tedious and fearsome to students, especially to kids. In most cases this results reluctance and uptake, while programming is the core of modern technology. Game is the signature of immersion. With its intense power of engaging players, it can wipe out the aversion and motivate students on coding. Here we have described a better way of learning programming with digital game. We have developed a cross platform game that provides a role playing game design for the player, at the same time a kinesthetic approach to reinforce the basic concepts of programming.
[Computers, cross platform interactive programming learning, computer science education, role playing game design, programming concepts, entertainment, Game based learning, Programming, Mobile communication, digital game, Encoding, Edutainment, Gamification, Programming profession, student motivation, gamification, Entertainment industry, computer games, interactive programming, Games, kids edutainment]
Fuzzy based weight to mine frequent patterns from human interaction in meeting using directed acyclic graph
2016 19th International Conference on Computer and Information Technology
None
2016
Meeting is a gathering of people to exchange information and plan joint activities for achieving a goal through verbal interactions. In a good meeting, participants' ideas are heard, decisions are made through discussions and activities are focused on desired results. The challenging part is to mine the most relevant interaction pattern from the meeting. Tree structures are not able to capture all kinds of interaction and also not able to distinguish between ranks among the participants. Thus, meetings can be modeled as weighted DAGs, from which weighted frequent interaction patterns can be discovered. However, the weights can vary based on a participant's age and confidence of an action like commenting on a proposal. As this may not capture all interaction patterns, the rank of a participant is needed to be considered. This paper proposes a technique to calculate Fuzzy based weight of the participants based on age, rank and strength of comment and the existing weighted DAG is used to mine weighted frequent interaction patterns.
[Computers, Algorithm design and analysis, Conferences, data mining, fuzzy set theory, directed acyclic graph, fuzzy reasoning, Data mining, weighted DAG, human interaction, fuzzy based weight, meeting, participant age, verbal interactions, Directed Acyclic Graph (DAG), Information technology, Fuzzy logic, information exchange, directed graphs, Hidden Markov models, weighted frequent interaction pattern mining, participant confidence, Fuzzy inference, most relevant interaction pattern mining, meetings, Fuzzy]
Blind Reader: An intelligent assistant for blind
2016 19th International Conference on Computer and Information Technology
None
2016
In the real world, books and documents are the sources of knowledge. But this knowledge is only bounded to people with clear vision. Our society includes a group of people who does not have a clear vision or people who are blind. For this group, world is like a black illusion. The shape and structure's information of an object is unavailable to them let alone reading a document. For blind acquiring knowledge by reading documents is cumbersome. Braille is one of the methods which is used to read a book or document. In this method, any document has to be converted to braille format to become understandable to a blind. The problem arises due to the fact that, this is an expensive procedure and many times not available. The solution is rather simple, introduce a smart device with a multimodal system that can convert any document to the interpreted form to a blind. A blind can read document only by tapping words which is then audibly presented through text to speech engine. &#x201C;Blind Reader&#x201D; - developed for touch devices which is user friendly and effective interactive system for visionless or low vision people.
[Computers, low vision, handicapped aids, blind, text to speech engine, haptic, Fingers, interactive systems, Blind Reader, Hardware, screen reader, braille, visionless people, Printing, multimodal system, Portable document format, screen magnifier, touch, smart phones, Haptic interfaces, dynamic scroll, low vision people, interactive system, smart reader, smart device, braille printer, intelligent assistant for blind, visually impaired]
Evaluation of LPC trajectory for Vowel-Consoant-Vowel sequence
2016 19th International Conference on Computer and Information Technology
None
2016
This paper shows an analysis of Linear Predictive Coding (LPC) coefficient for Vowel-Consonant-Vowel (VCV) sequence in order to determine the temporal transfer function of vocal tract. The Linear Prediction model is a linear combination of the speech sample and the glottal pulse as exciter. The acoustical output of the dynamic system is speech sample. Here, a sequence VCV is analyzed by using LPC coefficient and their mean, standard deviation and skewness. The investigation, focused on the variation of LPC coefficient can be applied successfully for speech recognition and vocal tract dynamics modeling.
[Computers, LPC trajectory, glottal pulse, Predictive models, VCV sequence, sequences, speech recognition, speech coding, temporal transfer function, Production, Linear Predictive Coding (LPC), skewness, linear predictive coding coefficient, Vowel-Consonant-Vowel (VCV), speech sample, mean deviation, acoustical output, Linear predictive coding, LPC coefficient, Standards, vocal tract, Vocal Tract Modeling, Speech, vocal tract dynamics modeling, transfer functions, Speech processing, vowel-consonant- vowel sequence, standard deviation]
Tracking of fixed shape moving objects based on modified particle filters
2016 19th International Conference on Computer and Information Technology
None
2016
High resolution imaging and its processing is one of the most researched areas now-a-days. Recognition of moving object in a remote scene is a challenging task because of presence of multiple objects present there. Similarly, to track an object after the recognition part is also difficult task because of constant change in not only the coordinates of the target object but also the objects that are cause of occlusion for it. To simplify the task, a constant updating of the algorithms have been carried out in this area pertaining to main two tasks in task. One being is how to reduce the time to recognize the object and other is to efficiently track it, possibly in real time. In this paper, a unified approach is presented which efficiently tracks an object using Particle filter. First step is to identify the coordinates of the object and then a bounding box is placed on it. Then tracking algorithm uses particle filter to efficiently track the moving object in successive frames using probabilistic estimation. The particle filter uses posterior distribution of random variables related to Markov chain for estimation.
[Computers, object recognition, tracking algorithm, Correlation, particle filtering (numerical methods), modified particle filters, Markov chain, Particle, motion estimation, object tracking, Particle filters, MACH, Mathematical model, image resolution, probabilistic estimation, Target tracking, Estimation, probability, random processes, random variables, Minimization, object occlusion, object coordinates, high resolution imaging, posterior distribution, Markov processes, bounding box, fixed shape moving object tracking, Random variables]
Smartphone based blood flow feature extraction and classification from Doppler spectrum images
2016 19th International Conference on Computer and Information Technology
None
2016
Ultrasound (US) Doppler spectrograms have been widely used for diagnosing vascular obstructions. This paper presents an Android smartphone based new approach for detecting the blood flow condition based on the US Doppler spectrogram images. A set of 59 spectrograms acquired from a US Doppler system is processed to extract features, and these non-redundant features are fed into a supervised classifier to determine the normal and abnormal blood flow. The classification is performed using the k-nearest neighbors (k-NN), Support vector machine (SVM), Naive Bayes (NB) and Multilayer perception (MLP) based classifiers. The SVM based classifier has shown superior performance, having an accuracy of 86.4 %, with a sensitivity and specificity of 96.4 % and 77.4 % respectively. The complete technique is implemented as an Android application and the results show the efficacy of the presented approach for the automated diagnosis of arterial diseases.
[ultrasound Doppler spectrograms, vascular obstructions, image classification, Image processing, Humanoid robots, biomedical ultrasonics, Doppler effect, Spectrogram, Open source software, MLP, arterial diseases, Ultrasonic imaging, Android (operating system), k-NN, feature extraction, Android smartphone, SVM based classifier, Classification, Doppler spectrum images, Android application, medical image processing, US, multilayer perceptrons, pattern classification, support vector machines, diseases, smart phones, smartphone based blood flow feature extraction, Blood flow, k-nearest neighbors, supervised classifier, support vector machine, multilayer perception based classifiers, NB, blood flow condition, naive Bayes, Feature extraction, Androids, patient diagnosis]
Artifact suppression from electroencephalography signals using stationary subspace analysis
2016 19th International Conference on Computer and Information Technology
None
2016
Different types of artifacts contaminate the electroencephalography (EEG) signals in brain computer interface (BCI) application. Electrocardiography (ECG) is such potential artifact which negatively affects the BCI performance. This paper presents a novel method for ECG artifact elimination from EEG using stationary subspace analysis (SSA). It is based on the consideration that the ECG components are relatively non-stationary than that of the EEG signals. Applying SSA, the total channels of raw EEG are partitioned into two groups - stationary and non-stationary. The non-stationary channels contain the ECG artifacts. A statistical test is used to measure the degree of non-stationarity. The channel with highest non-stationarity is selected as the source of ECG artifact. The normalized ECG source is used to segregate the target artifact from the measured EEG. The result of the proposed method is compared with that of the well-known statistical method independent component analysis (ICA). The experimental evaluation illustrates that the proposed method is superior to the ICA based approach in terms of ECG artifact suppression from raw EEG.
[SSA, Electric potential, Electrooculography, electroencephalography, ECG signals, stationary subspace analysis, EEG signal contamination, stationary channels, brain-computer interfaces, Time series analysis, electroencephalography signals, ICA, Muscles, Electroencephalography, BCI application, target artifact segregation, Space stations, electrocardiography, signal denoising, independent component analysis, statistical test, Electrocardiography, artifact suppression, brain computer interface]
Weighted normalized mutual information based change detection in remote sensing images
2016 19th International Conference on Computer and Information Technology
None
2016
Change detection from remote sensing images is getting more interest now a days because of abrupt changes in earth surface due to natural disasters or man-made activities. So it's an important research question of how to extract relevant information about the changes due to rainfall, droughts, flooding, destroying land cover areas and so on. This problem has been studied in some research however many of these did not consider the nonlinear relationship while detecting the changes. In this research, above limitation has been addressed and Weighted Normalized Mutual Information (WNMI) is utilized for the improvement. The WNMI technique has been applied between the reference and target images to find out the changes. Thus the changes between every object of the given dataset have been identified and able to observe the damage of any specific area as well as its subsequent recovery. Weighting has been done to count significance at the pixel level. The proposed technique can detect the changes more effectively than the traditional mutual information approach. Experimental analysis is carried on real remote sensing images and it is found that the proposed method can detect more than 96% of changes which is much better than the standard benchmark techniques.
[similarity measure, Input variables, weighted normalized mutual information based change detection, reference images, geophysical image processing, remote sensing, Entropy, Histograms, change detection, Random variables, remote sensing images, WNMI technique, target images, mutual information, pixel level, Hyperspectral image, hyperspectral imaging, Mutual information, Hyperspectral imaging]
Design and implementation of a low cost multichannel rectified EMG acquisition system
2016 19th International Conference on Computer and Information Technology
None
2016
Main objective of this work is designing and implementation of low cost electromyography (EMG) signal acquisition system which provides multichannel facilities. The major challenges of this work are to meet the quality as well as the cost at the same time. Our designed acquisition system comes with some attractive features like real time data display, offline data store, and simultaneous use of three channels. This multiple channel acquisition system is implemented using instrumentation amplifier (INA118) for pre-amplification stage. For smoothing and rectification purposes operational amplifiers (TL 072) have been used. In this system, 10 bit analog to digital (AD) converter of Arduino UNO has been used. Then the output data are displayed graphically by using open source software named by Serial Oscilloscope. The acquired data is validated by two ways. At first, the acquired data is used to control the prosthetic hand. Else, the acquired data by this designed acquisition system has been compared with certified Biopac Mp36 system and we have achieved 84.1% correlation coefficient. These two validation test proves the effectiveness of our designed system. In addition with that, the cost of this data acquisition is only 40 USD.
[Computers, real time data display, public domain software, Oscilloscopes, smoothing methods, Arduino UNO 10 bit analog to digital converter, offline data store, preamplification stage, Serial Oscilloscope, operational amplifiers, Electromyography, smoothing, correlation coefficient, instrumentation amplifiers, multichannel rectified EMG acquisition system, Prosthetics, open source software, multichannel, Data acquisition, rectification, INA118, Muscles, analogue-digital conversion, medical signal detection, prosthetic hand control, electromyography (EMG), correlation, electromyography, instrumentation amplifier, prosthetic hand, Gain, correlation methods, electromyography signal acquisition system]
Linear complexity of signed binary sequence over odd characteristic field
2016 19th International Conference on Computer and Information Technology
None
2016
In our previous work, well balanced pseudo random signed binary sequence generated by using trace function and Legendre symbol has been researched. Our previous sequence generated by applying primitive polynomial over odd characteristic field F<sub>p</sub>, trace function and Legendre symbol. The important features such as period, periodic autocorrelation, and cross-correlation have already been well discussed in our previous work. In this paper, the signed binary sequence is generated by utilizing one additional parameter A. Let p be an odd prime and F<sub>p</sub> is an odd characteristic prime field and m be the degree of the primitive polynomial f(x). The procedure for generating sequence is as follows: primitive polynomial f(x) generates maximum length vector sequence, then trace function TY(-) maps an element of extension field F<sub>p</sub>m to an element of prime field F<sub>p</sub>, next a non-zero scalar A &#x2208; F<sub>p</sub> is added to the trace value and finally Legendre symbol is used to map the scalars into signed binary sequence. In this paper, the authors have restricted the discussion on linear complexity and linear complexity profile properties of signed binary sequence based on some experimental results.
[Computers, Correlation, trace function, Complexity theory, Electronic mail, Security, Information technology, odd characteristic field, Legendre symbol, primitive polynomial, odd characteristic prime field, random sequences, extension field element, binary sequences, Legendre polynomials, pseudorandom signed binary sequence, Random sequences, signed binary sequence linear complexity]
A DNA cryptographic technique based on dynamic DNA sequence table
2016 19th International Conference on Computer and Information Technology
None
2016
This paper proposes a new technique for DNA cryptography that uses dynamic DNA sequence table to enhance the level of security. While handling with secure data, the requirements like compression, speed-up computation and processing etc are crucial issues. Bio-molecular DNA features possess the capability to cope up with these requirements. Existing DNA cryptographic techniques usually consider fixed DNA sequence table i.e., DNA bases and thereby the security is suspected to be breached by the intruder. To overcome this limitation, the proposed technique considers dynamic sequence table that assigns random ASCII characters to DNA sequence table initially. Then a finite number of iterations are applied based on a mathematical series where in every iteration the positions of ASCII characters are changed dynamically in the sequence table. Later on, One-Time-Pad (OTP) is applied on the modified encoding binary value. Again OTP ciphertext is processed through genomic conversion. Finally, it is converted into compressed ciphertext using amino acid table consisting of protein sequence that increases the confusion of the ciphertext. At last, the time requirements for encoding-decoding and encryption-decryption are evaluated and comparisons with other DNA techniques are presented.
[DNA cryptographic technique, Amino acids, dynamic DNA sequence table, dynamic sequence table, Encryption, DNA Sequence Table, OTP ciphertext, One Time Pad, random ASCII character assignment, sequence table, Iteration Process, modified encoding binary value, Biological Mechanism, amino acid table, data compression, security level enhancement, encoding-decoding time requirement, genomic conversion, cryptography, Encoding, secure data handling, compressed ciphertext, Biological information theory, encryption-decryption time requirement, one-time-pad, mathematical series, DNA, bioinformatics, DNA cryptography, protein sequence, biomolecular DNA features]
A consideration of towering scheme for efficient arithmetic operation over extension field of degree 18
2016 19th International Conference on Computer and Information Technology
None
2016
Barreto-Naehrig (BN) curve is a well studied pairing friendly curve of embedding degree 12, that uses arithmetic in F<sub>p</sub>i2. Therefore the arithmetic of F<sub>p</sub>12 extension field is well studied. In this paper, we have proposed an efficient approach of arithmetic operation over the extension field of degree 18 by towering. F<sub>p</sub>18 extension field arithmetic is considered to be the basis of implementing the next generation pairing based security protocols. We have proposed to use F<sub>p</sub> element to construct irreducible binomial for building tower of extension field up to F<sub>p</sub>6, where conventional approach uses the root of previous irreducible polynomial to create next irreducible polynomials. Therefore using Fp elements in irreducible binomial construction, reduces the number of multiplications in Fp to calculate inversion and multiplication over F<sub>p</sub>18, which effects acceleration in total arithmetic operation over F<sub>p</sub>18.
[Computers, Barreto-Naehrig curve, KSS curve, cryptographic protocols, BN curve, polynomials, Poles and towers, arithmetic, irreducible polynomials, Proposals, Information technology, extension field arithmetic, Elliptic curves, arithmetic operation, towering scheme, Cryptography, pairing based cryptography, next generation pairing based security protocols, irreducible binomial construction]
Comparative security analysis of software defined wireless networking (SDWN)-BGP and NETCONF protocols
2016 19th International Conference on Computer and Information Technology
None
2016
Software Defined Networking (SDN) is getting much attention for larger network implementation adding programmable feature in the network plane. Demand of wireless networking features is growing simultaneously. With increased network complexity and sizes, security has become an issue as vulnerabilities are still prominent for such complex network and needs to be monitored properly to provide early detection of security breaches and Denial of Service attack. Therefore, the communication protocols, an SDN architecture uses including OpenFlow, NETCONF, OpFlex and BGP etc. should be secure enough to manage such complex and large network. This paper analyzes mostly used protocols specifically for Software Defined Wireless Networking (SDWN): BGP, NETCONF, for underlying cloud or data center environment by applying Microsofts STRIDE security threat model. This work represents a comparative study among these most used protocols and holds appropriate network for their deployment.
[OpFlex, Protocols, NETCONF, Microsofts STRIDE security threat model, OpenFlow, Network Security, comparative security analysis, Communication system security, Computer crime, Software Defined Wireless Networking (SDWN), data center environment, Wireless communication, network complexity, cloud environment, protocols, BGP protocols, network security, pro- grammable feature, SDN architecture, network plane, communication protocols, software defined networking, Routing, BGP, denial of service attack, computer network security, STRIDE, security breach detection, NETCONF protocols, radiocommunication, Software, wireless networking features, software defined wireless networking]
Security enhancement in the receive diversity with co-operative relay for wireless networks
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper, a dual hop diversity communication system which is highly confidential for wireless applications is proposed to transmit the secret information from the source to receiver in the presence of interferences. In this proposed wireless communication system, the receiver is equipped with multiple antennas whereas the relays and interference are equipped with single antenna. The output signal is combined with a maximum ratio combiner (MRC) and then process it by the weighting vector and finally amplifying the receive signal by the relays. To do that the amplify-and-forward (AF) as well as decode-and forward (DF) strategies of the relays are used to increase the signal strength of the receive diversity and relay gain. Moreover, moment generating function (MGF) is used to maximize the diversity order for increasing the capacity of the receive signal and symbol error probability (SEP). Finally, the Monte Carlo simulation model is used to show the effectiveness of the proposed technique. From the simulation results, it is clear that the receive signal capacity can be significantly optimized in terms of SEP by using the AF, and DF strategies.
[telecommunication security, dual hop diversity communication system, decode-and-forward (DF), Amplify-and-forward (AF), symbol error probability (SEP), amplify and forward communication, AF strategy, maximum ratio combiner, decode and forward communication, Wireless communication, radiofrequency interference, interference presence, multiple antenna, diversity order, Monte Carlo methods, amplify-and-forward strategy, antenna arrays, Monte Carlo simulation model, wireless network, error statistics, relay networks (telecommunication), DF strategy, diversity reception, weighting vector, moment generating function, cooperative relay, Receivers, Interference, MGF, cooperative communication, wireless communication system, relay gain, symbol error probability, Relay networks (telecommunications), Diversity reception, receive diversity signal strength, security enhancement, moment generating function (MGF), receive diversity, receive signal amplification, decode-and forward strategy, Signal to noise ratio, MRC, SEP]
A content-aware hybrid architecture for answering questions from open-domain texts
2016 19th International Conference on Computer and Information Technology
None
2016
The current work blends the different paradigms of Question Answering systems and presents a content-aware hybrid architecture for an open-domain factoid questions. It combines a knowledge-based, information extraction-based and a web-based approach in a pipelined architecture to construct an answer to a question keeping the context and discourse of the question in view. The proposed semantic-aware hybrid architecture was compared with other QA systems designed over standard benchmark data. The work has shown enough potential in terms of accuracy and time domain complexity and can be used effectively as a semantic understanding-based QA system.
[Computers, text analysis, hybrid architecture, semantic parsing, knowledge-based approach, Semantics, question answering system, Web-based approach, Computer architecture, open-domain factoid questions, Knowledge discovery, semantic core, Context, Knowledge based systems, question answering, content-aware hybrid architecture, Information technology, time domain complexity, knowledge-based system, entity detection, open-domain texts, semantic understanding-based QA system, Internet, pipelined architecture, question answering (information retrieval), information extraction-based approach, computational complexity, text-knowledge]
Smartphone based ischemic heart disease (heart attack) risk prediction using clinical data and data mining approaches, a prototype design
2016 19th International Conference on Computer and Information Technology
None
2016
We developed a simple approach to predict risk of developing Ischemic Heart Disease (IHD) (Heart Attack) using smartphone. An Android based prototype software has been developed by integrating clinical data obtained from patients admitted with IHD. The clinical data from 787 patients has been analyzed and correlated with the risk factors like Hypertension, Diabetes, Dyslipidemia (Abnormal cholesterol), Smoking, Family History, Obesity, Stress and existing clinical symptom which may suggest underlying non detected IHD. The data was mined with data mining technology and a score is generated. Risks are classified into low, medium and high for IHD. On comparing and categorizing the patients whose data is obtained for generating the score; we found there is a significant correlation of having a cardiac event when low &amp; high and medium &amp; high category are compared; p=0.0001 and 0.0001 respectively. Our research is to make simple approach to detect the IHD risk and aware the population to get themselves evaluated by a cardiologist to avoid sudden deaths. Currently available tools has some limitations which makes them underutilized by population. Our research product may reduce this limitation and promote risk evaluation on time.
[Heart, Chi-square, IHD, data mining, heart attack risk prediction, clinical data mining, Data Mining, History, Android (operating system), Sociology, smoking, clinical symptom, obesity, stress, smartphone based ischemic heart disease risk prediction, cardiac event, Prediction, diseases, abnormal cholesterol, smart phones, Statistics, cardiology, Android based prototype software, Heart Disease, Diseases, Android, ACS, Diabetes, Androids, dyslipidemia, medical computing, hypertension, diabetes, Smartphone]
Combining a rule-based classifier with ensemble of feature sets and machine learning techniques for sentiment analysis on microblog
2016 19th International Conference on Computer and Information Technology
None
2016
Microblog, especially Twitter, have become an integral part of our daily life, where millions of users sharing their thoughts daily because of its short length characteristics and simple manner of expression. Monitoring and analyzing sentiments from such massive Twitter posts provide enormous opportunities for companies and other organizations to estimate the user acceptance of their products and services. But the ever-growing unstructured and informal user-generated posts in Twitter demands sentiment analysis tools that can automatically infer sentiments from Twitter posts. In this paper, we propose an approach for sentiment analysis on Twitter, where we combine a rule-based classifier with a majority voting based ensemble of supervised classifiers. We introduce a set of rules for the rule-based classifier based on the occurrences of emoticons and sentiment-bearing words. To train the supervised classifiers, we extract a set of features grouped into Twitter specific features, textual features, parts-of-speech (POS) features, lexicon based features, and bag-of-words (BoW) feature. A supervised feature selection method based on the chi-square statistics (&#x03C7;2) and information gain (IG) is applied to select the best feature combination. We conducted our experiments on Stanford sentiment140 dataset. Experimental results demonstrate the effectiveness of our method over the baseline and known related work.
[Computers, information gain, Sentiment analysis, text analysis, supervised classifiers, sentiment monitoring, twitter specific features, sentiment-bearing words, Microblog, machine learning techniques, chi-square statistics, feature combination, Uniform resource locators, Training, emoticon occurrences, feature extraction, knowledge based systems, bag-of-words feature, learning (artificial intelligence), sentiment lexicon, feature selection, parts-of-speech features, pattern classification, sentiment analysis, textual features, feature set extraction, short length characteristics, Stanford sentiment140 dataset, machine learning, Information technology, Support vector machines, massive twitter posts, supervised feature selection, BoW feature, Feature extraction, social networking (online), user acceptance, microblog, rule-based classifier, majority voting based ensemble, lexicon based features]
Predicting breast cancer recurrence using effective classification and feature selection technique
2016 19th International Conference on Computer and Information Technology
None
2016
Breast cancer is a major threat for middle aged women throughout the world and currently this is the second most threatening cause of cancer death in women. But early detection and prevention can significantly reduce the chances of death. An important fact regarding breast cancer prognosis is to optimize the probability of cancer recurrence. This paper aims at finding breast cancer recurrence probability using different data mining techniques. We also provide a noble approach in order to improve the accuracy of those models. Cancer patient's data were collected from Wisconsin dataset of UCI machine learning Repository. This dataset contained total 35 attributes in which we applied Naive Bayes, C4.5 Decision Tree and Support Vector Machine (SVM) classification algorithms and calculated their prediction accuracy. An efficient feature selection algorithm helped us to improve the accuracy of each model by reducing some lower ranked attributes. Not only the contributions of these attributes are very less, but their addition also misguides the classification algorithms. After a careful selection of upper ranked attributes we found a much improved accuracy rate for all three algorithms.
[Error analysis, data mining, Classification algorithms, SVM, ranker, Data mining, optimisation, decision tree, ROC curve, Decision trees, learning (artificial intelligence), feature selection, Na&#x00EF;ve Bayes, pattern classification, cancer patient data, support vector machines, probability, C4.5 decision tree, cancer recurrence probability optimization, breast cancer, Breast cancer, Support vector machines, recurrence, Naive Bayes, support vector machine, attribute selection, breast cancer recurrence prediction, UCI machine learning, cancer, Bayes methods, medical computing]
Location, time, and preference aware restaurant recommendation method
2016 19th International Conference on Computer and Information Technology
None
2016
The location-based social networks introduce a platform to understand the users' preferences. In general, the users of the social networks promote the exchange of data within their own networks. Such data are being used in the literature for a wide variety of location-aware recommendation systems. We investigate the role of amalgamation of real-time GPS logs, and historical check-in data in developing a restaurant recommendation system. In this paper, we propose a novel location, time and preference aware restaurant recommendation method using the users' current geospatial location, historical check-in data of the users, and the time of the recommendation request. In the proposed method, users' check-in histories are analyzed individually to discover users' visiting trends, food preference trends, and overall popularity of the restaurants. At the same time, each restaurant's operation time and the distance are modeled separately to compute recommendation scores of the restaurants. The recommendation scores are computed by considering four key factors, namely, i) user's preference scores, ii) the distance of the restaurants, iii) the time of a day, and iv) the popularity scores of the restaurants. Each of these key factors is modeled carefully to estimate realistic recommendation scores for the restaurants in a given geospatial range. We tested our proposed method using an available online dataset. The experimental results confirm the effectiveness of the proposed method.
[Computational modeling, Social network services, check-in logs, social networks, real-time GPS logs, Probabilistic logic, History, location-based social networks, geospatial location, location aware restaurant recommendation, Global Positioning System, Restaurant recommendations, time aware restaurant recommendation, mobile computing, recommender systems, historical check-in data, catering industry, preference aware restaurant recommendation, real-time systems, Market research, social networking (online), Data models, mutual reinforcement learning]
Protein structure prediction using chemical reaction optimization
2016 19th International Conference on Computer and Information Technology
None
2016
Protein Structure Prediction (PSP) is an NP-hard optimization problem that has been solved by many existing algorithms. Simply, it can be thought of a process of predicting the native 3D structure from its amino acid sequence. Chemical Reaction Optimization (CRO) is a recent metaheuristic algorithm that has been applied to many well-known problems and has shown better performance compared to the existing ones. So, we have applied CRO algorithm to solve the PSP problem. The four operators of CRO: on-wall ineffective collision, decomposition, inter-molecular ineffective collision, and synthesis have been designed to solve PSP problem. We have also designed a repair mechanism to get the correct structure. The experiment results show that CRO performs well in the case of PSP.
[Algorithm design and analysis, synthesis, CRO, repair, hydrophobic-polar model, Lattices, Amino acids, 3D structure, decomposition, CRO algorithm, Genetic algorithms, Proteins, protein structure, Three-dimensional displays, amino acid sequence, proteins, inter-molecular ineffective collision, biochemistry, on-wall ineffective collision, Prediction algorithms, protein structure prediction, chemical reaction optimization]
A novel comparative study between dual population genetic algorithm and artificial bee colony algorithm for function optimization
2016 19th International Conference on Computer and Information Technology
None
2016
This paper conducts a comparative study between an improved variants of genetic algorithm (GA) and a swarm intelligence algorithm (SIA), which are the Dual population Genetic Algorithm (DPGA) and Artificial Bee Colony (ABC) Algorithm. DPGA is a multi-population genetic algorithm (MPGA) that implements two population such as the main population and a complementary population. Since the added population has a totally different fitness function, it is preserved to supply sufficient diversity to the main population by crossover operation. However DPGA employs a dynamic strategy to maintain an appropriate distance between two populations for maintaining diversity. On the contrary ABC, a simple but exceptional derivative of SIA, applies division of labor in single population of artificial bees and allocates some of them to exploration while the others to exploitation. DPGA has its own techniques to sustain the significant balance between exploration vs. exploitation. Thus many such analytical comparisons between DPGA and ABC are the center of attention of this paper. Experiments are conducted on seven benchmark functions using ABC and results are compared with DPGA. The results demonstrate that DPGA performs well for some of the functions but by considering the result of mean absolute error, ABC performs far better than DPGA.
[Computers, artificial bee colony algorithm, crossover operation, dual population genetic algorithm, DPGA, Genetic Algorithm, artificial bee population, Optimization, Genetic algorithms, Convergence, Sociology, MPGA, ABC algorithm, ant colony optimisation, mean absolute error, Artificial Bee Colony Algorithm, exploration, Dual Population Genetic Algorithm, Swarm Intelligence Algorithm, SIA, genetic algorithms, function optimization, Statistics, Information technology, swarm intelligence algorithm, multipopulation genetic algorithm, swarm intelligence]
A modified anti-magic graph labeling for middle graph and total graph related to path
2016 19th International Conference on Computer and Information Technology
None
2016
Graph labeling is the assignment of integers to edges or vertices or both depending on some factors which is consequential in the field of graph theory. In this paper, anti-magic graph labeling of two graphs, middle graph and total graph of path graph have been mainly focused. Modified proofs of theorems on anti-magic graph labeling have been introduced here. Both of the theorems are applicable for strong anti-magic graph. The correctness of the proofs has been demonstrated through various examples.
[Computers, weak anti-magic graph, graph theory, modified theorem proofs, Wheels, Switches, path graph, Information technology, middle graph, Computer science, path, Bibliographies, modified antimagic graph labeling, Anti-magic graph, anti-magic graph labeling, strong anti-magic graph, total graph, theorem proving, Labeling]
Error reduction in arsenic detection through color spectrum analysis
2016 19th International Conference on Computer and Information Technology
None
2016
Groundwater contamination by Arsenic is a huge problem in many countries. In a developing country like Bangladesh, with widespread Arsenic contamination and lack of laboratory facilities, usually field detection kits are preferred to detect arsenic in tubewells. However, these kits produce a significant number of false positives/negatives due to human errors in matching the detection test-strip colors to the reference color chart. This paper introduces digital image processing methods and a smartphone application, which allow fast and inexpensive improvement in the test-strip classification of field detection kits. A smartphone captures a photo of the test strip used in the field detection kit, while the application detects the Arsenic level by comparison with reference colors. This automation reduces human errors while matching the colors using the eyes only, by adding an extra layer of cross-checking. Thus, the overall accuracy of the Arsenic detection process is improved.
[Computers, Strips, digital image processing, smartphone, Arsenic, image classification, test-strip color detection, tube-well (TW), Arsenic (As), error reduction, As, Image color analysis, arsenic contamination, arsenic detection, image colour analysis, test-strip classification, image processing, detection, geophysical image processing, groundwater, smart phones, image capture, arsenic, Information technology, image matching, Contamination, contamination, field detection kits, water quality, laboratory facilities, smartphone application, color difference, Water pollution, hydrological techniques, color spectrum analysis, groundwater contamination, color matching]
A fast farthest neighbor search algorithm for very high dimensional data
2016 19th International Conference on Computer and Information Technology
None
2016
Computing exact nearest and farthest neighbor is a challenging task, especially in the case of high-dimensional data. Several algorithms have been proposed to tackle the nearest neighbor problem. However, not much emphasis has been given on farthest neighbor problem. Most of the previous methods addressing this problem are not efficient when applied to high-dimensional data. In this paper, we present an algorithm for exact (not approximate) farthest neighbor computation for very high dimensional data (e.g., images). Our proposed farthest neighbor algorithm is motivated by the state-of-the-art exact nearest neighbor algorithm. We consider the problem of image classification in computer vision to evaluate the performance of our proposed algorithm. For this purpose, we use CIFAR-10 dataset which includes more than 50,000 images from 10 different object categories. Experimental results show that our farthest neighbor algorithm not only achieves high accuracy but also is very fast.
[Computers, CIFAR-10 dataset, image classification, neighbor search algorithm, high-dimensional data, Data structures, Complexity theory, farthest neighbor algorithm, Nearest neighbor searches, Upper bound, Euclidean distance, computer vision, Approximation algorithms, nearest neighbor problem, exact nearest neighbor algorithm]
A novel approach to select most effective attributes for SVM algorithm
2016 19th International Conference on Computer and Information Technology
None
2016
Support Vector Machine (SVM) is one of the most popular machine learning algorithms for pattern recognition of a specific dataset. The percentage of accuracy from a defined SVM model greatly depends on the selection of appropriate attributes for SVM model. But the most effective attributes selection for SVM algorithm is one of the most difficult tasks for any kind of data classification. A mathematical model is proposed in this paper through which effectiveness of attributes for SVM model can be calculated. The validity of this SVM model is justified by comparing the effectiveness of a SVM model with the rate of pattern recognition for corresponding SVM model. Linear, Radial Basis Function (RBF), Polynomial Kernel SVM algorithms are used for pattern recognition. The percentage of accuracy of pattern recognition increases with the effectiveness of SVM model. The range of value of effectiveness of SVM model is 0 to &lt;;x. We have tested our proposed algorithm for noninvasive Brain Computer Interface (BCI) system. The proposed mathematical model is applicable for any other linear or nonlinear system.
[Frequency-domain analysis, brain-computer interfaces, Attributes, Polynomial Kernel, Electroencephalography, nonlinear system, data classification, RBF, radial basis function networks, Mathematical model, learning (artificial intelligence), machine learning algorithms, Kernel, pattern recognition, pattern classification, support vector machines, Linear Kernel, BCI, polynomial Kernel SVM algorithms, Pattern recognition, Support vector machines, radial basis function, support vector machine, noninvasive brain computer interface, Support Vector Machines (SVM), Noninvasive Brain Computer Interface (BCI), Brain modeling, Radial Basis Function (RBF) Kernel]
Word embedding with hellinger PCA to detect the sentiment of bengali text
2016 19th International Conference on Computer and Information Technology
None
2016
The sentiment of a sentence or a comment can be detected more accurately by applying Word Embeddings. This article presents the idea of word co-occurrence matrix and Skip-Gram to determine the actual contexts of the words, Hellinger PCA to determine the most similar words and generate a sliding window of most probable context words around each word. It is shown that, by applying Word Embeddings to classify the sentiment of a comment achieves higher accuracy with larger corpus. For our corpus of 2500 comments, the accuracy achieved is 70%, which is rapidly increasing with the size of the corpus.
[Computers, Sentiment analysis, text analysis, context words, Context Word, SSWE, WordNet, Word Embeddings, Skip-Gram, Maximum Entropy, word embedding, Word2Vec, CBOW, Sliding Window, SentiWordNet, Ngram, Context, CCA, pattern classification, word cooccurrence matrix, Co-occurrence Matrix, Blogs, sentiment analysis, sentiment classification, Hellinger PCA, phrase pattern, hellinger PCA, Information technology, Bilingual word embeddings, matrix algebra, HLBL, PCA, POS tagger, skip-gram, Syntactics, VerbNet, bengali text sentiment, principal component analysis, Principal component analysis]
CorpoMate: A framework for building linguistic corpora from the web
2016 19th International Conference on Computer and Information Technology
None
2016
A linguistic corpus is a collection of an ample number of text documents serving as a data source for sampling human language, usually in computational linguistics. Conventional methods of building such a corpus involve frequent human intervention and poses difficulties during reproduction. To address the issues, the paper introduces CorpoMate, an extensible framework with a pipeline-inspired and modular architecture for automating the creation of linguistic corpora, from web resources via crawling websites or parsing feeds. It performs the necessary pre-processing as well as related tasks according to programmable queues of standard or customized tasks with easily swappable tools and, can export aggregated data into widely-accepted formats. Results from experiments performed on text processing tools and performance tests on the asynchronous, rule-based web crawling system justify the importance of having swappable tools along with the feasibility of the architecture described in the paper.
[Computers, text analysis, Buildings, Pipelines, computational linguistics, linguistic corpora, text documents, Web site crawling, Twitter, data aggregation, Corpus, Pragmatics, text processing tools, NLP, Web scraping, Computer architecture, feed parsing, Linguistics, Internet, CorpoMate, Feeds, asynchronous rule-based Web crawling system, Web resources, aggregated data]
Pitch and formant estimation of bangla speech signal using autocorrelation, cepstrum and LPC algorithm
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper, we present comparative study of digital speech processing on Bangla speech signal. We represent oral characteristics of Bangla alphabet in terms of pitch and formant. We worked with both vowels and consonants to show their difference in practical use. We take oral speech signals as voice record and extract phonemes to analyze in both time and frequency domains. Both male and female voices are included in the analysis to find the effect of gender on Bangla voice speech. We verify our work by showing similarity of the results in time and frequency domains. We calculate first three formants of Bangla phonemes to show their relative impacts on Bangla speech. This extensive study of Bangla speech would provide listed information and data for further research to extract some other important features in this field.
[cepstrum analysis, Correlation, Frequency-domain analysis, Formant, cepstral analysis, speech processing, Bangla alphabet oral characteristics, consonants, Bangla phoneme formant, pitch estimation, oral speech signals, digital speech processing, phoneme extraction, vowels, time domains, natural language processing, Estimation, formant estimation, frequency domains, Bangla Speech, LPC algorithm, male voices, female voices, Bangla voice speech, Cepstrum, Pitch, Speech, Feature extraction, voice record, Speech processing, autocorrelation analysis, LPC, Bangla speech signal]
Bidirectional LSTMs &#x2014; CRFs networks for bangla POS tagging
2016 19th International Conference on Computer and Information Technology
None
2016
Part-of-speech (POS) information is one of the fundamental components in the natural language processing pipeline, which helps in extracting higher-level information such as named entities, discourse, and syntactic structure of a sentence. For some languages, such as English, Dutch, and Chinese, it is considered as a solved problem due to the higher accuracy (97%) of the predicted system. Significant efforts have been made for such languages in terms of making the data publicly accessible and also organizing evaluation campaigns. Compared to that there are very fewer efforts for Bangla (ethnonym: Bangla; exonym: Bengali). In this paper, we present a knowledge poor approach for POS tagging, which we evaluated using publicly accessible dataset from LDC. The motivation of our approach is that we did not want to rely on any existing resources such as lexicon or named entity recognizer for designing the system as they are not publicly available and difficult to develop. We have not used any handcrafted features, rather we employed distributed representations of word and characters. We designed the system using Long Short Term Memory (LSTM) neural networks followed by Conditional Random Fields (CRFs) for designing the model with an inclusion of pre-trained word embedded model. We obtained promising results with an accuracy of 86.0%.
[pre-trained word embedded model, Machine learning algorithms, conditional random fields, POS information, natural language processing pipeline, named entities, Training, characters, speech processing, Natural language processing, long short term memory neural networks, LSTM neural networks, part-of-speech information, lexicon, sentence, natural language processing, POS tagging, bidirectional LSTM-CRF networks, syntactic structure, Bangla, Deep Learning, English, Bengali, Bangla POS tagging, Neural networks, Hidden Markov models, Tagging, Logic gates, Chinese, named entity recognizer, Dutch, neural nets]
Supervised approach of sentimentality extraction from bengali facebook status
2016 19th International Conference on Computer and Information Technology
None
2016
Sentiment is the only things that separate human and machine. To simulate the feelings for machines many researchers have been trying to create method and automated the process to extract opinion of particular news, product or life entity. Sentiment Analysis (SA) is a combination of opinions, emotions and subjectivity of a text. Currently SA is the most demanding task in Natural Language Processing. Social networking site like Facebook are mostly used in expressing the opinions about a particular entity of life. Newspaper published news about a particular event and user expressed their feedback in news comments. Online product feedback is increasing day by day. So reviews and opinions mining play a very important role in understanding people satisfactions. Such opinion mining has potential for knowledge discovery. The main target of SA is to find opinions from text extract sentiments from them and define their polarity, i.e positive or negative. In this domain most of the model was designed for English Language. This paper describes a novel approach using Naive Bayes classification model for Bengali Language. Here a supervised classification method is used with language rules for detecting sentiment for Bengali Facebook Status.
[Computers, opinion mining, supervised classification method, Sentiment analysis, social networking site, text opinions, English language, Conferences, data mining, knowledge discovery, Na&#x00EF;ve Bayes Rules, positive polarity text, Bengali facebook status, SA, online product feedback, text emotions, nai&#x0308;ve Bayes classification model, people satisfactions, learning (artificial intelligence), Facebook, text subjectivity, Stemming, pattern classification, natural language processing, sentiment analysis, Information technology, Pragmatics, sentimentality extraction, Parts of Speech (POS) Tagger, news comments, Antonyms word, social networking (online), language rules, Bayes methods, user feedback, N-gram model, negative polarity text, review mining]
An ensemble approach to detect review spam using hybrid machine learning technique
2016 19th International Conference on Computer and Information Technology
None
2016
Online reviews are becoming one of the vital components of e-commerce in recent years as so many people consider having different opinions prior to buying online products or apprehending any online service. Nowadays, in the era of web 2.0, it is completely understandable that people rely on online reviews more than ever while taking a decision. However, guaranteeing the authenticity of these sensitive and valuable information is hardly visible. Due to fulfill some immoral benefits, many people post fake review or fabricated opinion to uphold or devalue a certain product or service which certainly hampers the ingenuousness of the real fact. To detect fake reviews, many methodologies were introduced by harvesting the obvious content features, rating consistency, empirical conditions, helpfulness voting etc. The most of them are supervised models which mostly rely on pseudo fake reviews and the scarcity of good quality large-scale labeled dataset is still a hindrance. In this paper, we introduce an ensemble learning approach which combines two different types of learning methods (active and supervised) by creating a hybrid dataset of both real-life and pseudo reviews. This model holds 3 different filtering phases that is based on KL and JS distance, TF-IDF features and n-gram features of the review content. It achieves phenomenal results while working on almost 3600 reviews from different domains. In the best case, the precision, recall and f-score are above 95% and the accuracy it achieved is slightly above 88%. In the process, about 2000 reviews were manually labeled. After evaluating and comparing the results with other successful methods, it is quite clear that this detecting method is efficient and very promising.
[Computers, information authenticity, Web 2.0, online service, JS distance, unsolicited e-mail, information filtering, Fake Review, supervised learning methods, TF-IDF features, Training, content features, feature extraction, content review, KL distance, e-commerce, online products, ensemble learning approach, learning (artificial intelligence), Spam Detection, feature selection, active learning methods, Spam Review, filtering phases, Review spam detection, Information technology, content-based retrieval, hybrid machine learning technique, Support vector machines, online reviews, Machine learning, Writing, n-gram features, review spam detection, Data models, Internet]
Neighbourhood consistency based deep domain adaption analysis for multi category object detection
2016 19th International Conference on Computer and Information Technology
None
2016
Pattern classification in domains that follow dissimilar distribution and where target domain has insufficient labelled samples, requires transfer of knowledge across domains through a process called domain adaption. Deep learning research demonstrates the transferability of deep convolutional features that are activations of intermediate layers of convolutional neural networks for domain adaption. Traditional clustering based domain adaption approaches are practical to handle knowledge transfer scenario. This paper presents a scheme that uses local neighborhoods based consistency analysis of one supervised and another unsupervised model to effectively transfer knowledge using deep features. Contrasting conventional models this approach uses only two models to classify patterns except hard ones. Neighbourhood consistency analysis identifies the hard samples, and is classified using a third model. Experimental analysis has been carried out focusing change on category variation of different samples for train and test cases. The proposed approach yields encouraging experimental result on benchmark domain adaption dataset compared to a deep feature based single support vector machine classifier in terms of state of the art metrics demonstrating effective generalization of source domain information.
[Adaptation models, pattern classification, support vector machines, source domain information, neighbourhood consistency based deep domain adaption analysis, multicategory object detection, object detection, insufficient labelled samples, DeCAF, SVM, Clustering, Knowledge transfer, Deep Learning, Manifolds, Training, Support vector machines, knowledge transfer scenario, Analytical models, support vector machine classification, Machine learning, clustering based domain adaption, Domain Adaption, deep learning research]
User authentication based on mouse movement data using normalized features
2016 19th International Conference on Computer and Information Technology
None
2016
This paper presents a user authentication system based on mouse movement data. An available logging tool named Recording User Input (RUI) is used to collect three types of mouse actions - Mouse Move, Point-and-Click on Left or Right mouse button and Drag-and-Drop. Collected data are divided into N-number of blocks consisting of specific number of actions. From each block seventy four features are extracted to form feature vectors where number of new features is forty eight. Two types of classifiers are used to identify the user: Support Vector Machine (SVM) and Artificial Neural Network (ANN) are used to identify the user. A benchmark data is used to train and test the system. Experimental result shows that for both classifiers system with proposed features perform better. The results also show that Support Vector Machine outperforms the Artificial Neural Network Classifier.
[Computers, Biometric, Biometrics (access control), point-and-click, ANN classifiers, Support Vector Machine, logging tool, drag-and-drop, support vector machine classifiers, Training, feature extraction, mouse dynamics, authorisation, recording user input, left mouse button, user authentication, Testing, pattern classification, support vector machines, right mouse button, RUI, user identification, cyber behavioral biometrics, artificial neural network classifiers, Support vector machines, SVM classifiers, normalized features, mouse controllers (computers), Artificial Neural Network, user authentication system, feature vectors, Feature extraction, Mice, mouse actions, mouse move, neural nets, mouse movement data]
Accuracy analysis of recommendation system using singular value decomposition
2016 19th International Conference on Computer and Information Technology
None
2016
Recommendation systems use utility matrix to represent the user ratings for a particular items. But that matrix is sparse, that is, most of the user ratings are unknown. Predicting those unknown ratings is a big challenge of recommendation data mining task. Due to the sparse of data in utility matrix, few features become less important. Those features should be reduced to decline the computational complexity. Singular Value Decomposition (SVD) is a most powerful algorithm to predict unknown ratings by reducing the less significant features. Before applying SVD on utility matrix, all unknown ratings should be filled with some initial values. This paper focuses to generate two predictive matrixes by assigning two different initial values, where one is Zero and other is replacing unknown values with average item rating and then subtracting corresponding average user rating from the values. The accuracy of forecasted ratings has been justified over a sample dataset in this paper as well.
[SVD, Symmetric matrices, data mining, user ratings, recommendation data mining task, predictive matrixes, Matrix decomposition, Sparse matrices, utility matrix, Recommendation, Singular Value Decomposition (SVD), recommender systems, Dimensionality Reduction, recommendation system accuracy analysis, Prediction algorithms, Eigenvalues and eigenfunctions, Matrix converters, singular value decomposition, Singular value decomposition, computational complexity]
Performance analysis of supervised machine learning algorithms for text classification
2016 19th International Conference on Computer and Information Technology
None
2016
The demand of text classification is growing significantly in web searching, data mining, web ranking, recommendation systems and so many other fields of information and technology. This paper illustrates the text classification process on different dataset using some standard supervised machine learning techniques. Text documents can be classified through various kinds of classifiers. Labeled text documents are used to classify the text in supervised classifications. This paper applied these classifiers on different kinds of labeled documents and measures the accuracy of the classifiers. An Artificial Neural Network (ANN) model using Back Propagation Network (BPN) is used with several other models to create an independent platform for labeled and supervised text classification process. An existing benchmark approach is used to analysis the performance of classification using labeled documents. Experimental analysis on real data reveals which model works well in terms of classification accuracy.
[Measurement, supervised machine learning, text analysis, artificial neural network, ANN, BPN, data mining, text classification, Training, Static VAr compensators, backpropagation network, Web searching, query formulation, Web ranking, Artificial neural networks, labeled documents, classification, machine learning, Support vector machines, recommendation systems, recommender systems, back propagation network, Text categorization, backpropagation, Internet, neural nets, performance analysis, Logistics]
Crimecast: A crime prediction and strategy direction service
2016 19th International Conference on Computer and Information Technology
None
2016
Various researches on criminology provides us with a key piece of information about criminal psychology that, a criminal doesn't hover around unknown territory rather they commit crimes when opportunity provides in a concentrated or familiar area i.e. hotspots. So, a crime predicting model can be simulated using crime pattern theory which can analyze verified past crime data and predict future criminal activities. The aim of this paper is to introduce CRIMECAST which is a crime prediction and strategy direction service which attempts to predict probable future crimes by simulating probabilistic model implementation and Artificial Neural Network. CRIMECAST is a spatial crime analysis process that focuses on authentic crime history and predicts crime, develops strategy map, provides security alert. Our simulation on very big dataset show that CRIEMECAST outperforms all other methods of making crime predictions.
[artificial neural network, ANN, Predictive models, hotspots, CRIMECAST, Analytical models, psychology, Mathematical model, authentic crime history, criminal psychology, crime pattern theory, spatial crime analysis process, Meteorology, probabilistic model implementation, Hotspot Detection, Computational modeling, Crime Prediction, criminology, probability, Artificial neural networks, Probability, security alert, Oracle DB, crime predicting model, criminal law, neural nets, crime prediction, Probabalistic Model, strategy direction service]
AN intelligent road traffic management system using NVIDIA GPU
2016 19th International Conference on Computer and Information Technology
None
2016
Road traffic congestion remains a global phenomenon that causes great problems in the cities of the world; especially developing countries, resulting in massive delay, unpredictable travel times, increased fuel consumption, man-hour and monetary loss. In order to get a better solution, one of the preposition is to divert traffic to less congested route. One of the solution of collecting road traffic condition is crowd sourcing. We proposed that the crowd sourcing information will change road network graph of the city according to sourcing result. Based on this updated graph, the driver in jam needs information that which path is suitable for him based on current updated position of vehicle to intended destination almost in real time. To get a near real time performance for large graph, we try to investigate the use of parallel point to point graph search algorithm. For this purpose we use CUDA enable GPU for parallel implementation of Dijsktra's algorithm. We tested our modified algorithm for New York, Rome and Dhaka city (partial). It is found that parallel implementation gives a better result for all pair search.
[Computers, Dijsktra algorithm, Roads, Graphics processing units, intelligent road traffic management system, Dhaka city, road traffic congestion, road vehicles, Rome city, NVIDIA GPU, Real-time systems, unpredictable travel times, road traffic, New York, fuel consumption, crowdsourcing, crowd sourcing, Urban areas, parallel implementation, traffic engineering computing, graphics processing units, Computer science, CUDA, Arrays, road traffic condition, road network graph]
Bengali word embeddings and it's application in solving document classification problem
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper, we present Bengali word embeddings and it's application in the classification of news documents. Word embeddings are multi-dimensional vectors that can be created by exploiting the linguistic context of the words in large corpus. To generate the embeddings, we collected Bengali news document of last five years from the major daily newspapers. Word embeddings are generated using the Neural Network based language processing model Word2vec. We use the vector representations of the Bengali words to cluster them using K-means algorithm. We show that those clusters can be used directly to perform various natural language processing task by solving the problem of Bengali news document classification. We use the Support Vector Machine (SVM) for the classification task and achieve ~91% F1-score. The accuracy of our method demonstrates that our word embeddings could capture the semantics of word from the respective context correctly.
[Bengali word linguistic context, computational linguistics, Bengali word vector representations, multidimensional vectors, K-means clustering algorithm, Classification algorithms, Word Embedding, Training, neural network based language processing model Word2vec, Clustering algorithms, Context, document handling, information resources, pattern classification, support vector machines, news document classification problem, Computational modeling, natural language processing, Bengali word embeddings, Support vector machines, Word2vec, Bengali, vectors, support vector machine, pattern clustering, Neural networks, Word Cluster, word semantics, neural nets, Document Classification]
Towards optimal convolutional neural network parameters for bengali handwritten numerals recognition
2016 19th International Conference on Computer and Information Technology
None
2016
This work attempts to find the most optimal setting for a convolutional neural network (CNN) for Bengali digit dataset classification. Recognition of handwritten Bengali numerals has recently gained much interest among researchers due to the significant performance gain found in the recognition of English numerals using neural network based architecture. In this work, a new dataset of 70,000 samples were created first by taking handwriting of 1750 persons where 982 persons were male and the rests were female. These individual image samples are then converted to grayscale, normalized, inverted and pickled to complete the data preprocessing step. Later this dataset was recognized using several convolutional neural network settings where the most optimal setting being found to be two convolution layer with Tanh activation, one hidden layer with Tanh activation and one output layer with softmax activation. The proposed optimal number of feature maps is (35, 45). The minimum validation error has been found to be 1.22% which is the current best result compared to all other methods in the literature.
[Bengali handwritten numerals recognition, CNN, Gray-scale, Complexity theory, handwriting recognition, Tanh activation, data preprocessing step, Bengali Numerals, Convolution, convolutional neural network settings, convolution layer, Handwritten Numeral Recognition, optimal convolutional neural network parameters, Convolutional Neural Network, handwritten character recognition, natural language processing, neural network based architecture, Artificial neural networks, handwriting, softmax activation, Feature Map, Biological neural networks, Bengali digit dataset classification, Handwriting recognition, English numerals, neural net architecture]
Stacked auto encoder training incorporating printed text data for handwritten bangla numeral recognition
2016 19th International Conference on Computer and Information Technology
None
2016
Recognition of handwritten numerals has gained much interest in recent years due to its various application potentials. Bangla is a major language in Indian subcontinent and is the first language of Bangladesh; but unfortunately, study regarding handwritten Bangla numeral recognition (HBNR) is very few with respect to other major languages such as English, Roman etc. Some noteworthy research works have been conducted for recognition of Bangla handwritten numeral using artificial neural network (ANN) as ANN and its various updated models are found to be efficient for classification task. The aim of this study is to develop a better HBNR system and hence investigated deep architecture of stacked auto encoder (SAE) incorporating printed text (SAEPT) method. SAE is a variant of neural networks (NNs) and is applied efficiently for hierarchical feature extraction from its input. The proposed SAEPT contains the encoding of handwritten numeral into printed form in the course of pre-training and finally initializing a multi-layer perceptron (MLP) using these pre-trained weights. Unlike other methods, it does not employ any feature extraction technique. Benchmark dataset with 22000 hand written numerals with different shapes, sizes and variations are used in this study. The proposed method is shown to outperform other prominent existing methods achieving satisfactory recognition accuracy.
[artificial neural network, image classification, Deep Neural Networks, Image reconstruction, HBNR system, Training, printed text, feature extraction, Computer architecture, multilayer perceptron, handwritten numeral encoding, printed text data, multilayer perceptrons, handwritten character recognition, handwritten Bangla numeral recognition, Artificial neural networks, stacked autoencoder training, classification task, SAEPT method, hierarchical feature extraction, Bangla Handwritten Numeral, Stacked Auto Encoder, Handwriting recognition, Bangladesh, Feature extraction, image coding]
Improved scene classification using region semantics and spatial context information
2016 19th International Conference on Computer and Information Technology
None
2016
In this paper, we present an improved scene classification algorithm to classify scene images using region semantics and spatial context information. Unlike existing solutions that rely on predefined prototypes and inflexible spatial context information, we propose using the region semantics dictionary to represent the scene conveniently, which can handle a variety of region semantics, and to explore more efficient spatial context information based on sparse representation characteristics. We first use statistical Gaussian mixing models (GMMs) to obtain region semantics and location information and effectively solve the parameters using an adaptive expectation-maximization (EM) algorithm, then establish a region semantic dictionary for scene interpretation. Moreover, we formulate the explored spatial context information as a convex optimization problem that can be solved using the inexact augmented Lagrange multiplier (IALM) method. Finally, we can predict the label information accurately according to Bayesian decision rule. Our extensive experiments have demonstrated that the improved algorithm can outperform several other previous methods on publicly available scene datasets; therefore, we also demonstrate and confirm the efficacy of handling errors due to variants of the local region semantics.
[Visualization, Adaptation models, natural scenes, Dictionaries, decision theory, image classification, location information, scene interpretation, inexact augmented Lagrange multiplier, adaptive expectation-maximization algorithm, Covariance matrices, Semantics, scene classification, Bayesian decision rule, local region semantics, region semantics, Context, GMM, region semantic dictionary, convex programming, spatial context information, sparse representation characteristics, Scene classification, Gaussian processes, image representation, expectation-maximisation algorithm, Feature extraction, convex optimization, statistical Gaussian mixing models, Bayes methods, IALM, mixture models, scene representation, error handling]
Effects of heuristics in path planning for mobile robots in uncertain and dynamic environments
2016 19th International Conference on Computer and Information Technology
None
2016
In this work, several heuristics are proposed to assess their effects in path planning for mobile robots in uncertain and dynamic environments. Path planning for mobile robots is a widely used technique where an optimal collision-free path is searched through an environment. It is one of the most important primitives in robotics and is considered to be a hard problem. Since environments may contain static as well as dynamic obstacles, the optimal path must be collision-free. The situation becomes complicated for mobile robots when an environment consists of dynamic obstacles with random velocities and arbitrary directions. Though there have been a lot of research in static environments, concentration given on uncertain and dynamic environments is a few. Our goal is to find a path in such uncertain and dynamic environments considering simulation time and travel-distance. In order to analyze the robustness of our proposed heuristics, we plugged each of these heuristics in several search algorithms. We also compared the performances of these algorithms. Finally, this paper analyzes the effects of the proposed heuristics in path planning in uncertain and dynamic environments which are usual real-word scenarios for mobile robots.
[uncertain and dynamic environments, collision avoidance, obstacle avoidance, Heuristic algorithms, dynamic obstacles, uncertain systems, optimal collision-free path, optimal control, search algorithms, mobile robot, heuristic, Path planning, mobile robots, path planning, Mobile robots, Vehicle dynamics, travel-distance, dynamic environments, static environments, Robot kinematics, simulation time, robust control, Planning, uncertain environments]
Traffic sign recognition using hybrid features descriptor and artificial neural network classifier
2016 19th International Conference on Computer and Information Technology
None
2016
Traffic Sign Recognition (TSR) system is a significant component of Intelligent Transport System (ITS) as traffic signs assist the drivers to drive more safely and efficiently. This paper represents a new approach for TSR system using hybrid features formed by two robust features descriptors, named Histogram Oriented Gradient(HOG) features and Speeded Up Robust Features(SURF) and artificial neural network (ANN) classifier. In the detection step, the region of interest (sign area) is segmented using color based thresholding algorithm, post processed to filter the unwanted region. Next robust features vector named Distance to Borders (DtBs) of the segmented blob is formed to verify the shape of the traffic sign. Finally the recognition of the traffic sign is implemented using ANN classifier upon the training of hybrid features descriptor. The proposed system simulated on offline road scene images shows a high classification rate in the recognition stage. The performance of the ANN model is illustrated in terms of cross entropy, confusion matrix and receiver operating characteristic (ROC) curves. In addition, the performance of hybrid feature descriptor is compared with recognition based on HOG and SURF descriptor respectively. Also, performances of some classifier such as Support Vector Machine (SVM), Decision Trees, Ensembles Learners (Adaboost) and K-Nearest Neighbor (KNN) classifier are assessed with ANN approach. The simulation results illustrates that recognition using hybrid feature descriptor outperforms in all classifier and the recognition accuracy of ANN is higher than classifier stated above.
[DtBs, hybrid features descriptor, distance to borders, Roads, image classification, Segmentation, ITS, Confusion matrix, learning (artificial intelligence), image filter, Cross Entropy, HOG features, road traffic, color based thresholding algorithm, support vector machines, receiver operating characteristic curves, Artificial neural networks, intelligent transportation systems, traffic engineering computing, k-nearest neighbor, road scene images, ensemble learning, Image segmentation, SURF, feature descriptors, Artificial Neural Network, Feature extraction, intelligent transport system, speeded up robust features, confusion matrix, Shape, KNN classifier, artificial neural network classifier, classification rate, SVM, ANN classifier, driver assistance, Image color analysis, driver information systems, feature extraction, image segmentation, Robustness, shape recognition, image colour analysis, Traffic Sign Recognition, traffic sign shape recognition, Adaboost, Histogram Oriented Gradient, Speeded Up Robust Feature, cross entropy, Receiver Operating characteristic Curve, histogram oriented gradient features, ROC curves, feature vector, TSR system, support vector machine, decision trees, Distance to Borders, traffic sign recognition, neural nets]
Mutual information-based selection of audiovisual affective features to predict instantaneous emotional state
2016 19th International Conference on Computer and Information Technology
None
2016
Automatic prediction of continuous level emotional state requires selection of suitable affective features to develop a regression system based on supervised machine learning. This paper investigates the performance of low-level dynamic features for predicting two common dimensions of emotional state, namely, valence and arousal instantaneously. Low-complexity features are extracted from audio and visual modalities independently and fused in the feature level. Features with minimum redundancy and maximum relevancy are chosen by using the mutual information-based selection process. The performance of frame-by-frame prediction of emotional state using the moderate length features as proposed in this paper is evaluated on spontaneous and naturalistic human-human conversation of SEMAINE database. Experimental results show that the proposed features selected by mutual information can be used for instantaneous prediction of emotional state with an accuracy higher than traditional audio or visual features that are used for affective computation.
[Computers, supervised machine learning, SEMAINE database, Visualization, feature level, Conferences, maximum relevancy, affective computing, regression system, regression analysis, mutual information-based selection, automatic prediction, Mel frequency cepstral coefficient, emotion recognition, audiovisual affective features, continuous level emotional state, low-level dynamic features, feature extraction, instantaneous emotional state, learning (artificial intelligence), feature selection, Emotion recognition, naturalistic human-human conversation, low-complexity feature extraction, frame-by-frame prediction, visual modalities, Feature extraction, Speech, affective computation, audio modalities]
Towards a sustainable cyber-physical energy system design
2016 19th International Conference on Computer and Information Technology
None
2016
The concept of energy sustainability has been recently introduced in complex cyber-physical systems innovation studies. Previous empirical studies have only focused on the coordination of systems in a large system, but less research has been done on energy efficiency and management towards sustainability. We argue that in terms of energy consumption, a system comprising variety of technologies and applications should be energy sustainable. Hence, we focus on a framework of energy demand in a complex-robotic environment. As a preliminary study, we propose a system dynamics modeling of the energy sustainable complex-structure. As the work is ongoing, some initial simulation results are shown here. Based on our initial studies, we evaluate a number of implications of the management of energy in large-scale cyber-physical systems.
[Computers, Energy consumption, Energy Sustainability, Cyber-physical systems, Smart Assistive Care, Complex Cyber-Physical System, Optimization, complex-robotic environment, cyber-physical systems, sustainable cyber-physical energy system design, power aware computing, energy management systems, energy demand framework, energy efficiency, energy conservation, energy management, Energy efficiency, robots, Mathematical model, energy consumption, Robots]
ALW drone: A new design and efficient approach
2016 19th International Conference on Computer and Information Technology
None
2016
The purpose of the research is to make a unique drone which is capable to move autonomously through any environment by interacting with server by itself. The main uniqueness is that it can move through air, land and water. Nowadays various drones are seem to be available. But such a drone which can move through every environment is never seen before. The main motive is to get accuracy in rescue missions in the cases of environmental calamities and make it easier than before. Besides that this drone also use the environmental data and bio information as a method of communication with the environment. Environmental learning can be used to communicate with the nature and gather bio-information from the environment. For communication with the environment and learning about the environment this drone uses two kinds of database: temporary database and online database. This paper proposes a unique and modest algorithm which reduces the complexity of interaction with database and optimizes the relationship between drone's AI and database. By gathering bio-information and environmental data the proposed ALW (Air, Land, and Water) drone is able to communicate with the environment and adapt the changes in the environment.
[environmental learning, Heuristic algorithms, mobile robots, Servers, ALW drone, drone AI, database, real time communication, Databases, air-land-and-water drone, Hardware, learning (artificial intelligence), bio-information gathering, algorithm, online database, Propellers, environmental calamities, AI, environmental data gathering, communication method, drone, bio-information, environment, drone database, rescue missions, interaction, temporary database, Artificial intelligence, Drones]
Design &amp; implementation of a line following robot for irrigation based application
2016 19th International Conference on Computer and Information Technology
None
2016
The agriculture sector is the backbone of an economy which provides the basic ingredients to mankind and raw materials for industrialization. With the increasing number of the population over the world, the demand for agricultural products is also increased. In order to increase the production rate, irrigation technique should be more efficient. The irrigation techniques used till date are not in satisfactory level, especially in a developing country like Bangladesh. This paper has proposed a line follower robot for irrigation based application which may be considered as a cost-effective solution by minimizing water loss as well as an efficient system for irrigation purposes. This proposed system does not require an operator to accomplish its task. This gardening robot is completely portable and is equipped with a microcontroller, an on-board water reservoir, and an attached water pump. The area to be watered by the robot can be any field with plants, placed in a predefined path. It is capable of comparing movable objects and stationary plants to minimize water loss and finally watering them autonomously without any human intervention. The designed robot was tested and it performed nicely.
[stationary plants, Irrigation, agricultural products, Sensor systems, Robotics, irrigation, water loss minimization, industrial robots, Robot sensing systems, agriculture sector, Obstacle Avoidance, microcontrollers, Microcontroller, production rate, line following robot design, movable objects, Wireless sensor networks, microcontroller, service robots, Automated Irrigation System, gardening robot, Pins, irrigation based application, water pump, line following robot implementation]
Design and development of a DTMF controlled room cleaner robot with two path-following method
2016 19th International Conference on Computer and Information Technology
None
2016
Automatic control system for robotics is getting popular day by day because of its incredible reliability and especially for the time optimization capability with performance. In this paper, the implementation of a room cleaning robot is presented which has been developed using the Arduino Uno platform with control ability by cell phone from any distance using DTMF technology. It is an electronic device with self-controlled obstacle avoidance capability along with a waste and dust cleaning system. Generally, a vacuum cleaner is an effective technology for household cleaning purpose, but it requires to be operated manually by any person. The main purpose of developing a DTMF-controlled room cleaner robot is to implement an automatic cleaning system which can be operated from any distance by using the cell phone. The robot has been built on microcontroller platform and utilized DTMF technology to develop remote controlling operation. Two effective smart paths following methods have been utilized by the robot for cleaning operation along with upgraded vacuum system with necessary filter and air suction system.
[collision avoidance, telerobotics, upgraded vacuum system, vacuum cleaner, self- controlled obstacle avoidance capability, waste and dust cleaning system, cell phone, Relays, control ability, remote controlling operation, electronic device, automatic cleaning system, Robots, microcontrollers, Path-Following Method, Room Cleaning Robot, Obstacle avoidance, household cleaning purpose, automatic control system, DC motors, DTMF controlled room cleaner robot design, Cleaning, filter system, DTMF technology, air suction system, time optimization capability, home automation, smart path-following method, Vacuum systems, microcontroller, DTMF remote control, Arduino Uno platform, Pins, DTMF controlled room cleaner robot development, mobile handsets]
Trophallaxis and energy optimization in swarms of robots
2016 19th International Conference on Computer and Information Technology
None
2016
Ability to allocate task on the fly is considered to be one of the most desirable features in a swarm intelligent system. This paper presents a computational model in which swarms of autonomous agents (robots) carry out the task of cleaning the environment by collecting boxes from the environment and dumping them in a dump area. As agents work, they lose energy and when the energy is too low they need to go to the charging area to gain energy. Our model is inspired by how social insects and in particular how ants behave. Experimental results show that incorporating trophallactic behavior in swarms of robots improve the performance of the swarm in terms of the energy consumption over earlier strategies. The proposed model is found to be efficient, accurate and consistent with the biological equivalents.
[Computers, swarm intelligent system, Energy consumption, dump area, Computational modeling, Switches, robot swarms, multi-robot systems, social insects, mobile robots, task allocation, optimisation, trophallactic behavior, energy optimization, Robot kinematics, trophallaxis, Resource management, biological equivalents, Collision avoidance, energy consumption, autonomous agents, swarm intelligence]
A support vector machine approach for real time vision based human robot interaction
2016 19th International Conference on Computer and Information Technology
None
2016
Today humanoid robots are being exhibited to redact various task as a personal assistant of a human. To be an assistant, a robot needs to interact with human as a human. For this reason robot needs to understand the human gender, facial expression, facial gesture in real time. Ribo - A humanoid robot build in RoboSUST lab which has the ability to communicate in Bangla with the people speaking in Bengali. In this article the authors show the implementation of theoretical knowledge of the recognition of real time facial expression, detection of human gender and yes / no from facial gesture in Ribo. Real time facial expression and gender detection can be performed using Support Vector Machine (SVM). A prepared dataset containing the facial landmarks leveled as five different expression: sad, angry, smile, surprise and normal, is given to SVM to construct a classifier. For the prediction of any expression, facial images are taken in real time and provided the facial landmarks data to SVM. Local Binary Pattern(LBP) algorithm is used for extracting features from face images. These features leveled as male and female are responsible to build the classifier. The face gesture for detecting `yes/no' is performed by tracking the movement of face in a certain time. After those implementations the principal results will make a framework that will be used in Ribo to recognize human facial expression, facial gesture movement and detect human gender.
[Computers, facial expression, Ribo, RoboSUST lab, Real time facial expression, SVM, Human robot interaction, Machine Learning, gesture recognition, facial gesture, face recognition, speech processing, Real-time systems, humanoid robots, LBP, Face, human-robot interaction, support vector machines, Face recognition, human gender, humanoid robot, gender detection, Bangla, robot vision, real time vision based human robot interaction, Face detection, support vector machine approach, Support vector machines, Bengali, local binary pattern algorithm, personal assistant, face images, Landmarks, Facial gesture]
Factors affecting employees' behavioral intention to adopt accounting information system (AIS) in Bangladesh
2016 19th International Conference on Computer and Information Technology
None
2016
Accounting Information System (AIS) is a platform in which the core concepts of accounting, such as, recording, classifying, storing, safeguarding and interpreting data and reporting accounting information are performed by using various standardized and customized software for better financial control. A wide range of accounting software, such as, Tally, Troyee, AccPac, Oracle Applications, SAP, etc. are already being installed by various organizations in Bangladesh. Through this study, we wanted to know the determinants of employees' acceptance of various AIS in Bangladesh, by applying the Technology Acceptance Model (TAM). A total number of 82 employees from four different companies were provided with the structured questionnaires. Finally, 80 questionnaires were accepted and encoded. The results of the study are based on factors, such as, PU, PEOU, ATU, BIU, andPBC as suggested by Davis in his model known as TAM. All variables were analyzed by using SMART PLS, which is a full-flagged Structural Equation Modeling (SEM) tool. The result shows that Perceived Behavioral Control (PBC) significantly affects employees' behavioral intention to use AIS through mediated effect on PU and PEOU. Hence, PBC is acting as a strong antecedent of the latent variables PU and PEOU in Bangladesh.
[TAM, structured questionnaires, PU, Structural Equation Modeling (SEM), accounting, Information systems, PBC, employee acceptance determinants, Mathematical model, technology acceptance model, SMART PLS, Human Interaction, Computational modeling, full-flagged structural equation modeling tool, employee behavioral intention affecting factors, accounting information system, AIS, Information technology, perceived behavioral control, accounting software, Bangladesh, Behavioral Intention, Organizations, Software, SEM, Accounting Information System, behavioural sciences computing, statistical analysis, Technology Acceptance Model (TAM), Artificial intelligence, PEOU]
Policy issues: Allowing mobile operators to provide specialized e-services in Bangladesh
2016 19th International Conference on Computer and Information Technology
None
2016
E-Service in Bangladesh has become a catalytic agent for empowering additional effective government services. This article investigates issue covering clues on IT policy about the implementation of marketable commercial services as mobile e-services under the boundary expansion issue of large organization like Mobile Network Operator. In Bangladesh, Specialized services are provided by different vendors, where the service quality is always differed, because there is no integration of such collaboration. Operators can play a vital role by enabling the service through Value Added Service (VAS). Users and providers can find solutions that can help to increase penetration of this untouched sector. In this article, we organize some issues, provisioning approximately resilient guideline to the operators, so that operators cannot play monopoly and ensure a healthy competition. It concentrate on the effects of within and between standards race and competition between operators on performance developing and adopting new VAS services like e-health, as swelling of penetration of mobile services. Our empirical exploration provides an indication of introduction of specialized VAS service over Mobile Network, so users of the core services will be migrated to new data services. Finally, the control of the service level agreement of e-services with fair practice of competition among operators to positively change the curve of specialized service penetration is investigated.
[Industries, e-health, Policy, Mobile Network Operator (MNO), Licenses, Mobile communication, mobile operators, VAS, value added service, Convergence, mobile computing, telecommunication networks, Value Added Service (VAS), catalytic agent, IT policy, government policies, marketable commercial services, E-service, Telecommunications, policy issues, e-services, Bangladesh, government services, mobile network operator, service industries, Mobile computing]
Understanding customers' intention to use e-commerce in Bangladesh: An application of the technology acceptance model (TAM)
2016 19th International Conference on Computer and Information Technology
None
2016
Application of e-commerce has become an integral part of marketing due to an increasing diffusion of information and communication technology (ICT) throughout the world including Bangladesh. Due to its growing significance to the economy, it is critical to identify factors affecting customers' intention to use e-commerce in Bangladesh. This study used Technology Acceptance Model (TAM) to determine key factors influencing customers' intention to use e-commerce system. A structured questionnaire was used to collect data from 110 participants in Bangladesh. The collected data were analyzed using Partial Least Squares (PLS) methods, a statistical analysis technique based on the Structural Equation Modeling (SEM). The study found a significant positive effect of perceived usefulness and perceived ease of use on the behavioral intention of using e-commerce. However, the study found a negative effect of computer attitudes towards the customers' intention to use e-commerce system. The findings of this study have significant implications towards the intention of the users to use e-commerce in Bangladesh.
[Computers, least mean squares methods, TAM, behavioral intention, consumer behaviour, Electronic mail, statistical analysis technique, information and communication technology, e-commerce system, e-commerce, technology acceptance model, Business, electronic commerce, data analysis, Computational modeling, customers intention, structural equation modeling, PLS methods, marketing, Bangladesh, partial least squares methods, SEM, Information and communication technology, ICT, Reliability, statistical analysis]
User-modeling and recommendation based on mouse-tracking for e-commerce websites
2016 19th International Conference on Computer and Information Technology
None
2016
Emerging trend of recent online marketing has lead researchers to discover a way for the e-commerce websites to support its large number of users in the best possible manner. Meeting up with user's satisfaction is a difficult task. This paper focuses on how user-modeling and recommendation by mouse-tracking can be used for construction of an intelligent e-commerce website that can identify and fulfill all the needs of users.
[Computers, Tracking, Computational modeling, User-modeling, Modeling-algorithm (MA), HCI, recommender systems, online marketing, Mouse-tracking, User interest, SERP, mouse-tracking, Mice, Internet, user-modeling, Web sites, Usability, intelligent e-commerce Website, Business, electronic commerce, user satisfaction, Recommendation-algorithm (RA)]
A tale of institutional education in Bangladesh: Students' perspective
2016 19th International Conference on Computer and Information Technology
None
2016
Education, i.e., the backbone of a nation, encompasses a variety of role players and stakeholders among which students are, perhaps, the most important one. However, analyzing students' feedback on institutional education system from a macro level is yet to be done in the literature. To address this issue, in this paper, we conduct a study on different perspectives of institutional education based on students' feedback. Here, we mostly focus on the institutional education in Bangladesh. Our study is based on the data collected from the students through both on-line and offline surveys. We analyze the collected data to dig out various key aspects such as appropriateness of educational contents, relationship between teachers and students, and extents of malpractice in the institutional education. Additionally, we attempt for identifying different personalities who exhibit significant influence on the students. Our study reveals a number of key findings that may facilitate effective reformation and enhancement of the current educational system under focus.
[Computers, institutional education, student perspective, Bangladesh, Demography, Radiation detectors, educational contents, Education, Market research, educational institutions, Stakeholders, Information technology]
An improved and interactive system to fulfill basic human healthcare needs
2016 19th International Conference on Computer and Information Technology
None
2016
Adequate healthcare has always been a topic of concern in Bangladesh. As the population of Bangladesh is increasing exponentially, it has become impossible to deliver proper healthcare to all of its residence. For that reason alternate resources for healthcare have become almost a necessity today. Modern world is driven by technology and electronic healthcare system can be the solution we are looking for. With that in mind we took a step to merge a part of healthcare system with internet to ensure better healthcare for the inhabitants of Bangladesh.
[Drugs, Computers, electronic healthcare system, Google, approval system, basic human healthcare needs, blood bank, doctor search, healthcare, Bangladesh, Hospitals, Databases, interactive system, interactive systems, doctors location, Internet, health care, review system]
Know your customer (KYC) based authentication method for financial services through the internet
2016 19th International Conference on Computer and Information Technology
None
2016
Financial services through the internet are running under various threats like phishing, pharming (cyber attack intended to redirect a website's traffic to another fake site), malware, and evolving sophistication of compromise techniques. Multi-factor authentication (MFA) financial service system alleviates the risk and makes it secure. Various methods of MFA run in troubles like the authentication device lost or stolen, false sense of security (if used on login device), compromised answer of the generic question, higher implementation costs, unambiguous movement profile, calculated hash value stolen (there is no chance to replace it), etc. Compliance with Anti-Money Laundering (AML), Know Your Customer (KYC) and sanction requirements continue to be a key focus area for Financial Institutions' (FIs) management; firms must ensure that they are following appropriate compliance procedures to meet the increasing regulatory demands [1,2]. By addressing existing limitation of MFA, this paper proposes dynamic KYC based MFA authentication method to ensure the secured access of the financial services through the internet. Analysis and simulation results show that the proposed method provides control same as existing MFA/2FA techniques, but customers will get relief from holding additional security devices that incur huge cost.
[Computers, web security, invasive software, sanction requirements, antimoney laundering, online banking authentication, Web site traffic, FI management, MFA/2FA techniques, financial services, customer relationship management, Know Your Customer (KYC), strong authentication, Uniform resource locators, fake site, authentication device lost, financial institutions management, security devices, multi-factor authentication (MFA), multifactor authentication, financial data processing, calculated hash value stolen, AML, phishing, Navigation, malware, compliance procedures, MFA financial service system, KYC based authentication method, unambiguous movement profile, internet banking, know your customer based authentication method, pharming, Authentication, Hidden Markov models, dynamic KYC based MFA authentication method, regulatory demands, Internet, cyber attack]
A multilingual ontology based framework for wikipedia entry augmentation
2016 19th International Conference on Computer and Information Technology
None
2016
The domain of traditional web is gradually evolving with the adaptation of newer techniques, which includes semantic web. Integration of web content using ontologies in a language independent manner is a required feature in this process. For better utilization of the resources, it is necessary that the ontology, which is working as a central knowledge repository, to be language independent as well. This paper will propose a framework that will augment the domain of multilingual ontology and provide a universal technique to integrate infinite number of languages to the understanding of the multilingual ontology. Augmenting the Wikipedia entries in Bengali language using the English language entries has been used as an example to demonstrate the validity of the proposed process.
[multilingual ontology, Electronic publishing, central knowledge repository, Multilingual ontology, English language, natural language processing, Wikipedia entry augmentation, Encyclopedias, Ontologies, DBpedia, World Wide Web, Wikipedia, Data mining, Bengali language, Ontology mapping, Feature extraction, ontologies (artificial intelligence), Internet, Web sites, language independent manner, Web content integration]
Localizing pregnant women and newborns in rural areas and bridging health care gap
2016 19th International Conference on Computer and Information Technology
None
2016
The motive of the proposed web based/mobile application is to connect all expectant mothers and their newborns under one umbrella by keeping track of their weekly/monthly health records, health issues while also localizing them to make any health-driven actions faster and effective in rural areas. Technology has yet to have a firm grasp on the locals in rural areas, unlike the situation in the urban areas, where technology has advanced significantly, yet the bridge between rural and urban health development remains less substantial. The proposed solution can provide concerned individuals with different sets of data, involving health and living conditions, which would help deploy health care related solutions in rural areas with more finesse and efficiency. This solution would promote the people to avail the technological health care from the valid sources around their zone as it connects the user of the applications to nearby help sources in the form of hospitals and NGOs.
[Pediatrics, hospitals, History, Rural Area, Newborns, health issues, mobile computing, Android Application, monthly health records, Web based application, NGOs, health conditions, rural areas, health care, newborn localization, mobile application, smart phones, electronic health records, expectant mothers, Pregnancy, pregnant women localization, Hospitals, Expectant, living conditions, Web Application, Healthcare, Software, health-driven actions, Smart phones]
Fatigue testing of MEMS device developed by MetalMUMPs fabrication process
2016 19th International Conference on Computer and Information Technology
None
2016
A very important aspect to fabricate MEMS devices, is understanding of the mechanical properties at micro-scale level. The mechanical properties of components not only depend upon the material but also on the micro-structure. Unlike other devices, MEMS components are developed from specialized micro fabrication processes in which they are subjected to cyclic loads thus producing intrinsic stresses. Loading is also produced during actuation of these devices that leads to the development of fatigue. Fatigue phenomenon is of immense importance while deliberating upon reliability of MEMS devices. In this paper, a test specimen has been tested for fatigue testing for assessment of the yield strength. Design rules of MetalMUMPs process have been followed while designing the test device. The tests were performed in FEM analysis software and the results were compared with the previously designed gold structure. The paper is aimed at incisive review of fatigue behavior in the specimen tested with designed MEMS device at specified voltage limit with a view to present analytical results and conclusions.
[MetalMUMP fabrication process, micromechanical devices, reliability, Fatigue, MetalMUMPS, finite element analysis, Pull in Voltage, Stress, microfabrication processes, FEM, Micromechanical devices, yield strength, MEMS device reliability, fatigue testing, microfabrication, Nickel, MEMS, Finite element analysis, Yield Strength, FEM analysis software, Testing]
Sensitivity of a 10nm dual-gate GAA Si nanowire nMOSFET to process variation
2016 19th International Conference on Computer and Information Technology
None
2016
Advances in technology has allowed us to demand for high speed and low power devices in modern chips. These transistors are scaled down and examined with multi-gate architecture in order to improve electrostatic control over the channel and reduce power consumption. A novel dual-gate gate-all-around (GAA) junctionless nanowire transistor (JNT) is addressed in this work. Using TCAD device modeling, we report a process variability analysis and explored the device characteristics. A cylindrical nanowire with a channel length of 10 nm and a diameter of 5 nm is employed to study the process sensitivity of the device. A 1 nm thick HfO2 gate is used as a dielectric material. Analysis from deep depletion to strong accumulation mode with a 2e19 cm-3 n-type channel doping (Vds = 0.8V and T = 300 K) is reported. The dual-gate GAA JNT shows an improved electrical behavior with a variation to channel doping, channel diameter, channel length and oxide thickness. It also has an enhanced short channel behavior such as high Ion/Ioff (&gt;106), good sub threshold slope (~68 mV/decade) and low drain induced barrier lowering (~8mV/V) compared to conventional inversion mode transistors and regular nanowire GAA devices that makes the device a candidate for energy efficiency improvements.
[Junctionless Nanowire Transistor (JNT), Gallium arsenide, electrostatic control improvement, dielectric materials, dual-gate GAA JNT, power consumption, process variability analysis, multigate architecture, cylindrical nanowire, semiconductor doping, dual-gate gate-all-around junctionless nanowire transistor, junctionless nanowire transistors, semiconductor quantum wires, channel length, nanowires, MOSFET circuits, high speed devices, improved electrical behavior, energy efficiency improvements, size 10 nm, Short Channel Effect (SCE), Silicon, semiconductor device models, drain induced barrier lowering, channel diameter, oxide thickness, process variation sensitivity, dual-gate GAA silicon nanowire nMOSFET, deep depletion, low power devices, Dual-gate GAA, technology CAD (electronics), subthreshold slope, dielectric material, Simulation, TCAD device modeling, power consumption reduction, Si, Logic gates, Threshold voltage, Transistors, n-type channel doping, Sentaurus 3D TCAD, low-power electronics, accumulation mode]
IoT based autonomous percipient irrigation system using raspberry Pi
2016 19th International Conference on Computer and Information Technology
None
2016
This paper propounds a design for automatic water supplying system in farmland using raspberry pi 3, Arduino microcontrollers, WiFi module, GSM shield, relay boards and couple of sensors. The components we used in our system ensures overall fecund, scalable and spirited implementation. Depending upon the moisture level of farmland and daylight intensity, the system can detect the appropriate time of water supply in the trees and can also keep track of the water level to prevent water from being accumulated around the roots of the saplings. The analog data received from the sensors are transmitted by Arduino as digital signal via Wifi Module to the Raspberry Pi 3. The system is able to notify the administrator if water shortage arises in the main water supply and an administrator can also communicate with the system by sending SMS (Short message service) of a particular keyword. This system can be applied in farmland as well as small pot plants. Using this system, a very promising outcome is found in sustaining and cherishing the plants in a more scientific way.
[Computers, Irrigation, autonomous percipient irrigation system, Raspberry Pi, electronic messaging, relay boards, Farmland, Sensor systems, short message service, Relays, farmland, IoT, notification, irrigation, Wi-Fi module, GSM, water shortage, automatic water supplying system, digital signal, water supply, Arduino microcontrollers, WiFi module, Internet of Things, GSM shield, sensors, SMS, Raspberry Pi 3, wireless LAN, Arduino, IEEE 802.11 Standard, cellular radio]
Conceptual design of a low cost flight data acquisition system for analyzing flight behavior of small unmanned aerial vehicles
2016 19th International Conference on Computer and Information Technology
None
2016
This paper represents a conceptual design of a flight data acquisition system for small scale unmanned aerial vehicles. The system consists of three components-the unmanned aircraft, data acquisition hardware and display. The acquired data from various sensors of the acquisition system are stored in on board storage device and the data is essential for achieving situational awareness of the control of the UAV and post flight analysis for improved system behavior, accidents investigation, aerodynamics design improvement etc.
[situational awareness, post flight analysis, Aerospace electronics, aircraft control, UAV, flight controller, conceptual design, FDAS, storage management, aerospace computing, Unmanned aerial vehicles, Hardware, data acquisition, unmanned platform, Sensors, Servomotors, data acquisition hardware, aerodynamics design improvement, small scale unmanned aerial vehicles, Data acquisition, data logging, UAV control, accident investigation, autonomous aerial vehicles, sensors, board storage device, unmanned aircraft, data acquisition display, Aircraft, aircraft displays, flight dynamics, flight data acquisition system, system behavior]
Development of smart communication system for the autistic and the disabled
2016 19th International Conference on Computer and Information Technology
None
2016
Using technology to improve the life-style of people with special needs has become a hot research topic in today's world. The making of a smart device that can solve the communication issues faced by people with autism spectrum disorder(ASD) and disability is presented in this paper. Fueled by the intent of bridging the communication gap between these people and the rest of the world, we have built a small portable radio frequency(RF) communication device using basic electronic equipments that will allow them to stay in close contact with their family members; even if, they are miles apart from each other. A mere push of a button will notify everyone in the family of the current state of mind of the ASD and disabled people. Each button is loaded with a unique message in audio format, ready to be launched off to loudspeakers that will play them as loud as possible. With this project we are aiming to get rid of the communication problems faced by disabled people and thus help them realize that technology can make their lives shine brighter than it has ever before. Our prototype can be used at homes as well as in hospitals, where patients other than disabled people can also find it useful to their needs. Before getting started we did some intensive research on autism on the internet as well as made a trip to a place that schools autistic people. There we learned a great deal about the various methods used so far to educate and nurture them. One of the methods required the teachers to pronounce words slowly and repeat five times, then the students would be instructed to repeat after them, another involves the good old method that utilizes the power of visualization; in other words, seeing and learning.
[Computers, smart communication system, biomedical equipment, electronic equipment, loudspeakers, Radio frequency, ASD, Transmitters, biomedical electronics, disabled, autism spectrum disorder, communication, audio format, AAC, Receivers, biomedical communication, Information technology, medical disorders, RF, loudspeaker, disabled person, smart device, Pins, Internet, small portable radiofrequency communication device]
Binary multi-objective PSO and GA for adding new features into an existing product line
2016 19th International Conference on Computer and Information Technology
None
2016
Software product line evolution is a decision-making problem where it is determined that which features are the best candidates for the different product. In this paper, we analysis two popular optimization technique for finding qualified features for next release. In this case, the qualification of features is determined by the product value, product integrity, and AND dependencies between features. This study may help a software manager in feature selection. For this purpose, first, we encoded the feature model into a binary string. Then we able to find possible alternatives by using binary multi-objective genetic search algorithm NSGA-II and particle swarm optimization PSO. From these options, the product line manager can quickly select the best-fitted solutions according to the requirements. In this implementation, a feature model called ONLINE ELECTRONIC SHOPPING from Software Product Line Online Tool SPLOT is used where the total number of features is 290 assuming that 136 features are already implemented in the existing software. Finally, 30 out of 154 features are selected successfully in five product variants. The empirical result shows that GA performs better than PSO.
[multiobjective genetic search algorithm, NSGA-II, SPLOT, particle swarm optimisation, Companies, AND dependencies, software product line evolution, Software product lines, NRP, Optimization, SPL, Genetic algorithms, new features, multiobjective GA, Libraries, feature selection, software product lines, decision-making problem, Software algorithms, software product line online tool, genetic algorithms, particle swarm optimization, PSO, binary string, decision making, online electronic shopping, Software, multiobjective PSO]
A search log mining based query expansion technique to improve effectiveness in code search
2016 19th International Conference on Computer and Information Technology
None
2016
The effectiveness of a code search engine is reduced when query terms do not represent the information needs properly or terms are ambiguous. As a result, many irrelevant code snippets and software artifacts are retrieved that hinder the developers reusing existing source code. In this paper, a technique named QExpandator is proposed that improves the effectiveness in code search by expanding query terms with search topic and content specific keywords. It extracts user queries and clicked code fragments from the previous search history and represents each query in a vector document. Jaccard similarity score is calculated for each term in the document vector and a posting list of conceptually similar words is created based on the similarity score. Finally, to expand a user query, top scored terms are retrieved for each query term and appended to the original query. To evaluate the technique, 22 user queries were selected and an existing approach was employed. QExpandator shows 48.6% more effectiveness in terms of precision at 10 (P@10) than the existing one. Moreover, for each query, it increases P@10 from 60.9% to 90% on an average due to using search topic and context specific keywords.
[Context, Vocabulary, QExpandator, search engines, data mining, code search, query terms, Jaccard similarity score, Indexes, Information technology, clicked code fragments, vector document, code reuse, query processing, code search engine, content specific keywords, search log mining based query expansion, user query extraction, Search engines, Software, term mismatch, Matrix converters, search topic keywords]
A bug assignment technique based on bug fixing expertise and source commit recency of developers
2016 19th International Conference on Computer and Information Technology
None
2016
Automatic bug assignment is an essential activity aiming at assigning bugs to appropriate developers. Existing approaches consider either recent commits or previous bug fixes of developers, leading to recommendation of inexperienced or inactive developers respectively. Considering only one information source leads these approaches to low prediction accuracy. An approach called ERBA is proposed, which considers both expertise and recent activities of developers. ERBA first processes source code and commit logs to construct an index connecting the source entities with developer recent activities. Next, it takes fixed bug reports and builds another index, mapping the bug report keywords with developer bug fixing expertise. On arrival of new bug reports, the final module queries the two indexes using the new bug report terms, and applies tf-idf technique on the query result to calculate an ERBA score for developers. Finally, an ascending ordered list on ERBA score is suggested. For assessment of competency, a case study has been conducted on Eclipse JDT. It depicts that ERBA outperforms existing approach by improving prediction accuracy from 33.8% upto 44%. The result also represents that ERBA shows the first correct developer on average near 4.04 ranks, whereas existing approach shows in 7.27.
[source code (software), Bug reports, Vocabulary, program debugging, source entities, ERBA score, tf-idf technique, automatic bug assignment, Indexes, History, source code processes, Information technology, bug report keyword mapping, Recommendation, source commit recency, recommender systems, Bug assignment, Computer bugs, Text categorization, information source, Software, Eclipse JDT, Term weighting technique, bug fixing expertise]
Brochure abstract
2016 19th International Conference on Computer and Information Technology
None
2016
Presents abstracts for the articles comprising the conference proceedings.
[]
Respiration monitoring by using ECG
2017 20th International Conference of Computer and Information Technology
None
2017
Electrocardiogram (ECG or EKG) is an image of the electrical activity of heart which is formed as line tracings on paper. ECG helps the trained physicians to diagnose the status of the heart. The objective of this work is to utilize cardio generated voltages for respiratory study. This work analyses real-time ECG formed from standard lead systems to extract respiration rate of lungs. The job will be performed with the ECG dataset available at physionet.org, an internet database bank. An algorithm in (Matrix Laboratory) MATLAB environment is developed for extraction of the exact amplitudes of the R-waves and furthermore, these R-wave amplitudes are used to form pulsatile waves due to respiration. The respiration wave is then used to evaluate the respiration rate of the heart. This matches completely with the respiration information of the same subject/patient. Thus this analysis will help the person who has already gone through the ECG test to know about his respiration rate without going any further diagnosis.
[Heart, ECG dataset, respiration information, Instantaneous heart rate (IHR), Lung, pneumodynamics, patient monitoring, Respiration Rate (RR), electrocardiography, Databases, R-wave amplitudes, internet database bank, Electrocardiography, Lead, respiration wave, Filtering, heart electrical activity, (Matrix Laboratory) MATLAB environment, pulsatile waves, respiration monitoring, lung, medical signal processing, ECG test, lungs respiration rate, Electrocardiogram (ECG), Matlab]
Reversible data hiding with image bit-plane slicing
2017 20th International Conference of Computer and Information Technology
None
2017
Utilizing bit-plane slicing, a novel approach for reversible data hiding (RDH) is introduced in this paper. Instead of directly embedding in an input image, we propose to embed in a pair of bit-plane sliced images of the input image. Specifically, an (m + n)-bit input image is subdivided in two lower intensity images, i.e., n-bit image using n-LSB planes and m-bit image using m MSB planes. Embedding in a lower intensity image would offer relatively higher embedding rate, since the pixel-counts of the highest bin in the image histogram would be much higher than that of the original image. Moreover, embedding in the n-bit image would cause lower embedding distortion, while that in the m-bit image should contribute to a higher contrast enhancement. After embedding, histogram shifting (HS)-based embedding, those two images can be combined to get the (m+n)-bit embedded image. Comparing with a prominent HS-based RDH scheme, the proposed scheme has demonstrated significantly higher embedding rate and better contrast-enhancement.
[Visualization, PSNR, image bit-plane slicing, Distortion, Image restoration, Data mining, embedding distortion, reversible data hiding, Histograms, image enhancement, image histogram, histogram shifting, reversible data-hiding, data encapsulation, image coding, contrast enhancement, Bit-plane slicing, Image enhancement, bit-plane sliced input image]
Content based paddy leaf disease recognition and remedy prediction using support vector machine
2017 20th International Conference of Computer and Information Technology
None
2017
Rice is one of the staple foods of the world. But the production of rice is hampered by various kind of paddy diseases. One of the main diseases of paddy is leaf disease. Generally, it is very time-consuming and laborious for farmers of remote areas to identify paddy leaf diseases due to unavailability of experts. Though experts are available in some areas, disease detection is performed by naked eye which causes inappropriate recognition sometimes. An automated system can minimize these problems. In this paper, an automated system is proposed for diagnosis three common paddy leaf diseases (Brown spot, Leaf blast, and Bacterial blight) and pesticides and/or fertilizers are advised according to the severity of the diseases. K-means clustering is used for separating affected part from paddy leaf image. Visual contents (color, texture, and shape) are used as features for classification of these diseases. The type of paddy leaf diseases is recognized by Support Vector Machine (SVM) classifier. After recognition, the predictive remedy is suggested that can help agriculture related people and organizations to take appropriate actions against these diseases.
[bacterial blight disease, Shape, image classification, support vector machine classifier, crops, SVM, leaf blast, Texture, agriculture, leaf blast disease, Microorganisms, Image color analysis, K-Means Clustering, feature extraction, content based paddy leaf disease recognition, agriculture related people, plant diseases, image colour analysis, SVM classifier, disease detection, rice production, support vector machines, paddy leaf image, Color, diseases, brown spot disease, image texture, K-means clustering, Diseases, Fertilizers, Remedy, Support vector machines, Image segmentation, Paddy Leaf Disease, Feature extraction, visual contents]
Proposal and characterisation of a CPW feed miniaturized implantable patch antenna for biomedical applications
2017 20th International Conference of Computer and Information Technology
None
2017
Now-a-days implantable device for biomedical applications are the interesting research area through all over the world due to the health care industry is highly announced to develop better efficient system which are more suitable and user friendly for patient. The main concern of the implantable device is to monitor various ailments within the body efficiently and then sending the corresponding monitoring signal to the server or base station. In that case, an efficient and miniaturized implantable antenna design is a great concern to ensure proper communication between implanted device and base station. In this paper, an implantable microstrip patch antenna (IMPA) has been proposed and characterized for implantable biomedical applications in the frequency of industrial scientific and medical (ISM, 2.4 GHz to 2.48 GHz) band. The total size of the proposed antenna is 12&#x00D7;12&#x00D7;2 mm3. Here patch of the antenna is designed on a dielectric substrate named Teflon having dielectric constant of 2.1 and it also have tangent loss of 0.00028. This proposed antenna is designed and simulated by using CST (Computer Simulation Technology) microwave studio 3D electromagnetic solver. The proposed IMPA shows good results such as lower return loss (-31.628 dB), better gain (-13.6 dB), wide bandwidth (727 MHz), VSWR (1.05) very close to unity, good impedance matching as well as miniature in size (volume of 288mm3).
[Computer Simulation Technology, signal monitoring, patient monitoring, prosthetics, Microstrip antennas, implantable microstrip patch antenna, Antenna feeds, CST microwave studio 3D electromagnetic solver, Biomedical, implantable patch antenna, Teflon, permittivity, ISM, microwave antennas, dielectric substrate, microstrip antennas, Implantable, impedance matching, antenna design, CPW feed miniaturized implantable patch antenna, bioelectric phenomena, Skin, Coplanar waveguides, CPW feed, Gain, MPA]
Proposal of wide bandwidth and very miniaturized having dimension of μm range slotted patch THz microstrip antenna using PBG substrate and DGS
2017 20th International Conference of Computer and Information Technology
None
2017
In this paper, a wide bandwidth (26.4 GHz) and very small sized (180&#x00D7;210&#x00D7;10 μm3) slotted patch terahertz (THz) rectangular microstrip antenna using photonic band gap (PBG) substrate and defected ground structure (DGS) has been proposed and analyzed. At first a very compact with dimension of μm range rectangular microstrip patch antenna (RMPA) was designed by using computer simulation technology microwave studio (CST-MWS) at the operating frequency of 0.685 THz, which shows return loss of -30.552 dB, wide bandwidth of about 26 GHz, gain of 5.145 dBi, directivity of 6.727 dBi and VSWR of 1.061. Then we introduced PBG structure as substrate by creating same distant as well as same sized air gaps on the substrate. In that case the performance of the antenna is improved significantly than the previous one. Its return loss decreased to -38.973 dB at resonating frequency 0.701 THz. Its gain and directivity get increased to 5.156 dBi and 6.833 dBi respectively. Bandwidth is also enhanced compared to the previous one with a VSWR of 1.023. Next we have done some experiment by creating defects in ground plane. We optimized the dimensions of the defects and got better performance of the antenna having 0.704 THz resonant frequency with more minimized return loss of -47.862. Gain of this antenna is 5.090 dBi and directivity is increased to 6.835 dBi. Its VSWR is of 1.008 with the improved bandwidth. Finally, we have made some circular slots on the radiating patch and got best results. The final design of antenna resonates at center frequency of 0.703 THz with a return loss of -50.948 dB. The gain and directivity of the proposed antenna are of 5.075 dBi and 6.833 dBi respectively. The antenna shows VSWR of 1.006 which is very much close to unity (1) with the enhanced bandwidth compared to the all previously designed antenna.
[frequency 0.703 THz, THz, RMPA, Microstrip, PBG, antenna radiation patterns, Microstrip antennas, Bandwidth, CST-MWS, frequency 26.4 GHz, PBG structure, μm sized, frequency 0.701 THz, frequency 26.0 GHz, defected ground structures, frequency 0.685 THz, radiating patch, photonic band gap substrate, photonic band gap, microstrip antennas, Substrates, rectangular microstrip patch antenna, DGS, Slot antennas, Patch antennas, frequency 0.704 THz, Slot, THz resonant frequency]
A stylometric analysis on Bengali literature for authorship attribution
2017 20th International Conference of Computer and Information Technology
None
2017
This paper explores the authorship attribution problem in modern Bengali literature. By scrutinizing the writings of six Bangladeshi columnists of current time using some established and modified stylometric features the writing patterns of these writers have been observed. With statistical analysis on a corpus containing around seven hundred articles the most effective style markers that can create a significant difference among authors were identified. Based on these features a classification model and a voting system were developed to identify the original author of an unknown document. The developed voting system achieved 90.67% accuracy rate on a test corpus of three hundred articles.
[Vocabulary, text analysis, current time, bangla language processing, Training, authorship attribution, developed voting system, stylometric features the writing patterns, stylometry, original author, Statistical analysis, writings, Blogs, writers, stylometric analysis, machine learning, Information technology, Bangladeshi columnists, authorship attribution problem, classification model, Computer science, stylogenetics, Writing, statistical analysis, articles, modern Bengali literature, test corpus]
Investigation of cloud attenuation over Bangladesh for microwave communications
2017 20th International Conference of Computer and Information Technology
None
2017
Microwave communications enhance the scope of large bandwidth. On the other hand, cloud, rain and some other environmental issues greatly introduce attenuation in microwave link. As Bangladesh is located in a subtropical region, Cloud causes rampant attenuation in microwave communication. For designing a microwave or satellite link, it is important to consider the effect of cloud attenuation. The dominance of cloud at Ku (12 and 14GHz), Ka (20 and 30GHz) and V (40 and 50GHz) bands are investigated in this paper. The investigation is also based on ITU cloud attenuation model. This paper considers 14 geographical stations to collect raw data of weather for a specific time period (1994-2013). Collected data are used to determine the cloud attenuation for different frequencies of Ku, Ka and V bands. The contour map is also designed for different bands of inaccessibility. The results show that the dominance of cloud attenuation is significantly occurred in the northern part of Bangladesh.
[electromagnetic wave attenuation, frequency 20.0 GHz, millimetre wave propagation, frequency 30.0 GHz, V-band frequency, Clouds, frequency 40.0 GHz, Microwave communication, Predictive models, frequency 12.0 GHz, clouds, AD 1994 to 2013, Microwave integrated circuits, microwave links, geographical stations, frequency 14.0 GHz, Attenuation, atmospheric electromagnetic wave propagation, Mathematical model, contour map, microwave link, ITU cloud attenuation model, Microwave FET integrated circuits, Ku-band frequency, Bangladesh, cloud attenuation, frequency 50.0 GHz, Ka-band frequency, microwave communication, microwave propagation]
Understanding Dhaka city traffic intensity and traffic expansion using gravity model
2017 20th International Conference of Computer and Information Technology
None
2017
Analysis of traffic pattern recognition and traffic congestion expansion in real time are one of the exciting and challenging tasks which help the government to build a robust and sustainable traffic management system specially in a densely populated city like Dhaka. In this paper, we analyze the traffic intensity for small areas which are also known as junction points or corridors. We describe Dhaka city traffic expansion from a congestion point by using gravity model. However, we process real-time traffic data of Dhaka city rather than depend on survey and interview. We exactly show that traffic expansion of Dhaka city exactly follows gravity model. Expansion of traffic from a congestion point spreads out rapidly to its neighbor and impact of congested point decreases as the distance increases from that congested point. This analysis will help the government making a planned urbanized Dhaka city in order to reduce traffic jam.
[Junction point, Roads, Global Positioning System (GPS), gravity model, Dhaka city traffic expansion, congestion point, traffic jam reduction, corridors, sustainable traffic management system, Gravity model, Real-time systems, Junctions, traffic congestion expansion, Gravity, pattern recognition, road traffic, traffic pattern recognition, data analysis, Urban areas, Government, Traffic pattern recognition, traffic engineering computing, Global Positioning System, planned urbanized Dhaka city, junction points, Traffic congestion, Dhaka city traffic intensity, real-time traffic data]
Efficient LSB substitution for interpolation based reversible data hiding scheme
2017 20th International Conference of Computer and Information Technology
None
2017
Interpolation based reversible data hiding (IRDH) has created a new paradigm for data hiding research. This paper presents a new IRDH scheme with quadratic interpolation for digital image applications. A set of least significant bits (LSBs) are sought to be efficiently utilized in an adaptive manner for lower possible embedding distortion with higher embedding capacity. Particularly, a recent IRDH scheme employed a flag bit to track the form of embedded payload-bits (i.e., original or complement) in an embedded pixel. Whereas, we propose to utilize the correlation between the embeddable pixel and estimated versions of embedded pixel to embed one more data bit in place of the flag bit. Thus, our proposed scheme avoids using flag bit resulting in higher embedding capacity as demonstrated by our early experimental results. A significantly higher embedding capacity (i.e., about 30%-50% higher bpp) with reasonably better image quality (i.e., of about 1%-5% higher PSNR and SSIM values) is obtained for different values of capacity control parameter.
[data compression, data hiding, Distortion, Data mining, Image quality, quadratic interpolation, digital image applications, Interpolation, interpolation, data extraction, Nanoelectromechanical systems, reversible embedding, Rate-distortion, interpolation based reversible data hiding scheme, LSB substitution, data encapsulation, image coding, Payloads, IRDH scheme]
An improved algorithm for string matching using index based shifting approach
2017 20th International Conference of Computer and Information Technology
None
2017
Bioinformatics is now considered as one of the most studied branches in biological science which deals with the exploring of different kinds of methods for analyzing, storing and retrieving biological data, such as protein sequences and nucleic acid (DNA/RNA), configurations, functions, pathways, and genetic relations. In bioinformatics, genomic sequence analysis has led to the evolution of well-organized pattern discovery algorithms to produce search over large DNA sequences. The most important factor for efficiency of a matching algorithm is depending on the total number of character comparisons. Index Based Shift (IBS) algorithm which is the name chosen for our proposed algorithm was tested on different length of DNA sequences data set. The IBS algorithm shows great efficiency in terms of total number of character comparisons it requires than that of ABSBMH algorithm. The experimental result shows that our proposed IBS algorithm works better when length of pattern is increased.
[Algorithm design and analysis, DNA/RNA, pattern matching, RNA, shifting approach, data mining, IBS algorithm, character comparisons, genetics, proteins, genomics, Bioinformatics, biological data retrieval, genetic relations, index based shifting approach, Indexes, Computer science, matching algorithm, ABSBMH algorithm, exact string matching, biological sequence, Shift algorithm, DNA sequences data, biological science, pattern discovery algorithms, DNA, bioinformatics, genomic sequence analysis, Approximation algorithms, string matching, molecular biophysics, protein sequences, Pattern matching]
A heuristics-based keyword and phrase ranking in the text corpus for question answering systems
2017 20th International Conference of Computer and Information Technology
None
2017
This work presents an effective keyword extraction and ranking approach based on multiple text-based features from the text corpus for Question Answering systems by keeping the context and discourse of the text in view. Here, the extraction and ranking process is automatic. The approach uses data semantics and relationships to mark the keyword boundaries and intelligently annotates texts, uses various relationships among entities in the text and proposes various heuristic rules to calculate the ranking score of the keywords and phrases. The proposed approach can be integrated into information retrieval based system as well as in Question Answering systems. The current work is compared against other standard baseline methods to show its competence. The results achieved and the comparison with the various existing methods show the effectiveness and usefulness of the proposed model.
[text analysis, keyword boundaries, heuristics-based keyword, text corpus, keyword extraction, text-based features, Tools, information retrieval based system, heuristic rules, Data mining, data semantics, heuristics rules, Semantics, Supervised learning, question answering systems, Feature extraction, Knowledge discovery, phrase ranking, ranking score, Coal mining, question answering (information retrieval), keyword ranking, text annotation]
Disparity between the programmatic views and the user perceptions of mobile apps
2017 20th International Conference of Computer and Information Technology
None
2017
User perception in any mobile-app ecosystem, is represented as user ratings of apps. Unfortunately, the user ratings are often biased and do not reflect the actual usability of an app. To address the challenges associated with selection and ranking of apps, we need to use a comprehensive and holistic view about the behavior of an app. In this paper, we present and evaluate Trust based Rating and Ranking (TRR) approach. It relies solely on an apps' internal view that uses programmatic artifacts. We compute a trust tuple (Belief, Disbelief, Uncertainty - B, D, U) for each app based on the internal view and use it to rank the order apps offering similar functionality. Apps used for empirically evaluating the TRR approach are collected from the Google Play Store. Our experiments compare the TRR ranking with the user review-based ranking present in the Google Play Store. Although, there are disparities between the two rankings, a slightly deeper investigation indicates an underlying similarity between the two alternatives.
[mobile apps, Uncertainty, Humanoid robots, human factors, user ratings, Apps, Evidences, comprehensive view, mobile computing, Subjective Logic, mobile-app ecosystem, holistic view, Testing, order apps ranking, Google, Trust based Rating and Ranking approach, User Rating, information retrieval, Tools, Online Marketplace, user perception, Trust, TRR approach, Computer bugs, programmatic views, Androids]
Spectrogram segmentation for bird species classification based on temporal continuity
2017 20th International Conference of Computer and Information Technology
None
2017
This article presents an enhanced approach for bird species classification from their recorded audio signals. Observing that textures of syllables in audio spectrograms have noticeable discerning capabilities among different bird species, we adopt these texture features for bird species classification. First, we compute spectrogram from recoded audio. We propose an enhanced syllable extraction technique to identify the syllables in the spectrogram. Texture features, based on gray level cooccurrence matrix (GLCM), are computed and used for classification using an ensemble learning method. We obtain satisfactory accuracy when the approach is tested on real audio recordings of 11 different bird species.
[bird species classification, Fourier transforms, image classification, ensemble learning method, Classification algorithms, gray level cooccurrence matrix, audio recordings, Spectrogram, Training, Ensemble Classifier, feature extraction, texture features, Robustness, syllable extraction technique, learning (artificial intelligence), audio spectrograms, Birds, image texture, GLCM, matrix algebra, recorded audio signals, Spectrogram Segmentation, Pattern Recognition, audio signal processing, spectrogram segmentation, Feature extraction, temporal continuity, audio recording, GLCM Texture Feature, acoustic signal processing]
Ontological knowledge extraction from natural language text
2017 20th International Conference of Computer and Information Technology
None
2017
Ontology (Onto=Being and Logy=Knowledge, therefore the Knowledge of Being) has a significant impact on the study of natural language processing. By providing a formal representation of knowledge it ensures proper understanding of a particular domain. A comprehensive description to build a vocabulary on the given domain can never be feasible without ontologies; in other word conceptualizations. There have been a number of works in the recent times to perceive the ontological knowledge to build a strong vocabulary. Most of the existing ontology construction tools support construction of ontological relations (e.g., taxonomy, equivalence, etc.). But the main problem is that they do not support construction of domain relations, non-taxonomic conceptual relationships (e.g., causes, caused by, treat, treated by, has-member, contain, material-of, operated-by, controls, etc.) which are basically found in the text sources. The first notable work on this field is a Named Entity Recognition system developed by Stanford University. Stanford NER (also known as CRFClassifier) is a Java implementation of a Named Entity Recognizer. It can successfully recognize at most seven classes. In this research work we have utilized this NER, and proposed an algorithm which includes POS tagging, lemmatization, parsing and pronoun and co-reference resolution. We can squeeze out 22 classes from these seven primary classes. We have compared our work with an existing system named TextOntoEx. On the basis of performance matrices we have analyzed both of the works on a same natural language text. Result shows that our proposed system can find out more classes than TextOntoEx.
[word conceptualizations, text analysis, Dictionaries, Stanford University, natural language processing, Taxonomy, knowledge acquisition, named entity recognition system, Ontologies, Tools, natural language text, ontological relations, ontological knowledge extraction, text sources, Organizations, TextOntoEx system, ontologies (artificial intelligence), Natural language processing, named entity recognizer, Knowledge engineering tools, Stanford NER, Ontological Knowledge]
Polarity detection of online news articles based on sentence structure and dynamic dictionary
2017 20th International Conference of Computer and Information Technology
None
2017
The importance of online news article has evolved notably with the advancement of information and technology. However, some of the news are violent as well as obnoxious. So, identifying and categorizing online news article automatically is important as well as remains challenging. Using opinion mining and sentiment analysis, we propose an intuitive approach of detecting positive or negative news from an online news article. Our approach consists of a sentence identification phase, followed by a dynamic library of predefined negative and positive strings and at last marking whether the paragraph is positive, negative or neutral. Our approach detects the polarity of online news articles with around 91% accuracy rate. Sentence type identification before using dynamic dictionary of positive and negative words is the key factor which resolves the issue of finding out the part of the sentence which holds the polarity of the sentence.
[information resources, opinion mining, Sentiment analysis, Dictionaries, sentiment analysis, dynamic dictionary, data mining, Compounds, Data mining, Polarity detection, Sentence analysis, dynamic library, online news article, Semantics, User interfaces, positive news, negative news, Libraries, sentence structure, Phrase-level Sentiment analysis, Opinion mining, sentence identification phase, dictionaries]
Single image super-resolution using back-propagation neural networks
2017 20th International Conference of Computer and Information Technology
None
2017
There are several existing mathematical algorithms for colour image upscaling like Nearest Neighbour, Bicubic and Bilinear. This paper firstly investigates the performances of these three and it has been found that Bicubic performs the best in terms of structural similarity and execution time. A Bicubic with backpropagation based ANN method has been proposed to improve the results. Bicubic with ANN shows 6.5% higher SSIM, 6.9% higher PSNR, 8.7% higher SNR and 30.23% lower MSE values than pure Bicubic. The results of Bicubic with ANN are also compared with state of the art super-resolution techniques like SRCNN. Bicubic with ANN produces 1.48% higher SSIM and 3.44% higher PSNR than SRCNN.
[backpropagation neural networks, single image super-resolution, structural similarity, backpropagation based ANN method, Training, colour image upscaling, image enhancement, super-resolution techniques, mean square error methods, image resolution, Backpropagation, Image resolution, PSNR, SSIM, Super-resolution, mathematical algorithms, Indexes, Image Upscaling, Artificial Neural Network (ANN), Interpolation, Bicubic, Neural networks, execution time, backpropagation, neural nets, Signal to noise ratio]
An application of pre-trained CNN for image classification
2017 20th International Conference of Computer and Information Technology
None
2017
Image Classification is a branch of computer vision where images are classified into categories. This is a very important topic in today's context as large databases of images are becoming very common. Images can be classified as supervised or unsupervised techniques. This paper investigates supervised classification and evaluates performances of two classifiers as well as two feature extraction techniques. The classifiers used are Linear Support Vector Machine (SVM) and Quadratic SVM. The classifiers are trained and tested with features extracted using Bag of Words and pre-trained Convolution Neural Network (CNN), namely AlexNet. It has been observed that the classifiers are able to classify images with very high accuracy when trained with features from CNN. The image categories consisted of Binocular, Motorbikes, Watches, Airplanes, and Faces, which are taken from Caltech 265 image archive.
[image classification, pre-trained CNN, unsupervised techniques, Training, Convolution, supervised classification, feature extraction, convolution, Bag of Words (BoW), learning (artificial intelligence), Linear Support Vector Machine, convolution neural network, Convolution Neural Network (CNN), support vector machines, Computational modeling, Support Vector Machine (SVM), Quadratic SVM, Information technology, Caltech 265 image archive, quadratic SVM, Support vector machines, feature extraction techniques, computer vision, AlexNet, Feature extraction, feature extraction etc, Linear SVM, neural nets, Bag of Words, Image classification]
Bangla grapheme to phoneme conversion using conditional random fields
2017 20th International Conference of Computer and Information Technology
None
2017
Integrated with handheld devices, toys, KIOSKs, and call centers, Text to Speech (TTS) and Speech Recognition (SR) have become widely used applications in everyday life. One of the core components of said applications is Grapheme to Phoneme (G2P) conversion. The task at hand is the mapping of the written form to the spoken form, i.e. mapping one sequence to another. In Natural Language Processing (NLP), it is typically referred to as a sequence to sequence labeling task. The task however, is a language dependent one and has primarily been implemented for English and similar resource-rich languages. In comparison, very little has been done for digitally under-resourced languages such as Bangla (ethnonym: Bangla; exonym: Bengali). The current state-of-the-art Bangla Grapheme to Phoneme conversion is limited to rule-based and lexicon based approaches, the development of which requires a significant contribution of linguistic experts. In this paper, we propose a data-driven machine learning approach for Bangla G2P conversion. We evaluate the existing rule based approaches and design a machine learning model using Conditional Ran-dom Fields (CRFs). To train the machine learning models we have only used character level contextual features due to the fact that extracting hand crafted features requires specialized knowledge. We have evaluated the systems using two publicly available datasets. We have obtained promising results with a phoneme error rate of 1.51% and 14.88% for CRBLP and Google pronunciation lexicons, respectively.
[Text to Speech, Dictionaries, conditional random fields, sequence to sequence labeling task, G2P conversion, Task analysis, Training, Grapheme to Phoneme conversion, speech recognition, language dependent, feature extraction, Bangla Grapheme, learning (artificial intelligence), resource-rich languages, hand crafted feature extraction, Google, Speech Recognition, speech synthesis, natural language processing, lexicon based approaches, Conditional Random Fields, random processes, Natural Language Processing, Bangla, phoneme conversion, Pragmatics, rule-based approaches, phoneme error rate, Speech recognition, Speech, Pronunciation Generation, handheld devices, Grapheme to Phoneme (G2P), data-driven machine learning]
An automated system to detect and recognize vehicle license plates of Bangladesh
2017 20th International Conference of Computer and Information Technology
None
2017
Bangladesh is a country in South Asia that uses Retro Reflective license plates. The plate has two lines with words, letters, and digits. An automated system to detect and recognize these plates is presented in this paper. The system is divided into four parts: plate detection, extraction, character segmentation and recognition. At first, the input image is enhanced using CLAHE and a matched filter specially designed for license plates with two lines is applied. Then tilt correction using Radon transformation, binarization and cleaning are performed. For character segmentation, mean intensity based horizontal and vertical projection is used. In recognition, we have used two different Convolutional Neural Network (CNN) to classify digits and letters. Tesseract OCR is used for district names. We have developed a dataset of over 400 images of different vehicles (e.g., private car, bus, truck etc.) taken at different times of day (including nights). The plates in this dataset are in different angles, including blurry, worn-out and muddy ones. On this dataset, the proposed system achieved a success rate of 96.8% in detection, 89.5% in extraction, 98.6% in segmentation and 98.0% in character recognition.
[vehicle license plates, Bangladeshi vehicles, image classification, Licenses, horizontal projection, optical character recognition, road vehicles, feature extraction, image segmentation, Radon transforms, Mathematical model, Kernel, Radon transformation, Convolutional Neural Network, vertical projection, Image edge detection, traffic engineering computing, character recognition, matched filter, Character recognition, Optical character recognition software, Retro Reflective license plates, Image segmentation, ANPR, Plate Detection, ALPR, character segmentation, Character Recognition, image recognition, neural nets, plate detection]
River tide level prediction: A data mining approach for hydrographie time series data analysis
2017 20th International Conference of Computer and Information Technology
None
2017
Prediction is one of the most complicated and challenging tasks; if it comes to tidal prediction then it becomes more complicated because of the chaotic nature of gradual increment in water level. Since Bangladesh is a low lying land, performing tidal prediction in seaport area is really a key factor since the economy of the country largely depends on the smooth operations of exports and imports in the 3 major seaports. Predicting hundred percent accurate tide levels is very difficult. The main objective of this study was to propose a good machine learning model with the aid of data mining techniques for the projection of water level with higher accuracy for Karnaphuli River, which is one of the mainstream rivers in Bangladesh. The river comprises of 4 major boatyards and shipyards for anchoring lighter ship vassals and boats; those are Sadarghat, Kalurghat, Canal no-10, Canal no-18. For undertaking the experiments, we collect 5years (2008-2012) historical tide level dataset of Karnaphuli River from Chittagong Sea Port Authority (CPA). Finally, we got our proposed model that can predict tide level with more than 96% accuracy in overall scenarios, which outperform the previous models that are used in literature review and reference sections.
[water level, data mining, Predictive models, data mining approach, seaport area, sea ports, Tides, Training, lighter ship vassals, Chittagong Sea Port Authority, learning (artificial intelligence), machine learning model, Kernel, rivers, low lying land, data analysis, geophysics computing, time series, Rivers, tidal prediction, boats, machine learning, hydrographie time series data analysis, Chittagong Port Authority (CPA), tides, Support vector machines, water level prediction, ships, Bangladesh, tide levels, river tide level prediction, support vector regression (SVR), Data models, Karnaphuli river]
A performance analysis of a typical server running on a cloud
2017 20th International Conference of Computer and Information Technology
None
2017
In recent years, Cloud computing has become very popular from individuals to big enterprises because of the reduced cost, minimal management effort etc. Cloud providers like Amazon, Google are now offering resources for web deployment, storage, servers etc. However, due to the varying load and cost, evaluating the performance of task scheduling policies in these real Cloud environments is very challenging. In this paper, the performance of Earliest Deadline First (EDF) scheduling policy has been investigated using CloudSim and the hardware configuration of Amazon Web Services (AWS) and Google Cloud Platform (GCP) with the time-out of a web and FTP servers. In addition, a comparison between space-shared and time-shared task provisioning policies have been examined which shows that the average execution time can be minimised by using space-shared policy in both AWS and GCP.
[Cloud computing, Amazon Web Services, Server Load, Servers, Task analysis, task provisioning policies, resource allocation, scheduling, CloudSim, Cloudlets, cloud computing, AWS, Digital audio players, Load modeling, Google, Execution Time etc, Computational modeling, Google Cloud Platform, cloud environments, task scheduling policies, web deployment, earliest deadline first scheduling policy, Web services, big enterprises, GCP, cloud providers, EDF algorithm, Virtual Machine]
An empirical study of CoAP based service discovery methods for constrained IoT networks using Cooja simulator
2017 20th International Conference of Computer and Information Technology
None
2017
Service discovery is one of the key requirements to realize the vision of the Internet of Things (IoT). Constrained application protocol (CoAP) is a well-accepted model that utilizes both centralized and distributed service discovery methods for lightweight and constrained IoT networks. Since CoAP is in its early stage of adoption for the IoT networks, it is important to select an efficient method for the service discovery requirement. In this paper, we implement both centralized and distributed models of the CoAP based service discovery techniques. We have used Contiki OS and Cooja simulator to build the experimental setup for our prototype systems. Our detailed empirical analysis finds the gaps in general CoAP based service discovery techniques to recommend further improvement scopes.
[Protocols, Scalability, Service Discovery, constrained application protocol, CoAP based service discovery methods, Servers, Internet of Things, Cooja simulator, Temperature measurement, constrained IoT networks, Web services, centralized distributed models, Prototypes, distributed service discovery methods, Contiki OS, Humidity, Cooja Simulator, Internet, CoAP, protocols]
Design of a muscle movement based control system
2017 20th International Conference of Computer and Information Technology
None
2017
We have developed a controlling system that works in the infrared (IR) region of the spectrum and uses the muscle movement of the user. The idea is to use the movement of a person's different fingers or a single finger in different directions to indicate different controlling instructions. Finger movements are associated with muscle movements which can be detected by appropriate sensors. The developed system processes such signals and uses a microcontroller to operate and control the desired gadget. The whole system can be miniaturized to place in an armband making it a convenient tool for the disabled people. Experimental demonstration of the system is presented.
[handicapped aids, Thumb, finger movements, Receivers, Muscles, infrared (IR) sensor, biomechanics, disabled people, infrared region, wireless communication, Electrodes, single finger, muscle movement based control system, muscle movement, muscle, Electromyography, Sensors, communication for disabled people, electromyography (EMG) signal, microcontrollers]
A feature based method for real time vehicle detection and classification from on-road videos
2017 20th International Conference of Computer and Information Technology
None
2017
Vision Based vehicle detection and classification has become an active area of research for intelligent transportation system. But this task is very difficult and challenging due to the dynamic condition of roads. Many solutions have been given by the researchers to overcome these challenges. Some of them are giving good performance but computationally highly expensive and fail in some circumstances. In the proposed method, a feature based cost effective detection and classification method is proposed that is suitable for real time applications, provide satisfactory accuracy and computationally cheap. The proposed method uses haar-like features of the images and AdaBoost classifier for detection which provides a very fast detection rate with high accuracy. To reduce inconsiderable false positive rate generated by this method, we propose to use two virtual detection lines (VDL) that reduces the false positive rate. In order to predict the class of a vehicle, a feature based method is proposed. HOG, SIFT, SURF all are well represented feature for image classification. The existing feature based vehicle classification methods lack accuracy because of using those features inefficiently. In order to reduce those lacking, we propose to use bag of visual words (BOVW) model for classification. BOVW model also needs a lower computation time and resources. As the proposed method aims to be implemented in real time, we propose to use SURF feature for BOVW which is faster to compute and well described for recognizing an object. The BOVW then used for identifying the vehicle's class by multi class SVM classifier. Error correcting output code (ECOC) framework is used to achieve multi class prediction with SVM. Extensive experiments have been carried out on different traffic data of varying environments to evaluate the detection and classification performance of the proposed method. Experiment results demonstrate that the proposed method achieves a significant improvement in classification of heterogeneous vehicles in terms of accuracy with a considerable execution time as compared to other methods.
[Roads, image classification, feature based method, support vector machine (SVM), error correcting output code framework, multiclass prediction, virtual detection line (VDL), object detection, bag of visual words model, intelligent transportation system, real time vehicle detection, Vehicle detection, road vehicles, feature extraction, Real-time systems, Detection and classification of vehicles, speeded up robust feature (SURF), learning (artificial intelligence), video signal processing, VDL, SURF feature, error correction codes, support vector machines, Computational modeling, BOVW model, intelligent transportation systems, Haar-like features, ECOC framework, traffic engineering computing, multiclass SVM classifier, Support vector machines, AdaBoost classifier, on-road videos, bag of visual words (BOVW), error correcting output code (ECOC), computer vision, Haar transforms, vision based vehicle detection, Feature extraction, Cameras, virtual detection lines]
An application of machine learning to detect abusive Bengali text
2017 20th International Conference of Computer and Information Technology
None
2017
Bengali abusive text detection can be useful to prevent cyberbullying and online harassment as these types of crimes are increasing rapidly in Bangladesh. Machine learning approach can be useful to keep the system always updated with the new types of approaches used by the abusers. This paper investigates machine learning algorithms e.g. Random Forest, Multinomial Nai&#x0308;ve Bayes, Support Vector Machine (SVM) with Linear, Radial Basis Function (RBF), Polynomial and Sigmoid kernel and have compared with unigram, bigram and trigram based CountVectorizer and TfidfVectorizer features. The results show that SVM Linear kernel performs the best with trigram TfidfVectorizer features.
[text analysis, Machine learning algorithms, abusive Bengali text, cyberbullying harassment, online harassment prevention, Support Vector Machine etc, Classification algorithms, Multinomial Na&#x00EF;ve Bayes, Random Forest, Support vector machines, Radio frequency, Training, Bangladesh, Neural Network, Text categorization, feature extraction, learning (artificial intelligence), Bengali abusive text detection, machine learning algorithms, Kernel]
How discernible is user impromptu behavior when unlocking a touch screen?
2017 20th International Conference of Computer and Information Technology
None
2017
Being a popular computing device, smartphone (or tablet, wearable device) system's screen unlocking mechanisms have recently come under numerous successful attacks. In an aim to deter the attacks, we explored users' spontaneous touch behavioral traits in order to identify the users which we opine could be an added layer of security. We experimented with four conventional unlocking schemes: 4 Digit Pin, 3&#x00D7;3 &#x201C;Connect the Dots&#x201D;, Glow Pad, Slide Unlock and proposed one new unlocking mechanism, &#x201C;Pressure Circle&#x201D;. Six high-level features have been extracted from user touch activities and we designed our own verifier to identify users. For each unlocking scheme, experimental results generated from 992 impostor attacks performed on 32 user samples, show that users can be identified with an accuracy of up to 77.3% solely by considering their touch behavior. Overall, we achieve an average accuracy of 68.8%. We argue that, this inherent behavioral biometric trait when combined with the conventional unlocking schemes, would be less susceptible to attacks.
[unlocking scheme, behavioral biometric trait, user impromptu behavior, user touch activities, screen unlocking mechanisms, Humanoid robots, Slide Unlock, Security, impostor attacks, wearable device, security, Spoof attack, authorisation, 4 Digit Pin, behavioral biometrics, touch sensitive screens, Glow Pad, touch behavioral traits, user identification, screen unlock, unlocking schemes, touch behavior, Microsoft Windows, Feature extraction, touch screen, Pins, Androids, Acceleration]
Convolutional neural network approach for vision based student recognition system
2017 20th International Conference of Computer and Information Technology
None
2017
Computers are now too smart to interact with the human in different approaches. This interaction will be more acceptable for both human and computer if it is based on recognition process. In this article, author's concern is to integrate and develop a student recognition system using existing algorithms. Among various face recognition methods, here author use deep learning based face recognition method. This method uses Convolutional Neural Networks (CNN) to generate a low dimensional representation called embeddings. Then those embeddings are used to classify the person's facial image. By this system different types of applications like student attendance-system, building security etc. can be developed. After building the system, a resultant performance is also showed in this article.
[embeddings, student attendance-system, vision based student recognition system, Face recognition, recognition process, educational administrative data processing, Support Vector Machine(SVM), Face detection, face recognition method, convolutional neural network approach, deep learning, Embeddings, Convolution, Convolutional Neural Networks (CNN), face recognition, Feature extraction, Face, Convolutional neural networks, learning (artificial intelligence), neural nets]
Question classification using support vector machine with hybrid feature extraction method
2017 20th International Conference of Computer and Information Technology
None
2017
This paper presents an approach to categorizing Bangla language question into some predefined coarse-grained category that represents expected answer type of that particular question. Support vector machine was used with different kernel function to increase the accuracy of existing Bangla question classification system. Both predefined feature set and the stream of unigram based on the frequency of data set was considered to build feature matrix. For five cross validation average 89.14% accuracy was achieved using 380 top frequent words as the feature which outperformed existing single model based Bangla question classification system. For same cross validation, 88.62% accuracy was achieved with a combination of wh-word, wh-word position and question length as feature set.
[Algorithm design and analysis, text analysis, Question Taxon-omy, coarse-grained category, feature set, Feature Extraction, Classification algorithms, SVM, Task analysis, Wh-word, Semantics, feature extraction, Question Classification, Bangla question classification system, question length, hybrid feature extraction method, Bangla language, Kernel, feature matrix, pattern classification, support vector machines, natural language processing, word position, kernel function, Support vector machines, support vector machine, Feature extraction, Kernel Function]
Secure domain name service in software defined network
2017 20th International Conference of Computer and Information Technology
None
2017
Domain Name Service (DNS) is an important service generally used by other application layer protocols of TCP/IP protocol stack. These protocols use DNS to translate human readable web address to machine readable IP address which is then used by other protocols of network stack for communication between computers over the network. The correctness of DNS translation cannot be compromised as it may lead to unsecure transactions with in the network. Because of this, DNS is generally a soft target for attackers and is vulnerable to different security threats including DNS spoofing, DNS cache poisoning, etc. Many solutions for such threats are proposed for traditional IP network. In this paper we talk about security loops in DNS and propose a solution for it in Software Defined Network (SDN) environment.
[secure domain name service, Protocols, security loops, Ports (Computers), Switches, DNS, cache storage, application layer protocols, Servers, TCP/IP protocol stack, human readable web address, Software Defined Network, DNS cache poisoning, security threats, IP networks, SDN, DNS spoofing, software defined networking, network stack, Software defined networking, computer network security, IP network, machine readable IP address, transport protocols, Software Defined Network environment, DNS Spoofing, DNS Security]
Kalman filter based location estimation of machines in M2M communication over cellular networks
2017 20th International Conference of Computer and Information Technology
None
2017
Machine-to-machine (M2M) communication is a promising concept to reduce cellular resource by allowing users (CUEs) machine using Kalman filter data fusion technique in M2M communication. The Kalman filter is used afterward to combine prediction and update of the estimated position based on state space model with the estimate machine localization is more accurate. We present experimental results to demonstrate the precision of CUEs machine location estimated. Simulation and experimental results display that estimated CUEs machine position accuracy as close as 2m from measurement CUEs machine position.
[Base stations, CUEs machine location, CUEs machine position accuracy, Kalman Filter, Estimate Position, Cellular users (CUEs), machine localization, sensor fusion, M2M, Noise measurement, cellular resource, Machine-to-machine communication, Cellular networks, Machine-to-machine communications, Mode Selection, Data integration, machine-to-machine communication, Position measurement, Machine-to-machine (M2M) communication, Kalman filters, cellular networks, cellular radio, Kalman filter data fusion technique]
Automatic authorship detection from Bengali text using stylometric approach
2017 20th International Conference of Computer and Information Technology
None
2017
Authorship detection is the process of predicting authorship of an unknown text. Every writer has a different style of writing of their own. Detecting authorship from text by analyzing writing style of an author is known as stylometry. In this paper, we propose a stylometric feature based approach for detecting authorship from Bengali texts. The system the classify authorship using n-grams, a feature ranking and selection system using information gain (IG). We used 3125 passages written by 10 Bengali authors for evaluating performance. The evaluation result shows that the propose system achieved 96% accuracy in authorship detection using random forest classifier and also reveal that n-gram features are very good discriminators among linguistic style of different Bengali authors.
[information gain, stylometry, pattern classification, text analysis, Correlation, Dictionaries, Bengali authors, natural language processing, automatic authorship detection, authorship classification, stylometric approach, Training, stylometric feature, random forest classifier, authorship detection, normalization, Vegetation, n-gram features, Feature extraction, Speech, Bengali text, writing style, IG]
The combination of spectral entropy, zero crossing rate, short time energy and linear prediction error for voice activity detection
2017 20th International Conference of Computer and Information Technology
None
2017
In this paper, an efficient classification of voice segment from the silence segment, unvoiced segment algorithm, which is both more accurate and laid-back to implement is proposed by comparing to some previous algorithms. The proposed algorithm uses spectral entropy and short time features such as zero crossing rate, short time energy, linear prediction error are used for voice activity detection (VAD). A compound parameter, D, is calculated by using all these four parameters. Dmax is calculated from all the frames of the signal. Then the value of D/Dmax is used to determine whether the frames are classified as speech and non-speech and silence frames. The threshold values have to be obtained empirically. Experimental results show that the method of this paper can detect end-points of voice signal more accurately and outperforms the conventional VAD algorithms. The method we used in this work was evaluated on TIMIT Acoustic-Phonetic Continuous Speech Corpus. This corpus is mostly used for speech recognition application and contains clean speech data and is compared with some of the most recent proposed algorithms.
[Voice activity detection, Algorithm design and analysis, voice signal, unvoiced segment algorithm, Entropy, zero crossing rate, Classification algorithms, silence segment, TIMIT Acoustic-Phonetic Continuous Speech Corpus, entropy, speech recognition, short time energy, short time features, linear prediction error, silence frames, voice activity detection, spectral entropy, VAD, Speech, Prediction algorithms, Feature extraction, speech recognition application, compound parameter, energy]
Bi-level multi-objective image segmentation using texture-based color features
2017 20th International Conference of Computer and Information Technology
None
2017
This paper presents a bi-level multi-objective evolutionary image segmentation approach based on texture-based color features. There are many approaches for image segmentation. The majority of these approaches utilize either texture or color features of images. However, in most cases, only the texture or the color features are not sufficient for superior segmentation. Our proposed approach addresses this issue and employs a bi-level segmentation approach integrating both types of features. This approach uses color histogram-based texture feature in the first level of segmentation. In the second level, a multi-objective evolutionary algorithm is applied to the generated segmented images to produce the final set of non-dominated segmented results by optimizing three objectives simultaneously. The proposed approach is able to partition test images in a number of segments consistent with human visual perception. In term of quantitative evaluation, our approach also provides better results than existing approaches as shown in experimental results.
[bi-level multiobjective image segmentation, Pareto-front, bi-level multiobjective evolutionary image segmentation approach, Color, texture-based color features, Observers, texture feature, Indexes, color histogram, Biological cells, image texture, multiobjective evolutionary algorithm, Image segmentation, Histograms, evolutionary computation, Image color analysis, Multi-objective image segmentation, image segmentation, evolutionary optimization, partition test images, image colour analysis, color feature, bi-level segmentation approach]
A novel encryption model for text messages using delayed chaotic neural network and DNA cryptography
2017 20th International Conference of Computer and Information Technology
None
2017
In this paper, a novel model for encrypting text messages using time varying delayed Hopfield neural network and a posterior DNA cryptographic model is proposed. The chaotic neural network applied here is used to generate a binary sequence which is later passed to a permutation function and generate the key for the first level encryption. The plaintext is converted to a corresponding binary sequence after a conversion to ASCI value and encrypted by switching of chaotic neural network maps and a permutation function which is dependent on the binary sequence generated from the chaotic neural network. An additional DNA cryptographic model is used over the cipher text obtained from the first level encryption to robust the security of the proposed model.
[Hopfield neural nets, electronic messaging, Encryption, Hopfield neural network, Time varying delay, Trajectory, chaotic neural network maps, ASCI value, binary sequence, Chaotic neural network, biocomputing, chaos, encryption model, cryptography, posterior DNA cryptographic model, text messages, delayed chaotic neural network, synchronisation, cipher text, Chaotic communication, Text message encryption, Neural networks, DNA, DNA cryptography, binary sequences]
A machine learning approach on latent semantic analysis for ambiguity checking on Bengali literature
2017 20th International Conference of Computer and Information Technology
None
2017
The Ambiguity of words is one of the most complex things in any language. Many algorithms had been worked on to solve this distinguishing problem in English literature. But in the Bengali literature, Text summarizing using LSA has been worked on already. But ambiguity checking is mostly new. Here we 1 assessed a few papers on different calculations that executed on English dialect and after writing audit, we saw that vector space model is more productive and we utilized that. We initially made a Bengali lexicon with profoundly utilized Bengali expressions of various significance. Later we enlisted all the stop words for Bengali dialect and deducted those as the presence of these stop words causes many-sided quality and recurrence on executing the calculation. At that point, we surrounded a Bengali Stemmer. It was the most difficult assignment as Bengali dialect has countless linguistic structure and separating these, we made a Stemmer that gives around 89.76% exactness on Bengali dialect. From that point onward, we actualized LSA utilizing Vector Space in Bengali dialect. With our Stemmer, VSM gives generally reasonable outcome for Bengali language structure and sentence structure. For uncertainty checking, this is so far the primary endeavor in Bengali writing.
[Bengali writing, text analysis, Dictionaries, Uncertainty, Bengali lexicon, computational linguistics, ambiguity checking, English literature, Bengali stemmer, Bengali dictionary, Stop Words, Bengali dialect, Plagiarism, Semantics, sentence structure, Mathematical model, learning (artificial intelligence), Distinguishing, machine learning approach, natural language processing, English dialect, latent semantic analysis, Vector Space Model, Support vector machines, Bengali literature, Bengali expressions, vector space, Ambiguity, Bengali Stemmer]
Crime news analysis: Location and story detection
2017 20th International Conference of Computer and Information Technology
None
2017
It is always a good choice to consider multiple sources when analyzing crime. Newspaper can be a good source of crime analysis as it always contains the massive amount of data related to any particular subject but the fact is they are not structured enough to make a definite decision. In this paper, we represent a system to analyze crime news from online newspaper using different data mining techniques. These techniques help to extract useful pieces of information about crime from unstructured crime news. The analysis will provide the definite information about a news whether it is a crime related news or not, crime location, similarity and grouping crime news as stories based on a crime news.
[online newspaper, crime news analysis, Crawlers, data mining, information retrieval, location detection, Data mining, machine learning, Support vector machines, police data processing, crime location, information extraction, Databases, Law enforcement, story detection, data mining techniques, crime analysis, Feature extraction, Libraries, crime news grouping, unstructured crime news, crime related news]
Web service performance enhancement for portable devices modifying SOAP security principle
2017 20th International Conference of Computer and Information Technology
None
2017
Nowadays most of the wed applications are bound to Service Oriented Architecture (SOA) [1], [11], [14] such as - inter-operable web services, and their performance and security greatly depend on the backend structure of web service. Web Service has two layers of security - message level security (WS-Security) and transport level security [18]. In WS-Security, embedding of security modules within SOAP message increases the line of code which lengthens the response time and thus decreases the overall performance of web services [13], [20]. Our experiments on different web services showed that, among three security modules of WS-Security of SOAP message, authentication module requires processing time the most, in Table I. Another essential part of web service is WSDL and security for WSDL is emphasized by few researchers recently as it is open to access and vulnerable for presenting SOAP messaging information [1], [4], [13]. Restricted access of WSDL file can play a strong role to increase the authentication and overall security of web service as in [4]. In this paper, we have proposed a new model to improve web service performance modifying the security principle of SOAP message and tethering WSDL file to facilitate authentication. Experimental results on our model have shown significant improvements in the line of code and response time over the traditional approach without compromising any WS-Security module's contrivances (Authentication, Signature, and Encryption).
[Performance evaluation, SOAP security principle, Encryption, Simple object access protocol, WS-Security, Web service performance, Message level security, SOAP messaging information, service-oriented architecture, encryption module, WSDL access, authentication module, transport level security, signature module, cryptography, inter-operable web services, web service performance enhancement, security modules, security of data, Web services, Authentication, XML, SOAP message, WSDL file, service oriented architecture, Time factors, WS-Security module]
A machine learning approach for stylometric analysis of Bangla literature
2017 20th International Conference of Computer and Information Technology
None
2017
The term Stylogenetics refers to the eloquent analysis of authors literary corpora which are based on clustering. While writing, a writer focuses on some frequent things subconsciously. We1 focused on these things and tried to detect the affinity and divergence of the writing of different authors. In this approach, our proposal is regarding on some particular features to distinguish authors individuality who writes and establishes their own viewpoint on similar issues. Here we assembled Bengali Blogs scripted by twenty Bangladeshi authors of two different fields e.g. Political, Educational and analyzed the corpus. Via our methodology, we evaluated some features such as negative Word frequency in particular position, Rapidity of use of highest length word and sentence, Suffix Count, Use of particular Punctuation, Common Recognizable word frequency, Classification of Parts of speech, Numeric words frequency and so on. First, we trained the system using these features and then distinguished from random data sets using two machine learning approaches, Support Vector Machines (SVM) and Naive Bayes classifier. This proposal provides more accuracy than previously established works as all the collected corpus here, are of different writers writing, on the analogous field.
[text analysis, Bangla literature, authors individuality, Numeric words frequency, SVM, Analogous, Proposals, Bangladeshi authors, particular Punctuation, literature, negative Word frequency, Common Recognizable word frequency, learning (artificial intelligence), sentence, pattern classification, machine learning approach, support vector machines, natural language processing, eloquent analysis, Blogs, Bengali Blogs, stylometric analysis, Clustering, Distinguish, Support vector machines, Computer science, Naive Bayes, Stylogenetics, Speech recognition, Machine learning, Writing, Affinity, Frequency, Bayes methods, authors literary corpora, Principal component analysis]
An initial centroid selection method based on radial and angular coordinates for K-means algorithm
2017 20th International Conference of Computer and Information Technology
None
2017
Clustering is a technique for dividing a set of similar objects into same groups and dissimilar objects into different groups. Among different clustering algorithms, the K-means algorithm is considered as the most popular due to its simplicity. However, the outcome from the K-means algorithm is highly sensitive to the initial centroid selection. As a consequence, the selection of initial centroids in the K-means algorithm plays a crucial part in accuracy and efficiency. To select the initial centroids more effectively, in this paper, we propose a new method based on radial and angular coordinates. To check the feasibility of the proposed method, we compare our method with the standard K-means algorithm. For the comparison, we use synthetic data sets with different size of instances and number of clusters. The experiment shows that in most of the cases the proposed method clearly dominates over the standard K-means algorithm in terms of execution time and required number of iterations.
[Algorithm design and analysis, angular coordinates, Euclidian coordinate system, Partitioning algorithms, Classification algorithms, Standards, dissimilar objects, K-means algorithm, pattern clustering, clustering algorithms, Clustering algorithms, Approximation algorithms, n-spherical coordinate system, Mathematical model, synthetic data sets, initial centroid selection method, k-means algorithm, radial coordinates, initial centroid selection]
A short term wind speed forcasting using SVR and BP-ANN: A comparative analysis
2017 20th International Conference of Computer and Information Technology
None
2017
Forecasting wind speed is a very important part in weather forecasting. Because of the nonlinear behaviors of nature and climate changes, wind speed prediction becomes a challenging task, particularly country like Bangladesh where lots of areas are costal and season changes in frequent. This study is done to make an attempt to predict the wind speed using two very potential and wide frames of statistical data mining and machine learning approaches; Support Vector Regression (SVR) and Artificial Neural Network (ANN) with back propagation technique. 7years (2008-2014) historical dataset of wind speed of Chittagong costal area were collected from Bangladesh meteorological division (BMD) for undertaking the experiment. Leaky ReLu function was applied as the rectifier to the input data to control the thresholds and activations of neurons in MLP. The aim of this study was to propose a model that can predict short term wind speed with maximum accuracy. Finally, after considerable amount of experimentations the outcome from this study is; our proposed SVR and ANN models are able to predict wind speed with more than 99% accuracy in short term prediction. Moreover, ANN can outperform SVR in some situations with highest 99.80% accuracy. But SVR models are best suited for overall wind speed forecasting in different horizons with highest 99.60% accuracy. These results outperform the performances of previous recent works that are mentioned in literature review and reference sections of this paper.
[Support Vector Regression (SVR), data mining, regression analysis, Predictive models, SVR, Wind speed forecasting, Training, Bangladesh meteorological division (BMD), weather forecasting, Wind speed, learning (artificial intelligence), Kernel, BP-ANN, statistical data mining, statistical learning, support vector machines, Neurons, wind speed forecasting, Artificial neural networks, Support Vector Regression, geophysics computing, machine learning, Artificial Neural Network (ANN), Support vector machines, Artificial Neural Network, backpropagation, neural nets, Bangladesh meteorological division]
A support vector machine mixed with statistical reasoning approach to predict movie success by analyzing public sentiments
2017 20th International Conference of Computer and Information Technology
None
2017
Wisdom of Crowds is often considered a very powerful tool for predicting anything. In this paper we explore the power of public sentiments on predicting the success of movies. In short, we differentiated between positive and negative comments using Support Vector Machine and then use Statistical Reasoning to predict movie success. We used non linear RBF kernel for our sentiment classifier which achieved better accuracy than the classifiers that use linear kernels in the famous IMDB Movie Review Dataset (89.51% accuracy) and also in the Pang and Lee Movie Review Dataset (86.86% accuracy). Using our system we can predict whether a movie will be successful or not with an accuracy of 90.3%. We also compared our approach with other authors in the literature.
[Wisdom of Crowds, linear kernels, entertainment, TF Threshold, Feature Matrix, SVM, Sparse matrices, Machine Learning, Training, statistical reasoning approach, Polarity Score, radial basis function networks, Motion pictures, non linear RBF kernel, Kernel, sentiment classifier, negative comments, pattern classification, public sentiment analysis, support vector machines, nonlinear RBF kernel, RBF Kernel, Statistical Reasoning, positive comments, Bigram, movie success, Indexes, Support vector machines, IMDB Movie Review Dataset, support vector machine, TF-IDF, Feature extraction, social networking (online), statistical analysis, Uni-gram]
A probabilistic approach for obtaining an optimized number of services using weighted matrix and multidimensional scaling
2017 20th International Conference of Computer and Information Technology
None
2017
While migrating from Monolith to Microservices architectural style, it is often confusing to define service boundaries. This paper describes a probabilistic technique to define the services on a Monolithic system considering three major aspects - update rate, scaling rate, and technology diversity necessary to realize each service properly. To do so, we have used the weighted matrix and Multidimensional Scaling (MDS). This technique helps to define an optimal number of services for implementing the system in the Microservices Architecture. This paper will be useful for organizations trying to migrate from Monolithic System to Microservices.
[optimized number, probability, Probabilistic logic, Information technology, probabilistic approach, Monolith architectural style, Weighted Matrix, matrix algebra, Monolithic System, Multidimensional Scaling, service boundaries, Microservices Architecture, Computer architecture, Organizations, Feature extraction, Software, weighted matrix, multidimensional scaling, Microservices architectural style, service-oriented architecture]
Imputation of missing healthcare data
2017 20th International Conference of Computer and Information Technology
None
2017
In the field of data mining, missing values has always been a crucial factor. Incorrect imputation of missing values could lead to inaccurate research as well as wrong predictions. Developing a generalized imputation strategy that can be used across a variety of dataset is very hard. Because each dataset has its attributes and characteristics, and finding another dataset of similar property could be a very hard task. In Bangladesh, real healthcare data is very noisy which makes the knowledge discovery by health researchers very difficult. One of the main sources of noise in healthcare data is missing values. In this paper, we presented a general model for the imputation of any kind of missing data. We also implemented three different algorithms namely Amelia, FURIA, and MICE in real healthcare dataset to impute missing values. Experimental results on 65000 patient records show that MICE algorithm performs better among the three.
[health data, data mining, medical information systems, generalized imputation strategy, missing data, healthcare data, Bangladesh, incorrect imputation, healthcare dataset, patient records, health care, MICE algorithm, missing value imputation]
Design and implementation of microcontroller based assistive robot for person with blind autism and visual impairment
2017 20th International Conference of Computer and Information Technology
None
2017
Blindness is a more severe problem among the disabilities of Human. It is difficult to lead a normal life for blind like a sighted person. Because they cannot feel their surroundings. So, most of the blind peoples require travel aids to travel freely in an unknown environment. Researchers were invented several devices to make independent navigation for blinds. But most of them are invented for a specific task or the devices do not cost friendly. To gain independent navigation, a device with more features is required. For this purpose, &#x201C;Design and Implementation of a Blind Assistive Robot&#x201D; is designed to fulfill the goal. The device eliminates the requirement of human assistance for blind while traveling outside. It is made of reliable parts and has a relatively low cost compared to industrial bots. Every part of the device are simulated and tested. The device can run by any operator and requires low power. Arduino Mega (ATMega2560) used as the processor, ultrasonic sensors used to detect obstacles, stair, and hole, IR sensors used to sense line. Two dc motors are connected through a full bridge motor driver L98 module to navigate. A buzzer and a vibration motor used to give notification to the user. The device will follow a predefined line and will follow the wall, will detect an obstacle, hole and stair and gives sound notification to the user. The device will act as an assistant to the blind peoples.
[handicapped aids, Arduino Mega processor, blind peoples, blind, microcontroller based assistive robot, blind autism, ultrasonic sensors, Acoustics, blind assistive robot, mobile robots, IR sensors, Vibrations, sighted person, assistive robot, visual impairment, Robot sensing systems, ATMega2560, independent navigation, microcontrollers, travel aids, DC motors, sensors, human assistance, Pins, Arduino]
Efficient route selection in ad hoc on-demand distance vector routing
2017 20th International Conference of Computer and Information Technology
None
2017
The protocol diversities of mobile ad hoc have already got hold of the field to a peak of a matured and developed area. Still, the restraint of delay and bandwidth of mobile ad hoc network have kept a little room to draft a routing protocol for the pursuit of providing quality of service. In the paper, we proposed protocol namely Efficient Route Selection in Ad Hoc On-Demand Distance Vector Routing. We select the best path among multiple paths from source to destination using covariance and delay. We consider the delay, link stability and energy to devise a covariance-based metric to discover the most balanced path. We also propose a metric for the selection of a node that acts as a local backup node for the most vulnerable nodes on the selected path. We accomplish our implementation in NS3and it shows the more reliable path and less end to end delay than other counterpart protocols.
[mobile ad hoc network, link breakage, link stability, routing protocol, Routing, covariance, quality of service, Mobile ad hoc networks, ad hoc on-demand distance vector routing, delays, mobile ad hoc networks, routing protocols, telecommunication network reliability, Routing protocols, Delays]
Cloud-POA: A cloud-based map only implementation of PO-MSA on Amazon multi-node EC2 Hadoop Cluster
2017 20th International Conference of Computer and Information Technology
None
2017
Sequence alignment in bioinformatics and compu-tational biology has always been a challenging task. With Next Generation Sequencing (NGS) techniques in hand, researchers are now capable of studying biological systems at a level never been possible before. Scientists now have billions of bytes of biological data to work with, trillions of sequences to align. But this comes at a cost of requiring computing machines having a tremendous amount of computational and analytical power. Purchasing this huge amount of hardware and setting up a standalone infrastructure would not only cost an unnecessarily massive amount of money and labor but also would become troublesome to maintain. Moreover, for aligning a huge number of DNA or Protein sequences a scalable multiple sequence alignment (MSA) algorithms is needed with decent accuracy. In such context, this paper presents a novel implementation of Partial Order Alignment (POA) algorithm on a multi-node Hadoop Cluster running on MapReduce framework. The implementation was done in Amazon AWS platform with multiple EC2 instances. It is a map-only implementation with Hadoop Streaming. The result of this implementation shows a drastic reduction in runtime with no accuracy degradation.
[computational biology, Cloud computing, public domain software, Amazon multinode EC2 Hadoop Cluster, Partial Order Alignment algorithm, Mapreduce, biological systems, parallel processing, cloud-POA, MSA algorithms, Proteins, multiple EC2 instances, Runtime, Amazon AWS platform, Clustering algorithms, Distributed databases, proteins, scalable multiple sequence alignment algorithms, cloud computing, Bioinformatics, Next Generation Sequencing techniques, data analysis, computing machines, POA, biological data, Software algorithms, huge number, Hadoop Streaming, NGS techniques, Hadoop, analytical power, pattern clustering, multinode Hadoop Cluster, bioinformatics, computational power, data handling, NGS, PO-MSA]
A simple acute myocardial infarction (Heart Attack) prediction system using clinical data and data mining techniques
2017 20th International Conference of Computer and Information Technology
None
2017
Acute Myocardial Infarction (Heart Attack), a Cardiovascular Disease (CVD) leads to Ischemic Heart Disease(IHD) is one of the major killers worldwide. A proficient approach is proposed in this paper that can predict the chances of heart attack when a person is bearing chest pain or equivalent symptoms. We have developed a prototype by integrating clinical data collected from patients admitted in different hospitals attacked by Acute Myocardial Infarction (AMI). 25 attributes related to symptoms of heart attack are collected and analyzed where chest pain, palpitation, breathlessness, syncope with nausea, sweating, vomiting are the prominent symptoms of a person getting heart attack. The data mining techniques namely decision tree and random forest are used to analyze heart attack dataset where classification of more common symptoms related to heart attack is done using c4.5 decision tree algorithm, alongside, random forest is applied to improve the accuracy of the classification result of heart attack prediction. A guiding system to suspect the chest pain as having heart attack or not may help many people who tend to neglect the chest pain and later land up in catastrophe of heart attacks.
[Heart, data mining, Ischemic Heart Disease, heart attack prediction, Data Mining, random forest, Data mining, clinical data, Random Forest, decision tree, Pain, data mining techniques, Decision trees, learning (artificial intelligence), Cardiovascular Disease (CVD), heart attack dataset, Cardiovascular Disease, pattern classification, cardiovascular system, Decision Tree, Cardiac arrest, palpitation, diseases, medical information systems, acute myocardial infarction prediction system, Heart Attack, chest pain, decision trees, Myocardium, Androids, breathlessness, Smartphone]
A comparison between Support Vector Machine (SVM) and bootstrap aggregating technique for recognizing Bangla handwritten characters
2017 20th International Conference of Computer and Information Technology
None
2017
This paper represents the optical character recognition for Bangla handwritten characters using the popular classifier SVM and Bootstrap Aggregating technique. The segmentation process in Bangla is difficult because of complex letters and &#x201C;Matra (top horizontal line)&#x201D; in the words. For the feature extraction method there was no particular algorithm found, which was efficient enough, so in this experiment the Hog feature extraction and Binary pixel feature extraction methods were used. Hog features and Binary pixel features were combined for the proposed system. To recognize a character Support Vector Machine (SVM) and Bootstrap Aggregating were used. Experimental results for the SVM classifier and Bootstrap aggregating shows 100% accuracy for trained characters and for random untrained characters, SVM classifier shows accuracy about 89.8% and for the Bootstrap Aggregating method the accuracy is 93%.
[image classification, Segmentation, Matra, feature extraction method, Feature Extraction, object detection, SVM, Hog feature extraction, optical character recognition, feature extraction, image segmentation, Hog features, learning (artificial intelligence), SVM classifier, random untrained characters, Bangla handwritten characters, Bootstrap Aggregating, handwritten character recognition, binary pixel feature extraction methods, support vector machines, Character recognition, Optical character recognition software, Information technology, extraction methods, Support vector machines, bootstrap aggregating technique, Image segmentation, Handwriting recognition, support vector machine, classifier SVM, Bangla Optical Character Recognition, Feature extraction]
Novel venus photonic crystal fiber structure with low loss over S&#x002B;C&#x002B;L&#x002B;U wavelength bands and large negative dispersion
2017 20th International Conference of Computer and Information Technology
None
2017
In the era of modern optical communication, a new types of fiber known as Photonic Crystal Fiber (PCF) has entered as an emerging research area with its unique and splendid optical properties and design flexibility. PCF structures are composed of an arrangement of holes where different position or diameter of these holes is liable for different optical properties. In the past study, researchers have proposed several regular or irregular structures with different shapes like Hexagonal, Octagonal, Circular, Spiral, Honeycomb etc. due to its enormous design flexibility. In this research, a novel PCF structure of Venus geometry is proposed and optical properties like confinement loss and dispersion are extracted. For design and simulation purpose of the proposed structure, COMSOL Multiphysics software is used. For better light confinement in any cladding holes, defect is created using perturb material and low confinement loss is obtained within S+C+L+U wavelength bands. Variations in cladding holes made the proposed fiber to exhibit large negative dispersion within the wavelength range 1.5pm to 1.7pm This structure would be a promising candidate for long distance secure data transmission, Wavelength Division Multiplexing (WDM), ass Dispersion Compensating Fiber (DCF) and other telecommunication applications.
[negative dispersion, regular structures, wavelength division multiplexing, holey fibres, PCF structure, cladding holes, optical fibre losses, Photonic crystal fibers, Optical fiber dispersion, Optical losses, light confinement, venus photonic crystal fiber structure, Optical fiber communication, S+C+L+U wavelength bands, simulation purpose, Wavelength Division Multiplexing, confinement loss, Optical fibers, PCF: dispersion, optical fibre cladding, optical fibre communication, COMSOL Multiphysics software, multiple defects, irregular structures, modern optical communication, Dispersion Compensating Fiber, optical properties, Venus geometry, wavelength 1.5 pm to 1.7 pm, venus structure, optical fibre dispersion, photonic crystals, long distance secure data transmission]
Link prediction by correlation on social network
2017 20th International Conference of Computer and Information Technology
None
2017
In a social network, the topology of the network grows through the formation of the link. the connection between two nodes in a social network indicates a confidence in terms of the similarity of some activities. Generally, a new link in the social network is created from different perspectives such as familiarity, cohesiveness, geographical locations etc. The concept of the link in the social network has been utilized to discover the hidden meaning of different fields such as e-commerce, bioinformatics and information retrieval. The prediction of a new link between two nodes in the social network is normally accomplished based on the nature of the topology and the similarity function among the nodes is defined with the help of the number of common friends. In this paper, we propose two link prediction algorithms: Local Link Prediction Algorithm and Global Link prediction by taking into consideration of user's activities as well as the common friends. We apply two formulas called correlation based cScore and influential score based iScore to measure the similarity between the two predicted nodes. Finally, we analyze the performance of the proposed algorithms by using DBLP, PPI, PB, and USAir data sets and the experimental result attests that our link predicted algorithm outperforms over the existing algorithms.
[correlation based cscore, Correlation, local link prediction algorithm, e -commerce, data mining, Node Activities, network theory (graphs), complex networks, Local Link Prediction, social network, Social Network, Global Link prediction algorithm, LLPA, Network topology, USAir data set, Prediction algorithms, Influential Score, Mathematical model, learning (artificial intelligence), influential score based iscore, Social network services, information retrieval, Topology, network topology, Global Link Prediction, Link Prediction, Organizations, bioinformatics, social networking (online), computational complexity, correlation methods, GPLA]
Exploring human emotion via Twitter
2017 20th International Conference of Computer and Information Technology
None
2017
Sentiment analysis or opinion mining on twitter data is an emerging topic in research. In this paper, we have described a system for emotion analysis of tweets using only the core text. Tweets are usually short, more ambiguous and contains a huge amount of noisy data, sometimes it is difficult to understand the user's opinion. The main challenge is to feature extraction for the purpose of classification and feature extraction depends on the perfection of preprocessing of a tweet. The preprocessing is the most difficult task, since it can be done in various ways and the methods or steps applied in preprocessing are not distinct. Most of the researches in this topic, have been focused on binary (positive and negative) and 3-way (positive, negative and neutral) classifications. In this paper, we have focused on emotion classification of tweets as multi-class classification. We have chosen basic human emotions (happiness, sadness, surprise, disgust) and neutral as our emotion classes. According to the experimental results, our approach improved the performance of multi-class classification of twitter data.
[opinion mining, Sentiment analysis, emotion classes, bag of words, data mining, Twitter, Task analysis, emotion recognition, POS tag, twitter, multiclass classification, emotion analysis, feature extraction, naive bayes, pattern classification, unigram, sentiment analysis, tweet, classification, emotion classification, Support vector machines, twitter data, noisy data, Tagging, Feature extraction, social networking (online), human emotion, core text]
Performance analysis of OpenFlow based software defined wired and wireless network
2017 20th International Conference of Computer and Information Technology
None
2017
Software Defined Networking (SDN) brings reliability, flexibility, scalability, security and advance features in the present era of networking and communication. As the demand for Wired and wireless network grow rapidly, the consistency of the network shrinks quickly and becomes harder to continue. Moreover, the smart devices are getting popular from users and thus more reliable, scalable and flexible wired and wireless network is required. In such consequences, SDN emerges as a new network paradigm for the future Internet and Next Generation Network (NGN). This paper analyzes different performance metrics of an OpenFlow based Software Defined Wired and Wireless Network using a single OpenFlow controller. The OpenFlow technology is convenient to the SDN application that enables OpenFlow controller to control multiple hardware or softwares. The proposed OpenFlow based Software Defined Wired and Wireless Network is executed in a single controller in Mininet-WiFi programmed Python script for real life experience. The QoS performance metrics such as throughput, Round Trip Time (RTT), delay and jitter is analyzed. The results ensure the consistency of the proposed OpenFlow based Software Defined Wired and Wireless Network and paves the way to implement integrated network.
[access point, OpenFlow controller, Ports (Computers), OpenFlow, software defined networking, Throughput, wireless network Software Defined Networking, Servers, Software defined networking, Open vSwitch, Wireless networks, Mininet-WiFi, Python script, wired network Software Defined Networking, Software, wireless, computer network reliability, wireless LAN, SDN, OpenFlow technology]
Histopathological breast-image classification with image enhancement by convolutional neural network
2017 20th International Conference of Computer and Information Technology
None
2017
Finding malignancy from Histopathological images is always a challenging task. So far research has been carried out to classify Histopathological images using various techniques and methods. Recently, the state-of-the art Convolutional Neural Network (CNN) has largely been utilized for natural image classification. In this paper, using the advancement of CNN techniques, we have classified a set of Histopathological Breast images into Benign and Malignant classes, which can save doctors and physicians time and also allow patients a second opinion about the disease.
[histopathological breast-image classification, image classification, natural image classification, Task analysis, Specificity, Convolution, image enhancement, feature extraction, Lighting, Classification, malignant classes, Recall, Kernel, benign classes, medical image processing, Convolutional Neural Network, CNN techniques, convolutional neural network, Retinex Filter, diseases, image texture, image representation, histopathological images, Feature extraction, cancer, Convolutional neural networks, neural nets, Cancer]
Explorer-0100: An autonomous next generation Mars rover
2017 20th International Conference of Computer and Information Technology
None
2017
In the field of planetary exploration, rovers play a vital role. This paper highlights about the design and development of advanced Mars rover &#x201C;Explorer-0100&#x201D;. The rover successfully participated in European Rover Challenge - 2016. The rover can be operated remotely and autonomously in the rough terrain of Mars. It has an operation range of 1 km radius and can perform various tasks assigned to it. This paper discusses the system architecture, power distribution system, communication system, rover control system and components of the rover briefly. Moreover, we have tried to enlist the capability to complete the tasks which have been assigned to it by providing test data set. Finally, we have discussed our progress to use this rover in different aspects of our society. During an earthquake or any other sorts of natural calamity, a smaller version of it can assist human in the rescue operation.
[Actuators, astronomical instruments, planetary exploration, Wheels, rover control system, Batteries, mobile robots, Task analysis, Robotics, Space vehicles, Mars, Rocker-bogie Suspension System, rescue operation, Autonomous, autonomous next generation Mars rover, Human Assistant, European Rover Challenge, power distribution system, natural calamity, Explorer-0100, Mars Rover, planetary rovers, Bridge circuits, ERC, advanced Mars rover, size 1.0 km, planetary surfaces]
Design of a low cost and convenient Hadoop application for extracting consumer behavior
2017 20th International Conference of Computer and Information Technology
None
2017
Access Log of an e-commerce website contains the movement of the consumers. This movement carries valuable information about the needs, choices and purchase history of the consumers. This information can be extracted to influence the purchase of the consumers through appropriate ads, offers and discounts. Moreover, this information has interesting correlations with various business parameters, which helps to make a strong prediction for business intelligence. To make these insights more sensible, log data should be as big as possible. It makes big data analytics suitable for such application. Hadoop is an open source platform for big data analysis and can be used for the benefit of the e-commerce companies. But without proper tools like Pig, Hive etc, Hadoop applications are difficult to access. In this article, a unique and convenient system called Command Line Interface for Consumer Behavior is presented to solve this particular problem. One can easily find the consumer behavior using the access logs with this system. It has used command line feature of Hadoop to pass the IP or Hostname to extract consumer behavior.
[consumer behaviour, Data mining, competitive intelligence, Task analysis, business intelligence, command line interface for consumer behavior, consumer behavior, e-commerce, e-commerce companies, hadoop, e-commerce website, big data, access log, electronic commerce, business parameters, data analysis, consumer choices, big data analytics, Big Data, Tools, consumer purchase history, marketing data processing, web log, Hadoop application, Consumer behavior, Information and communication technology, consumer needs, consumer behavior extraction, big data analysis]
Bangladeshi hand sign language recognition from video
2017 20th International Conference of Computer and Information Technology
None
2017
Hand sign recognition has become a significant research topic as it requires voiceless communication with sign languages in many circumstances. In Bangladesh, some system has been established for hand signs recognition for bangle alphabets. In case of expressing the feelings of deaf and dumb people there are no alternative to words and sentences. In this regard framework has proposed a system for recognizing the Bangladeshi hand sign language of words and sentences and translating the signs into text with static hand images which have been generated from input videos. In our proposed system, RGB images which contain hand signs are firstly converted into YCbCr color space for skin color segmentation. After that hand portions which are our region of interest (ROI) have been obtained by eliminating other skin portions and Local Binary Pattern (LBP) has been applied on it for feature extraction. Finally, for classifying candidate features Support Vector Machine (SVM) is performed. To demonstrate the total effectiveness of this method experimental result have compared with other existing work related to Bangladeshi hand sign language. The experimental results denote that the proposed method outperforms the existing works. The system has achieved recognition accuracy of 94.26% for words and 94.49% for sentences.
[Bangladeshi hand sign language recognition, Local Binary Pattern (LBP), support vector machines, hand sign recognition, Assistive technology, image classification, Gesture recognition, Support vector machines, Image segmentation, hand images, Image color analysis, sign languages, feature extraction, image segmentation, YCbCr color space, Feature extraction, Skin, image colour analysis, hand portions, video signal processing, SVM classifier, sign language recognition, skin color segmentation]
A simple way of image encryption using pixel shuffling and pixel manipulation
2017 20th International Conference of Computer and Information Technology
None
2017
Encryption is an important matter for which secured transmitting is taking place in every aspect of life. Validity and security can't be imagined in any aspect of communication without it. There are several methods for encrypting images like as chaos based encryption, stenography, watermarking etc. Chaotic encryption is comparatively popular schemes and many studies have covered this sector. But upgrade is still possible because of transmission properties, control parameter, computational complexity, industrial aspects, and so on. This study has shown an easy way to perform a complex encryption using simple logistic map. Variable parameter, robustness and easy to implement are core outcome of this study. All of this has done in two core steps where first step is for manipulating the pixel value and second step is for pixel position shuffling. For adding more strength 128 bit encryption scheme has been included.
[image processing, pixel position, chaos based encryption, Correlation, cryptography, Encryption, image encryption, pixel shuffling, Information technology, watermarking, Histograms, Pixel manipulation, security, word length 128.0 bit, pixel manipulation, pixel value, Streaming media, chaotic encryption, Logistics, stenography, transmission properties, computational complexity, logistic map]
Bangla handwritten numeral character recognition using directional pattern
2017 20th International Conference of Computer and Information Technology
None
2017
Handwritten character recognition has become a challenging and interesting field in the recent days due to its complex character shapes and huge pragmatic applications. A lot of research is already done and underway on English alphabets and numerals recognition. But in case of Bangla, even being the fifth largest spoken language in the world, has not undergone that much research. Besides, different complex shape of Bangla character makes the recognition more challenging. In this paper, we propose a directional pattern approach for feature extraction of Bangla numeric characters which attains a high accuracy of recognition. We use Local Directional Pattern (LDP) and Gradient Directional Pattern (GDP) for feature extraction and then two well-known machine learning algorithms, K-Nearest Neighbour (KNN) and Support Vector Machine (SVM), to classify the numeric character. We also ensemble the pattern oriented results to enhance the accuracy. Experimental results on the benchmark dataset CMATERdb 3.1.1 demonstrates an astounding recognition rate of accuracy 95.62% without preprocessing the data.
[Bangla Character Recognition, handwritten character recognition, Bangla Digit Recognition, Economic indicators, support vector machines, Bangla numeric characters, image classification, natural language processing, Character recognition, Bangla handwritten numeral character recognition, Directional Pattern, Support vector machines, Bangla character, directional pattern approach, Local Directional Pattern, Handwriting recognition, Databases, feature extraction, Feature extraction, Gradient Directional Pattern, learning (artificial intelligence)]
An efficient successive authentication scheme for vehicular ad-hoc networks
2017 20th International Conference of Computer and Information Technology
None
2017
VANET (Vehicular Ad-hoc NETwork) is a network architecture designed to provide road safety, improve transport efficiency, and reduce traffic congestion by combining vehicles with high mobility and fixed road-side units with or without Vehicular Cloud. Connected vehicles aided with sensors need to be authenticated before transmitting message. Two major problems in existing state-of-the-art schemes which are redundant authentication of vehicles with each RSUs it passes through and authenticating each message by trusted authority with large cryptographic overheads. In this paper we proposed an authentication scheme which exploits a very new technique of successive message passing among different RSUs (Road-side Units) to verify vehicles which are already authenticated. In our scheme, RSUs share authentication related information of vehicles with the help of newly proposed RSUs detection algorithm. In addition to that, messages from vehicles are verified with assistance from RSUs using symmetric key based Hashed Message Authentication Code (HMAC) signatures. This results in remarkably reduced cryptographic overheads and communication delay occurred during authentication of vehicles and messages in VANETs. Extensive performance evaluations and comparison with state-of-the-art schemes will show how our scheme provides exceptionally improved performance for VANETs.
[telecommunication security, vehicular ad-hoc networks, Electronic mail, road safety, Privacy, Vehicular Ad-hoc Network, Vehicular Cloud, traffic congestion, Road-side units, Cryptography, Hashed Message Authentication Code signatures, vehicular ad hoc networks, transport efficiency, message passing, mobile radio, efficient successive authentication scheme, Successive Authentication, VANET, cryptography, network architecture, successive message passing, Authentication, Vehicular ad hoc networks, message authentication, redundant authentication, Delays, Vehicular Ad-hoc NETwork]
A system for recommending colleges to HSC appointees using slope one algorithm
2017 20th International Conference of Computer and Information Technology
None
2017
With the expansion of population, the growing number of students are inducing an increase in the number of educational institutions in Bangladesh. Every year, lots of students, after passing their SSC examination, are in a quandary to choose a suitable college. Is it possible to mitigate their tribulation by helping them to choose their suitable college? Is there any prospect of building a recommender system that will suggest a list of suitable college to a student? What are the facts that can be used to measure the effectiveness of a college for a student? In this study, we worked to build such a recommender system. We figured out some facts that are efficacious to construct a sorted list of colleges according to their suitability for a particular student. To build the recommender system, user-based and item-based collaborative filtering methods have been used. Students are categorized according to their performance in SSC examination, group, gender, spatial information etc.
[collaborative filtering, SSC examination, HSC appointees, College recommendation, educational administrative data processing, item-based collaborative filtering, Education, slope one algorithm, Decision trees, Recommender systems, Recommender system, college recommendation, suitable college, Slope one algorithm, user-based collaborative filtering, Computer science, recommender systems, Bangladesh, recommender system, Rating based recommendation, Collaboration, Feature extraction, Data models, educational institutions, Deviation, Deviation Matrix]
Isolated Bangla handwritten character recognition with convolutional neural network
2017 20th International Conference of Computer and Information Technology
None
2017
The handwritten character recognition (HCR) problem has been studied extensively during the last few decades with varying level of success. Although one of the earliest optical character recognition work was done using an artificial neural network, due to low computational speed and other computational resource constraints, researchers had to move away to formulate the HCR problem of different languages (including Bangla) using the hand crafted features based classification methods. The recent progress of deep learning technologies and significant developments of parallel computing hardware have established a solid platform for the researchers in producing state-of-the-art reliable performance in many fields. Even though deep learning is proving its applicability in different computer vision problems since AlexNet was proposed in 2012, due to lack of significantly large Bangla handwritten character dataset, Bangla HCR could not progress far. Recently, a few initiatives have been taken to build large scale Bangla handwritten character datasets and made them available for public use. In this paper, we propose a modified ResNet-18 architecture (a convolutional neural network architecture) in recognizing Bangla handwritten characters. The proposed method is applied to two recently created isolated Bangla handwritten datasets. The used datasets are relatively large and practical to apply deep learning. Using the proposed method, we achieve the state-of-the-art recognition performance.
[artificial neural network, Image recognition, linguistics, image classification, deep learning technologies, handwritten character recognition problem, computational resource constraints, parallel computing hardware, Task analysis, computer vision problems, optical character recognition, feature extraction, isolated Bangla character recognition, Bangla HCR, Computer architecture, learning (artificial intelligence), HCR problem, Bangla handwritten character dataset, Convolutional Neural Network, handwritten character recognition, natural language processing, convolutional neural network, Character recognition, Optical character recognition software, Handwriting recognition, Machine learning, computer vision, ResNet-18 architecture, neural nets, text detection]
Design a lightweight low power multiplier circuits using quantum dot cellular automata architecture
2017 20th International Conference of Computer and Information Technology
None
2017
In this paper, we propose a powerful multiplier circuit using QCA 3-dot cell. The architecture is very competent to calculate the multiplication between two numbers. Here, we use a 4-bit 3-dot QCA adder to construct our proposed circuit, which has less number of cells and area from other QCA based adders. [1,2,3] Our new and novel design is improved a lot from existing multipliers, the suggested multiplier circuit enhances 28% on cell, 99% on area than the current finest known one.
[multiplying circuits, Full Adder, Quantum dots, Inverters, 3-Dot, QCA, cellular automata, quantum dots, adders, Wiring Logic, Microprocessors, Computer architecture, DH-HEMTs, Logic gates, Multiplier, QCA 3-dot cell, quantum dot cellular automata architecture, low-power electronics, QCA based adders, Adders, logic design, multiplier circuits]
Improving reliable cloud computing environment using fuzzy logic
2017 20th International Conference of Computer and Information Technology
None
2017
Cloud computing technology has added a dynamic value with its dynamic features in the internet based system. Because of its emerging applied fields, it has become much popular technology. In this paper we used Fuzzy logic to calculate reliability in cloud computing environment. We've used Adaptability, Processor Speed and Performance in the input section. Using the Fuzzy system we've shown the Reliability of the system which is shown in the output section. We mainly focused on improving Reliability of the cloud computing environment.
[Cloud computing, Fuzzy system, input section, Quality of service, fuzzy logic, dynamic features, Security, Engines, Processor Speed, reliable cloud, Fuzzy logic, cloud computing environment, cloud computing technology, internet based system, Reliability, cloud computing, dynamic value, Fuzzy systems]
Gossip: A social interest based routing algorithm for pocket switched network
2017 20th International Conference of Computer and Information Technology
None
2017
Nodes of a sparsely distributed and an erratically connected network like Mobile Ad hoc Network (MANET) and Delay Tolerant Network (DTN) has no prior knowledge of node connectivity and suffers greatly from the lack of stable infrastructures. Thus selecting an effective message forwarding algorithm is of prime concern for efficient message propagation and resource sharing. To overcome the boundaries due to these limitations, user device's mobility pattern can be exploited to avail faster message transmission, while contributing to energy conservation and establishing effective communication. When implemented using the wireless technologies, such algorithms can provide a wide application domain in scenarios such as communication in disasters or war affected zones, where common communication methods are not approachable. In this paper, a novel approach named &#x201C;Gossip&#x201D; has been proposed. It works through forming social profiles for each individual hand-held device carriers while depending on their social interests; and afterwards chooses an aptest hop node relying on that social profile for successful message delivery.
[Algorithm design and analysis, message transmission, packet switching, Switches, social interest, mobility management (mobile radio), Pocket Switched Network, Message forwarding algorithm, social interest based routing algorithm, mobile ad hoc networks, pocket switched network, mobility pattern, Social Interest, Transient analysis, effective message forwarding algorithm, efficient message propagation, wireless technologies, Mobile Ad hoc Network, node connectivity, successful message delivery, Routing, delay tolerant networks, aptest hop node, Delay Tolerant Network, social profile, Photography, Computer science, PSN challenges, MANET, Gossip, telecommunication network routing, resource sharing, hand-held device carriers, PSN application domain, Delays, DTN]
Detection and analysis of hijab based on visual feature of neck and hair
2017 20th International Conference of Computer and Information Technology
None
2017
In the recent decade facial feature and hijab detection is an important task for human computer interaction. To detect and recognize the women in all aspects hijab detection is a vital issue as hijab covers many important features in face such as ear, head and face's corner portions. In this regard, a hijab detection framework is proposed in this work. For that, initially, the face and eyes regions are detected by using efficient Viola-Jones cascade classifier. After that, the facial and it's connected surrounding face color region (FCR) is extracted by YCbCr color model. As hair is located upper surrounding portion of the FCR, the left, right and top FCR border points are estimated by utilizing eyes bounding box's left, right height's center point and bounding box center point. This eyes bounding box center point is also used to estimate the bottom neck point (NP) from FCR and chin point (CP) from bottom facial region. Furthermore, the y coordinate of the NP and CP is used to confirm the neck cover. And the left, right and top FCR border points are used to confirm the hair cover. Finally, based on the neck and hair cover the hijab is detected. The proposed method is tested using various hijab as well as non-hijab images and results are presented to demonstrate the efficiency and effectiveness.
[YCbCr color model, virtual reality, NP, hijab detection, image classification, eyes regions, object detection, chin point, visual feature, hair cover, Image color analysis, feature extraction, Skin color, face recognition, neck point, ViolaJones cascade classifier, Neck, image colour analysis, Face, box center point, Hair, YCbCr, Viola-Jones cascade classifier, Face detection, hijab detection framework, CP, Human computer interaction, FCR, computer vision, Feature extraction, Skin, clothing, human computer interaction, face color region]
Motor imagery classification using subband tangent space mapping
2017 20th International Conference of Computer and Information Technology
None
2017
This paper presents a novel approach of motor imagery (MI) classification using subband implementation of tangent space mapping (TSM). The multichannel electroencephalography (EEG) signals are decomposed into five subbands. The sample covariance matrix (SCM) of individual subband is projected to tangent space using TSM yielding the tangent features. Thus obtained features space has a high dimension. The principle component analysis (PCA) is employed to reduce the dimension of the feature space based on the p-value of one-way ANOVA. The classification is performed by support vector machine (SVM) with reduced dimension. The experimental results with publicly available datasets show that the proposed method significantly improves the performance motor imagery classification.
[reduced dimension, multichannel electroencephalography signals, motor imagery classification, brain-computer interfaces, image classification, Electroencephalography, high dimension, SVM, Covariance matrices, MI classification, Manifolds, features space, feature extraction, principle component analysis, subband implementation, Robustness, one-way ANOVA, covariance matrices, electroencephalography, Symmetric matrices, subband decomposition, support vector machines, feature space, subband tangent space mapping, tangent features, EEG, sample covariance matrix, signal classification, TSM, Support vector machines, medical signal processing, support vector machine, Brain computer interface (BC), Feature extraction, electroencepahlogram (EEG), statistical analysis, principal component analysis]
CrowdsouRS: A crowdsourced reputation system for identifying deceptive online contents
2017 20th International Conference of Computer and Information Technology
None
2017
In recent years, accelerated web-based technologies have revolutionized content generation and broadcast mecha-nisms through the Internet. Social media, blogs, e-newspaper, auction sites facilitate the creation and exchange of user-generated contents, which rarely go through any fact-finding mechanism or rigorous editorial process. This has fuelled the creation and publication of fake news in the web. The proliferation of social networks has been exploited to accelerate the distribution and propagation of such fake news at an unprecedented level, creating a major concern for the web. There have been several efforts undertaken to rectify this problem, unfortunately, none seems to be effective to root out this concerning issue. In this paper, we present CrowdsouRS, a Crowd-sourced Reputation System, implemented as a browser extension, for the web that leverages the wisdom of the crowd to identify and tag deceptive online contents. It aggregates reputation scores for a web page from multiple users, which is then visualized in order to help other users to determine if the contents of the web page are deceptive. We have evaluated the usability and effectiveness of CrowdsouRS with a number of users and our evaluations suggest that users find the tool useful in serving its purpose.
[fact-finding mechanism, Crowd sourcing, Servers, data aggregation, e-newspaper, crowdsourced reputation system, Fake news, Engines, Uniform resource locators, broadcast mechanisms, Databases, Browser extension, content generation, Reputation, Facebook, social media, fake news, electronic commerce, crowdsourcing, accelerated web-based technologies, social networks, web page, CrowdsouRS, Browsers, user-generated contents, Trust, multiple users, deceptive online content identification, reputation score aggregation, Web pages, social networking (online), Internet, Web sites, auction sites]
A framework for enhancing Bangla document images printed on newsprint papers
2017 20th International Conference of Computer and Information Technology
None
2017
The process of modifying images to achieve improved resultant images is called image enhancement. Though there are some researches done on image enhancement of Bangla document images but research on enhancement of newsprint Bangla document images is almost an untouched field of research. So for enhancing Bangla newsprint document images we have divided our whole working procedure into three phases. At the first phase, we have tried to explore the main reason behind the failure of well known image enhancement techniques on newsprint papers by running out analytic experimentation. In second phase we proposed a tentative framework by applying commonly used mathematical morphology method in a modified way. But due to huge diversity of Bangla letters this approach also did not provide the expected output. Finally, in the third phase we have proposed a framework addressing the issues of the second phase, where we have used bitmap color substitution method along with the well known Sauvola's algorithm. To enhance the result more we have then modified Sauvola's algorithm to fill missing edges along the horizontal and vertical lines.
[Image recognition, 8-connectivity, natural language processing, document image processing, threshold, Optical character recognition software, Task analysis, resultant images, image enhancement techniques, binarization, Bangla newsprint document images, dilation, Image color analysis, image enhancement, Morphology, erosion, mathematical morphology, Colored noise, Image enhancement]
Impact of social networking sites on post-partum depression in women: An analysis in the context of Bangladesh
2017 20th International Conference of Computer and Information Technology
None
2017
Postpartum Depression (PPD) refers to moderate or severe depression in a woman after childbirth. It is strikingly common in new mothers from all regions of the world with a prevalence of around 10-15%. PPD can have severe adverse effects on maternal and child health, such as suicidal tendency of the mother, infanticide as well as poor cognitive and developmental growth of the child. Despite this, few women seek medical attention due to ignorance, negligence and financial limitations; the latter is especially true for those who live in developing countries. Nowadays, social networking sites (SNS) e.g., Facebook can act as accessible and effective tools for the prevention and treatment of PPD. In this paper, we analyze the opinions and awareness level of Bangladeshi people about PPD and impact of using SNS during postpartum period on reducing PPD based on our survey (N = 93). We also discuss possible SNS-based interventions and design implications that can effectively and feasibly reduce PPD in women in developing countries.
[PPD treatment, Pediatrics, Blogs, Tools, Social networking sites, post-partum depression, Pregnancy, Privacy, Bangladesh, patient treatment, women's health, social networking (online), SNS, PPD prevention, postpartum depression, obstetrics, maternal well-being, Facebook, medical computing, financial limitations, social networking sites]
DB2KB: A framework to publish a database as a knowledge base
2017 20th International Conference of Computer and Information Technology
None
2017
In the current `Web of Documents' architecture, computers work as a broker of information. Data are published in this web as human understandable form and most of them are stored in databases. Semantic Web technology was introduced with a vision of converting current `Web of documents' to `Web of data' where data are published and exchanged in a machine-readable and understandable format. Moreover, data are linked with other data so that relevant data can be gathered and reasoned. Although databases provide efficient data storage and retrieval through SQL, they are not concerned about semantics of data and hence are not suitable for active inference and reasoning. In this paper, we present a framework called DB2KB. DB2KB converts a database to a knowledge base. It uses Semantic Web constructs to capture the semantics of data in the knowledge base. It also establishes links between local knowledge base and external knowledge bases. We describe a use case and show how our system can be used to solve the problem specified in the use case. Different evaluation techniques show that our system solves the problem of the use case with satisfactory performance.
[Vocabulary, data retrieval, reasoning, Resource description framework, human understandable form, local knowledge base, data storage, Databases, Semantics, knowledge based systems, DB2KB, Semantic Web constructs, Knowledge Base, Semantic Web technology, document handling, semantic Web, Knowledge based systems, OWL, database publishing, machine-readable format, inference mechanisms, SQL, RDB2RDF, Semantic Web, Web of Documents architecture, Web of data, Data models, active inference, external knowledge bases]
Reduction of gesture feature dimension for improving the hand gesture recognition performance of numerical sign language
2017 20th International Conference of Computer and Information Technology
None
2017
A major form of non-touch human-computer interaction (HCI) is hand gesture recognition. This is one of the appealing ways to interact with computers and a natural part of how we communicate. However, as a part of HCI, human hand gesture recognition is a challenging issue. From this point of view, this paper presents an effective hand gesture recognition system with hand feature selection for low cost video acquisition device. In this proposed model, hand features are extracted from video frame using discrete wavelet transformation and singular value decomposition. A genetic algorithm with effective fitness function is used to select optimal hand features by eliminating redundant and irrelevant features for improving the recognition performance. Finally, support vector machine is used to recognize the hand gestures for numerical hand gesture accuracy of American Sign Language. The proposed model is validated using a constructed hand gesture dataset. The proposed model is compared with non-feature selection based models, where the feature selection-embedded model outperforms the traditional hand recognition process.
[American Sign Language, fitness function, hand gestures, Human-computer-interaction (HCI), genetic algorithm, gesture recognition, feature extraction, video frame, hand recognition process, video signal processing, feature selection, singular value decomposition, video acquisition device, human hand gesture recognition, numerical hand gesture accuracy, support vector machines, Assistive technology, discrete wavelet transforms, constructed hand gesture dataset, hand gesture recognition system, Gesture recognition, hand feature selection, Webcam, Discrete wavelet transforms, genetic algorithms, hand gesture recognition performance, nonfeature selection based models, HCI, Human computer interaction, Support vector machines, human-computer interaction, Hand Gesture Recognition, support vector machine, Hidden Markov models, optimal hand features, Feature extraction, human computer interaction, numerical sign language, gesture feature dimension, Feature Dimension Reduction]
Towards developing an intelligent system to suggest optimal path based on historic and real-time traffic data
2017 20th International Conference of Computer and Information Technology
None
2017
Traffic congestion is a common scenario in the metropolitan areas specially in developing countries like Bangladesh where people lose valuable time of their busy schedule by getting trapped in heavy traffic. Moreover, reliable traffic congestion avoidance or prediction mechanism for providing real-time traffic jam information and route selection is not up to the mark in Bangladesh. In this paper, we have proposed an intelligent system with a cost function using Ant Colony Optimization (ACO) and a meta-heuristic approach, which will calculate optimal paths of lowest travel cost considering both historic and real time traffic data and different time windows of a day. It will also dynamically re-route the path in case of heavy congestion during travel time for avoiding unusual situations. Experimental results show that the designed algorithm of the proposed system performs accordingly with reliable realtime traffic prediction and it's suggested routes provide better navigation and may save valuable time.
[lowest travel cost, optimal paths, Roads, Heuristic algorithms, optimal path, meta-heuristic, meta-heuristic approach, different time windows, route selection, heavy traffic, cost function, Prediction algorithms, Cost function, Real-time systems, traffic congestion, Mathematical model, ant colony optimisation, busy schedule, Meteorology, traffic prediction, Ant Colony Optimization, road traffic, metropolitan areas, intelligent transportation systems, traffic engineering computing, historic time traffic data, Bangladesh, travel cost, reliable realtime traffic prediction, intelligent system, travel time, real-time traffic jam information, real-time traffic data, reliable traffic congestion avoidance]
RansHunt: A support vector machines based ransomware analysis framework with integrated feature set
2017 20th International Conference of Computer and Information Technology
None
2017
Ransomware is one of the most increasing malwares used by cyber-criminals in recent days. This type of malware uses cryptographic technology that encrypts a user's important files, folders makes the computer systems unusable, holds the decryption key and asks for the ransom from the victims for recovery. The recent ransomware families are very sophisticated and difficult to analyze &amp; detect using static features only. On the other hand, latest crypto-ransomwares having sandboxing and IDS evading capabilities. So obviously, static or dynamic analysis of the ransomware alone cannot provide better solution. In this paper, we will present a Machine Learning based approach which will use integrated method, a combination of static and dynamic analysis to detect ransomware. The experimental test samples were taken from almost all ransomware families including the most recent &#x201C;WannaCry&#x201D;. The results also suggest that combined analysis can detect ransomware with better accuracy compared to individual analysis approach. Since ransomware samples show some &#x201C;run-time&#x201D; and &#x201C;static code&#x201D; features, it also helps for the early detection of new and similar ransomware variants.
[Algorithm design and analysis, invasive software, Heuristic algorithms, Dynamic analysis, individual analysis approach, Electronic mail, static features, Cyber-crime, RansHunt, sandboxing, static code features, feature extraction, Static analysis, integrated feature set, Malware, Cryptography, learning (artificial intelligence), malwares, ransomware analysis framework, support vector machines, static analysis, ransomware variants, cryptography, crypto-ransomwares, dynamic analysis, Ransomware, ransomware families, Hybrid cryptosystem, Machine learning, Feature extraction, ransomware samples]
Named entity classification using dependency grammar
2017 20th International Conference of Computer and Information Technology
None
2017
Named Entity (NE) classification is a task to classify words or group of words in a sentence into some predefined classes like, Person, Organization, Location etc. It is used in different natural language processing (NLP) tasks like machine translation, information retrieval, text annotation, question answering, summarization etc. To classify the Named Entity, researchers use different techniques like rule based entity sorting, statistical class modeling, linear chaining etc. However, most of those techniques do not capture grammatical dependencies of words hidden in the sentence, which is very essential to classify a Named Entity. In our research, we consider grammatical dependencies of words hidden in the sentence and use them to learn a classification model which classifies the Named Entity. In a classification task among three different classes, average performance gain of the proposed system over the state of the art system exceeds more than 6% in F1 Score.
[dependency grammar, text analysis, computational linguistics, Task analysis, word processing, NLP tasks, Semantics, words classification, Natural language processing, Contextual Dependency, learning (artificial intelligence), Dependency Grammar, named entity classification, pattern classification, natural language processing, NE classification, Probability, Grammar, natural language processing tasks, classification task, classification model learning, grammars, Named Entity Classification, Organizations, Logic gates]
Application for integrating microcontrollers to Internet of Things
2017 20th International Conference of Computer and Information Technology
None
2017
Internet was introduced to the common people over two decades ago and over the years we have witnessed its massive growth into the mass population through personal computers, laptops and cell phones. Until recently, the Internet was a commodity of the people, i.e. end users of the Internet were primarily humans. In the past few years we have witnessed that the end users of the Internet are no longer only people but devices as well. With the advent of IPv6 addressing, it is possible to connect countless devices to the Internet with unique IP addresses. Thus scientists and engineers have been able to hookup all kinds of devices to the Internet, a concept known as Internet of Things (IoT). Another domain of recent interest is the use of microcontrollers to build various home and laboratory projects. In this paper we propose a model that will allow home grown engineers as well as professional engineers to easily control their microcontroller based devices globally via Internet.
[commodity, IPv6 addressing, common people, Servers, countless devices, IoT, home grown engineers, Microprocessors, Sensors, IP networks, laptops, microcontrollers, cell phones, laboratory projects, mass population, Microcontrollers, Raspberry pi, professional engineers, massive growth, personal computers, Internet of Things, unique IP addresses, microcontroller based devices, Embedded systems, end users, Internet IPv6, scientists, Internet, humans, Arduino]
Rhythmic component retrieval from EOG artifact of electroencephalography signals
2017 20th International Conference of Computer and Information Technology
None
2017
Electroencephalography (EEG) signal has been widely used to recognize the intention and cognition in brain computer interface (BCI) applications. Non-Cerebral signals like electro-oculogram (EOG) artifacts with higher energy suppress important low energy EEG components. It is required to completely remove these artifacts without loss of EEG data. This paper presents multivariate empirical mode decomposition (MEMD) based filtering approach to suppress the unwanted artifacts. The multichannel EEG signal is decomposed into a finite number of band limited signals called intrinsic mode functions (IMFs). A subband thresholding is implemented to separate artifacts using the IMFs with fractional Gaussian noise (fGn) as reference signal. Thus separated artifact includes some EEG components. Bandpass filtering is implemented to extract the EEG signal from artifact obtained by energy based thresholding. The experimental results conducted with real EEG signals show the effectiveness of the proposed method.
[Electrooculography, Empirical mode decomposition, electro-oculography, brain-computer interfaces, Filter banks, Electroencephalography, NonCerebral signals, signal denoising, electroencephalography (EEG), electro-oculogram artifacts, Bandpass filter, Gaussian noise, multivariate empirical mode decomposition, EOG artifact, unwanted artifacts, Market research, multichannel EEG signal, electroencephalography, filtering approach, cognition, brain computer interface applications, empirical mode decomposition (EMD), electroencephalography signal, rhythmic component retrieval, fractional Gaussian noise, medical signal processing, brain computer interface, electro-oculogram (EOG)]
A machine learning approach to predict movie box-office success
2017 20th International Conference of Computer and Information Technology
None
2017
Predicting society's reaction to a new product in the sense of popularity and adaption rate has become an emerging field of data analysis. The motion picture industry is a multi-billion-dollar business, and there is a massive amount of data related to movies is available over the internet. This study proposes a decision support system for movie investment sector using machine learning techniques. This research helps investors associated with this business for avoiding investment risks. The system predicts an approximate success rate of a movie based on its profitability by analyzing historical data from different sources like IMDb, Rotten Tomatoes, Box Office Mojo and Metacritic. Using Support Vector Machine (SVM), Neural Network and Natural Language Processing the system predicts a movie box office profit based on some pre-released features and post-released features. This paper shows Neural Network gives an accuracy of 84.1% for pre-released features and 89.27% for all features while SVM has 83.44% and 88.87% accuracy for pre-released features and all features respectively when one away prediction is considered. Moreover, we figure out that budget, IMDb votes and no. of screens are the most important features which play a vital role while predicting a movie's box-office success.
[Industries, profitability, entertainment, motion picture industry, movie industry, movie box-office success, Electronic mail, Support Vector Machine, SVM, machine learning techniques, Box Office Mojo, multibillion-dollar business, neural network, cinematography, predicting society, investment risks, Motion pictures, Rotten Tomatoes, Metacritic, learning (artificial intelligence), data analysis, support vector machines, natural language processing, sentiment analysis, Natural Language Processing, investment, emerging field, adaption rate, IMDb votes, machine learning, movie box office profit, decision support systems, Computer science, Support vector machines, approximate success rate, Neural Network, historical data, support vector machine, Neural networks, forecasting theory, movie investment sector, decision support system, Feature extraction, Internet, neural nets]
Sentiment analysis for Bangla sentences using convolutional neural network
2017 20th International Conference of Computer and Information Technology
None
2017
Sentiment analysis, also known as opinion mining or emotion analysis, is a process to determine emotional reaction of people towards an interaction or an event. An opinion may be positive, negative or neutral depends on individuals judgment or evaluation towards a topic or an event. Usually sentiments can be varied in cultures and languages. On sentiment analysis, an extensive amount of research works can be seen for well-resourced languages like English, Japanese etc. However, such works are relatively less observed for low-resourced language like Bangla. In this paper, we propose a framework that analyzes sentiments from texts written in Bangla. In our proposal, we use Bangla comments and generate a classification model. The model is generated by a neural network variance called Convolutional Neural Network. The classifier model obtains a classification accuracy of 99.87%, which is 6.87% better than the available state-of-the art Bangla sentiment classifier.
[opinion mining, Sentiment analysis, Vocabulary, pattern classification, Convolutional Neural Network, Bangla sentiment classifier, Neurons, sentiment analysis, data mining, convolutional neural network, emotional reaction, Bangla, Generators, Bangla text, Sentiment Analysis, classification model, Bangla comments, Convolution, Semantics, convolution, Convolutional neural networks, Bangla sentences, feedforward neural nets]
A noble color-texture hybrid method for content-based image retrieval
2017 20th International Conference of Computer and Information Technology
None
2017
In this paper, a color and texture hybrid framework CCLBM is proposed for content based image retrieval. The purpose of this study was to construct a method for image retrieval that is relatively simple in computational complexity and still maintains good retrieval results. The proposed color-texture hybrid method CCLBM combines Color Coherence Vector for color information and Local Binary Pattern for texture information. The method was tested and benchmarked on COREL-1K and COREL-5K image databases in terms of Average Retrieval Precision (ARP) and Average Retrieval Rate (ARR). The results show that CCLBM scores 82.52% and 67.13% in terms of Average Retrieval Precision (ARP) in COREL-1K and COREL-5K databases respectively (for top 10 similar image retrieval).
[color information, color coherence vector, texture information, Image retrieval, Average Retrieval Precision, noble color-texture hybrid method, content-based retrieval, image texture, Histograms, Image color analysis, Color Coherence Vector, local binary pattern, Coherence, image retrieval, Feature extraction, color-texture hybrid method CCLBM, Average Retrieval Rate, image colour analysis, content based image retrieval, content-based image retrieval]
Survey of domain specific languages to build packet parsers for industrial protocols
2017 20th International Conference of Computer and Information Technology
None
2017
The advent of the industrial automation due to the fourth industrial revolution (Industry 4.0) in industrial sectors is shifting the focus of industrial systems, such as Programmable Logic Controllers (PLC), Distributed Control Systems (DCS), etc. inside a production network towards Internet of Things (IoT). In the production network, these industrial systems communicate with each other by exchanging data packets. Due to this shift, the traditional industrial systems which were not connected to the Internet are now transformed into `smart factories', where these systems are able to communicate via the Internet. However, making the industrial system online also imposes threats of being attacked by malware and viruses. Industrial automation needs security measures, such as Intrusion Detection System (IDS), to analyze the data packets in order to protect the industrial systems and production network from these threats. These data packets contain headers of industrial protocols, such as Modbus and Profinet. Present approaches such as hand-written parsers face some challenges to parse these packets. Therefore, this paper sheds some light on some of these approaches to be able to make parser development simple, fast, and easy to maintain considering especially the industrial protocols. Parsers for several protocols such as Modbus, Profinet, OPC UA, and S7comm have been developed and evaluated. Evaluation of the implementation shows that these protocols can be parsed correctly and efficiently. This output of this parser is being used as inputs of an Industrial IDS system.
[Industries, invasive software, production engineering computing, packet parsers, factory automation, production industries, production network, data packet analysis, Production, specification languages, industrial automation, domain specific languages, Routing protocols, industrial protocols, Automation, smart factories, malware, Distributed Control Systems, Internet of Things, Information technology, Domain specific languages, grammars, Industrial IDS system, industrial system, Internet]
Security analysis of IEEE 802.21 standard in software defined wireless networking
2017 20th International Conference of Computer and Information Technology
None
2017
Software Defined Networking (SDN) is the best choice in establishing a software controlled inter-domain network. Convergence of different Wireless link technologies bring the mobile users to choose the network being in any geographical location. IEEE 802.21 is such a standard for exchanging networking information for connecting with the network being at any region in the world. Integrated with SDN wireless network this functionality of IEEE 802.21 standard can discover programmable network services with profound resource utilization. However, the information exchange should circulate through a reliable source. Hence, the security analysis of IEEE 802.21 Media Independent Handover (MIH) mechanism for Software Defined Wireless Network (SDWN) is the primary concern of this research work. This study, conducts architectural and functional analysis of MIH integrated with SDWN interface for mobility management of the wireless nodes. The outcome specifies a possible integration with future deployment opportunities in information exchange of IEEE 802.21 MIH for programmable network devices.
[telecommunication security, Software Defined Wireless Network, networking information, functional analysis, mobility management (mobile radio), mobility management, Security, Communication system security, Software Defined Wireless Networking (SDWN), Wireless networks, SDN wireless network, IEEE 802.21 MIH, Handover, software controlled inter-domain network, programmable network services, security analysis, Network Security Analysis, programmable network devices, IEEE 802.21 Media Independent Handover mechanism, SDWN interface, software defined networking, Handover Security, MIH, Wireless link technologies, information exchange, Handover Management, wireless nodes, Software Defined Networking, Manganese]
The voice enabled stick
2017 20th International Conference of Computer and Information Technology
None
2017
Now a day many visually impaired people face many problems when they walk on the roads or streets. These people need a device which is used in avoiding obstacle and helping in navigation. This paper introduces a novel embedded system device called voice enabled stick (VES) which is used to help the visually impaired people to navigate among the obstacles. This device is comprised of a long handle and three ultra-sonic sensors mounted at the end to extract the distance from the nearest obstacle. The output of the stick is in the form of voice to a head phone which gives commands to the user (visually impaired-person) to move right, turn left, and go straight or to stop. The user hears the voice commands from the device and navigates through the obstacles easily without any prior knowledge on the device. In this paper past existing devices have reviewed and the parameters related to these devices and VES are discussed. The improvement in different existing devices for blind people, their limitations and their advantages and disadvantages are examined. Comparison of different existing sticks with our proposed device is also recognized. Challenges issues and difficulties related to propose VES needs to be overcome have been highlighted.
[ultra-sonic sensors, voice commands, handicapped aids, Navigation, obstacle navigation, VES, pedestrians, Acoustics, Path planning, object detection, novel embedded system device, Sensor arrays, active guidance, navigation, Voice controlled module, visually impaired pedestrians, Embedded system, embedded systems, Dogs, blind people, Hardware, intelligent sensors, voice enabled stick, Ultra-sonic sensors]
Optimization of selected parameters of the forest growth model Biome-BGC (version ZALF) using HOPSPACK
2017 20th International Conference of Computer and Information Technology
None
2017
Biome-BGC model is widely used for estimating C, N, and water budgets in terrestrial ecosystems. Like other complex forest models, Biome-BGC contains many eco-physiological and site specific parameters. Estimation of these parameters is critical to Biome-BGC model applications. Many techniques are currently available to parameter estimation including derivative-free inverse optimization techniques. In this work, we tried to optimize some selected Biome-BGC parameters using a derivative-free optimization tool, HOPSPACK, for 27 ICP Forests Level II plots. Rather than using a single objective function, we used nine frequently used objective functions. We concluded that intensive care is required during parameter optimization using such a derivative-free optimization tool because the outcome might be sensitive to the initial parameter values and the sequence of optimization of parameters. Inclusion of more objective functions increased robustness of the outcome.
[version ZALF, derivative-free optimization tool, Optimization, ecology, optimisation, Iterative closest point algorithm, Biome-BGC model applications, parameter estimation, objective function, geophysical techniques, Biological system modeling, Computational modeling, Biome-BGC parameters, Tools, geophysics computing, Linear programming, water budgets, Biome-BGC, Calibration, terrestrial ecosystems, forest growth model, inverse optimization techniques, HOPSPACK, forestry, complex forest models]
Scalable micro-service based approach to FHIR server with golang and No-SQL
2017 20th International Conference of Computer and Information Technology
None
2017
Fast Health Interoperability Resource (FHIR) for Electronic Health Record (EHR) not only opens a new era for health-care systems to exchange data between themselves but also allows other various systems following the same framework to communicate between them. FHIR is engaging and evolving rapidly and it is creating the need of massive data storage, retrieval and transfer requirements. For a server that implements this framework, has to be agile to cope with the growing and massive data handling. E-health-care data also seem to grow proportional to time. So, an ideal FHIR Server has to be scalable to fit this demand over time. Several different tools could help store, anonymize, analyze and extract data because there are no single tools to get all those things done. In this work, we propose a scalable, agile, and reliable Micro-service based Architecture for FHIR server that is highly available with the help of Document Oriented Database and helps to store health-care data. The proposed architecture is fast responsive. It gives our system the flexibility to use different tools that implement FHIR Framework interface in such way that the whole system could be made vertically scalable and the system can perform better in terms of time complexity compared to a Monolithic based implementation of the framework. Experimental results show that the proposed model can be highly used as an alternative to currently available FHIR system implementation.
[open systems, Scalability, massive data handling, health-care systems, Document Oriented Database, FHIR, Servers, Interoperability, scalability, Electronic Health Record, REST API, Databases, FHIR Framework interface, Health-Care system, Benchmark testing, e-health-care data storage, health care, Business, scalable microservice, data analysis, NoSQL databases, Tools, electronic health records, Logic gates, micro-service, Fast Health Interoperability Resource server]
Back to the basics: Read critically, reflect prudently and write analytically
2017 20th International Conference of Computer and Information Technology
None
2017
Over the last couple of years, there has been a tremendous progress in Computer Science education research, techniques and new pedagogies. Although, such research shows an increase in student learning, retention and engagement; recently, we have been seeing students struggling with basic skills, such as critical reading and analytical writing. This problem is exacerbated, when faculty skip certain course content because of time limit or lack of student preparation or to satisfy other student learning outcomes. Therefore, students are not exposed to content which may expose them to higher order thinking as required by critical reading and analytical writing. This paper presents a technique that addresses both of these issues by creating reflective writing assignments, which attempts to improve students' fundamental skills along with exposing them to more advanced topics.
[computer science education, student learning outcomes, course content, IEEE Sections, time limit, Programming, Reflection, analytical writing, critical reading, Computer Science education research, higher order thinking, basic skills, Operating systems, Computer architecture, Writing, student preparation, textual analysis, reflective writing, HPC, fundamental skills, reflective writing assignments]
Bengali speech recognition: A double layered LSTM-RNN approach
2017 20th International Conference of Computer and Information Technology
None
2017
Speech recognition may be an intuitive process for humans, but it turns out to be intimidating to make computer automatically recognize speeches. Although recent progresses in speech recognition have been very promising in other languages, Bengali lacks such progress. There are very little research works published for Bengali speech recognizer. In this paper, we have investigated long short term memory (LSTM), a recurrent neural network, approach to recognize individual Bengali words. We divided each word into a number of frames each containing 13 mel-frequency cepstral coefficients (MFCC), providing us with a useful set of distinctive features. We trained a deep LSTM model with the frames to recognize the most plausible phonemes. The final layer of our deep model is a softmax layer having equal number of units to the number of phonemes. We picked the most probable phonemes for each time frame. Finally, we passed these phonemes through a filter where we got individual words as the output. Our system achieves word detection error rate 13.2% and phoneme detection error rate 28.7% on Bangla-Real-Number audio dataset.
[Phonemes, Bengali speech recognizer, Error analysis, double layered LSTM-RNN approach, Mel frequency cepstral coefficient, softmax layer, intuitive process, Training, phoneme detection error rate, speech recognition, individual words, RNN, Bengali speech recognition, cepstral analysis, deep LSTM model, learning (artificial intelligence), MFCC, phonemes, Bangla-real-number audio dataset, ASR, recurrent neural network, LSTM, natural language processing, recurrent neural nets, speeches, final layer, Hidden Markov models, Speech recognition, long short term memory, Logic gates, individual Bengali words, Speech, mel-frequency cepstral coefficients, word detection error rate]
A novel technique for recognition and tracking of moving objects based on E-MACH and proximate gradient (PG) filters
2017 20th International Conference of Computer and Information Technology
None
2017
Recognition and Tracking of Images is still one of the most sought after areas in the field of Image Processing mainly because of applications and presentations associated with it. Recognition of objects in a crowded and occluded environment is very challenging task as all the other objects besides the object of interest acts as noise. Other challenges associated with recognition and Tracking is the ever changing coordinates of the object during the tracking procedure. In this paper both the above mentioned challenges have been addressed using a novel technique which merges a very efficient recognition and tracking procedures. First recognition of image is done using modified Maximum Average Correlation Filter (MACH) which will identify the two dimensional coordinates of the object if interest in a secluded environment. The coordinates are then updated using a Proximal Gradient (PG) filter which uses Particle Filter as for systematic and periodic updation of images recursively. Proximal Gradient Filter uses the random variables and their posterior distribution for efficient prediction of coordinates of object of interest. End result is very efficient and fast tracking of object in subsequent frames.
[Algorithm design and analysis, object recognition, Correlation, MACH filter, Tracking, particle filtering (numerical methods), Proximal Gradient filter, image filtering, object detection, particle filter, Training, Image Processing, Filtering algorithms, object tracking, Particle filters, Mathematical model, gradient methods, Target tracking, PG filter, image motion analysis, Maximum Average Correlation Filter, image recognition, Recognition, Proximal Gradient, correlation methods]
Exhaustive study of essential constraint satisfaction problem techniques based on N-Queens problem
2017 20th International Conference of Computer and Information Technology
None
2017
Constraint Satisfaction Problem (CSP) is observed in various applications, i.e., scheduling problems, timetabling problems, assignment problems, etc. Researchers adopt a CSP technique to tackle a certain problem; however, each technique follows different approaches and ways to solve a problem network. In this exhaustive study, it has been possible to visualize the processes of essential CSP algorithms from a very concrete constraint satisfaction example, N-Queens Problem, in order to possess a deep understanding about how a particular constraint satisfaction problem will be dealt with by the studied and implemented techniques. Besides, benchmark results - time vs. value of N in N-Queens - have been generated from the implemented approaches, which help understand at what factor each algorithm produces solutions; especially, in N-Queens puzzle. Thus, extended decisions can be made to instantiate a real life problem within CSP's framework.
[Algorithm design and analysis, Visualization, Least Constrained Values (LCV), Minimum Remaining Values (MRV), essential CSP algorithms, essential constraint satisfaction problem techniques, Electronic mail, Forward Checking (FC), life problem, studied implemented techniques, scheduling, N-Queens puzzle, constraint handling, search problems, problem network, Constraint Satisfaction Problem (CSP), Filtering, scheduling problems, concrete constraint satisfaction example, constraint theory, particular constraint satisfaction problem, Arc Consistency (AC), N-Queens problem, exhaustive study, Backjumping Algorithm (BJ), CSP framework, Computer science, Backtracking Algorithm (BT), Maintaining Arc Consistency (MAC), constraint satisfaction problems, CSP technique, assignment problems, Concrete, timetabling problems, N-Queens Problem]
Evaluation of machine translation approaches to translate English to Bengali
2017 20th International Conference of Computer and Information Technology
None
2017
This paper describes the different types of machine translation (MT) approaches, where MT refers to the use of computers for the task of translating automatically from one language to another. It is highly challenging to build up a proper MT system which will works with full accuracy for translating foreign languages to native languages but this paper aims at providing a solution that could be helpful for building a MT system which will convert the English sentences into Bengali. Moreover, total 12 tenses such as-present indefinite, continuous, perfect, perfect continuous; past indefinite, continuous, perfect, perfect continuous; future indefinite, continuous, perfect and perfect continuous are used for the purpose of translating English sentence into Bengali that will require finding out the meaning from our own database. After comparing the experimental results based on different machine translation approaches with Google translator, it is found that one of our investigated as well as implemented methods, Corpus approach, provides higher accuracy in comparison with Google translator and other implemented methods.
[Algorithm design and analysis, Google, Dictionaries, natural language processing, Machine Translation, English sentences, Natural Language Processing, Google translator, Language Translation, Task analysis, Information technology, Bengali, MT system, Machine learning, machine translation, Natural language processing, Artificial intelligence, Corpus approach, language translation]
Product recommendation: A deep learning factorization method using separate learners
2017 20th International Conference of Computer and Information Technology
None
2017
Exponential growth in information has made it totally unimaginable to manually find a relevant product in a quick time, entailing the need for a mechanical recommendation system which would remember the users and recommend most suitable items. Most of the approaches for such machinery have been to first find similarity in users or in items, and then exploit these similarities to recommend the products. These methods produce better results when demographic information about users and items are given to them. In this paper, we propose a deep neural network model which does not require any information be given to it other than the rating triples. We created spurious user profiles and item characteristics by using separate learner weights at the bottommost layer. The weights in the upper layers took these information, created by the weights at bottommost layer, to produce a real valued rating. Our model produced an RMSE 4.1824 on Jester 4-million datasti, and this shows our deep network is comparable to the state of the art models.
[collaborative filtering, matrix factorization, latent factors, demographic information, product recommendation, production engineering computing, deep learning factorization method, Containers, Electronic mail, neural network, mechanical recommendation system, Training, Computer science, user profiles, recommender systems, Neural networks, deep neural network model, content-based filtering, Mathematical model, learning (artificial intelligence), Recommender systems, neural nets, machinery]
Bengali handwritten character recognition using deep convolutional neural network
2017 20th International Conference of Computer and Information Technology
None
2017
Handwritten character recognition is a nontrivial task as it seeks to recognize the correct class for user independent handwritten characters. This problem becomes even more challenging for a highly stylized, morphologically complex, and potentially juxtapositional characters comprising language like Bengali. As a result, the improvements over the years in Bengali character recognition are significantly less as compared to the other languages. In this paper, we propose a convolutional deep model to recognize Bengali handwritten characters. We first learnt a useful set of features by using kernels and local receptive fields, and then we have employed densely connected layers for the discrimination task. Our system has been tested on BanglaLekha-Isolated dataset. It achieves 98.66% accuracy on numerals (10 character classes), 94.99% accuracy on vowels (11 character classes), 91.60% accuracy on compound letters (20 character classes), 91.23% accuracy on alphabets (50 character classes), and 89.93% accuracy on almost all Bengali characters (80 character classes). Most of the errors incurred by our model in recognition task are due to extreme proximity in shapes among characters. A significant number of errors was caused by the mislabeled, irrecoverably distorted, and illegal data examples.
[Bengali compound characters, handwritten character recognition, local receptive fields, CNN, alphabets, deep convolutional neural network, Bengali handwritten character recognition, morphologically complex characters, Probability distribution, Character recognition, Compounds, Task analysis, potentially juxtapositional characters, Handwriting recognition, user independent handwritten characters, Bengali numerals, feature extraction, highly stylized characters, kernels, Feature extraction, Convolutional neural networks, neural nets, deep neural network]
Eye monitored device for disable people
2017 20th International Conference of Computer and Information Technology
None
2017
Technology is all about serving mankind. People are getting needy for technology as the time is clicking. Technologists love to break obstacles; it is what they are doing for ease of homo sapiens. In addition to it we have also lodge a model to engineer modish thing with technology. Statistics suggests that there are many cases of paralyzed people noticed every year including people suffering from locked in syndrome; is a medical mode in which most of the body muscles are paralyzed except the movement of eyes. Our project strive to mould the life of such people effortless, painless and manageable to re-establish the happiness, satisfaction, cheerfulness and self-possession of such people. Our paper put forward the execution of inexpensive support for disable people and to manufacture eye monitored device which controls the switching of any electrical devices / loads based on the movement of eyes. The paper demonstrates that communication is originated from the movement of eyes towards left, right, upper, lower and blinking of eyes monitored by MATLAB script which will then advice the relay wired with microcontroller to switch on / off and control the electrical loads through serial communication.
[handicapped aids, serial communication, Ports (Computers), user interfaces, Relays, MATLAB, Eye Monitoring, electrical devices, Viola Jones, eye monitored device, disable people, muscle, Face, control engineering computing, microcontrollers, medical mode, telemedicine, Microcontrollers, paralyzed people, modish thing, MATLAB script, gaze tracking, homo sapiens, disable, Object detection, Cameras, medical computing, serving mankind, Matlab]
A survey on emotion detection: A lexicon based backtracking approach for detecting emotion from Bengali text
2017 20th International Conference of Computer and Information Technology
None
2017
Emotion recognition ability has been introduced as a core component of emotional competence. Every emotion has different ways to be expressed such as text, speech, lyrics etc. This paper reflects the current experimental study and their outcomes on emotion detection from different textual data. In case of lexicon-based analysis, the position of emotional lexicons really varies the state of an emotion. In this empirical study, our focus was to find how people use the emotional keywords to express their emotions. We have presented an emotion detection model to extract emotion from Bengali text at the sentence level. In order to detect emotion from Bengali text, we have considered two basic emotion `happiness' and `sadness'. Our proposed model detects emotion on the basis of the sentiment of each sentence associated with it. A lexicon based backtracking approach has been introduced for recognizing the sentiments of sentences to show how frequently people express their emotion in the last part of a sentence. Proposed method can produce a result with 77.16 accuracies.
[text analysis, sentiment, Emotion detection, emotion extraction, bengali text, emotional competence, emotion recognition, emotional keywords, emotion detection model, Databases, feature extraction, lexicon based backtracking, sentence level, emotional lexicons, lexicon, Emotion recognition, natural language processing, Genetic expression, Blogs, lexicon-based analysis, Computer science, Supervised learning, Speech, emotion recognition ability, backtracking, Bengali text]
Range estimation in radar using maximum likelihood estimator
2017 20th International Conference of Computer and Information Technology
None
2017
RADAR technology is getting significance place in estimation and detection of short and long range targets. One of the important estimation parameter is radar delay time. In this paper, Maximum Likelihood Estimator (MLE) for range estimation using static radar is derived. Range of radar is estimated by estimating the round trip delay time. Delay is observed from the active transmitted pulse of radar. In addition, the Cramer Rao Lower Bound (CRLB) for range is also calculated. Simulation of MLE is performed. Monte Carlo simulations are also done to show that MLE achieves the CRLB at high signal to noise ratio.
[Maximum likelihood estimation, radar detection, Correlation, Ground penetrating radar, static radar, MLE, Maximum Likelihood Estimator, trip delay, range estimation, maximum likelihood estimation, MATLAB, maximum likelihood estimator, Monte Carlo methods, Cramer Rao lower bound, Monte Carlo, radar delay time, delay time, delays, Radar antennas, parameter estimation, CRLB, Delays, radar technology]
Reducing network overhead of IoTDTLS protocol employing ChaCha20 and Poly1305
2017 20th International Conference of Computer and Information Technology
None
2017
IP based communication protocols will play a key role enabling the pervasive connectivity among the devices within IoT network. Such a resource-restricted platform specific protocol is Datagram Transport Layer Security (DTLS) protocol. Applicability of DTLS to constrained environments is still a controversial issue because of its network overhead, memory management, and latency. This work presents a security scheme with two-way verification for wireless sensor nodes employing established standards with rather insignificant data transfer and handshaking latency. The recommended security scheme is built with a new cipher suite incorporated with an authentication algorithm &#x201C;Poly1305&#x201D; and an encryption algorithm &#x201C;ChaCha20&#x201D;.
[wireless sensor nodes, Protocols, cryptographic protocols, wireless sensor networks, insignificant data transfer latency, Network security, Encryption, Datagram Transport Layer Security protocol, Servers, authentication algorithm Poly1305, IoTDTLS protocol, IoT network, handshaking latency, AEAD, IP networks, two-way verification, encryption algorithm ChaCha20, Ciphers, ChaCha20, IP based communication protocols, resource-restrictred platform specific protocol, Internet of Things, pervasive connectivity, computer network security, Poly1305, OpenSSL, GCM, transport protocols, Authentication, message authentication, DTLS, ContikiOS, Cooja, Computer Network]
TCAD based performance analysis of junctionless cylindrical double gate all around FET up to 5nm technology node
2017 20th International Conference of Computer and Information Technology
None
2017
In this paper, a cylindrical double gate-all-around technology is merged with junctionless technology to maximize gate control as well as to avoid loss due to the formation of junction in the source-channel-drain region. This advancement allows downscaling a junctionless cylindrical double gate all around FET (JL-CDGAA FET) up to 5nm channel length with acceptable performance in many of its parameters. A comparative study between JL-CDGAA FET and GAA FET is also presented in this work.
[Performance evaluation, MOSFET, size 5.0 nm, source-channel-drain region, scaling, subthreshold swing, Off current, On-Off current ratio, Gallium arsenide, junctionless FET, Ions, gate control, technology CAD (electronics), TCAD based performance analysis, Logic gates, junctionless cylindrical double gate-all-around, JL-CDGAA FET, semiconductor device models, cylindrical double gate-all-around FET, gate-all-around]
Conference committees
2017 20th International Conference of Computer and Information Technology
None
2017
Provides a listing of current committee members and society officers.
[]
Opening chief guest
2017 20th International Conference of Computer and Information Technology
None
2017
The following topics are dealt with: feature extraction; learning (artificial intelligence); support vector machines; natural language processing; text analysis; image classification; pattern classification; neural nets; data mining; Internet.
[text analysis, pattern classification, support vector machines, image classification, natural language processing, feature extraction, data mining, Internet, learning (artificial intelligence), neural nets]
