1284
A metric-based approach to detect abstract data types and state encapsulations
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
This article presents an approach to identify abstract data types (ADT) and abstract state encapsulations (ASE, also called abstract objects) in source code. This approach groups together functions, types, and variables into ADT and ASE candidates according to the proportion of features they share. The set of features considered includes the context of these elements, the relationships to their environment, and informal information. A prototype tool has been implemented to support this approach. It has been applied to three C systems (each between 30-38 Kloc). The ADTs and ASEs identified by the approach are compared to those identified by software engineers who did not know the proposed approach. In a case study, this approach has been shown to identify, in most cases, more ADTs and ASEs than five published techniques applied on the same systems. This is important when trying to identify as many ADTs and ASEs as possible.
[Encapsulation, Software prototyping, software engineers, state encapsulations, Reverse engineering, abstract data types, source code, C systems, Tellurium, Connectors, Design engineering, metric-based approach, Software architecture, Prototypes, Collaboration, abstract objects, data encapsulation, Software engineering, software metrics]
Explanation based scenario generation for reactive system models
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Reactive systems control many useful and complex real-world devices. Tool-supported specification modelling helps software engineers design such systems correctly. One such tool is a scenario generator, which constructs an input event sequence for the spec model that reaches a state satisfying given criteria. It can uncover counterexamples to desired safety properties, explain feature interactions in concrete terms to requirements analysts, and even provide online help to end users learning how to use a system. However, while exhaustive search algorithms work in limited domains, the problem is highly intractable for the functionally rich models that correspond naturally to complex systems engineers wish to design. This paper describes a novel heuristic approach to the problem that is applicable to a large class of infinite state reactive systems. The key idea is to piece together scenarios that achieve subgoals into a single scenario achieving the conjunction of the subgoals. The scenarios are mined from a library captured independently during requirements acquisition. Explanation-based generalization then abstracts them so they may be coinstantiated and interleaved. The approach is implemented, and I present the results of applying the tool to tasks arising from a case study of telephony feature interactions.
[Algorithm design and analysis, online help, input event sequence, telephony feature interactions, Control systems, counterexamples, explanation, formal specification, Design engineering, Software design, heuristic programming, Abstracts, explanation-based generalization, Libraries, Safety, software tools, specification modelling, heuristic approach, reactive system models, explanation based scenario generation, generalisation (artificial intelligence), exhaustive search algorithms, end users, requirements acquisition, telephony, Systems engineering and theory, Concrete, safety properties, Software tools, scenario generator]
Deductive synthesis of event-based software architectures
14th IEEE International Conference on Automated Software Engineering
None
1999
Describes the application of the Rebound (REuse Based On UNDerstanding) framework to synthesize event-based software architectures. Within Rebound, deductive techniques are used to select components and wrap them for integration. The framework guides the selection of wrappers based on the problem specification and the components that are available for reuse. The wrapper specifications are used to generate matching conditions for component retrieval, creating a "retrieval for adaptation" scenario. The output of the synthesis activity is mapped to an event-based JavaBeans architecture.
[wrapper selection, Lattices, wrapper specifications, adaptation, formal specification, Rebound framework, software architecture, deductive synthesis, Runtime, component integration, program understanding-based software component reuse, matching conditions, component wrapping, Computer architecture, Software standards, Standards development, Plugs, component retrieval, Java, NASA, reverse engineering, event-based software architectures, Application software, inference mechanisms, component selection, JavaBeans architecture, software reusability, Computer industry, problem specification, computer aided software engineering, subroutines]
A declarative approach for designing and developing adaptive components
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
An adaptive component is a component that is able to adapt its behavior to different execution contexts. Building an adaptive application is difficult because of component dependencies and the lack of language support. As a result, code that implements adaptation is often tangled, hindering maintenance and evolution. To overcome this problem, we propose a declarative approach to program adaptation. This approach makes the specific issues of adaptation explicit. The programmer can focus on the basic features of the application, and separately provide clear and concise adaptation information. Concretely, we propose adaptation classes, which enrich Java classes with adaptive behaviors. A dedicated compiler automatically generates Java code that implements the adaptive features. Moreover, these adaptation declarations can be checked for consistency to provide additional safety guarantees. As a working example throughout this paper, we use an adaptive sound encoder in an audio-conferencing application. We show the problems associated with a traditional implementation using design patterns, and how these problems are elegantly solved using adaptation classes.
[program verification, adaptive behavior, adaptation classes, adaptive component development, Quality of service, adaptive component design, Feedback control, automatic code generation, teleconferencing, program compilers, consistency checking, software evolution, dedicated compiler, program adaptation, safety, design patterns, Bandwidth, Java classes, software engineering, component dependencies, safety guarantees, Safety, object-oriented methods, Logic, adaptation declarations, Java, execution context, Instruments, declarative approach, abstract data types, language support, Application software, software maintenance, Programming profession, adaptive systems, Computer languages, adaptive application, subroutines, audio-conferencing application, adaptive sound encoder]
Verify properties of mobile code
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Summary form only given. Given a program and a specification, you may want to verify mechanically and efficiently that this program satisfies the specification. Software verification techniques typically involve theorem proving. If a formal specification is easily available, consumption of computational resources is a major issue. Meanwhile, we shall not overlook the psychological factors. Often, you need extra expertise to verify a program. Tools that can automatically verify programs are helpful. On the other hand, ubiquitous computing has made the correctness of a program both a security and a performance issue. If you run a piece of mobile code on your machine, you will expect that the code does not access storages unlawfully. To make sure bad things won't happen, performance is sacrificed. If programs are written in an intermediate language that is able to capture and verify properties mentioned above, your host machine will benefit from it. This paper focuses on providing a type-theoretic solution to the verification of mobile programs. One of our primary tools is index types. Index types are a form of non-traditional types. An index type system extends the type system of a language with indices and predicates on those indices. Index types can express properties of program. To type check a program annotated with index types, we often will call an external decision procedure. Another concept used is the proof-carrying code. One of the major advantages of proof-carrying code is that a lot of theorem proving is shifted offline. When we use proof-carrying code to verify a property, the time spent on verification is mainly on proof-checking, which is considerably cheaper than theorem proving.
[Pervasive computing, Algorithm design and analysis, Java, Data analysis, program verification, software verification, mobile code properties verification, Psychology, Ubiquitous computing, State-space methods, Formal specifications, formal specification, proof-carrying code, Runtime, System recovery, type-theoretic solution, theorem proving, index types]
Deviation analysis through model checking
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Inaccuracies, or deviations, in the measurements of monitored variables in a control system are facts of life that control software must accommodate $the software is expected to continue functioning correctly in the face of an expected range of deviations in the inputs. Deviation analysis can be used to determine how a software specification will behave in the face of such deviations in data from the environment. The idea is to describe the correct values of an environmental quantity; along with a range of potential deviations, and then determine the effects on the outputs of the system. The analyst can then check whether the behavior of the software is acceptable with respect to these deviations. In this report we wish to propose a new approach to deviation analysis using model checking techniques. This approach allows for more precise analysis than previous techniques, and refocuses deviation analysis from an exploratory analysis to a verification task, allowing us to investigate a different range of questions regarding a system's response to deviations.
[Chemical industry, software specification, Chemical analysis, control software, Computerized monitoring, NASA, exploratory analysis, Control systems, Displays, Sensor systems, deviation analysis, formal specification, Computer science, formal verification, model checking, environmental quantity, Electric variables control, Software measurement]
Predicting fault prone modules by the Dempster-Shafer belief networks
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
This paper describes a novel methodology for predicting fault prone modules. The methodology is based on Dempster-Shafer (D-S) belief networks. Our approach consists of three steps: first, building the D-S network by the induction algorithm; second, selecting the predictors (attributes) by the logistic procedure; third, feeding the predictors describing the modules of the current project into the inducted D-S network and identifying fault prone modules. We applied this methodology to a NASA dataset. The prediction accuracy of our methodology is higher than that achieved by logistic regression or discriminant analysis on the same dataset.
[knowledge engineering, fault diagnosis, software reliability, logistic procedure, induction algorithm, Predictive models, software quality, attributes selection, Fault diagnosis, Accuracy, NASA dataset, fault prone modules prediction, belief networks, prediction accuracy, logistic regression, Classification tree analysis, D-S belief networks, discriminant analysis, NASA, Power system modeling, Statistics, predictors selection, Dempster-Shafer, Software quality, Lab-on-a-chip, Logistics]
Refactoring use case models on episodes
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Use case models are widely used to capture functional requirements of a system. However, to obtain well-organized use case models is not easy. Refactoring is an approach to reorganize the internal structure of models in order to improve them or extend them in some way. This work looks at refactoring of use case models based on the information captured in episode models. We introduce 10 refactoring rules for use case refactoring in detail, including their verification of the behavior-preserving property. We also present a case study based on the automated teller machine.
[Computer aided software engineering, object-oriented programming, Corporate acquisitions, Object oriented modeling, episode models, automated teller machine, Mathematics, refactoring rules, behavior-preserving property verification, program compilers, system functional requirements, formal verification, Software systems, use case models, use case refactoring, Books]
Winning the DARPA Grand Challenge: A Robot Race through the Mojave Desert
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Summary form only given. The DARPA Grand Challenge was the most significant event in the field of robotics in more than a decade. A mobile ground robot had to traverse 132 miles of punishing desert terrain in less than ten hours. In 2004, the best robot only made 7.3 miles. A year later, Stanford won this historical challenge and cashed the } prize. This talk, delivered by the leader of the Stanford Racing Team, will provide insights into the software architecture of Stanford's winning robot "Stanley." The robot heavily relied on advanced artificial intelligence, and it used a pipelining architecture to turn sensor data into vehicle controls. The talk will introduce the audience into the fascinating world of autonomous robotics, share many of the race insights, and discuss some of the implications for the future of our society
[Mojave Desert, mobile ground robot, sensor data, sensor fusion, desert terrain, mobile robots, robot programming, Mobile robots, Intelligent robots, artificial intelligence, Remotely operated vehicles, software architecture, Software architecture, Stanley, robot race, Computer architecture, Robot sensing systems, vehicle controls, Intelligent sensors, Pipeline processing, autonomous robotics, Intelligent vehicles, pipeline processing, DARPA Grand Challenge, Artificial intelligence, pipelining architecture]
Mining Scenario-Based Triggers and Effects
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
We present and investigate the problem of mining scenario-based triggers and effects from execution traces, in the framework of Damm and Harel's live sequence charts (LSC); a visual, modal, scenario-based, inter-object language. Given a 'trigger scenario', we extract LSCs whose pre-chart is equivalent to the given trigger; dually, given an 'effect scenario', we extract LSCs whose main-chart is equivalent to the given effect. Our algorithms use data mining methods to provide significant sound and complete results modulo user-defined thresholds. Both the input trigger and effect scenarios, and the resulting candidate modal scenarios, are represented and visualized using a UML2- compliant variant of LSC. Thus, existing modeling tools can be used both to specify the input for the miner and to exploit its output. Experiments performed with several applications show promising results.
[trigger scenario, Shape, program diagnostics, data mining, execution traces, scenario-based language, Data mining, Object recognition, History, modal language, specification mining, formal specification, visual language, live sequence charts, Focusing, scenario-based triggers mining, scenario-based effects mining, interobject language, effect scenario, Performance analysis, Monitoring]
Spectrum-Based Multiple Fault Localization
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Fault diagnosis approaches can generally be categorized into spectrum-based fault localization (SFL, correlating failures with abstractions of program traces), and model-based diagnosis (MBD, logic reasoning over a behavioral model). Although MBD approaches are inherently more accurate than SFL, their high computational complexity prohibits application to large programs. We present a framework to combine the best of both worlds, coined BARINEL. The program is modeled using abstractions of program traces (as in SFL) while Bayesian reasoning is used to deduce multiple-fault candidates and their probabilities (as in MBD). A particular feature of BARINEL is the usage of a probabilistic component model that accounts for the fact that faulty components may fail intermittently. Experimental results on both synthetic and real software programs show that BARINEL typically outperforms current SFL approaches at a cost complexity that is only marginally higher. In the context of single faults this superiority is established by formal proof.
[software program, statistical and reasoning approaches, Costs, fault diagnosis, model-based reasoning, program verification, spectrum based fault localization, probabilistic component model, Mathematics, Embedded software, Fault diagnosis, Software fault diagnosis, Bayesian reasoning, Mathematical model, Logic, program diagnostics, formal proof, program spectra, Debugging, model based diagnosis, Computational complexity, synthetic program, coined BARINEL, Bayesian methods, program trace abstraction, Bayes methods, Software engineering, computational complexity]
A rule-based approach to the semantic lifting of model differences in the context of model versioning
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In model-based software engineering, models are primary artifacts which iteratively evolve and which are often developed in teams. Therefore, comparison and merge tools for models are indispensable. These tools must compare models in a technology-dependent runtime representation and will initially derive low-level changes, which can differ considerably from user-level editing commands. Low-level differences are often incomprehensible and should be semantically lifted to the level of editing operations. This transformation of differences depends on the model type, supported editing operations, and user preferences; thus specific transformers are needed, and building them is a challenge. We present a rule-based approach to this problem: low-level differences are represented based on the Eclipse Modeling Framework. They are transformed into representations of editing operations using a rule-based model transformation engine. The necessary transformation rules are automatically derived from basic transformation rules for the editing operations.
[Context, semantic lifting, Eclipse modeling framework, Computational modeling, rule-based model transformation engine, Unified modeling language, model versioning, model-based software engineering, Pattern recognition, configuration management, Runtime, low-level differences, Semantics, knowledge based systems, software engineering, Context modeling]
Puzzle-based automatic testing: bringing humans into the loop by solving puzzles
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Recently, many automatic test generation techniques have been proposed, such as Randoop, Pex and jCUTE. However, usually test coverage of these techniques has been around 50-60% only, due to several challenges, such as 1) the object mutation problem, where test generators cannot create and/or modify test inputs to desired object states; and 2) the constraint solving problem, where test generators fail to solve path conditions to cover certain branches. By analyzing branches not covered by state-of-the-art techniques, we noticed that these challenges might not be so difficult for humans. To verify this hypothesis, we propose a Puzzle-based Automatic Testing environment (PAT) which decomposes object mutation and complex constraint solving problems into small puzzles for humans to solve. We generated PAT puzzles for two open source projects and asked different groups of people to solve these puzzles. It was shown that they could be effectively solved by humans: 231 out of 400 puzzles were solved by humans at an average speed of one minute per puzzle. The 231 puzzle solutions helped cover 534 and 308 additional branches (7.0% and 5.8% coverage improvement) in the two open source projects, on top of the saturated branch coverages achieved by the two state-of-the-art test generation techniques.
[open source projects, Pex, PAT, program testing, public domain software, Randoop, Code Coverage, object mutation problem, puzzle-based automatic testing environment, automatic test generation techniques, Human Computation, jCUTE, complex constraint solving problems, Testing]
Identifying execution points for dynamic analyses
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Dynamic analyses rely on the ability to identify points within or across executions. In spite of this being a core task for dynamic analyses, new solutions are frequently developed without an awareness of existing solutions, their strengths, their weaknesses, or their caveats. This paper surveys the existing approaches for identifying execution points and examines their analytical and empirical properties that researchers and developers should be aware of when using them within an analysis. In addition, based on limitations in precision, correctness, and efficiency for techniques that identify corresponding execution points across multiple executions, we designed and implemented a new technique, Precise Execution Point IDs. This technique avoids correctness and precision issues in prior solutions, enabling analyses that use our approach to also produce more correct results. Empirical comparison with the surveyed techniques shows that our approach has 25% overhead on average, several times less than existing solutions.
[Context, Runtime, Silicon carbide, Radiation detectors, Instruments, precision, system monitoring, dynamic analysis, precise execution point ID, Core dumps, execution points identification, Indexing]
General LTL Specification Mining (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Temporal properties are useful for describing and reasoning about software behavior, but developers rarely write down temporal specifications of their systems. Prior work on inferring specifications developed tools to extract likely program specifications that fit particular kinds of tool-specific templates. This paper introduces Texada, a new temporal specification mining tool for extracting specifications in linear temporal logic (LTL) of arbitrary length and complexity. Texada takes a user-defined LTL property type template and a log of traces as input and outputs a set of instantiations of the property type (i.e., LTL formulas) that are true on the traces in the log. Texada also supports mining of almost invariants: properties with imperfect confidence. We formally describe Texada's algorithms and evaluate the tool's performance and utility.
[Context, data mining, LTL specification mining, tool-specific template, temporal logic, linear temporal logic, Texada, Cognition, dynamic analysis, Complexity theory, Data mining, specification mining, formal specification, temporal specification mining tool, program specification, user-defined LTL property type template, arbitrary length, software behavior, Semantics, temporal property, Software, LTL formula, Software engineering]
QUICKAR: Automatic query reformulation for concept location using crowdsourced knowledge
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
During maintenance, software developers deal with numerous change requests made by the users of a software system. Studies show that the developers find it challenging to select appropriate search terms from a change request during concept location. In this paper, we propose a novel technique-QUICKAR-that automatically suggests helpful reformulations for a given query by leveraging the crowdsourced knowledge from Stack Overflow. It determines semantic similarity or relevance between any two terms by analyzing their adjacent word lists from the programming questions of Stack Overflow, and then suggests semantically relevant queries for concept location. Experiments using 510 queries from two software systems suggest that our technique can improve or preserve the quality of 76% of the initial queries on average which is promising. Comparison with one baseline technique validates our preliminary findings, and also demonstrates the potential of our technique.
[Context, concept location, Java, Vocabulary, Query reformulation, automatic query reformulation, Stack Overflow programming questions, word co-occurrence, Programming, semantic similarity, crowdsourced knowledge, software maintenance, Stack Overflow, query processing, Databases, Semantics, semantic relevance, QUICKAR, adjacency list, Software, semantically relevant queries]
EHBDroid: Beyond GUI testing for Android applications
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
With the prevalence of Android-based mobile devices, automated testing for Android apps has received increasing attention. However, owing to the large variety of events that Android supports, test input generation is a challenging task. In this paper, we present a novel approach and an open source tool called EHBDroid for testing Android apps. In contrast to conventional GUI testing approaches, a key novelty of EHBDroid is that it does not generate events from the GUI, but directly invokes callbacks of event handlers. By doing so, EHBDroid can efficiently simulate a large number of events that are difficult to generate by traditional UI-based approaches. We have evaluated EHBDroid on a collection of 35 real-world large-scale Android apps and compared its performance with two state-of-the-art UI-based approaches, Monkey and Dynodroid. Our experimental results show that EHBDroid is significantly more effective and efficient than Monkey and Dynodroid: in a much shorter time, EHBDroid achieves as much as 22.3% higher statement coverage (11.1% on average) than the other two approaches, and found 12 bugs in these benchmarks, including 5 new bugs that the other two failed to find.
[test input generation, program testing, Instruments, graphical user interfaces, Humanoid robots, conventional GUI testing approaches, Tools, automated testing, Android, Android (operating system), mobile computing, UI-based approaches, large-scale Android apps, Android based mobile devices, event generation, XML, EHBDroid, Android applications, event handlers, open source tool, Androids, Testing, Graphical user interfaces]
Specification and verification of the Co/sub 4/ distributed knowledge system using LOTOS
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
This paper relates the formal specification and verification of a consensual decision protocol based on Co/sub 4/, a computer environment dedicated to the building of a distributed knowledge base. This protocol has been specified in the ISO formal description technique LOTOS. The CADP tools from the EUCALYPTUS LOTOS toolset have been used to verify different safety and liveness properties. The verification work has confirmed an announced violation of knowledge consistency and has put forth a case of inconsistent hierarchy, four cases of unexpected message reception and some further local corrections in the definition of the protocol.
[LOTOS, Knowledge based systems, Humans, distributed knowledge base, consensual decision protocol, State-space methods, Specification languages, Co/sub 4/ distributed knowledge system, formal specification, Equations, ISO formal description technique, Cryptographic protocols, Concurrent computing, Algebra, formal verification, liveness properties, knowledge based systems, safety, specification languages, Safety, knowledge consistency, protocols, verification, Formal verification]
Retrieving software components that minimize adaptation effort
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Given a software library whose entries are represented by formal specifications, we distinguish between two retrieval procedures: exact retrieval, whereby, given a query K, we identify all the library components that are correct with respect to K; approximate retrieval, which is invoked when exact retrieval fails, and identifies the library components that minimize adaptation effort. To this effect, we define four measures of functional distance between specifications, and discuss algorithms that minimize these measures over a set of components; then we discuss whether these measures can be used to predict adaptation effort.
[approximate retrieval, retrieval procedures, software library, Inspection, Formal specifications, formal specification, formal specifications, software libraries, Uniform resource locators, query processing, Software libraries, library components, functional distance, adaptation effort, software reusability, computer aided software engineering, Software measurement, software components]
Automatic high-quality reengineering of database programs by temporal abstraction
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
The relational database model is currently the target of choice for the conversion of legacy software that uses older models (such as indexed-sequential, hierarchical or network models). The relational model makes up for its lower efficiency by a greater expressive power and by optimization of queries, using indexes and other means. However, sophisticated analysis is required in order to take advantage of these features, since converting each database access operation separately does not use the greater expressive power of the target database and does not enable it to perform useful optimizations. By analyzing the behavior of the host program around the database access operations, it is possible to discover patterns such as filtering, joins and aggregative operations. It is then possible to remove those operations from the host program and re-implement them in the target database query language. This paper describes an automatic system, called MIDAS (MIgrator of Database Application Systems), that performs high-quality reengineering of legacy database programs in this way. The results of MIDAS were found to be superior to those of the naive one-to-one translation in terms of readability, size, speed and network data traffic.
[joins, MIDAS, Relational databases, Electronic mail, relational database model, Database languages, expressive power, aggregative operations, query processing, indexes, legacy software conversion, network data traffic, Pattern analysis, filtering, temporal abstraction, legacy database programs, Object oriented databases, Filtering, database access operations, Object oriented modeling, Spatial databases, Application software, relational databases, target database query language, Computer science, systems re-engineering, readability, query optimization, temporal databases, automatic high-quality reengineering, computer aided software engineering]
Interactive component-based software development with Espresso
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Most component models in use today are language-independent, but also platform-dependent and not designed specifically to support a tool-based visual development paradigm. Espresso is a new component model that was designed with the goal of supporting software development through tool-based visual component composition. Being implemented in Java, Espresso components can run on any Java-enabled platform.
[Java, Espresso component model, Laboratories, interactive component-based software development, Programming, Application software, Software libraries, interactive programming, tool-based visual development paradigm, Production, Packaging, Mice, Hardware, software tools, visual programming, Contracts, Java-enabled platform]
Using KIV to specify and verify architectures of knowledge-based systems
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Building knowledge-based systems from reusable elements is a key factor in developing them economically. However, one has to ensure that the assumptions and functionality of the reused building block fit together with each other and the specific circumstances of the actual problem and knowledge. We use the Karlsruhe Interactive Verifier (KIV) for this purpose. We show how the verification of conceptual and formal specifications of knowledge-based systems can be performed with it. KIV was originally developed for the verification of procedural programs but it serves well for verifying knowledge-based systems. Its specification language is based on abstract data types for the functional specification of components and dynamic logic for the algorithmic specification. It provides an interactive theorem prover integrated into a sophisticated tool environment supporting aspects like the automatic generation of proof obligations, generation of counter examples, proof management, proof reuse etc. Such a support is essential for making the verification of complex specifications feasible. We provide some examples on how to specify and verify tasks, problem-solving methods, and their relationships.
[Knowledge engineering, Terminology, proof reuse, reusable elements, proof management, Environmental management, formal specification, Counting circuits, Software architecture, formal verification, knowledge based systems, Computer architecture, theorem proving, Logic, specification language, Knowledge based systems, Buildings, abstract data types, formal specifications, algorithmic specification, knowledge-based systems, functional specification, Karlsruhe interactive verifier, interactive theorem prover, dynamic logic, Problem-solving]
Modelling the application domains of software engineering technologies
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
The effectiveness of software engineering technologies depends very much on the situation in which they are applied. In order to further improve software development practices one needs to explicitly describe and utilise knowledge about application domains of software engineering technologies. The paper suggests a modelling formalism for supporting systematic reuse of software engineering technologies during planning of software projects and improvement programs.
[software development practices, project management, Project management, software development management, Programming, Application software, Technology planning, Best practices, planning, Technology management, software project planning, improvement program planning, Packaging, software reusability, systematic software engineering technology reuse, software engineering technologies, Software measurement, application domain modelling, Software engineering, Context modeling]
On the verification of VDM specification and refinement with PVS
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Although the formal method VDM has been in existence since the 1970s, there are still no satisfactory tools to support verification in VDM. The paper deals with one possible means of approaching this problem by using the PVS theorem-prover. It describes a translation of a VDM-SL specification into the PVS specification language using, essentially, the very transparent translation methods described by Agerholm (1996). PVS was used to typecheck the specification and to prove some non-trivial validation conditions. Next, a more abstract specification of the same system was also expressed in PVS, and the original specification was shown to be a refinement of this one. The drawbacks of the translation are that it must be done manually (though automation may be possible), and that the "shallow embedding" technique which is used does not accurately capture the proof rules of VDM-SL. The benefits come from the facts that the portion of VDM-SL which can be represented is substantial and that it is a great advantage to be able to use the powerful PVS proof-checker.
[specification type-checking, Protocols, program verification, Laboratories, Mathematics, formal specification, transparent translation methods, nontrivial validation conditions, specification languages, Safety, theorem proving, Carbon capture and storage, Informatics, verification, Automation, PVS theorem prover, VDM-SL specification translation, VDM formal method, abstract specification, Educational institutions, Specification languages, PVS specification language, VDM specification, Writing, shallow embedding technique, proof rules]
Distributed cooperative formal methods tools
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
This paper describes some tools to support formal methods, and conversely some formal methods for developing such tools. We focus on distributed cooperative proving over the web. Our tools include a proof editor/assistant, servers for remote proof execution, a distributed truth protocol, an editor generator; and a new method for interface design called algebraic semiotics, which combines semiotics with algebraic specification. Some examples are given.
[Java, Protocols, Design methodology, Drives, Maintenance engineering, Data structures, remote proof execution, explanation, Computer science, proof editor, distributed truth protocol, algebraic semiotics, Computer industry, Internet, distributed cooperative formal methods tools, distributed cooperative proving, Software tools, algebraic specification, editor generator, interface design]
Exploiting domain-specific knowledge to refine simulation specifications
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Discusses our approach to the problem of refining high-level simulation specifications. Our domain is simulated combat training for tank platoon members. Our input is a high-level specification for a training scenario and our output is an executable specification for the behavior of a network-based combat simulator. Our approach combines a detailed model of the tank training domain with nonlinear planning and constraint satisfaction techniques. Our initial implementation is successful in large part because of our use of domain knowledge to limit the branching factor of the planner and the constraint satisfaction engine.
[Algorithm design and analysis, Helicopters, simulated combat training, network-based combat simulator, Turning, digital simulation, training, formal specification, Engines, nonlinear planning, Design optimization, executable specification, planning (artificial intelligence), Weapons, constraint satisfaction, computer based training, branching factor, constraint handling, military computing, domain-specific knowledge, tank platoon members, high-level simulation specification refinement, Data structures, Formal specifications, Scheduling algorithm, Numerical simulation, computer aided software engineering]
Research directions for automated software verification: using trusted hardware
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Service providers hosting software on servers at the request of content providers need assurance that the hosted software has no undesirable properties. This problem applies to browsers which host applets, networked software which can host software agents, etc. The hosted software's properties are currently verified by testing and/or verification processes by the hosting computer. This increases cost, causes delay, and leads to difficulties in version control. By furnishing content providers with a physically secure computing device with an embedded certified private key, such properties can be verified and/or enforced by the secure computing device at the content provider's site; the secure device can verify such properties, statically whenever possible, and by inserting checks into the executable binary when necessary. The resulting binary is attested by a trusted signature, and can be hosted with confidence. The position paper is a preliminary report that outlines scientific and engineering goals in this project.
[Software testing, engineering goals, Costs, trusted signature, program verification, program testing, executable binary, trusted hardware, Software safety, scientific goals, physically secure computing device, Information systems, Network servers, content providers, servers, Hardware, Software agents, automated software verification, embedded certified private key, Java, Runtime environment, applets, hosted software, computer networks, version control, testing, hosting computer, browsers, software agents, configuration management, networked software, security of data, checks, service providers, Formal verification]
NORA/HAMMR: making deduction-based software component retrieval practical
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Deduction-based software component retrieval uses pre- and postconditions as indexes and search keys and an automated theorem prover (ATP) to check whether a component matches. This idea is very simple but the vast number of arising proof tasks makes a practical implementation very hard. We thus pass the components through a chain of filters of increasing deductive power. In this chain, rejection filters based on signature matching and model checking techniques are used to rule out non-matches as early as possible and to prevent the subsequent ATP from "drowning". Hence, intermediate results of reasonable precision are available at (almost) any time of the retrieval process. The final ATP step then works as a confirmation filter to lift the precision of the answer set. We implemented a chain which runs fully automatically and uses SETHEO for model checking and the automated prover SETHEO as confirmation filter. We evaluated the system over a medium-sized collection of components. The results encourage our approach.
[deduction-based software component retrieval, Thyristors, model checking techniques, Pipelines, NORA/HAMMR, Information retrieval, proof tasks, inference mechanisms, search keys, software libraries, automated theorem prover, Filters, Software libraries, Runtime, signature matching, Databases, software reusability, confirmation filter, computer aided software engineering, theorem proving, software tools, rejection filters, Software engineering]
Automatic synthesis of recursive programs: the proof-planning paradigm
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
We describe a proof plan that characterises a family of proofs corresponding to the synthesis of recursive functional programs. This plan provides a significant degree of automation in the construction of recursive programs from specifications, together with correctness proofs. This plan makes use of meta-variables to allow successive refinement of the identity of unknowns, and so allows the program and the proof to be developed hand in hand. We illustrate the plan with parts of a substantial example-the synthesis of a unification algorithm.
[unification algorithm, Automation, programming theory, Logic programming, program verification, functional programming, correctness proofs, Process control, automatic synthesis, proof-planning paradigm, Bridges, Filters, recursive programs, Automatic control, Artificial intelligence, recursive functional programs]
Tools supporting the creation and evolution of software development knowledge
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Software development is a knowledge-intensive activity involving the integration of diverse knowledge sources that undergo constant change. The volatility of knowledge in software development requires that knowledge bases are able to support a continuous knowledge acquisition process where tools are available that can make use of partial knowledge. To address these issues, case-based technology is used in combination with an organizational learning process to create an approach that turns Standard Development Methodologies (SDM) into living documents that capture project experiences and emerging requirements as they are encountered in an organization. A rule-based system is used to tailor the SDM to meet the characteristics of individual projects and provide relevant development knowledge throughout the development lifecycle.
[Knowledge engineering, Process design, organizational learning process, Knowledge acquisition, Knowledge based systems, software development knowledge, rule-based system, knowledge acquisition, knowledge acquisition process, Programming, case-based technology, Best practices, Computer science, standard development methodologies, knowledge-intensive activity, Software design, development lifecycle, knowledge based systems, case-based reasoning, software engineering, Standards development, Software engineering]
A formal automated approach for reverse engineering programs with pointers
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Given a program S and a precondition Q, the strongest postcondition, denoted sp(S,Q), is defined as the strongest condition that holds after the execution of S, given that S terminates. By defining the formal semantics of each of the constructs of a programming language, a formal specification of the behavior of a program written using the given programming language can be constructed. In this paper we address the formal semantics of pointers in order to handle a realistic model of programming languages that incorporate the use of pointers. In addition, we present a tool for supporting the construction of formal specifications of programs that include the use of pointers.
[Software maintenance, Costs, Reverse engineering, NASA, reverse engineering programs, formal automated approach, Programming, reverse engineering, Electronic mail, Formal specifications, software maintenance, formal specification, formal specifications, Computer science, Computer languages, formal semantics, Propulsion, programming language, pointers]
Notes on refinement, interpolation and uniformity
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
The connection between some modularity properties and interpolation is revisited and restated in a general "logic-independent" framework. The presence of uniform interpolants is shown to assist in certain proof obligations, which suffice to establish the composition of refinements. The absence of the desirable interpolation properties from many logics that have been used in refinement motivates a thorough investigation of methods to expand a specification formalism orthogonally, so that the critical uniform interpolants become available. A potential breakthrough is outlined in this paper.
[proof obligations, programming theory, specification formalism orthogonal expansion, Subspace constraints, Laboratories, abstract data types, uniformity, Educational institutions, Formal specifications, software maintenance, formal specification, uniform interpolants, formal logic, program development, Interpolation, interpolation, modularity properties, logic-independent framework, Concrete, Logic, algebraic specification, step-wise refinement]
A static analysis for program understanding and debugging
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
The paper presents a static pointer analysis technique for a subset of C. The tool supports user-defined assertions inserted in the body of the program. Assertions are of two kinds: static assertions automatically verified by the analyser, and hypothetical assertions treated as assumptions by the analyser. The technique deals with recursive data structures and it is accurate enough to handle circular structures.
[program debugging, recursive data structures, Costs, program verification, C subset, assumptions, program understanding, Tail, static assertions, data structures, Logic, Testing, static pointer analysis technique, Debugging, Data structures, reverse engineering, Mechanical factors, Programming profession, Computer languages, Computer bugs, automatic verification, user-defined assertions, system monitoring, hypothetical assertions, circular structures]
A contribution to program comprehension by program analysis: application to numerical programs
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
This paper deals with non-functional aspects of software. It presents an approach towards the integration of methods in order to handle properties of numerical programs. We develop a program analysis technique which allows us to formalise, evaluate and check non-functional properties of programs. These property evaluations are used in two main areas. The first one is related to program design to choose data representations and to perform program transformations. The second is related to reverse engineering, and particularly to software reuse and maintenance. As example, a functional language with numerical type only is considered, and the accuracy of the numerical computations is the considered non-functional property.
[Software maintenance, Reverse engineering, program design, numerical type, program transformations, numerical computational accuracy, Program processors, Operating systems, data representations, numerical analysis, program analysis, functional language, data structures, program comprehension, software reuse, Commutation, Data security, program diagnostics, Data structures, reverse engineering, Application software, software maintenance, nonfunctional property evaluation, Equations, numerical programs, software reusability, functional languages]
Modular flow analysis for concurrent software
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Modern software systems are designed and implemented in a modular fashion by composing individual components. The advantages of early validation are widely accepted in this context, i.e., that defects in individual module designs and implementations may be detected and corrected prior to system-level validation. This is particularly true for errors related to interactions between system components. In this paper, we describe how a whole-program automated static analysis technique can be adapted to the validation of individual components, or groups of components, of sequential or concurrent software systems. This work builds off of an existing approach, FLAVERS, that uses program flow analysis to verify explicitly stated correctness properties of software systems. We illustrate our modular analysis approach and some of its benefits by describing part of a case-study with a realistic concurrent multi-component system.
[Software testing, client-server systems, realistic concurrent multi-component system, Costs, program flow analysis, program verification, finite automata, system-level validation, explicitly stated correctness properties, data flow analysis, Programming, FLAVERS, modular flow analysis, concurrent software systems, parallel programming, Information analysis, Software design, individual module designs, Software systems, early validation, computer aided software engineering, Performance analysis, whole-program automated static analysis technique, modular analysis approach]
A structured approach for synthesizing planners from specifications
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Plan synthesis approaches in AI fall into two categories: domain-independent and domain-dependent. The domain-independent approaches are applicable across a variety of domains, but may not be very efficient in any one given domain. The domain-dependent approaches can be very efficient for the domain for which they are designed, but would need to be written separately for each domain of interest. The tediousness and the error-proneness of manual coding have hither-to inhibited work on domain-dependent planners. In this paper we describe a novel way of automating the development of domain dependent planners using knowledge-based software synthesis tools. Specifically, we describe an architecture called CLAY in which the Kestrel Interactive Development System (KIDS) is used in conjunction with a declarative theory of domain independent planning, and the declarative control knowledge specific to a given domain, to semi-automatically derive customized planning code. We discuss what it means to write declarative theory of planning and control knowledge for KIDS, and illustrate it by generating a range of domain-specific planners using state space and plan space refinements. We demonstrate that the synthesized planners can have superior performance compared to classical refinement planners using the same control knowledge.
[error-proneness, program verification, World Wide Web, Control systems, formal specification, specifications, planning (artificial intelligence), knowledge based systems, Computer architecture, planners synthesis, Control system synthesis, Automatic control, manual coding, declarative control knowledge, AI, State-space methods, Computer science, knowledge-based software synthesis tools, Kestrel Interactive Development System, Artificial intelligence, Software tools, CLAY, Software engineering, structured approach, domain-independent approach]
Facilitating an automated approach to architecture-based software reuse
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Over the past several years, a number of techniques have been developed to address various issues involving software reuse, such as component classification, retrieval, and integration. However, it is not adequate to only have reuse techniques that address reuse issues separately. Instead, a seamless integration of these reuse techniques is critical to achieve effective reuse. In this paper, we present an integrated approach to software reuse. Based on software architecting techniques and formal methods, this approach addresses various reuse issues in a systematic and (semi) automatic fashion. An architecture-based software reuse and integration environment that supports this approach is also described.
[Productivity, component retrieval, Pipelines, Programming, Application software, architecture-based software reuse, formal specification, automated approach, reuse techniques, Computer science, Filters, component integration, Software architecture, Software quality, formal methods, software reusability, Software systems, computer aided software engineering, integration environment, component classification, Software reusability, software architecting techniques]
Feedback handling in dynamic task nets
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
While a software process is being executed, many errors and problems occur which require to reconsider previously executed process steps. In order to handle feedback in a process management system, several requirements need to be addressed: adaptability, human intervention, impact analysis, change propagation, restoration of the work context, and traceability. Feedback management in DYNAMITE meets these requirements. DYNAMITE is based on dynamic task nets and specifically supports feedback through feedback relations, task versions, and customized semantics of data flows. A methodology for feedback handling is also represented.
[Performance evaluation, Humans, human factors, change propagation, formal specification, Engines, task versions, feedback, Engineering management, human intervention, Feedback, work context restoration, feedback handling, customized data flow semantics, Books, Testing, dynamic task nets, data flow analysis, Data structures, feedback relations, adaptability, traceability, impact analysis, Packaging, process management system, DYNAMITE, Software engineering, software process]
Processing natural language requirements
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
The importance of requirements, which in practice often means natural language requirements, for a successful software project cannot be underestimated. Although requirement analysis has been traditionally reserved to the experience of professionals, there is no reason not to use various automatic techniques to the same end. In this paper we present Circe, a Web-based environment for aiding in natural language requirements gathering, elicitation, selection, and validation and the tools it integrates. These tools have been used in several experiments both in academic and in industrial environments. Among other features, Circe can extract abstractions from natural language texts, build various models of the system described by the requirements, check the validity of such models, and produce functional metric reports. The environment can be easily extended to enhance its natural language recognition power or to add new models and views on them.
[natural language requirements processing, functional metric reports, formal specification, Information analysis, formal verification, Failure analysis, Web-based environment, Natural language processing, requirement analysis, validation, natural language recognition, Information resources, Natural languages, natural language texts, Documentation, Decoding, software project, Power system modeling, Circe, selection, systems analysis, Writing, natural language requirements, natural languages, programming environments, Power engineering and energy]
Strategies of structural synthesis of programs
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Strategies of the structural synthesis of programs (SSP) of a deductive program synthesis method which is suited for compositional programming in large and is in practical use in a number of programming environments are outlined. SSP is based on a decidable logical calculus where complexity of the proof search is still PSPACE. This requires paying special attention to efficiency of search. Besides the general case of SSP, the authors present synthesis with independent subtasks, synthesis of iterations on regular data structures in terms of SSP and a number of heuristics used for speeding up the search.
[Merging, proof search, Calculus, structured programming, decidable logical calculus, PSPACE complexity, decidability, heuristics, iteration synthesis, structural program synthesis strategies, data structures, theorem proving, search efficiency, search problems, Logic programming, independent subtasks, deductive program synthesis method, Data structures, Specification languages, Programming environments, Computer science, compositional programming, programming environments, regular data structures, computational complexity]
Applying concept formation methods to object identification in procedural code
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Legacy software systems present a high level of entropy combined with imprecise documentation. This makes their maintenance more difficult, more time consuming, and costlier. In order to address these issues, many organizations have been migrating their legacy systems to new technologies. In this paper, we describe a computer-supported approach aimed at supporting the migration of procedural software systems to the object-oriented (OO) technology, which supposedly fosters reusability, expandability, flexibility, encapsulation, information hiding, modularity, and maintainability. Our approach relies heavily on the automatic formation of concepts based on information extracted directly from code to identify objects. The approach tends, thus, to minimize the need for domain application experts. We also propose rules for the identification of OO methods from routines. A well known and self-contained example is used to illustrate the approach. We have applied the approach on medium/large procedural software systems, and the results show that the approach is able to find objects and to identify their methods from procedures and functions.
[Encapsulation, Software maintenance, encapsulation, computer-supported approach, procedural code, Reliability engineering, reusability, Entropy, Data mining, flexibility, modularity, Computer architecture, concept formation methods, object-oriented programming, information hiding, Documentation, Maintenance engineering, object-oriented technology, Educational institutions, legacy software systems, systems re-engineering, software portability, procedural software systems, expandability, Software systems, computer aided software engineering, object identification, data encapsulation, maintainability]
Automated configuration of distributed applications from reusable software architectures
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
In this paper a reuse-oriented perspective is taken to designing and implementing configurable distributed applications. An application domain is defined as a family of systems that have some features in common and others that differentiate them. During domain engineering, reusable specifications, architectures and component types are developed, which capture the similarities and variations of the family of systems that compose the application domain. Target systems are generated by tailoring the reusable specification and architecture given the requirements of the target system, and configuring a target system based on the tailored architecture. The paper describes an automated approach for configuring distributed applications from a reusable architecture and library of predefined component types.
[Software maintenance, object-oriented programming, domain engineering, Supercapacitors, distributed processing, Application software, formal specification, Design engineering, predefined component types, Software libraries, reusable software architectures, Software architecture, Computer architecture, software reusability, automated configuration, reuse-oriented perspective, reusable specifications, Software systems, configurable distributed applications, Software reusability, Software engineering]
Formally specifying engineering design rationale
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
This paper briefly describes our initial experiences in applied research of formal approaches to the generation and maintenance of software systems supporting structural engineering tasks. We describe the business context giving rise to this activity, and give an example of the type of engineering problem we have focused on. We briefly describe our approach to software generation and maintenance, and point out the challenges that we appear to face in transferring this technology into actual practice.
[Knowledge engineering, Software maintenance, structural engineering, structural engineering computing, software systems, maintenance of software, software generation, Companies, Product design, Structural engineering, software maintenance, formal specification, Design engineering, Manufacturing processes, Software libraries, Software systems, Competitive intelligence, formal approaches, engineering design]
Extracting objects from legacy imperative code
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
This paper presents a methodology for extracting objects from legacy imperative code. The parameter-based object identification (PBOI) methodology is based on the thesis that object attributes manifest themselves as data items passed from subprogram to subprogram in the imperative paradigm. A taxonomy of imperative subprograms is presented and the PBOI methodology is defined. Several examples are provided.
[legacy imperative code, object-oriented programming, parameter-based object identification, Taxonomy, PBOI methodology, Data mining, object extraction, systems re-engineering, Algorithms, Investments, Prototypes, data items, Aging, subroutines, imperative subprograms, Usability, object attributes]
Towards semantic-based object-oriented CASE tools
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Despite their strengths, object-oriented methods (OOMs) and their supporting CASE tools often do not produce model that are amenable to rigorous semantic analysis. This is a direct result of their loosely defined semantics. The authors outline their ongoing work on providing a semantic base for OOMs.
[Computer aided software engineering, Costs, object-oriented programming, Engineering profession, Object oriented modeling, semantic-based object-oriented CASE tools, Knowledge representation, Formal specifications, Computer science, Prototypes, Fusion power generation, semantic analysis, computer aided software engineering, software tools, object-oriented methods, Artificial intelligence, loosely defined semantics]
Augmenting abstract syntax trees for program understanding
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Program understanding efforts by individual maintainers are dominated by a process known as discovery, which is characterized by low-level searches through the source code and documentation to obtain information that is important to the maintenance task. Discovery is complicated by the delocalization of information in the source code, and can consume from 40-60% of a maintainer's time. This paper presents an ontology for representing code-level knowledge based on abstract syntax trees, that was developed in the context of studying maintenance problems in a small software company. The ontology enables the utilization of automated reasoning to counter delocalization, and thus to speed up discovery.
[Software maintenance, Laboratories, Ontologies, Counting circuits, abstract syntax trees, Industrial training, program understanding, automated reasoning, software information systems, trees (mathematics), Documentation, source code, documentation, Educational institutions, reverse engineering, software maintenance, inference mechanisms, Computer science, information delocalization, knowledge representation, discovery, low-level searches, Software systems, Computer industry, computer aided software engineering, code-level knowledge representation]
Moving proofs-as-programs into practice
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Proofs in the Nuprl system, an implementation of a constructive type theory, yield "correct-by-construction" programs. In this paper a new methodology is presented for extracting efficient and readable programs from inductive proofs. The resulting extracted programs are in a form suitable for use in hierarchical verifications in that they are amenable to clean partial evaluation via extensions to the Nuprl rewrite system. The method is based on two elements: specifications written with careful use of the Nuprl set-type to restrict the extracts to strictly computational content; and on proofs that use induction tactics that generate extracts using familiar fixed-point combinators of the untyped lambda calculus. In this paper the methodology is described and its application is illustrated by example.
[lambda calculus, rewriting systems, program verification, Induction generators, NASA, inductive proofs, Calculus, type theory, Application software, correct-by-construction programs, specifications, Postal services, hierarchical verifications, partial evaluation (compilers), Computer languages, fixed-point combinators, untyped lambda calculus, partial evaluation, Nuprl rewrite system, theorem proving, proofs-as-programs, Software engineering, constructive type theory]
Data flow analysis within the ITOC information system design recovery tool
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Most contemporary fourth-generation languages (4GL) are tightly coupled with the database server, and other subsystems, that are provided by the vendor. As a result organizations that wish to change database vendors are typically forced to rewrite their applications using the new vendor's 4GL. The anticipated cost of this redevelopment can deter an organization from changing vendors, hence denying it the benefits that would otherwise result, e.g., the exploitation of more sophisticated database server technology. If tools existed that could reduce the rewriting effort, the large upfront cost of migrating the organization's applications would also be reduced, which could make the transition economically feasible. The ITOC project is part of a large collaborative research initiative between the Centre for Software Maintenance at the University of Queensland and Oracle Corporation. The objective of this project is to design and implement a tool that automatically recovers both the application structure and the static schema definition of 4GL information system applications. These recovered system components are transformed into constructs that populate Oracle's Designer 2000 CASE repository. An essential component of the ITOC process is to determine the relationships between different columns in the database and between references to those columns and fields that appear within the user interface. This in turn requires analysis of data flow between variables in the 4GL programs. While data flow analysis has been applied in many applications, for example, code optimization and program slicing, this paper presents the results of using data flow analysis in the construction of a novel design recovery tool for 4GL-based information.
[Software maintenance, Costs, Computer aided software engineering, application structure, query languages, Research initiatives, design recovery tool, 4GL-based information, Information analysis, Information systems, Databases, contemporary fourth-generation languages, ITOC information system design recovery tool, redevelopment, rewriting effort, Data analysis, Collaborative software, data flow analysis, reverse engineering, Application software, software maintenance, systems re-engineering, software portability, computer aided software engineering, static schema definition]
Reactive system validation using automated reasoning over a fragment library
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
While a user might be able to confidently validate a generalized fragment by inspection, since calling an off-hook user should result in a busy signal (when the system only has one line per user), more complex behavior can be impractical to validate by inspection. This is particularly true when the fragment describes an intermediate protocol step, for example, because correctness is often stated in terms of all possible protocol outcomes. The paper illustrates this problem with a fragment describing a step in the CS-NC protocol used by the personal channel agent (PCA), the paper's primary case study. An example correctness property proved in the paper is Property V: given two properly initialized PCAs, CS-NC correctly transmits the new channel from one to the other, and an eavesdropper's action(s) will be detected by at least one of the PCAs, assuming (1) every protocol message sent is eventually received, and (2) only an eavesdropper can discover keys or channel identifiers (with significant probability).
[Protocols, program testing, program verification, reactive system validation, CS-NC protocol, keys, Switches, World Wide Web, HTML, formal specification, software libraries, channel identifiers, automated reasoning, Telephony, Automatic control, personal channel agent, software tools, intermediate protocol step, correctness, Inspection, protocol message, Software libraries, eavesdropper actions, Concrete, fragment library, protocol outcomes, Principal component analysis]
Precise specification and automatic application of design patterns
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Despite vast interest in design patterns, the specification and application of patterns is generally assumed to rely on manual implementation. We describe a precise method of specifying how a design pattern is applied: by phrasing it as an algorithm in a meta-programming language. We present a prototype of a tool that supports the specification of design patterns and their realization in a given program. Our prototype allows automatic application of design patterns without obstructing the source code test from the programmer, who may edit it at will. We demonstrate pattern specification in meta-programming techniques and a sample outcome of its application.
[Algorithm design and analysis, Process design, software tool prototype, object-oriented programming, precise pattern specification, Manuals, Mathematics, Application software, Gas insulated transmission lines, formal specification, source code editing, Programming profession, Computer science, object-oriented design, Collaboration, Prototypes, automatic pattern application, design patterns, source code test, computer aided software engineering, object-oriented methods, software tools, meta-programming language, algorithm]
Formal specification of human-computer interaction by graph grammars under consideration of information resources
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
A successful design of an interactive system requires a clear understanding of human-machine interaction. For the specification of such a system a precise consideration of the user's context during each step of the development process is therefore necessary. Moreover, a formal specification method for expressing interaction is highly desirable in order to achieve a precise and continuous specification process between the requirements and design stages. In this paper several of the environmental cues influencing the user during the interaction with a system are considered. These environmental cues are modelled using the concept of information resources. Interaction is described using these resources and formally specified by the notation of graph grammars in order to be able to reason about a system, to assess a system wrt important properties and, finally to achieve a continuous specification process between the requirements and design stages. This approach will be demonstrated by specifying a safety-critical system concerning the interaction between the pilot and the flight management system on the flight deck of an aircraft.
[human factors, safety-critical software, Mathematics, user interfaces, aircraft control, formal specification, graph grammars, Human resource management, information resources, Information resources, Knowledge representation, aircraft, design stages, flight deck, Knowledge management, Formal specifications, Computer science, environmental cues, human-computer interaction, interactive system, Interactive systems, safety-critical system, User interfaces, flight management system, Aircraft]
Mechanising requirements engineering: reuse and the application of domain analysis technology
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
The paper describes efforts that have made to mechanise the requirements engineering process in an industrial avionics domain. The authors' approach is based on an analysis of both the application domain and the task domain. The paper describes the processes they have used for domain analysis, and the tool they have developed to support mechanisation. They give an initial evaluation of the approach and close with a summary of lessons learnt.
[Visual BASIC, Automation, reuse, Humans, task domain analysis, Aerospace electronics, Programming, domain analysis technology, Paper technology, avionics, Application software, formal specification, aerospace industry, Engines, requirements engineering mechanisation, Computer science, application domain analysis, software reusability, aerospace computing, Computer industry, computer aided software engineering, industrial avionics domain]
Genetic algorithms for dynamic test data generation
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
In software testing, it is often desirable to find test inputs that exercise specific program features. To find these inputs by hand is extremely time-consuming, especially when the software is complex. Therefore, numerous attempts have been made to automate the process. Random test data generation consists of generating test inputs at random, in the hope that they will exercise the desired software features. Often, the desired inputs must satisfy complex constraints, and this makes a random approach seem unlikely to succeed. In contrast, combinatorial optimization techniques, such as those using genetic algorithms, are meant to solve difficult problems involving the simultaneous satisfaction of many constraints. In this paper, we discuss experiments with a test generation problem that is harder than the ones discussed in earlier literature-we use a larger program and more complex test adequacy criteria. We find a widening gap between a technique based on genetic algorithms and those based on random test generation.
[Software testing, Performance evaluation, Materials testing, Minimization methods, program testing, software testing, test adequacy criteria, test data generation, random test generation, genetic algorithms, Genetic algorithms, Fuzzy logic, Constraint optimization, test generation, combinatorial optimization, program features, Automatic testing, Benchmark testing, Automatic control]
Declarative specification of software architectures
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Scaling formal methods to large, complex systems requires methods of modeling systems at high levels of abstraction. In this paper, we describe such a method for specifying system requirements at the software architecture level. An architecture represents a way breaking down a system into a set of interconnected components. We use architecture theories to specify the behavior of a system in terms of the behavior of its components via a collection of axioms. The axioms describe the effects and limits of component variation and the assumptions a component can make about the environment provided by the architecture. As a result of the method the verification of the basic architecture can be separated from the verification of the individual component instantiations. We present an example of using architecture theories to model the task coordination architecture of a multi-threaded plan execution system.
[Design methodology, Hierarchical systems, Formal languages, software architectures, Independent component analysis, abstraction, LAN interconnection, declarative specification, multi-threaded plan execution system, formal specification, Software architecture, USA Councils, Computer architecture, formal methods, interconnected components, Software systems, system requirements, task coordination architecture, Mathematical model, component variation]
Modeling software processes by using process and object ontologies
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
In order to model software processes based on ontologies engineering techniques, this paper presents a methodology to manually construct the following ontologies: an object ontology based on constituent elements for objects, and a process ontology based on relationships between inputs and outputs, such as subsumption relationships. Next, using the constructed ontologies, software process plans are generated for user queries, with both user interaction and constraint satisfaction by generate and test paradigm. Furthermore, experimental results show that the methodology works well in generating software process plans for a query about a software process plan from a basic design and to a detailed design.
[Software testing, Knowledge engineering, Real time systems, user queries, Information resources, object ontologies, Ontologies, process ontology, software process plans, Information analysis, constraint satisfaction, ontologies engineering, software processes, Filling, Concrete, computer aided software engineering, constraint handling, Joining processes, Software engineering]
Correct-schema-guided synthesis of steadfast programs
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
It can be argued that for (semi-)automated software development, program schemas are indispensable, since they capture not only structured program design principles but also domain knowledge, both of which are of crucial importance for hierarchical program synthesis. Most researchers represent schemas purely syntactically (as higher-order expressions). This means that the knowledge captured by a schema is not formalised. We take a semantic approach and show that a schema can be formalised as an open (first-order) logical theory that contains an open logic program. By using a special kind of correctness for open programs, called steadfastness, we can define and reason about the correctness of schemas. We also show how to use correct schemas to synthesise steadfast programs.
[domain knowledge, programming theory, Logic programming, Terminology, program verification, informal knowledge capture, program synthesis, structured program design principles, Data structures, higher-order expressions, semi-automated software development, program schema formalisation, Computer science, syntactic representation, steadfast programs, open logic program, Control system synthesis, logic programming, hierarchical program synthesis, Concrete, schema correctness, open first-order logical theory, Functional programming]
From formal specifications to natural language: a case study
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Because software specifications often serve as a formal contract between the developer and the customer, systems have been proposed that help the software client better understand specifications by automatically paraphrasing them in natural language. The REVIEW system applies natural language generation within Metaview, a metasystem that facilitates the construction of CASE environments to support software specification tasks. This paper summarizes a technical report that presents REVIEW through a case study involving the Object Model of Rumbaugh's OMT specification methodology (1991).
[software specification, Computer aided software engineering, Terminology, Natural languages, Strategic planning, Graph theory, Formal specifications, formal specification, formal specifications, REVIEW system, Databases, Writing, natural languages, Data models, computer aided software engineering, Metaview, software tools, Contracts, natural language, CASE environments]
Application of formal methods to the development of a software maintenance tool
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Partial evaluation is an optimization technique traditionally used in compilation. We have adapted this technique to the understanding of scientific application programs during their maintenance, and we have implemented a tool that analyzes Fortran 90 application programs and performs an interprocedural pointer analysis. This paper presents how we have specified this analysis with different formalisms (inference rules with global definitions and set and relational operators). Then we present the tool implementing these specifications. It has been implemented in a generic programming environment and a graphical interface has been developed to visualize the information computed during the partial evaluation (values of variables, already-analyzed procedures, scope of variables, removed statements, etc.).
[Computer interfaces, Software maintenance, Visualization, optimization technique, relational operators, graphical user interfaces, Fortran 90 application programs, Mathematics, graphical interface, information visualization, formal specification, set operators, generic programming environment, Information analysis, variable values, partial evaluation (compilers), data visualisation, partial evaluation, Performance analysis, software tools, natural sciences computing, inference rules, optimising compilers, program diagnostics, mathematical operators, removed statements, Application software, Formal specifications, software maintenance, inference mechanisms, Programming environments, scientific application program understanding, Computer languages, variable scope, interprocedural pointer analysis, compilation, already-analysed procedures, formal methods, global definitions, software maintenance tool, alias analysis, programming environments]
Towards a design assistant for distributed embedded systems
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
This paper presents an overview of a distributed embedded system design assistant called Systems Engineers Workbench (SEW 2.0). SEW 2.0 uses a suite of design frameworks, with supporting representations, to structure the inherent complexity. A set of design assist operatives further improves the ability of the designer to explore the domain space while evaluating performance/resource tradeoffs.
[Performance evaluation, distributed embedded systems, Design automation, SEW 2.0, distributed processing, Routing, Application software, design assistant, Design engineering, Operating systems, design frameworks, Embedded system, real-time systems, design assist operatives, Systems engineering and theory, Hardware, Space exploration, software tools, Systems Engineers Workbench]
TESS: automated support for the evolution of persistent types
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Persistent data often has a long lifetime. During its lifetime, the types that are used to structure the data may undergo evolution to support new requirements or provide more efficient services. This evolution often makes the persistent data inaccessible unless it also evolves with the types. Existing systems that support type and data evolution focus on changes isolated to individual types, thereby limiting what can be easily accomplished during maintenance. We extend this work by presenting a model of compound type changes that can also describe changes simultaneously involving multiple types and their effects on data. We then describe TESS (Type Evolution Software System), a system to automate the evolution of types and their associated data when the types undergo compound type changes.
[Java, object-oriented programming, Object oriented databases, Eyes, Object oriented modeling, object-oriented databases, persistent data evolution, abstract data types, Relational databases, compound type changes, software maintenance, Computer science, Computer languages, TESS, Software systems, Type Evolution Software System, Database systems, computer aided software engineering, persistent types]
Enhancing the component reusability in data-intensive business programs through interface separation
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Visual development environments have provided good support in the reuse of graphical user interface, report and query generation, and simpler database retrieval and updating. However, many commonly used components for computation and database processing and updating are still required to be repeatedly designed and developed. The main problem is that current methods do not support the separation of component interface from a component. Component interface is not an intrinsic property of the component. Incorporating it into the component affects the reusability of the component adversely. A program representation is proposed in this paper to address the problem. The representation enhances the reusability of a component through separating the component interface front the component.
[Visual BASIC, data-intensive, interface separation, graphical user interface, Information retrieval, Data engineering, Data processing, reusability, Visual databases, visual development environments, Business communication, component interface, database processing, business programs, User interfaces, software reusability, Libraries, Logic, programming environments, component reusability]
Mapping software architectures to efficient implementations via partial evaluation
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
Flexibility is recognized as a key feature in structuring software, and many architectures have been designed to that effect. However, they often come with performance and code size overhead, resulting in a flexibility vs. efficiency dilemma. The source of inefficiency in software architectures can be identified in the data and control integration of components, because flexibility is present not only at the design level but also in the implementation. We propose the use of program specialization in software engineering as a systematic way to improve performance and in some cases, to reduce program size. In particular, we advocate the use of partial evaluation, which is an automatic technique to produce efficient, specialized instances of generic programs. We study several representative, flexible mechanisms found in software architectures: selective broadcast, pattern matching, interpreters, layers, and generic libraries. We show how partial evaluation can systematically be applied in order to optimize those mechanisms.
[program specialization, Costs, pattern matching, software architectures, Software safety, program compilers, software libraries, selective broadcast, interpreters, partial evaluation (compilers), Software architecture, Computer architecture, Broadcasting, layers, partial evaluation, software engineering, inefficiency, generic libraries, code size overhead, generic programs, Software libraries, Software systems, Systems engineering and theory, flexible mechanisms, Pattern matching, Software engineering]
An automated object-oriented testing for C++ inheritance hierarchy
Proceedings 12th IEEE International Conference Automated Software Engineering
None
1997
This paper proposes a concept named unit repeated inheritance (URI) in Z notation to realize object-oriented testing of an inheritance hierarchy. Based on this unit, an inheritance level technique (ILT) method as a guide to test object-oriented software errors in the inheritance hierarchy is described. In addition, two testing criteria, intralevel first and interlevel first, are formed based on the proposed mechanism. Moreover, in order to make the test process automatic, we use LEX and YACC to automatically generate a lexical analyzer and a parser to demonstrate a declaration of C++ source code. And, we also construct a windowing tool used in conjunction with a conventional C++ programming environment to assist a programmer to analyze and test his/her C++ programs.
[Software testing, object-oriented testing, System testing, Acoustic testing, object-oriented programming, program testing, unit repeated inheritance, windowing tool, inheritance, Information management, C++ inheritance hierarchy, Formal specifications, C language, C++ programs, URI, Programming profession, Programming environments, programming environment, interlevel first, intralevel first, Automatic testing, Error correction, Business]
Planning equational verification in CCS
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Most efforts to automate the formal verification of communicating systems have centred around finite-state systems (FSSs). However, FSSs are incapable of modelling many practical communicating systems, and hence there is interest in a novel class of problems, which we call VIPSs (Value-passing Infinite-state Parameterised Systems). Existing approaches using model checking over FSSs are insufficient for VIPSs, due to their inability both to reason with and about domain-specific theories, and to cope with systems having an unbounded or arbitrary state space. We use the Calculus of Communicating Systems (CCS) with parameterised constants to express and specify VIPSs. We use the laws of CCS to conduct the verification task. This approach allows us to study communicating systems, regardless of their state space, and the data such systems communicate. Automating theorem proving in this system is an extremely difficult task. We provide automated methods for CCS analysis; they are applicable to both FSSs and VIPSs. Adding these methods to the Clam proof-planner, we have implemented an automated theorem prover that is capable of dealing with problems outside the scope of current methods. This paper describes these methods, gives an account as to why they work and provides a short summary of experimental results.
[parameterised constants, Clam proof-planner, finite-state systems, Electronic switching systems, finite state machines, equations, automatic theorem proving, planning (artificial intelligence), formal verification, equational verification planning, automatic formal verification, Automatic control, theorem proving, Carbon capture and storage, VIPS, domain-specific theories, Automation, Identity-based encryption, State-space methods, Equations, Computer science, value-passing infinite-state parameterised systems, calculus of communicating systems, CCS, Calculus of Communicating Systems, unbounded state space, model checking, Artificial intelligence, Formal verification, state-space methods]
Synthesizing software architecture descriptions from Message Sequence Chart specifications
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Message Sequence Chart (MSC) specifications have found their way into many software engineering methodologies and CASE tools, in particular to represent early life-cycle requirements and high-level design specifications. We analyze iterating and branching MSC specifications with respect to their software architectural content. We present algorithms for the automated synthesis of Real-Time Object-Oriented Modeling (ROOM) models from MSC specifications and discuss their implementation in the MESA toolset.
[Real time systems, Computer aided software engineering, Protocols, diagrams, formal specification, software architecture, Message Sequence Chart specifications, Software architecture, high-level design specifications, specification languages, software architecture descriptions, ROOM models, MSC specifications, object-oriented programming, Object oriented modeling, MESA toolset, Software algorithms, software engineering methodologies, CASE tools, Heating, real-time systems, computer aided software engineering, Temperature control, Error correction, life-cycle requirements, Real-Time Object-Oriented Modeling, Software engineering]
Explaining synthesized software
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Motivated by NASA's need for high-assurance software, NASA Ames' Amphion project has developed a generic program generation system based on deductive synthesis. Amphion has a number of advantages, such as the ability to develop a new synthesis system simply by writing a declarative domain theory. However, as a practical matter, the validation of the domain theory for such a system is problematic because the link between generated programs and the domain theory is complex. As a result, when generated programs do not behave as expected, it is difficult to isolate the cause, whether it be an incorrect problem specification or an error in the domain theory. The paper describes a tool being developed that provides formal traceability between specifications and generated code for deductive synthesis systems. It is based on extensive instrumentation of the refutation-based theorem prover used to synthesize programs. It takes augmented proof structures and abstracts them to provide explanations of the relation between a specification, a domain theory, and synthesized code. In generating these explanations, the tool exploits the structure of Amphion domain theories, so the end user is not confronted with the intricacies of raw proof traces. This tool is crucial for the validation of domain theories as well as being important in every-day use of the code synthesis system.
[generic program generation system, generated programs, explanation, formal specification, deductive synthesis, refutation-based theorem prover, Space technology, Abstracts, Solar system, software tools, theorem proving, code synthesis system, Instruments, NASA, NASA Ames' Amphion project, incorrect problem specification, tool, Geometry, domain theory error, synthesized software explanation, Software libraries, formal traceability, Writing, Signal synthesis, declarative domain theory, Software engineering, high-assurance software, augmented proof structures]
Automating UI generation by model composition
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Automated user-interface generation environments have been criticized for their failure to deliver rich and powerful interactive applications. To specify more powerful systems, designers require multiple specialized modeling notations. The model-composition problem is concerned with automatically deriving powerful, correct, and efficient user interfaces from multiple models specified in different notations. Solutions balance the advantages of separating code generation into specialized code generators with deep, model-specific knowledge against the correctness and efficiency obstacles that result from such separation. We present a correct and efficient solution that maximizes the advantage of separation through run-time composition mechanisms.
[user interface management systems, Automation, Liver, automated user-interface generation, Educational institutions, specification, formal specification, program compilers, run-time composition mechanisms, Read only memory, Computer science, model composition, Power engineering computing, code generation, User interfaces, multiple specialized modeling notations, software tools, interactive applications, Power generation, Graphical user interfaces, Power engineering and energy]
Identifying pre-conditions with the Z/EVES theorem prover
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Starting from a graphical data model (a subset of the OMT object model), a skeleton of formal specification can be generated and completed to express several constraints and provide a precise formal data description. Then standard operations to modify instances of this data model can be systematically specified. Since these operations may invalidate the constraints, it is interesting to identify their pre-conditions. In this paper, the Z-EVES theorem prover is used to calculate and try to simplify the pre-conditions of these operations. Then the developer may identify a set of conditions and use the prover to verify that they logically imply the pre-condition.
[Encapsulation, Automation, Unified modeling language, Natural languages, preconditions identification, OMT object model, data description, Electrical capacitance tomography, Formal specifications, formal specification skeleton, formal specification, constraints, Read only memory, data models, formal data description, data model instance modification operations, graphical data model, Skeleton, computer aided software engineering, theorem proving, object-oriented methods, logical implication, Testing, Z-EVES theorem prover]
Development, assessment, and reengineering of language descriptions
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Discusses tools that aid in the development, assessment and reengineering of language descriptions (i.e. syntactic descriptions of a language). We develop assessment tools that give an indication as to what is wrong with an existing language description, and give hints towards its correction. From a correct and complete language description, it is possible to generate a parser, a manual and online documentation. The parser is geared towards reengineering purposes, but is also used to parse the examples that are contained in the documentation. The reengineered language description is a basic ingredient for a reengineering factory that can manipulate this language. The described tool support can also be used to develop a language standard without syntax errors in the language description and its code examples.
[assessment tools, online documentation, system documentation, Production facilities, Code standards, programming languages, document generation, code examples, grammar, software tools, syntactic descriptions, Standards development, user manual, error-free language standard, description correction, computer-aided language engineering, message sequence charts, Documentation, reengineering, system renovation, Pattern recognition, Programming environments, Computer science, tool support, Computer languages, systems re-engineering, language descriptions, grammars, parser, Computer errors, Concrete, computer aided software engineering, software standards]
Planware-domain-specific synthesis of high-performance schedulers
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Planware is a domain-specific generator of high-performance scheduling software, currently being developed at the Kestrel Institute. Architecturally, Planware is an extension of the Specware system with domain-independent and domain-dependent parts. The domain-independent part includes a general algorithm design facility (including mechanisms to synthesize global-search and constraint propagation algorithms), as well as support for theorem-proving and witness finding. The domain-dependent part includes scheduling domain knowledge and architecture representations, and other domain-specific refinement knowledge that relates the scheduling domain to general algorithm design and data type refinement. Using Planware, the user interactively specifies a problem and then the system automatically generates a formal specification and refines it.
[Algorithm design and analysis, Process design, domain knowledge, Taxonomy, constraint propagation algorithm synthesis, Transportation, witness finding, architecture representations, Planware, formal specification, Design optimization, Constraint optimization, Specware system, domain-independent part, scheduling, Constraint theory, theorem proving, Logic, constraint handling, domain-specific refinement knowledge, domain-specific synthesis, high-performance scheduling software, general algorithm design facility, automatic formal specification generation, Formal specifications, global search algorithm synthesis, Scheduling algorithm, data type refinement, planning, domain-dependent part, application generators]
Don't verify, abstract!
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Describes a notation and tool for demonstrating to a third-party certifier that software written in a subset of Ada is safe, and gives some experience of using them on real projects. The thesis underlying the design is that people write adequate code, but that they make design and implementation decisions which can conflict with each other to introduce safety problems. The usual paradigm of formally specifying and then developing and verifying the code is less cost-effective than writing the code and then abstracting it to a level that is suitable for human judgements to be made. This is because there are more people who know how to write good code than those who can write effective formal specifications. The tool processes a formal, or informal, argument that code meets its safety requirements using literate programming and concepts from the refinement calculus developed at Oxford University.
[safety requirements, Costs, Humans, refinement calculus, Calculus, Software safety, literate programming, formal specification, design decisions, Program processors, Ada subset, implementation decisions, safety, notation, software tools, third-party certifier, cost-effectiveness, code verification, code abstraction, code development, Documentation, adequate code, human judgements, Sparks, Formal specifications, certification, software tool, Writing, computer aided software engineering, Software tools, Ada]
Developing the designer's toolkit with software comprehension models
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Cognitive models of software comprehension are potential sources of theoretical knowledge for tool designers. Although their use in the analysis of existing tools is fairly well established, the literature has shown only limited use of such models for directly developing design ideas. This paper suggests a way of utilizing existing cognitive models of software comprehension to generate design goals and suggest design strategies early in the development cycle. A crucial part of our method is a scheme for explaining the value of tool features by describing the mechanisms that are presumed to underly the expected improvements in task performance.
[Software testing, Software maintenance, Vocabulary, design ideas development, reverse engineering, task performance improvements, Cognition, software comprehension models, Electrical capacitance tomography, design goals generation, Programming profession, tool feature value, Software design, program understanding, psychology, design strategies, cognitive models, computer aided software engineering, software tools, software designer's toolkit, Software tools, Usability, software development cycle]
Component-based software process support
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Only recently has the research community started to consider how to make software process models interoperable and reusable. The task is difficult Software processes are inherently creative and dynamic difficult to define and repeat at an enactable level of detail. Additionally, interoperability and reusability have not been considered important issues. Recent interoperability and reusability solutions advocate the development of standard process model representations based on common concepts or generic schemas, which are used as a basis for translating between heterogeneous process representations. The authors propose an alternative approach through the development of process-based components. They present the Open Process Components Framework, a component based framework for software process modeling. In this approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of process representations, an explicit representation of process state, and an extendable set of class relationships.
[Encapsulation, Solid modeling, standard process model representations, open systems, Petri nets, reusability, heterogeneous process representations, common concepts, object-oriented methods, Standards development, Open Process Components Framework, Software reusability, object-oriented programming, explicit process state representation, Object oriented modeling, software process modeling, interoperability, Application software, generic schemas, Computer science, Computer languages, component-based software process support, software reusability, extendable class relationships, computer aided software engineering]
Detection of exclusive OR global predicates
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Detecting global predicates in a distributed program is a useful tool for debugging and testing the program. Past research has considered several restricted forms of predicates, including conjunctive predicates and linked predicates, and their detection algorithms. The authors introduce an exclusive OR global predicates to describe exclusive usages of shared resources in distributed programs. An exclusive OR global predicate holds for a given run only when one or zero local predicate is true at every consistent global state during the run. One exclusive OR global predicate is enough to describe the mutual exclusion condition of n processes, while it takes O(n/sup 2/) conjunctive predicates. Moreover, the exclusive OR condition is easily detectable by sequentializing all true events in a given run. A centralized algorithm of detecting exclusive OR global predicates is presented.
[conjunctive predicates, program debugging, detection algorithms, processes, sequentialized true events, Event detection, program testing, consistent global state, Laboratories, Electronic equipment testing, Debugging, exclusive shared resource usage, Telecommunication computing, centralized algorithm, Distributed computing, linked predicates, exclusive OR global predicate detection, Automatic testing, mutual exclusion condition, distributed program, Detection algorithms, distributed programming, Clocks, computational complexity]
Towards a theory for integration of mathematical verification and empirical testing
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
From the viewpoint of a project manager who is responsible for the verification and validation (V&V) of a software system, mathematical verification techniques provide a potentially valuable addition to otherwise standard empirical testing. However, the value they add, both in terms of coverage and in fault detection, has been difficult to quantify. Potential cost savings from replacing testing with mathematical techniques cannot be realized until the tradeoffs can be quantified. This paper first describes a framework for a theory of software fault detection that is based on software reliability and formalized fault models. The novelty of this approach is that it takes into account the relative utility of the various tools for fault detection. Second, the paper describes a utility model for integrating mathematical and empirical techniques with respect to fault detection and coverage analysis for software. Third, the paper discusses how to determine the optimal combination of black-box testing, white-box (structural) testing and formal methods in V&V of a software system. Finally, a demonstration of how this utility model can be used in practice is offered using a case study from a NASA software system.
[Software testing, System testing, Costs, program verification, program testing, software fault detection, cost savings, software system verification, software reliability, Project management, utility model, Software standards, Mathematical model, mathematical verification, software project management, white-box testing, programming theory, software system validation, coverage analysis, tradeoffs, NASA, black-box testing, NASA software system, empirical testing, Software reliability, fault detection tool utility, case study, Fault detection, formalized fault models, structural testing, formal methods, Software systems]
An automated approach for supporting software reuse via reverse engineering
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Formal approaches to software reuse rely heavily upon a specification matching criterion, where a search query using formal specifications is used to search a library of components indexed by specifications. In previous investigations, we addressed the use of formal methods and component libraries to support software reuse and construction of software based on component specifications. A difficulty for all formal approaches to software reuse is the creation of the formal indices. We have developed an approach to reverse engineering that is based on the use of formal methods to derive formal specifications of existing programs. In this paper, we present an approach for combining software reverse engineering and software reuse to support populating specification libraries for the purposes of software reuse. In addition, we discuss the results of our initial investigations into the use of tools to support an entire process of populating and using a specification library to construct a software application.
[Reverse engineering, formal specification, software libraries, Postal services, component library searching, Propulsion, specification matching criterion, software tools, component specifications, indexing, NASA, specification libraries, reverse engineering, search query, Formal specifications, Application software, formal specifications, formal indices, Computer science, Software libraries, software application construction, formal methods, software reusability, Software systems, computer aided software engineering, automated software reuse support]
An automated framework for structural test-data generation
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Structural testing criteria are mandated in many software development standards and guidelines. The process of generating test data to achieve 100% coverage of a given structural coverage metric is labour-intensive and expensive. This paper presents an approach to automate the generation of such test data. The test-data generation is based on the application of a dynamic optimisation-based search for the required test data. The same approach can be generalised to solve other test-data generation problems. Three such applications are discussed-boundary value analysis, assertion/run-time exception testing, and component re-use testing. A prototype tool-set has been developed to facilitate the automatic generation of test data for these structural testing problems. The results of preliminary experiments using this technique and the prototype tool-set are presented and show the efficiency and effectiveness of this approach.
[Software testing, Costs, program testing, exception handling, Programming, component reuse testing, optimisation, boundary-value problems, Simulated annealing, automated structural test-data generation, Software standards, assertion/run-time exception testing, Standards development, search problems, dynamic optimisation-based search, program control structures, structural testing criteria, Automation, efficiency, structural coverage metric, Application software, prototype tool-set, Computer science, software development standards, boundary value analysis, Automatic testing, software reusability, computer aided software engineering, subroutines]
Towards the automated debugging and maintenance of logic-based requirements models
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
We describe a tools environment which automates the validation and maintenance of a requirements model written in many-sorted first order logic. We focus on a translator that produces an executable form of the model; blame assignment functions, which input batches of mis-classified tests (i.e. training examples) and output likely faulty parts of the model; and a theory reviser; which inputs the faulty parts and examples and outputs suggested revisions to the model. In particular we concentrate on the problems encountered when applying these tools to a real application: a requirements model containing air traffic control separation standards, operating methods and airspace information.
[Knowledge engineering, program debugging, theory reviser, Mathematics, blame assignment functions, formal logic, air traffic control, formal verification, tools environment, logic-based requirements models, Logic, algebraic specification, executable form, Testing, Knowledge based systems, Debugging, airspace information, requirements model maintenance, Air traffic control, Formal specifications, Bridges, requirements model debugging, computer aided software engineering, many-sorted first order logic, Artificial intelligence]
Dowsing: a tool framework for domain-oriented browsing of software artifacts
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Program understanding relates a computer program to the goals and requirements it is designed to accomplish. Application-domain analysis is a source of information that can aid program understanding by guiding the source-code analysis and providing structure to its results. The authors use the term "dowsing" to describe the process of exploring software and the related documentation from an application-domain point of view. They have designed a tool framework to support dowsing and have populated it with a variety of commercial and research tools.
[Software maintenance, Vocabulary, Information resources, software exploration, Reverse engineering, system documentation, Documentation, source-code analysis, documentation, reverse engineering, Application software, software maintenance, Information analysis, Computer languages, program understanding, computer program, software artifact, software tools, tool framework, Software tools, Pattern analysis, application-domain analysis, domain-oriented browsing]
A tool for automated system analysis based on modular specifications
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
An effective means for analyzing and reasoning on software systems is to use formal specifications to simulate their execution. The simulation traces can be used for specification testing and reused, later in the development process, for functional testing of the system. It is widely acknowledged that, to deal with the complexity of industrial-size systems, specifications must be structured into modules providing abstraction mechanisms and clear interfaces. In past work (D. Mandrioloi et al., 1995), we defined and implemented a method for simulating specifications written in the TRIO temporal logic language, and applied it to functional testing of time-critical industrial systems. In this paper, we report on a tool for analyzing TRIO specifications taking advantage of their modular structure, overcoming the well-known state-explosion problem and making the proposed method really scalable. We discuss the fundamental operations and the algorithms on which the tool is based. Then we illustrate its use in a realistic case study inspired by an industrial application. Finally, we comment on the overall results in terms of the usability of the tool and the effectiveness of the approach, and we suggest some future improvements.
[simulation trace reuse, state-explosion problem, System testing, Acoustic testing, temporal logic, industrial-size systems, Electrical capacitance tomography, Logic testing, formal specification, Analytical models, functional testing, usability, abstraction mechanisms, software tools, Mathematical model, module interfaces, formal languages, modular specifications, software execution simulation, automated system analysis tool, case study, specification testing, systems analysis, Ear, virtual machines, Writing, TRIO temporal logic language, scalable method, Animation, computer aided software engineering, subroutines, Time factors]
From Z to BON/Eiffel
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
The article shows how to make a transition from the Z formal notation to the Business Object Notation (BON). It is demonstrated that BON has the expressive power of Z, with the additional advantages of object-orientation and a supporting seamless development method. The transition is illustrated for some widely used Z constructs. The translation lays the groundwork for a semi-automated tool for extracting classes from Z specifications.
[class extraction, Business Object Notation, Z specifications, Formal specifications, formal specification, object-orientation, seamless development method, Bridges, Computer science, Z formal notation translation, Computer languages, semi-automated tool, Set theory, Computer industry, Libraries, object-oriented methods, software tools, Software tools, Eiffel]
Testing using log file analysis: tools, methods, and issues
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Large software systems often keep log files of events. Such log files can be analyzed to check whether a run of a program reveals faults in the system. We discuss how such log files can be used in software testing. We present a framework for automatically analyzing log files, and describe a language for specifying analyzer programs and an implementation of that language. The language permits compositional, compact specifications of software, which act as test oracles; we discuss the use and efficacy of these oracles for unit- and system-level testing in various settings. We explore methodological issues such as efficiency and logging policies, and the scope and limitations of the framework. We conclude that testing using log file analysis constitutes a useful methodology for software verification, somewhere between current testing practice and formal verification methodologies.
[System testing, Identity-based encryption, program testing, program verification, software verification, logging policies, test oracles, unit-level testing, log file analysis, Formal specifications, formal specification, Programming profession, Computer science, Reactive power, formal verification, Prototypes, specification languages, system-level testing, software tools, system faults, specification language, Formal verification]
ASSISTing exit decisions in software inspection
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Software inspection is a valuable technique for detecting defects in the products of software development. One avenue of research within inspection concerns the development of computer support. It is hoped that such support will provide even greater benefits when applying inspection. A number of prototype systems have been developed by researchers, yet these suffer from some fundamental limitations. One of the most serious of these concerns is the lack of facilities to monitor the process, and to provide the moderator with quantitative information on the performance of the process. The paper begins by briefly outlining the measurement component that has been introduced into the system, and especially discusses an approach to estimating the effectiveness of the current process.
[Poles and towers, software inspection, moderator, ASSIST, Electrical capacitance tomography, Read only memory, Postal services, defect detection, measurement component, Prototypes, software tools, software development products, quantitative information, computer support, Software prototyping, Inspection, process performance, Time measurement, process monitoring, Computer science, exit decisions, Current measurement, system monitoring, computer aided software engineering, software metrics]
Automated knowledge acquisition and application for software development projects
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
The application of empirical knowledge about the environment-dependent software development process is mostly based on heuristics. In this paper, we show how one can express these heuristics by using a tailored fuzzy expert system. Metrics are used as input, enabling a prediction for a related quality factor like correctness, defined as the inverse of criticality or error-proneness. By using genetic algorithms, we are able to extract the complete fuzzy expert system out of the available data of a finished project. We describe its application for the next project executed in the same development environment. As an example, we use complexity metrics which are used to predict the error-proneness of software modules. The feasibility and effectiveness of the approach is demonstrated with results from large switching system software projects. We present a summary of the lessons learned and give our ideas about further applications of the approach.
[error-proneness, expert systems, program verification, quality factor prediction, Programming, empirical knowledge application, criticality, electronic switching systems, Data mining, telecommunication computing, automated knowledge acquisition, Genetic algorithms, environment-dependent software development process, heuristics, fuzzy expert system, switching system, fuzzy systems, complexity metrics, program correctness, Switching systems, Knowledge acquisition, Q factor, knowledge acquisition, genetic algorithms, software modules, Application software, software development projects, Software systems, computer aided software engineering, Error correction, subroutines, Hybrid intelligent systems, software metrics]
Programmatic testing of the Standard Template Library containers
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
We describe part of an STL conformance test suite under development. Test suites for all of the STL containers have been written, demonstrating the feasibility of thorough and highly automated testing of industrial component libraries. We describe affordable test suites that provide good code and boundary value coverage, including the thousands of cases that naturally occur from combinations of boundary values. We show how two simple oracles can provide fully automated output checking for all the containers. We refine the traditional categories of black-box and white-box testing to specification-based, implementation-based and implementation-dependent testing, and show how these three categories highlight the key cost/thoroughness trade-offs.
[Costs, program testing, boundary value coverage, Standard Template Library containers, Containers, automated testing, conformance testing, formal specification, Read only memory, software libraries, oracles, Libraries, Hardware, industrial component libraries, Software reusability, white-box testing, specification-based testing, STL containers, black-box testing, implementation-based testing, implementation-dependent testing, STL conformance test suite, Automatic testing, Keyboards, Computer industry, Mice]
Automated software test data generation for complex programs
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
We report on GADGET, a new software test generation system that uses combinatorial optimization to obtain condition/decision coverage of C/C++ programs. The GADGET system is fully automatic and supports all C/C++ language constructs. This allows us to generate tests for programs more complex than those previously reported in the literature. We address a number of issues that are encountered when automatically generating tests for complex software systems. These issues have not been discussed in earlier work on test-data generation, which concentrates on small programs (most often single functions) written in restricted programming languages.
[Software testing, Performance evaluation, System testing, object-oriented programming, program testing, Instruments, C++ language, C language, software test data generation, C++ programs, Genetic algorithms, GADGET, software test generation system, combinatorial optimization, optimisation, Automatic testing, Prototypes, complex programs, Simulated annealing, C programs, Software systems, condition decision coverage]
A configurable automatic instrumentation tool for ANSI C
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Automatic software instrumentation is usually done at the machine level or is targeted at specific program behavior for use with a particular monitoring application. The paper describes CCI, an automatic software instrumentation tool for ANSI C designed to serve a broad range of program execution monitors. CCI supports high level instrumentation for both application-specific behavior as well as standard libraries and data types. The event generation mechanism is defined by the execution monitor which uses CCI, providing flexibility for different monitors' execution models. Code explosion and the runtime cost of instrumentation are reduced by declarative configuration facilities that allow the monitor to select specific events to be instrumented. Higher level events can be defined by combining lower level events with information obtained from semantic analysis of the instrumented program.
[Costs, runtime cost, automatic software instrumentation, software libraries, Radiofrequency interference, Condition monitoring, Runtime, event generation mechanism, declarative configuration facilities, data types, software tools, Instruments, Computerized monitoring, code explosion, configurable automatic instrumentation tool, libraries, Explosions, Application software, program processors, Software libraries, program execution monitors, application-specific behavior, semantic analysis, system monitoring, computer aided software engineering, ANSI C, program visualisation, Software tools, high level instrumentation, instrumented program]
Illustrating object-oriented library reuse by example: a tool-based approach
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
The authors present a tool-based approach that examines how example programs reuse a particular library. The approach can facilitate reuse by: (1) guiding the developer towards important library classes of general utility; (2) guiding the developer towards library classes particularly useful for a specific application domain; and (3) providing access to the relevant source code in each example for further inspection. The approach is supported by CodeWeb, a reuse tool they have built for C++ and Java libraries.
[Java, object-oriented programming, tool-based approach, Inspection, library classes, Electrical capacitance tomography, Sparks, Application software, software libraries, Computer science, Java libraries, Software libraries, object-oriented library reuse by example, C++ libraries, software reusability, object-oriented methods, software tools, CodeWeb reuse tool, Books]
Brewing fresh Java from legacy Lisp-an experiment in automated reverse engineering
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
The issues of re-engineering and reverse engineering have become important ones in the computing industry. A legacy system that has evolved has usually been worked on by many different programmers and reflects the different programming styles as practised by those programmers. We address the re-engineering of a large system, the TAMPR automatic program transformation system, written in pure Lisp. TAMPR is an essential tool in ongoing research on potential applications of automated program transformation. The program implementing the TAMPR system is better designed and more consistently coded than most legacy systems. Why, then, is reverse engineering being attempted for this system, given that it suffers few of the problems of more traditionally implemented legacy systems? We are interested in studying the problem of abstraction in reverse engineering, and the TAMPR system, with its systematic design and coding, provides a good starting point for studying approaches to automated abstraction to an object-oriented form. In addition, while the system in its present form meets the current needs of its users, there are problems with providing widely available, efficient implementations of the system. The target language for this experiment in reverse engineering is Java. Java was chosen because of its widespread availability, claimed portability and its integration with components for the construction of graphical user interfaces. We use TAMPR transformations to reverse engineer the TAMPR program itself.
[object-oriented form, graphical user interfaces, Reverse engineering, Laboratories, Electrical capacitance tomography, user needs, efficient implementations, component integration, Java, object-oriented programming, programming styles, legacy Lisp code, reverse engineering, TAMPR automatic program transformation system, software maintenance, Programming profession, systems reengineering, program interpreters, systems re-engineering, software portability, legacy systems, Writing, Computer industry, LISP, computer aided software engineering, automated reverse engineering, automated abstraction, Software engineering]
ADLscope: an automated specification-based unit testing tool
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Specification-based testing is important because it relates directly to what the program is supposed to do and can detect certain errors that are often not detected by traditional code-based testing techniques such as branch coverage and statement coverage. We have developed an automated testing tool, called ADLscope, that utilizes the formal specification of a program unit as the basis for test coverage measurement. A tester uses ADLscope to test application programming interfaces (APIs) written in the C programming language. The API must be formally specified in the Assertion Definition Language (ADL). The tester uses ADLscope to generate coverage conditions from a program's ADL specifications. When the API is tested, ADLscope automatically measures how many of the coverage conditions have been covered by the tests.
[Software testing, Materials testing, application program interfaces, program testing, ADL specifications, application programming interfaces, Electrical capacitance tomography, conformance testing, error detection, automated specification-based unit testing tool, formal specification, Tellurium, Read only memory, Assertion Definition Language, coverage conditions, specification languages, software tools, ADLscope, Sun, Computer science, Automatic testing, test coverage measurement, Computer errors, C programming language., computer aided software engineering, API formal specification, Gas detectors]
A design pattern based approach to generating synchronization adaptors from annotated IDL
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Middleware forms such as CORBA and DCOM provide standard component interfaces, interaction protocols and communication services to support interoperability of object-oriented applications operating in heterogeneous and distributed environments. General-purpose services and facilities foster re-use and help reduce development costs. Yet the degree of automation of the software development process is limited to the generation of skeleton and stub code from component interface specifications given in a common interface definition language (IDL). This is mainly due to the fact that the expressiveness of current IDLs is limited to the specification of type and operation signatures. Important properties of crucial components of security-, safety-critical or reactive applications such as object behavior, timing or synchronization constraints cannot be documented formally, let alone checked automatically. In this paper, we continue developing solutions for adding specifications of semantic properties to component interfaces and automatically synthesizing code that instruments corresponding semantic checks. Independently from the concrete syntax and semantics of such specification elements, we present a collection of design patterns that allow the designer to seamlessly integrate the synthesized code with the code frames generated by standard IDL compilers. We study these approaches along the concrete example of extending CORBA IDL with synchronization constraints and we evaluate several implementations, solely based on standardized features of the CORBA standard.
[Protocols, interaction protocols, Programming, Communication standards, CORBA standard, type signatures, specification languages, operation signatures, object-oriented methods, code frames, annotated IDL, object behavior, timing, synchronization constraints, interoperability, distributed environments, Middleware, automatic code synthesis, synchronisation, skeleton code, stub code, object-oriented applications, semantic properties specification, Concrete, Timing, component interface specifications, IDL compilers, standard component interfaces, Costs, open systems, security-critical applications, safety-critical software, design pattern, general-purpose services, expressiveness, Skeleton, distributed object management, middleware, client-server systems, automatic programming, Automation, heterogeneous environments, interface definition language, Application software, safety-critical applications, software development process automation, reactive applications, synchronization adaptors, semantic checks, communication services, standardized features]
On detecting and handling inconsistencies in integrating software architecture design and performance evaluation
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
We consider the problem of detecting and handling inconsistencies in software development processes using a graph based approach. It seems to be a natural way to express the various options and possibilities in attacking the problem. We apply the techniques developed in the area of software architecture design which uses in a structured way performance models in order to produce a design which incorporates also nonfunctional requirements in terms of quantitative performance.
[Spirals, graph theory, Programming, software architecture design, performance evaluation, Electrical capacitance tomography, Software safety, performance models, quantitative performance, Reactive power, software architecture, Software design, Software architecture, inconsistency handling, nonfunctional requirements, Collaborative work, Logic, graph based approach, software development processes, Gas detectors, software performance evaluation]
Reusability hypothesis verification using machine learning techniques: a case study
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Since the emergence of object technology, organizations have accumulated a tremendous amount of object-oriented (OO) code. Instead of continuing to recreate components that are similar to existing artifacts, and considering the rising costs of development, many organizations would like to decrease software development costs and cycle time by reusing existing OO components. This paper proposes an experiment to verify three hypotheses about the impact of three internal characteristics (inheritance, coupling and complexity) of OO applications on reusability. This verification is done through a machine learning approach (the C4.5 algorithm and a windowing technique). Two kinds of results are produced: (1) for each hypothesis (characteristic), a predictive model is built using a set of metrics derived from this characteristic; and (2) for each predictive model, we measure its completeness, correctness and global accuracy.
[cycle time, complexity, Computer aided software engineering, Costs, software reusability hypothesis verification, Predictive models, Programming, inheritance, Electrical capacitance tomography, machine learning techniques, object technology, coupling, formal verification, learning (artificial intelligence), Software reusability, global accuracy, Testing, development costs, Automation, object-oriented programming, model correctness, predictive model, internal characteristics, object-oriented code, C4.5 algorithm, Application software, case study, component reuse, Machine learning, software reusability, windowing technique, computer aided software engineering, model completeness, subroutines, software metrics, computational complexity]
Management of evolving specifications using category theory
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Structure is important in large specifications for understanding, testing and managing change. Category theory has been explored as framework for providing this structure, and has been successfully used to compose specifications. This work has typically adopted a "correct-by-construction" approach: components are specified, proved correct and then composed together in such a way as to preserve their properties. However, in a large project, it is desirable to be able to mix specification and composition steps such that, at any particular moment in the process, we may have established only some of the properties of the components and some of the composition relations. In this paper, we propose adaptations to the categorical framework in order to manage evolving specifications. We demonstrate the utility of the framework on the analysis of a part of a software change request for the Space Shuttle.
[component properties preservation, software change request, Space shuttles, space vehicles, software maintenance, categorical framework, formal specification, change management, evolving specifications management, component composition, correct-by-construction approach, formal verification, Space Shuttle, management of change, aerospace computing, category theory, composition relations, Testing, component correctness proving]
An empirical study of the evolution of a software system
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
The cost-effective and reliable evolution of systems is a significant software engineering challenge. Our approach is based on a combination of product modelling, process modelling and software metrics. We describe an empirical laboratory study following the evolution of three releases of a publicly available exemplar system. Analysis of the metrics which were collected improves our understanding of how a system evolves. Process metrics can provide information about how the product is put together, and product metrics suggest explanations for the development processes observed.
[Software maintenance, Costs, Laboratories, Humans, reliability, process metrics, product modelling, software evolution, software releases, product metrics, Software metrics, Computer architecture, empirical software engineering, software engineering, software development processes, software system evolution, cost-effectiveness, software reuse, process modelling, Product design, software maintenance, Computer science, Software systems, Software engineering, software metrics]
Schema-guided synthesis of constraint logic programs
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
By focusing on the families of assignment and permutation problems (such as graph colouring and n-Queens), we show how to adapt D.R. Smith's (1990) KIDS approach for the synthesis of constraint programs (with implicit constraint satisfaction code), rather than applicative Refine programs with explicit constraint propagation and pruning code. Synthesis is guided by a global search schema and can be fully automated with little effort, due to some innovative ideas. CLP (Sets) programs are equivalent in expressiveness to our input specifications. The synthesised CLP (FD) programs would be, after optimising transformations, competitive with carefully hand-crafted ones.
[global search schema, programming theory, Logic programming, constraint logic programs, Optimization methods, CLP, applicative Refine programs, Search problems, implicit constraint satisfaction code, Information technology, formal specification, graph colouring, Information science, Boolean functions, assignment problems, permutation problems, schema-guided program synthesis, KIDS approach, Cost function, computer aided software engineering, n-Queens, constraint handling, search problems]
Automated integrative analysis of state-based requirements
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Statically analyzing requirements specifications to assure that they possess desirable properties is an important activity in any rigorous software development project. The analysis is performed on an abstraction of the original requirements specification. Abstractions in the model may lead to spurious errors in the analysis output. Spurious errors are conditions that are reported as errors, but information abstracted out of the model precludes the reported conditions from being satisfied. A high ratio of spurious errors to true errors in the analysis output makes it difficult, error-prone, and time consuming to find and correct the true errors. We describe an iterative and integrative approach for analyzing state-based requirements that capitalizes on the strengths of a symbolic analysis component and a reasoning component while circumventing their weaknesses. The resulting analysis method is fast enough and automated enough to be used on a day-to-day basis by practicing engineers, and generates analysis reports with a small ratio of spurious errors to true errors.
[automated integrative analysis, Error analysis, state-based requirements analysis, reasoning, Birth disorders, Programming, static analysis, software development project, Data structures, Electrical capacitance tomography, spurious errors, formal specification, symbolic analysis, Read only memory, Information analysis, Computer science, Boolean functions, formal verification, systems analysis, requirements specifications, computer aided software engineering, Error correction]
Statically checkable design level traits
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
The paper is concerned with those properties of software that can be statically surmised from the source code. Many such properties have been extensively studied from the perspective of compiler construction technology. However, live variable analysis, alias analysis and the such are too low level to be of interest to the software engineer. The authors identify a family of statically checkable properties that should represent a higher level abstraction, and reach the detailed design level. Properties in this family which is defined by five precise distinguishing criteria are called traits. Some examples of traits include mutability, const correctness, ownership, and pure functions. In fact, in many ways, traits are non-standard types. They argue that traits should bring about similar benefits to these of static typing in terms of clarity, understandability, adherence to design decisions, and robustness. They further argue that traits can be used for better checking of substitutability in inheritance relationships. Having made the case for traits, they proceed to describing a taxonomy for classifying and understanding traits and show how it can be used to better understand previous work on this topic. The paper also discusses the abstract computational complexity of traits and compares previous research from that perspective.
[live variable analysis, detailed design level, traits, understandability, Laboratories, Taxonomy, inheritance, Gas insulated transmission lines, higher level abstraction, design decisions, const correctness, inheritance relationship, abstract computational complexity, Cities and towns, software engineering, mutability, software properties, Collaborative software, source code, robustness, statically checkable design level traits, Rivers, Milling machines, Computer science, Computer languages, pure function, ownership, clarity, Software systems, system monitoring, substitutability, alias analysis, computational complexity, compiler construction technology]
Specification-based browsing of software component libraries
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Specification-based retrieval provides exact content-oriented access to component libraries but requires too much deductive power. Specification-based browsing evades this bottleneck by moving any deduction into an off-line indexing phase. In this paper, we show how match relations are used to build an appropriate index and how formal concept analysis is used to build a suitable navigation structure. This structure has the single-focus property (i.e. any sensible subset of a library is represented by a single node) and supports attribute-based (via explicit component properties) and object-based (via implicit component similarities) navigation styles. It thus combines the exact semantics of formal methods with the interactive navigation possibilities of informal methods. Experiments show that current theorem provers can solve enough of the emerging proof problems to make browsing feasible. The navigation structure also indicates situations where additional abstractions are required to build a better index and thus helps to understand and to re-engineer component libraries.
[single-focus property, Lattices, library subset representation, interactive navigation, formal specification, off-line indexing, navigation structure, Delay, software libraries, Information science, software component libraries, theorem provers, implicit component similarities, explicit component properties, exact semantics, theorem proving, deductive power, Navigation, attribute-based navigation, indexing, content-oriented access, object-based navigation, information retrieval, Information retrieval, Application software, Formal specifications, formal concept analysis, abstractions, systems re-engineering, Software libraries, informal methods, specification-based browsing, match relations, component library reengineering, specification-based retrieval, formal methods, proof problems, Indexing]
The very idea of software development environments: a conceptual architecture for the arts' environment paradigm
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
For the last three years the authors have been building an instantiation of a system development paradigm, called ARTS. The paradigm consists of a view of what a system development environment is, in general terms, and a methodology for instantiating the paradigm for particular and specific domains of application. The motivation for and the explanation of the paradigm are derived from extant epistemological models of the method of natural science. They assert that these models are directly applicable to the domain of software and systems construction, and that, from them, one can derive principles and explanations for what a software development environment should be. They present briefly the statement view of scientific theories, a conceptual architecture for software development environments whose rationale is given in terms of the statement view and some examples of how the present version of ARTS realises this conceptual architecture.
[Software testing, System testing, extant epistemological models, natural science method, Subspace constraints, software development environments, Programming, statement view, Telecommunications, Application software, scientific theories, ARTS, conceptual architecture, systems construction, software construction, Computer architecture, Software systems, system development paradigm, programming environments, Contracts, Software engineering]
Task oriented software understanding
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
The main factors that affect software understanding are the complexity of the problem solved by the program, the program text, the user's mental ability and experience and the task being performed. The paper describes a planning approach solution to the software understanding problem that focuses on the user's task and expertise. First, user questions about software artifacts have been studied and the most commonly asked questions are identified. These questions are organized into a question model and procedures for answering them are developed. Then, the patterns in user questions while performing certain tasks have been studied and these patterns are used to build generic task models. The explanation system uses these task models in several ways. The task model, along with a user model, is used to generate explanations tailored to the user's task and expertise. In addition, the task model allows the system to provide explicit task support in its interface.
[Software maintenance, complexity, Humans, Software performance, generic task models, Code standards, explanation, user modelling, explanation system, software artifacts, Automatic programming, user questions, Logic programming, program text, user mental ability, Documentation, reverse engineering, software maintenance, user experience, user model, Computer science, planning approach, task oriented software understanding, explicit task support, Writing, Computer industry, question model]
A coordination system approach to software workflow process evolution
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Describes a coordination-based approach to the dynamic evolution of (software) workflow processes. Our interest is in widely distributed workflow processes, i.e. systems that allow each instance of a process model to be enacted in a distributed fashion, with different parts of the process being enacted on different nodes of the system. More specifically, we are interested in the problem of dynamic workflow process evolution in such a distributed context, where the propagation of changes to all the concerned nodes has to be performed in an orderly manner. We address the problem of dynamic workflow process evolution from a coordination system approach, considering the workflow system as a coordination system and the workflow evolution as a coordinated evolution of the coordination schemes. We illustrate the problem of workflow evolution in a software engineering context, and describe a method using the reflexive features of our underlying coordination system to support dynamic workflow process evolution in a distributed workflow system.
[Wide area networks, Printing, system nodes, reflexive features, Europe, Process control, process model instance enactment, coordination system, Electronic switching systems, Application software, dynamic evolution, Coordinate measuring machines, Job production systems, coordinated evolution, node change propagation, Software systems, software engineering, widely distributed workflow processes, workflow management software, distributed programming, Software engineering, software workflow process evolution]
Specification-based testing of Ada units with low encapsulation
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Describes a system that improves testing quality by supporting automatic test data selection, execution and result verification. The system tests poorly-encapsulated Ada units against formal specifications. This task is difficult partly because the unit's interface is not explicit, but rather is buried in the body/implementation code. We attack this problem by making the unit's interface explicit and complete. This is accomplished via automatic and manual analysis of the body. The complete interface is represented using an extended algebraic signature notation. Once the signature has been discovered, it can be reformulated so that a collection of well-defined, static mappings are established between it and the signature of the unit's formal specification. These mappings guide the development of test artifact transformers and oracles, which support automatic test data selection, execution and result verification. This paper discusses problems that arise as a result of testing under low encapsulation, discusses our solution using an ongoing example, and compares our solution to earlier solutions.
[Encapsulation, Software testing, System testing, encapsulation, program testing, program verification, Manuals, software quality, extended algebraic signature notation, poorly-encapsulated Ada units, oracles, automatic test execution, software testing quality, complete interface, static mappings, algebraic specification, formal specification-based testing, Identity-based encryption, automatic test data selection, Formal specifications, Computer science, Automatic testing, implementation code, automatic result verification, test artifact transformers, computer aided software engineering, implicit unit interface, data encapsulation, Software engineering, Ada]
Requirements engineering and verification using specification animation
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Presents an overview of the Possum specification animation system and its integration into the Cogito methodology and toolset. Possum allows interpretation (or animation) of specifications written in Sum, the specification language of Cogito. We distinguish two potential uses for Possum and illustrate each of these with an example. The first is the use of Possum for specification verification, where the analysis of properties of specifications by the specification designer is emphasised. The second use is specification validation, where the specification is checked against the informal requirements of the system.
[Visualization, Costs, Design methodology, requirements verification, Cogito methodology, specification interpretation, specification properties analysis, Electrical capacitance tomography, Formal specifications, formal specification, Possum specification animation system, computer animation, requirements engineering, formal verification, specification validation, Cogito toolset, Animation, Software systems, computer aided software engineering, informal system requirements, Sum specification language, Australia, program visualisation, specification verification]
Parameterising (algebraic) specifications on diagrams
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
The paper presents an extension of previous work on the parameterisation of logical and algebraic specifications leading to a novel formalisation of parameterisation which is general enough to become independent of the specificities of the underlying formalism, and flexible enough to accommodate the manipulation of complex parameterised specifications where the parameters are presented by means of diagrams of specifications.
[formalisation, Art, logical specification parameterisation, Laboratories, Programming, Educational institutions, complex parameterised specifications, Concrete, diagrams, Electrical capacitance tomography, algebraic specification parameterisation, algebraic specification]
A visualization concept for hierarchical object models
Proceedings 13th IEEE International Conference on Automated Software Engineering
None
1998
Most current object modeling methods and tools have weaknesses both in the concepts of hierarchical decomposition and in the visualization of these hierarchies. Some methods do not support hierarchical decomposition at all. Those methods which do employ tools that provide explosive zoom as the only means for the visualization of hierarchies. The authors present an approach for the visualization of hierarchical object models based on the notion of fisheye views. This concept integrates local detail and global context of a view in the same diagram and eases navigation in hierarchical structures without offending the principle of abstraction. The work is part of an effort to create a method and language called ADORA that provides strong support for hierarchical decomposition.
[object-oriented programming, Navigation, Object oriented modeling, local detail, abstraction, Flow graphs, tools, hierarchical decomposition, ADORA, Graphics, Computer science, Graphical models, navigation, fisheye views, Data visualization, global context, object-oriented languages, Software systems, Explosives, visualization concept, object-oriented methods, program visualisation, hierarchical object models, Context modeling]
A comparative study between linear programming verification (LPV) and other verification methods
14th IEEE International Conference on Automated Software Engineering
None
1999
Compares our linear programming technology for software verification (LPV) with other verification systems: explicit exploration using partial order reduction (Spin) and implicit exploration using BDDs (Xeve/Esterel). The case study is a safety property of an easily-scalable problem (a bus arbiter). The results show that exploration-based methods (Spin and Xeve/Esterel) have an overall exponential complexity, restricting their use on small instances. The LPV technique, which does not rely on exploration, is the only one fast enough (quadratic complexity) to handle systems that are 50 times larger than the other techniques presented in this paper can do. Moreover, in opposition to exploration-based methods, LPV produces real proven facts that mean this technique shares some common points with theorem proving. We believe that the scale-change robustness of LPV shows that linear programming can be applied successfully to the verification of industrial systems.
[program verification, Circuits, Spin, linear programming, implicit exploration, scale-change robustness, quadratic complexity, Design engineering, binary decision diagrams, Boolean functions, bus arbiter, safety, Hardware, explicit exploration, theorem proving, Logic, Contracts, partial order reduction, easily-scalable problem, Linear programming, Data structures, linear programming verification, Programming profession, exponential complexity, industrial systems, Automata, software verification methods, safety property, Xeve/Esterel, computational complexity]
Data mining library reuse patterns in user-selected applications
14th IEEE International Conference on Automated Software Engineering
None
1999
In this paper, we show how data mining can be used to discover library reuse patterns in user-selected applications. This can be helpful in building and debugging applications that use a particular library by observing how other developers have used that library in their applications. Specifically, we consider the problem of discovering association rules that identify library components that are often reused in combination by application components. For example, such a rule might tell us that application classes that inherit from a particular library class often override certain member functions. By querying and/or browsing such association rules, a developer can discover patterns for reusing library components. We illustrate the approach using our tool, CodeWeb, by demonstrating characteristic ways in which applications reuse components in the ET++ application framework.
[program debugging, data mining, application building, Data engineering, Electrical capacitance tomography, Data mining, ET++ application framework, software libraries, association rule discovery, association rule browsing, software tools, Books, application debugging, Debugging, library reuse patterns, Application software, Association rules, Sparks, application classes, Computer science, Software libraries, user-selected applications, CodeWeb tool, software reusability, library class, association rule querying]
Beyond components-connections-constraints: dealing with software architecture difficulties
14th IEEE International Conference on Automated Software Engineering
None
1999
Recent work in the area of software architectures is exploring the use of formal methods for specification and analysis of software architecture designs. The work is particularly aimed at achieving better possibilities for evolution, sharing and reuse among software entities. However, it is believed that specifying architecture components and connections without revealing their underlying design assumptions increases the problems with evolution, their reuse and even interoperability among independently operating components. The paper reveals some of the issues underlying architectural difficulties and concludes that implicit design assumptions pose obstacles. We ascertain that separation of architecture ontology from its design modeling is necessary and provides enough context to examine the assumptions behind the central design decisions in architecture abstractions. We propose a basic architecture ontology that is aimed at providing a formal reconciliation of architecture design assumptions and discuss its usefulness to software engineering.
[independently operating components, components-connections-constraints, open systems, architecture design assumptions, software architecture designs, formal reconciliation, software architecture difficulties, specification, implicit design assumptions, architecture abstractions, design assumptions, formal specification, software architecture, design modeling, Software architecture, architecture ontology, formal methods, Bismuth, software reusability, software engineering, central design decisions, software entities]
vUML: a tool for verifying UML models
14th IEEE International Conference on Automated Software Engineering
None
1999
The Unified Modelling Language (UML) is a standardised notation for describing object oriented software designs. We present vUML, a tool that automatically verifies UML models where the behaviour of the objects is described using UML Statecharts diagrams. The tool uses the SPIN model checker to perform the verification, but the user does not have to know how to use SPIN or the PROMELA language. If an error is found during the verification, the tool creates a UML sequence diagram showing how to reproduce the error in the model.
[UML Statecharts diagrams, automatic programming, object-oriented programming, Error analysis, program verification, Object oriented modeling, Unified modeling language, standardised notation, object oriented software designs, vUML, Electrical capacitance tomography, Counting circuits, Computer science, SPIN model checker, UML sequence diagram, UML model verification, automatic verification, Software systems, PROMELA language, Unified Modelling Language]
An approach to software requirements elicitation using precepts from activity theory
14th IEEE International Conference on Automated Software Engineering
None
1999
The goal of this work is to show that precepts from activity theory can be used in software requirements elicitation. Thus, we propose an approach to requirements elicitation using principles from this theory. Initially, we mention common problems found in requirements elicitation. Then we explain the main precepts of activity theory. Finally, we present a case study using the suggested approach to requirements elicitation.
[Psychology, Humans, Documentation, Programming, Electrical capacitance tomography, Mediation, formal specification, Read only memory, Engineering management, Sociology, activity theory, Software tools, software requirements elicitation]
An approach to automatic code generation for safety-critical systems
14th IEEE International Conference on Automated Software Engineering
None
1999
Automated translation, or code generation, of a formal requirements model to production code can alleviate many of the problems associated with design and implementation. In this paper, we outline the requirements of such code generation to obtain a high level of confidence in the correctness of the translation process. We then describe a translator for a state-based modeling language called RSML (Requirements Specification Modeling Language) that largely meets these requirements.
[Production systems, Law, safety-critical software, Programming, system implementation problems, formal requirements model automated translation, automatic code generation, program compilers, production code, RSML translator, Design engineering, specification languages, state-based modeling language, system design problems, Productivity, Requirements Specification Modeling Language, automatic programming, Computer science, program interpreters, safety-critical systems, translation process correctness, Software systems, Error correction codes, Hardware design languages, Legal factors]
System for automated validation of embedded software in multiple operating configurations
14th IEEE International Conference on Automated Software Engineering
None
1999
Embedded controllers in safety critical applications rely on the highest of software quality standards. Testing is performed to ensure that requirements and specifications are met for all the different environments in which the controllers operate. This paper describes the architecture of a system that uses a relational database for tracking tests, requirements, and configurations. The relational sub-schemas are integrated in a data warehouse and allow traceability from requirements to tests. A normalized representation of test cases enables the system to reason about the test topologies and is used for constructing clusters of similar tests. A representative test from each cluster can in turn provide a rapid estimation, of the software's requirement coverage and quality.
[Performance evaluation, System testing, data warehouse, program verification, Relational databases, safety critical applications, Software safety, software quality, relational database, Application software, relational databases, automated validation, Embedded software, embedded software, multiple operating configurations, Software quality, Computer architecture, Automatic control, requirement coverage, Software standards, data warehouses]
Development of a constraint-based airlift scheduler by program synthesis from formal specifications
14th IEEE International Conference on Automated Software Engineering
None
1999
We describe the formal specification and automated synthesis of a strategic airlift scheduler for the Air Mobility Command of the US Air Force. The program synthesis system, the Kestrel Interactive Development System, composes a formal domain theory with a formal description of a class of algorithms (global search with constraint propagation) to produce provably correct and highly efficient code that outperforms more conventional approaches to this scheduling problem.
[formal domain theory, constraint propagation, scheduling problem, formal specification, strategic airlift scheduler, Constraint optimization, Databases, highly efficient code, command and control systems, constraint based airlift scheduler, Control system synthesis, scheduling, interactive systems, Large-scale systems, global search, constraint handling, Monitoring, automatic programming, Identity-based encryption, aircraft, US Air Force, Airports, formal description, Scheduling, Formal specifications, Aircraft propulsion, formal specifications, automated synthesis, Air Mobility Command, program synthesis system, provably correct, Kestrel Interactive Development System]
Exploration harnesses: tool-supported interactive discovery of commercial component properties
14th IEEE International Conference on Automated Software Engineering
None
1999
A key problem in component-based software development (CBSD) is that developers have incomplete knowledge of components. In many cases, the only available source of such information is experimentation. In this paper we argue that the provision of tool support for automated and repeatable experiments can provide significant value to designers. Such tools, which we call exploration harnesses, promise to help enterprises to exploit prefabricated evolving third party components. We evaluated the exploration harness concept by building a prototype and using it to support the exploration of large components in the design of a dynamic fault-tree analysis tool called Galileo. Galileo employs package-oriented programming, in which shrink-wrapped packages such as Microsoft Word and Visio Technical are used as large components. Using our exploration harness helped us to discover a range of relevant but undocumented properties of such components, across several versions, which enabled us to make better informed design decisions.
[Software testing, exploration harnesses, Costs, package-oriented programming, Programming, Visio Technical, enterprises, Software design, repeatable experiments, Prototypes, software tools, shrink-wrapped packages, Protection, undocumented properties, commercial component properties, tool-supported interactive discovery, design decision making, Inspection, fault trees, prefabricated evolving third party components, component-based software development, Microsoft Word, computer graphics, Intellectual property, Packaging, dynamic fault-tree analysis tool, Galileo, automated experiments]
Software test generation using refinement types
14th IEEE International Conference on Automated Software Engineering
None
1999
A novel approach for automatic software test generation is presented, which combines ideas from structural and functional testing as well as formal verification methods. It involves as an intermediate step, the construction of graphs and refinement types, which can be regarded as an automatically constructed semi-specification and used for formal verification. The technique is illustrated using a simple functional language, with algorithms for assigning refinement types and for test generation. Some desirable theoretical properties of the approach are briefly considered. It is also compared informally to other well-known as well as new techniques for automatic test generation.
[Software testing, automatic programming, program testing, program verification, refinement types, intermediate step, type theory, automatic software test generation, simple functional language, Formal specifications, Logic testing, Read only memory, functional testing, automatically constructed semi-specification, Tree graphs, formal verification, Automatic testing, Software quality, formal verification methods, Informatics, Formal verification, functional languages]
Evolving object-oriented designs with refactorings
14th IEEE International Conference on Automated Software Engineering
None
1999
Refactorings are behaviour-preserving program transformations that automate design-level changes in object-oriented applications. Our previous research established that many schema transformations, design patterns and hot-spot meta-patterns are automatable. This research evaluates whether refactoring technology can be transferred to the mainstream by restructuring non-trivial C++ applications. The applications that we examine were evolved manually by software engineers. We show that an equivalent evolution could be reproduced significantly faster and cheaper by applying a handful of general-purpose refactorings. In one application, over 14 K lines of code, that otherwise would have been coded by hand, were transformed automatically. Our experiments identify the benefits, limitations and topics of further research related to the transfer of refactoring technology to a production environment.
[hot-spot meta-patterns, object-oriented programming, Object oriented databases, refactorings, schema transformations, Computer science, object-oriented design evolution, Microarchitecture, C++ application restructuring, automatic design level changes, design patterns, production environment, software engineering, object-oriented methods, behaviour-preserving program transformations]
Automatically structuring textual requirement scenarios
14th IEEE International Conference on Automated Software Engineering
None
1999
Scenarios are valuable for supporting communication among system developers in the initial phases of requirements engineering. But the problem of how to fruitfully deal with large informal or semi-formal scenario collections consisting of weakly structured texts is still a key research issue. We report on a novel approach of automatically structuring textual document collections. We focus on its application to scenario management and discuss how structuring scenarios can help to support the analysis and maintenance of scenario collections. To evaluate the approach, we present a case study within a concrete software engineering effort.
[text analysis, Electrical capacitance tomography, textual document collections, scenario management, Jacobian matrices, Software design, textual requirement scenarios, system developers, concrete software engineering effort, Prototypes, Production, semi-formal scenario collections, scenario collections, automatic programming, Collaborative software, structuring scenarios, software maintenance, case study, requirements engineering, Collaboration, systems analysis, weakly structured texts, Collaborative work, Software systems, Systems engineering and theory, automatic structuring]
An automatic and optimized test generation technique applying to TCP/IP protocol
14th IEEE International Conference on Automated Software Engineering
None
1999
In this paper an automatic and optimized technique for test generation for communication protocol control and data portion is described, the goal is to minimize the number of tests with a guaranteed coverage. The test generation algorithm is applied to the client layer part of the TCP/IP protocol. The protocol used for the experiment is TCP-Reno, which is specified in the SDL language and is one of the commonly referenced implementations. For such a sophisticated protocol, the algorithm efficiently constructs 22 tests that cover all the required portions of the protocol.
[automatic test software, automatic test generation technique, System testing, Protocols, program testing, Communication system control, communication protocol control, TCP-Reno, communication protocol data portion, Specification languages, Circuit faults, Circuit testing, formal specification, Switching circuits, optimized test generation technique, Automatic testing, transport protocols, Automata, TCPIP, SDL language, guaranteed coverage, TCP/IP protocol, client layer part, algorithm]
A metric based technique for design flaws detection and correction
14th IEEE International Conference on Automated Software Engineering
None
1999
During the evolution of object-oriented (OO) systems, the preservation of correct design should be a permanent quest. However, for systems involving a large number of classes and which are subject to frequent modifications, the detection and correction of design flaws may be a complex and resource-consuming task. Automating the detection and correction of design flaws is a good solution to this problem. Various authors have proposed transformations that improve the quality of an OO system while preserving its behavior. In this paper, we propose a technique for automatically detecting situations where a particular transformation can be applied to improve the quality of a system. The detection process is based on analyzing the impact of various transformations on software metrics using quality estimation models.
[software quality estimation models, program debugging, Automation, object-oriented programming, Object oriented modeling, program diagnostics, software design flaw correction, software design flaw detection, Electrical capacitance tomography, software quality, system behaviour preservation, error detection, object-oriented systems evolution, Bridges, Reactive power, system modifications, Software metrics, system classes, computer aided software engineering, software transformations, software metrics, error correction]
An integration of deductive retrieval into deductive synthesis
14th IEEE International Conference on Automated Software Engineering
None
1999
Deductive retrieval and deductive synthesis are two conceptually closely related software development methods which apply theorem proving techniques to support the construction of correct programs. In this paper, we describe an integration of both methods which combines their complementary benefits and alleviates some of their drawbacks. The core of our integration is an algorithm which automatically extracts queries from the synthesis proof state and submits them to a specialized retrieval system. Retrieved components are then used to close open subgoals in the proof. We use a higher-order framework for synthesis in which higher-order meta-variables are used to represent program fragments still to be synthesized. Hence, the introduction of a new meta-variable is an attempt to synthesize a new fragment and so highlights a possible reuse step. This observation allows us to invoke retrieval only after a substantial change rather than at every proof step and prevents overloading the retrieval mechanism. Our integration raises the granularity level of synthesis by avoiding a substantial number of proof steps. It also provides a framework for adapting "near-miss" components in the case that an exact match cannot be retrieved.
[Programming, formal specification, deductive synthesis, program fragments, automatic query extraction, Control system synthesis, Automatic control, Libraries, theorem proving, near-miss component adaptation, correct program construction, algorithm, Identity-based encryption, Automation, software development methods, open subgoal closing, reuse step, NASA, higher-order meta-variables, granularity level, Logic design, Formal specifications, Application software, deductive retrieval, synthesis proof state, theorem proving techniques]
Controlled natural language can replace first-order logic
14th IEEE International Conference on Automated Software Engineering
None
1999
Many domain specialists are not familiar or comfortable with formal notations and formal tools like theorem provers or model generators. To address this problem, we developed Attempto Controlled English (ACE), a subset of English that can be unambiguously translated into first-order logic and thus can conveniently replace first-order logic as a formal notation. We describe how ACE has been used as a front-end to EP Tableaux, a model generation method complete for unsatisfiability and for finite satisfiability. We specified in ACE, a database example that was previously expressed in the EP Tableaux language PRQ, automatically translated the ACE specification into PRQ, and with the help of EP Tableaux reproduced the previously found results.
[model generators, unsatisfiability, first-order logic, formal tools, Programming, computability, Electronic switching systems, formal notation, EP Tableaux, database management systems, formal specification, PRQ, Databases, ACE specification, model generation method, theorem provers, Constraint theory, Logic, Automation, formal languages, domain specialists, Natural languages, Formal specifications, controlled natural language, Computer science, Software quality, finite satisfiability, front-end, Attempto Controlled English, formal notations]
Rule-based strategic reflection: observing and modifying behaviour at the architectural level
14th IEEE International Conference on Automated Software Engineering
None
1999
As software systems become larger and more complex, a relevant part of code shifts from the application domain to the management of the system's run-time architecture (e.g., substituting components and connectors for run-time automated tuning). We propose a novel design approach for component based systems supporting architectural management in a systematic and conceptually clean way and allowing for the transparent addition of architectural management functionality to existing systems. The approach builds on the concept of reflection, extending it to the programming-in-the-large level, thus yielding architectural reflection (AR). The paper focuses on one aspect of AR, namely the monitoring and dynamic modification of the system's overall control structure (strategic reflection), which allows the behaviour of a system to be monitored and adjusted without modifying the system itself.
[application domain, Protocols, software systems, run-time automated tuning, software management, software architecture, Runtime, Software architecture, architectural reflection, architectural level, component based systems, run-time architecture management, Computer architecture, Monitoring, dynamic modification, object-oriented programming, architectural management functionality, overall control structure, rule based strategic reflection, Reflection, Connectors, novel design approach, programming-in-the-large level, Software systems, system monitoring]
Automatically detecting mismatches during component-based and model-based development
14th IEEE International Conference on Automated Software Engineering
None
1999
A major emphasis in software development is placed on identifying and reconciling architectural and design mismatches. These mismatches happen during software development on two levels: while composing system components (e.g. COTS or in-house developed) and while reconciling view perspectives. Composing components into a system and 'composing' views (e.g. diagrams) into a system model are often seen as being somewhat distinct aspects of software development. However, as this paper shows, their approaches in detecting mismatches complement each other very well. In both cases, the composition process may result in mismatches that are caused by clashes between development artefacts. Our component-based integration approach is more high-level and can be used early on for risk assessment while little information is available. Model-based integration, on the other hand needs more information to start with but is more precise and can handle large amounts of redundant information. This paper describes both integration approaches and discusses their commonalities and differences. Both integration approaches are automatable, and some tool support is already available.
[risk assessment, Design methodology, Unified modeling language, Programming, development artefacts, model-based software development, software tools, Standards development, high-level component-based integration approach, Identity-based encryption, automatic mismatch detection, model-based integration, system model, component-based software development, system components composition, design mismatches, Software packages, architectural mismatches, Ear, Packaging, Software systems, computer aided software engineering, redundant information, subroutines, view perspectives, Software engineering]
Automatic software clustering via Latent Semantic Analysis
14th IEEE International Conference on Automated Software Engineering
None
1999
The paper describes the initial results of applying Latent Semantic Analysis (LSA) to program source code and associated documentation. Latent Semantic Analysis is a corpus based statistical method for inducing and representing aspects of the meanings of words and passages (of natural language) reflective in their usage. This methodology is assessed for application to the domain of software components (i.e., source code and its accompanying documentation). The intent of applying Latent Semantic Analysis to software components is to automatically induce a specific semantic meaning of a given component. Here LSA is used as the basis to cluster software components. Results of applying this method to the LEDA library and MINIX operating system are given. Applying Latent Semantic Analysis to the domain of source code and internal documentation for the support of software reuse is a new application of this method and a departure from the normal application domain of natural language.
[LEDA library, computational linguistics, Sparse matrices, Read only memory, Operating systems, internal documentation, software tools, LSA, software components, automatic software clustering, automatic programming, Identity-based encryption, Statistical analysis, corpus based statistical method, software reuse, MINIX operating system, Natural languages, Documentation, documentation, semantic meaning, Application software, Matrix decomposition, Computer science, software reusability, Latent Semantic Analysis, natural languages, statistical analysis, program source code, natural language, normal application domain]
Towards adaptive web agents
14th IEEE International Conference on Automated Software Engineering
None
1999
There is an increasingly large demand for software systems which are able to operate effectively in dynamic environments. In such environments, automated software engineering is extremely valuable since a system needs to evolve in order to respond to changing requirements. One way for software to evolve is for it to reflect upon a model of its own design. A key challenge in reflective evolution is credit assignment: given a model representing the design elements of a complex system, how might that system localize, identify and prioritize prospective candidates for potential modification. We describe a model-based credit assignment mechanism. We also report on an experiment on evolving the design of Mosaic 2.4, an early network browser.
[Identity-based encryption, credit assignment, Roads, changing requirements, Displays, Educational institutions, adaptive web agents, Electrical capacitance tomography, automated software engineering, software agents, Intelligent agent, Microwave integrated circuits, Mosaic 2.4, Ear, online front-ends, Software systems, computer aided software engineering, Web sites]
UNA based iterative test data generation and its evaluation
14th IEEE International Conference on Automated Software Engineering
None
1999
A number of approaches have been proposed to automatically generate test data to traverse a given path in a program. We present a program execution based approach to generate test data for a given path. The technique derives a desired input for a test path by iteratively refining an arbitrarily chosen input. A set of linear constraints on the increments to the input are derived to refine the input. We solve this constraint set using a Unified Numerical Approach (UNA) developed in this paper. Our technique can generate both integer and floating point inputs as well as handle arrays and loops. We determine a basis set of paths for a program and use our technique to generate test data for this set. We implemented and experimentally evaluated our technique. We present results of generating input for scientific programs. The experimental results show that the technique is effective in that it generates input for most of the paths in the basis sets and also efficiently detects linear infeasible paths. Our experiments also show that our technique is efficient in the number of iterations required to generate test data. The time performance shows that it provides a practical method to automatically generate test data for scientific programs.
[Software testing, program control structures, experimental results, program testing, Input variables, iterative test data generation, program execution, loops, floating point inputs, integer inputs, Computer science, linear constraints, Unified Numerical Approach, Automatic testing, scientific programs, numerical analysis, Relaxation methods, arrays, Iterative methods]
NAVCo: negotiation-based adaptive view coordination
14th IEEE International Conference on Automated Software Engineering
None
1999
In mission critical applications of distributed information systems, autonomous information resources are coordinated to meet the information demands of client specific decision-support views. The current approach to view coordination relies on design-time trade-offs to select a static view coordination policy from a set of available policies. This approach is not robust and does not respond well in a dynamic environment with shared infrastructure, and dynamically changing missions, priorities, preferences, and constraints. This paper introduces, NAVCo, a negotiation-based adaptive view coordination approach that allows view coordination policies To be dynamically negotiated and adapted at run-time in response to dynamically changing conditions.
[Information resources, negotiation-based, Mission critical systems, NAVCo, Distributed information systems, autonomous information resources, adaptive view coordination, Runtime, distributed databases, Robustness, mission critical applications, data warehouses, distributed information systems, decision-support views]
Industrial applications of software synthesis via category theory
14th IEEE International Conference on Automated Software Engineering
None
1999
Over the last two years, we have demonstrated the feasibility of applying category-theoretic methods in specifying, synthesizing, and maintaining industrial strength software systems. We have been using a first-of-its-kind tool for this purpose. Kestrel's Specware/sup TM/ software development system. In this paper, we describe our experiences and give an industrial perspective on what is needed to make this technology have broader appeal to industry. Our overall impression is that the technology does work for industrial strength applications, but that it needs additional work to make it more usable. We believe this work marks a turning point in the use of mathematically rigorous approaches to industrial strength software development and maintenance. It is interesting to note that when this technology is applied to software systems whose outputs are designs for airplane parts, the design rationale that is captured is not only software engineering design rationale, but also design rationale from other engineering disciplines (e.g., mechanical, material, manufacturing, etc.). This suggests the technology provides an approach to general systems engineering that enables one to structure and reuse engineering knowledge broadly.
[Knowledge engineering, Software maintenance, Airplanes, Programming, Turning, formal specification, airplane part design, Kestrel Specware software development system, Software design, aerospace computing, software tools, software engineering design rationale, software specification, engineering knowledge reuse, software synthesis, Application software, software maintenance, mathematically rigorous approaches, engineering knowledge structure, Computer industry, Software systems, category theory, industrial applications, systems engineering, computer aided software engineering, Software engineering]
Fixing some transformation problems
14th IEEE International Conference on Automated Software Engineering
None
1999
Defining domain-specific abstractions for generator systems leads to a quandary between choosing abstractions that exhibit powerful programming amplification through the combinatorial opportunities provided by composition, and choosing abstractions that can be easily transformed into high-performance code. Most generators opt for abstraction to improve programming productivity, which usually compromises target program performance. Transformation-based generators widen the quandary through deep factorization of operators and operands to amplify expressive power, but this explodes the search space. My hypothesis is that existing architectures are inadequate to achieve simultaneously high levels of abstraction, high-performance target programs and small solution search spaces. To explore architectural variations to address this quandary, I have implemented a generator in Common LISP designed specifically to address these problems. It is called the Anticipatory Optimization Generator (AOG) because it allows programmers to anticipate optimization opportunities and to prepare an abstract, distributed plan that attempts to achieve them.
[search space explosion, architectural variations, Fiber reinforced plastics, program transformations, program performance, expressive power, transformation-based generator systems, composition, operators, logic programming, high-performance code, pattern-directed transformation, software performance evaluation, search problems, deep factorization, operands, optimising compilers, domain-specific abstractions, programming productivity, tag-directed transformation, Common LISP, program generation, programming amplification, abstract distributed plan, Anticipatory Optimization Generator, LISP]
UMLAUT: an extendible UML transformation framework
14th IEEE International Conference on Automated Software Engineering
None
1999
Advanced users often find themselves restricted by the limited facilities of most UML CASE tools when they want to do complex manipulations of UML models, e.g., apply design patterns, generate code for simulation and validation etc. We describe UMLAUT, a freely available UML transformation framework for manipulating UML models. These manipulations are expressed as algebraic compositions of reified elementary transformations. They are thus open to extensions through inheritance and aggregation. To illustrate the interest of our approach, we show how the model of a UML distributed application can be automatically transformed into a labeled transition system validated using advanced protocol validation technology.
[UMLAUT, Protocols, Computer aided software engineering, program verification, advanced users, Unified modeling language, Standardization, labeled transition system, inheritance, aggregation, Electrical capacitance tomography, complex manipulations, Engines, design patterns, UML distributed application, advanced protocol validation technology, distributed programming, reified elementary transformations, Graphical user interfaces, Testing, Java, automatic programming, Identity-based encryption, object-oriented programming, extendible UML transformation framework, UML CASE tools, UML models, algebraic compositions]
Separating concerns in direct manipulation user interfaces
14th IEEE International Conference on Automated Software Engineering
None
1999
Direct-manipulation user interfaces are difficult to implement as a tapered hierarchy. Features such as drag enabling and continuous graphical feedback require frequent interaction and collaboration among a large number of objects in multiple layers. These collaborations complicate the design of the interfaces in the various layers. We present a new component-interface model called a "mode component\
[layer boundaries, drag enabling, graphical user interfaces, Stacking, tapered hierarchy, multiple layers, direct-manipulation user interface design, Electrical capacitance tomography, feedback, Reactive power, software architecture, Software design, collaboration feedback, Feedback, object-oriented methods, mode component, Reflection, Application software, object collaborations, continuous graphical feedback, component-interface model, Computer science, object interaction, Collaboration, User interfaces, subroutines, separation of concerns]
A formal ontology for re-use of software architecture documents
14th IEEE International Conference on Automated Software Engineering
None
1999
Software architecture has been established as a viable level of representation for reuse in practical software engineering efforts. The main reason for this is that an architectural view of software is sufficiently abstract to have many instantiations. Even with technologies such as CORBA and JavaBeans, which emphasize reuse of components, the realization of widespread reuse has been severely limited. While architectural reuse has been successful, it has thus far suffered from an ad-hoc semantics, and even savvy architecture practitioners are unsure precisely what is being reused. We have been engaged in research into reuse of software documents, such as design documents, statements of work, contracts, etc., that capture and reuse architectural level knowledge of software solutions. We have found that, given a sufficiently robust knowledge based tool for maintaining documents, a formal ontology or meta-model for software architectures is required to achieve reuse of these architecture-level documents. We present such an ontology.
[design documents, architectural view, Ontologies, architectural level knowledge, contracts, formal specification, software architecture, Software architecture, Computer architecture, software solutions, Robustness, software architecture document reuse, architectural reuse, Contracts, Java, statements of work, practical software engineering efforts, ad-hoc semantics, architecture-level documents, Educational institutions, Connectors, Computer science, formal ontology, robust knowledge based tool, software reusability, instantiations, Software engineering]
Automatic proofs of properties of simple C/sup --/ modules
14th IEEE International Conference on Automated Software Engineering
None
1999
We address the problem of automatically verifying properties of modules written in the C/sup --/ language, a very simple imperative language. We develop a framework for automatically proving properties of modules written in C/sup --/. Our approach consists of two steps. At the first step, the C/sup -$/module is automatically transformed into a set of axioms written in the language of equational logic. This transformation is bused on the algebraic semantics of C/sup --/ modules. At the second step, the theorem prover NICE is used to mechanically perform the proof of the desired properties. Our system enables us to prove many properties completely automatically from the C/sup -$/code alone. We illustrate computer applications on programs computing integers and linked lists.
[program verification, Laboratories, algebraic semantics, Automatic logic units, Electronic switching systems, Mathematics, Electrical capacitance tomography, C language, Read only memory, integers, axioms, theorem prover, theorem proving, equational logic, very simple imperative language, automatic programming, linked lists, simple C/sup --/ modules, Reasoning about programs, programming language semantics, Equations, Programming profession, automatic proofs, automatic verification, computer applications, Computer applications, NICE]
Implementing effective automatic cryptographic protocol analysis
14th IEEE International Conference on Automated Software Engineering
None
1999
A cryptographic protocol is a short series of message exchanges, usually involving encryption, intended to establish secure communication over an insecure network. A protocol fails if an active wiretapper can obtain confidential information or impersonate a legitimate user, without performing cryptanalysis, by blocking, replaying, relabeling or otherwise modifying messages. Since the number of possible wiretapper-induced distortions of a protocol grows exponentially with the size of the protocol, most tools for detecting protocol failure require extended, expert user guidance. The Automatic Authentication Protocol Analyzer, 2nd Version (AAPA2), in contrast, automatically correctly identifies 88% of the protocols in an independently selected collection of protocols as failed or not failed, on a modest computer, in an average of only 2.6 minutes per protocol. This paper summarizes the AAPA2's results, sketches how it produces them and gives references providing more information.
[telecommunication security, Laboratories, message blocking, insecure network, 2nd Version, telecommunication computing, message modification, Jacobian matrices, formal verification, 2.6 min, encryption, expert user guidance, Feedback, Failure analysis, network analysers, Logic functions, Libraries, Cryptography, protocols, protocol failure detection, Contracts, wiretapper-induced distortions, automatic cryptographic protocol analysis, confidential information, message exchanges, cryptography, message relabelling, computer aided analysis, Cryptographic protocols, secure communication, active wiretapper, AAPA2, Authentication, message authentication, legitimate user impersonation, message replaying, independently selected protocol collection, Automatic Authentication Protocol Analyzer]
Enhancing annotation visibility for software inspection
14th IEEE International Conference on Automated Software Engineering
None
1999
Annotation of software artifacts is common in software development, and vital for software inspection. People viewing annotated artifacts encounter delocalization: they must understand various parts of an artifact (and their annotations) to understand the part they are viewing. We taxonomize delocalization within software systems into lateral delocalization (different items of the artifact within the same development phase), longitudinal delocalization (related items in different phases), and historical delocalization (successive versions of the same item). We report on a pilot study of code inspection with AnnoSpec, an inspection tool supporting visibility of laterally-delocalized annotations. Our results suggest that addressing delocalization may help people perform inspections more effectively.
[Software testing, Terminology, Roads, software systems, lateral delocalization, software inspection, longitudinal delocalization, Programming, code inspection, Filters, software artifacts, software tools, inspection, laterally-delocalized annotations, Filtering, development phase, program diagnostics, inspection tool, historical delocalization, Inspection, annotation visibility, annotated artifacts, delocalization, Software systems, AnnoSpec, Software tools, Software engineering]
A visualization tool for constraint program debugging
14th IEEE International Conference on Automated Software Engineering
None
1999
Constraint programming is an emerging technology that offers an original approach allowing for efficient and flexible solving of complex problems. Its main advantage relies in its ability to compute with partial information expressed in terms of constraints. These constraints are monotonically, accumulated during the program execution in order to restrict the problem search space. We address one of the cornerstones of this technology, namely the current lack of debugging facilities. In particular visualization and understanding of the underlying constraint system during program executions is very important. We propose to structure this huge, flat and intricate part of the execution data in order to provide access to high level examination of its evolution. More precisely, we present a means to hierarchically organize sets of constraints in order to divide them into manageable parts while presenting computation correctness. Soundness of our method is shown, an algorithm supporting it is given, and an implemented prototype exhibiting its effectiveness is described.
[Visualization, program debugging, Costs, visualization tool, Logic programming, partial information, Debugging, debugging facilities, program execution, Electronic switching systems, Programming profession, Programming environments, Computer bugs, Prototypes, software tools, search space, constraint handling, program visualisation, constraint system, constraint program debugging, computation correctness, constraint programming]
An overview of Lutess a specification-based tool for testing synchronous software
14th IEEE International Conference on Automated Software Engineering
None
1999
Test data generation and test execution are both time-consuming activities when done manually. Automated testing methods promise to save a great deal of human effort. This especially applies to reactive programs which have complex behaviour over time and which require long test sequences. We present Lutess, a testing environment for synchronous reactive software. Lutess produces automatically and dynamically test data with respect to some environment constraints of the program under test. Moreover, it allows to trace the test execution and spot the situations where the program violates its properties. Lutess offers several specification-based testing methods. They aim at simulating more realistic environment behaviours, producing relevant data to test thoroughly a given property or driving the program under test into interesting situations. To produce the test data, the methods use different types of guides: statistical distribution of the input generation, properties, or behavioural patterns. Lutess proved to be powerful and easy to use in industrial case studies. Lutess won the Best Tool Award of the first Feature Interaction Detection Contest. The tool is described from a practical point of view.
[Software testing, System testing, Computer vision, Statistical analysis, program testing, test data generation, Test pattern generators, Application software, test execution, formal specification, Lutess, reactive programs, Read only memory, specification-based tool, Postal services, Automatic testing, statistical distribution, synchronous software testing, automated testing methods, software tools, Software tools]
Combining fault avoidance, fault removal and fault tolerance: an integrated model
14th IEEE International Conference on Automated Software Engineering
None
1999
Fault avoidance, fault removal and fault tolerance represent three successive lines of defense against the contingency of faults in software systems and their impact on system reliability. Beyond the colorful discussions of the relative merits of these techniques, the law of diminishing returns advocates that they be used in concert, where each is applied whenever it is most effective. Such a premise remains an idle act of faith so long as these techniques cannot be captured by a uniform model. This paper proposes such a model and illustrates how it can be used in practice to improve the quality of software products.
[Software maintenance, program debugging, fault avoidance, fault contingency, Programming, system reliability, Calculus, software quality, formal specification, law of diminishing returns, Fault tolerance, formal verification, integrated model, Software measurement, program validation, fault tolerance, Formal specifications, Application software, software fault tolerance, fault removal, Computer languages, uniform model, Aggregates, Software quality, software product quality]
Siddhartha: a method for developing domain-specific test driver generators
14th IEEE International Conference on Automated Software Engineering
None
1999
Siddhartha applies the domain-specific language (DSL) paradigm to solve difficult problems in specification-based testing (SBT). Domain-specific test case data specifications (TestSpecs) and difficult-to-test program design styles engender difficult SBT problems, which are the essential phenomena of interest to Siddhartha. Difficult-to-test program design styles are explicitly represented by domain-specific, unit test driver reference designs that accommodate the problematic program design styles. DSLs are developed to represent both TestSpecs and Driver reference designs. A DSL language processing tool (a translator) is developed that maps TestSpecs into Drivers. We developed a prototype implementation of Siddhartha via Reasoning SDK (formerly known as Software Refinery) and developed two domain-specific TestSpec/spl rarr/Driver translators. Each translator generated Drivers that revealed new failures in a real-world digital flight control application program.
[program testing, translator, Reasoning SDK, real-world digital flight control application program, Aerospace electronics, formal specification, automatic testing, Prototypes, Automatic control, aerospace computing, domain-specific unit test driver reference designs, DSL, Siddhartha, specification-based testing, domain-specific test driver generator development, domain-specific language paradigm, difficult-to-test program design styles, TestSpecs, DSL language processing tool, Application software, Formal specifications, Aerospace control, Domain specific languages, Computer science, program interpreters, domain-specific test case data specifications, Automatic testing, Driver reference designs]
Automatic generation of test oracles: from pilot studies to application
14th IEEE International Conference on Automated Software Engineering
None
1999
There is a trend towards the increased use of automation in V&V (verification and validation). Automation can yield savings in time and effort. For critical systems, where thorough V&V is required, these savings can be substantial. We describe a progression from pilot studies to development and use of V&V automation. We used pilot studies to ascertain opportunities for, and suitability of automating various analyses whose results would contribute to V&V. These studies culminated in the development of an automatic generator of automated test oracles. This was then applied and extended in the course of testing an AI planning system that is a key component of an autonomous spacecraft.
[program testing, program verification, Laboratories, Humans, Control systems, autonomous spacecraft, Space vehicles, Feathers, planning (artificial intelligence), automation, Space technology, pilot studies, Propulsion, aerospace computing, verification, validation, automatic test oracle generation, Automation, space vehicles, AI planning system testing, critical systems, Automatic testing, application generators, computer aided software engineering, Artificial intelligence]
Towards automatic imperative program synthesis through proof planning
14th IEEE International Conference on Automated Software Engineering
None
1999
An approach to automatic imperative program synthesis is presented which builds upon Gries' (1981) vision of developing a program and its proof hand in hand. To achieve this vision we rely on the proof planning paradigm, which enables the coupling of both heuristic and deductive components. By formalising structured programming and proof heuristics within the proof planning framework we focus the search for a correct program. Encoding these heuristics within a proof plan and strengthening proof planning, by embedding it within the conventional AI planning paradigm, enables a significant degree of automation.
[deductive components, Computer vision, automatic programming, Automation, Logic programming, heuristic components, structured programming, automatic imperative program synthesis, proof planning paradigm, Postal services, planning (artificial intelligence), heuristic programming, automation, Writing, proof heuristics, computer aided software engineering, theorem proving, structured programming formalisation, Artificial intelligence, AI planning paradigm, correct program]
Automated translation of UML models of architectures for verification and simulation using SPIN
14th IEEE International Conference on Automated Software Engineering
None
1999
The Unified Modeling Language (UML) is fast becoming an industry standard for object-oriented modeling and analysis. Applying the UML to model, analyze and design dependable systems require methods and tools for model checking that are integrated with the UML and its support environment. Recent advances in model-checking technologies have led to the development of approaches and tools to check the correctness of security protocols as well as check the correctness properties (such as deadlock properties) of architectural abstractions of component based systems implementing specific styles of coordination. This paper focuses on use of a specific model-checking technology, SPIN, in model checking architectures specified using UML. In particular the paper develops an approach to check desired properties of a class of distributed component based software architectures characterized by indirect connection via mediators and shared space. The approach is demonstrated in the context of an architectural design implementing the NetBill protocol for e-commerce.
[Protocols, program verification, Unified modeling language, software reliability, dependable systems, Electrical capacitance tomography, Electronic commerce, object-oriented modeling, model-checking technology, software architecture, component based systems, Computer architecture, deadlock properties, e-commerce, object-oriented methods, verification, electronic commerce, NetBill protocol, correctness, Unified Modeling Language, Object oriented modeling, SPIN, Birth disorders, Power system modeling, program interpreters, security protocols, UML models, Software engineering, Context modeling]
Automatic synthesis of control software for an industrial automation control system
14th IEEE International Conference on Automated Software Engineering
None
1999
We present a case study on automatic synthesis of control software from formal specifications for an industrial automation control system. Our aim is to compare the effectiveness (i.e. design effort and controller quality) of automatic controller synthesis from closed loop formal specifications with that of manual controller design, followed by automatic verification. Our experimental results show that for industrial automation control systems, automatic synthesis is a viable and profitable (especially as far as design effort is concerned) alternative to manual design, followed by automatic verification.
[automatic controller synthesis, program verification, closed loop formal specifications, Industrial control, Control systems, formal specification, design effort, Production, Control system synthesis, Automatic control, Electrical equipment industry, Manufacturing automation, automatic programming, industrial automation control system, formal specifications, case study, controller quality, manual controller design, Manufacturing industries, closed loop systems, automatic verification, industrial control, Computer industry, Circuit synthesis, control system analysis computing, control software synthesis]
Retrenchment: extending the reach of refinement
14th IEEE International Conference on Automated Software Engineering
None
1999
Discusses a simple example that demonstrates various expressive limitations of the refinement calculus, and suggests a liberalization of refinement, called retrenchent, which supports an analogous formal development calculus. Useful concrete system behaviour can be specified outside the domain of pure refinement, and a case is made for fluidity between I/O and state components across the development step. A syntax and a formal definition are presented for retrenchment, which has some necessary properties for a formal development calculus: transitivity gives stepwise composition of retrenchments, while monotonicity w.r.t. the specification language constructors gives piecewise construction of retrenchments.
[stepwise composition, I/O components, refinement calculus, Calculus, monotonicity, Specification languages, retrenchent, Proposals, formal definition, expressive limitations, Computer science, formal development calculus, piecewise construction, Veins, transitivity, specification languages, concrete system behaviour, state components, syntax, Concrete, Electrical equipment industry, Logic, specification language constructors]
Dynamic accommodation of change: automated architecture configuration of distributed systems
14th IEEE International Conference on Automated Software Engineering
None
1999
A major challenge in developing and coordinating distributed agents is to accommodate changes introduced at one agent and to propagate it to every other interested party. This presents a special difficulty if the information maintained in a node is highly structured. Examples of such problems are dynamic accommodation of structural changes in distributed software systems or consistency management of multiple viewpoints in typical multiple perspectives/stakeholders settings: modifications or changes not envisaged at "design time" have to be handled at run time without disturbing those parts of the system unaffected by the change. The key feature realizing such dynamic modifications and extensions is a clean separation between internal operations or actions of an individual node or agent and their structural coordination. Thus, general change rules for creation/deletion and connection/disconnection of nodes/agents can be formulated. We consider as an important problem area, dynamic change management of distributed systems. We propose to use distributed graph transformation as the underlying formalism to realize both the specification of evolving distributed systems as well as dynamic change management, thus taking a step towards building configurable distributed systems. The results obtained can easily be transferred to the field of consistency management.
[distributed system specification, distributed graph transformation, dynamic modifications, distributed software systems, Electrical capacitance tomography, formal specification, structural changes, structural coordination, general change rules, graph grammars, Computer architecture, dynamic change accommodation, distributed programming, configurable distributed systems, automatic programming, distributed agent coordination, dynamic change management, multiple perspectives/stakeholders settings, specification, internal operations, consistency management, configuration management, multiple viewpoints, automated architecture configuration, management of change, Software systems]
Modular and incremental analysis of concurrent software systems
14th IEEE International Conference on Automated Software Engineering
None
1999
Modularization and abstraction are the keys to practical verification and analysis of large and complex systems. We present in an incremental methodology for the automatic analysis and verification of concurrent software systems. Our methodology is based on the theory of abstract interpretation. We first propose a compositional data flow analysis algorithm that computes invariants of concurrent systems by composing invariants generated separately for each component. We present a novel compositional rule allowing us to obtain invariants of the whole system as conjunctions of local invariants of each component. We also show how the generated invariants are used to construct, almost for free, finite state abstractions of the original system that preserve safety properties. This reduces dramatically the cost of computing such abstractions as reported in previous work. We finally give a novel refinement algorithm that refines the constructed abstraction until the property of interest is proved or a counterexample is exhibited. Our methodology is implemented in a framework that combines deductive methods supported by theorem proving techniques and algorithmic methods supported by model checking and abstract interpretation techniques.
[Algorithm design and analysis, compositional data flow analysis algorithm, Costs, program verification, Laboratories, System analysis and design, parallel programming, Concurrent computing, abstract interpretation techniques, compositional rule, Safety, Data flow computing, theorem proving, verification, Data analysis, abstract interpretation, data flow analysis, concurrent software systems, incremental analysis, model checking, Software systems, Concrete, finite state abstractions, safety properties, theorem proving techniques]
Advanced modelling and verification techniques applied to a cluster file system
14th IEEE International Conference on Automated Software Engineering
None
1999
This paper describes the application of advanced formal modelling techniques and tools from the CADP toolset to the verification of CFS, a distributed file system kernel. After a short overview of the specification of CFS, we describe the techniques used for model generation and verification, and their application to CFS. Two original aspects are put forth: firstly, the model is generated in a compositional way, by putting together separately generated sub-components, secondly, the extensible, data-aware temporal logic checker XTL is used to express and verify properties of the system. In particular an XTL extension providing richer diagnostics is presented.
[program verification, Design methodology, NASA, Memory architecture, distributed file system kernel, cluster file system, temporal logic, verification techniques, Explosions, State-space methods, Specification languages, formal specification, Read only memory, data-aware temporal logic checker, Cryptographic protocols, CADP toolset, XTL, File systems, network operating systems, formal modelling, distributed databases, Writing, shared memory systems, model generation]
An ML editor based on proofs-as-programs
14th IEEE International Conference on Automated Software Engineering
None
1999
C/sup Y/NTHIA is a novel editor for the functional programming language ML in which each function definition is represented as the proof of a simple specification. Users of C/sup Y/NTHIA edit programs by applying sequences of high-level editing commands to existing programs. These commands make changes to the proof representation from which a new program is then extracted. The use of proofs is a sound framework for analysing ML programs and giving useful feedback about errors. Amongst the properties analysed within C/sup Y/NTHIA at present is termination. C/sup Y/NTHIA has been successfully used in the teaching of ML in two courses at Napier University, Scotland. C/sup Y/NTHIA is a convincing, real-world application of the proofs-as-programs idea.
[functional programming, function definition, utility programs, proof representation, teaching, program editing, ML editor, formal specification, Read only memory, ML language, Napier University, Program processors, specification proofs, Feedback, program analysis, programming courses, Functional programming, proofs-as-programs, Informatics, termination, computer science education, text editing, high-level editing command sequences, program diagnostics, NASA, Debugging, error feedback, program extraction, Programming environments, Computer languages, functional programming language, C/sup Y/NTHIA, Error correction]
Verification of picture generated code
14th IEEE International Conference on Automated Software Engineering
None
1999
Tools such as Simulink(R) are being used by engineers around the world to model and solve real problems about dynamical systems. In particular control systems are modelled graphically, simulated and then code is generated automatically. Code generated from such a tool is being proposed for use in safety critical control systems. The problem for certifiers is why should they trust such generated code? This paper describes a particular approach to solving this problem which supports review by a certifier. A small case study is presented to illustrate the approach and the technical issues involved.
[Decision support systems, program verification, safety-critical software, picture generated code, safety critical control systems, digital simulation, Simulink, certification, dynamical systems, trusted software, control system analysis computing, visual programming, verification]
Architectural element matching using concept analysis
14th IEEE International Conference on Automated Software Engineering
None
1999
A large portion of software development effort is focused on modification and evolution of existing software systems. To feed forward-engineering and design activities, analysts must first recover and synthesize a complete and consistent set of architectural representations. Architectural Synthesis is one method to build this representation. During the Architectural Synthesis of a software system, an analyst must combine information derived from a variety of sources (which we call perspectives). This combination process requires the analyst to make decisions about which elements in the perspectives denote the same underlying parts of the software system. We present an automated technique for matching these elements based upon a mathematical technique called concept analysis. This technique constructs a spectrum of matching relations using a lattice of concepts drawn from the perspectives and descriptive information about the system's application domain. The results show the promise of using concept analysis to match elements and aid in synthesizing a large number of perspectives.
[application domain, matching relations, architectural representations, PROM, Merging, Lattices, Architectural Synthesis, Data mining, formal specification, software system, Information analysis, Reactive power, software architecture, forward-engineering, Computer architecture, automated technique, descriptive information, combination process, automatic programming, design activities, mathematical technique, software maintenance, architectural element matching, Leg, Connectors, concept analysis, Feeds, software development effort]
Towards discovery, specification, and verification of component usage
14th IEEE International Conference on Automated Software Engineering
None
1999
Impediments to software quality are exacerbated when applications-under-test are developed using component-based software engineering. Component misuse is one such impediment. Component misuse occurs when a component is used in a way that differs from the component producer's expectation. This paper explores the cause of the component misuse problem and proposes a technique to discover, specify, and verify component usage. This technique utilizes regular expressions as the formalism to deal with component usage. This research is part of the software retrospector effort, which aims at a better approach to software analysis and testing for component-based software.
[Software testing, Heart, Costs, program verification, component-based software engineering, Time to market, Debugging, component misuse problem, component usage, software quality, Computer science, Software libraries, software analysis, Software quality, software engineering, Impedance, Software engineering]
Applying test automation to type acceptance testing of telecom networks: a case study with customer participation
14th IEEE International Conference on Automated Software Engineering
None
1999
This paper presents results of a joint case study of Ericsson and the German cellular network provider Mannesmann Mobilfunk, targeted at automating type acceptance tests. Faced with a growing number of tests required to verify the quality of the telecom switch software, both companies seek to improve testing efficiency by means of test automation. In a joint effort, a test platform originally created by Ericsson for supporting statistical usage tests was enhanced with features designed by Mannesmann. The platform was then employed to automate existing test instructions and the gain of test automation was measured in all phases, from implementation through execution under type acceptance conditions to final analysis. This paper presents the results and conclusions of this case study.
[Performance evaluation, System testing, Computer aided software engineering, program testing, program verification, test automation, Electrical capacitance tomography, software quality, electronic switching systems, telecom networks, telecommunication computing, type acceptance testing, Erbium, GSM, Automation, telecom switch software, customer participation, cellular network, Mannesmann Mobilfunk, Telecommunications, Ericsson, case study, statistical usage tests, Automatic testing, Land mobile radio cellular systems]
AML: an Architecture Meta-Language
14th IEEE International Conference on Automated Software Engineering
None
1999
The language AML (Architecture Meta-Language) is used to specify the semantics of architecture description languages (ADLs). It is a very primitive language, having declarations for only three constructs: elements, kinds and relationships. Each of these constructs may be constrained via predicates in temporal logic. The essence of AML is the ability to specify structure and to constrain the dynamic evolution of that structure. Dynamic evolution concerns arise with considerable variations in the time scale. One may constrain how a system can evolve by monitoring its development lifecycle. Another approach to such concerns involves limiting the systems' construction primitives to those from appropriate styles. One may wish to constrain what implementations are appropriate; concerns for interface compatibility are then germane. Finally, one may want to constrain the ability of the architecture to be modified as it is running. AML attempts to provide specification constructs that can be used to express all of these constraints without committing to which time scale will be used to enforce them.
[Pulleys, architecture description languages, temporal logic, Architecture Meta-Language, kinds, Machinery, constraints, software architecture, relationships, Computer architecture, specification languages, specification constructs, time scale, Logic, interface compatibility, Monitoring, AML, language semantics specification, temporal logic predicates, Natural languages, architectural modification, programming language semantics, system development lifecycle monitoring, Hip, declarations, dynamic evolution, construction primitives, Gears, structural specification, elements, system monitoring, Architecture description languages, Software engineering]
Component-based systems as an aid to design validation
14th IEEE International Conference on Automated Software Engineering
None
1999
There is a continuing need for software engineers to design better-quality systems more quickly. Component-based technologies promise to make this possible, but modern systems are too complex for a full analysis of their behaviour to be practical. We propose that a reasonable alternative is to analyse abstract models of the essential features of a system. Since these models are abstract, they need contain only those details that are relevant to the aspect of the system under consideration. Consequently, they can be small enough to be constructed quickly and analysed thoroughly using formal methods. Tools are required which are accessible to the novice but which remain powerful enough to build models with a formal foundation so that they can be used by system designers who have limited expertise in the use of formal methods. We propose our tool, RolEnact, as a candidate for this role.
[Java, component-based systems, RolEnact, system design validation, abstract models, software quality, Power system modeling, Computer science, Design engineering, systems analysis, Software quality, formal methods, Hardware, computer aided software engineering, software engineering, software tools, subroutines, Software tools, Software engineering]
Model checking programs
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
The majority of the work carried out in the formal methods community throughout the last three decades has (for good reasons) been devoted to special languages designed to make it easier to experiment with mechanized formal methods such as theorem provers and model checkers. In this paper, we give arguments for why we believe it is time for the formal methods community to shift some of its attention towards the analysis of programs written in modern programming languages. In keeping with this philosophy, we have developed a verification and testing environment for Java, called Java PathFinder (JPF), which integrates model checking, program analysis and testing. Part of this work has consisted of building a new Java Virtual Machine that interprets Java bytecode. JPF uses state compression to handle large states, and partial order reduction, slicing, abstraction and run-time analysis techniques to reduce the state space. JPF has been applied to a real-time avionics operating system developed at Honeywell, illustrating an intricate error, and to a model of a spacecraft controller, illustrating the combination of abstraction, run-time analysis and slicing with model checking.
[Real time systems, program verification, program testing, model-checking programs, Aerospace electronics, program testing environment, Java PathFinder, aerospace control, programming languages, error, Runtime, spacecraft controller, Operating systems, Java bytecode interpretation, program analysis, aerospace computing, state-space reduction, program slicing, mechanized formal methods, state compression, Testing, Java, partial order reduction, real-time avionics operating system, abstraction, program verification environment, Virtual machining, space vehicles, State-space methods, avionics, run-time analysis, Java Virtual Machine, Computer languages, real-time systems, virtual machines, operating systems (computers), Error correction, programming environments, state-space methods]
Exploring the design of an intentional naming scheme with an automatic constraint analyzer
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Lightweight formal modeling and automatic analysis were used to explore the design of the intentional naming system (INS), a new scheme for resource discovery in a dynamic networked environment. We constructed a model of INS in Alloy a lightweight relational notation, and analyzed it with the Alloy Constraint Analyzer, a fully automatic simulation and checking tool. In doing so, we exposed several serious flaws in both the algorithm of INS and the underlying naming semantics. We were able to characterize the conditions under which the existing INS scheme works correctly, and evaluate proposed fixes.
[Alloy relational notation, Java, client-server systems, Law, resource discovery, client server system, dynamic networked environment, automatic simulation tool, intentional naming system, relational databases, Computer science, lightweight formal modeling, Analytical models, Network servers, Databases, Alloy Constraint Analyzer, software tools, constraint handling, naming services, Legal factors, naming semantics]
Management of change in structured verification
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
The use of formal methods in large complex applications implies the need for an evolutionary formal program development in which specification and verification phases are interleaved. Any change of a specification either by adding new parts or by changing erroneous parts affects existing verification work in a subtle way. We present a truth maintenance system for structured specification and verification. It is based on the simple but powerful notion of a development graph as an underlying data structure to represent an actual consistent state of a formal development. Based on this notion we try to minimize the consequences of changes of existing verification work.
[Modular construction, Terminology, program verification, graph theory, data structure, truth maintenance, formal specification, structured verification, graph, evolutionary formal program development, Refining, truth maintenance system, management of change, formal methods, data structures, Artificial intelligence]
A comparison of questionnaire-based and GUI-based requirements gathering
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Software development includes gathering information about tasks, work practices and design options from users. Traditionally requirements gathering takes two forms. Interviews and participatory design (PD) practices gather rich information about the task and the domain but require face-to-face communication between the software engineers and the users. When such communication is not possible, traditional software engineering frequently relies on questionnaires and other paper-based methods. Unfortunately, questionnaires often fail to capture implicit aspects of user tasks that may be identified through one-on-one interactions. This project investigates a method of gathering requirements whereby users, working independently of software engineers, construct rough interfaces augmented with textual argumentation. Our initial study has compared the use of GRC (Graphical Requirements Collector) with questionnaire-based requirements gathering.
[GUI-based requirements gathering, Information resources, software development, graphical user interfaces, textual argumentation, graphical user interface, Documentation, Software performance, Programming, participatory design, task analysis, formal specification, Graphical Requirements Collector, user tasks, interviews, Computer science, Design engineering, Software packages, Engineering management, systems analysis, Solids, GRC, questionnaire-based requirements gathering, Software engineering]
CM-Builder: an automated NL-based CASE tool
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
This paper describes a natural language-based CASE tool called CM-Builder which aims at supporting the analysis stage of software development in an object-oriented framework. CM-Builder uses robust natural language processing techniques to analyse software requirements texts written in English and build an integrated discourse model of the processed text, represented in a semantic network. This semantic network is then used to automatically construct an initial UML class model representing the object classes mentioned in the text and the relationships among them. The initial model can be directly input to a graphical CASE tool for further refinement by a human analyst. CM-Builder has been quantitatively evaluated in blind trials against a collection of unseen software requirements texts and we present the results of this evaluation, together with the evaluation methodology. The results are very encouraging and demonstrate that tools such as CM-Builder have the potential to play an important role in the software development process.
[Computer aided software engineering, Unified modeling language, Humans, natural language-based CASE tool, CM-Builder, Programming, integrated discourse model, Robustness, software tools, graphical CASE tool, object-oriented programming, software development, Object oriented modeling, Natural languages, natural language interfaces, semantic network, Computer science, requirements analysis, object-oriented framework, systems analysis, semantic networks, computer aided software engineering, Artificial intelligence, Software tools, UML class model]
Finding comparatively important concepts between texts
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Finding important concepts is a common task in requirements engineering. For example, it is needed when building models of a domain or organising requirements documents. Since a lot of information is available in textual form, methods to identify important concepts from texts are potentially useful. Traditional methods for finding important concepts from texts rely on the assumption that the most frequent concepts are the most important. We present an approach that does not depend on this assumption. It makes use of two texts to find important concepts comparatively. We show that this approach is viable. It discovers concepts similar to those found by traditional approaches as well as concepts that are not frequent. Finally, we discuss the possibility of extending this work to requirements classification.
[important concepts, text analysis, Text analysis, Natural languages, Humans, classification, Bridges, requirements engineering, requirements classification, Text recognition, requirements documents, systems analysis, textual form, Informatics]
Upgrading legacy instances of reactive systems
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
A software product typically goes through many "upgrades" (version changes) over its lifetime. Reactive systems, such as e-mail clients, software agents, proxies, traffic controllers, and telephone switches are no exception. Evolving such stateful systems is made difficult by the fact that new versions of the software must deal correctly with legacy instances. Users of earlier versions have invested significant resources in creating the state of the legacy instance, and usually require that this state be upgraded appropriately when the new system version is activated. However, validating the correctness of this upgrading behavior is particularly difficult, whether through testing or more formal techniques like model checking, because legacy states are typically unreachable to the new version of the software. This paper explores this problem and requirements for its solution; presents a simple conceptual and modeling/programming upgrade framework, based upon the idea of a supermodel that allows upgrade behavior to be validated using mainstream approaches; and gives techniques for simplifying the validation problem.
[Software testing, formal techniques, program verification, program testing, reactive systems, Switches, legacy instance upgrading, State-space methods, Electronic mail, programming upgrade framework, software maintenance, model checking, supermodel, Telephony, Software systems, Software agents, software upgrades, Books, Cryptography, software product, program validation]
Renaming detection
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Finding changed identifiers in programs is important for program comparison and merging. Comparing two versions of a program is complicated if renaming has occurred. Textual merging is highly unreliable if, in one version, identifiers were renamed, while in the other version, code using the old identifiers was added or modified. A tool that automatically detects renamed identifiers between pairs of program modules is presented. The detector is part of a suite of intelligent differencing and merging programs that exploit the static semantics of programming languages. No special editor is needed for tracking changes. The core of the renaming detector is language independent. The detector works with multiple file pairs, taking into account renamings that affect multiple files. Renaming detectors for Java and Scheme have been implemented. A case study is presented that demonstrates proof of concept. With renaming detection, a higher quality of program comparison and merging is achievable.
[System testing, Java, merging, Corporate acquisitions, Scheme, Heuristic algorithms, textual merging, Merging, renaming detection, program comparison, intelligent differencing programs, programming language semantics, program identifiers, Programming profession, Program processors, software tool, Collaboration, Detectors, Dynamic programming, software tools, naming services]
Mutation operators for specifications
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Testing has a vital support role in the software engineering process, but developing tests often takes significant resources. A formal specification is a repository of knowledge about a system, and a recent method uses such specifications to automatically generate complete test suites via mutation analysis. We define an extensive set of mutation operators for use with this method. We report the results of our theoretical and experimental investigation of the relationships between the classes of faults detected by the various operators. Finally, we recommend sets of mutation operators which yield good test coverage at a reduced cost compared to using all proposed operators.
[Software testing, program testing, program verification, Genetic mutations, Formal specifications, formal specification, test suites, Automatic testing, mutation operators, mutation analysis, Software systems, test coverage, Hardware, software engineering, Logic, Formal verification]
The use of abduction and recursion-editor techniques for the correction of faulty conjectures
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
The synthesis of programs, as well as other synthetic tasks, often ends up with an unprovable, partially false conjecture. A successful subsequent synthesis attempt depends on determining why the conjecture is faulty and how it can be corrected. Hence, it is highly desirable to have an automated means for detecting and correcting fault conjectures. We introduce a method for patching faulty conjectures. The method is based on abduction and performs its task during an attempt to prove a given conjecture. On input /spl forall/X.G(X), the method builds a definition for a corrective predicate, P(X), such that /spl forall/X.P(X)/spl rarr/G(X) is a theorem. The synthesis of a corrective predicate is guided by the constructive principle of "formulae as types\
[program verification, program editing commands, inference, Programming, Turning, type theory, conjecture proving, correctness guarantee, construction commands, synthetic tasks, theorem proving, faulty conjecture correction, error correction, automatic programming, program control structures, corrective predicate, types, Buildings, abduction, program synthesis, terminating recursive theorem, recursive equational procedures, inference mechanisms, Equations, computation, Computer science, Fault detection, formulae, recursion-editor techniques, program transformation, faulty conjecture patching, well-defined conditional theorem]
A DSL approach to improve productivity and safety in device drivers development
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Although new peripheral devices are emerging at a frantic pace and require the fast release of drivers, little progress has been made to improve the development of such device drivers. Too often, this development consists of decoding hardware intricacies, based on inaccurate documentation. Then, assembly-level operations need to be used to interact with the device. These low-level operations reduce the readability of the driver and prevent safety properties from being checked. This paper presents an approach based on domain-specific languages (DSLs) to overcome these problems. We define a language, named Devil (DEVice Interaction Language), dedicated to defining the basic communication with a device. Unlike a general-purpose language, Devil allows a description to be checked for consistency. This not only improves the safety of the interaction with the device but also uncovers bugs early in the development process. To asses our approach, we have shown that Devil is expressive enough to specify a large number of devices. To evaluate productivity and safety improvements over traditional development in C, we report an experiment based on mutation testing.
[program verification, program testing, high level languages, Devil language, device drivers, language expressiveness, assembly-level operations, Device Interaction Language, productivity, safety, debugging, Hardware, software engineering, description consistency checking, DSL, Product safety, Assembly, Productivity, Safety devices, Documentation, mutation testing, Decoding, device communication definition, Domain specific languages, readability, device driver development, domain-specific language, Computer bugs, peripheral devices]
Circular coinductive rewriting
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Circular coinductive rewriting is a new method for proving behavioral properties, that combines behavioral rewriting with circular coinduction. This method is implemented in our new BOBJ (Behavioral OBJects) behavioral specification and computation system, which is used in examples throughout this paper. These examples demonstrate the surprising power of circular coinductive rewriting. The paper also sketches the underlying hidden algebraic theory and briefly describes BOBJ and some of its algorithms.
[rewriting systems, algorithms, programming theory, Induction generators, behavioral rewriting, behavioral property proving, Mathematics, BOBJ, hidden algebraic theory, inference mechanisms, Equations, Computer science, behavioral specification system, behavioral computation system, Power engineering computing, Algebra, circular coinductive rewriting, behavioral objects, Logic functions, circular coinduction, object-oriented methods, algebraic specification]
Formal construction of the Mathematically Analyzed Separation Kernel
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Describes the formal specification and development of a separation kernel. The Mathematically Analyzed Separation Kernel (MASK), has been used by Motorola on a smartcard project, and as part of a hardware cryptographic platform called the Advanced INFOSEC (INFOrmation SECurity) Machine (AIM). Both MASK and AIM were jointly developed by Motorola and the National Security Agency (NSA). This paper first describes the separation kernel concept and its importance to information security. Next, it illustrates the Specware formal development methodology that was used in the development of MASK. Experiences and lessons learned from this formal development process are discussed. Finally, the results of the MASK development process are described, project successes are discussed, and related MASK research is highlighted.
[smart cards, Advanced INFOSEC Machine, National Security Agency, formal specification, MASK development process, Operating systems, Hardware, Books, Cryptography, Kernel, National security, firmware, Context, operating system kernels, smartcard project, Data security, AIM, information security, cryptography, Mathematically Analyzed Separation Kernel, Certification, Information security, hardware cryptographic platform, Motorola, Specware formal development methodology]
Simultaneous checking of completeness and ground confluence
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Algebraic specifications provide a powerful method for the specification of abstract data types in programming languages and software systems. Completeness and ground confluence are fundamental notions for building algebraic specifications in a correct and modular way. In this paper, we present a procedure for simultaneously checking completeness and ground confluence for specifications with free/non-free constructors and parameterized specifications. If the specification is not complete or not ground-confluent, then our procedure outputs the set of patterns on whose ground instances a function is not defined and it can easily identify the rules that break ground confluence. Our procedure is complete and always terminates under the assumption of an oracle for deciding (joinable) inductive properties. In contrast to previous work, our method does not rely on completion techniques and does not require the computation of critical pairs of axioms. The method has been implemented in the prover SPIKE. This system has allowed us to prove the completeness and the ground confluence of many specifications in a completely automatic way, where related techniques diverge or generate very complex proofs.
[software systems, undefined function, correct modular method, Electronic mail, algebraic specifications, programming languages, Uniform resource locators, formal verification, joinable inductive properties, Prototypes, Communications technology, theorem proving, algebraic specification, simultaneous checking procedure, termination, Data analysis, ground instances, oracle, Buildings, abstract data types, Data structures, ground confluence checking, Equations, completeness checking, free constructors, Automatic testing, SPIKE theorem prover, nonfree constructors, parameterized specifications, Software systems]
Using Little-JIL to coordinate agents in software engineering
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Little-JIL, a new language for programming the coordination of agents, is an executable, high-level process programming language with a formal (yet graphical) syntax and rigorously defined operational semantics. Little-JIL is based on two main hypotheses. The first is that the specification of coordination control structures is separable from other process programming language issues. Little-JIL provides a rich set of control structures while relying on separate systems for support in areas such as resource, artifact and agenda management. The second hypothesis is that processes can be executed by agents who know how to perform their tasks but can benefit from coordination support. Accordingly, each step in Little-JIl is assigned to an execution agent (human or automated). These agents are responsible for initiating steps and performing the work associated with them. This approach has so far proven effective in allowing us to clearly and concisely express the agent coordination aspects of a wide variety of software, workflow and other processes.
[resource management, execution agent, multi-agent systems, Laboratories, Humans, Control systems, visual languages, specification languages, software processes, software engineering, Little-JIL, coordination control structure specification, rigorously defined operational semantics, program control structures, coordination support, agenda management, Educational institutions, formal graphical syntax, executable high-level process programming language, artifact management, Computer science, agent coordination programming language, Computer languages, Content addressable storage, workflow processes, computer aided software engineering, Resource management, Software tools, Software engineering]
Practical large scale what-if queries: case studies with software risk assessment
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
When a lack of data inhibits decision-making, large-scale what-if queries can be conducted over the uncertain parameter ranges. Such queries can generate an overwhelming amount of data. We describe a general method for understanding that data. Large-scale what-if queries can guide Monte Carlo simulations of a model. Machine learning can then be used to summarize the output. The summarization is an ensemble of decision trees. The TARZAN system [so-called because it swings through (or searches) the decision trees] can poll the ensemble looking for majority conclusions regarding what factors change the classifications of the data. TARZAN can succinctly present the results from very large what-if queries. For example, in one of the studies presented, we can view the significant features from 10/sup 9/ what-if queries on half a page.
[Computer aided software engineering, Costs, COCOMO-II, software risk assessment, Project management, data mining, majority conclusions, large-scale what-if queries, case studies, Open source software, query processing, Monte Carlo methods, Databases, safety, data classification, uncertain parameter ranges, Large-scale systems, learning (artificial intelligence), output summarization, Decision making, NASA, data understanding, TARZAN system, machine learning, decision support systems, large-scale systems, ensemble learning, Monte Carlo simulations, significant features, decision-making, Machine learning, decision trees, decision support system, computer aided software engineering, software cost estimation, Risk management, decision tree ensemble polling]
Exploring and validating the contributions of real-world knowledge to the diagnostic performance of automated database design tools
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Automated database design tools employ knowledge-based systems technology in order to provide intelligent support to humans during the process of database analysis and design. However, the level to which these tools can simulate the diagnostic capabilities of human designers when performing a design task remains in question. Human designers employ what might be called "knowledge of the real world" in carrying their design activities; such knowledge is employed by only a few automated database design tools. Therefore, in recent years, there have been a number of attempts to develop tools that are capable of exploiting such real-world knowledge. It has been claimed that the use of such knowledge has the potential to increase the diagnostic performance of automated database design tools. However, to date, little if any formal exploration and validation of this claim has taken place. This paper presents our activities in exploring and validating the implications for exploiting three approaches facilitating the use and exploitation of real-world knowledge in the diagnostic performance of database design tools. The results obtained have demonstrated that the improvement of certain aspects of diagnostic performance has been achieved. However, the extent to which these aspects have been attained and subsequently statistically validated varies.
[statistical validation, Dictionaries, Data analysis, knowledge-based systems technology, Humans, Mathematics, diagnostic performance, Thesauri, database management systems, Information analysis, Information science, diagnostic expert systems, automated database design tools, intelligent support, real-world knowledge, application generators, computer aided software engineering, Performance analysis, software tools, Mathematical model, Deductive databases, software performance evaluation]
Using graph rewriting to specify software architectural transformations
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
In order to understand, analyze and possibly modify software, we commonly examine and manipulate its architecture. For example, we may want to examine the architecture at different levels of abstraction or repair the architecture if it has deviated from our mental model of the software. We can view such manipulations as architectural transformations, and more specifically, as graph transformations. In this paper, we use graph rewriting to specify these transformations so that we can work towards automating them. Specifically, we use the PROGRES tool to formulate executable graph-rewriting specifications for various architectural transformations in order to demonstrate the strengths of using graph rewriting. We have applied our executable specifications to small graphs and our results confirm that graph rewriting offers a high-level visual notation that can be used to neatly specify graph modifications, as well as supporting prototype implementations. It also provides a convenient and intuitive framework for exploring various architectural transformations.
[Software maintenance, graph transformations, software architectural transformation specification, graph theory, abstraction levels, Data mining, formal specification, Information analysis, software architecture, Software architecture, program understanding, graph modifications, Prototypes, software modification, Computer architecture, Cognitive science, executable specifications, Software prototyping, rewriting systems, PROGRES tool, reverse engineering, graph rewriting, software maintenance, Computer science, prototype implementation, software analysis, high-level visual notation, Software systems, computer aided software engineering]
Specialization patterns
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Design patterns offer many advantages for software development, but can introduce inefficiency into the final program. Program specialization can eliminate such overheads, but is most effective when targeted by the user to specific bottlenecks. Consequently, we propose that these concepts are complementary. Program specialization can optimize programs written using design patterns, and design patterns provide information about the program structure that can guide specialization. Concretely, we propose specialization patterns, which describe how to apply program specialization to optimize uses of design patterns. We analyze the specialization opportunities provided by specific uses of design patterns. Based on the analysis of each design pattern, we define the associated specialization pattern. These specialization opportunities can be declared using the specialization classes framework, developed previously. In our experiments, such specialization significantly improves performance.
[Performance evaluation, program specialization, Art, object-oriented programming, Optimizing compilers, software development, software reuse, specialization patterns, Pareto optimization, Data structures, bottlenecks, Uninterruptible power systems, Programming profession, Design optimization, design patterns, software reusability, Velocity measurement, Pattern analysis, specialization class framework]
Identification of potentially infeasible program paths by monitoring the search for test data
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
A tool and techniques are presented for test data generation and identification of a path's likely unfeasibility in structural software testing. The tool is based on the dynamic technique and search using genetic algorithms. Our work introduces a new fitness function that combines control and data flow dynamic information to improve the process of search for test data. The unfeasibility issue is addressed by monitoring the genetic algorithm's search progress. An experiment shows the validity of the developed solutions and the benefit of using the tool.
[Software testing, Costs, program testing, Input variables, fitness function, test data generation, Electronic mail, potentially infeasible program paths, data flow dynamic information, test data search, Genetic algorithms, genetic algorithm, search, experiment, software tools, structural software testing, Monitoring, search problems, Automation, data flow analysis, genetic algorithms, Flow graphs, software tool, Automatic testing, Software tools]
Generating test data for branch coverage
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Branch coverage is an important criteria used during the structural testing of programs. We present a new program execution based approach to generate input data that exercises a selected branch in a program. The test data generation is initiated with an arbitrarily chosen input from the input domain of the program. A new input is derived from the initial input in an attempt to force execution through any of the paths through the selected branch. The method dynamically switches among the paths that reach the branch by refining the input. Using a numerical iterative technique that attempts to generate an input to exercise the branch, it dynamically selects a path that offers less resistance. We have implemented the technique and present experimental results of its performance for some programs. Our results show that our method is feasible and practical.
[Software testing, Data analysis, experimental results, program testing, Switches, test data generation, program execution, numerical iterative technique, Computer science, structural program testing, program branch coverage, Performance analysis, Software tools, software performance, software performance evaluation]
Test sequences generation from LUSTRE descriptions: GATEL
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
We describe a test sequence generation method from LUSTRE descriptions and its companion tool, GATEL. The LUSTRE language is declarative and describes synchronous data-flow computations. It is used for reactive control/command systems, mainly for electrical power production applications. Such critical applications require a high level of reliability. While this language benefits from powerful verification tools, there is still a demand for adequate testing techniques. The method and the tool described can be applied during unit and integration testing, according to a structural (glass box) or functional (black box) test selection strategy. The test generation tool uses some interpretation of the language constructs as boolean and integer interval constraints. Test sequence generation is automated using constraint logic programming techniques. The method and the tool are illustrated on an example extracted from an industrial case study.
[Software testing, program testing, program verification, Glass, reliability, Logic testing, verification tools, test generation tool, Automatic control, LUSTRE, constraint logic programming, Safety, software tools, constraint handling, parallel languages, test sequence generation method, reactive command control systems, Logic programming, electrical power production, GATEL, declarative language, Power system modeling, Automatic testing, integer interval constraints, Signal generators, synchronous dataflow computations, Power system reliability, boolean constraints]
Automating the composition of middleware configurations
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
A method is presented for the automatic construction of all possible valid compositions of different middleware software architectures. This allows reusing the latter in order to create systems providing a set of different non-functional properties. These compositions are constructed by using only the structural information of the architectures, i.e. their configurations. Yet, they provide a valuable insight on the different properties of the class of systems that can be constructed when a particular set of non-functional properties is required.
[middleware configuration composition automation, nonfunctional properties, client-server systems, Merging, software configuration, Mechanical factors, Decoding, Application software, Middleware, configuration management, Fault tolerance, software architecture, Voting, Computer architecture, Software systems, Concrete, software tools]
/spl pi/-SPACE: a formal architecture description language based on process algebra for evolving software systems
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
This paper presents an architecture description language, called /spl pi/-SPACE, designed to cope with requirements on evolving software architectures. /spl pi/-SPACE provides constructs for describing architectures based on the /spl pi/-calculus. It supports the adding, replacing and removing of components and connectors in an evolving software system. Applications of /spl pi/-SPACE are motivated by work on the relationships between business processes (including software development) and the software systems which support them. Businesses need flexible software that enhances their ability to adapt and survive in changing organizations.
[pi-SPACE, Programming, /spl pi/-SPACE, business processes, Calculus, Electronic mail, Application software, formal specification, Connectors, pi calculus, formal architecture description language, software architecture, Algebra, Software architecture, process algebra, /spl pi/-calculus, Computer architecture, specification languages, evolving software systems, organizations, Software systems, Architecture description languages, software components]
Towards explicit representation of architectural design assumptions
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Architecture description languages (ADLs) are means to specify software architectures. During the last few years, a number of ADLs addressed structural aspects of software systems. However, constructing architecture descriptions (ADs) requires specific types of domain knowledge and introduces specific restrictions on the problems to be solved. Such requirements and restrictions play a key role in specifying, reusing and evolving ADs, in acquiring domain knowledge, and in defining the problems to be tackled by the software systems. We discuss the different roles that assumptions play in architecture-centered software development and we derive the requirements for capturing them as part of an ADL. We show how such requirements introduce bias for ADL formalisms.
[domain knowledge, Costs, software reuse, architecture description languages, structural software aspects, Programming, Ontologies, ADL, Electronic mail, Application software, Information technology, formal specification, software architecture, Software architecture, Computer architecture, specification languages, Software systems, Architecture description languages]
Java model checking
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
This paper presents initial results in model checking multi-threaded Java programs. Java programs are translated into the SAL (Symbolic Analysis Laboratory) intermediate language, which supports dynamic constructs such as object instantiations and thread call stacks. The SAL model checker then exhaustively checks the program description for deadlocks and assertion failures, using traditional model checking optimizations to curb the state explosion problem. Most of the advanced features of the Java language are modeled within our framework.
[program verification, Laboratories, Java model checking, Yarn, object instantiations, thread call stacks, assertion failure, deadlocks, Java, object-oriented programming, SAL language, multi-threading, Data structures, Explosions, Symbolic Analysis Laboratory language, program description, Computer science, Computer languages, state explosion problem, concurrency control, System recovery, Writing, multithreaded Java programs, Internet, optimizations]
Non-interference analysis for mixed criticality code in avionics systems
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Future aircraft system procurements are expected to utilise a new form of modular architecture. However, the architectures being put forward only provide for hardware partitioning, and there is little protection for safety-critical processes from interference by rogue processes. This paper puts forward a mixed static/dynamic analysis approach for assuring software partitioning of processes within a single hardware partition. Such an approach is a necessity in cost effective modular architectures if all processes are not to be classified and developed as safety-critical.
[Costs, aircraft computers, safety-critical software, avionics systems, Aerospace electronics, mixed criticality code, aircraft control, safety-critical processes, software architecture, mixed static dynamic analysis, hardware partitioning, Computer architecture, Military aircraft, Hardware, Safety, Protection, aircraft system procurements, Procurement, military aircraft, software partitioning, Interference, modular architecture, Aerospace control, cost effective, noninterference analysis]
Automated security checking and patching using TestTalk
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
In many computer system security incidents, attackers successfully intruded computer systems by exploiting known weaknesses. Those computer systems remained vulnerable even after the vulnerabilities were known because it requires constant attention to stay on top of security updates. It is often both time-consuming and error-prone to manually apply security patches to deployed systems. To solve this problem, we propose to develop a framework for automated security checking and patching. The framework, named Securibot, provides a self-operating mechanism for security checking and patching. Securibot performs security testing using security profiles and security updates. It can also detect compromised systems using attack signatures. Most important, the Securibot framework allows system vendors to publish recently discovered security weaknesses and new patches in a machine-readable form so that the Securibot system running on deployed systems can automatically check out security updates and apply the patches.
[Performance evaluation, TestTalk, Securibot, automated security checking, Computer science, automated security patching, attack signatures, security of data, Automatic testing, self-operating mechanism, Information security, Computer errors, security updates, compromised system detection, security profiles, Internet, DSL, Computer security, Web server, Business]
Controlled automation of consistency enforcement
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Consistency enforcement aims at modifying a program specification such that the result is consistent with respect to a specified set of invariants. Our approach requires the modified program specification to be a maximal consistent diminution of the original one with respect to some partial order. One choice for this partial order is operational specialization, another one arises from the preservation of certain transition invariants. For both choices of the order we obtain a commutativity and a compositionality result which enable a library based pragmatic approach. This sets up a controlled form of automation.
[Automation, Logic programming, consistency enforcement, library based pragmatic approach, formal specification, Guidelines, controlled automation, Runtime, transition invariants, commutativity, Automatic control, compositionality, maximal consistent diminution, operational specialization, Deductive databases, partial order, program specification modification]
An overview of a method and its support tool for generating B specifications from UML notations
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
This paper presents, through an example, an overview of our method which generates B specifications from an application described using UML notations. We are interested in data intensive applications. This allows us to automatically generate basic update operations from class diagrams. Then these operations are combined to elaborate more complex transactions described in UML by state and collaboration diagrams. The obtained B machines are directly usable in AtelierB and proofs can be performed allowing the consistency of the application to be checked. Finally the outlines of the prototype support tool are described.
[Unified modeling language, class diagrams, Formal specifications, state diagrams, formal specification, data intensive application, collaboration diagrams, Refining, Collaboration, Prototypes, Fusion power generation, proofs, specification languages, Skeleton, software tools, B specification generation, AtelierB, update operations, UML notations]
A transformational viewpoint on design patterns
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
We propose that design patterns be systematically analyzed and reformulated to exhibit a reasoning binding solutions to precisely stated problems, given a set of mechanisms. We show that these mechanisms can be described as program transformations and how an assistant tool could systematically explore the set of potential solutions induced by these transformations. This will both help the selection of the appropriate patterns and their instantiation in the context of the application under development.
[Design automation, object-oriented programming, Logic programming, Production facilities, Logic design, Proposals, program transformations, Information analysis, Computer languages, assistant tool, design patterns, software tools, Virtual prototyping, Pattern analysis, Object oriented programming]
Issues for the automatic generation of safety critical software
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
This paper presents the advantages and disadvantages of automatic code generation of safety critical software. It discusses three broad approaches to its generation in the context of the high levels of assurance required. Finally a number of issues that commercial tool vendors must address are discussed along with consequent research issues that follow.
[System testing, automatic programming, Costs, Automation, Induction generators, Synthesizers, safety-critical software, Programming, Software safety, automatic code generation, program compilers, commercial tool vendors, assurance, Automatic testing, Software quality, software tools, safety critical software, Hardware design languages]
An experiment in scientific program understanding
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
This paper concerns automated analysis of the meaning or semantics of scientific and engineering code. The procedure involves taking a user's existing code, adding semantic declarations for some primitive variables, and automatically identifying formulae. Parsers encode domain knowledge and recognize formulae in different disciplines including physics, numerical methods, mathematics, and geometry. The parsers will automatically recognize and document some static, semantic concepts and help locate some program semantic errors. Results are shown for three intensively studied codes and seven blind test cases; all test cases are state of the art scientific codes. These techniques may apply to a wider range of scientific codes. If so, the techniques could reduce the time, risk, and effort required to develop and modify scientific codes.
[numerical methods, mathematics, formulae recognition, Software performance, Manuals, parsers, scientific code, semantics, scientific program understanding, engineering code, Investments, meaning, physics, primitive variables, natural sciences computing, Object oriented programming, Testing, domain knowledge encoding, semantic declarations, Knowledge representation, Documentation, reverse engineering, automated analysis, Physics, Equations, grammars, program semantic errors, geometry, Acceleration]
Representing technology to promote reuse in the software design process
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
This paper discusses a representation for the specification of technology components exploiting the separation of domain requirements (i.e. functional, data and timing requirements) from installation requirements (i.e. implementation specific infrastructure requirements). A process is outlined for building a technology component archive and a set of coefficients is introduced to assist designers in identifying "best-fit" candidates.
[reuse, Buildings, technology component archive, Ontologies, domain requirements, Middleware, Research and development, Software design, Databases, Investments, Isolation technology, software reusability, installation requirements, Computer industry, Timing, software design process]
Toward the automatic assessment of evolvability for reusable class libraries
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Many sources agree that managing the evolution of an OO system constitutes a complex and resource-consuming task. This is particularly true for reusable class libraries, as the user interface must be preserved to allow for version compatibility. Thus, the symptomatic detection of potential instabilities during the design phase of such libraries may serve to avoid later problems. This paper presents a fuzzy logic-based approach for evaluating the interface stability of a reusable class library, by using structural metrics as stability indicators.
[interface stability, Machine learning algorithms, Stability, Object oriented modeling, user interfaces, software maintenance, fuzzy logic-based approach, software libraries, user interface, Fuzzy logic, object-oriented system, Software libraries, Bayesian methods, reusable class libraries, Machine learning, Software quality, automatic evolvability assessment, software reusability, structural metrics, Decision trees, Resource management, stability indicators]
Translating use cases to sequence diagrams
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
We present a semi-automatic approach to translate a use case to a sequence diagram, which can be easily used in software design. It needs to normalize a use case manually. It accommodates user instructions on how to translate some parts of the use case description while producing message sends from other parts mechanically.
[Computer aided software engineering, semi-automatic approach, sequence diagrams, software design, message sends, Information retrieval, Transaction databases, Guidelines, Concurrent computing, Computer science, program interpreters, Computer languages, Software design, use case translation, Software systems, user instructions, Pins]
New visual interface for engineering use case models
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
This paper describes a new approach to visualization of scenarios within the use case-based engineering of functional requirements-the so-called video camera metaphor. The video camera metaphor facilitates involvement of business people, customers, problem domain experts and other non-technical stakeholders into capturing and validating formal requirements models. The key tool, supporting the video camera metaphor is the so-called interface editor which allows to draft the prototype user interface and automatically generates a user-friendly front-end to the set of formal modelling tools. The essence of the video camera interface is to associate sequences of events on a UML sequence diagram or an ITU-T message sequence chart with sequences of activations of the elements of the generated user interface. The video camera interface allows capturing scenarios through direct activation of both the input and the output elements of the generated user interface. The generated user interface is also used to replay scenarios for validation purposes.
[Visualization, System testing, Computer aided software engineering, use case model engineering, visualization, graphical user interfaces, Unified modeling language, video camera metaphor, interface editor, formal specification, UML sequence diagram, Design engineering, Prototypes, functional requirements, ITU-T message sequence chart, video camera interface, Power system modeling, user interface, automatic user-friendly front-end generation, visual interface, formal modelling tools, User interfaces, Cameras, Power engineering and energy, formal requirements models]
Semantic abstraction rules for class diagrams
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
When dealing with object-oriented models like class and object diagrams, designers easily get overwhelmed by large numbers of model elements and their interdependencies. To deal with the complexities of large-scale software models, this paper presents rules and methods for automated abstraction. Our approach is tool supported and allows designers to periodically "zoom out" of a model to investigate and reason about its bigger picture. Our technique has also proven to be well-suited for consistency checking and reverse engineering.
[object-oriented models, Unified modeling language, Reverse engineering, class diagrams, large-scale software models, Programming, software tool support, diagrams, consistency checking, model overview, Software design, model element interdependencies, formal verification, object-oriented design, object-oriented methods, software tools, Navigation, Object oriented modeling, reverse engineering, semantic abstraction rules, large-scale systems, Computer science, object diagrams, Packaging, Concrete, computer aided software engineering, automated abstraction, Joining processes]
Automatic derivation of Petri net based distributed specification with optimal allocation of resources
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
In this paper, we present a method for the synthesis of extended Petri net-based distributed specifications. Our method finds an optimal allocation of resources (computational data) that optimizes the derived distributed specification, based on some reasonable communication-cost criteria.
[Context, Protocols, Input variables, Petri nets, Optimization methods, Petri net-based distributed specification, distributed processing, communication complexity, Information technology, Distributed computing, formal specification, specification optimization, optimisation, resource allocation, computational data allocation, Fires, Integer linear programming, Cost function, automatic specification derivation, computer aided software engineering, optimal resource allocation, Resource management, communication cost criteria]
Combining the best attributes of qualitative and quantitative risk management tool support
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Tools have been developed that support risk identification and management activities during different phases of a project lifecycle. For the earlier stages of the project lifecycle, a tool for the qualitative identification and manipulation of risk and risk mitigation data was developed. For the later stages of the lifecycle, a separate tool for the quantitative manipulation of requirements, risk and risk mitigation data was developed. These two tools were then combined into a single tool. The combination of these qualitative and quantitative risk management tools is the focus of this paper. The combination was first envisioned as simply a convenience, ensuring that the results from the early lifecycle risk management would flow smoothly into the later lifecycle management. However, it was found that the combination led to the possibility of extending many of the capabilities of each tool into the other tool's phases. The net result is a combination that exhibits the best attributes of both qualitative and quantitative risk management tool support.
[risk management, Job shop scheduling, project support environments, project management, project lifecycle phases, NASA, Laboratories, Project management, software development management, tool support, requirements analysis, Feathers, Technology management, Propulsion, risk identification, quantitative risk management, Risk management, Resource management, Software tools, risk mitigation data, qualitative risk management]
A group critic system for object-oriented analysis and design
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Presents a group critic system for object-oriented analysis and design. A group critic system is a critiquing system which is aware that the problems it finds in the design are the result of different users acting on different goals, and all are responsible for the problem. The environment also integrates a construction kit and an argumentative hypermedia system. We used annotation to point out criticisms, so that users can view the critiquing system as a true colleague. Annotations are also used as the cooperation medium among the designers.
[Unified modeling language, hypermedia, Programming, group critic system, design problems, user goals, cooperative software development, Software design, argumentative hypermedia system, object-oriented design, Fires, construction kit, cooperation medium, groupware, object-oriented methods, criticisms, Informatics, designer responsibilities, Signal design, annotation, Object oriented modeling, design rationale, object-oriented analysis, critiquing system, Software systems, computer aided software engineering, Software engineering]
Computing interfaces in Java
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Investigates the separation between types and classes by putting to use a special feature of Java regarding classes, interfaces and inheritance. We propose an original method which, from a single inheritance class hierarchy, extracts a multiple inheritance interface hierarchy, which contains all the types of the original hierarchy, each class being linked to the interface representing its type. In the resulting structure, interfaces are well-organized and follow a natural multiple specialization, which would not have been possible using only the single inheritance which comes with Java. Our method is based on the use of a Galois lattice, which is a reference for the elaboration of hierarchies. We introduce and justify the need for a new algorithm that efficiently builds an essential part of the Galois lattice.
[Computer interfaces, Java, object-oriented programming, types, hierarchy elaboration, interface computation, multiple inheritance interface hierarchy, Lattices, classes, inheritance, Data mining, Galois fields, Galois lattice, inheritance class hierarchy, multiple specialization]
Extending UML to support domain analysis
Proceedings ASE 2000. Fifteenth IEEE International Conference on Automated Software Engineering
None
2000
Develops a domain engineering approach and the supporting tools to define software product lines. The approach is composed of a process and a notation. The notation is an extension of UML (Unified Modeling Language), while the process is influenced by the Synthesis and FAST domain engineering methods. Supporting tools have been built on top of a commercial CASE tool for UML. This paper focuses on the domain analysis part of the approach and specifically on the notation and tools to support it. The approach is being applied to sub-domains of the guidance, navigation and control domain, which are a set of functionalities used on-board satellites developed by the NASA Goddard Space Flight Center.
[Computer aided software engineering, Costs, Unified modeling language, Time to market, UML extensions, Satellite navigation systems, aerospace control, computerised navigation, navigation, satellite on-board functionalities, Synthesis, notation, specification languages, aerospace computing, supporting tools, software product lines, process, domain engineering, Unified Modeling Language, Natural languages, NASA, commercial CASE tool, control, domain analysis, Application software, artificial satellites, guidance, Software systems, computer aided software engineering, FAST, Software tools]
Implementation of specification conjunction and domain interaction in Rosetta
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Summary form only given. System level design is nowadays a complex process due to heterogeneity of domains of components in a single system. The language standard, Rosetta, was proposed as a means to help in such designs. An important feature in Rosetta is known as domain interaction and involves the analysis of interaction between models from domain to domain. The resulting iteration models can then be used to ensure correctness of the system at the design level. The goal of my doctoral dissertation is therefore to implement the semantics of domain interactions in Rosetta and to apply it in the specification of real world examples.
[Computer interfaces, program verification, Computational modeling, Predictive models, Control systems, specification conjunction, formal specification, Organizing, System-level design, Computer science, system level design, domain interaction, Computer architecture, specification languages, Systems engineering and theory, Modems, Rosetta, program correctness]
Automatic translation from UML specifications to B
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Summary form only given. The research to merge the relatively simple and graphical nature of UML with the firm semantics bases of B has been mentioned several time in the literature. The goal is to propose automatic derivation schemes from UML to B specifications. Then, the construction of UML specifications is rigorously controlled by analyzing derived a specifications thanks to powerful B support tools. Dealing with the modeling UML behavioral diagrams in B, which has not been treated so far, is the main objective of the current work. We emphasize on the translation from use case, interaction and statechart diagrams into B specifications. We formalize each class operation, use case, event as a B operation which is encapsulated in a B abstract machine where are modeled the class data involved by the class operation, use case or event in question. In order to get the derived B specification more structured, we consider the class operation calling-called dependency, the use case structuring and the relation between events and its triggered transition. To complete the derivation schemes, we have proposed three derivation procedures based on class operations, use case and events I In addition, we have envisaged the following verifications on UML specifications: (i) the consistency of the class invariant; (ii) the conformity of object and state-chart diagrams regarding the class diagrams; (iii) the conformity of class operations, use cases regarding the class invariant; (iv) the class operation calling-called dependency and (v) the use case structuring.
[Object oriented modeling, Unified modeling language, B specifications, Application software, formal specification, calling-called dependency, Bridges, program interpreters, Content addressable storage, behavioral diagrams, USA Councils, specification languages, Automatic control, Writing, automatic derivation schemes, automatic translation, Books, UML specifications, Software engineering]
Model-checking real-time concurrent systems
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Summary form only given, as follows. A concurrent real-time system is a system of many components, that should deliver the result in a particular time interval. The design of such a system is generally complex, with high possibility of errors. Thus it is very important to be able to verify the correctness of the design itself, before going on to implementation stage. Model-checking is a powerful approach to design verification which provides techniques for automatic determination of whether a design (model) of the system satisfies desired properties expressed in formal logic. Main problems that model-checking algorithms have to address are: state space of any concurrent system grows exponentially with the number of components of the system - state explosion problem; Addition of time (for modeling real-time systems) means that there are infinitely many concrete states of the system. Both of these mean that model-checking takes a long time and a lot of space. There are a number of approaches to model-checking providing partial solutions to these problems. However a lot of improvement is still desired to make practical model-checking of real systems feasible. Moreover, the more expressive the design technique is, and the more expressive the specification language is, the more complex becomes the problem of model-checking. Current state of the art model-checkers have fairly simple modeling means and specification languages, thus restricting developer in their capabilities. In this project a relatively new approach to model checking is taken - the use of abstract game theory, with the model-checking algorithm being implemented as an abstract game. In this approach reasoning is made over sets of states satisfying some properties, not individual states, thus reducing the size of the state-space to be searched. Also in this project the more expressive models of concurrent real-time systems and the more expressive specification logics are to be brought together to allow checking of complex properties of complex systems. A tangible deliverable will be a model-checking tool that should have a number of advantages over current state of the art model-checkers.
[Real time systems, correctness, program verification, abstract game theory, concurrent real-time system, Automatic logic units, game theory, reasoning, Logic design, Explosions, Specification languages, State-space methods, inference mechanisms, Power system modeling, Game theory, program compilers, formal logic, model checking, implementation stage, state explosion problem, Concrete, specification language]
Automated validation of software models
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The paper describes the application of an automated verification tool to a software model developed at Ford Motor Company. Ford already has in place an advanced model-based software development framework that employs the Matlab(R), Simulink(R), and Stateflow(R) modeling tools. During this project, we applied the invariant checker Salsa to a Simulink(R)/Stateflow(R) model of automotive software to check for nondeterminism, missing cases, dead code, and redundant code. During the analysis, a number of anomalies were detected that had not been found during manual review. We argue that the detection and correction of these problems demonstrates a cost-effective application of formal verification that elevates our level of confidence in the model.
[program verification, Software performance, Programming, automated software model validation, anomaly detection, Simulink, redundant code, formal specification, invariant checker, dead code, Automotive engineering, formal verification, modeling tools, missing cases, Ford Motor Company, Mathematical model, automated verification tool, Biomedical engineering, nondeterminism, automatic programming, advanced model-based software development framework, automobiles, automotive software, Application software, Computer languages, Software packages, Salsa, Computer industry, Stateflow, Software tools, Matlab]
Certifying domain-specific policies
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Proof-checking code for compliance to safety policies potentially enables a product-oriented approach to certain aspects of software certification. To date, previous research has focused on generic, low-level programming-language properties such as memory type safety. In this paper we consider proof-checking higher-level domain-specific properties for compliance to safety policies. The paper first describes a framework related to abstract interpretation in which compliance to a class of certification policies can be efficiently calculated. Membership equational logic is shown to provide a rich logic for carrying out such calculations, including partiality, for certification. The architecture for a domain-specific certifier is described, followed by an implemented case study. The case study considers consistency of abstract variable attributes in code that performs geometric calculations in Aerospace systems.
[Logic programming, program verification, abstract interpretation, NASA, domain-specific certifier, membership equational logic, memory type safety, proof checking code, Software safety, product-oriented approach, abstract variable attributes, Equations, Certification, higher-level domain-specific properties, certification, Computer science, Computer languages, software certification, geometric calculations, domain-specific policies certification, Aerospace safety, Product safety, software standards, Software engineering]
Automated conversion from a requirements document to an executable formal specification
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Many formal specification languages have been developed to engineer complex systems. However natural language (NL) has remained the choice of domain experts to specify the system because formal specification languages are not easy to master. Therefore NL requirements documentation must be reinterpreted by software engineers into a formal specification language. When the system is very complicated, which is mostly the case when one chooses to use formal specification, this conversion is both non-trivial and error-prone, if not implausible. This challenge comes from many factors such as miscommunication between domain experts and engineers. However the major bottleneck of this conversion is from the inborn characteristic of ambiguity of NL and the different level of the formalism between the two domains of NL and the formal specification. This is why there have been very few attempts to automate the conversion from requirements documentation to a formal specification language. This research project is developed as an application of formal specification and linguistic techniques to automate the conversion from a requirements document written in NL to a formal specification language. Contextual Natural Language Processing (CNLP) is used to handle the ambiguity problem in NL and Two Level Grammar (TLG) is used to deal with the different formalism level between NL and formal specification languages to achieve automated conversion from NL requirements documentation into a formal specification (in our case the Vienna Development Method - VDM++). A knowledge base is built from the NL requirements documentation using CNLP by parsing the documentation and storing the syntactic, semantic, and contextual information.
[automated conversion, Java, requirements document, Unified modeling language, Documentation, two level grammar, Formal specifications, High level languages, formal specification, Phase detection, Bridges, formal specification languages, domain experts, executable formal specification, Prototypes, contextual natural language processing, specification languages, Systems engineering and theory, natural languages, Natural language processing, natural language]
Towards a precise definition of the OMG/MDA framework
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We are currently witnessing an important paradigm shift in information system construction, namely the move from object and component technology to model technology. The object technology revolution has allowed the replacement of the over twenty-year-old step-wise procedural decomposition paradigm with the more fashionable object composition paradigm. Surprisingly, this evolution seems to have triggered another even more radical change, the current trend toward model transformation. A concrete example is the Object Management Group's rapid move from its previous Object Management Architecture vision to the latest Model-Driven Architecture. This paper proposes an interpretation of this evolution through abstract investigation. In order to stay as language-independent as possible, we have employed the neutral formalism of Sowa's conceptual graphs to describe the various situations characterizing this organization. This will allow us to identify potential problems in the proposed modeling framework and suggest some possible solutions.
[model technology, object-oriented programming, Object oriented modeling, Unified modeling language, model transformation, OMG/MDA framework, conceptual graphs, Application software, object technology revolution, Information systems, information system construction, systems analysis, Management information systems, Computer architecture, Ear, Rendering (computer graphics), model-driven architecture, Concrete, Logic, paradigm shift]
Specification modeling and validation applied to a family of network security products
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
A high-bandwidth, always-on Internet connection makes computers in homes and small offices attractive targets for network-based attacks. Network security gateways can protect such vulnerable hosts from attackers, but differing sets of customer needs require different feature mixes. The safest way to address this market is to provide a family of products, each member of which requires little or no end-user configuration. Since the products are closely related, the effort to validate n of them should be much less than n times the effort to validate one; however validating the correctness and security of even one such device is notoriously difficult, due to the oft-observed fact that no practical amount of testing can show the absence of security flaws. One would instead like to prove security properties, even when the products are implemented using off-the-shelf technologies that don't lend themselves to formal reasoning. The author describes how the specification modeling and validation tools of the Interactive Specification Acquisition Tools (ISAT) suite are used to help validate members of a particular family of network security gateway products built using widely available open source technologies.
[security flaws, off-the-shelf technologies, program verification, specification modeling, internetworking, feature mixes, network security gateways, security properties, ISAT suite, formal reasoning, formal specification, specification validation, Operating systems, Computer networks, Safety, IP networks, Computer security, customer needs, high-bandwidth always-on Internet connection, Testing, specification modeling tools, network security products, validation tools, correctness, Inspection, network-based attacks, end user configuration, Home computing, security of data, Linux, open source technologies, Internet, programming environments, Interactive Specification Acquisition Tools, Software engineering]
Higher order function synthesis through proof planning
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The close association between higher order functions and algorithmic skeletons is a promising source of automatic parallelisation of programs. An approach to automatically synthesizing higher order functions from functional programs through proof planning is presented Our work has been conducted within the context of a parallelising compiler for SML, with the objective of exploiting parallelism latent in potential higher order function use in programs.
[parallelising compilers, higher order function synthesis, functional programming, functional programs, higher order function, automatic parallelisation of programs, Concurrent computing, Program processors, Parallel programming, parallelising compiler, SML, proof planning, Automatic control, Parallel processing, Skeleton, Functional programming, higher order functions, algorithmic skeletons]
Automatic verification of Java design patterns
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Design patterns are widely used by object oriented designers and developers for building complex systems in object oriented programming languages such as Java. However, systems evolve over time, increasing the chance that the pattern in its original form will be broken. We attempt to show that many design patterns (implemented in Java) can be verified automatically. Patterns are defined in terms of variants, mini-patterns, and artifacts in a pattern description language called SPINE. These specifications are then processed by Hedgehog, an automated proof tool that attempts to prove that Java source code meets these specifications.
[program verification, pattern description language, Formal languages, object oriented programming languages, Production facilities, specifications, mini-patterns, automated proof tool, Runtime, complex systems, object oriented designers, specification languages, theorem proving, Books, Informatics, Java, automatic programming, object-oriented programming, Java source code, Buildings, Java design patterns, Bridges, Computer languages, Hedgehog, SPINE, Memory management, automatic verification]
Providing early feedback in the development cycle through automated application of model checking to software architectures
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The benefits of evaluating properties of software architectures stem from two important software architecture roles: (1) providing an opportunity to evaluate requirements and correct defects prior to implementation; and (2) serving as a blueprint for system developers. The paper focuses on a new software architecture evaluation tool called Architecture Analysis Dynamic Environment (Arcade) that uses model checking to provide software architecture safety and liveness evaluation during the requirements gathering and analysis phase. Model checking requires expertise not typically held by systems analysts and software developers. Thus, two barriers to applying model checking must be addressed: (1) translation of the software architecture specification to a form suitable for model checking, and (2) interpretation of the results of model checking. Arcade provides an automated approach to these barriers, allowing model checking of software architectures to be added to the list of techniques available to software analysts and developers focusing on requirements gathering and analysis.
[program verification, systems analysts, safety-critical software, software developers, software analysts, Software safety, formal specification, automated application, automated approach, Business communication, software architecture, software architecture safety, Software architecture, Feedback, Computer architecture, Architecture Analysis Dynamic Environment, Intelligent systems, software development cycle, software architecture evaluation tool, software architecture specification, requirements analysis phase, Application software, Power system modeling, Arcade, model checking, systems analysis, Software systems, Systems engineering and theory, liveness evaluation, requirements gathering]
Enhancing partial-order reduction via process clustering
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Partial-order reduction is a well-known technique to cope with the state-space-explosion problem in the verification of concurrent systems. Using the hierarchical structure of concurrent systems, we present an enhancement of the partial-order-reduction scheme of G.J. Holzman and D. Peled (1995) and D. Peled (1994). A prototype of the new algorithm has been implemented on top of the verification tool SPIN. The first experimental results are encouraging.
[program verification, SPIN, Unified modeling language, LTL, hierarchical structure, partial-order reduction enhancement, Explosions, State-space methods, Distributed computing, parallel programming, Concurrent computing, state-space-explosion problem, formal verification, model checking, pattern clustering, Clustering algorithms, Prototypes, concurrency control, Interleaved codes, process clustering, concurrent system verification, Safety, partial-order-reduction scheme]
The synthesis of a Java card tokenisation algorithm
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We describe the development of a Java bytecode optimisation algorithm by the methodology of program extraction. We develop the algorithm as a collection of proofs and definitions in the Coq proof assistant, and then use Coq's extraction mechanism to automatically generate a program in OCaml. The extraction methodology guarantees that this program is correct. We discuss the feasibility of the methodology and suggest some improvements that could be made.
[Java, Smart cards, automatic programming, Automation, optimising compilers, Java byte code optimisation algorithm, extraction methodology, Coq proof assistant, Calculus, program extraction, Constraint optimization, extraction mechanism, Java card tokenisation algorithm synthesis, Computer bugs, OCaml, proofs, Computer industry, theorem proving, Functional programming, Informatics, Formal verification, program correctness]
Identification of high-level concept clones in source code
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Source code duplication occurs frequently within large software systems. Pieces of source code, functions, and data types are often duplicated in part or in whole, for a variety of reasons. Programmers may simply be reusing a piece of code via copy and paste or they may be "re-inventing the wheel". Previous research on the detection of clones is mainly focused on identifying pieces of code with similar (or nearly similar) structure. Our approach is to examine the source code text (comments and identifiers) and identify implementations of similar high-level concepts (e.g., abstract data types). The approach uses an information retrieval technique (i.e., latent semantic indexing) to statically analyze the software system and determine semantic similarities between source code documents (i.e., functions, files, or code segments). These similarity measures are used to drive the clone detection process. The intention of our approach is to enhance and augment existing clone detection methods that are based on structural analysis. This synergistic use of methods will improve the quality of clone detection. A set of experiments is presented that demonstrate the usage of semantic similarity measure to identify clones within a version of NCSA Mosaic.
[latent semantic indexing, computational linguistics, Wheels, large software systems, source code reuse, Information analysis, high-level concepts, information retrieval technique, structural analysis, semantic similarities, data types, source code duplication, clone detection, program diagnostics, Cloning, abstract data types, NCSA Mosaic, Documentation, information retrieval, static analysis, Information retrieval, semantic similarity. measure, source code documents, source code text, Programming profession, Computer science, similarity measures, synergistic methods, high-level concept clone identification, software reusability, Software systems, Indexing]
Combining static analysis and model checking for software analysis
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We present an iterative technique in which model checking and static analysis are combined to verify large software systems. The role of the static analysis is to compute partial order information which the model checker uses to reduce the state space. During exploration, the model checker also computes aliasing information that it gives to the static analyzer which can then refine its analysis. The result of this refined analysis is then fed back to the model checker which updates its partial order reduction. At each step of this iterative process, the static analysis computes optimistic information which results in an unsafe reduction of the state space. However, we show that the process converges to a fixed point at which time the partial order information is safe and the whole state space is explored.
[Algorithm design and analysis, Java, program testing, program verification, software verification, aliasing information, iterative technique, static analysis, Explosions, State-space methods, Software safety, Aerospace industry, Information analysis, partial order information, formal verification, model checking, software analysis, Software systems, Computer industry, Space exploration]
Acceptance based assurance
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Assurance of software systems has traditionally been sought through the rigour of the development process. The higher the assurance, the more demanding the development process; the highest assurance requiring the use of formal methods during development. This approach has been followed for decades with some success, but increased assurance brings a disproportionate increase in cost and risk. A change in emphasis is suggested from the development of a system to its acceptance. The benefits for high assurance systems are illustrated through a case study and preliminary experience of high assurance techniques are reported.
[Costs, program verification, software systems assurance, Government, Project management, safety-critical software, Programming, Boilers, Software safety, software quality, Simulink, Application software, Management training, high assurance systems, development process, case study, acceptance based software assurance, formal methods, Software systems, Z, safety critical software, Standards development]
Test purposes: adapting the notion of specification to testing
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Nowadays, test cases may correspond to elaborate programs. It is therefore sensible to try to specify test cases in order to get a more abstract view of these. This paper explores the notion of test purpose as a way to specify a set of test cases. It shows how test purposes are exploited today by several tools that automate the generation of test cases. It presents the major relations that link test purposes, test cases and reference specification. It also explores the similarities and differences between the specification of test cases, and the specification of programs. This opens perspectives for the synthesis and the verification of test cases, and for other activities like test case retrieval.
[Performance evaluation, System testing, Automation, reference specification, program testing, specification to testing, specification of programs, Programming, test cases, test case retrieval, test purposes, Application software, Logic testing, formal specification, Software libraries, Automatic testing, Collaboration, Robots]
Program execution based module cohesion measurement
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Module cohesion describes the degree to which different actions performed by a module contribute towards a unified function. High module cohesion is a desirable property of a program. The program modifications during successive maintenance interventions can have negative effect on the structure of the program resulting in less cohesive modules. Therefore, metrics that measure module cohesion are important for software restructuring during maintenance. The existing static slice based module cohesion metrics significantly overestimate cohesion due to the limitations of static slicing. In this paper, we present a novel program execution based approach to measure module cohesion of legacy software. We define cohesion metrics based on definition-use pairs in the dynamic slices of the outputs. Our approach significantly improves the accuracy of cohesion measurement. We implemented our technique and measured module cohesion for several programs. Cohesion measurements using our technique were found to be more insightful than static slice based measurements.
[Software maintenance, Production systems, System testing, dynamic slices, static slicing, program execution based module cohesion measurement, History, software maintenance, cohesion measurements, legacy software, Computer science, Software metrics, Operating systems, maintenance interventions, Computer errors, Software measurement, software restructuring, program slicing, software metrics]
Instantiating and detecting design patterns: putting bits and pieces together
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Design patterns ease the designing, understanding, and re-engineering of software. Achieving a well-designed piece of software requires a deep understanding and a good practice of design patterns. Understanding existing software relies on the ability to identify architectural forms resulting from the implementation of design patterns. Maintaining software involves spotting places that can be improved by using better design decisions, like those advocated by design patterns. Nevertheless, there is a lack of tools automatizing the use of design patterns to achieve well-designed pieces of software, to identify recurrent architectural forms, and to maintain software. We present a set of tools and techniques to help OO software practitioners design, understand, and re-engineer a piece of software using design-patterns. A first prototype tool, PATTERNS-BOX, provides assistance in designing the architecture of a new piece of software, while a second prototype tool, PTIDEJ, identifies design patterns used in an existing one. These tools, in combination, support maintenance by highlighting defects in an existing design, and by suggesting and applying corrections based on widely-accepted design pattern solutions.
[Software maintenance, Software prototyping, OO software practitioners, PTIDEJ, object-oriented programming, design pattern solutions, Documentation, design pattern instantiation, reverse engineering, Application software, software maintenance, design decisions, systems re-engineering, recurrent architectural forms, Software design, Software libraries, PATTERNS-BOX, software understanding, Prototypes, Computer architecture, software reusability, software re-engineering, software tools, Software tools]
Modeling and verification of distributed real-time systems based on CafeOBJ
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
CafeOBJ is a wide spectrum formal specification language based on multiple logical foundations: mainly initial and hidden algebra. A wide range of systems can be specified in CafeOBJ thanks to its multiple logical foundations. However, distributed real-time systems happen to be excluded from targets of CafeOBJ. The authors propose a method of modeling and verifying such systems based on CafeOBJ, together with timed evolution of UNITY computational models.
[Real time systems, Protocols, program verification, distributed real-time systems, Computational modeling, hidden algebra, timed evolution, Formal specifications, Distributed computing, distributed real-time systems modeling, CafeOBJ, Aerospace control, UNITY computational models, multiple logical foundations, Information science, Patient monitoring, Algebra, distributed algorithms, real-time systems, specification languages, wide spectrum formal specification language, Timing, initial algebra, algebraic specification]
Tailoring a COTS group support system for software requirements inspection
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The inspection of early life-cycle artifacts such as requirement documents promises great benefits. However, research demonstrates that the inspection process is complex and expensive and that tool support would be highly desirable. Existing inspection tools focus largely on the inspection of source code. We have therefore devised groupware support for inspecting requirements. Based on our experience with adopting a group support system (GSS) for requirements negotiation, we decided to tailor this commercial GSS to support inspection of requirements. The paper discusses our concept of a Groupware-supported Requirements Inspection Process (GRIP) and shows that tailoring a COTS GSS works well to automate this process.
[Performance evaluation, inspection process, COTS GSS, Costs, program verification, groupware support, commercial GSS, group support system, Yarn, formal specification, early life-cycle artifact inspection, groupware, software requirements inspection, requirement documents, software tools, inspection tools, inspection, Automation, Collaborative software, Inspection, COTS group support system, tool support, computer-supported cooperative work, Groupware supported Requirements Inspection Process, source code inspection, GRIP, Software systems, Collaborative work, Software tools, requirements negotiation, Software engineering]
Automated software engineering using concurrent class machines
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Concurrent Class Machines are a novel state-machine model that directly captures a variety of object-oriented concepts, including classes and inheritance, objects and object creation, methods, method invocation and exceptions, multithreading and abstract collection types. The model can be understood as a precise definition of UML activity diagrams which, at the same time, offers an executable, object-oriented alternative to event-based statecharts. It can also be understood as a visual, combined control and data flow model for multithreaded object-oriented programs. We first introduce a visual notation and tool for Concurrent Class Machines and discuss their benefits in enhancing system design. We then equip this notation with a precise semantics that allows us to define refinement and modular refinement rules. Finally, we summarize our work on generation of optimized code, implementation and experiments, and compare with related work.
[Design automation, Medical simulation, Unified modeling language, exception handling, classes, object creation, object-oriented concepts, inheritance, semantics, automated software engineering, visual notation, Embedded software, Automotive engineering, abstract collection types, multithreaded object-oriented programs, specification languages, Aerospace testing, software engineering, concurrent class machines, multithreading, optimized code, event-based statecharts, object-oriented programming, multi-threading, Object oriented modeling, UML activity diagrams, system design, state-machine model, Power system modeling, exceptions, data flow model, Multithreading, method invocation, Software engineering]
Semi-automated verification of Erlang code
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Erlang is a functional programming language with support for concurrency and message passing communication that is used at Ericsson for developing telecommunication applications. We consider the challenge of verifying temporal properties of systems programmed in Erlang with dynamically evolving process structures. To accomplish this, a rich verification framework for goal-directed, proof system-based verification is used. The paper investigates the problem of semi-automating the verification task by identifying the proof parameters crucial for successful proof search.
[Software testing, program verification, dynamically evolving process structures, semi-automated verification task, proof search, telecommunication computing, parallel programming, Concurrent computing, Reactive power, telecommunication applications, theorem proving, Functional programming, automatic programming, message passing, semi-automated Erlang code verification, goal-directed proof system based verification, Ericsson, Application software, Software debugging, concurrency, Computer science, proof parameters, Computer languages, Message passing, verification framework, functional programming language, temporal properties, message passing communication, Pattern matching, functional languages]
A technique for mutation of Java objects
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Mutation analysis inserts faults into a program to create test sets that distinguish the mutant from the original program. Inserted faults must represent plausible errors. Standard transformations can mutate scalar values such as integers, floats, and character data. Mutating objects is an open problem, because object semantics are defined by the programmer and can vary widely. We develop mutation operators and support tools that can mutate Java library items that are heavily used in commercial software. Our mutation engine can support reusable libraries of mutation components to inject faults into objects that instantiate items from these common Java libraries. Our technique should be effective for evaluating real-world software testing suites.
[plausible errors, program testing, standard transformations, mutation components, Genetic mutations, scalar values, character data, software libraries, reusable libraries, Runtime, mutation operators, mutation analysis, Java library items, Testing, fault insertion, Java, object semantics, Java object mutation, Object oriented modeling, real-world software testing suites, Programming profession, Computer science, Software libraries, Computer errors, software reusability, commercial software, support tools, common Java libraries, Software tools]
Adequate reverse engineering
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Reverse engineering a program constructs a high-level representation suitable for various software development purposes such as documentation or reengineering. Unfortunately however, there are no established guidelines to assess the adequacy of such a representation. We propose two such criteria, completeness and accuracy, and show how they can be determined during the course of reversing the representation. A representation is successfully reversed when it is given as input to a suitable code generator, and a program equivalent to the original is produced. To explore this idea, we reverse engineer a small but complex numerical application, represent our understanding using algebraic specifications, and then use a code generator to produce code from the specification. We discuss the strengths and weaknesses of the approach as well as alternative approaches to reverse engineering adequacy.
[Reverse engineering, algebraic specifications, program compilers, Guidelines, program reverse engineering, high-level representation, Engineering management, numerical analysis, adequate reverse engineering, Polynomials, code generator, algebraic specification, Testing, software development, complex numerical application, Power system management, Documentation, documentation, reengineering, Educational institutions, reverse engineering, Computer science, systems re-engineering, Software systems, representation reversal]
A concurrency test tool for Java monitors
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The Java programming language supports monitors. Monitor implementations, like other concurrent programs, are hard to test due to the inherent non-determinism. This paper presents the ConAn (Concurrency Analyser) tool for generating drivers for the testing of Java monitors. To obtain adequate controllability over the interactions between Java threads, the generated driver contains processes that are synchronized by a clock. The driver automatically executes the calls in the test sequence in the prescribed order and compares the outputs against the expected outputs specified in the test sequence. The method and tool are illustrated on an asymmetric producer-consumer monitor and their application to two other monitors is discussed.
[ConAn tool, Java, Java programming language, Synchronization, concurrent programs, Yarn, concurrency test tool, asymmetric producer-consumer monitor, Concurrent computing, Computer science, Computer languages, Computer displays, Automatic testing, Java monitors, concurrency control, Controllability, system monitoring, software tools, Java threads, Clocks]
An automated tool for analyzing Petri nets using Spin
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The Spin model checker is a system that has been used to model and analyze a large number of applications in several domains including the aerospace industry. One of the novelties of Spin is its relatively simple specification language, Promela, as well as the powerful abilities of the model checker. The Petri net notation is a mathematical tool for modeling various classes of systems, especially those that involve concurrency and parallelism. The Honeywell Domain Modeling Environment (DOME) is a tool that supports system design using a wide variety of modeling notations, including UML diagrams and Petri nets. We describe a tool that supports the use of the Spin model checker to analyze and verify Petri net specifications that have been constructed using the DOME tool. In addition to discussing the translation of Petri nets into Promela, we present several example Petri net specifications as well as their analysis using Spin.
[program verification, DOME, Petri nets, Unified modeling language, parallelism, UML diagrams, automated tool, Electronic mail, formal specification, Aerospace industry, parallel programming, Concurrent computing, Promela, Spin model checker, Petri net specifications, specification languages, mathematical tool, Mathematical model, automatic programming, system design, Specification languages, Application software, Power system modeling, concurrency, Computer science, Petri net notation, Honeywell Domain Modeling Environment, simple specification language, modeling notations, Petri net analysis]
Scalable consistency checking between diagrams - the VIEWINTEGRA approach
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The Unified Modeling Language (UML) supports a wide range of diagrams for modeling software development concerns. UML diagrams are independent but connected; their meta-model describes them under a common roof. Despite the advances of UML, we found that the problem of ensuring consistency between UML diagrams has not been solved. We have developed an approach for automated consistency checking, called VIEWINTEGRA.. Our approach provides excellent support for active (preventive) and passive (detective) consistency checking. We make use of consistent transformation to translate diagrams into interpretations and we use consistency comparison to compare those interpretations to other diagrams. Our approach was applied to a number of applications where we found the separation of transformation and comparison to be highly beneficial in addressing consistency-checking scalability and usability issues. The paper introduces our UML-based transformation framework, discusses how it aids comparison, and demonstrates how it improves consistency checking.
[software development concerns, UML-based transformation framework, Unified Modeling Language, Scalability, program diagnostics, Unified modeling language, Reverse engineering, VIEWINTEGRA, automated consistency checking, UML diagrams, Programming, data integrity, active consistency checking, consistency comparison, consistent transformation, passive consistency checking, consistency-checking scalability, specification languages, Software systems, Collaborative work, usability issues, Usability]
Developing generative frameworks using XML
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Generative programming methods provide some significant advantages for the repeated deployment of product line architectures. The paper considers XML as a tool for building and describing applications that use generative programming methods. It describes techniques for the creation of a generative framework, presents a case study and discusses the results of practical application of these methods in a real world, enterprise scale, product line architecture. The paper presents the advantages of using an XML descriptor that can be easily transformed to generate both static and dynamically configurable software components for direct deployment in an application framework. Two implementation approaches are considered: an indirect approach using XSL for the transformations; and a direct approach where the XML descriptor is parsed and dealt with programmatically. The relative advantages of these two approaches are discussed. The paper provides practical examples and presents lessons learned from the application of the techniques.
[practical application, formal specification, program compilers, real world enterprise scale product line architecture, Databases, generative programming methods, Computer architecture, indirect approach, Logic, product line architectures, dynamically configurable software components, direct deployment, implementation approaches, hypermedia markup languages, Buildings, Process control, XML descriptor, generative framework development, Application software, Formal specifications, case study, XSL, Job production systems, XML, application generators, application framework, Software engineering]
Generating EDI message translations from visual specifications
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Electronic data interchange (EDI) systems are used in many domains to support inter-organisational information exchange. To get systems using different EDI message formats to communicate, complex message translations (where data must be transformed from one EDI message format into another), are required. We describe a visual language and support environment which greatly simplify the task of the systems integrator by using a domain-specific visual language to express data formats and format translations. Complex message translations are automated by an underlying transformation engine. We describe the motivation for this system, its key visual language and transformation engine features, a prototype environment, and experience translating it into a commercial product.
[commercial product, Protocols, Costs, key visual language, complex message translations, Data handling, electronic messaging, EDI message formats, visual languages, formal specification, Engines, EDI message translation generation, EDI systems, Message-oriented middleware, visual language, Prototypes, domain-specific visual language, systems integrator, hypermedia markup languages, data formats, transformation engine, Medical treatment, Biomedical informatics, inter-organisational information exchange, prototype environment, Computer science, electronic data interchange, electronic data interchange systems, XML, support environment, format translations, visual specifications, transformation engine features]
Context-aware browsing of large component repositories
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The paper proposes a novel approach to locating software components from a large component repository: context-aware browsing. Without any explicit input from software developers, this approach automatically locates and presents a list of software components that could possibly be used in the current development situation. This automation of the component location process not only greatly reduces the search space of components so that software developers can easily browse and choose the desired components, but also enables software developers to use components whose existence they do not even anticipate. A software agent that supports context-aware browsing has been developed and evaluated.
[Productivity, Java, automatic programming, context-aware browsing, Automation, object-oriented programming, development situation, Laboratories, software agent, information retrieval, Programming, large component repositories, software developers, Application software, software agents, component location process, software libraries, Software libraries, Software packages, Packaging, Software agents, search space, software components]
Automated test-data generation from formal models of software
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Verification and Validation (V&V) of software for critical embedded control systems often consumes upto 70% of the development resources. Testing is one of the most frequently used V&V technique for verifying such systems. Many regulatory agencies that certify control systems for use require that the software be tested to certain specified levels of coverage. Currently, developing test cases to meet these requirements takes a major portion of the resources. Automating this task would result in significant time and cost savings. The objective of this paper is to automate the generation of such test cases. We propose an approach where we rely on a formal model of the required software behavior for test-case generation, as well as, an oracle to determine if the implementation produced the correct output during testing.
[Software testing, System testing, Costs, formal models of software, program verification, program testing, software verification, Control systems, State-space methods, formal specification, Embedded software, Computer science, formal model, Automatic testing, software validation, Tail, Automatic control, test-case generation, automated test data generation, critical embedded control systems]
Shared variables interaction diagrams
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Scenario-based specifications offer an intuitive and visual way of describing design requirements of distributed software systems. For the communication paradigm based on messages, message sequence charts (MSC) offer a standardized and formal notation amenable to formal analysis. In this paper we define shared variables interaction diagrams (SVID) as the counterpart of MSCs when processes communicate via shared variables. After formally defining SVIDs, we develop an intuitive as well as formal definition of refinement for SVIDs. This notion provides a basis for systematically adding details to SVID requirements.
[Algorithm design and analysis, message sequence charts, Unified modeling language, shared variables interaction diagrams, distributed software systems, formal notation, Distributed computing, Information science, Software design, design requirements, Layout, scenario-based specifications, Software systems, formal analysis, software engineering, communication paradigm, Timing, Pattern analysis, Pattern matching]
AGATE, access graph based tools for handling encapsulation
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Encapsulation and modularity are supported by various static access control mechanisms that manage implementation hiding and define interfaces adapted to different client profiles. Programming languages use numerous and very different mechanisms, the cumulative application of which is sometimes confusing and hard to predict. Furthermore, understanding and reasoning about access control independently from the programming languages is quite difficult. Tools based on a language-independent model of access control are presented to address these issues. These tools support access control handling via visualisation of access, checking of design requirements on access and source code generation. We believe in the contribution of such tools for improving understanding and enhancing use of access control from design to implementation.
[Encapsulation, Access control, Visualization, client profiles, Unified modeling language, reasoning, software quality, modularity, design requirements, data visualisation, Computer architecture, specification languages, source code generation, Protection, Java, static access control mechanisms, Maintenance, inference mechanisms, access graph based tools, Computer languages, language-independent model, Software quality, AGATE, encapsulation handling, visualisation]
Tracing execution of software for design coverage
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Test suites are designed to validate the operation of a system against requirements. One important aspect of a test suite design is to ensure that system operation logic is tested completely. This is a difficult task. Code coverage tools support test suite designers by providing the information about which parts of source code are covered during system execution. Unfortunately, code coverage tools produce only source code coverage information. For a test engineer it is often hard to understand what the noncovered parts of the source code do and how they relate to requirements. We propose a generic approach that provides design coverage of the executed software, simplifying the development of new test suites. We demonstrate our approach on common design abstractions such as statecharts and structure diagrams. We implement the design coverage using tracing and a trace analysis framework. Using design coverage, test suites could be created faster by focussing on untested design elements.
[statecharts, System testing, Visualization, program testing, Formal languages, test suite design, Logic testing, formal logic, untested design elements, Software design, generic approach, trace analysis framework, system operation logic testing, test suite designers, code coverage tools, design coverage, Instruments, program diagnostics, Logic design, Programming profession, Radio access networks, common design abstractions, Automatic testing, test engineer, structure diagrams, source code coverage information, system execution, software execution tracing]
Automated check of architectural models consistency using SPIN
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
In recent years the necessity for handling different aspects of the system separately has introduced the need to represent SA (software architectures) from different viewpoints. In particular, behavioral views are recognized to be one of the most attractive features in the SA description, and in practical contexts, state diagrams and scenarios are the most widely used tools to model this view. Although very expressive, this approach has two drawbacks: system specification incompleteness and view consistency. Our work can be put in this context with the aim of managing incompleteness and checking view conformance: we propose the use of state diagrams and scenario models for representing system dynamics at the architectural level; they can be incomplete and we want to prove that they describe, from different viewpoints, the same system behavior. To reach this goal, we use the SPIN model checker and we implement a tool to manage the translation of architectural models in Promela and LTL.
[Software maintenance, program verification, LTL, Time to market, software architectures, formal specification, view conformance, SPIN model checker, Promela, software architecture, Software architecture, architectural level, automatic programming, Power system management, data integrity, state diagrams, system specification incompleteness, Power system modeling, Synthetic aperture sonar, behavioral views, architectural models, view consistency, system dynamics, Software quality, architectural model consistency, Software systems, automated check, Software tools, SA description, Context modeling]
Exploiting heap symmetries in explicit-state model checking of software
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Detecting symmetries in the structure of systems is a well known technique falling in the class of bisimulation (strongly) preserving state space reductions. Previous work in applying symmetries to aid model checking focuses mainly on process topologies and user specified data types. We applied the symmetry framework to model checking object-based programs that manipulate dynamically created objects, and developed a linear-time heuristic for finding the canonical representative of a symmetry equivalence class. The strategy was implemented in the object-based model checker dSPIN and some experiments, yielding encouraging results, have been carried out.
[bisimulation, Protocols, program verification, Automatic logic units, heap symmetries, Yarn, dSPIN, canonical representative, Runtime, state space reductions, user specified data types, object-based model checker, software checking, Hardware, data structures, bisimulation equivalence, explicit-state model checking, Java, object-oriented programming, linear-time heuristic, symmetry equivalence class, object-based programs, Explosions, State-space methods, Topology, process topologies, model checking, symmetry framework, Interleaved codes, equivalence classes, dynamically created objects]
Wins and losses of algebraic transformations of software architectures
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
In order to understand, analyze and modify software, we commonly examine and manipulate its architecture. For example, we may want to examine the architecture at different levels of abstraction. We can view such manipulations as architectural transformations, and more specifically, as graph transformations. We evaluate relational algebra as a way of specifying and automating the architectural transformations. Specifically, we examine Grok, a relational calculator that is part of the PBS toolkit. We show that relational algebra is practical in that we are able to specify many of the transformations commonly occurring during software maintenance and, using a tool like Grok, we are able to manipulate, quite efficiently, large software graphs; this is a "win". However, this approach is not well suited to express some types of transforms involving patterns of edges and nodes; this is a "loss". By means of a set of examples, the paper makes clear when the approach wins and when it loses.
[Software maintenance, graph transformations, relational algebra, Reverse engineering, graph theory, Grok, software architectures, Data mining, software maintenance, formal specification, Information analysis, Computer science, Information science, software architecture, graph transformation, Algebra, large software graphs, algebraic transformations, Computer architecture, relational calculator, PBS toolkit, Software systems, Software tools, architectural transformations]
Static consistency checking for distributed specifications
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Software engineers building a complex system make use of a number of informal and semi-formal notations. We describe a framework, xlinkit, for managing the consistency of development artifacts expressed in such notations. xlinkit supports distributed software engineering by providing a distribution-transparent language for expressing constraints between specifications. It specifies a semantics for those constraints that permits the generation of hyperlinks between inconsistent elements. We give a formal semantics for link generation, and show how we expressed the rules of the UML foundation/core modules in our language. We outline how we implemented xlinkit as a light-weight web service using open standard technology and present the results of an evaluation against several sizeable UML models provided by industrial partners.
[distribution-transparent language, program testing, Unified modeling language, open standard technology, xlinkit framework, formal specification, web service, formal semantics, Distributed Specifications, Engineering management, Web and internet services, specification languages, static consistency checking, distributed software engineering, development artifacts, Educational institutions, Computer science, Bridges, Web services, XML, UML foundation/core modules, Joining processes, programming environments, hyperlinks, Software engineering]
Monitoring programs using rewriting
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We present a rewriting algorithm for efficiently testing future time Linear Temporal Logic (LTL) formulae on finite execution traces. The standard models of LTL are infinite traces, reflecting the behavior of reactive and concurrent systems which conceptually may be continuously alive. In most past applications of LTL, theorem provers and model checkers have been used to formally prove that down-scaled models satisfy such LTL specifications. Our goal is instead to use LTL for up-scaled testing of real software applications, corresponding to analyzing the conformance of finite traces against LTL formulae. We first describe what it means for a finite trace to satisfy an LTL formula and then suggest an optimized algorithm based on transforming LTL formulae. We use the Maude rewriting logic, which turns out to be a good notation and being supported by an efficient rewriting engine for performing these experiments. The work constitutes part of the Java PathExplorer (JPAX) project, the purpose of which is to develop a flexible tool for monitoring Java program executions.
[Software testing, Heuristic algorithms, temporal logic, Java PathExplorer project, Logic testing, Engines, rewriting, theorem provers, Maude rewriting logic, theorem proving, standard models, infinite traces, Java, rewriting systems, Computerized monitoring, NASA, Data structures, optimized algorithm, Application software, Computer science, programs monitoring, software applications, finite execution traces, down-scaled models, linear temporal logic formulae, model checkers, system monitoring, Java program executions]
Composition and refinement of behavioral specifications
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
This paper presents a mechanizable framework for specifying, developing, and reasoning about complex systems. The framework combines features from algebraic specifications, abstract state machines, and refinement calculus, all couched in a categorical setting. In particular, we show how to extend algebraic specifications to evolving specifications (especs) in such a way that composition and refinement operations extend to capture the dynamics of evolving, adaptive, and self-adaptive software development, while remaining efficiently computable. The framework is partially implemented in the Epoxi system.
[Algorithm design and analysis, Epoxi system, finite automata, behavioral specifications, refinement calculus, reasoning, Programming, Reliability engineering, Data structures, refinement operations, Calculus, mechanizable framework, algebraic specifications, inference mechanisms, formal specification, abstract state machines, Design optimization, Connectors, Design engineering, Algebra, Refining, complex systems specification, self-adaptive software development]
Connectors synthesis for deadlock-free component based architectures
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Nowadays component-based technologies offer straightforward ways of building applications from existing components. Although these technologies might differ in terms of the level of heterogeneity among components they support, e.g. CORBA or COM versus J2EE, they all suffer the problem of dynamic integration. That is, once components are successfully integrated in a uniform context how is it possible to check, control and assess that the dynamic behavior of the resulting application will not deadlock? The authors propose an architectural, connector-based approach to this problem. We compose a system in such a way that it is possible to check whether and why the system deadlocks. Depending on the kind of deadlock, we have a strategy that automatically operates on the connector part of the system architecture in order to obtain a suitably equivalent version of the system which is deadlock-free.
[COM, object-oriented programming, connector synthesis, Buildings, dynamic behavior, dynamic integration, architectural connector-based approach, J2EE, automatic operation, Reachability analysis, deadlock free, Phase detection, heterogeneity, Connectors, CORBA, software architecture, Fault detection, deadlock-free component based architectures, component-based technologies, concurrency control, system architecture, System recovery, Assembly, distributed object management]
Generation of distributed system test-beds from high-level software architecture descriptions
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Most distributed system specifications have performance benchmark requirements. However, determining the likely performance of complex distributed system architectures during development is very challenging. We describe a system where software architects sketch an outline of their proposed system architecture at a high level of abstraction, including indicating client requests, server services, and choosing particular kinds of middleware and database technologies. A fully working implementation of this system is then automatically generated, allowing multiple clients and servers to be run. Performance tests are then automatically run for this generated code and results are displayed back in the original high-level architectural diagrams. Architects may change performance parameters and architecture characteristics, comparing multiple test run results to determine the most suitable abstractions to refine to detailed designs for actual system implementation. We demonstrate the utility of this approach and the accuracy of our generated performance test-beds for validating architectural choices during early system development.
[Software testing, System testing, distributed system specifications, early system development, database technologies, program compilers, formal specification, generated code, software architecture, client requests, multiple clients, Software architecture, Databases, distributed system test-bed generation, Computer architecture, architectural choices, performance benchmark requirements, Virtual prototyping, Monitoring, distributed programming, middleware, client-server systems, multiple servers, multiple test run results, server services, high-level architectural diagrams, architecture characteristics, performance parameters, Middleware, software architects, high-level software architecture descriptions, Automatic testing, XML, system architecture, complex distributed system architectures]
Automatically restructuring programs for the Web
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The construction of interactive server-side Web applications differs substantially from the construction of traditional interactive programs. In contrast, existing Web programming paradigms force programmers to save and restore control state between user interactions. We present an automated transformation that converts traditional interactive programs into standard CGI programs. This enables reuse of existing software development methodologies. Furthermore, an adaptation of existing programming environments supports the development of Web programs.
[Web programs, Web programming paradigms, Displays, user interfaces, traditional interactive programs, Databases, Automatic control, interactive systems, interactive server-side Web applications, user interactions, automatic program restructuring, information resources, Java, automatic programming, automated transformation, standard CGI programs, Programming profession, control state, software development methodology reuse, Computer languages, Web pages, Writing, software reusability, Force control, programming environments, Software engineering]
Enforcing business policies through automated reconfiguration
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
In this paper, we address dynamic reconfiguration from the point of view of the enforcement of the policies that organisations wish to see imposed through the way information systems support business. We address the process of evolution by proposing a primitive-coordination context-for modelling the circumstances in which reconfiguration can and should take place. The idea is for business policies to emerge as properties of process executions when controlled through the coordination contexts that will have been defined for supporting business activities.
[process of evolution, software prototyping, Unified modeling language, Process control, Electronic switching systems, management information systems, Connectors, Business communication, Software architecture, process executions, business policies enforcement, dynamic reconfiguration, information systems, Informatics, Contracts, automated reconfiguration, Context modeling, Software engineering]
Formally testing fail-safety of electronic purse protocols
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Designing and implementing security-critical systems correctly is difficult. In practice, most vulnerabilities arise from bugs in implementations. We present work towards systematic specification-based testing of security-critical systems using the CASE tool AutoFocus. Cryptographic systems are formally specified with state transition diagrams, a notation for state machines in the AutoFocus system., We show how to systematically generate test sequences for security properties based on the model that can be used to test the implementation for vulnerabilities. In particular we focus on the principle of fail-safety. We explain our method at the example of a part of the Common Electronic Purse Specifications (CEPS). Most commonly, attacks address vulnerabilities in the way security mechanisms are used, rather than the mechanisms themselves. Being able to treat security aspects with a general CASE tool within the context of system development enables detection of such vulnerabilities.
[EFTS, specification-based testing, cryptographic systems, System testing, Computer aided software engineering, electronic purse protocols, Laboratories, Natural languages, Electronic equipment testing, Access protocols, cryptography, Security, conformance testing, common electronic purse specifications, state machines, state transition diagrams, Computer bugs, CASE tool, security-critical systems, Cryptography, protocols, Protection, AutoFocus]
Better reasoning about software engineering activities
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Software management oracles often contain numerous subjective features. At each subjective point, a range of behaviors is possible. Stochastic simulation samples a subset of the possible behaviors. After many such stochastic simulations, the TAR2 treatment learner can find control actions that have (usually) the same impact despite the subjectivity of the oracle.
[Costs, project management, subjective features, subjectivity, software engineering activity reasoning, TAR2 treatment learner, Project management, Stochastic processes, software development management, stochastic simulation, inference mechanisms, software project, Programming profession, Computer science, Histograms, subjective point, software management oracles, knowledge based systems, software tools, Resource management, control actions, Software engineering]
A tool for lazy verification of security protocols
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We present the lazy strategy implemented in a compiler of cryptographic protocols, Casrul. The purpose of this compiler is to verify protocols and to translate them into rewrite rules that can be used by several kinds of automatic or semi-automatic tools for finding flaws, or proving properties. It is entirely automatic, and the efficiency of the generated rules is guaranteed because of the use of a lazy model of intruder behavior. This efficiency is illustrated on several examples.
[rewriting systems, automatic programming, protocol verification, cryptographic protocols, lazy model, cryptography, Electronic mail, Societies, Intruder behavior, Cryptographic protocols, security protocols, generated rules, Program processors, formal verification, automatic tools, Casrul, lazy strategy, rewrite rules, Logic, protocols, compiler, lazy verification, FETs, Testing]
Modeling class operations in B: Application to UML behavioral diagrams
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
An appropriate approach for translating UML to B formal specifications allows one to use UML and B jointly in a unified, practical and rigorous software development. We formally analyze UML specifications via their corresponding B formal specifications. This point is significant because B support tools like AtelierB are available. We can also use UML specifications as a tool for building B specifications, so the development of B specifications become easier In this paper we address the problem of automatic derivation from UML behavioral diagrams into B specifications, which has been so far an open issue. A new approach for modeling class operations in B is presented Each class operation is mapped into a B operation. A class operation and its involved data are mapped into the same B abstract machine (BAM). The class operation calling-called dependency is used to arrange derived B operations into BAMs. For each calling-called pair of class operations, the B operation of the called operation participates in the implementation of the B operation of the calling operation.
[class operations modelling, finite automata, Object oriented modeling, Unified modeling language, automatic derivation, Proposals, Magnesium compounds, formal specification, program compilers, calling-called dependency, B formal specifications, B support tools, UML behavioral diagrams, specification languages, B abstract machine, Software systems, Software standards, Libraries, Large-scale systems, UML specifications, rigorous software development, AtelierB]
A UML validation toolset based on abstract state machines
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The Unified Modeling Language has become widely accepted as a standard in software development. Several tools have been produced to support UML model validation. These tools translate a UML model into a validation language such as PROMELA. However they have some shortcomings: there is no proof of correctness (with respect to the UML semantics) for these tools; and there is no tool that supports validation for both the static and dynamic aspects of a UML model. In order to overcome these shortcomings, we present a toolset which is based on the semantic model using abstract state machines. Since the toolset is derived from the semantic model, the toolset is correct with respect to the semantic model. In addition, this toolset can be used to validate both the static and dynamic aspects of a model.
[object-oriented programming, finite automata, program verification, Unified Modeling Language, Object oriented modeling, Unified modeling language, UML validation toolset, PROMELA, Programming, Application software, abstract state machines, semantic model, Computer science, model validation, XML, software development standard, specification languages, Software standards, Safety, Standards development, validation language]
Model checking for an executable subset of UML
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The paper presents an approach to model checking software system designs specified in xUML (http://www.kc.com/html/xuml.html), an executable subset of UML. This approach is enabled by the execution semantics of xUML and is based on automatic translation from xUML to S/R, the input language of the COSPAN model checker (R.H. Hardin et al., 1996). Model transformations are applied to reduce the state space of the resulting S/R model that is to be verified by COSPAN. An xUML level logic for specifying properties to be checked is defined. Automated support is provided for translating properties specified in the logic to S/R representations and mapping error traces generated by COSPAN to xUML representations.
[program verification, input language, state space, Unified modeling language, COSPAN model checker, execution semantics, S/R representations, Proposals, formal specification, specification languages, model transformations, xUML level logic, S/R language, executable UML subset, Handicapped aids, xUML representations, error traces, Logic design, State-space methods, programming language semantics, Phase detection, Computer languages, model checking, Software systems, Interleaved codes, model checking software system designs, automated support, automatic translation, language translation]
Automating the performance and reliability analysis of enterprise information systems
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Good quality models for the analysis of complex enterprise information systems (EIS) are hard to build and require lots of experience and effort, which are not always available. A possible solution to deal with the previous issue is to build automated procedures for quality model generation. Such procedures will encapsulate previous existing knowledge on quality modeling and their use will reduce the cost of developing quality models. The authors concentrate on the performance and reliability of EIS and investigate the automatic generation of quality models from EIS architectural descriptions comprising additional information related to the aspects that affect the quality of the EIS.
[Availability, automatic programming, Costs, automated procedures, enterprise information system reliability, Time measurement, management information systems, software quality, quality modeling, Information analysis, Information systems, Filters, Software architecture, quality model generation, EIS architectural descriptions, Management information systems, Performance analysis, Large-scale systems, automatic model generation, complex enterprise information systems, software performance evaluation, quality models]
Unfriendly COTS integration - instrumentation and interfaces for improved plugability
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
It is becoming increasingly desirable to incorporate commercial-off-the-shelf (COTS) tools as software components into larger software systems. Due to their large user base, COTS tools tend to be cheap, reasonably reliable, and functionally powerful. Reusing them as components has the benefit of significantly reducing development cost and effort. Despite these advantages, developers encounter major obstacles in integrating most COTS tools because these tools have been constructed as stand-alone applications and make assumptions about their environment that do not hold when used as part of larger software systems. Most significantly, while they frequently contain programmatic interfaces that allow other components to obtain services from them on a direct call basis, they almost always lack the notification and data synchronicity facilities required for active integration. The authors present an integration framework for adding these notification and data synchronization facilities to COTS tools so that they can be integrated as active software components into larger systems. We illustrate our integration framework through tool suites we constructed around Mathworks' Matlab/Stateflow and Rational's Rose (two widely-used, large COTS tools). Our experience to date is that it is indeed possible to transform standalone COTS tools into software components.
[Costs, application program interfaces, mathematics computing, data synchronization facilities, Programming, tool suites, Rose, integration framework, standalone COTS tools, development cost, Space technology, software packages, Matlab/Stateflow, software tools, direct call basis, software components, Software reusability, commercial-off-the-shelf tools, object-oriented programming, programmatic interfaces, Instruments, active integration, Large scale integration, active software components, stand-alone applications, Software libraries, large user base, data synchronicity facilities, larger software systems, Software systems, Power system reliability, unfriendly COTS integration, Software tools]
Design rationale for software maintenance
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
For a number of years, members of the artificial intelligence (AI) in design community have studied design rationale (DR), the reasons behind decisions made while designing. A record of what decisions were made, and why, is especially valuable for software maintenance. One reason for this is that the software lifecycle is a long one. Large projects may take years to complete and spend even more time out in the field being used (and maintained). The combination of a long lifecycle and the typically high personnel turnover in the software industry increases the probability that the original designer is unlikely to be available for consultation when problems arise. J. Lee's survey (1997) presents an excellent overview of DR research. There has also been work specific to software design, such as Boehm's WinWin (1994).
[Process design, Software maintenance, Collaborative software, software lifecycle, Documentation, design rationale, Personnel, software maintenance, artificial intelligence, Computer science, Software design, software industry, Prototypes, Computer industry, design community, Artificial intelligence]
Automata-based verification of temporal properties on running programs
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
This paper presents an approach to checking a running program against Linear Temporal Logic (LTL) specifications. LTL is a widely used logic for expressing properties of programs viewed as sets of executions. Our approach consists of translating LTL formulae to finite-state automata, which are used as observers of the program behavior. The translation algorithm we propose modifies standard LTL to Buchi automata conversion techniques to generate automata that check finite program traces. The algorithm has been implemented in a tool, which has been integrated with the generic JPaX framework for runtime analysis of Java programs.
[Algorithm design and analysis, program verification, finite automata, linear temporal logic specifications, temporal logic, Logic testing, formal specification, Runtime, Space technology, running programs, Java programs, Java, program behavior, Computerized monitoring, NASA, finite state automata, automata-based verification, Buchi automata conversion, Automata, temporal properties, runtime analysis, Computer industry, generic JPaX framework, observers, Software engineering]
Amphion/NAV: deductive synthesis of state estimation software
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Previous work on domain-specific deductive program synthesis described the Amphion/NAIF system for generating Fortran code from high-level graphical specifications describing problems in space system geometry. Amphion/NAIF specifications describe input-output functions that compute geometric quantities (e.g., the distance between two planets at a point in time, or the time when a radio communication path between a spacecraft and earth is occluded) by composing together Fortran subroutines from the NAIF subroutine library developed at the Jet Propulsion Laboratory. In essence, Amphion/NAIF synthesizes code for glueing together the NAIF components in a way such that the generated code implements the specification, with a concurrently generated proof that this implementation is correct. Amphion/NAIF demonstrated the success of domain-specific deductive program synthesis and is still in use today within the space science community. However, a number of questions remained open that we will attempt to answer in this paper.
[program verification, Laboratories, Fortran code, radio communication path, Amphion/NAV, geometric quantities, state estimation software, inference mechanisms, formal specification, Geometry, Space vehicles, Earth, deductive synthesis, input-output functions, Algorithms, Planets, Propulsion, state estimation, high-level graphical specifications, Libraries, domain-specific deductive program synthesis, space system geometry, State estimation, Radio communication]
Generation of functional test sequences from B formal specifications presentation and industrial case-study
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The paper presents an original method to generate test sequences. From formal specifications of the system to be tested, an equivalent system of constraints is derived, and then the domain of each state variable of this system is partitioned into subdomains. Using this partition, limit states are computed with a specific solver that uses constraint logic programming with sets. This specific solver is then used to build test sequences by traversing the constrained reachability graph of the specifications. Finally, the formal specifications are used as an oracle by using them to determine the expected output for a given input. The results of an industrial case-study of the Smart Card GSM 11-11 standard are presented and discussed.
[Performance evaluation, System testing, Smart cards, program testing, test sequence generation, smart cards, set theory, Logic testing, formal specification, B formal specifications, Design engineering, Smart Card GSM 11-11 standard, state variable, industrial case-study, constraint logic programming, constraint handling, expected output, GSM, constrained reachability graph, reachability analysis, Logic programming, Computational modeling, Formal specifications, Content addressable storage, limit states, functional test sequence generation]
An analysis-revision cycle to evolve requirements specifications
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We argue that the evolution of requirements specifications can be supported by a cycle composed of two phases: analysis and revision. We investigate an instance of such a cycle, which combines two techniques of logical abduction and inductive learning to analyze and revise specifications respectively.
[Heart, logical abduction, Induction generators, Educational institutions, analysis, formal specification, analysis-revision cycle, Information analysis, revision, Bridges, formal logic, inductive learning, requirements engineer, Engineering management, state transition systems, systems analysis, change management process, Logic, requirement specifications evolution, learning by example]
Security specification and verification
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Formalizing security requirements has received a significant attention since the 70s. However a general method for specifying security requirements is still missing. Especially, little work has been presented on specifying and verifying that a given application is a secure resource consumer The purpose of this work is to set up a methodology for (1) specifying security requirements of service providers and (2) proving that some application securely uses some resources. The developed theory will be evaluated and applied in two different areas: secure mobile code development and secure COTS-based software development.
[security verification, Multilevel systems, Programming, Application software, Power system security, formal specification, Power engineering computing, formal verification, security of data, service providers, security specification, secure COTS-based software development, security requirements, Computer security, Power generation, Testing, Power engineering and energy, Software engineering, secure mobile code development]
Towards an evolutionary formal software development
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Although formal methods have been successfully applied in various industrial applications, their use in software development is still restricted to individual case studies. To overcome this situation we aim at a methodology for an evolutionary formal software development which allows for a stepwise and incremental development process along the line of rapid prototyping. The approach is based on work on a formal management of change for formal developments which is able to maintain proofs when changing specifications.
[Software testing, Software prototyping, software prototyping, stepwise development process, Life testing, rapid prototyping, Programming, Application software, evolutionary formal software development, Sufficient conditions, incremental development process, Prototypes, systems analysis, Computer industry, formal management of change, software engineering, formal developments, Artificial intelligence, Context modeling]
Programs are abstract data types
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We propose to view programs as abstract data types and to perform program changes by applying well-defined operations on programs. The ADT view of programs goes beyond the approach of syntax-directed editors and proof-editors since it is possible to combine basic update operations into larger update programs that can be stored and reused. It is crucial for the design of update operations and their composition to know which properties they can preserve when they are applied to a program. The author argues in favor of the abstract data type view of programs, and presents a general framework in which different programming languages, update languages, and properties can be studied.
[Software maintenance, Costs, text editing, Synthesizers, larger update programs, abstract data types, Calculus, program changes, basic update operations, programming languages, Programming profession, Computer languages, Reactive power, ADT view, Program processors, syntax-directed editors, Error correction, theorem proving, software tools, proof editors, update languages, Pattern matching, programming, update operations]
Action Language Verifier
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
Action Language is a specification language for reactive software systems. We present the Action Language Verifier which consists of: 1) a compiler that converts Action Language specifications to composite symbolic representations, and 2) an infinite-state symbolic model checker which verifies (or falsifies) CTL properties of Action Language specifications. Our symbolic manipulator (Composite Symbolic Library) combines a BDD manipulator (for boolean and enumerated types) and a Presburger arithmetic manipulator (for integers) to handle multiple variable types. Since we allow unbounded integer variables, model checking queries become undecidable. We present several heuristics used by the Action Language Verifier to achieve convergence.
[Thyristors, program verification, Binary decision diagrams, Switches, boolean types, formal specification, Composite Symbolic Library, binary decision diagrams, CTL properties, decidability, compiler generators, unbounded integer variables, heuristics, specification languages, Presburger arithmetic manipulator, compiler, Action Language specifications, specification language, reactive software systems, Action Language Verifier, enumerated types, Object oriented modeling, composite symbolic representations, BDD manipulator, multiple variable types, Specification languages, Formal specifications, Computer science, infinite-state symbolic model checker, Software libraries, Software systems, Arithmetic, symbolic manipulator, model checking queries]
Knowledge base approach to consistency management of UML specifications
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The use of the Unified Modelling Language (UML) during systems development has been growing in scale and complexity, often resulting in inconsistent specifications. We present a knowledge base goal-driven approach for consistency management of UML specifications represented as axioms which define goals. We propose an inference procedure as a flexible pattern-based abduction used to build and morph paths based on the specifications. The approach involves a two-step interaction process between the specifications: observation and comparison. Prototypes of the knowledge base engine and of a tool to map UML specifications in XMI format (eXtensible Metadata Interchange) to the knowledge base have been developed to demonstrate and evaluate the approach.
[knowledge base, knowledge base engine, Unified modeling language, Programming, Data mining, formal specification, Engines, axioms, knowledge base goal-driven approach, XMI format, Prototypes, knowledge based systems, knowledge base approach, specification languages, UML specifications, two-step interaction process, Software prototyping, flexible pattern-based abduction, Object oriented modeling, Knowledge management, data integrity, inconsistent specifications, inference mechanisms, consistency management, inference procedure, systems development, electronic data interchange, Chromium, Software systems, Unified Modelling Language]
Strategies for automated specification-based testing of synchronous software
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The paper presents new techniques for specification based-testing of synchronous software with the Lutess tool. Lutess provides a framework consisting of automatically building generators which interact with the software under test and feed it with test input sequences. In the past few years, it has been established that operational profiles as well as scenarios are powerful tools, allowing for a better fault detection ability. As opposed to this last technique which relies on the ability of the human tester to specify scenarios, we propose an approach based solely on software specification to automatically generate input sequences which may correspond to fault revealing scenarios.
[Software testing, input sequences, program testing, Humans, Software safety, Lutess tool, formal specification, program compilers, automatic generator design, operational profiles, Computer architecture, synchronous software, automatic programming, fault detection ability, software specification, Computational modeling, fault revealing scenarios, test input sequences, automated specification-based testing, Automatic testing, Fault detection, human tester, Synchronous generators, Feeds, specification based-testing, Software tools]
A new way of automating statistical testing methods
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We propose a novel way of automating statistical structural testing of software, based on the combination of uniform generation of combinatorial structures, and of randomized constraint solving techniques. More precisely, we show how to draw test cases which balance the coverage of program structures according to structural testing criteria. The control flow graph is formalized as a combinatorial structure specification. This provides a way of uniformly drawing execution paths which have suitable properties. Once a path has been drawn, the predicate characterizing those inputs which lead to its execution is solved using a constraint solving library. The constraint solver is enriched with powerful heuristics in order to deal with resolution failures and random choice strategies.
[Software testing, Algorithm design and analysis, combinatorial structure specification, program testing, combinatorial mathematics, constraint solving library, Probability distribution, Tree graphs, random choice strategies, heuristics, program structures, uniform generation, constraint handling, statistical structural testing, random testing, statistical testing method automation, automatic programming, structural testing criteria, Statistical analysis, software testing, resolution failures, execution paths, Flow graphs, Software libraries, randomized constraint solving techniques, control flow graph, Automatic testing, Packaging, combinatorial structures, statistical analysis]
Automatic test data generation for programs with integer and float variables
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
The paper presents a novel approach for automated test data generation of imperative programs containing integer, boolean and/or float variables. Our approach is based on consistency techniques integrating integer and float variables. We handle statement, branch and path coverage criteria. Our purpose is thus to automatically generate test data that will cause the program to execute a statement, to traverse a branch or to traverse a specified path. For path coverage, the specified path is transformed into a path constraint which is solved by an interval-based constraint solving algorithm handling integer, boolean and real variables. A valid test input is then extracted from the interval solutions. For statement (and branch) coverage, a path reaching the specified statement or branch is dynamically constructed. Our algorithm for path coverage is then applied. The search for a suitable path and the solving of path constraints make extensive use of consistency techniques. We propose a simple consistency notion called eBox consistency, for generalizing box consistency to integer and float variables. The eBox consistency is sufficient for our purpose. A prototype has been developed and experimental results show the feasibility of our approach. This work is an extension of work by A. Gotlieb (2000) for float and boolean variables.
[Software testing, System testing, Costs, program testing, Programming, digital arithmetic, constraint satisfaction, Prototypes, automatic test data generation, box consistency, interval-based constraint solving algorithm, valid test input, Software prototyping, automatic programming, Automation, eBox consistency, software testing, constraint theory, interval solutions, data integrity, Flow graphs, path coverage, consistency techniques, float variables, Automatic testing, Fault detection, path constraint, integer variables, automated test data generation, boolean variables, imperative programs, path coverage criteria]
TestEra: a novel framework for automated testing of Java programs
Proceedings 16th Annual International Conference on Automated Software Engineering
None
2001
We present TestEra, a novel framework for automated testing of Java programs. TestEra automatically generates all non-isomorphic test cases within a given input size and evaluates correctness criteria. As an enabling technology, TestEra uses Alloy, a first-order relational language, and the Alloy Analyzer. Checking a program with TestEra involves modeling the correctness criteria for the program in Alloy and specifying abstraction and concretization translations between instances of Alloy models and Java data structures. TestEra produces concrete Java inputs as counterexamples to violated correctness criteria. The paper discusses TestEra's analyses of several case studies: methods that manipulate singly linked lists and red-black trees, a naming architecture, and a part of the Alloy Analyzer.
[Software testing, first-order relational language, Java data structures, program testing, program verification, enabling technology, Laboratories, violated correctness criteria, Prototypes, data structures, Alloy Analyzer, non-isomorphic test cases, Marine technology, Java, automatic programming, formal languages, correctness criteria, Computational modeling, abstraction, Data structures, concrete Java inputs, automated Java program testing, TestEra, singly linked lists, red-black trees, Computer science, Automatic testing, input size, Concrete, concretization translations, naming architecture]
A model of planning and enactment support in complex software development projects
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Summary form only given. We propose an approach to facilitating not only project enactment but also project planning and monitoring, by tracking all decisions made during project planning and enactment and managing dependencies between these decisions. In order to identify those dependencies relevant for a decision, we established an extendable model of planning and plan enactment, which explicitly describes the activities likely to occur while planning and enacting a software development project, as well as standard dependencies between these activities. We formalized this model by adapting an existing dependency management system, the Redux Model of Design, to record planning and plan enactment decisions and their dependencies. Furthermore, we are currently identifying heuristics to automatically capture typical dependencies, and defining rules to provide automatic planning support where possible. This approach allows to interleave planning and plan enactment, and to feed enactment data back into the plan, either by automatically reacting to enactment events and plan changes, or by notifying the appropriate person(s). Thus, we provide extensive support for the process of planning and enacting software development projects, in the form of dependency management, user notifications, and, where possible, automation of selective process steps.
[complex software development projects, project management, planning support, standard dependencies, software development management, Programming, software development project, project planning and monitoring, automatic planning support, project enactment, enactment support, dependency management system, Software engineering]
Distributed modular model checking
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Summary form only given. Model checking is a formal method that verifies whether a finite state model of a system satisfies a specification given as a temporal logic formula. The most severe problem model checking suffers from is the so called state explosion problem. Distribution is one of the techniques that combat the state explosion. The aim is to distribute the state space among a number of computers so as to be able to verify larger systems. Another approach that deals with the state explosion problem is modularity, i.e. exploiting the structure of the system. We propose to employ modular techniques to the distributed model checking problem. This can be useful especially for software, as the software model checking algorithms suffer from state explosion more severely than the hardware model checking techniques even when the system consists of one sequential finite-state component. Moreover, software programs have typically richer syntactic structure that can be exploited. Besides elaborating a theoretical background for distributed model checking based on the modular approach, we also intend to develop modular approaches to partitioning the state space, in particular to define partition functions that reduce the necessary communication in the distributed environment.
[distributed modular model checking, temporal logic formula, partition functions, state explosion, temporal logic, state space partitioning, formal specification, formal verification, modularity, distributed algorithms, finite state model, communication reduction, Software engineering]
Adapting applications on the fly
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Summary form only given. Adapting a component-based application means adapting one or more of its components, and in general, adapting a component at run-time means disconnecting it from the application and connecting a new version of this component. Many approaches like OLAN and DCUP are based on component models in which each component is constituted by a functional part and a control part. This last part provides a set of services to administrate the component. In our approach we aim to separate the control part outside the component. The benefit is the possibility to apply our solutions to existing models that have not been designed to support the dynamic adaptation. Through a first experimentation on the JavaBeans component model, we define an adaptation process composed of several steps: (1) defining the mapping rules between the old and the new component (correspondence between properties and services); (2) passivating the old and the new components; (3) transferring the state of the old component into the new one according to the mapping rules defined before; (4) disconnecting the old component and connecting the new one; (5) activating the new component. These steps may constitute the basic activities of an adaptation machine.
[Java, component activation, Automation, object-oriented programming, component administration, component disconnection, properties services correspondence, run-time adaptation, Reliability engineering, passivation, state transfer, Application software, JavaBeans component model, Runtime, mapping rule definition, component connection, Bibliographies, USA Councils, adaptation machine, Dynamic programming, Joining processes, distributed object management, Software engineering, component-based application]
Process support for tools interoperability
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Summary form only given. Our work seeks to build a platform that makes entities of various types (component, COTS, tools, etc.) interoperate in order to build a new application. We call this new kind of application a federation. Our federations use workflow as a support for application integration and interoperability. In this approach, the process is not defined in term of tools and their parameters; instead, the process is high level and describes only abstract steps without knowledge on how these steps will be carried out. Therefore, the federation offers a means to describe and control the synchronization between the abstract and executable process, and a set of concrete tools. The federation ensures that the execution of the abstract level involves a compatible real execution at the concrete level. Indeed, the real execution requires the collaboration of several tools. The description on how the abstract level is refined into the real execution satisfies consistency rules and interoperability paradigms. We think our work contributes by providing a high level view in which the application can be described, independently from the real tools specificities, and by providing the means to describe the application behavior and the tools can be used and modified flexibly and dynamically.
[federation, object-oriented programming, components, Collaborative tools, workflow, tool interoperability, data integrity, consistency rules, Application software, process support, real execution, application integration, COTS, integrated software, Collaborative work, Concrete, high level process, software tools, abstract execution, Software engineering]
Automatic synthesis of distributed systems
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Summary form only given. Our research aims towards a new method of synthesis for distributed systems using Mazurkiewicz traces for specification and asynchronous automata for models. Mazurkiewicz trace languages are languages closed under an explicit independence relation between actions and therefore they are suitable to describe concurrent behaviour. The main objectives of this work are: (a) to develop a specification language based on a distributed version of temporal logic on traces that is able to express properties about the independence of actions; (b) to design a synthesis procedure based on improvements and heuristics of the algorithms for asynchronous automata; (c) to implement the new procedure efficiently (and so to turn the theory into a reliable tool that can be used in practice); (d) to apply it to case studies in areas like small distributed algorithms (e.g. mutual exclusion, communication protocols) and asynchronous circuit design. The idea used for the core of the synthesis procedure is that of unfoldings, a successful technique based on branching time partial order semantics. Promising preliminary results were obtained: we were able to automatically synthesize mutual exclusion algorithms from regular trace specifications.
[Algorithm design and analysis, Protocols, concurrent behaviour, Heuristic algorithms, automata theory, high level synthesis, mutual exclusion, temporal logic, small distributed algorithms, asynchronous circuits, distributed temporal logic, formal verification, Logic circuits, branching time partial order semantics, specification languages, asynchronous automata models, distributed systems, Distributed algorithms, specification language, action independence, automatic synthesis, communication protocols, Reliability theory, explicit independence relation, Logic design, unfoldings, Specification languages, programming language semantics, Mazurkiewicz trace languages, distributed algorithms, Automata, concurrency control, asynchronous circuit design, Circuit synthesis]
Automatic inter-procedural test case generation
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Summary form only given. Our work is based on a new approach of the automatic structural test case generation problem defined previously. It uses constraint logic programming (CLP) to try and solve the problem of generating test cases in order to attain the structural covering of a procedure. A test tool prototype, named Inka has been developed by Thales Systemes Aeroportes. Inka. is designed for automatic structural test case generation for C programs. The operating cycle of Inka is cut in three parts. Our work in the Inka project is to find a way to treat large programs. Our approach of this problem is to find an alternative between stubs and complete unfolding.
[Software prototyping, Computer aided software engineering, Logic programming, program testing, automatic interprocedural test case generation, Flow graphs, Logic testing, test tool prototype, Automatic testing, Prototypes, logic programming, Inka, constraint logic programming, constraint handling, Software engineering]
Semantic links and co-evolution in object-oriented software development
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Summary form only given. This research focuses on the problem of the semantic linking and co-evolution of the different design diagrams and models of an object-oriented software application. The blueprint of an object-oriented software application consists mainly of models drawn in a modeling language. The state-of-the-art modeling language in object-oriented software development is the Unified Modeling Language (UML). Our research hypothesis is that using decidable fragments of first order logic to express the different UML diagrams enables the semantic linking of the different diagrams and models and enables the support of co-evolution which can be semi-automated, enhancing the reusability, maintainability and understandability of the design of the software application and of the software application in general. We propose to develop a formal framework to support the linking of the different diagrams and models within the software development life cycle (SDLC). The advantages of such a framework are: reasoning capabilities are provided, co-evolution is more guaranteed, adaptability of the design is improved and reuse and understandability of the software design increases. To support co-evolution of the design models in a semi-automatic way we investigate the query capabilities of these logic families.
[Software maintenance, understandability, Unified modeling language, Programming, reusability, semantic links, object-oriented software development, software development life cycle, Software design, semi-automated co-evolution, decidability, specification languages, Software reusability, query capabilities, object-oriented programming, Unified Modeling Language, Object oriented modeling, decidable fragments, reverse engineering, Logic design, Application software, software maintenance, programming language semantics, design models, design adaptability, UML, reasoning capabilities, software reusability, first order logic, design diagrams, Joining processes, maintainability, Software engineering]
Automatic validation of deployed J2EE components using aspects
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Validating that software components meet their requirements under a particular deployment scenario is very challenging. We describe a new approach that uses component aspects, describing functional and nonfunctional cross-cutting concerns impacting components, to perform automated deployed component validation. Aspect information associated with J2EE component implementations is inspected after component deployment by validation agents. These agents run automated tests to determine if the deployed components meet their aspect-described requirements. We describe the way component aspects are encoded, the automated agent-based testing process we employ, and our validation agent architecture and implementation.
[Java, program testing, Collaborative software, cross-cutting concerns, Application software, Middleware, formal specification, automated agent-based testing process, automatic validation, Databases, Automatic testing, component aspects, Computer architecture, User interfaces, Search engines, Collaborative work, validation agents, aspect described requirements, validation agent architecture, deployed J2EE components, software components validation, automated deployed component validation]
Assumption generation for software component verification
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Model checking is an automated technique that can be used to determine whether a system satisfies certain required properties. The typical approach to verifying properties of software components is to check them for all possible environments. In reality, however, a component is only required to satisfy properties in specific environments. Unless these environments are formally characterized and used during verification (assume-guarantee paradigm), the results returned by verification can be overly pessimistic. This work defines a framework that brings a new dimension to model checking of software components. When checking a component against a property, our model checking algorithms return one of the following three results: the component satisfies a property for any environment; the component violates the property for any environment; or finally, our algorithms generate an assumption that characterizes exactly those environments in which the component satisfies its required property. Our approach has been implemented in the LTSA tool and has been applied to the analysis of a NASA application.
[program verification, program testing, NASA, Humans, Explosions, Application software, Embedded software, Computer science, model checking, formal verification, Space technology, Operating systems, Character generation, assumption generation, Software systems, software tools, software component verification, required properties, software components, LTSA tool]
Generative design patterns
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
A design pattern encapsulates the knowledge of object-oriented designers into re-usable artifacts. A design pattern is a descriptive device that fosters software design re-use. There are several reasons why design patterns are not used as generative constructs that support code re-use. The first reason is that design patterns describe a set of solutions to a family of related design problems and it is difficult to generate a single body of code that adequately solves each problem in the family. A second reason is that it is difficult to construct and edit generative design patterns. A third major impediment is the lack of a tool-independent representation. A common representation could lead to a shared repository to make more patterns available. We describe a new approach to generative design patterns that solves these three difficult problems. We illustrate this approach using tools called CO/sub 2/P/sub 2/S and Meta-CO/sub 2/P/sub 2/S but our approach is tool-independent.
[Process design, Java, Natural languages, Humans, Meta-CO2P2S tool, Documentation, generative design patterns, tool-independent representation, program compilers, Programming profession, shared repository, Software design, code generation, object-oriented design, Web pages, Collaboration, object-oriented methods, software tools, Impedance, software design reuse, CO2P2S tool, Software engineering]
Generating product-lines of product-families
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
GenVoca is a methodology and technology for generating product-lines, i.e. building variants of a program. The primitive components from which applications are constructed are refinements or layers, which are modules that implement a feature that many programs of a product-line can share. Unlike conventional components (e.g., COM, CORBA, EJB), a layer encapsulates fragments of multiple classes. Sets of fully formed classes can be produced by composing layers. Layers are modular, albeit unconventional, building blocks of programs. But what are the building blocks of layers? We argue that facets is an answer. A facet encapsulates fragments of multiple layers, and compositions of facets yields sets of fully formed layers. Facets arise when refinements scale from producing variants of individual programs to producing variants of multiple integrated programs, as typified byproduct families (e.g., MS Office). We present a mathematical model that explains relationships between layers and facets. We use the model to develop a generator for tools (i.e., product family) that are used in language-extensible Integrated Development Environments (IDEs).
[Encapsulation, product-lines generation, language-extensible integrated development environments, graphical user interfaces, Electrical capacitance tomography, fully formed classes, formal specification, GenVoca, Read only memory, Hip, Reactive power, Packaging, mathematical model, primitive components, programming environments, facet]
Experience report on automated procedure construction for deductive synthesis
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Deductive program synthesis systems based on automated theorem proving offer the promise of "correct by construction" software. However, the difficulty encountered in constructing usable deductive synthesis systems has prevented their widespread use. Amphion is a real-world, domain-independent program synthesis system. It is specialized to specific applications through the creation of an operational domain theory and a specialized deductive engine. This paper describes an experiment aimed at making the construction of usable Amphion applications easier. The software system Theory Operationalization for Program Synthesis (TOPS) has a library of decision procedures with a theory template for each procedure. TOPS identifies axioms in the domain theory that are an instance of a library of procedure and uses partial deduction to augment the procedure with the capability to construct ground terms for deductive synthesis. Synthesized procedures are interfaced to a resolution theorem prover. Axioms in the original domain theory that are implied by the synthesized procedures are removed. During deductive synthesis, each procedure is invoked to test conjunctions of literals in the language of the theory of that procedure. When possible, the procedure generates ground terms and binds them to variables in a problem specification. These terms are program fragments. Experiments show that the procedures synthesized by TOPS can reduce theorem proving search at least as much as hand tuning of the deductive synthesis system.
[deductive program synthesis systems, program testing, operational domain theory, experience report, deductive engine, inference mechanisms, deductive synthesis, axioms, automated procedure construction, Amphion, correct by construction software, automated theorem proving, Chromium, TOPS, theorem proving, Software engineering]
No Java without caffeine: A tool for dynamic analysis of Java programs
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
To understand the behavior of a program, a maintainer reads some code, asks a question about this code, conjectures an answer, and searches the code and the documentation for confirmation of her conjecture. However, the confirmation of the conjecture can be error-prone and time-consuming because the maintainer has only static information at her disposal. She would benefit from dynamic information. In this paper, we present Caffeine, an assistant that helps the maintainer in checking her conjecture about the behavior of a Java program. Our assistant is a dynamic analysis tool that uses the Java platform debug architecture to generate a trace, i.e., an execution history, and a Prolog engine to perform queries over the trace. We present a usage scenario based on the n-queens problem, and two real-life examples based on the Singleton design pattern and on the composition relationship.
[Software maintenance, program debugging, execution history, Law, Java platform debug architecture, Electronic mail, History, Engines, n-queens problem, Singleton design pattern, static information, Performance analysis, software tools, Java programs, Caffeine, Java, Software algorithms, program behaviour understanding, Documentation, reverse engineering, dynamic analysis, real-life examples, usage scenario, Prolog engine, Legal factors]
Constructing CORBA-supported oracles for testing: a case study in automated software testing
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
As the complexity of applications and therefore of their testing process grows, the importance of automating the testing activity increases. The testing process includes test case generation, test sequencing, oracle construction, test execution and result interpretation. Automatic generation of test cases from formal specifications has received considerable attention. Relatively little work has been reported, however, on constructing oracles for supporting efficient and automatic execution of such test cases. We present a technique for constructing a CORBA-supported VDM oracle for black-box testing starting from a VDM-SL specification. This specification is used to automatically verify the results of operations implemented in a high-level programming language. We present a case study of the technique applied to a Java application for generic access control. The technique is applicable to any CORBA-compliant programming language.
[Software testing, Access control, System testing, Computer aided software engineering, program testing, test sequencing, VDM-SL specification, software reliability, automated software testing, Vienna development method, test execution, formal specification, high-level programming language, Vienna Development Method, test case generation, specification languages, generic access control, distributed object management, VDM, Java, object-oriented programming, black-box testing starting, Software reliability, Formal specifications, Application software, formal specifications, case study, Computer languages, result interpretation, Automatic testing, CORBA-supported oracles]
On CASE tool usage at Nokia
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
We present the results of a research work targeted to understanding CASE tools usage in Nokia. By means of a survey questionnaire, we collected data aimed to identify what features are most useful and best implemented in current CASE tools according to senior developers and managers. With the aid of both descriptive and inferential statistical data analysis methods, we found out that the features that are rated most useful belong to the graphical editing, version management and document generation categories. The statistical methods we use allow us to extend the results to the whole population with a certain degree of confidence. The analysis of the data seems to give the indication that there is a general level of dissatisfaction on the quality of currently available CASE tools. Also, there is evidence that some of the most advanced features (reverse engineering, code generation) are not deemed as useful as others. Further research should focus on extending the survey to other types of industries, and attempt generalization of the results. This may constitute precious feedback for the software tools industry in order to develop products that correspond more to industry needs.
[CASE tool usage, Computer aided software engineering, Data analysis, Automation, Statistical analysis, inferential statistical data analysis methods, Documentation, Programming, software tools industry, reverse engineering, Mobile handsets, program compilers, configuration management, program understanding, code generation, Computer industry, computer aided software engineering, graphical editing, version management, document generation categories, software tools, Software tools, Software engineering]
An approach to rapid prototyping of large multi-agent systems
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Engineering individual components of a multi-agent system and their interactions is a complex and error-prone task in urgent need of methods and tools. Prototyping is a valuable technique to help software engineers explore the design space while gaining insight and a "feel" for the dynamics of the system; prototyping also allows engineers to learn more about the relationships among design features and the desired computational behaviour. In this paper we describe an approach to building prototypes of large multi-agent systems with which we can experiment and analyse results. We have implemented an environment embodying our approach. This environment is supported by a distributed platform that helps us achieve controlled simulations.
[Software prototyping, Multiagent systems, Protocols, multi-agent systems, software prototyping, Horn clauses, rapid prototyping, error-prone task, Design engineering, computational behaviour, Prototypes, Automata, large multi-agent systems, logic programming, Space exploration, Virtual prototyping, Artificial intelligence, Informatics, design space, controlled simulations]
CpprofJ: aspect-capable call path profiling of multi-threaded Java applications
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
A primary goal of program performance understanding tools is to focus the user's attention directly on optimization opportunities where significant cost savings may be found. Optimization opportunities fall into (at least) three broad categories: the call context of a general component may obviate the need for some of its generality; cross-cutting program aspects may be implemented suboptimally for the particular context of use; and thread dependencies may cause unintended delays. This paper enhances prior work in call path profiling in several ways. First, it provides two different call path oriented views on program performance, a server view and a thread view. The former helps one optimize for throughput, while the latter is useful for optimizing thread latency. The views incorporate a typed time notation for representing different program activities, such as monitor wait and thread preemption times. Second, the new framework allows aspect-oriented program profiling, even when the original program was not designed in an aspect oriented fashion. Finally, the approach is implemented in a tool, CPPROFJ, an aspect-capable call path profiler for Java. It exploits recent developments in the Java APIs to achieve accurate and portable sampling-based profiling. Three case studies illustrate its use.
[application program interfaces, Throughput, Yarn, program performance, Delay, Design optimization, multi-threaded Java, Program processors, typed time notation, thread preemption times, Cost function, Monitoring, software performance evaluation, Testing, Java, aspect-capable call path profiling, object-oriented programming, multi-threading, CpprofJ, program performance understanding tools, thread dependencies, reverse engineering, aspect-oriented program profiling, Computer bugs, monitor wait, call context]
Generating expected results for automated black-box testing
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
In this paper we describe a technique for generating expected results for automated black-box testing. Generating expected results allows larger automated test suites to be created, moving us toward continuous product testing. Our technique uses a program's Input-Output (IO) relationships to identify unique combinations of program inputs that influence program outputs. With this information, a small set of test cases is executed and checked for correctness. Given the correctness of this set, the expected results for the larger combinatorial test set can be generated automatically. Included in the paper is an experimental study in which checking the results of 384 test cases allows us to generate expected results and fully automate nearly 600,000 test cases.
[Software testing, System testing, program testing, Humans, automated test suites, Explosions, continuous product testing, Application software, Computer science, Fault diagnosis, combinatorial test set, Automatic testing, Fault detection, Software quality, expected results generation, automated black-box testing]
Towards certifying domain-specific properties of synthesized code
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
We present a technique for certifying domain-specific properties of code generated using program synthesis technology. Program synthesis is a maturing technology that generates code from high-level specifications in particular domains. For acceptance in safety-critical applications, the generated code must be thoroughly tested which is a costly process. We show how the program synthesis system AUTOFILTER can be extended to generate not only code but also proofs that properties hold in the code. This technique has the potential to reduce the costs of testing generated code.
[Costs, program testing, synthesized code, NASA, Mission critical systems, safety-critical software, testing, Sensor phenomena and characterization, domain-specific property certification, Extraterrestrial measurements, formal specification, program compilers, Equations, certification, safety-critical applications, AUTOFILTER program synthesis system, high-level specifications, code generation, Space technology, proofs, Safety, State estimation, Testing]
SeDiTeC-testing based on sequence diagrams
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
In this paper we present a concept for automated testing of object-oriented applications and a tool called SeDiTeC that implements these concepts for Java applications. SeDiTeC uses UML sequence diagrams, that are complemented by test case data sets consisting of parameters and return values for the method calls, as test specification and therefore can easily be integrated into the development process as soon as the design phase starts. SeDiTeC supports specification of several test case data sets for each sequence diagram as well as to combine several sequence diagrams to so-called combined sequence diagrams thus reducing the number of diagrams needed. For classes and their methods whose behavior is specified in sequence diagrams and the corresponding test case data sets SeDiTeC can automatically generate test stubs thus enabling testing right from the beginning of the implementation phase. Validation is not restricted to comparing the test case data sets with the observed data, but can also include validation of pre- and postconditions.
[Software testing, test specification, automatic test stub generation, program testing, Unified modeling language, classes, Programming, Java applications, diagrams, automated testing, formal specification, automatic testing, SeDiTeC tool, combined sequence diagrams, postconditions, formal verification, specification languages, return values, software tools, validation, Java, object-oriented programming, method calls, Application software, test case data sets, Automatic testing, Collaboration, preconditions, object-oriented applications, UML sequence diagrams, Software systems, Software engineering]
Enabling iterative software architecture derivation using early non-functional property evaluation
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
The structure of a software architecture strongly influences the architecture's ability to prescribe systems satisfying functional requirements, non functional requirements, and overall qualities such as maintainability, reusability, and performance. Achieving an acceptable architecture requires an iterative derivation and evaluation process that allows refinement based on a series of tradeoffs. Researchers at the University of Texas at Austin are developing a suite of processes and supporting tools to guide architecture derivation from requirements acquisition through system design. The various types of decisions needed for concurrent derivation and evaluation demand a synthesis of evaluation techniques, because no single technique is suitable for all concerns of interest. Two tools in this suite, RARE and ARCADE, cooperate to enable iterative architecture derivation and architecture property evaluation. RARE guides derivation by employing a heuristics knowledge base, and evaluates the resulting architecture by applying static property evaluation based on structural metrics. ARCADE provides dynamic property evaluation leveraging simulation and model-checking. This paper presents a study whereby RARE and ARCADE were employed in the early stages of an industrial project to derive a Domain Reference Architecture (DRA), a high-level architecture capturing domain functionality, data, and timing. The discussion emphasizes early evaluation of performance qualities, and illustrates how ARCADE and RARE cooperate to enable iterative derivation and evaluation. These evaluations influenced DRA refinement as well as subsequent design decisions involving application implementation and computing platform selection.
[RARE, Laboratories, early nonfunctional property evaluation, Maintenance engineering, system design, Application software, Intelligent structures, software maintenance, heuristics knowledge base, Yarn, formal specification, Connectors, software architecture, iterative software architecture derivation, requirements acquisition, Software architecture, knowledge based systems, Computer architecture, functional requirements, structural metrics, Timing, Intelligent systems, ARCADE, software performance evaluation]
Dependence management for dynamic reconfiguration of component-based distributed systems
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
The growing popularity of wired and wireless Internet requires distributed systems to be more flexible, adaptive and easily extensible. Dynamic reconfiguration of component-based distributed systems is one possible solution to meet these demands. However, there are some challenges for building dynamically reconfigurable distributed systems. Managing dependencies among components is one of the most crucial problems we have to solve before a system can be dynamically reconfigured at runtime. This paper describes a dependence management for dynamic reconfiguration of distributed systems. The dependence management analyzes not only the static dependencies among components, but also the dynamic dependencies that take place at runtime, in order to support an efficient consistent reconfiguration of distributed systems. In addition, the dependence management can deal with nested dependencies during a dynamic reconfiguration.
[object-oriented programming, wired Internet, component-based distributed systems, Mobile handsets, Vehicle dynamics, Research and development, Vehicles, Computer science, Runtime, nested dependencies, Telematics, dynamic dependencies, dynamic reconfiguration, dependence management, Internet, IP networks, wireless Internet, Personal digital assistants, distributed object management, static dependencies]
System testing for object-oriented frameworks using hook technology
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
An application framework provides a reusable design and implementation for a family of software systems. If the framework contains defects, the defects will be passed on to the applications developed from the framework. Framework defects are hard to discover at the time the framework is instantiated. Therefore, it is important to remove all defects before instantiating the framework. The problem addressed in this paper is developing an automated state-based test suite generator technique that uses hook technology to produce test suites to test frameworks at the system level. A case study is reported and its results show that the proposed technique is reasonably effective at detecting faults. A supporting tool that automatically produces framework test cases, executes them, and evaluates the results is presented.
[System testing, Automation, object-oriented programming, program testing, reusable design, Object oriented modeling, software systems, system testing, Application software, Sparks, automated state-based test suite generator technique, hook technology, Automatic testing, Fault detection, software reusability, object-oriented frameworks, Software systems, Concrete, Software engineering]
Knowledge-based synthesis of numerical programs for simulation of rigid-body systems in physics-based animation
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Physics-based animation programs are important in a variety of contexts, including education, science and entertainment among others. Manual construction of such programs is expensive, time consuming and prone to error. We have developed a system for automatically synthesizing physics-based animation programs for a significant class of problems: constrained systems of rigid bodies, subject to driving and dissipative forces. Our system includes a graphical interface for specifying a physical scenario, including objects, geometry, dynamical variables and coordinate systems, along with a symbolic interface for specifying forces and constraints operating in the scenario. The entities defined in the graphical interface serve as the underlying vocabulary for specifications constructed in the symbolic interface. We use an algorithmically controlled rewrite system to construct a numerical simulation program that drives a real-time animation of the specified scenario. The algorithm operates by partitioning the constraints and dynamic variables into classes, assigning each class to be implemented in a different component of a general simulation program scheme. Our approach provides many of the benefits of formal deductive methods of program synthesis, while keeping the computational costs of program synthesis more in line with conventional program generator technology. We have successfully tested our system on numerous examples.
[Real time systems, Vocabulary, program testing, symbolic interface, physics-based animation programs, graphical interface, computer animation, knowledge based systems, Control system synthesis, numerical simulation program, real-time animation, Computational efficiency, knowledge-based synthesis, algorithmically controlled rewrite system, Educational programs, Computational modeling, program diagnostics, physical scenario, Partitioning algorithms, numerical programs, Geometry, physics computing, rigid-body systems simulation, formal deductive methods, Numerical simulation, Animation, coordinate systems, physics-based animation]
Monitoring requirements: a case study
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
In our study of composite systems, we found a class of requirements that could not be guaranteed to hold. Specifically, these requirements required the environment of the overall system to behave in ways that could not be controlled. The best we could do in such cases was to note the assumptions placed on the environment for the requirements to be met, and then monitor the environment at runtime to detect deviations from our assumptions about its behavior. This paper discusses a short example of carrying out this type of monitoring. It introduces three tools to support requirements monitoring: (1) a tool to capture a requirement formally, (2) a tool to translate that requirement into a runtime specification, and (3) a tool to actually do the runtime monitoring.
[Runtime environment, Computer aided software engineering, Computerized monitoring, Degenerative diseases, Interconnected systems, Control systems, requirements monitoring, formal specification, Computer science, formal verification, runtime monitoring, Automata, system monitoring, Safety, runtime specification, Injuries]
Predicting software stability using case-based reasoning
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Predicting stability in object-oriented (OO) software, i.e., the ease with which a software item can evolve while preserving its design, is a key feature for software maintenance. We present a novel approach which relies on the case-based reasoning (CBR) paradigm. Thus, to predict the chances of an OO software item breaking downward compatibility, our method uses knowledge of past evolution extracted from different software versions. A comparison of our similarity-based approach to a classical inductive method such as decision trees, is presented which includes various tests on large datasets from existing software.
[Software testing, Software maintenance, object-oriented programming, Stability, Q factor, object-oriented software, Predictive models, software versions, software quality, software maintenance, large datasets, similarity-based approach, downward compatibility, inductive method, Software quality, decision trees, Computer industry, case-based reasoning, Decision trees, software stability prediction, Object oriented programming, Software engineering, software metrics]
VIATRA - visual automated transformations for formal verification and validation of UML models
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
The VIATRA (visual automated model transformations) framework is the core of a transformation-based verification and validation environment for improving the quality of systems designed using the Unified Modeling Language by automatically checking consistency, completeness, and dependability requirements. In the current paper, we present an overview of (i) the major design goals and decisions, (ii) the underlying formal methodology based on metamodeling and graph transformation, (iii) the software architecture based upon the XMI standard, and (iv) several benchmark applications of the VIATRA framework.
[Computer aided software engineering, Unified modeling language, automatic completeness requirements checking, Environmental economics, System analysis and design, software architecture, Software architecture, formal verification, Mathematical analysis, specification languages, automatic consistency requirements checking, Mathematical model, benchmark applications, Object oriented modeling, Application software, transformation-based verification and validation environment, formal validation, quality, VIATRA, UML models, graph transformation, metamodeling, visual automated model transformations framework, automatic dependability requirements checking, Formal verification, XMI standard]
Model-based tests of truisms
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Software engineering (SE) truisms capture broadly-applicable principles of software construction. The trouble with truisms is that such general principles may not apply in specific cases. This paper tests the specificity of two SE truisms: (a) increasing software process level is a desirable goal; and (b) it is best to remove errors during the early parts of a software lifecycle. Our tests are based on two well-established SE models: (1) Boehm et.al.'s COCOMO II cost estimation model; and (2) Raffo's discrete event software process model of a software project life cycle. After extensive simulations of these models, the TAR2 treatment learner was applied to find the model parameters that most improved the potential performance of the real-world systems being modelled. The case studies presented here showed that these truisms are clearly sub-optimal for certain projects since other factors proved to be far more critical. Hence, we advise against truism-based process improvement. This paper offers a general alternative framework for model-based assessment of methods to improve software quality: modelling + validation + simulation + sensitivity. That is, after recording what is known in a model, that model should be validated, explored using simulations, then summarized to find the key factors that most improve model behavior.
[Software testing, Costs, Object oriented modeling, TAR2 treatment learner, Life testing, software lifecycle, Debugging, software process level, software quality, software project life cycle, formal specification, Runtime, software construction, COCOMO II cost estimation model, systems analysis, Software quality, Life estimation, software process improvement, Cities and towns, discrete event software process model, software engineering truisms, Software engineering]
Combining and adapting software quality predictive models by genetic algorithms
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
The goal of quality models is to predict a quality factor starting from a set of direct measures. Selecting an appropriate quality model for a particular software is a difficult, non-trivial decision. In this paper, we propose an approach to combine and/or adapt existing models (experts) in such way that the combined/adapted model works well on the particular system. Test results indicate that the models perform significantly better than individual experts in the pool.
[Performance evaluation, Q factor, direct measures, Predictive models, software quality, genetic algorithms, software quality predictive model adaptation, quality factor, Genetic algorithms, Computer science, Software quality, software quality predictive model combination, Decision trees, Testing, Software engineering, Classification tree analysis, software metrics]
From early requirements to user interface prototyping: a methodological approach
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
The objective of this paper is to define a software production process which represents the correspondence between the primitive elements of a business model (represented in the framework i*) and the user interface of the software system. The representation of the user interface is compliant with the Unified Model Language (UML). We use a use case model as an intermediary between the business requirements and the application software. By doing this, we go a step further in the process of properly embedding early requirements engineering into the software production process, because organizational users can validate their requirements as early as possible. This is done through the validation of the user interfaces which are generated as a software representation of these requirements. These interfaces can also be reused for further refinement as a useful starting point in the software development process.
[Software prototyping, Production systems, application software, software prototyping, business requirements, Unified modeling language, user interface prototyping, Programming, software production process, user interfaces, Application software, Embedded software, Business communication, formal verification, use case model, early requirements, UML, Prototypes, specification languages, User interfaces, Software systems, business model, requirements validation, business data processing]
What makes finite-state models more (or less) testable?
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
This paper studies how details of a particular model can effect the efficacy of a search for detects. We find that if the test method is fixed, we can identity classes of software that are more or less testable. Using a combination of model mutators and machine learning, we find that we can isolate topological features that significantly change the effectiveness of a defect detection tool. More specifically, we show that for one defect detection tool (a stochastic search engine) applied to a certain representation (finite state machines), we can increase the average odds of finding a defect from 69% to 91%. The method used to change those odds is quite general and should apply to other defect detection tools being applied to other representations.
[Software testing, Costs, program testing, software testing, Stochastic processes, model mutators, Design for experiments, topological features, Mechanical factors, finite state machines, machine learning, Computer science, finite-state model testability, Automata, Machine learning, defect detection tool, Search engines, learning (artificial intelligence), Software engineering]
Generating test data for functions with pointer inputs
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Generating test inputs for a path in a function with integer and real parameters is an important but difficult problem. The problem becomes more difficult when pointers are passed as inputs to a function. In this case, the shape of the input data structure as well as the data values in the fields of this data structure need to be determined for traversal of the given path. The existing techniques to address this problem are inefficient since they use backtracking to simultaneously satisfy the constraints on the pointer variables and the data values used along the path. In this paper, we develop a novel approach that allows the generation of the shape of an input data structure to be done independently of the generation of its data values so as to force the control flow of a function along a given path. We also present a new technique that generates the shape of the input data structure by solving a set of pointer constraints derived in a single pass of the statements along the path. Although simple, our approach is powerful in handling pointer aliasing. It is efficient and provides a practical solution to generating test data for functions with pointer inputs.
[Tree data structures, program testing, Input variables, test data generation, Data structures, Computer science, Shape control, Binary trees, real parameters, integer parameters, Relaxation methods, data structures, functions with pointer inputs, backtracking, Iterative methods, Force control, Testing, input data structure]
Report on the workshop on the state of the art in automated software engineering
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
false
[]
A temporal logic approach to the specification of reconfigurable component-based systems
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
We propose a formal specification language for dynamically reconfigurable component-based systems, based on temporal logic. The main aim of the language is to allow one to specify behaviours of component-based systems declaratively, with special emphasis on behaviours in which the architectural structure of the system changes dynamically. Due to the semantics and organisation of our language, it is straightforward to hierarchically build reconfigurable systems in terms of subsystems and basic component parts, and reason about them within the language. Despite its expressive power, the language is rather simple.
[Knowledge engineering, object-oriented programming, Reconfigurable logic, temporal logic, Educational institutions, Mechanical factors, LAN interconnection, Specification languages, formal specification language, semantics, Formal specifications, formal specification, Connectors, Computer science, architectural structure, software architecture, specification languages, dynamically reconfigurable component-based systems, Architecture description languages]
Automating requirements traceability: Beyond the record &amp; replay paradigm
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Requirements traceability (RT) aims at defining relationships between stakeholder requirements and artifacts produced during the software development life-cycle. Although techniques for generating and validating RT are available, RT in practice often suffers from the enormous effort and complexity of creating and maintaining traces or from incomplete trace information that cannot assist engineers in real-world problems. In this paper we will present a tool-supported technique easing trace acquisition by generating trace information automatically. We will explain the approach using a video-on-demand system and show that the generated traces can be used in various engineering scenarios to solve RT-related problems.
[trace acquisition, complexity, Automation, video-on-demand system, Maintenance engineering, Programming, Information retrieval, stakeholder requirements, Design engineering, requirements traceability automation, formal verification, software development life-cycle, systems analysis, Systems engineering and theory, real-world problems, Software engineering, software metrics, artifacts]
Interfaces for modular feature verification
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Feature-oriented programming organizes programs around features rather than objects, thus better supporting extensible, product-line architectures. Programming languages increasingly support this style of programming, but programmers get little support from verification tools. Ideally, programmers should be able to verify features independently of each other and use automated compositional reasoning techniques to infer properties of a system from properties of its features. Achieving this requires carefully designed interfaces: they must hold sufficient information to enable compositional verification, yet tools should be able to generate this information automatically because experience indicates programmers cannot or will not provide it manually. We present a model of interfaces that supports automated, compositional, feature-oriented model checking. To demonstrate their utility, we automatically detect the feature-interaction problems originally found manually by R. Hall in an email suite case study.
[compositional verification, Modular construction, Computer vision, Assembly systems, email suite, feature-oriented programming, Data structures, modular feature verification interfaces, Explosions, Programming profession, Concurrent computing, Computer languages, verification tools, product-line architectures, formal verification, automated compositional reasoning, Large-scale systems, software tools, Protection, feature-oriented model checking]
Analyzing dependencies in large component-based systems
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Component-based development has become an important area in the software engineering field. In spite of this, there has been little effort to understand and to manage the different forms of dependencies that can occur in systems built from components. Dependencies reflect the potential for one component to affect or be affected by the elements (e.g., other components) that compose the system. Understanding dependencies is an essential requirement to perform important tasks, such as evolution and testing, during a component-based system's life cycle. In this paper, we present a technique to analyze dependencies in large component-based systems.
[Performance evaluation, System testing, object-oriented programming, program testing, Life testing, Documentation, Independent component analysis, testing, evolution, Information analysis, dependencies, Computer science, XML, large component-based systems, software engineering, Large-scale systems, Software engineering]
Adding value to formal test oracles
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Test oracles are programs which check the output of test cases run on other programs. We describe techniques which add value to formally-defined test oracles in three ways: (a) by measuring functional coverage of test suites, (b) by giving automated support to the process of validating the oracles, and (c) by automating the generation of test cases from the oracles. The techniques involve the use of coverage measures and AI-based search algorithms. We describe the application of these techniques in the verification and validation of a complex piece of real-world software.
[Software testing, formal test oracles, program testing, Wireless application protocol, functional coverage measurement, test case output checking, test suites, Temperature sensors, automatic testing, formal verification, AI-based search algorithms, Software measurement, search problems, verification, validation, Formal specifications, Application software, Computer science, added value, Automatic testing, Heat engines, Thermal sensors, automated support, automatic test case generation, programs]
Automated validation of class invariants in C++ applications
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
In this paper, we describe a non-invasive approach for validation of class invariants in C++ applications. Our approach is fully automated so that the user need only supply the class invariants for each class hierarchy to be checked and our validator constructs an InvariantVisitor, a variation of the Visitor Pattern, and an InvariantFacilitator. Instantiations of the InvariantVisitor and InvariantFacilitator classes encapsulate the invariants in C++ statements and facilitate the validation of the invariants. We describe both our approach and our results of validating invariants in keystone, a well tested parser front-end for C++.
[Software testing, InvariantFacilitator, Costs, Shape, graphical user interfaces, C++ applications, Programming, noninvasive approach, software quality, C++ language, Application software, automated validation, parser front-end, Computer science, Visitor Pattern, Software design, class invariants, Software quality, class hierarchy, Computer industry, InvariantVisitor, Contracts]
Identifying cause and effect relations between events in concurrent event-based components
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Concurrent event-based components present characteristics that impose difficulties in understanding their dynamic behavior, mainly for interpreting the cause and effect relations between input and output events in component interactions. In this paper, we propose a technique to help in the process of understanding the dynamic behavior of concurrent event-based components. It checks the event trace (generated by monitoring the component execution) against a specification of the component communication protocol (even with a possibly incomplete or incorrect specification). The technique identifies and presents the more probable cause and effect relations between the component events, providing also a measurement related to this probability.
[Context, Protocols, object-oriented programming, multi-threading, Instruments, dynamic behavior, Humans, cause and effect relations, specification, Yarn, formal specification, input events, Computer science, Communication system software, event trace checking, Software systems, system monitoring, output events, concurrent event-based components, component interactions, protocols, component communication protocol, Monitoring]
A framework for automatic debugging
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
This paper presents an application framework in which declarative specifications of debugging actions are translated into execution monitors that can automatically detect bugs. The approach is non-intrusive with respect to program source code and provides a high level of abstraction for debugging activities.
[program debugging, Event detection, Object oriented modeling, declarative specifications, Debugging, Virtual machining, Application software, formal specification, debugging activities, Voice mail, Computer science, Computer displays, automatic debugging framework, Writing, application framework, execution monitors, program source code, Software engineering]
Systematic bridging the gap between requirements and OO design
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Today, most of the OO software development methodologies analyze requirements in terms of objects and their interactions. As a result, they have the problem of forcing analysts to make design decisions on objects and their interactions at the early stage of requirements analysis. This paper gives a brief introduction of an enhanced DFD model called data flow net (DF net) that resolves the impeding mismatch between DFD based models and OO models. It also gives a brief introduction of an approach that uses DF net to specify use-cases in the requirements analysis stage without the need of making the above-mentioned major design decisions. In the design stage, the proposed approach transforms the DF nets specified during the requirements analysis stage systematically and precisely into OO designs. It bridges the gap between requirements and OO design. A significant part of the transformation can be automated.
[Process design, data flow net, Object oriented modeling, data flow analysis, Programming, object oriented design, dataflow diagrams, diagrams, formal specification, Design for disassembly, Bridges, requirements analysis, Design engineering, use-cases, software development methodologies, object-oriented methods, Impedance, Software engineering]
Towards usable and relevant model checking techniques for the analysis of dependable interactive systems
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
Model checking is a formal technique for the automated analysis of system models against formal requirements. Once a suitable model and property have been specified, no further interaction by the analyst is required. However, this does not make the method necessarily user friendly since the checker must be provided with appropriate and complex input data. Furthermore, counter-examples generated by the system are often difficult to interpret. Because of this complexity, model checking is not commonly used, and exhaustive exploration of system models based on finite state descriptions is not exploited within industrial dependable systems design. The paper describes the development of an integrated collection of tools around SMV, intended to make it more accessible to practicing software engineers and in particular those concerned with the human interface issues in complex safety critical systems.
[Computer interfaces, Costs, Humans, safety-critical software, Aerospace electronics, specification, Software safety, formal specification, formal requirements, Computer science, formal verification, model checking, Interactive systems, formal methods, finite state descriptions, Computer industry, Software tools, Usability, SMV, safety critical systems]
Automatic test case optimization using a bacteriological adaptation model: application to .NET components
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
In this paper, we present several complementary computational intelligence techniques that we explored in the field of .Net component testing. Mutation testing serves as the common backbone for applying classical and new artificial intelligence (AI) algorithms. With mutation tools, we know how to estimate the revealing power of test cases. With AI, we aim at automatically improving test case efficiency. We therefore looked first at genetic algorithms (GA) to solve the problem of test. The aim of the selection process is to generate test cases able to kill as many mutants as possible. We then propose a new AI algorithm that fits better to the test optimization problem, called bacteriological algorithm (BA): BAs behave better that GAs for this problem. However, between GAs and BAs, a family of intermediate algorithms exists: we explore the whole spectrum of these intermediate algorithms to determine whether an algorithm exists that would be more efficient than BAs.: the approaches are compared on a .Net system.
[System testing, Computer aided software engineering, object-oriented programming, .Net component testing, program testing, Adaptation model, Genetic mutations, Spine, mutant killing, AI algorithm, mutation testing, genetic algorithms, selection process, Genetic algorithms, automatic testing, bacteriological algorithm, Automatic testing, network operating systems, automatic test case optimization, bacteriological adaptation model, Robustness, Artificial intelligence, Computational intelligence]
Automatic verification of any number of concurrent, communicating processes
Proceedings 17th IEEE International Conference on Automated Software Engineering,
None
2002
The automatic verification of concurrent systems by model-checking is limited due to the inability to generalise results to systems consisting of any number of processes. We use abstraction to prove general results, by model-checking, about feature interaction analysis of a telecommunications service involving any number of processes. The key idea is to model-check a system of constant number (m) of concurrent processes, in parallel with an "abstract" process which represents the product of any number of other processes. The system, for any specified set of selected features, is generated automatically using Perl scripts.
[Gold, automatic programming, model-checking, program verification, Peer to peer computing, program diagnostics, feature interaction analysis, concurrency theory, Telecommunication computing, concurrent systems, automatic verification, Concrete, Logic, Perl scripts, Telecommunication services]
Proceedings 18th IEEE International Conference on Automated Software Engineering
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
The following topics are discussed: requirements, interfaces, and groupware; software architectures and distributed systems; model checking; software components; software evolution and maintenance; and automated software engineering.
[workstation clusters, message passing, Tree searching, program verification, software testing, Software verification and validation, software architectures, temporal logic, user interfaces, tree searching, software maintenance, automated software engineering, Temporal logic, software evolution, requirements engineering, model checking, Message passing, groupware, distributed systems, software engineering, software components, Software engineering]
Specification and synthesis of hybrid automata for physics-based animation
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Physics-based animation programs can often be modeled in terms of hybrid automata. A hybrid automaton includes both discrete and continuous dynamical variables. The discrete variables define the automaton's modes of behavior. The continuous variables are governed by mode-dependent differential equations. This paper describes a system for specifying and automatically synthesizing physics-based animation programs based on hybrid automata. The system presents a program developer with a family of parameterized specification schemata. Each scheme describes a pattern of behavior as a hybrid automaton passes through a sequence of modes. The developer specifies a program by selecting one or more schemata and supplying application-specific instantiation parameters for each of them. Each scheme is associated with a set of axioms in logic of hybrid automata. The axioms serve to document the semantics of the specification scheme. Each scheme is also associated with a set of implementation rules. The rules synthesize program components implementing the specification in general physics-based animation architecture. The system allows animation programs to be developed and tested in an incremental manner. The system itself can be extended to incorporate additional schemata for specifying new patterns o behavior, along with new sets of axioms and implementation rules. It has been implemented and tested on over a dozen examples. We believe this research is a significant step toward a specification and synthesis system that is flexible enough to handle a wide variety of animation programs, yet restricted enough to permit programs to be synthesized automatically.
[mode-dependent differential equations, Educational programs, automata theory, hybrid automata, incremental testing, Educational institutions, discrete variables, parameterized specification schemata, formal specification, specification system, Automotive engineering, Computer science, physics computing, computer animation, continuous variables, Layout, Automata, Differential equations, application-specific instantiation parameters, Animation, Physics education, Logic, synthesis system, physics-based animation]
A programmable client-server model: robust extensibility via DSLs
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
The client-server model has been successfully used to support a wide variety of families of services in the context of distributed systems. However, its server-centric nature makes it insensitive to fast changing client characteristics like terminal capabilities, network features, user preferences and evolving needs. To overcome these key limitations, we present an approach to enabling a server to adapt to different clients by making it programmable. A service-description language is used to program server adaptations. This language is designed as a domain-specific language to offer expressiveness and conciseness without compromising safety and security. We show that requiring the deployment of new protocols or server implementations. We illustrate our approach with the Internet Message Access Protocol (IMAP). An IMAP server is made programmable and a language, named Pems, is introduced to program robust variations of e-mail services. Our approach is uniformly used to develop a platform for multimedia communication services. This platform is composed of programmable servers for telephony service, e-mail processing, remote-document processing and stream adapters.
[digital subscriber line, stream adapters, electronic mail, user preferences, service-description language, Electronic mail, Network servers, Internet Message Access Protocol, programmable model, distributed systems, Robustness, DSL, Web server, distributed programming, Pems, network features, Context-aware services, client-server systems, telephony service, client-server model, server-centric nature, Access protocols, robust extensibility, program server adaptations, Domain specific languages, server implementations, IMAP, terminal capabilities, domain-specific language, multimedia communication services, transport protocols, telephony, Streaming media, evolving needs, remote-document processing, e-mail processing, Internet, programming environments, Context modeling]
XRay views: understanding the internals of classes
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Understanding the internal workings of classes is a key prerequisite to maintaining an object-oriented software system. Unfortunately, classical editing and browsing tools offer mainly linear and textual views of classes and their implementation. These views fail to expose the semantic relationships between the internal parts of a class. We propose X-Ray views - a technique based on concept analysis - which reveal the internal relationships between groups of methods and attributes of a class. X-Ray views are composed of elementary collaborations between attributes and methods, and help the engineer to build a mental model of how a class works internally. In this paper we present X-Ray views, and illustrate the approach by applying it on the Smalltalk class UIBuilder.
[Software maintenance, X-Ray views, Lattices, UIBuilder, Runtime, Smalltalk, internal relationships, Cognitive science, Protection, object-oriented software system, Legged locomotion, object-oriented programming, mental model, Collaborative tools, class understanding, elementary collaborations, reverse engineering, software maintenance, systems re-engineering, logical views, Collaboration, Software systems, Collaborative work, concept analysis, Smalltalk class]
Architecture style-based calculi for non-functional properties
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Engineers wield various "calculi" to help determine solutions to their problems, calculation tools varying in power from tensile strength tables to the differential calculus. A calculus is normally based on induction over an algebraic structure. Here the author explores how architecture styles can be used to describe such structures. An example calculus based on an "integration" style is presented, which is intended for use as a substyle of other architecture styles. Calculation rules in terms of the architectural elements can be used to compute non-functional attributes of artifacts described in such styles. Naturally, computerized support for calculi will help to automate the tasks of software engineers.
[Process design, architectural elements, Reliability engineering, Calculus, software architecture, Power engineering computing, Algebra, Computer architecture, software evaluation, nonfunctional properties, differential calculus, software engineers, program diagnostics, nonfunctional attributes, calculation tools, calculation rules, integration style, Computer languages, Software libraries, architecture styles, architecture style-based calculi, software analysis, reasoning about programs, Power engineering and energy, Software engineering]
DeCo: a declarative coordination framework for scientific model federations
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Program federation is assembling a software system from cooperating but independent application programs. We present DeCo, a declarative approach to creating and coordinating federations, and discuss its application in the domain of scientific computing. DeCo is meant to automate several aspects of the typically manual process of program federation. A federation is expressed in the functional language Haskell, extended with constructs for describing and coordinating the participating programs and data files. The declarative expression of a federation in terms of data flow among the component programs captures synchronization requirements implicitly and exploits the inherent concurrency automatically. Haskell compilation, notably its rigorous type checking, ensures the consistency of the federation. Operation of the coordination framework is demonstrated on a federation of FORTRAN programs that simulate environmental processes in the Neuse River estuary of North Carolina.
[declarative expression, North Carolina, Assembly systems, Scientific computing, Electronic mail, inherent concurrency, DeCo, synchronization requirements, Concurrent computing, scientific model federations, declarative coordination framework, Operating systems, program federation, automatic programming, Automation, scientific computing, Neuse river estuary, program assemblers, Rivers, Application software, Computer science, environmental science computing, Software systems, FORTRAN, Haskell compilation, data flow, functional languages]
A model-driven approach to non-functional analysis of software architectures
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
We present an approach to managing formal models using model driven architecture (MDA) technologies that deliver analysis techniques through integration with the design tools and repositories that practitioners use. Expert modeling knowledge is captured in domain-specific languages and meta-model constraints. These are represented using UML (Unified Modeling Language) and collocated with designs and analysis models, providing a flexible and visible approach to managing semantic associations. The approach relies on standards to permit deployment in multiple tools. We demonstrate our approach with an example in which queuing-network models are associated with UML design models to predict average case performance.
[model driven architecture, Unified modeling language, software architectures, formal model management, Predictive models, Electronic mail, model-driven approach, formal logic, software architecture, Technology management, Software architecture, Computer architecture, specification languages, Performance analysis, nonfunctional analysis, domain-specific languages, queueing theory, Unified Modeling Language, Object oriented modeling, meta-model constraints, expert modeling knowledge, Educational institutions, Computer science, queuing-network models, queueing network, multiple tools, MDA technologies, UML, average case performance]
Automated requirements-based generation of test cases for product families
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Software product families (PF) are becoming one of the key challenges of software engineering. Despite recent interest in this area, the extent to which the close relationship between PF and requirements engineering is exploited to guide the V&V tasks is still limited. In particular, PF processes generally lack support for generating test cases from requirements. In this paper, we propose a requirements-based approach to functional testing of product lines, based on a formal test generation tool. Here, we outline how product-specific test cases can be automatically generated from PF functional requirements expressed in UML (Unified Modeling Language). We study the efficiency of the generated test cases on a case study.
[Software testing, Computer aided software engineering, program testing, Unified modeling language, Automatic test pattern generation, Subcontracting, PF functional requirements, formal specification, formal test generation, software architecture, functional testing, Defense industry, formal verification, product families, specification languages, software engineering, product-specific test cases, Automation, Unified Modeling Language, requirements-based generation, Test pattern generators, software PF, requirements engineering, Automatic testing, V&amp;V tasks, UML, software reusability, product lines, Software engineering]
Model-based verification of Web service compositions
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
In this paper, we discuss a model-based approach to verifying Web service compositions for Web service implementations. The approach supports verification against specification models and assigns semantics to the behavior of implementation model so as to confirm expected results for both the designer and implementer. Specifications of the design are modeled in UML (Unified Modeling Language), in the form of message sequence charts (MSC), and mechanically compiled into the finite state process notation (FSP) to concisely describe and reason about the concurrent programs. Implementations are mechanically translated to FSP to allow a trace equivalence verification process to be performed. By providing early design verification, the implementation, testing, and deployment of Web service compositions can be eased through the understanding of the differences, limitations and undesirable traces allowed by the composition. The approach is supported by a suite of cooperating tools for specification, formal modeling and trace animation of the composition workflow.
[Process design, Unified modeling language, Message Sequence Charts, concurrent programs, Distributed computing, formal specification, FSP notation, formal verification, Web and internet services, specification languages, MSC, model-based verification, Testing, message passing, Unified Modeling Language, trace equivalence verification, Educational institutions, Web services, formal modeling, UML, trace animation, concurrency control, System recovery, finite state process, Animation, Internet, Usability]
An incremental approach to task-specific information delivery in SE processes
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
In this paper, we present a system to proactively provide software process participants with information items that are available and relevant in the context of their current tasks. Knowledge on which items to offer is obtained in two ways. First, situation-specific information needs that arise for participants are captured, together with the information sources that are accessed to satisfy them. These are recommended to other participants in similar situations using collaborative filtering techniques. Second, to allow for more systematic recommendation functionality, a formal language is provided to specify preconditions on when to offer certain information items. The resulting hybrid system thus supports an incremental phase-in of knowledge management techniques into an organization, allowing the organization to decide how much effort to spend on knowledge engineering.
[Knowledge engineering, Software maintenance, knowledge engineering, task-specific information delivery, knowledge management techniques, Formal languages, software process participants, knowledge management, incremental phase-in, collaborative filtering techniques, systematic recommendation functionality, Technology management, Management information systems, information items, software engineering, formal languages, Filtering, Information retrieval, Knowledge management, situation-specific information needs, incremental approach, information sources, Collaboration, software engineering processes, Internet, information needs, formal language]
A new structural coverage criterion for dynamic detection of program invariants
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Dynamic detection of program invariants is emerging as an important research area with many challenging problems. Generating suitable test cases that support accurate detection of program invariants is crucial to the dynamic approach of program invariant detection. In this paper, we propose a new structural coverage criterion called invariant-coverage criterion for dynamic detection of program invariants. We also show how the invariant-coverage criterion can be used to improve the accuracy of dynamically detected invariants. We first used the Daikon tool to report likely program invariants using the branch coverage and all definition-use pair coverage test suites for several programs. We then generated invariant-coverage suites for these likely invariants. When Daikon was run with the invariant-coverage suites, several spurious invariants reported earlier by the branch coverage and definition-use pair coverage test suites were removed from the reported invariants. Our approach also produced more meaningful invariants than randomly generated test suites.
[Software testing, program invariants, program testing, Computerized monitoring, program diagnostics, path testing, test data generation, Programming, execution traces, dynamic analysis, branch coverage test suites, invariant-coverage suites, dynamic detection, Computer science, Runtime, Automatic testing, invariant-coverage criterion, software engineering, structural coverage criterion, definition-use pair coverage test suites, Daikon tool, Software engineering]
Automatically inferring concern code from program investigation activities
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
When performing a program evolution task, developers typically spend a significant amount of effort investigating and reinvestigating source code. To reduce this effort, we propose a technique to automatically infer the essence of program investigation activities as a set of concern descriptions. The concern descriptions produced by our technique list methods and fields of importance in the context of the investigation of an object-oriented system. A developer can rely on this information to perform the change task at hand, or at a later stage for a change that involves the same concerns. The technique involves applying an algorithm to a transcript of a program investigation session. The transcript lists which pieces of source code were accessed by a developer when investigating a program and how the different pieces of code were accessed. We applied the technique to data obtained from program investigation activities for five subjects involved in two different program evolution tasks. The results show that relevant concerns can be identified with a manageable level of noise.
[automatic programming, Costs, source code investigation, software prototyping, program diagnostics, Scattering, Access protocols, Documentation, program evolution task, software maintenance, Noise level, inferring concern code, Computer science, object-oriented system, inference algorithm, Clustering algorithms, transcript lists, concern descriptions, Inference algorithms, Database systems, program investigation, Graphical user interfaces]
A pragmatic study of binary class relationships
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
A discontinuity exists between modeling and object-oriented programming languages. This discontinuity is a consequence of ambiguous notions in modeling languages and lack of corresponding notions in object-oriented programming languages. It hinders the transition between software implementation and design and hampers software maintenance. This discontinuity is particularly acute for binary class relationships, such as the association, aggregation, and composition relationships. We present a solution to bridge the discontinuity between implementation and design for the binary class relationships: we propose consensual definitions of the binary class relationships in terms of four properties (exclusivity, invocation site, lifetime, multiplicity). We describe algorithms to detect these properties in Java source code.
[Algorithm design and analysis, Software maintenance, Java, object-oriented programming, multiplicity property, Java source code, Object oriented modeling, Reverse engineering, Software algorithms, software design, binary class relationships, software maintenance, invocation site, Bridges, Computer languages, modeling languages, Software design, software implementation, object-oriented programming languages, lifetime property, Software tools, exclusivity property]
Generating design pattern detectors from pattern specifications
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
We present our approach to support program understanding by a tool that generates static and dynamic analysis algorithms from design pattern specifications to detect design patterns in legacy code. We therefore specify the static and dynamic aspects of patterns as predicates, and represent legacy code by predicates that encode its attributed abstract syntax trees. Given these representations, the static analysis is performed on the legacy code representation as a query derived from the specification of the static pattern aspects. It provides us with pattern candidates in the legacy code. The dynamic specification represents state sequences expected when using a pattern. We monitor the execution of the candidates and check their conformance to this expectation. We demonstrate our approach and evaluate our tool by detecting instances of the observer, composite and decorator patterns in Java code using Prolog to define predicates and queries.
[Algorithm design and analysis, Protocols, Heuristic algorithms, design pattern detector generation, formal specification, decorator patterns, abstract syntax trees, software architecture, Detectors, Computer architecture, specification languages, Performance analysis, PROLOG, Pattern analysis, legacy code, Java, object-oriented programming, program diagnostics, observer patterns, composite patterns, static analysis, Prolog, dynamic analysis, design pattern specifications, execution monitoring, state sequences, Software systems, Concrete]
Automatic generation of content management systems from EER-based specifications
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
ERW (entity-relationship Web browser) is an innovative open-source system for handling complex databases using a Web browser. Once the details of an enhanced entity-relationship schema have been specified in XML (eXtended Markup Language), ERW generates a complete application that lets the user interact with the database. Then, specification percolation makes it possible to customize heavily the application while maintaining the flexibility of a model-driven approach.
[EER-based specifications, Ontologies, Multimedia databases, entity-relationship schema, Web architecture, ERW, model-driven approach, formal specification, Open source software, content management, Authorization, eXtended Markup Language, entity-relationship modelling, automatic generation, Erbium, hypermedia markup languages, Runtime environment, automatic programming, open-source system, Content management, specification percolation, Spatial databases, content management systems, ER Web browser, complex databases, XML, online front-ends, User interfaces, Internet, extended ER]
Automating component adaptation for reuse
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Reuse is a sound and practical design technique in many engineering disciplines. Although successful instances of software reuse are becoming more common, the cost of reuse tends to outweigh the potential benefits. The costs of software reuse include establishing and maintaining a library of reusable components, searching for applicable components to be reused, as well as adapting components toward a solution to a design problem. In this paper, we present a framework, called SPARTACAS, for automating specification-based component retrieval and adaptation. Components that partially satisfy the constraints of a design problem are adapted using adaptation architectures. Adaptation architectures modify the behavior of a software component by imposing interactions with other components. Based on the functionality specified in the problem and the partially-matched component, a sub-problem that specifies the missing functionality is synthesized. The sub-problem is used to query the library for components for adaptation. The framework was implemented and evaluated empirically, the results suggest that automated adaptation using architectures successfully promotes software reuse, and hierarchically organizes a solution to a design problem.
[Software maintenance, Costs, reusable components, specification-based component retrieval, Reliability engineering, component specification, adaptation architectures, formal specification, Engines, software component, Design engineering, software architecture, Acoustical engineering, Computer architecture, sequential adaptation, object-oriented programming, software reuse, Maintenance engineering, automated component adaptation, alternative adaptation, Software libraries, SPARTACAS, Software quality, software reusability, parallel adaptation]
VUML: a viewpoint oriented UML extension
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
One of the main challenges of our modern societies is the development of information systems accessible to every citizen with respect to his culture, rights, education etc. A number of such information systems (servers) are now provided on the Web in e-learning, tourism, environment, health, transport, etc. But the development and the maintenance of those systems are not guided by users' profile (viewpoints) and thus such systems are very difficult to adapt, reuse and maintain when a viewpoint must be added/removed/updated. To meet these requirements, we propose an extension of UML called VUML (View based Unified Modeling Language). VUML provides the concept of multiviews component whose goal is to store and deliver information according to users' viewpoints. A multiviews component consists of a default view and a set of specific views related to the base through an extension relation. This approach allows for dynamic change of viewpoints and offers mechanisms to describe views dependencies. To favor reuse and transition to coding, we propose an implementation generic pattern targeting object-oriented languages.
[Unified modeling language, multiviews component, Database languages, Information systems, software architecture, system maintenance, Web tourism, e-learning, specification languages, system development, Permission, Database systems, information systems, View based Unified Modeling Language, Java, object-oriented programming, Object oriented databases, Object oriented modeling, Web transport, Web health, information storage, Electronic learning, Observatories, system adaptation, VUML, Web environment, information delivery, object-oriented languages, viewpoint oriented UML extension, system reusability]
Automated environment generation for software model checking
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
A key problem in model checking open systems is environment modeling (i.e., representing the behavior of the execution context of the system under analysis). Software systems are fundamentally open since their behavior is dependent on patterns of invocation of system components and values defined outside the system but referenced within the system. Whether reasoning about the behavior of whole programs or about program components, an abstract model of the environment can be essential in enabling sufficiently precise yet tractable verification. In this paper, we describe an approach to generating environments of Java program fragments. This approach integrated formally specified assumptions about environment behavior with sound abstractions of environment implementations to form a model of the environment. The approach is implemented in the Bandera environment generator (BEG) which we describe along with our experience using BEG to reason about properties of several nontrivial concurrent Java programs.
[Java, Costs, open systems, program verification, NASA, BEG, Independent component analysis, Computational Intelligence Society, software model checking, Yarn, formal specification, automated environment generation, Bandera environment generator, software architecture, Space technology, compiler generators, environment modeling, Open systems, Software systems, sound abstractions, Context modeling, tractable verification, Java program fragments, nontrivial concurrent programs]
Theoretical foundations of updating systems
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Software systems inevitably require update and revision during their lifetime. The concept of features is often used to model system update: a feature is a unit of functionality which may be integrated into a base system. Possible features of an email client program include: spam filtering; absence messages; selective forwarding; and encryption. In our work, we use AI techniques to understand the operation of feature integration more clearly. In particular, we have taken SMV (symbolic model verifier) feature integrator (SFI), a tool which automates feature integration on systems described using the model checker SMV. Then we have taken update which is an operation of theory change, closely related to belief revision, and defined over propositional logic. We formulate and prove a theorem stating that SFI feature integration is an update operation.
[Filtering, AI techniques, propositional logic, program diagnostics, software systems, electronic mail, belief maintenance, software maintenance, artificial intelligence, feature integration, Computer science, Home computing, system updating, SFI, Telephony, Software systems, SMV feature integrator, theorem proving, Logic, Cryptography, symbolic model verifier, Artificial intelligence, Software engineering]
Tool-assisted unit test selection based on operational violations
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Unit testing, a common step in software development, presents a challenge. When produced manually, unit test suites are often insufficient to identify defects. The main alternative is to use one of a variety of automatic unit test generation tools: these are able to produce and execute a large number of test inputs that extensively exercise the unit under test. However, without a priori specifications, developers need to manually verify the outputs of these test executions, which is generally impractical. To reduce this cost, unit test selection techniques may be used to help select a subset of automatically generated test inputs. Then developers can verify their outputs, equip them with test oracles, and put them into the existing test suite. In this paper, we present the operational violation approach for unit test selection, a black-box approach without requiring a priori specifications. The approach dynamically generates operational abstractions from executions of the existing unit test suite. Any automatically generated tests violating the operational abstractions are identified as candidates for selection. In addition, these operational abstractions can guide test generation tools to produce better tests. To experiment dynamic approach, we integrated the use of Daikon (a dynamic invariant detection tool) and Jtest (a commercial Java unit testing tool). An experiment is conducted to assess this approach.
[Software testing, Java, Costs, program testing, software development, operational violations, detection tool, Programming, unit test suites, Java unit testing tool, tool-assisted unit test selection, Formal specifications, automatic unit test generation, black-box approach, Daikon, Computer science, Runtime, operational abstractions, Automatic testing, Fault detection, priori specifications, Jtest, software engineering]
SPQR: flexible automated design pattern extraction from source code
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Previous automated approaches to discovering design patterns in source code have suffered from a need to enumerate static descriptions of structural and behavioral relationships, resulting in a finite library of variations on pattern implementation. Our approach, system for pattern query and recognition, or SPQR, differs in that we do not seek statically to encode each variant of the patterns that we wish to find. Our system finds pattern variants that were not explicitly defined, but instead are inferred dynamically during code analysis by a theorem prover, providing practical tool support for software construction, comprehension, maintenance, and refactoring. We use a logical inference system to reveal large numbers of patterns and their variations from a small number of definitions by encoding in a formal denotational semantics a small number of fundamental OO concepts (elemental design patterns), encode the rules by which these concepts are combined to form patterns (reliance operators), and encode the structural/behavioral relationships among components of objects and classes (rho-calculus). A chain of fully automated tools provides a path from source code to revealed patterns. We describe our approach in this paper with a concrete example to drive the discussion, accompanied by formal treatment of the foundational topics.
[Software maintenance, elemental design patterns, formal treatment, Drives, automated design pattern extraction, Amorphous materials, structural relationships, formal logic, code analysis, software construction, theorem prover, behavioral relationships, SPQR, Libraries, software refactoring, fundamental OO concepts, Pattern analysis, software comprehension, object-oriented programming, source code, Encoding, Pattern recognition, formal denotational semantics, software maintenance, inference mechanisms, logical inference system, Computer languages, system for pattern query and recognition, rho-calculus, Concrete, Software tools, reliance operators, pattern variants]
Depiction and playout of multi-threaded program executions
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Execution of a shared memory multi-threaded program is non-deterministic even for a fixed input. Consequently, a limited amount of the program behavior should be traced and recorded during run-time. However, if the tracing overheads are too high, we have the risk of slowing down the program considerably and even distorting the program behavior. In this paper, we propose to collect and store only the synchronization dependencies during run-time. These dependences are visualized as a message sequence chart (MSC). We do not record the data dependences across threads resulting from unsynchronized reads and writes of a shared variable. Instead all possible orderings of unsynchronized reads/writes are analyzed post-mortem. To describe all these behaviors, we use an important extension of message sequence charts called live sequence charts (LSC). Our MSC/LSC based description of a multi-threaded program execution can be simulated in an automated manner. This can help in understanding program behavior.
[LSC based description, automatic programming, multi-threading, program behavior, program diagnostics, post-mortem playout, Debugging, run-time synchronization dependences, Read-write memory, MSC based description, Yarn, multithreaded program execution, Multiprocessing systems, synchronisation, Runtime, message sequence chart, live sequence charts, Data visualization, shared memory systems, tracing overheads, Software engineering]
Graph rewriting and transformation (GReAT): a solution for the model integrated computing (MIC) bottleneck
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Graph grammars and transformations (GGT) have been a field of theoretical study for over two decades. However, it has produced only a handful of practical implementations. GGT needs a widely used practical application to exploit its potential. On the other hand model integrated computing (MIC) has grown from the practical standpoint and is widely used and recognized in both industry and practice today. In the MIC approach, developing model-interpreters is time consuming and costly, proving to be a bottleneck. This reduces MIC's reach and impact on the programming community. In this paper I propose to use GGT methodologies to solve MIC's bottleneck problem. The solution should place the MIC technology such that it can play a defining role in the next generation of high-level programming languages.
[Visualization, GReAT, practical implementations, high-level programming languages, Unified modeling language, Metamodeling, high level languages, model integrated computing, Microwave integrated circuits, Analytical models, graph grammars, practical standpoint, software engineering, Performance analysis, rewriting systems, graph grammars and transformations, graph rewriting and transformation, MIC bottleneck, Application software, model-interpreters developing, Intersymbol interference, integrated software, GGT methodologies, Software systems, Computer industry]
Debugging overconstrained declarative models using unsatisfiable cores
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Declarative models, in which conjunction and negation are freely used, are susceptible to unintentional overconstraint. Core extraction is a new analysis that mitigates this problem in the context of a checker based on reduction to SAT (systems analysis tools). It exploits a recently developed facility of SAT solvers that provides an "unsatisfiable core" of an unsatisfiable set of clauses, often much smaller than the clause set as a whole. The unsatisfiable core is mapped back into the syntax of the original model, showing the user fragments of the model found to be irrelevant. This information can be a great help in discovering and localizing overconstraint, and in some cases pinpoints it immediately. The construction of the mapping is given for a generalized modeling language, along with a justification of the soundness of the claim that the marked portions of the model are irrelevant. Experiences in applying core extraction to a variety of existing models are discussed.
[program debugging, SAT solvers, Debugging, computability, soundness justification, Specification languages, model debugging, Risk analysis, overconstrained declarative models, Analytical models, systems analysis, systems analysis tools, Safety, unsatisfiable cores, overconstraint discovery, constraint language, core extraction, overconstraint localizing, Testing, Software engineering]
Visual constraint diagrams: runtime conformance checking of UML object models versus implementations
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
This paper presents visual constraint diagrams (VCD), an extension to UML (Unified Modeling Language) object diagrams for expressing constraints over object models. VCD allows designers to express well-formedness constraints that cannot be expressed using class diagrams alone; an example of such a constraint is that a linked list data structure cannot have any loops. VCD offers two advances over existing techniques: (1) they allow constraints to be expressed within the visual notation of UML, without resorting to complex textual notations such as OCL; and (2) VCD can be checked at runtime, increasing the value of design documents to developers. An editor and a checker for VCD have been implemented as part of the Rosetta software design tool.
[program debugging, Computer aided software engineering, runtime conformance checking, program testing, visual constraint diagrams, Unified modeling language, Meetings, Laboratories, UML object models, Rosetta software design tool, linked list data structure, Runtime, Software design, well-formedness constraints, specification languages, software engineering, constraint handling, object-oriented programming, Unified Modeling Language, Documentation, Data structures, Programming profession, VCD, Software systems, UML visual notation]
Certifying measurement unit safety policy
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Measurement unit safety policy checking is a topic in software analysis concerned with ensuring that programs do not violate basic principles of units of measurement. Such violations can hide significant domain-specific errors which are hard or impossible to find otherwise. Measurement unit analysis by means of automatic deduction is addressed in this paper. We draw general design principles for measurement unit certification tools and discuss our prototype for the C language, which includes both dynamic and static checkers. Our approach is based on assume/assert annotations of code, which are properly interpreted by our deduction-based tools and ignored by standard compilers. We do not modify the language in order to support units. The approach can be extended to incorporate other safety policies without great efforts.
[static checkers, program verification, measurement unit certification tools, Software safety, C language, formal logic, software architecture, Measurement units, Program processors, Prototypes, Libraries, Software measurement, code assert annotations, domain-specific errors, program diagnostics, automatic deduction, deduction-based tools, measurement unit analysis, certifying measurement unit safety policy, inference mechanisms, Certification, certification, Computer science, Computer languages, software analysis, code assume annotations, Packaging, dynamic checkers, software metrics]
Extending diagnosis to debug programs with exceptions
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Even with modern software development methodologies, the actual debugging of source code, i.e., location and identification of errors in the program when errant behavior is encountered during testing, remains a crucial part of software development. To apply model-based diagnosis techniques which have long been state of the art in hardware diagnosis, for automatic debugging a model of a given program must be automatically created from the source code. This work describes a model that reflects the sequential execution semantics of the Java language, including exceptions and unstructured control flow, thereby providing unprecedented scope in the application of model-based diagnosis to programs. Notably, this approach omits the strict view of a component representing one statement of earlier work and provides a more flexible mapping from code to model.
[program debugging, Java, program diagnosis, sequential execution, program testing, software development, program diagnostics, source code, model-based diagnosis, error identification, Java language, software engineering, automatic model debugging, error location, Software engineering, hardware diagnosis]
Semi-automatic fault localization and behavior verification for physical system simulation models
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Mathematical modeling and simulation of complex physical systems are emerging as key technologies in engineering. Modern approaches to physical system simulation allow users to specify simulation models with the help of equation-based languages. Due to the high-level declarative abstraction of these languages program errors are extremely hard to find. This paper presents an algorithmic semi-automated debugging framework for equation-based modeling languages. We show how program slicing and dicing performed at the intermediate code level combined with assertion checking techniques can automate, to a large extent, the error finding process and behavior verification for physical system simulation models.
[program debugging, fault diagnosis, program verification, semiautomated debugging framework, program errors, assertion checking techniques, program dicing, Physics computing, software packages, error finding process, software engineering, complex physical systems, Mathematical model, program slicing, Object oriented modeling, Computational modeling, Computer simulation, fault localization, mathematical simulation, physical system simulation, equation-based modeling languages, Software debugging, Equations, Programming profession, Computer bugs, high-level declarative abstraction, mathematical modeling, behavior verification, Software engineering]
What test oracle should I use for effective GUI testing?
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Test designers widely believe that the overall effectiveness and cost of software testing depends largely on the type and number of test cases executed on the software. In this paper we show that the test oracle used during testing also contributes significantly to test effectiveness and cost. A test oracle is a mechanism that determines whether software executed correctly for a test case. We define a test oracle to contain two essential parts: oracle information that represents expected output; and an oracle procedure that compares the oracle information with the actual output. By varying the level of detail of oracle information and changing the oracle procedure, a test designer can create different types of test oracles. We design 11 types of test oracles and empirically compare them on four software systems. We seed faults in software to create 100 faulty versions, execute 600 test cases on each version, for all 11 types of oracles. In all, we report results of 660,000 test runs on software. We show (1) the time and space requirements of the oracles, (2) that faults are detected early in the testing process when using detailed oracle information and complex oracle procedures, although at a higher cost per test case, and (3) that employing expensive oracles results in detecting a large number of faults using relatively smaller number of test cases.
[Software testing, System testing, Costs, program testing, graphical user interfaces, software testing, graphical user interface, Educational institutions, test oracle, Application software, oracle procedure, empirical studies, oracle space requirement, Computer science, test cost, Fault detection, oracle information, Software systems, test effectiveness, software engineering, GUI testing, oracle time requirements, Graphical user interfaces, Software engineering]
An empirical study on groupware support for software inspection meetings
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Software inspection is an effective way to assess product quality and to reduce the number of defects. In a software inspection, the inspection meeting is a key activity to agree on collated defects, to eliminate false positives, and to disseminate knowledge among the team members. However, inspection meetings often require high effort and may lose defects found in earlier inspection steps due to ineffective meeting techniques. Only few tools are available for this task. We have thus been developing a set of groupware tools to lower the effort of inspection meetings and to increase their efficiency. We conducted an experiment in an academic environment with 37 subjects to empirically investigate the effect of groupware tool support for inspection meetings. The main findings of the experiment are that tool support considerably lowered the meeting effort, supported inspectors in identifying false positives, and reduced the number of true defects lost.
[groupware tool, Automation, Costs, program testing, groupware support, Collaborative software, program diagnostics, software development management, software inspection, Inspection, product quality assessment, software quality, software maintenance, inspection meeting, false positives elimination, Software quality, Object detection, groupware, Collaborative work, Systems engineering and theory, knowledge dissemination, Meeting planning, Large-scale systems, software tools]
The feature signatures of evolving programs
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
As programs evolve, their code increasingly becomes tangled by programmers and requirements. This mosaic quality complicated program comprehension and maintenance. Many of these activities can benefit from viewing the program as a collection of features. We introduce an inexpensive and easily comprehensible summary of program changes called the feature signature and investigate its properties. We find a remarkable similarity in the nature of feature signatures across multiple nontrivial programs, developers and magnitude changes. This indicates that feature signatures are a meaningful notion worth studying. We then show numerous applications of feature signatures to software evolution, establishing their utility.
[System testing, multiple nontrivial magnitude changes, program testing, Humans, Control systems, Batteries, feature signatures, test suites, software evolution, conceptual changes, software architecture, multiple nontrivial developers, multiple nontrivial programs, program comprehension, object-oriented programming, Documentation, Application software, software maintenance, program changes, Programming profession, Computer science, program maintenance, Software systems, Software tools, evolving programs]
An infrastructure to support meta-differencing and refactoring of source code
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
The proposed research aims to construct an underlying infrastructure to support (semi) automated construction of refactorings and system wide transformation via a fine grained syntax level differencing approach. We term this differencing approach meta-differencing as it has additional knowledge of the types of entities being differenced. The general approach is built on top of an XML representation of the source code, specifically srcXML by J. Maletic et al. (2002). This representation explicitly embeds high level syntactic information within the source code in such a way as to not interfere with program development and maintenance. Because both the source code and the difference are represented in XML, the transformational language, XSLT, can be used to model these changes. We propose to develop an environment (development/maintenance) that automatically generates XSLT programs based on changes to a program.
[Algorithm design and analysis, syntax level differencing, SouRce Code Markup Language, system wide transformation, transformational language, History, Data mining, Information analysis, software architecture, eXtended Markup Language, Extensible StyLesheet Language, Robustness, hypermedia markup languages, automatic programming, srcXML, automated refactorings construction, source code refactoring, XSLT, software maintenance, source code meta-differencing, XML representation, Computer science, program development, program maintenance, XML, programming environments, Software engineering]
Aspectizing server-side distribution
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
We discuss how a collection of domain-specific and domain-independent tools can be combined to "aspectize" the distributed character of server-side applications, to a much greater extent than with prior efforts. Specifically, we present a framework that can be used with a large class of unaware applications to turn their objects into distributed objects with minimal programming effort. Our framework is developed on top of three main components: AspectJ (a high-level aspect language), XDoclet (a low-level aspect language), and NRMI (a middleware facility that makes remote calls behave more like local calls). We discuss why each of the three components offers unique advantages and is necessary for an elegant solution, why our approach is general, and how it constitutes a significant improvement over past efforts to isolate distribution concerns.
[object-oriented programming, XDoclet, NRMI, high level languages, domain-specific tools, low-level aspect language, programming effort, middleware facility, aspectizing, server-side distribution, distributed objects, remote procedure calls, remote calls, domain-independent tools, distributed object management, distributed programming, AspectJ, Software engineering, middleware, high-level aspect language]
Applying AutoBayes to the analysis of planetary nebulae images
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
We take a typical scientific data analysis task, the analysis of planetary nebulae images taken by the Hubble Space Telescope, and describe how program synthesis can be used to generate the necessary analysis programs from high-level models. We describe the AutoBayes synthesis system, discuss its fully declarative specification language, and present the automatic program derivation starting with the scientists' original analysis.
[Uncertainty, Hydrogen, high-level models, Electronic mail, AutoBayes synthesis system, planetary nebulae, astronomy computing, specification languages, Gaussian model, belief networks, Pattern analysis, specification language, image processing, Data analysis, planetary nebulae images, Instruments, NASA, Hubble Space Telescope, code derivation, program synthesis, Specification languages, automatic program derivation, physics computing, Image analysis, scientific data analysis, Gaussian processes, Telescopes, Bayesian networks, statistical analysis]
Visual specification of concurrent systems
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
The work on a visual formalism for specification of concurrent systems is presented. It is proposed to match requirements of state-of-the-art component-based design methods. Special emphasis is given to specification of heterogeneous systems in which the different models of computation can be mixed together. We briefly summarize recent research related to the topic and give a sketch of the basic ideas for definition of the proposed language. The already achieved results of our work are presented as well.
[Computational modeling, Design methodology, Unified modeling language, Petri nets, formal visual language, data-flow based approach, Application software, visual languages, formal specification, state-based approach, component-based design methods, Software design, UML, concurrency control, concurrent systems, specification languages, heterogeneous systems specification, Software systems, Hardware, visual formalism, visual specification, Informatics, Software engineering]
Parallel breadth-first search LTL model-checking
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
We propose a practical parallel on-the-fly algorithm for enumerative LTL (linear temporal logic) model checking. The algorithm is designed for a cluster of workstations communicating via MPI (message passing interface). The detection of cycles (faulty runs) effectively employs the so called back-level edges. In particular, a parallel level-synchronized breadth-first search of the graph is performed to discover back-level edges. For each level, the back-level edges are checked in parallel by a nested depth-first search to confirm or refute the presence of a cycle. Several optimizations of the basic algorithm are presented and advantages and drawbacks of their application to distributed LTL model-checking are discussed. Experimental implementation of the algorithm shows promising results.
[workstation clusters, program verification, parallel breadth-first search, LTL model checking, faulty runs detection, MPI, temporal logic, Parallel algorithms, Concurrent computing, Clustering algorithms, cycle detection, parallel on-the-fly algorithm, Computer networks, back-level edges, Workstations, Logic, message passing, message passing interface, linear temporal logic, Explosions, linear systems, State-space methods, tree searching, Reachability analysis, workstation cluster, Software engineering]
Automated software testing using a metaheuristic technique based on Tabu search
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
The use of techniques for automating the generation of software test cases is very important as it can reduce the time and cost of this process. The latest methods for automatic generation of tests use metaheuristic search techniques, i.e. genetic algorithms and simulated annealing. There is a great deal of research into the use of genetic algorithms to obtain a specific coverage in software testing but there is none using the metaheuristic Tabu search technique. In this paper, we explain how we have created an efficient testing technique that combines Tabu search with Korel chaining approach. Our technique automatically generates test data in order to obtain branch coverage in software testing.
[Software testing, System testing, metaheuristic technique, Costs, Automation, program testing, simulated annealing, automated software testing, Programming, Korel chaining approach, automatic test generation, genetic algorithms, Genetic algorithms, Computer science, heuristic programming, Automatic testing, Tabu search, metaheuristic search techniques, Simulated annealing, Software quality, automatic test data generation, software engineering, branch coverage, search problems]
A type system for statically detecting spreadsheet errors
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
We describe a methodology for detecting user errors in spreadsheets, using the notion of units as our basic elements of checking. We define the concept of a header and discuss two types of relationships between headers, namely is-a and has-a relationships. With these, we develop a set of rules to assign units to cells in the spreadsheet. We check for errors by ensuring that every cell has a well-formed unit. We describe an implementation of the system that allows the user to check Microsoft Excel spreadsheets. We have run our system on practical examples, and even found errors in published spreadsheets.
[Educational programs, spreadsheet error detection, headers, Scientific computing, program diagnostics, user errors, Microsoft Excel spreadsheets, spreadsheet programs, is-a relationship, Spatial databases, Spreadsheet programs, Application software, has-a relationship, Programming profession, Computer science, Computer languages, type system, Computer errors, software engineering, Software engineering]
Analysis of inconsistency in graph-based viewpoints: a category-theoretical approach
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Eliciting the requirements for a proposed system typically involves different stakeholders with different expertise, responsibilities, and perspectives. Viewpoints-based approaches have been proposed as a way to manage incomplete and inconsistent models gathered from multiple sources. In this paper, we propose a category-theoretical framework for the analysis of fuzzy viewpoints. Informally, a fuzzy viewpoint is graph in which the elements of a lattice are used to specify the amount of knowledge available about the details of nodes and edges. By defining an appropriate notion of morphism between fuzzy viewpoints, we construct categories of fuzzy viewpoints and prove that these categories are (finitely) complete. We then show how colimits can be employed to merge the viewpoints and detect the inconsistencies that arise independent of any particular choice of viewpoint semantics. We illustrate an application of the framework through a case-study showing how fuzzy viewpoints can serve as a requirements elicitation tool in reactive systems.
[category-theoretical approach, Merging, graph theory, reactive systems, Lattices, fuzzy set theory, Programming, requirements elicitation tool, Application software, formal specification, Information analysis, Computer science, graph-based viewpoints, Fuzzy sets, fuzzy viewpoints, inconsistency detection, category theory, Logic, inconsistency analysis, Software engineering, Fuzzy systems]
Test suite design for code generation tools
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
In model-based development, executable specifications (models) are used for the design of the software to be developed. New techniques allow the automatic generation of compact code directly from the model via so-called code generators. However, at present, code generators do not possess the same quality characteristics as C or ADA compilers which have been proven in use. The use of test suites, which make it possible to check compilers systematically, is also a promising approach for code generators. This paper describes the design of such a test suite for code generators, and introduces a new testing approach for code generator transformations.
[System testing, Automatic programming, program testing, program verification, software development, automatic code generators, software design, Application software, test suite, Certification, code generation tools, Embedded software, Flowcharts, Software design, code generator transformations, compiler generators, Embedded system, C compilers, Character generation, model-based development, software engineering, Software tools, executable specifications, ADA compilers]
Automation for exception freedom proofs
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Run-time errors are typically seen as unacceptable within safety and security critical software. The SPARK approach to the development of high integrity software addresses the problem of run-time errors through the use of formal verification. Proofs are constructed to show that each run-time check will never raise an error, thus proving freedom from run-time exceptions. Here we build upon the success of the SPARK approach by increasing the level of automation that can be achieved in proving freedom from exceptions. Our approach is based upon proof planning and a form of abstract interpretation.
[program verification, run-time exceptions, Hydrogen, exception freedom proofs, SPARK approach, safety-critical software, Software safety, software quality, programming languages, software architecture, Runtime, automation, formal verification, high integrity software, safety critical software, Computer security, Automation, abstract interpretation, security critical software, Sparks, Application software, run-time errors, Computer languages, proof planning, Computer errors, Formal verification]
Model checking software requirement specifications using domain reduction abstraction
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
As an automated verification and validation tool, model checking can be quite effective in practice. Nevertheless, model checking has been quite inefficient when dealing with systems with data variables over a large (or infinite) domain, which is a serious limiting factor for its applicability in practice. To address this issue, we have investigated a static abstraction technique, domain reduction abstraction, based on data equivalence and trajectory reduction, and implemented it as a prototype extension of the symbolic model checker NuSMV. Unlike on-the-fly dynamic abstraction techniques, domain reduction abstraction statically analyzes specifications and automatically produces an abstract model which can be reused over time; a feature suitable for regression verification.
[Air safety, program verification, program testing, validation tool, Control systems, formal specification, Prototypes, Automatic control, Software prototyping, automatic programming, trajectory reduction, NASA, symbolic model checker, Medical control systems, software requirement specifications, static abstraction technique, domain reduction abstraction, Computer science, model checking, data equivalence, NuSMV, regression verification, Software systems, Temperature control, automated verification]
Testing database transaction concurrency
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Database application programs are often designed to be executed concurrently by many users. By grouping related database queries into transactions, DBMS (database management system) can guarantee that each transaction satisfies the well-known ACID properties: atomicity, consistency, isolation, and durability. However, if a database application is decomposed into transactions in an incorrect manner, the application may fail when executed concurrently due to potential offline concurrency problems. This paper presents a dataflow analysis technique for identifying schedules of transaction execution aimed at revealing concurrency faults of this nature, along with techniques for controlling the DBMS or the application so that execution of transaction sequences follows generated schedules. The techniques have been integrated into AGENDA, a tool set for testing relational database application programs. Preliminary empirical evaluation is presented.
[transaction processing, concurrency faults, program testing, ACID properties, Relational databases, consistency, Concurrent computing, Fault diagnosis, Information science, database transaction, Database systems, software engineering, database queries, Testing, relational database application, Data analysis, database management system, dataflow analysis, atomicity, concurrency testing, database application programs, durability, isolation, Transaction databases, Application software, relational databases, Programming profession, offline concurrency, transaction sequences, concurrency control, AGENDA, DBMS]
Deriving user interface requirements from densely interleaved scientific computing applications
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Deriving user interface requirements is a key step in user interface generation and maintenance. For single purpose numeric routines, user interface requirements are relatively simple to derive. However, general numeric packages, which are solvers for entire classes of problems, are densely interleaved with strands shared and mixed among user options. This complexity forms a significant barrier to the derivation of user interface requirements and therefore to user interface generation and maintainance. Our methodology uses a graph representation to find potential user decision points implied by the control structure of the code. This graph is then iteratively refined to form a decision point diagram, a state machine representation of all possible user traversals through a user interface for the underlying code.
[Scientific computing, graphical user interfaces, Reverse engineering, graph theory, automated software engineering, formal specification, user interface requirements, Automatic control, graph representation, state machine representation, Graphical user interfaces, hypermedia markup languages, user interface generation, reverse engineering, Application software, software maintenance, user interface maintenance, Computer science, interleaved scientific computing, XML, User interfaces, Packaging, user decision points, decision point diagram, Software engineering, eXtensible Markup Language]
Overview of OpenModel-based validation with partial information
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Multi-stakeholder distributed systems (MSDS), such as the Internet email and instant messaging systems, and e-business Web service networks, raise new challenges for users, developers, and systems analysts. Traditional requirements engineering, validation, and debugging approaches cannot handle two primary problems of MSDS: the lack of consistent high level requirements and the ignorance problem caused by lack of communication among stakeholders. OpenModel described by R. Hall (2002) addresses this ignorance problem: each MSDS node publishes a behavioral model of itself so that remote stakeholders can reason about their interactions with it. However, stakeholders will typically wish to hold back private state information, such as user identities and cryptographic keys. An OpenModel-based validation tool must tolerate missing information and yet still give useful analyses where possible. These paper overviews OMV, a novel approach to validation in the face of partial information based upon symbolic simulation of OpenModel models. We briefly illustrate our studies of the OMV tool in the domains of email and instant messaging.
[program debugging, Protocols, program verification, electronic mail, electronic messaging, OpenModel-based validation, Control systems, Electronic mail, requirements debugging, formal specification, Design engineering, MSDS, symbolic simulation, e-business Web service networks, Web and internet services, groupware, Automatic control, e-mail, software tools, state information, OpenModel validator, IP networks, cryptographic keys, requirements validation, Debugging, multistakeholder distributed systems, instant messaging systems, requirements engineering, Web services, OMV tool, Systems engineering and theory, Internet]
An approach for tracing and understanding asynchronous architectures
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Applications built in a strongly decoupled, event-based interaction style have many commendable characteristics, including ease of dynamic configuration, accommodation of platform heterogeneity, and ease of distribution over a network. It is not always easy to humanly grasp the dynamic behavior of such applications, since many threads are active and events are asynchronously (and profusely) transmitted. We present a set of requirements for an aid to assist in the human understanding and exploration of the behavior of such applications through the incremental refinement of rules for determining causality relationships between messages sent among components. A prototype tool is presented, indicating one viable approach to meeting these requirements. Experience with the tool reinforces some of the requirements and indicates others.
[Humans, asynchronous architectures, prototype tool, Yarn, formal specification, Embedded software, asynchronous transmission, software architecture, Prototypes, Computer architecture, software tools, distributed network, Software prototyping, message passing, Debugging, Application software, incremental refinement, message causality, event-based interaction, causality relationships, Connectors, platform heterogeneity, message capture, dynamic configuration, Clocks]
On the automatic evolution of an OS kernel using temporal logic and AOP
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Automating software evolution requires both identifying precisely the affected program points and selecting the appropriate modification at each point. This task is particularly complicated when considering a large program, even when the modifications appear to be systematic. We illustrate this situation in the context of evolving the Linux kernel to support Bossa, an event-based framework for process-scheduler development. To support Bossa, events must be added at points scattered throughout the kernel. In each case, the choice of event depends on properties of one or a sequence of instructions. To describe precisely the choice of event, we propose to guide the event insertion by using a set of rules, amounting to an aspect that describes the control-flow contexts in which each event should be generated. In this paper, we present our approach and describe the set of rules that allows proper event insertion. These rules use temporal logic to describe sequences of instructions that require events to be inserted. We also give an overview of an implementation that we have developed to automatically perform this evolution.
[Unix, Protocols, Linux kernel, Automatic logic units, temporal logic, software evolution, automatic evolution, Operating systems, Kernel, Contracts, operating system kernels, automatic programming, object-oriented programming, AOP, Bossa, control-flow contexts, operating system, Scattering, Debugging, instruction sequences, software maintenance, process-scheduler development, Programming profession, Linux, OS kernel, Software engineering]
Fault localization with nearest neighbor queries
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
We present a method for performing fault localization using similar program spectra. Our method assumes the existence of a faulty run and a larger number of correct runs. It then selects according to a distance criterion the correct run that most resembles the faulty run, compares the spectra corresponding to these two runs, and produces a report of "suspicious" parts of the program. Our method is widely applicable because it does not require any knowledge of the program input and no more information from the user than a classification of the runs as either "correct" or "faulty". To experimentally validate the viability of the method, we implemented it in a tool, Whither, using basic block profiling spectra. We experimented with two different similarity measures and the Siemens suite of 132 programs with injected bugs. To measure the success of the tool, we developed a generic method for establishing the quality of a report. The method is based on the way an "ideal user" would navigate the program using the report to save effort during debugging. The best results obtained were, on average, above 50%, meaning that our ideal user would avoid looking half of the program.
[fault diagnosis, Navigation, similar program spectra, program diagnostics, fault localization, Debugging, Electronic mail, software quality, Whither, Programming profession, Nearest neighbor searches, Siemens suite, Computer science, Runtime, Computer bugs, block profiling spectra, distance criterion, software tools, nearest neighbor queries, Testing, Software engineering]
Detecting requirements interactions: a three-level framework
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
This paper deals with the problem of requirements interaction. We introduce a three level framework to detect requirements interactions at different levels of cost, time, and complexity. Level 2 where we use semiformal methods to detect interactions contains the main contribution of the research. Also we combine existing approaches (e.g. informal and formal) with our semiformal approach to provide a comprehensive framework for developers to use. The approach is illustrated using two case studies, one from the telecommunications domain and the other one being a lift control system. The results obtained are very encouraging with regards to the time and effort spent on requirements interaction detection.
[Productivity, Knowledge engineering, requirements interaction, Costs, Drives, Control systems, Telecommunication control, formal specification, telecommunications domain, Design engineering, software architecture, semiformal methods, Robustness, three-level framework, interaction detection, lift control system, Formal verification, Software engineering]
Refactoring C with conditional compilation
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Refactoring, an important technique for increasing flexibility of the source code, can be applied with much ease and efficiency by using automated tools. There is currently a lack of refactoring tools for C with full support for preprocessor directives because directives complicate refactorings in many ways. This paper describes refactoring of C programs in the presence of conditional compilation directives and how we propose to support them in a refactoring tool.
[program debugging, object-oriented programming, Data preprocessing, source code refactoring, software quality, software maintenance, C language, preprocessor directives, program compilers, Programming profession, refactoring tools, conditional compilation, C refactoring, program processors, Linux, C programs, software tools, Kernel, Usability, Testing, Software engineering, automated tools]
Unspeculation
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Modern architectures, such as the Intel Itanium, support speculation, a hardware mechanism that allows the early execution of expensive operations possibly even before it is known whether the results of the operation are needed. While such speculative execution can improve execution performance considerably, it requires a significant amount of complex support code to deal with and recover from speculation failures. This greatly complicates the tasks of understanding and re-engineering speculative code. This paper describes a technique for removing speculative instructions from optimized binary programs in a way that is guaranteed to preserve program semantics, thereby making the resulting "unspeculated" programs easier to understand and more amenable to reengineering using traditional reverse engineering techniques.
[modern architectures, Software maintenance, program verification, support speculation, Reverse engineering, Delay, parallel programming, unspeculation, program semantics preservation, speculative instructions, Computer architecture, Hardware, Logic, reverse engineering, software maintenance, reverse engineering techniques, Computer science, Computer aided instruction, systems re-engineering, Software libraries, hardware mechanism, Processor scheduling, unspeculated programs, re-engineering speculative code, optimized binary programs]
Automating relative debugging
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
The creation of a new program version based on an existing version is known as software evolution. In 1994, Abramson and Sosic proposed relative debugging to assist users to locate errors in programs developed with software evolutionary techniques. Relative debugging is a paradigm described by D. Abramson et al. (1996) that enables programmers to locate errors by comparing the developed (evolved) program with the original (existing) program as they are concurrently executed. The aim of the proposed research is to further enhance the currently defined paradigm by (partially or completely) automating the process of relatively debugging. We investigate the possibility of automatically identifying the data structures and program points, normally performed by the user, where values should be equivalent during program execution. Minimizing the user's involvement will reduce the cost of enhancing, maintaining and porting software, and has the potential to provide significant productivity gains on current practices in software development.
[Software maintenance, program debugging, software prototyping, define-use chain navigation tool, Humans, relative debugging automation, software evolution, software porting, data structures, software enhancement, debugging enhancement, program points, Autoguard, Productivity, Technological innovation, automatic programming, DUCT, software development, Debugging, Data structures, program comparison, data flow browser, software maintenance, Information technology, Computer science, software portability, Australia, error location, process automation, Software engineering]
A Java component model for evolving software systems
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
This paper presents a component model for designing and implementing flexible software components in Java. Our model defines a mapping of how the fundamental concepts of component-based development (CBD) should be implemented using the object-oriented (OO) constructs, available in the Java programming language. The benefit of this mapping is to shorten the distance between component-based software architecture and its implementation, enhancing the reusability, adaptability and maintainability of component-based software systems.
[Java, Software maintenance, object-oriented programming, Object oriented modeling, software prototyping, CBD, Java component model, Java programming language, software maintainability, component-based software architecture, software maintenance, Connectors, Computer languages, software architecture, object-oriented constructs, Software design, Software architecture, software adaptability, evolving software systems, Packaging, software reusability, Software systems, Concrete, component-based development]
Communicating requirements using end-user GUI constructions with argumentation
18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.
None
2003
Unsuccessful communication is often at the root of inadequate requirements specification according to C. Potts et al. (1994). This can lead to requirements that do not capture complete stakeholder expectations. Stakeholders can include managers, software engineers, end-users, clients, etc. End-users provide a rich source of information about a system as they will directly interact with the final system. They also tend to have a solid knowledge of the domain including the tasks being automated. Thus, a major goal early in the software engineering process is gathering meaningful requirements from end-users. This paper explores the use of mock end-user graphical interface construction supplemented with textual argumentation as a means of communicating software requirements information to software requirements analysts and providing automated assistance for requirements analysts examining this information.
[Terminology, graphical user interfaces, textual argumentation, software engineering process, communicating requirements, formal specification, software requirements, Information analysis, Engineering management, requirements specification, requirements analysts, Graphical user interfaces, multiple end-user constructions, Information resources, graphical user inteface, Natural languages, Documentation, Computer science, systems analysis, end-user GUI constructions, Solids, automated assistance, requirements elicitation, Software engineering, requirements gathering]
Automated dynamic reconfiguration using AI planning
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The goal of this work is to automate the failure recovery in distributed component based systems using a planning based approach in dynamic reconfiguration. Our approach comprises of a three step process i.e. sense-plan-act (SPA). 'Sense' is the process of sensing the state of the system. 'Plan' is the process of finding a target configuration and a subsequent plan that takes the system from the present configuration to a target configuration. 'Act' is the process of executing the plan on the system. This work presents the preliminary ideas and the current status of this work.
[Availability, Automation, sense-plan-act process, Computerized monitoring, Process planning, Humans, distributed processing, AI planning, software maintenance, automated dynamic reconfiguration, distributed component based systems, Middleware, system recovery, Computer science, planning (artificial intelligence), Operating systems, Writing, failure recovery automation, Artificial intelligence]
A computational framework for supporting software inspections
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Software inspections improve software quality by the analysis of software artifacts, detecting their defects for removal before these artifacts are delivered to the following software life cycle activities. Some knowledge regarding software inspections have been acquired by empirical studies. However, we found no indication that computational support for the whole software inspection process using appropriately such knowledge is available. This paper describes a computational framework whose requirements set was derived from knowledge acquired by empirical studies to support software inspections. To evaluate the feasibility of such framework, two studies have been accomplished: one case study, which has shown the feasibility of using the framework to support inspections, and an experimental study that evaluated the supported software inspection planning activity. Preliminary results of this experimental study suggested that unexperienced subjects are able to plan inspections with higher defect detection effectiveness, and in less time, when using this computational framework.
[Costs, computational support, program testing, Decision making, Software performance, Inspection, software quality, Proposals, Guidelines, Computer science, software inspection planning, planning, Processor scheduling, software artifacts, defect detection, computational framework, Software quality, Systems engineering and theory, software life cycle, inspection]
The education of a software engineer
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
A successful software engineer must possess a wide range of skills and talents. Project managers know how difficult it is to find, motivate, and retain such people. Educators face a complementary, and perhaps more challenging, problem: how to prepare such engineers. The challenge of what to teach software engineers evolves over time as technologies, applications, and requirements change. As software technology has rapidly spread through every aspect of modern societies, the challenge of educating software engineers has taken on new form and become more complex and urgent. The author presents the broad outline of an educational program for a complete software engineer. A new curriculum for computer science has been developed based on these ideas and it started in October 2004 at the University of Lugano in Switzerland.
[Knowledge engineering, Software testing, Educational programs, computer science education, software technology, educational program, Lugano University, Unified modeling language, Project management, software engineer education, project managers, Application software, Computer science, Software architecture, Education, educational courses, software engineering, computer science curriculum, Software engineering]
An environment for building a system out of its requirements
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
A toolset for system design and analysis is described. The tool allows individual translated functional requirements to be entered graphically as behavior trees. Once integrated these behavior trees form a problem domain representation of the design. This representation is automatically mapped to CSP to enable model checking with FDR. A number of consistency checks on the design can be performed. Examples are used to illustrate the results produced by the toolset.
[CSP, communicating sequential processes, Buildings, Natural languages, system design, formal specification, consistency checking, System analysis and design, Graphics, system analysis, Component architectures, problem domain representation, Tree graphs, formal verification, model checking, FDR, systems analysis, design behavior trees, Genetics, Libraries, functional requirements, Australia, visual programming, Formal verification]
ScriptEase: generative design patterns for computer role-playing games
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Recently, some researchers have argued that generative design patterns (GDPs) can leverage the obvious design re-use that characterizes traditional design patterns into code re-use. This work provides additional evidence that GDPs are both useful and productive. Specifically, the current state-of-the-art in the domain of computer games is to script individual game objects to provide the desired interactions for each game adventure. We use BioWare Corp.'s popular Neverwinter Nights computer role-playing game to show how GDPs can be used to generate game scripts. This is a particularly good domain for GDPs, since game designers often have little or no programming skills. We demonstrate our approach using a new GDP tool called ScriptEase.
[Art, object-oriented programming, Economic indicators, ScriptEase, Natural languages, Documentation, generative design patterns, programming skills, program compilers, Programming profession, code reuse, Neverwinter Nights computer role-playing game, Software architecture, code generation, Character generation, Prototypes, computer games, Games, Production, game scripts, object-oriented methods, GDP tool, scripting languages]
Introduction to doctoral symposium
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
false
[Feedback, Atmosphere, Abstracts, Collaborative work, Software engineering]
T-UPPAAL: online model-based testing of real-time systems
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The goal of testing is to gain confidence in a physical computer based system by means of executing it. More than one third of typical project resources are spent on testing embedded and real-time systems, but still it remains ad-hoc, based on heuristics, and error-prone. Therefore systematic, theoretically well-founded and effective automated real-time testing techniques are of great practical value. Testing conceptually consists of three activities: test case generation, test case execution and verdict assignment. We present T-UPPAAL-a new tool for model based testing of embedded real-time systems that automatically generates and executes tests "online" from a state machine model of the implementation under test (IUT) and its assumed environment which combined specify the required and allowed observable (realtime) behavior of the IUT. T-UPPAAL implements a sound and complete randomized testing algorithm, and uses a formally defined notion of correctness (relativized timed input/output conformance) to assign verdicts. Using online testing, events are generated and simultaneously executed.
[Real time systems, System testing, Acoustic testing, program testing, program verification, test case execution, automatic testing, Physics computing, Embedded system, test case generation, embedded systems, randomized testing algorithm, T-UPPAAL, verdict assignment, implementation under test, automated real-time testing, Computer science, Automatic testing, Automata, Computer errors, embedded real-time systems, state machine model, physical computer based system, Clocks, online model-based testing]
On-the-fly generation of k-path tests for C functions
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We propose a novel method, called PathCrawler, for the automatic generation of structural tests satisfying the all-paths criterion or its k-path variant. The source code is instrumented so as to recover the symbolic execution path each time that the program under test is executed. This code is first executed using inputs arbitrarily selected from the input domain. The resulting symbolic path is transformed into a path predicate by projection of the conditions onto the input variables. The next test is obtained by using constraint logic programming to find new input values outside the domain of the path which is already covered. The instrumented code is then executed on this test and so on, until all feasible paths have been covered. Our method combines static and dynamic analysis in a way that avoids the disadvantages of both. It is currently being implemented for the C language.
[Software testing, program testing, Input variables, all-paths criterion, symbolic path transformation, k-path tests, Data mining, C language, Logic testing, program compilers, Sequential analysis, input variables, automatic structural test generation, path predicate, C functions, on-the-fly generation, constraint logic programming, constraint handling, k-path variant, automatic test pattern generation, instrumented source code, Logic programming, Instruments, program diagnostics, static analysis, dynamic analysis, Flow graphs, PathCrawler, Automatic testing, symbolic execution path, Software engineering]
Adaptable concern-based framework specialization in UML
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Architectural-level reuse of software can be achieved in the form of application frameworks. Then, the architecture of a system can be copied from a framework, and the developer is liberated to application development. In this scheme, patterns utilized for specializing the framework play a critical role. Unfortunately, the bigger the specialization pattern, the harder it is to adapt the pattern to a particular design due to increasing number of bindings between pattern roles and the elements of the design. We introduce a tool supported methodology based on UML in which specialization patterns are grouped to match different concerns, i.e. conceptual matters of interest, they treat. Also, user-controlled instantiation of individual patterns is allowed to promote learning the architectural conventions. We argue that this approach overcomes some limitations, especially the lack of adaptability, of wizards that are commonly used for similar purposes.
[object-oriented programming, user-controlled instantiation, Unified Modeling Language, specialization pattern, Unified modeling language, application development, design pattern, software architecture, adaptable concern-based framework specialization, architectural-level software reuse, UML, system architecture, software reusability, tool supported methodology, software tools, application framework, pattern roles, Software engineering]
Automating traceability for generated software artifacts
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Program synthesis automatically derives programs from specifications of their behavior. At a lower level, compilation automatically derives machine code from source code (i.e. from a specification of its behavior). An advantage of program synthesis/compilation, as opposed to manual coding, is that there is a direct link between the specification and the derived program. This link is, however, not very fine-grained: it can be best characterized as program is-derived-from specification. When the generated program needs to be understood or modified, more fine-grained linking is useful. We present a technique for automatically deriving traceability relations between parts of a specification and parts of the synthesized program. The technique is very lightweight and we expect it to work - with varying degrees of success - for any process in which one artifact is automatically derived from another. We illustrate the generality of the technique by applying it to two kinds of automatic generation: synthesis of Kalman filter programs from specifications using the AUTOFILTER program synthesis system, and generation of assembly language programs from C source code using the GCC C compiler. We evaluate the effectiveness of the technique in the latter application.
[assembly language, C source code, Assembly systems, Mission critical systems, C language, program compilers, formal specification, traceability relations, AUTOFILTER program synthesis system, Program processors, GCC C compiler, Software standards, Safety, Kalman filters, automatic generation, automatic programming, program is-derived-from specification, software artifact generated, program diagnostics, NASA, assembly language program generation, program compilation, Application software, Programming profession, machine code, Kalman filter programs, traceability automation, Joining processes, Software engineering, fine-grained linking]
Introduction to tool demonstrations
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
false
[Software testing, System testing, Automatic testing, Electronic equipment testing, Software performance, Inspection, Software tools, Logic testing, Mobile computing, Software engineering]
Mapping template semantics to SMV
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We show how to create a semantics-based, parameterized translator from model-based notations to SMV, using template semantics. Our translator takes as input a specification and a set of user-provided parameters that encode the specification's semantics; it produces an SMV model suitable for model checking. Using such a translator, we can model check a specification that has customized semantics. Our work also shows how to represent complex composition operators, such as rendezvous, in the SMV language, in which there is no matching language construct.
[program verification, Computational modeling, customized semantics, High temperature superconductors, matching language construct, State-space methods, model-based notations, formal specification, complex composition operators, semantics-based parameterized translator, Computer science, Concurrent computing, program interpreters, SMV language, Algebra, model checking, SMV model, specification languages, Writing, template semantics, Carbon capture and storage, user-provided parameters, Software engineering, specification semantics]
CodeCrawler - polymetric views in action
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
CodeCrawler is a language independent software visualization tool. It is mainly targeted at visualizing object-oriented software, and in its newest implementation has become a general information visualization tool. It has been validated in several industrial case studies over the past few years. CodeCrawler strongly adheres to lightweight principles: it implements and visualizes polymetric views, visualizations of software enriched with information such as software metrics and other source code semantics.
[Visualization, Java, object-oriented programming, Object oriented modeling, Size measurement, Displays, polymetric views, Coordinate measuring machines, Software metrics, object-oriented software visualization, source code semantics, Position measurement, language independent software visualization tool, CodeCrawler, program visualisation, Software tools, Software engineering, software metrics]
A scalable approach to user-session based testing of Web applications through concept analysis
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The continuous use of the Web for daily operations by businesses, consumers, and government has created a great demand for reliable Web applications. One promising approach to testing the functionality of Web applications leverages user-session data collected by Web servers. This approach automatically generates test cases based on real user profiles. The key contribution of This work is the application of concept analysis for clustering user sessions for test suite reduction. Existing incremental concept analysis algorithms can be exploited to avoid collecting large user-session data sets and thus provide scalability. We have completely automated the process from user session collection and reduction through replay. Our incremental test suite update algorithm coupled with our experimental study indicate that concept analysis provides a promising means for incrementally updating reduced test suites in response to newly captured user sessions with some loss in fault detection capability and practically no coverage loss.
[Software testing, Algorithm design and analysis, program testing, software reliability, incremental test suite update, fault detection, Web applications, test suite reduction, test case generation, Clustering algorithms, user-session based testing, user session collection, Web server, Government, scalable approach, user-session data, Web servers, Computational Intelligence Society, Application software, Computer science, user profiles, Automatic testing, Fault detection, incremental concept analysis, user session clustering, Internet]
Automated analysis of timing information in UML diagrams
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
This work introduces an approach to adding timing information to UML diagrams for modeling embedded systems. In order to perform automated formal analysis of these UML diagrams with timing information, we extend a previously developed UML formalization framework to provide Promela semantics for the timing elements of the UML diagrams. The paper describes the application of our approach to an electronically controlled steering system obtained from one of our industrial collaborators.
[UML formalization framework, Unified Modeling Language, automated formal analysis, Unified modeling language, UML diagrams, Control systems, diagrams, Information analysis, formal verification, Embedded system, Steering systems, Electronics industry, embedded systems, electronically controlled steering system, Automatic control, timing information, Electrical equipment industry, Timing, Performance analysis, Promela semantics]
From testing to diagnosis: an automated approach
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The need for testing-for-diagnosis strategies has been identified for a long time, but the explicit link from testing to diagnosis is rare. Here, we start with the study of an algorithm for fault localization that consists of crosschecking information collected from test cases execution traces. Analyzing the type of information needed for an efficient localization, we identify the attribute (called dynamic basic block) that restricts the accuracy of a diagnosis algorithm. Based on this attribute, a test criterion is proposed and validated through rigorous case studies: it shows that test cases can be completed to reach a high level of diagnosis accuracy. So, the dilemma between a reduced testing effort (with as few test cases as possible) and the diagnosis accuracy (that needs as much test cases as possible to get more information) is partly solved by selecting only test cases relevant for diagnosis.
[Software testing, fault diagnosis, program testing, program diagnostics, fault localization, dynamic basic block, testing-for-diagnosis strategies, automated approach, test case execution traces, Automatic testing, diagnosis algorithm, information crosschecking, test criterion, Software engineering]
Automated support for framework selection and customization
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
This work proposes a new technique for automated support of selection and customization of application frameworks, by using labeled transition systems (LTSs) together with metrics technique. We model the behavior of the frameworks and the system behavior specified in the requirements specification by using two LTSs respectively. We attach some measures to the LTS of the framework, such as the number of the hot spots to be filled. These measures are used to estimate developer's efforts in filling the hot spots for each implementation. The result of estimating the efforts guides the developers to select the implementation, and the structure of the application-specific codes to be filled in can be automatically generated from the selected implementation. Furthermore, we discuss case studies in the area of Web application, where Struts and Turbine can be used.
[application framework selection, application program interfaces, Object oriented modeling, application framework customization, Programming, application-specific codes, Application software, formal specification, Computer science, Web application, requirements specification, Filling, automated support, labeled transition systems, metric technique, Internet, Turbines, software metrics, system behavior]
Analyzing interaction orderings with model checking
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Human-computer interaction (HCI) systems control an ongoing interaction between end-users and computer-based systems. For software-intensive systems, a graphic user interface (GUI) is often employed for enhanced usability. Traditional approaches to validation of GUI aspects in HCI systems involve prototyping and live-subject testing. These approaches are limited in their ability to cover the set of possible human-computer interactions that a system may allow, since patterns of interaction may be long running and have large numbers of alternatives. In this paper, we propose a static analysis that is capable of reasoning about user-interaction properties of GUI portions of HCI applications written in Java using modern GUI frameworks, such as Swing/spl trade/. Our approach consists of partitioning an HCI application into three parts: the Swing library, the GUI implementation, i.e., code that interacts directly with Swing, and the underlying application. We develop models of each of these parts that preserve behavior relevant to interaction ordering. We describe how these models are generated and how we have customized a model checking framework to efficiently analyze their combination.
[System testing, program verification, graphical user interfaces, graphic user interface, interaction ordering, Control systems, live-subject testing, Prototypes, Computer graphics, HCI systems, human-computer interaction systems control, Graphical user interfaces, GUI implementation, Software prototyping, Java, program diagnostics, static analysis, software-intensive systems, user-interaction properties, Human computer interaction, system prototyping, model checking, User interfaces, Java language, Swing library, human computer interaction, Usability]
Modeling behavior in compositions of software architectural primitives
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Software architectures and architectural styles are increasingly used for designing large-scale software systems. Alfa is a framework for the composition of style-based software architectures from a small set of primitives. It models the behavior of architectural components and connectors as orderings among events at their inputs and outputs. Formalizing such behavior is useful for checking conformance of architectures to their styles. We therefore propose a formal approach that uses data-abstract constraint automata to model the behavior of Alfa's compositions, and to verify their behavioral style conformance. We have also developed an automated conformance analyzer for Alfa.
[Alfa compositions, software architectural styles, automated conformance analyzer, Computational modeling, automata theory, large-scale software systems, Ducts, behavioral style conformance verification, Discrete event simulation, Relays, Connectors, Computer science, software architecture, Content addressable storage, formal verification, Automata, Computer architecture, data abstract constraint automata, Software engineering, style-based software architectures]
Property-oriented test generation from UML Statecharts
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
UML Statecharts is an important modeling formalism with hierarchy, concurrency, broadcast-communication mechanisms and data variables. These facilities help the developers to describe the complex behaviors of an object along its lifecycle naturally. However, the ease of modeling is at the expense of testing: the system developed from Statecharts usually has numerous behaviors and is therefore extremely difficult to receive comprehensive and in-depth testing. This work presents an approach to deriving targeted test sequences from UML Statecharts according to tester-specified temporal logic properties. Using this approach, testing efforts can be focused on specific properties of the system and usually only a small portion of the total behaviors will be tested. This method suits well the occasions when the testers are interested in just some specific properties of the system or when they have to focus on its critical properties in case that limited budget is available.
[Software testing, UML Statecharts, Unified Modeling Language, program testing, Unified modeling language, hierarchy, temporal logic, system specific properties, concurrency, test sequences, tester-specified temporal logic properties, property-oriented test generation, modeling formalism, data variables, Automatic testing, broadcast communication mechanisms, Software engineering]
ScriptEase: generating scripting code for computer role-playing games
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The state-of-the-art in game scripting is to manually script individual game objects that interact in the game. Thousands of non-player characters (NPCs) and props need to be scripted before they play a part in a game adventure. This situation introduces serious concerns about programming effort and reliability. We demonstrate ScriptEase, a tool to facilitate the game scripting process. It is based on generative design patterns for automatic code generation of scripts associated with game objects. ScriptEase is intended for a broad audience, from programmers to game designers and users without programming experience. Game designers can use commonly occurring patterns to generate scripting code without any programming knowledge. This demonstration illustrates the entire process of creating and scripting game props and NPCs.
[Art, object-oriented programming, ScriptEase, generative design patterns, Toy industry, automatic code generation, program compilers, Programming profession, Engines, game design, computer role-playing games, computer games, Games, Computer graphics, scripting code generation, non-player characters, Computer industry, game objects, Hardware, game scripting, Joining processes, Testing]
Architecture for generating Web-based, thin-client diagramming tools
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Thin-client visual design tools can provide a number of advantages over traditional thick-client diagramming tools but are challenging to build. We describe a component-based extension to a thick-client meta-CASE tool that we have been developing that allows any specified diagram editor to be realized as a thin-client tool.
[thick-client metaCASE tool, Computer aided software engineering, object-oriented programming, Shape, Collaborative software, Unified modeling language, user interfaces, Web-based diagramming tools, component-based tool extension, Connectors, diagram editor, Design engineering, software architecture, thin-client visual design tools, thin-client diagramming tools, Computer architecture, online front-ends, Collaborative work, Rendering (computer graphics), computer aided software engineering, Workstations, software tools, Internet]
CHET: a system for checking dynamic specifications
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Software specifications describe how code is supposed to behave. Software model-checking and related activities statically investigate software behavior to ensure that it meets a particular specification. We have developed a tool, CHET, that uses model-checking techniques to do large-scale checking of dynamic specifications in real systems. The tool uses a finite state specification of the properties to check in terms of abstract events. It first finds all instances in the system where this specification is applicable. For each such instance, it creates an abstract model of the software with respect to the events and then checks this model against the specification. Key aspects of CHET include a full inter procedural flow analysis to identify instances of the specifications and restrict the resultant models, and greatly simplified abstract programs that are easily checked. The system has been used to check a variety of specifications in moderate-sized Java programs.
[program testing, program verification, software model-checking, abstract events, procedural flow analysis, History, formal specification, Pressing, Computer architecture, Automatic control, software abstract model, Java program, Large-scale systems, software tools, dynamic specification checking, Java, abstract program, finite state specification, software specifications, Computer science, Software libraries, software behavior, Packaging, CHET tool, Software systems, large-scale checking]
Validating personal requirements by assisted symbolic behavior browsing
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Risks and hazards abound for users of today's large scale distributed telecommunications and e-commerce systems. Service nodes are documented loosely and incompletely, omitting functional details that can violate stakeholder requirements and thwart high level goals. For example, it is not enough to know that a book finding service locates a book for no more than a set price; does the chosen book vendor use an acceptable delivery mode and service? Does it retain or abuse personal information? The OpenModel paradigm provides the basis for a solution: instead of interface information alone, each node publishes a behavioral model of itself. However, large scale and multi-stakeholder systems rule out the use of traditional validation technologies, because state spaces are far too large and incompletely known to support concrete simulation, exhaustive search, or formal proof. Moreover, high level personal requirements like privacy, anonymity, and task success are impossible to formalize completely. This work describes a new methodology, assisted symbolic behavior browsing, and an implemented tool, GSTVIEW, that embodies it to help the user recognize potential violations of high level requirements. The paper also describes case studies of applying GSTVIEW in the domains of email and Web services.
[electronic mail, privacy, Hazards, OpenModel paradigm, State-space methods, Electronic mail, Telecommunication computing, Distributed computing, formal specification, telecommunications, Privacy, Web services, formal verification, Space technology, assisted symbolic behavior browsing, GSTVIEW tool, high level personal requirements validation, e-commerce, anonymity, Large-scale systems, Internet, Books, email, electronic commerce]
A differencing algorithm for object-oriented programs
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
During software evolution, information about changes between different versions of a program is useful for a number of software engineering tasks. For many of these tasks, a purely syntactic differencing may not provide enough information for the task to be performed effectively. This problem is especially relevant in the case of object-oriented software, for which a syntactic change can have subtle and unforeseen effects. We present a technique for comparing object-oriented programs that identifies both differences and correspondences between two versions of a program. The technique is based on a representation that handles object-oriented features and, thus, can capture the behavior of object-oriented programs. We also present JDIFF, a tool that implements the technique for Java programs, and empirical results that show the efficiency and effectiveness of the technique on a real program.
[Software testing, Java, Software maintenance, Costs, object-oriented programming, Collaborative software, syntactic differencing, Merging, object-oriented software, Electronic mail, software maintenance, Information analysis, software evolution, differencing algorithm, object-oriented features, JDIFF tool, Software systems, software engineering, software tools, Java programs, Software engineering]
Instant and incremental transformation of models
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
This work introduces a framework for the instant and incremental transformation of changes among models. It can be configured to understand where and when changes happen in a given source model and the impact these changes have onto a given target model. It can also be configured to select translation rules as needed to update the target model. Incremental transformation is an alternative to the batch transformation and is significantly more efficient in maintaining the synchronicity among large-scale models.
[Software maintenance, Spirals, Scalability, Design methodology, Unified modeling language, instant model transformation, Programming, incremental model transformation, Embedded system, software modeling, Collaborative work, Computer industry, software engineering, Large-scale systems, translation rules]
Aspect mining using event traces
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Aspect mining tries to identify crosscutting concerns in existing systems and thus supports the adaption to an aspect-oriented design. This work describes the first aspect mining approach that detects crosscutting concerns in legacy systems based on dynamic analysis. The analysis uses program traces that are generated in different program executions as underlying data pool. These traces are then investigated for recurring execution patterns based on different constraints, such as the requirement that the patterns have to exist in different calling contexts in the program trace. The implemented approach was evaluated in several case studies over systems with more than 80 kLoC. The tool was able to identify automatically both existing and seeded crosscutting concerns.
[Algorithm design and analysis, Software maintenance, object-oriented programming, event traces, program trace, Heuristic algorithms, aspect mining, Software algorithms, NASA, dynamic analysis, Data mining, software maintenance, Information analysis, program traces, Runtime, execution patterns, legacy systems, Software systems, system monitoring, aspect-oriented design, Timing, program executions]
Verifying interactive Web programs
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Web programs are important, increasingly representing the primary public interfaces of commercial organizations. Unfortunately, Web programs also exhibit numerous flaws. In addition to the usual correctness problems faced by software, Web programs must contend with numerous subtle user operations such as clicking the Back button or cloning and submitting a page multiple times. Many existing Web verification tools fail to even consider, much less effectively handle, these operations. This work describes a model checker designed to identify errors in Web software. We present a technique for automatically generating novel models of Web programs from their source code; these models include the additional control flow enabled by these user operations. In this technique, we exploit a constraint-based approach to avoid overapproximating this control flow; this approach allows us to evade exploding the size of the model. Further, we present a powerful base property language that permits specification of useful Web properties, along with several property idioms that simplify specification of the most common Web properties. Finally, we discuss the implementation of this model checker and a study of its effectiveness.
[property idioms, program verification, constraint-based approach, Web property specification, program compilers, Web verification tools, Databases, control flow, Automatic control, interactive systems, Robustness, interactive Web program verification, Size control, correctness problems, model checker, Testing, Resumes, Cloning, data flow analysis, source code, Application software, Programming profession, commercial organizations, base property language, error identification, Automatic generation control, Internet, Web software, public interfaces]
Evaluation of tool support for architectural evolution
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Evolution of software architectures is, different from architectural design, an area that only few tools have covered. We claim this is due to the lack of support for an important concept of architectural evolution: the notion of architectural design decisions. The absence of this concept in architectural evolution leads to several problems. In order to address these problems, we present a set of requirements that tools should support for architectural evolution. We evaluate existing software architecture tools against these architectural requirements. The results are analyzed and an outline for future research directions for architectural evolution tool support is presented.
[architectural design, software architecture, Software architecture, Augmented virtuality, software architectures, Computer architecture, Computer industry, Software systems, software tools, architectural evolution, Software tools, Software engineering]
Rostra: a framework for detecting redundant object-oriented unit tests
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Object-oriented unit tests consist of sequences of method invocations. Behavior of an invocation depends on the state of the receiver object and method arguments at the beginning of the invocation. Existing tools for automatic generation of object-oriented test suites, such as Jtest and J Crasher for Java, typically ignore this state and thus generate redundant tests that exercise the same method behavior, which increases the testing time without increasing the ability to detect faults. This work proposes Rostra, a framework for detecting redundant unit tests, and presents five fully automatic techniques within this framework. We use Rostra to assess and minimize test suites generated by test-generation tools. We also present how Rostra can be added to these tools to avoid generation of redundant tests. We have implemented the five Rostra techniques and evaluated them on 11 subjects taken from a variety of sources. The experimental results show that Jtest and JCrasher generate a high percentage of redundant tests and that Rostra can remove these redundant tests without decreasing the quality of test suites.
[Software testing, program testing, application program interfaces, Laboratories, JCrasher, method arguments, object-oriented test suites, Rostra, Jtest, Prototypes, software tools, method behavior, Java, object-oriented programming, redundant object-oriented unit test detection, test-generation tools, test suite quality, Computer science, method invocations, Automatic testing, Fault detection, Object detection, Time factors, receiver object, Artificial intelligence]
Using a structure-based configuration tool for product derivation
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Because of the possibly large variability in families of software systems and the complex dependencies between individual software components, product derivation in the context of software-intensive systems is not a trivial task. In this demonstration, we show the domain-independent structure-based configuration tool KONWERK, extended with a knowledge base containing a configuration model representing the domain of car periphery supervision systems. Using this tool, reasoning methods known from structure-based configuration are applied in the area of software-intensive systems. Starting with i) a model describing the variability of already realized software components and ii) a concrete task specification for a specific product, during a knowledge-based product derivation process a description of the needed software components is derived. This description is to be used for realizing, i.e. compiling and linking the desired product. The applicability and usefulness of such an approach will be shown in the demonstration.
[object-oriented programming, KONWERK, knowledge-based product derivation, reasoning methods, Sensor systems, Calibration, Application software, inference mechanisms, Adaptive control, software-intensive systems, Automotive engineering, Aggregates, domain-independent structure-based configuration tool, knowledge based systems, Software systems, Hardware, Concrete, software engineering, car periphery supervision systems, software components, Joining processes]
A statistical model to locate faults at input levels
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We present a statistical model to locate faults at the input level based on the failure patterns and the success patterns. The model neither needs to be fed with software module, code or trace information, nor does it require re-executing the program. To evaluate the model, precision and recall are adopted as the criteria. Five programs are examined and 17 testing experiments are conducted in which the model gains 0.803 in precision and 0.697 in recall on average.
[Chaos, Software testing, Visualization, success patterns, program testing, program verification, Debugging, Fault location, statistical model, failure patterns, software fault tolerance, software module, model evaluation, Computer bugs, statistical analysis, Software engineering]
RCAT: a performance analysis tool
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The existing software performance tools concentrate on identification of program bottlenecks. Although such identification is important, it may not necessarily be the only source of performance deficiency. This work presents a performance tool called RCAT that detects a new type of performance deficiency referred to as redundant computation. Redundant computation is the execution of a statement(s) that does not influence the program output. Our experience showed that significant performance improvements might be achieved as a result of elimination or reduction of redundant computations. RCAT helps the programmers to improve their C programs by localizing the potential performance deficiencies.
[Visual BASIC, Java, Computer aided software engineering, RCAT, Software performance, Educational institutions, performance analysis tool, Application software, C language, software performance tools, Frequency, C programming, Performance analysis, Timing, program bottleneck identification, software performance evaluation, Software engineering]
Establishment of automated regression testing at ABB: industrial experience report on 'avoiding the pitfalls'
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Establishment of automated testing is a high risk as well as a high investment project. It is not uncommon to come across real-life stories about failed test automation attempts in the literature. In fact, it is often cited that failure rates for test automation projects are as high as - if not higher than - any other software development projects. What are the 'common pitfalls ' that all these projects fall into? Can they be avoided via well laid mitigation plans? How does the process of 'pitfall avoidance' relate to the process of 'risk management' in test automation projects? After two consecutive projects, ABB has established automated build functional regression testing of its aspect integrator platform (AIP). This paper reports on the experiences from these projects. To this end, it puts the emphasis on the importance of incorporating the 'common pitfalls' in the formal risk management throughout the automated testing establishment life cycle, and the effects of such an act on the execution and success of the projects. Further, it elaborates on the lessons learnt during practice.
[Software testing, Real time systems, System testing, risk management, Automation, program testing, Project management, Life testing, software development management, regression analysis, Programming, investment project, automated regression testing, software maintenance, aspect integrator platform, automatic testing, software development projects, automated testing establishment life cycle, test automation projects, Automatic testing, Investments, functional regression testing, formal risk management, Risk management]
Evaluating clone detection techniques from a refactoring perspective
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
In the last decade, several researchers have investigated techniques to automatically detect duplicated code in programs exceeding hundreds of thousands lines of code. All of these techniques have known merits and deficiencies, but as of today, little is known on how these techniques fit into the refactoring process of object-oriented systems. This work compares three representative detection techniques (simple line matching, parameterized matching, and metric fingerprints) by means of five small to medium sized cases and analyses the differences between the reported matches. Based on this comparison, we conclude that (1) simple line matching is best suited for a partial, yet advanced restructuring with little effort; (2) metric fingerprints work best for refactoring a system with minimal effort; (3) parameterized matching demands more effort yet allows a more profound, less obvious restructuring of the code.
[Java, simple line matching, object-oriented programming, clone detection, program diagnostics, Buildings, Cloning, code restructuring, Fingerprint recognition, object-oriented systems, code cloning, refactoring process, metric fingerprints, Lab-on-a-chip, parameterized matching, duplicated code detection, Pattern matching, software performance evaluation, Classification tree analysis, Software engineering, software metrics]
Test-suite reduction for model based tests: effects on test quality and implications for testing
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Model checking techniques can be successfully employed as a test case generation technique to generate tests from formal models. The number of tests cases produced, however, is typically large for complex coverage criteria such as MCDC. Test-suite reduction can provide us with a smaller set of test cases that present the original coverage-often a dramatically smaller set. One potential drawback with test-suite reduction is that this might affect the quality of the test-suite in terms of fault finding. Previous empirical studies provide conflicting evidence on this issue. To further investigate the problem and determine its effect when testing formal models of software, we performed an experiment using a large case example of a flight guidance system, generated reduced test-suites for a variety of structural coverage criteria while presenting coverage, and recorded their fault finding effectiveness. Our results show that the size of the specification based test-suites can be dramatically reduced and that the fault detection of the reduced test-suites is adversely affected. In this report we describe our experiment, analyze the results, and discuss the implications for testing based on formal specifications.
[Software testing, Performance evaluation, System testing, program testing, program verification, test-suite reduction, test case generation technique, Software performance, fault detection, structural coverage criteria, Electronic mail, conformance testing, formal specification, DC generators, test quality, specification-based testing, flight guidance system, software testing, NASA, model based testing, automated test generation, Computer science, complex coverage criteria, MCDC, model checking, Automatic testing, Fault detection, formal model testing, fault tolerant computing, specification based test-suites]
Requirements monitoring for service-based systems: towards a framework based on event calculus
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
This work proposes a framework for run-time monitoring of the compliance of systems composed of Web-services with requirements set for them. The framework assumes systems composed of Web-services which are coordinated by a service composition process expressed in BPEL4WS and uses event calculus to specify the requirements to be monitored. These requirements include behavioural properties of the system which are automatically extracted from the specification of its composition process in BPEL4WS and/or assumptions that system providers can specify in terms of events extracted from this specification. Requirements are checked using a variant of techniques for checking integrity constraints against temporal deductive databases.
[Availability, Runtime environment, service composition process, Instruments, temporal deductive databases, Humans, Formal languages, temporal logic, requirements monitoring, Calculus, event calculus, formal specification, service-based systems, Condition monitoring, integrity constraints, Web services, run-time monitoring, BPEL4WS, requirements specification, Software systems, Internet, Deductive databases]
Automated data mapping specification via schema heuristics and user interaction
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Data transformation problems are very common and are challenging to implement for large and complex datasets. We describe a new approach for specifying data mapping transformations between XML schemas using a combination of automated schema analysis agents and selective user interaction. A graphical tool visualizes parts of the two schemas to be mapped and a variety of agents analyze all or parts of the schema, voting on the likelihood of matching subsets. The user can confirm or reject suggestions, or even allow schema matches to be automatically determined, incrementally building up to a fully-mapped schema. An implementation of the mapping specification can then be generated.
[schema heuristics, Java, automated schema analysis agents, program verification, graphical user interfaces, Buildings, Programming, Data engineering, software agents, program compilers, formal specification, Computer science, data transformation problems, Voting, visual graphics tool, XML, Data visualization, Prototypes, data visualisation, automated data mapping specification, selective user interaction, XML schemas, Software tools, graphical tool visualization]
Helping object-oriented framework use and evaluation by means of historical use information
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The effort needed in learning how to use an object-oriented framework can be reduced with information about the way that the framework is normally used. Also, this kind of information can help the evaluation of framework use, by means of the comparison of how a development is near or far the supposed way of framework use. Another advantage related to availability of historical information about framework use is to produce a feedback for framework developers, which can observe design weakness highlighted by frequent presumed framework misuse. This work proposes an approach for quantifying the typical way a framework is used, useful for learning how to use it, and by means of statistical comparison, to evaluate framework-based software as well as frameworks. A set of tools developed for automatic obtaining and for browsing this kind of information is presented too.
[framework use learning, Software maintenance, Data analysis, object-oriented programming, historical use information, Application software, Statistics, Information analysis, Computer science, framework-based software, Databases, object-oriented framework, Feedback, framework use evaluation, Software tools, software performance evaluation]
Interactive visualization of concurrents programs [concurrents read concurrent]
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Our studies focused on applying program visualization techniques to concurrent programs, our approach is oriented to interaction, that is, to let the user the capability to interact with the classes that build up the programs at the sake of a better comprehension of its dynamic behaviour. Our goal is to help the programmers in interactively debugging Java programs with functionalities explicitly devoted to dealing with concurrent programs.
[Java program debugging, Java, program debugging, Debugging, Data structures, reverse engineering, Yarn, interactive concurrent program visualization, Programming profession, Runtime, Computer bugs, software understanding, Data visualization, program visualisation, Software tools, Monitoring, distributed programming]
Combining the box structure development method and CSP
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We combine the box structure development method (BSDM) by Mills et al. (1986) and communicating sequential processes (CSP) by Hoar (1985), with the goal of integrating them into an industrial software development environment. BSDM forms an ideal bridge between the actual system being developed and the abstract models used for formal analysis. CSP complements BSDM by providing the mathematical framework for formal verification, together with its model checker FDR. We present generic algorithms for translating specifications from BSDM into CSP, illustrate how they can be formally verified using FDR and summarise their effectiveness in practice.
[Software testing, industrial software development, System testing, CSP, Scalability, communicating sequential processes, Programming, mathematical framework, abstract models, box structure development method, specification translation, formal specification, Bridges, Concurrent computing, formal verification, FDR, Computer industry, Software systems, formal analysis, Mathematical model, model checker, Formal verification]
Automatic method completion
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Modern software development environments include tools to help programmers write code efficiently and accurately. For example many integrated development environments include variable name completion, method name completion and recently refactoring tools have been added to some environments. This work extends the idea of automatic completion to include completion of the body of a method by employing machine learning algorithms on the near duplicate code segments that frequently exist in large software projects.
[Heart, method name completion, Java, Software maintenance, Machine learning algorithms, variable name completion, automatic method completion, software development environments, Cloning, Educational institutions, integrated development environments, machine learning, program compilers, Programming profession, refactoring tools, Computer science, large software project, duplicate code segment, Detectors, Software systems, software engineering, learning (artificial intelligence)]
Collaborative tools for mobile requirements acquisition
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Conventional software tools that automate or support requirements engineering (RE) activities are typically available on traditional desktop-based workstations. The advancing capabilities of mobile devices significantly enhance the scope of automating RE activities. The vision of the proposed research is to design, implement, and evaluate mobile RE tools that automate and support mobile, collaborative requirements acquisition and negotiation activities. This work describes the pursued research approach and presents results achieved so far.
[Automation, Collaborative tools, mobile collaborative requirement negotiation, formal specification, Embedded software, collaborative tools, desktop-based workstations, mobile computing, requirements engineering, Collaboration, systems analysis, groupware, mobile devices, Collaborative work, Systems engineering and theory, mobile collaborative requirement acquisition, Workstations, software tools, Personal digital assistants, Software tools, Mobile computing]
Modeling and simulation of context-aware mobile systems
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
This work presents an approach for analysis, design and simulation of mobile systems. The approach is developed based on UML-like meta models and graph transformation techniques to support sound methodological principals, formal analysis and refinement. With conceptual and concrete level of modeling and simulation, the approach could support application development and the development of new mobile platforms.
[context-aware mobile systems, Computational modeling, Context awareness, digital simulation, Roaming, mobile system simulation, Application software, Middleware, modelling, formal specification, mobile system analysis, application development, mobile platforms, Analytical models, graph grammars, mobile computing, graph transformation, mobile system design, formal analysis, Concrete, Computer networks, UML-like meta models, Mobile computing, Context modeling]
Verifiable concurrent programming using concurrency controllers
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We present a framework for verifiable concurrent programming in Java based on a design pattern for concurrency controllers. Using this pattern, a programmer can write concurrency controller classes defining a synchronization policy by specifying a set of guarded commands and without using any of the error-prone synchronization primitives of Java. We present a modular verification approach that exploits the modularity of the proposed pattern, i.e., decoupling of the controller behavior from the threads that use the controller. To verify the controller behavior (behavior verification) we use symbolic and infinite state model checking techniques, which enable verification of controllers with parameterized constants, unbounded variables and arbitrary number of user threads. To verify that the threads use a controller in the specified manner (interface verification) we use explicit state model checking techniques, which allow verification of arbitrary thread implementations without any restrictions. We show that the correctness of the user threads can be verified using the concurrency controller interfaces as stubs, which improves the efficiency of the interface verification significantly. We also show that the concurrency controllers can be automatically optimized using the specific notification pattern. We demonstrate the effectiveness of our approach on a Concurrent Editor implementation which consists of 2800 lines of Java code with remote procedure calls and complex synchronization constraints.
[program verification, infinite state model checking, user thread correctness, controller behavior, user interfaces, Yarn, design pattern, modular verification, explicit state model checking, concurrency controller interfaces, complex synchronization constraints, Concurrent computing, Condition monitoring, parameterized constants, controller verification, Automatic control, notification pattern, Safety, Concurrent Editor implementation, symbolic model checking, distributed programming, symbolic state model checking, Java, object-oriented programming, multi-threading, synchronization policy, Concurrency control, Programming profession, interface verification, synchronisation, Computer science, concurrency control, concurrency controllers, Java language, remote procedure calls, behavior verification, Java code, Error correction, verifiable concurrent programming, unbounded variables, arbitrary thread implementation verification]
Parameterized interfaces for open system verification of product lines
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Software product-lines view systems as compositions of features. Each component corresponds to an individual feature, and a composition of features yields a product. Feature-oriented verification must be able to analyze individual features and to compose the results into results on products. Since features interact through shared data, verifying individual features entails open system verification concerns. To verify temporal properties, features must be open to both propositional and temporal information from the remainder of the composed product. This paper addresses both forms of openness through a two-phase technique. The first phase analyzes individual features and generates sufficient constraints for property preservation. The second phase discharges the constraints upon composition of features into a product. We present the technique as well as the results of a case study on an email protocol suite.
[Heart, Protocols, object-oriented programming, open systems, program verification, parameterized interfaces, email protocol suite, temporal information, user interfaces, temporal property verification, propositional information, software product line, open system verification, Optical propagation, two-phase technique, Open systems, Computer architecture, shared data, Software systems, feature-oriented verification, Software engineering, Formal verification, property preservation]
Preface
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Presents the welcome message from the conference proceedings.
[]
Conference committee
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Provides a listing of current committee members.
[]
Steering Committee
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Provides a listing of current committee members.
[]
Program Committee
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Provides a listing of current committee members.
[]
Data-mining synthesised schedulers for hard real-time systems
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The analysis of hard real-time systems, traditionally performed using RMA/PCP or simulation, is nowadays also studied as a scheduler synthesis problem, where one automatically constructs a scheduler which can guarantee avoidance of deadlock and deadline-miss system states. Even though this approach has the potential for a finer control of a hard real-time system, using fewer resources and easily adapting to further quality aspects (memory/energy consumption, jitter minimisation, etc.), synthesised schedulers are usually extremely large and difficult to understand. Their big size is a consequence of their inherent precision, since they attempt to describe exactly the frontier among the safe and unsafe system states. It nevertheless hinders their application in practise, since it is extremely difficult to validate them or to use them for better understanding the behaviour of the system. In this paper, we show how one can adapt data-mining techniques to decrease the size of a synthesised scheduler and force its inherent structure to appear, thus giving the system designer a wealth of additional information for understanding and optimising the scheduler and the underlying system. We present, in particular, how it can be used for obtaining hints for a good task distribution to different processing units, for optimising the scheduler itself (sometimes even removing it altogether in a safe manner) and obtaining both per-task and per-system views of the schedulability of the system
[Real time systems, Energy consumption, deadline-miss system, data mining, Jitter, decision-tree induction, system recovery, hard real-time systems, Analytical models, Control system synthesis, scheduling, software engineering, Performance analysis, RMA-PCP, synthesised schedulers, Computational modeling, deadlock system, safe system, unsafe system, Processor scheduling, schedulability analysis, real-time systems, decision trees, System recovery, Software engineering]
Experiences integrating and scaling a performance test bed generator with an open source CASE tool
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We report on our experiences developing a performance test-bed generator for industrial usage by extending an open-source UML CASE tool. This tool generates client and server code, database configuration and deployment scripts from a high-level software architecture description. It automates the code generation, compilation, deployment and performance metric result collection processes. We identify a range of problems that arose from our previous research on performance test-bed generation that needed to be addressed to scale this automated software engineering technique. We describe a range of approaches we used to solve these problems in our new tool. We then report on industrial deployment and evaluation of our new tool and discuss the effectiveness of these solutions
[Software testing, database configuration, Computer aided software engineering, program testing, public domain software, Software performance, deployment scripts, high-level software architecture description, automated software engineering, program compilers, Open source software, software architecture, industrial deployment, Software architecture, Computer architecture, performance test bed generator, software tools, code compilation, Software prototyping, client-server systems, code deployment, Unified Modeling Language, open source CASE tool, UML CASE tool, Application software, industrial evaluation, architecture analysis, software tool extension, industrial usage, code generation, Automatic testing, client-and-server code, software performance testing, computer aided software engineering, performance metric result collection, Software engineering]
Modeling Web-based dialog flows for automatic dialog control
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
In Web-based applications, the dialog control logic is often hidden in or entwined with the presentation and/or application logic, even if the latter tiers are well-separated. This makes it difficult to control complex dialog structures like nested dialogs, and to reconcile the device-independent business logic with the device-specific interaction patterns required by different clients' I/O capabilities. To avoid continuous re-implementation of the dialog control logic, we present a dialog control framework that is separate from the presentation and business tiers, and manages arbitrarily nested dialog flows on different presentation channels. The framework relies on dialog specifications developed using the dialog flow notation, which are translated into an object-oriented dialog flow model for efficient run-time lookups. This way, the framework automates the dialog control aspect of Web-based application development and leaves only the tasks of implementing the business logic, designing the hypertext pages, and specifying the dialog flow to the developer
[dialog flow specifications, Web-based dialog flow modeling, dialog control logic, dialog control framework, run-time lookups, user interfaces, formal specification, Runtime, Automatic control, interactive systems, Marketing and sales, Books, presentation channels, Logic devices, run time lookups, object-oriented programming, nested dialog flows, Object oriented modeling, device independent business logic, Logic design, device specific interaction patterns, Application software, hypertext page design, Computer science, automatic dialog control, Web-based application development, complex dialog structure control, object-oriented dialog flow model, Telematics, Internet, dialog flow notation]
Inferring specifications to detect errors in code
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
A new static program analysis method for checking structural properties of code is proposed. The user need only provide a property to check; no further annotations are required. An initial abstraction of the code is computed that over-approximates the effect of function calls. This abstraction is then iteratively refined in response to spurious counterexamples. The refinement involves inferring a context-dependent specification for each function call, so that only as much information about a function is used as is necessary to analyze its caller. When the algorithm terminates, the remaining counterexample is guaranteed not to be spurious, but because the program and its heap are finitized, absence of a counterexample does not constitute proof
[Java, program verification, program diagnostics, code abstraction, function calls, context-dependent specification, static program analysis, formal specification, code error detection, Information analysis, Computer science, Detectors, Computer errors, Writing, structural property checking, Iterative algorithms, Performance analysis, Artificial intelligence, Software engineering]
Using transient/persistent errors to develop automated test oracles for event-driven software
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Today's software-intensive systems contain an important class of software, namely event-driven software (EDS). All EDS take events as input, change their state, and (perhaps) output an event sequence. EDS is typically implemented as a collection of event-handlers designed to respond to individual events. The nature of EDS creates new challenges for test automation. In this paper, we focus on those relevant to automated test oracles. A test oracle is a mechanism that determines whether a software executed correctly for a test case. A test case for an EDS consists of a sequence of events. The test case is executed on the EDS, one event at a time. Errors in the EDS may "appear" and later ''disappear" at several points (e.g., after an event is executed) during test case execution. Because of the behavior of these transient (those that disappear) and persistent (those that don't disappear) errors, EDS require complex and expensive test oracles that compare the expected and actual output multiple times during test case execution. We leverage our previous work to study several applications and observe the occurrence of persistent/transient errors. Our studies show that in practice, a large number of errors in EDS are transient and that there are specific classes of events that lead to transient errors. We use the results of this study to develop a new test oracle that compares the expected and actual output at strategic points during test case execution. We show that the oracle is effective at detecting errors and efficient in terms of resource utilization
[Software testing, Automation, Costs, program testing, graphical user interfaces, transient errors, automated test oracles, software intensive systems, test case execution, Educational institutions, Discrete event simulation, Application software, Embedded software, Computer science, automatic testing, resource allocation, persistent errors, Automatic testing, Computer errors, event driven software, resource utilization]
A dataflow language for scriptable debugging
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Debugging is a laborious, manual activity that often involves the repetition of common operations. Ideally, users should be able to describe these repetitious operations as little programs. Debuggers should therefore be programmable, or scriptable. The operating environment of these scripts, however, imposes interesting design challenges on the programming language in which these scripts are written. This paper presents our design of a language for scripting debuggers. The language offers powerful primitives that can precisely and concisely capture many important debugging and comprehension metaphors. The paper also describes a debugger for the Java language built in accordance with these principles. We have implemented this debugger to run alongside the Java virtual machine. The paper includes concrete examples of applying this debugger to programs
[Java, program debugging, Resumes, Debugging, Manuals, scriptable debugging, dataflow language, Virtual machining, Programming profession, Programming environments, Computer science, Computer languages, virtual machines, Java language, program debugger, programming language, Java virtual machine, parallel languages, Software engineering]
Consistency checking in an infrastructure for large-scale generative programming
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Ubiquitous computing increases the pressure on the software industry to produce ever more and error-free code. Two recipes from automated programming are available to meet this challenge: On the one hand, generative programming raises the level of abstraction in software development by describing problems in high-level domain-specific languages and making them executable. On the other hand, in situations where one needs to produce a family of similar programs, product line engineering supports code reuse by composing programs from a set of common assets (or features). AHEAD (algebraic hierarchical equations for application design) is a framework for generative programming and product line engineering that achieves additional productivity gains by scaling feature composition up. Our contribution is GRAFT, a calculus that gives a formal foundation to AHEAD and provides several mechanisms for making sure that feature combinations are legal and that features in themselves are consistent
[Law, program verification, error-free code, high-level domain-specific language, Calculus, GRAFT, ubiquitous computing, formal specification, program compilers, consistency checking, Design engineering, software industry, AHEAD, Large-scale systems, product line engineering, Productivity, algebraic hierarchical equations for application design, automatic programming, Automatic programming, software development, Ubiquitous computing, data integrity, Equations, code reuse, Domain specific languages, large-scale generative programming, software reusability, automated programming, Computer industry]
Heuristic search with reachability tests for automated generation of test programs
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Our research complements the current research on automated specification-based testing by proposing a scheme that combines the setup process, test execution, and test validation into a single test program for testing the behavior of object-oriented classes. The test program can be generated automatically given the desired test cases and closed algebraic specifications of the classes. The core of the test program generator is a partial-order planner which plans the sequence of instructions required in the test program. A first cut implementation of the planner has been presented by Leow et al. (2004) based on simple depth-first search. This paper presents a more efficient and effective heuristic search algorithm that performs reachability tests using the Omega calculator. Test results show that heuristic search with reachability tests significantly reduce the search time required to generate a valid sequence of instructions
[Performance evaluation, Software testing, System testing, test validation, program testing, program verification, Heuristic algorithms, Input variables, automated test program generation, test cases, setup process, test execution, heuristic programming, depth-first search, closed algebraic specifications, instruction sequence, algebraic specification, search problems, automatic programming, Automatic programming, reachability analysis, reachability tests, Data structures, partial-order planner, automated specification-based testing, heuristic search, Computer science, Omega calculator, Automatic testing, object-oriented classes, Software systems]
COMPASS: tool-supported adaptation of interactions
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
This paper presents an aspect-oriented approach and tool to consistently exchange and adapt interactions among software units. This is done by first identifying components, their interactions and interaction patterns. Second, the identified interaction points of components are represented as aspect-oriented ports encapsulating the source elements related to the interaction. The interactions themselves are represented as first-class entities in the form of aspect-oriented connectors connecting the ports of components. These component, port and connector entities constitute an architectural model. Third, the developer reconfigures and adapts interactions exchanging the port and connector entities. This triggers corresponding source code transformations realized as meta programs using the Recoder tool. This approach is implemented as the COMPASS (COM position with aspects) tool, that can currently analyze and transform Java source code based on the infrastructure provided by the Recoder framework. The approach and tool are successfully validated replacing a direct method call between a producer and a consumer component by communication via a buffer object
[direct method call, Protocols, aspect-oriented approach, Recoder framework, aspect-oriented tool, Recoder tool, Programming, aspect-oriented connector, tool-supported adaptation, Electronic mail, architectural model, source code transformation, software architecture, consistently exchange, tool-supported interaction adaptation, buffer object, software tools, Java, object-oriented programming, metaprogram, Java source code, COM position with aspects tool, connector entities, interaction patterns, software unit interactions, aspect-oriented ports, Wrapping, source elements, Connectors, Bridges, software units, Feature extraction, port entities, Software tools, Joining processes, COMPASS tool]
A case study in JML-based software validation
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
This paper reports on a testing case study applied to a small Java application, partially specified in JML. It illustrates that JML can easily be integrated with classical testing tools based on combinatorial techniques and random generation. It also reveals difficulties to reuse, in a testing context, JML annotations written for a proof process.
[Java, System testing, Smart cards, Computer aided software engineering, Automation, Automatic testing, Banking, Specification languages, Lifting equipment, Context modeling]
Automated performance validation of software design: an industrial experience
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Early performance validation allows software developers to address performance issues since the first phases of software life cycle, when crucial decisions on the software system are taken. However, the lack of completely automated methodologies and the need of special skills for early software performance modelling and analysis prevent their introduction in real industrial context. The availability of such automation should facilitate the application of these methodologies on industrial products without delaying the software development process. Goal of this paper is to report our experience in the modelling and analysis of performance requirements of a real telecommunication system at the design level
[automated performance validation, telecommunication system, software design, Software performance, formal specification, telecommunication computing, software system, Delay, software performance modelling, performance requirement modelling, software architecture, Software design, formal verification, Performance analysis, software life cycle, software performance evaluation, Availability, Automation, object-oriented programming, software development, software performance analysis, Application software, performance requirement analysis, Computer industry, Software systems, Context modeling]
A case study of coverage-checked random data structure testing
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We study coverage-checked random unit testing (CRUT), the practice of repeatedly testing units on sequences of random function calls until given code coverage goals are achieved. Previous research has shown that this practice can be a useful complement to traditional testing methods. However, questions remained as to the breadth of its applicability. In this paper, we report on a case study in which we applied CRUT to the testing of two mature public-domain data structures packages. We show that CRUT helped in identifying faults, in debugging, in extracting and specifying actual behaviour, and in achieving greater assurance of the correctness of the debugged software
[Software testing, program debugging, software debugging, Computer aided software engineering, fault diagnosis, program testing, public domain software, fault identification, Fault diagnosis, data structures, software engineering, behaviour specification, Documentation, Debugging, Data structures, coverage-checked random data structure testing, debugging correctness, code coverage goals, Computer science, public-domain data structures packages, Automatic testing, coverage-checked random unit testing, random function calls, Packaging, behaviour extraction, Software engineering]
Understanding aspects via implicit invocation
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Aspect-oriented programming (AOP) promises improved modularity in software design. However, it also presents novel mechanisms and departs from traditional design theory, leaving researchers in need of a theory and developers in need of guidance as to its appropriate use. This paper rests on the idea that the nature and expressive power of AOP lie largely in programming-language-provided implicit invocation (II) mechanisms, with join points as events, pointcuts as event patterns, advice as methods invoked by events, and aspects as classes that also create eventmethod bindings. The contribution of this paper is the idea that exposing the II roots of AOP can expedite development of a theory and practice of AOP. We present a formal reduction from AOP to II, then, as a data point, we show that model checking techniques previously developed for II systems can be used to check formal properties of AOP systems automatically.
[Computer science, Encapsulation, Pediatrics, Software design, Design methodology, Scattering, Concrete, Object oriented programming, Programming profession, Software engineering]
Using a genetic algorithm and formal concept analysis to generate branch coverage test data automatically
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Automatic test generators (ATGs) are an important support tool for large-scale software development. Contemporary ATGs include JTest that does white box testing down to the method level only and black box testing if a specification exists, and AETG that tests pairwise interactions among input variables. The first automatic test generation approaches were static, based on symbolic execution (Clarke, 1976). Korel suggested a dynamic approach to automatic test data generation using function minimization and directed search (Korel, 1990). A dynamic approach can handle array, pointer, function and other dynamic constructs more accurately than a static approach but it may also be more expensive since the program under test is executed repeatedly. Subsequent ATGs explored the use of genetic algorithms (Jones et al., 1996; Michael et al., 2001; Pargas et al., 1999) and simulated annealing (Tracey et al., 1998). These ATGs address the problem of producing test data for low level code coverage like statement, branch and condition/decision and depend on branch function (Korel, 1990) style instrumentation (Jones et al., 1996; Michael et al., 2001) and/or the program graph (Jones et al., 1996; Pargas et al., 1999). Unlike previous work, our ATG, called genet, produces test data for branch coverage with simpler instrumentation than branch functions, does not use program graphs, and is programming language independent, genet uses a genetic algorithm (GA) (Holland, 1975) to search for tests and formal concept analysis (FCA) (Ganter and Wille, 1999) to organize the relationships between tests and their execution traces. The combination of GA with FCA is novel. Further, genet extends the opportunistic approach of GADGET (Michael et al., 2001) by targeting several uncovered branches simultaneously. The relationships that genet learns provides useful insights for test selection, test maintenance and debugging
[Algorithm design and analysis, Software testing, programming language independent, program debugging, program graph, automatic branch coverage test data generation, program testing, branch-and-condition decision, Input variables, JTest, low level code coverage like statement, Programming, test maintenance, directed search, formal specification, large-scale software development, Genetic algorithms, genetic algorithm, GADGET, Simulated annealing, Large-scale systems, black box testing, automatic test pattern generation, automatic test generators, object-oriented programming, simulated annealing, Instruments, genet, data flow analysis, genetic algorithms, style instrumentation, formal concept analysis, Computer languages, Automatic testing, function minimization, white box testing, branch function, symbolic execution, test selection]
Combination model checking: approach and a case study
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We present combination model checking approach using a SAT-based bounded model checker together with a BDD-based symbolic model checker to provide a more efficient counter example generation process. We provide this capability without compromising the verification capability of the symbolic model checker. The basic idea is to use the symbolic model checker to determine whether or not a property holds in the model. If the property holds, we are done. If it does not, we preempt the counterexample generation and use the SAT-based model checker for this purpose. An application of the combination approach to a version of a Flight Guidance System (FGS) from Rockwell Collins, Inc. shows huge performance gain when checking a collection of several hundred properties.
[Computer science, Computer aided software engineering, Boolean functions, NASA, Performance gain, Data structures, Acceleration, Usability, Counting circuits, Software engineering]
Context-aware code certification
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
One challenging issue in automated software engineering is to ensure safety of software execution in changing contexts. In such a scenario, various users, the "code consumers\
[program debugging, verification condition generator, Costs, constraint solving, context-specific runtime errors, program verification, safety-critical software, context-dependent safety requirements, Floyd-Hoarc verification, HTML, Software safety, context-aware code certification, automated software engineering, formal specification, Counting circuits, bug identification, Runtime, automated safety certification, context-dependent safety checking, safety verification, Java, program diagnostics, Application software, Certification, program specification, metalevel interface specifications, annotated programs, Computer bugs, metalevel reasoning, Software engineering]
ISPIS: a framework supporting software inspection processes
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
This paper describes ISPIS, a computational framework for supporting the software inspection process whose requirements set was derived from knowledge acquired by empirical studies. ISPIS allows the inspection of all artifact types by geographically distributed teams. Specific defect detection support is provided by the integration of external tools. A case study has shown the feasibility of using ISPIS to support real inspections
[Knowledge engineering, program testing, program diagnostics, Software performance, software inspection, Inspection, Programming, Proposals, Distributed computing, Information analysis, Computer science, software process improvement, Systems engineering and theory, ISPIS, Software tools, defect detection support, inspection]
Decision support for test management in iterative and evolutionary development
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Testing resources and time are usually limited, especially by schedules for market-driven products and in fast-paced software development projects. In this paper the author presents his PhD proposal about a framework to support rapid and informed decision-making for test management in the context of iterative and evolutionary development. The objective is to continually focus testing on the value it provides for the project and, thus, enable effective and efficient testing even under uncertainty and schedule pressure
[Software testing, iterative software development, System testing, Costs, Automation, Uncertainty, Robust stability, program testing, software prototyping, test management, Project management, software development management, evolutionary software development, testing resources, Automatic testing, Investments, rapid decision-making, Resource management, informed decision-making]
Formal framework for automated analysis and verification of Web-based applications
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
We present an ongoing Ph.D. research developing a formal approach for modeling an existing Web application using communicating finite automata model. We build the automata from a recorded browsing session. The obtained model could then be used to verify user-defined properties of the application with a model checker. We present an implementation of the approach that uses the model checker Spin
[Software prototyping, finite automata, Spin, communicating finite automata model, Web-based applications, HTML, Application software, automated analysis, Logic testing, formal verification, Automatic testing, Automata, Prototypes, Production, Performance analysis, Internet, automated verification, model checker, Graphical user interfaces]
Decompositional verification of component-based systems-a hybrid approach
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
Component-based software development has been increasingly adopted as a standard engineering practice to build large systems with prefabricated components. Although this development method enjoys the great benefits of reusing valuable software assets, reducing development costs, improving productivity, etc., it also poses serious challenges to the quality assurance problem of component-based systems, since prefabricated components can not be simply trusted and they could be a new source of system failures. Solving this problem is of vital importance to safety-critical and mission-critical systems. This paper introduces a decompositional verification approach for component-based systems through both formal analysis (model-checking) and traditional software testing
[Software testing, Performance evaluation, System testing, program testing, safety-critical software, Programming, system recovery, decompositional verification, Quality assurance, formal verification, formal analysis, Software standards, Standards development, mission-critical systems, object-oriented programming, component-based systems, software testing, software assets, quality assurance problem, prefabricated components, component-based software development, system failures, Certification, Computer science, safety-critical systems, model checking, Software engineering]
Group support for distributed collaborative concurrent software modeling
Proceedings. 19th International Conference on Automated Software Engineering, 2004.
None
2004
The distributed development of software is increasingly widespread, driven by the globalization of companies and business and enabled by the improvements in communication and computing (Grinter et al., 1999). The distributed development of software introduces new aspects of cooperative work in which a greater emphasis is placed upon technological support of the software development process. Software development activities deal with the complexity by constructing and validating models of the application domain. Models are important artifacts used for communication within the organizations, developers and stakeholders as well. Constructing correct, complete, consistent, and unambiguous models is a tough task that needs concurrent participation of multiple users possibly geographically dispersed. Distribution of software development through meetings is harder to control. Distributed meetings play typically a critical role in teamwork. During which large amount of implicit knowledge is exchanged through negotiation and conflicts resolution. Hence, efforts in supporting distributed development should focus on better supporting distributed meetings (Boulila, 2003)). This research focuses on the specific problem of distributed brainstorming and the construction of UML models of software through successive distributed teamwork meetings. In Particular we investigate the issues we considered in designing a unified framework based on CSCW concepts and software engineering to support concurrent object-oriented software analysis and design phases. We present an activity-based model and a prototype called GroupUML based on the framework
[distributed teamwork, Unified Modeling Language, Collaborative software, Object oriented modeling, Globalization, UML software models, CSCW concepts, Companies, Programming, distributed software development, Application software, Distributed computing, Business communication, concurrent object-oriented software analysis, distributed collaborative concurrent software modeling, systems analysis, groupware, Collaborative work, software engineering, cooperative work, Teamwork, concurrent object-oriented software design, GroupUML]
Preface
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Presents the welcome message from the conference proceedings.
[]
Conference Committee
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Provides a listing of current committee members.
[]
Program Committee
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Provides a listing of current committee members.
[]
Program Committee
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Provides a listing of current committee members.
[]
Introduction to tool demonstrations
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Automated software engineering is concerned with how to apply computation to automate or partially automate software engineering tasks to achieve significant improvements in quality and productivity. Tools therefore play a dominant role within automated software engineering. The tool demonstration session at the ASE'06 conference accounts for that by providing a forum to show and discuss new tool developments that highlight scientific contributions to the field of Automated Software Engineering. This year we have 9 tool demonstrations, each of which was reviewed by two reviewers. Each tool demonstration will have a time slot in the conference program, and also will be given a space at the conference site where conference attendees will be able to ask additional questions and see a personal demonstration. The demonstrations to be given this year bring together a diverse set of tools that provide support for main tasks of the software engineering life cycle. The tools presented range from tools supporting software generation and transformation, requirement engineering, software testing, software verification, and software modeling.
[]
Verifying Specifications with Proof Scores in CafeOBJ
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Verifying specifications is still one of the most important undeveloped research topics in software engineering. It is important because quite a few critical bugs are caused at the level of domains, requirements, and/or designs. It is also important for the cases where no program codes are generated and specifications are analyzed and verified only for justifying models of problems in real world. This paper gives a survey of our research activities in verifying specifications with proof scores in CafeOBJ. After explaining fundamental issues and importance of verifying specifications, an overview of CafeOBJ language, the proof score approach in CafeOBJ including its applications to several areas are given. This paper is based on our already published books or papers (Diaconescu and Futatsugi, 1998; Futatsugi et al., 2005), and refers to many of our related publications. Interested readers are invited to look into them
[program debugging, object-oriented programming, program verification, software bugs, program diagnostics, Humans, proof scores, Electronic mail, Formal specifications, Application software, formal specification, software requirements, Equations, Information science, Silver, program codes, Computer bugs, software designs, software engineering, theorem proving, Books, CafeOBJ language, Software engineering, specification verification]
Automatic Property Checking for Software: Past, Present and Future
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Summary form only given. Over the past few years, we have seen several automatic static analysis tools being developed and deployed in industrial-strength software development. I will survey several of these tools ranging from heuristic and scalable analysis tools (such as PREFix, PREFast and Metal), to sound analysis tools based on counter example driven refinement (such as SLAM). Then, I will present two exciting recent developments in counterexample driven refinement: (1) generalizing counterexample driven refinement to work with any abstract interpretation, and (2) combining directed testing with counterexample driven refinement
[Software testing, software property checking, software development, program diagnostics, counterexample driven refinement, Programming, heuristic analysis tools, Metal, Counting circuits, automatic static analysis, Simultaneous localization and mapping, PREFast, Automatic testing, Refining, PREFix, scalable analysis tools, Computer industry, software tools, Software tools, Software engineering]
Automated Information Aggregation for Scaling Scale-Resistant Services
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Machine learning provides techniques to monitor system behavior and predict failures from sensor data. However, such algorithms are "scale resistant" $high computational complexity and not parallelizable. The problem then becomes identifying and delivering the relevant subset of the vast amount of sensor data to each monitoring node, despite the lack of explicit "relevance" labels. The simplest solution is to deliver only the "closest" data items under some distance metric. We demonstrate a better approach using a more sophisticated architecture: a scalable data aggregation and dissemination overlay network uses an influence metric reflecting the relative influence of one node's data on another, to efficiently deliver a mix of raw and aggregated data to the monitoring components, enabling the application of machine learning tools on real-world problems. We term our architecture level of detail after an analogous computer graphics technique
[scalable data aggregation, Machine learning algorithms, Military computing, dissemination overlay network, Computerized monitoring, scale-resistant services, Switches, Sensor systems, system behavior monitoring, machine learning, Computational complexity, Intelligent sensors, software fault tolerance, Condition monitoring, software architecture, Machine learning, system monitoring, Iterative algorithms, learning (artificial intelligence), failure prediction, information aggregation, computational complexity]
Generating Domain-Specific Visual Language Editors from High-level Tool Specifications
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Domain-specific visual language editors are useful in many areas of software engineering but developing such editors is challenging and time-consuming. We describe an approach to generating a wide range of these graphical editors for use as plug-ins to the Eclipse environment. Tool specifications from an existing meta-tool, Pounamu, are interpreted to produce dynamic, multi-view, multiuser Eclipse graphical editors. We describe the architecture and implementation of our approach, examples of its use realizing domain-specific modelling tools, and strengths and limitations of the approach
[Eclipse environment, Computer aided software engineering, Navigation, graphical user interfaces, Unified modeling language, high-level tool specifications, graphical editors, Pounamu metatool, formal specification, Domain specific languages, Computer science, domain-specific modelling, software architecture, Web services, Software architecture, Computer architecture, domain-specific visual language editor, software engineering, software tools, Usability, programming environments, visual programming, Software engineering]
An Automated Formal Approach to Managing Dynamic Reconfiguration
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Dynamic reconfiguration is the process of making changes to software at run-time. The motivation for this is typically to facilitate adaptive systems which change their behavior in response to changes in their operating environment or to allow systems with a requirement for continuous service to evolve uninterrupted. To enable development of reconfigurable applications, we have developed OpenRec, a framework which comprises a reflective component model plus an open and extensible reconfiguration management infrastructure. Recently we have extended OpenRec to verify whether an intended (re)configuration would result in an application's structural constraints being satisfied. Consequently OpenRec can automatically veto proposed changes that would violate configuration constraints. This functionality has been realized by integrating OpenRec with the ALLOY Analyzer tool via a service-oriented architecture. ALLOY is a formal modelling notation which can be used to specify systems and associated constraints. In this paper, we present an overview of the OpenRec framework. In addition, we describe the application of ALLOY to modelling re-configurable component based systems and highlight some interesting experiences with integrating OpenRec and the ALLOY Analyzer
[Adaptive systems, program verification, formal modelling notation, system specification, formal specification, software architecture, Runtime, Software architecture, Engineering management, service-oriented architecture, Availability, object-oriented programming, Power supplies, dynamic reconfiguration management, Service oriented architecture, application structural constraints, reconfigurable applications, OpenRec, Application software, software maintenance, run-time software changes, Power system modeling, adaptive systems, configuration management, Software systems, ALLOY Analyzer tool]
Differencing and Merging of Architectural Views
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Existing approaches to differencing and merging architectural views are based on restrictive assumptions such as requiring view elements to have unique identifiers or exactly matching types. We propose an approach based on structural information by generalizing a published polynomial-time tree-to-tree correction algorithm (that detects inserts, renames and deletes) into a novel algorithm to additionally detect restricted moves and support forcing and preventing matches between view elements. We incorporate the algorithm into tools to compare and merge component-and-connector (C&amp;C) architectural views. Finally, we provide an empirical evaluation of the algorithm on case studies to find and reconcile interesting divergences between architectural views
[component-and-connector architectural view, Costs, architectural view merging, Merging, Cloning, trees (mathematics), polynomial time tree-to-tree correction algorithm, Connectors, software architecture, Runtime, Tree graphs, Software architecture, architectural view differencing, Investments, Computer architecture, structural information, Polynomials, computational complexity]
An Empirical Comparison of Automated Generation and Classification Techniques for Object-Oriented Unit Testing
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Testing involves two major activities: generating test inputs and determining whether they reveal faults. Automated test generation techniques include random generation and symbolic execution. Automated test classification techniques include ones based on uncaught exceptions and violations of operational models inferred from manually provided tests. Previous research on unit testing for object-oriented programs developed three pairs of these techniques: model-based random testing, exception-based random testing, and exception-based symbolic testing. We develop a novel pair, model-based symbolic testing. We also empirically compare all four pairs of these generation and classification techniques. The results show that the pairs are complementary (i.e., reveal faults differently), with their respective strengths and weaknesses
[Software testing, test input generation, automated test classification, fault diagnosis, object-oriented programming, program testing, Object oriented modeling, exception handling, symbolic testing, Formal specifications, automated test generation, Programming profession, Computer science, Automatic testing, symbolic execution, object-oriented programs, Concrete, Artificial intelligence, Random sequences, Software engineering, object-oriented unit testing, random testing]
Command-Form Coverage for Testing Database Applications
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
The testing of database applications poses new challenges for software engineers. In particular, it is difficult to thoroughly test the interactions between an application and its underlying database, which typically occur through dynamically-generated database commands. Because traditional code-based coverage criteria focus only on the application code, they are often inadequate in exercising these commands. To address this problem, we introduce a new test adequacy criterion that is based on coverage of the database commands generated by an application and specifically focuses on the application-database interactions. We describe the criterion, an analysis that computes the corresponding testing requirements, and an efficient technique for measuring coverage of these requirements. We also present a tool that implements our approach and a preliminary study that shows the approach's potential usefulness and feasibility
[Software testing, System testing, program testing, program diagnostics, command-form coverage, database application testing, Doped fiber amplifiers, Educational institutions, Data engineering, Application software, database management systems, Database languages, dynamically-generated database commands, Runtime, Feedback, Automata, application-database interactions, test adequacy criterion, software engineering]
Automatic Identification of Bug-Introducing Changes
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Bug-fixes are widely used for predicting bugs or finding risky parts of software. However, a bug-fix does not contain information about the change that initially introduced a bug. Such bug-introducing changes can help identify important properties of software bugs such as correlated factors or causalities. For example, they reveal which developers or what kinds of source code changes introduce more bugs. In contrast to bug-fixes that are relatively easy to obtain, the extraction of bug-introducing changes is challenging. In this paper, we present algorithms to automatically and accurately identify bug-introducing changes. We remove false positives and false negatives by using annotation graphs, by ignoring non-semantic source code changes, and outlier fixes. Additionally, we validated that the fixes we used are true fixes by a manual inspection. Altogether, our algorithms can remove about 38%~51% of false positives and 14%~15% of false negatives compared to the previous algorithm. Finally, we show applications of bug-introducing changes that demonstrate their value for research
[program debugging, annotation graphs, automatic bug identification, software bugs, nonsemantic source code changes, graph theory, Project management, Inspection, bug fixing, History, Risk analysis, Software debugging, false negatives, Open source software, false positives, configuration management, outlier fix, Computer bugs, Fixtures, Computer industry, Electrical equipment industry]
Modularity Analysis of Logical Design Models
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Traditional design representations are inadequate for generalized reasoning about modularity in design and its technical and economic implications. We have developed an architectural modeling and analysis approach, and automated tool support, for improved reasoning in these terms. However, the complexity of constraint satisfaction limited the size of models that we could analyze. The contribution of this paper is a more scalable approach. We exploit the dominance relations in our models to guide a divide-and-conquer algorithm, which we have implemented it in our Simon tool. We evaluate its performance in case studies. The approach reduced the time needed to analyze small but representative models from hours to seconds. This work appears to make our modeling and analysis approach practical for research on the evolvability and economic properties of software design architectures
[Simon tool, divide and conquer methods, Scalability, Peer to peer computing, logical design models, architectural modeling, divide-and-conquer algorithm, formal specification, modularity analysis, Computer science, software design architecture, software architecture, Software design, architectural analysis, constraint satisfaction, Automata, Computer architecture, model dominance relation, Constraint theory, reasoning about programs, constraint handling, Context modeling, Software engineering]
A Portable Compiler-Integrated Approach to Permanent Checking
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Program checking technology is now a mature technology, but is not yet used on a large scale. We identify one cause of this gap in the decoupling of checking tools from the everyday development tools. To radically change the situation, we explore the integration of simple user-defined checks into the core of every development process: the compiler. The checks we implement express constrained reachability queries in the control flow graph taking the form "from x to y avoiding z\
[reachability queries, Linux kernel, Software performance, data flow graphs, C language, partial evaluation (compilers), language-independent code pattern matching, Program processors, permanent checking, compiler integration, Kernel, reachability analysis, code patterns, data flow analysis, syntactic information, Flow graphs, programming language semantics, program checking, Programming profession, dataflow information, control flow graph, Linux, portable compiler, semantic information, Software tools, Usability, Pattern matching, Software engineering]
Integrating and Scheduling an Open Set of Static Analyses
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
To improve the productivity of the development process, more and more tools for static software analysis are tightly integrated into the incremental build process of an IDE. If multiple interdependent analyses are used simultaneously, the coordination between the analyses becomes a major obstacle to keep the set of analyses open. We propose an approach to integrating and scheduling an open set of static analyses which decouples the individual analyses and coordinates the analysis executions such that the overall time and space consumption is minimized. The approach has been implemented for the Eclipse IDE and has been used to integrate a wide range of analyses such as finding bug patterns, detecting violations of design guidelines, or type system extensions for Java
[Productivity, Java, program debugging, Data analysis, software development, program diagnostics, static analysis, type theory, Eclipse IDE, Application software, bug pattern finding, Information analysis, Guidelines, design guideline violation detection, type system extension, scheduling, Data models, software engineering, Buffer overflow, Pattern analysis, Software tools]
Reverse Engineering of Design Patterns from Java Source Code
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Recovering design patterns can enhance existing source code analysis tools by bringing program understanding to the design level. This paper presents a new, fully automated pattern detection approach. The new approach is based on our reclassification of the GoF patterns by their pattern intent. We argue that the GoF pattern catalog classifies design patterns in the forward-engineering sense; our reclassification is better suited for reverse engineering. Our approach uses lightweight static program analysis techniques to capture program intent. This paper also describes our tool, PINOT, that implements this new approach. PINOT detects all the GoF patterns that have concrete definitions driven by code structure or system behavior. Our tool is faster, more accurate, and targets more patterns than existing pattern detection tools. PINOT has been used successfully in detecting patterns in Java AWT, JHotDraw, Swing, Apache Ant, and many other programs and packages
[JHotDraw, Reverse engineering, static program analysis, Data mining, program compilers, Software design, program understanding, PINOT, Java AWT, Prototypes, Swing, pattern detection, Pattern analysis, Apache Ant, Java, object-oriented programming, Java source code, program diagnostics, reverse engineering, design pattern recovery, Pattern recognition, Pattern classification, Packaging, design pattern classification, Concrete, source code analysis, code structure, system behavior]
ArchTrace: Policy-Based Support for Managing Evolving Architecture-to-Implementation Traceability Links
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Traditional techniques of traceability detection and management are not equipped to handle evolution. This is a problem for the field of software architecture, where it is critical to keep synchronized an evolving conceptual architecture with its realization in an evolving code base. ArchTrace is a new tool that addresses this problem through a policy-based infrastructure for automatically updating traceability links every time an architecture or its code base evolves. ArchTrace is pluggable, allowing developers to choose a set of traceability management policies that best match their situational needs and working styles. We discuss ArchTrace, its conceptual basis, its implementation, and our evaluation of its strengths and weaknesses in a retrospective analysis of data collected from a 20 month period of development of Odyssey, a large-scale software development environment. Results are promising: with respect to the ideal set of traceability links, the policies applied resulted in 95% precision at 89% recall
[Software maintenance, policy-based infrastructure, Data analysis, traceability detection, Navigation, software development, Buildings, Programming, Odyssey, traceability management, traceability link update, software maintenance, ArchTrace tool, software evolution, Computer science, configuration management, software architecture, conceptual architecture, Software architecture, code base evolution, Computer architecture, Large-scale systems, software tools, Informatics]
Automating Software Traceability in Very Small Companies: A Case Study and Lessons Learne
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
There is a wide consensus on the benefits of software traceability. However, traceability techniques are still not commonly used in industry $typically only in larger companies and if mandated by standards such as the CMMI or ISO 15504. Success stories in small companies are quite rare. However, small companies represent a significant share of the IT industry and a better understanding of their needs is essential for the research community. This paper presents APIS, a traceability environment we developed and introduced in a very small software company. We discuss the traceability approach and report on key lessons learned. We have found in the project that comparably simple automation techniques are surprisingly effective. We believe that the lessons learned in this project are relevant for researchers and practitioners facing similar challenges
[IT industry, Automation, program diagnostics, Laboratories, software traceability, ISO standards, software company, APIS traceability environment, software architecture, Databases, Inhibitors, Engineering management, Computer industry, software tools, Standards development, Software engineering, Business]
Bogor/Kiasan: A k-bounded Symbolic Execution for Checking Strong Heap Properties of Open Systems
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
This paper presents Kiasan, a bounded technique to reason about open systems based on a path sensitive, relatively sound and complete symbolic execution instead of the usual compositional reasoning through weakest precondition calculation that summarizes all execution paths. Kiasan is able to check strong heap properties, and it is fully automatic and flexible in terms of its cost and the guarantees it provides. It allows a user-adjustable mixed compositional/non-compositional reasoning and naturally produces error traces as fault evidence. We implemented Kiasan using the Bogor model checking framework and observed that its performance is comparable to ESC/Java on similar scales of problems and behavioral coverage, while providing the ability to check much stronger specifications
[Java, Software maintenance, Costs, fault diagnosis, open systems, Bogor model checking, specification checking, program diagnostics, Software performance, error traces, Programming, noncompositional reasoning, formal specification, fault evidence, Computer languages, heap property checking, Kiasan technique, Open systems, reasoning about programs, k-bounded symbolic execution, Software reusability, Software tools, compositional reasoning, Contracts]
Security Analysis of Crypto-based Java Programs using Automated Theorem Provers
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Determining the security properties satisfied by software using cryptography is difficult: Security requirements such as secrecy, integrity and authenticity of data are notoriously hard to establish, especially in the context of cryptographic interactions. Nevertheless, little attention has been paid so far to the verification of such implementations with respect to the secure use of cryptography. We propose an approach to use automated theorem provers for first-order logic to formally verify crypto-based Java implementations, based on control flow graphs. It supports an abstract and modular security analysis by using assertions in the source code. Thus large software systems can be divided into small parts for which a formal security analysis can be performed more easily and the results composed. The assertions are validated against the program behavior in a run-time analysis. Our approach is supported by the tool JavaSec available as open-source and validated in an application to a Java Card implementation of the Common Electronic Purse Specifications and the Java implementation Jessie of SSL
[program verification, first-order logic, formal specification, Common Electronic Purse Specifications, Open source software, source code assertions, data secrecy, Runtime, formal verification, control flow graphs, abstract security analysis, Automatic control, Performance analysis, theorem proving, Cryptography, Logic, Jessie, JavaSec tool, Java, crypto-based Java programs, Data security, program diagnostics, flow graphs, cryptography, modular security analysis, data integrity, data authenticity, run-time analysis, Flow graphs, Java Card, Software systems, data privacy, automated theorem provers]
Accurate Centralization for Applying Model Checking on Networked Applications
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Software model checkers can be applied directly to single-process programs, which typically are multithreaded. Multi-process applications cannot be model checked directly. While multiple processes can be merged manually into a single one, this process is very labor-intensive and a major obstacle towards model checking of client-server applications. Previous work has automated the merging of multiple applications but mostly omitted network communication. Remote procedure calls were simply mined, creating similar results for simple cases while removing much of the inherent complexities involved. Our goal is a fully transparent replacement of network communication. Other language features were also modeled more precisely than in previous work, resulting in a program that is much closer to the original. This makes our approach suitable for testing, debugging, and software model checking. Due to the increased faithfulness of our approach, we can treat a much larger range of applications than before
[Software testing, Java, client-server systems, program debugging, multi-threading, program testing, program verification, Object oriented modeling, Merging, Debugging, network communication, multiprocess applications, Application software, software model checking, Yarn, networked applications, Sockets, single-process programs, client-server applications, Virtual manufacturing, multithreaded programs, Informatics]
Using Decision Trees to Predict the Certification Result of a Build
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Large teams of practitioners (developers, testers, etc.) usually work in parallel on the same code base. A major concern when working in parallel is the introduction of integration bugs in the latest shared code. These latent bugs are likely to slow down the project unless they are discovered as soon as possible. Many companies have adopted daily or weekly processes which build the latest source code and certify it by executing simple manual smoke/sanity tests or extensive automated integration test suites. Other members of a team can then use the certified build to develop new features or to perform additional analysis, such as performance or usability testing. For large projects the certification process may take a few days. This long certification process forces team members to either use outdated or uncertified (possibly buggy) versions of the code. In this paper, we create decision trees to predict ahead of time the certification result of a build. By accurately predicting the outcome of the certification process, members of large software teams can work more effectively in parallel. Members can start using the latest code without waiting for the certification process to be completed. To perform our study, we mine historical information (code changes and certification results) for a large software project which is being developed at the IBM Toronto Labs. Our study shows that using a combination of project attributes (such as the number of modified subsystems in a build and certification results of previous builds), we can correctly predict 69% of the time that a build will fail certification. We can as well correctly predict 95% of the time if a build will pass certification
[Performance evaluation, Software testing, program debugging, integration bugs, program testing, program diagnostics, performance testing, Merging, source code, Control systems, latent bugs, software build, code changes, Certification, certification, configuration management, usability testing, Automatic testing, Computer bugs, decision trees, software certification, Performance analysis, Decision trees, Usability]
Managing the Complexity of Large Free and Open Source Package-Based Software Distributions
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
The widespread adoption of free and open source software (FOSS) in many strategic contexts of the information technology society has drawn the attention on the issues regarding how to handle the complexity of assembling and managing a huge number of (packaged) components in a consistent and effective way. FOSS distributions (and in particular GNU/Linux-based ones) have always provided tools for managing the tasks of installing, removing and upgrading the (packaged) components they were made of While these tools provide a (not always effective) way to handle these tasks on the client side, there is still a lack of tools that could help the distribution editors to maintain, on the server side, large and high-quality distributions. In this paper we present our research whose main goal is to fill this gap: we show our approach, the tools we have developed and their application with experimental results. Our contribution provides an effective and automatic way to support distribution editors in handling those issues that were, until now, mostly addressed using ad-hoc tools and manual techniques
[component installation, Assembly systems, public domain software, information technology, Manuals, component removal, Programming, open source package-based software distribution, component upgrade, software maintenance, Information technology, Open source software, Electrooptic effects, configuration management, Technology management, Software packages, Linux, Packaging, complexity management, Software engineering]
Concurrent Engineering support in Software Engineering
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
The evolution of software engineering methodology, from waterfall to spiral, from spiral to agile, indicates that high concurrency, iterative development and short cycles are key factors for effective software engineering. It is widely accepted that supporting (i.e., formalizing controlling, automating and optimizing) concurrent engineering processes is needed to increase predictability of cost, quality and development time. Unfortunately, current systems (e.g., workflows, software configuration management, ...) are too simple and deterministic; they do not include real support for concurrent engineering. We claim this shortcoming is one of the major reasons why current workflow and process support do not significantly help in the support of software engineering. In this paper we present the Celine system, which extends workflows with the definition of high-level executable description of concurrent engineering and therefore contributes to provide effective control over cost, quality and development time
[Spirals, Celine system, Control systems, concurrent engineering, software configuration management, Concurrent computing, configuration management, Engineering management, software process improvement, Automatic control, iterative development, Cost function, Software systems, Concurrent engineering, software engineering, Iterative methods, Software engineering]
Mining Aspects from Version History
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Aspect raining identifies cross-culling concerns in a program, to help migrating it to an aspect-oriented design. Such concerns may not exist from the beginning, but emerge over time. By analysing where developers add code to a program, our history-based aspect mining (BAM) identifies and ranks cross-cutting concerns. We evaluated the effectiveness of our approach with the history of three open-source projects. BAM scales up to Industrial-sized projects: for example, we were able to identify a locking concern that cross-cuts 1284 methods in Eclipse. Additionally, the precision of HAM increases with project size and history: for Eclipse, it reaches 90% for the top-10 candidates
[Java, object-oriented programming, public domain software, Laboratories, Scattering, data mining, Eclipse, History, Helium, open-source projects, Open source software, Computer science, configuration management, version history, System recovery, Lab-on-a-chip, aspect-oriented design, Weaving, history-based aspect mining]
Identifying Refactorings from Source-Code Changes
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Software has been and is still mostly refactored without tool support. Moreover, as we found in our case studies, programmers tend not to document these changes as refactorings, or even worse label changes as refactorings, although they are not. In this paper we present a technique to detect changes that are likely to be refactorings and rank them according to the likelihood. The evaluation shows that the method has both a high recall and a high precision - it finds most of the refactorings, and most of the found refactoring candidates are really refactorings
[source-code changes, data mining, Documentation, Data mining, History, Application software, software changes, Programming profession, Open source software, Computer science, configuration management, refactoring identification, Databases, Computer errors, Software tools]
Sieve: A Tool for Automatically Detecting Variations Across Program Versions
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Software systems often undergo many revisions during their lifetime as new features are added, bugs repaired, abstractions simplified and refactored, and performance improved. When a revision, even a minor one, does occur, the changes it induces must be tested to ensure that invariants assumed in the original version are not violated unintentionally. In order to avoid testing components that are unchanged across revisions, impact analysis is often used to identify code blocks or functions that are affected by a change. In this paper, we present a novel solution to this general problem that uses dynamic programming on instrumented traces of different program binaries to identify longest common subsequences in strings generated by these traces. Our formulation allows us to perform impact analysis and also to detect the smallest set of locations within the functions where the effect of the changes actually manifests itself. Sieve is a tool that incorporates these ideas. Sieve is unobtrusive, requiring no programmer or compiler intervention to guide its behavior. Our experiments on multiple versions of op ensource C programs shows that Sieve is an effective and scalable tool to identify impact sets and can locate regions in the affected functions where the changes manifest. These results lead us to conclude that Sieve can play a beneficial role in program testing and software maintenance
[Software testing, software revisions, Software maintenance, code block change identification, Sequences, program testing, Instruments, program diagnostics, Genetic mutations, software systems, open-source C programs, dynamic programming, software maintenance, Computer science, configuration management, Sieve tool, impact analysis, Automatic testing, Computer bugs, program version variation detection, Software systems, program binaries, Dynamic programming, software tools]
From Capability Specifications to Code for Multi-Agent Software
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Current ICT application domains, such as Web services and autonomic computing, call for highly flexible systems, capable of adapting to changing operational environments as well as to user needs. Multi-agent system framework do include mechanisms that make flexibility and adaptability possible. In our research we focus on how to take into account environmental constraints and stakeholder needs in the design of software agent capabilities
[Process design, Production systems, capability specification, Multiagent systems, multi-agent systems, Computational modeling, Computer integrated manufacturing, software agent, Application software, formal specification, software agents, Software design, Web services, multiagent software, Computer architecture, Software agents, autonomic computing]
An Instant Message-Driven User Interface Framework for Thin Client Applications
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Today, thin client applications often rely on the infrastructure of the WWW to deliver their user interfaces (UIs) to clients. While this approach does not require the deployment of application logic on the client, Web-based UIs typically do not provide the same level of usability as window-based UIs. We therefore present a UI framework that combines the flexibility of a thin presentation logic with the usability of a full-featured UI: Our approach uses an XMPP-based instant messaging infrastructure to exchange XUL interface descriptions and events between the application logic on the server and a generic UI rendering engine on the client
[Java, client-server systems, Costs, Protocols, network computers, thin client applications, electronic messaging, XUL interface descriptions, World Wide Web, HTML, user interfaces, instant message-driven user interface, thin presentation logic, XMPP-based instant messaging infrastructure, Engines, Network servers, User interfaces, Rendering (computer graphics), Web-based user interface, user interface rendering engine, Internet, Logic, Usability]
Using communicative acts in interaction design specifications for automated synthesis of user interfaces
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Instead of developing user interfaces (UIs) directly, we argue for specifying an interaction design from which UIs can be automatically synthesized. We present an approach to using communicative acts in high-level specification of interaction design, which is implemented and allows automated synthesis of interfaces for multiple devices. Communicative acts derive from speech act theory and carry desired intentions in interactions. Models of communicative acts, UI domain objects and interaction sequences comprise interaction design specifications in our approach and are based on a metamodel that we have defined. As a result, the usability of a synthesized user interface of a real-world application turned out to be good
[user interface synthesis, Automatic programming, metamodel, Unified Modeling Language, Natural languages, speech act theory, Humans, Context awareness, interaction design specification, user interfaces, Speech synthesis, formal specification, Interactive systems, communicative acts, User interfaces, human computer interaction, Usability, Assembly, Software engineering]
Annotation Inference for Safety Certification of Automatically Generated Code (Extended Abstract)
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Automated code generation is an enabling technology for model-based software development and promises many benefits, including higher quality and reduced turn-around times. However, the key to realizing these benefits is generator correctness: nothing is gained from replacing manual coding errors with automatic coding errors. In this paper, we describe an alternative technique that uses a generic post-generation annotation inference algorithm. We exploit both the highly idiomatic structure of automatically generated code and the restriction to specific safety properties. Since generated code only constitutes a limited subset of all possible programs, the new "eureka" insights required in general remain rare in our case. Since safety properties are simpler than full functional correctness, the required annotations are also simpler and more regular. We can thus use patterns to describe all code constructs that require annotations and templates to describe the required annotations. We use techniques similar to aspect-oriented programming to add the annotations to the generated code: the patterns correspond to (static) point-cut descriptors, while the introduced annotations correspond to advice. The annotation inference algorithm can run completely separately from the generator and is generic with respect to the safety property, although we use initialization safety as running example here. It has been implemented and applied to certify initialization safety for code generated by Auto-Bayes and AutoFilter
[Virtual colonoscopy, coding errors, NASA, initialization safety, annotation inference, automated code generation, Programming, type theory, Software safety, program compilers, Certification, certification, safety certification, functional correctness, model-based software development, security of data, Space technology, Inference algorithms, Error correction, Aerospace safety, reasoning about programs, idiomatic structure, static point-cut descriptors, Testing]
A Unified Model for Product Data Management and Software Configuration Management
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Software configuration management (SCM) is the discipline of managing the evolution of a software system. Product data management (PDM) is the discipline of controlling the evolution of a product design. These two domains have been evolving independently and fairly disconnected. Nowadays, the development of modern products involves a substantial and growing part of software development. However, due to the limitations of approaches taken by both domains, efforts to build a unified configuration management (CM) model of SCM and PDM have had limited success. This paper presents a novel unified CM model and its associated CM infrastructure/tools that are configurable and tailorable to support any engineering area. Key contributions include a novel methodology, a unified CM model, and associated tools that allow for the automatic generation of CM code for supporting both hardware designs and software artifacts in multidisciplinary engineering areas
[software system evolution, Object oriented modeling, software development management, Data engineering, Product design, product data management, software maintenance, software configuration management, configuration management, Design engineering, product design, Databases, code generation, Engineering management, Software systems, Data models, Software tools, Hardware design languages]
Human-Friendly Line Routing for Hierarchical Diagrams
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Hierarchical diagrams are well-suited for visualizing the structure and decomposition of complex systems. However, the current tools poorly support modeling, visualization and navigation of hierarchical models. Especially the line routing algorithms are poorly suited for hierarchical models: for example, they produce lines that run across nodes or overlap with other lines. In this paper, we present a novel algorithm for line routing in hierarchical models. In particular, our algorithm produces an esthetically appealing layout, routes in real-time, and preserves the secondary notation of the diagrams as far as possible
[Visualization, Navigation, Filtering, Unified Modeling Language, Unified modeling language, Humans, complex system decomposition, Routing, formal specification, structure visualization, hierarchical diagrams, Layout, data visualisation, human-friendly line routing, Explosives, Informatics, visual programming, Context modeling]
Contradiction Finding and Minimal Recovery for UML Class Diagrams
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
UML (unified modeling language) is the de facto standard model representation language in software engineering. We believe that automated contradiction detection and repair of UML become very important as UML has been widely used. In this paper, we propose a debugging system using logic programming paradigm for UML class diagram with class attributes, multiplicity, generalization relation and disjoint relation. We propose a translation method of a UML class diagram into a logic program, and using a meta-interpreter we can find (set-inclusion-based) minimal sets of rules which leads to contradiction. Then, we use a minimal hitting set algorithm developed by one of the authors to show minimal sets of deletion of rules in order to avoid contradiction
[program debugging, program verification, Unified modeling language, contradiction finding, formal specification, Communication standards, class attributes, Software design, logic programming, Software standards, Communications technology, software engineering, Informatics, generalization relation, Logic programming, Unified Modeling Language, set-inclusion-based rule set, Software algorithms, UML class diagrams, Debugging, model representation language, unified modeling language, disjoint relation, program interpreters, debugging system, minimal hitting set algorithm, metainterpreter, Software engineering]
Programming Language Inherent Support for Constrained XML Schema Definition Data Types and OWL DL
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Recently, the Web Ontology Language (OWL) and XML schema definition (XSD) have become ever more important when it comes to conceptualize knowledge and to define programming language independent type systems. However, writing software that operates on ontological data and on XML instance documents still suffers from a lack of compile time support for OWL and XSD. Especially, obeying lexical- and value space constraints that may be imposed on XSD simple data types and preserving the consistency of assertional ontological knowledge is still error prone and laborious. Validating XML instance documents and checking the consistency of ontological knowledge bases according to given XML schema definitions and ontological terminologies, respectively, requires significant amounts of code. This paper presents novel compile time- and code generation features, which were implemented as an extension of the C# programming language. Zhi# provides compile time-and runtime support for constrained XML schema definition simple data types and it guarantees terminological validity for modifications of assertional ontological data
[Terminology, computational linguistics, Companies, Ontologies, value space constraints, type theory, program compilers, ontological knowledge, consistency checking, C# programming language, ontological terminologies, compile time support, Web Ontology Language, constrained XML schema definition data types, Zhi# language, OWL, ontological data, Application software, knowledge representation languages, Semantic Web, XML instance documents, Computer languages, lexical-space constraints, code generation, XML, Writing, Software systems, ontologies (artificial intelligence)]
A methodology for automated test generation guided by functional coverage constraints at specification level
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
This paper presents an approach to automate test generation from a formal specification and a set of functional test objectives while taking into account coverage constraints at the specification level. We use existing test generation techniques and tools, our contribution is on the methodological side. We define an innovative approach adapted to the industrial domain and its constraints
[Software testing, Real time systems, System testing, Automation, program testing, Aerospace electronics, functional coverage constraints, Formal specifications, Flow graphs, formal specification, automated test generation, Certification, Embedded software, Automatic testing]
An Automated Approach for Goal-driven, Specification-based Testing
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
This paper presents a specification-based approach that addresses several known challenges including false positives and domain knowledge errors. Our approach begins with a goal graph and plans. Source code is annotated with goals and events and precompiled to emit those at run time. Plans are automatically translated into a rule-based recognizer. An oracle is produced from the pre- and postconditions associated with the plan's goals. When the program is executed, goals and events are emitted and automatically tested against plans and oracles. The concept is demonstrated on a small example and a larger publicly available case study
[specification-based testing, System testing, Automation, program testing, graph theory, rule-based recognizer, goal graph, source code annotation, formal specification, false positives, domain knowledge errors, Automatic testing, Computer errors, Software engineering]
Effective Generation of Interface Robustness Properties for Static Analysis
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
A software system interacts with its environment through system interfaces. Robustness of software systems are governed by various temporal properties related to these interfaces, whose violation leads to system crashes and security compromises. These properties can be formally specified for system interfaces and statically verified against a software system. But manually specifying a large number of interface properties for static verification is often inaccurate or incomplete, apart from being cumbersome. In this paper, we propose a novel framework that effectively generates interface properties for static checking from a few generic, high level robustness rules that capture interface behavior. We implement our framework for an existing static analyzer with simple dataflow extensions and apply it on POSIX-API system interfaces used in 10 Redhat-9.0 open source packages. The results show that the framework can effectively generate a large number of useful interface properties from a few generically specified rules
[Unix, System testing, Data analysis, program verification, program diagnostics, interface robustness properties, static analysis, static verification, Computer crashes, user interfaces, Redhat-9.0 open source package, POSIX-API system interface, formal specification, software system, Computer science, robustness rules, Packaging, Software systems, Robustness, Concrete, static checking, Software engineering, system interfaces]
Automatic Generation of Detection Algorithms for Design Defects
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Maintenance is recognised as the most difficult and expansive activity of the software development process. Numerous techniques and processes have been proposed to ease the maintenance of software. In particular, several authors published design defects formalising "bad" solutions to recurring design problems (e.g., anti-patterns, code smells). We propose a language and a framework to express design defects synthetically and to generate detection algorithms automatically. We show that this language is sufficient to describe some design defects and to generate detection algorithms, which have a good precision. We validate the generated algorithms on several programs
[Algorithm design and analysis, Software maintenance, program debugging, Operations research, software development, Design methodology, program diagnostics, Debugging, Programming, software maintenance, formal specification, antipatterns, code smells, Software quality, detection algorithm, software design defects, Books, Detection algorithms, Informatics]
Software Library Usage Pattern Extraction Using a Software Model Checker
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
The need to manually specify temporal properties of software systems is a major barrier to wider adoption of software model checking, because the specification of software temporal properties is a difficult, time-consuming, and error-prone process. To address this problem, we propose to automatically extract software library usage patterns, which are one type of temporal specifications. Our approach uses a model checker to check a set of software library usage pattern candidates against existing programs using that library, and identifies valid patterns based on model checking results. These valid patterns can help programmers learn about common software library usage. They can also be used to check new programs using the same library. We applied our approach to C programs using the OpenSSL library and the C standard library, and extracted valid usage patterns using BLAST. We also successfully used the extracted valid usage patterns to detect an error in an open source project hosted by SourceForge.net
[software model checker, application program interfaces, OpenSSL library, data mining, Documentation, C library, Application software, Data mining, formal specification, Programming profession, software libraries, Computer science, Software libraries, formal verification, Software quality, C programs, Computer errors, Software systems, software temporal property specification, software library usage pattern extraction, Logic, BLAST]
Automated Round-trip Software Engineering in Aspect Weaving Systems
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
We suggest an approach to automated round-trip software engineering in source-level aspect weaving systems that allows for transparent mapping of manual edits in the woven program back to the appropriate source of origin, which is either the application core or the aspect space
[Automatic programming, Banking, Invasive software, Application software, Programming environments, Concurrent computing, configuration management, Information science, Interactive systems, automated round-trip software engineering, Weaving, software engineering, manual edit mapping, source-level aspect weaving systems, Software engineering]
Towards Automatic Assertion Refinement for Separation Logic
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Separation logic holds the promise of supporting scalable formal reasoning for pointer programs. Here we consider proof automation for separation logic. In particular we propose an approach to automating partial correctness proofs for recursive procedures. Our proposal is based upon proof planning and proof patching via assertion refinement
[Automation, Hydrogen, Automatic logic units, Logic design, Calculus, EMP radiation effects, automatic assertion refinement, Proposals, formal reasoning, Programming profession, proof patching, formal logic, proof automation, partial correctness proofs, separation logic, proof planning, reasoning about programs, theorem proving, pointer programs]
Automated Reasoning on Aspects Interactions
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
The aspect-oriented paradigm allows weaving aspects in different join points of a program. Aspects can modify object fields and method control flow, thus possibly introducing subtle and undesired interactions (conflicts) among aspects and objects, which are not easily detectable. In this paper we propose a fully automated approach to discover conflicts among classes and aspects directly from Java bytecode. The novelty of this work is the usage of a rule engine for identifying possible conflicts among advices, methods, and fields. The knowledge base is obtained through static analysis of classes and aspects bytecode. The possible conflicts are represented by means of rules that can be easily extended and customized
[conflict discovery, Java, Visualization, object-oriented programming, Java bytecode, program diagnostics, Debugging, static analysis, Data mining, Engines, aspect interaction, method control flow, Runtime, object interaction, automated reasoning, Object detection, Automatic control, Weaving, Java Extensible Conflict Manager, reasoning about programs, rule engine, Software engineering]
Detecting Precedence-Related Advice Interference
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Aspect-oriented programming (AOP) has been proposed in literature to overcome modularization shortcomings such as the tyranny of the dominant decomposition. However, the new language constructs introduced by AOP also raise new issues on their own-one of them is potential interference among aspects. In this paper we present an interference criterion to detect and thus help programmers to avoid advice order related problems
[Java, program debugging, object-oriented programming, precedence-related advice interference, aspect interference, program diagnostics, advice order related problems, Interference, Telecommunications, Application software, Yarn, Programming profession, Stress, Program processors, Prototypes, aspect-oriented programming, Software engineering]
Round-Trip Engineering of Framework-Based Software using Framework-Specific Modeling Languages
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
This research combines three distinct areas: domain-specific modeling, object-oriented application frameworks and round-trip engineering. We introduce framework-specific modeling languages (FSMLs) which are used for the modeling of framework-based software and enable automated round-trip engineering. We describe a prototype implementation of an example FSML. We also present research outline and future work
[framework-specific modeling language, Software prototyping, Navigation, Object oriented modeling, object-oriented application, Documentation, Programming, domain-specific modeling, Application software, formal specification, framework-based software, Design engineering, Prototypes, specification languages, Concrete, Software tools, round-trip engineering]
Integrated Variability Modeling of Features and Architecture in Software Product Line Engineering
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Existing methods and tools supporting product line variability management typically emphasize either the feature or the architecture level. There have been attempts to combine these aspects, but no widely accepted method is available so far. This paper reports ongoing research in designing and implementing product line variability models, where the focus lies in treating features and architectural elements as parts of an integrated model. The research is carried together with our industry partner Siemens VAI
[software product line engineering, Laboratories, Aluminum, Product design, Iron, integrated variability modeling, product line variability management, Steel, formal specification, software architecture, Engineering management, Computer architecture, software reusability, Architecture description languages, Software tools, Software engineering]
Software Connectors for Highly Distributed and Voluminous Data Intensive Systems
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
We describe a research agenda for selecting software connectors which quantifiably satisfy different scenarios for large volume data distribution. We outline the necessity for a framework which allows a user to select amongst the different distribution connectors available. The framework is based on a classification of distribution connectors along eight key dimensions of data distribution
[Taxonomy, Laboratories, NASA, distributed data intensive systems, distributed processing, data integrity, data distribution connectors, Middleware, Connectors, Computer science, Mars, Space technology, Propulsion, Reconnaissance, voluminous data intensive systems, data handling, software connectors]
Coverage Metrics to Measure Adequacy of Black-Box Test Suites
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
In black-box testing, one is interested in creating a suite of tests from requirements that adequately exercise the behavior of a software system without regard to the internal structure of the implementation. In current practice, the adequacy of black-box test suites is inferred by examining coverage on an executable artifact, either source code or a software model. We propose the notion of defining structural coverage metrics directly on high-level formal software requirements. These metrics provide objective, implementation-independent measures of how well a black-box test suite exercises a set of requirements. We focus on structural coverage criteria on requirements formalized as linear temporal logic (LTL) properties and explore how they can be adapted to measure finite test cases. These criteria can also be used to automatically generate a requirements-based test suite. Unlike model or code-derived test cases, these tests are immediately traceable to high-level requirements
[Software testing, System testing, program testing, program verification, software system behavior, NASA, black-box testing, Aerospace electronics, temporal logic, linear temporal logic, Logic testing, Computer science, Automatic testing, high-level formal software requirements, structural coverage metrics, Software systems, Software measurement, Contracts, software metrics]
Management of Incomplete and Inconsistent Views
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Views have long been used as a means to structure and manage conceptual models. Model management aims to keep track of the relationships between a set of views as they evolve, and to describe the manipulations performed over them in terms of a set of predefined operators. A major challenge in model management is handling the incompleteness and inconsistency of views. In this extended abstract, we describe a general framework for the management of incomplete and inconsistent views. We use our framework as a basis for exploring the systematic application of important design principles, such as traceability, reusability and separation of concerns, in model management
[Terminology, software traceability, Conference management, conceptual models, model management, Application software, formal specification, view completeness, Computer science, view consistency, view management, Databases, Engineering management, software reusability, Logic, Software engineering, Context modeling]
Energy-Awareness in Distributed Java-Based Software Systems
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Distributed Java-based software systems are increasingly deployed onto heterogeneous mobile platforms with limited battery resources. In this domain, it is a crucial issue to preserve the energy resource and prolong the system's lifetime. To address this problem, we first suggest a framework that allows the system engineer to estimate the energy consumption of a distributed Java-based software system both during system construction-time and during runtime. Based on our energy estimation framework, we then present two efficient approaches for reducing the software system's energy consumption and consequently increasing the system's lifetime. Finally, we discuss our strategy for evaluating our estimation framework and energy saving approaches
[Java, Energy consumption, heterogeneous mobile platforms, Batteries, energy resource preservation, Runtime, mobile computing, energy awareness, Energy resources, resource allocation, Life estimation, battery resource, Software systems, Systems engineering and theory, Lifetime estimation, distributed Java-based software systems, energy consumption, Power engineering and energy]
LSS: A Tool for Large Scale Scenarios
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Today's complex computational and embedded systems compute complex quantities from complex inputs, with behavior dependent upon the (distributed) state of the system and its environment. Describing the intended behavior of such a system is challenging. The most natural and commonly used approach is to give a collection of scenarios, where each scenario is a sequence of inputs and expected outputs. An analyst elicits scenarios from subject matter experts (SMEs) to document functional requirements. Scenarios can be represented in a variety of ways, from informal narratives through (formal) linear event sequences. Formal behavior scenarios are useful in many software engineering tasks, including requirements engineering, specification- and architectural-modeling, and test generation. Uses in modeling include model inference, validation, and measuring resource usage and reliability. Finally, scenarios are useful in discovering and documenting gaps in specifications. Existing scenario-based tools and methodologies break down when scenarios grow large. Elicitation becomes impractical, as it requires the human to specify all the steps. Even assuming one can capture thousands or more of input events, it is problematic for the human to describe the correct expected outputs at each point of interest. Finally, even if one can capture all inputs and outputs, the complexity of such an artifact gives one little confidence that it represents desirable behavior in detail, as the risk of undetected errors grows at least linearly with the size. Finally, the behavior of systems described by large scale scenarios typically requires a large and complex set of them, which creates the added difficulty of assuring oneself of covering a "representative" subset of a huge space
[linear event sequences, model inference, requirement engineering, large scale scenarios, program verification, software reliability, Humans, architectural modeling, Distributed computing, formal specification, Wireless communication, system analysis, software architecture, Weapons, software validation, Embedded system, functional requirements, software engineering, resource usage measurement, Large-scale systems, Data communication, subject matter experts, Embedded computing, Computational modeling, program diagnostics, test generation, systems analysis, Software engineering, system behavior]
A new web browser including a transferable function to Ajax codes
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
We propose a new Web browser supporting asynchronous communication with Web server computers like Ajax approach. When users access Web applications on our browser, the users can receive similar benefits from Ajax even if the Web applications do not adopt Ajax approach. The browser has a transferable function from normal HTML sources to Ajax codes
[Java, Web server computer, asynchronous communication, Web applications, HTML sources, Displays, Educational institutions, HTML, transferable function, Application software, Asynchronous communication, Layout, Web browser, Web pages, online front-ends, Internet, Web server, Ajax codes, hypermedia markup languages]
Tobias-Z: An executable formal specification of a test generator
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Tobias is a combinatorial testing tool that was used succesfully on several case studies. Currently, the evolution of the tool goes through a significant redevelopment effort. A first step is the production of an executable specification of the Tobias Test Generator. The goal of this specification effort is to provide a synthetic and precise description of Tobias to the developers of the new tool. The specification is expressed in the Z language, supported by the Jaza animator. The executable character of the specification is exploited (1) to assess non-regression of the specification with respect to the existing tool, and (2) to explore new functionalities for the tool
[Java, System testing, Jaza animator, test generator, combinatorial mathematics, program testing, Tobias-Z, Explosions, Formal specifications, formal specification, Filters, executable formal specification, Automatic testing, Prototypes, specification nonregression assessment, Animation, combinatorial testing tool, software tools, Kernel, Power generation]
Model-driven Monitoring: Generating Assertions from Visual Contracts
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
The Visual Contract Workbench is a tool that supports model-driven development of software systems by lifting the Design by Contract idea, which is usually used at the code level, to the model level. It uses visual contracts for graphically specifying the pre- and post-conditions of an operation. Java classes with JML (Java modeling language) assertions are generated from visual contracts to facilitate automatic monitoring of the correctness of the programmers' implementation
[Java, program verification, Unified Modeling Language, Computerized monitoring, Visual Contract Workbench, graphical specification, formal specification, program compilers, Programming profession, software system development, design by contract, Computer languages, Runtime, Program processors, Software design, Java modeling language assertions, Binary codes, Java classes, correctness monitoring, Skeleton, visual programming, Contracts]
The Rearranger - A New Assembler Utility
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
According to a recently discovered theorem, the flow graph of any program, no matter how much "spaghetti code" it contains, may be reordered in such a way that it shows a loop structure. The rearranged program has no backward branches except for loopbacks, which go to the head of some loop from somewhere within that loop. No new vertices or variables are introduced; only the order of the vertices is changed. Rearrangement may be automated, for either low-level or high-level languages. We have constructed a tool, which we call a Rearranger, to act on a description of an assembly language, followed by a program written in that language. A spaghetti-code version of depth-first search has been programmed for Intel, MPS, G3, Motorola 68000, and the IBM mainframe, and successfully rearranged for all these machines
[assembly language, program control structures, flow graphs, Rearranger, Flow graphs, tree searching, High level languages, Computer science, Tree graphs, spaghetti code, program rearrangement, depth-first search, loop structure, software tools, Assembly, Software engineering, program flow graph]
TOPCASED Combining Formal Methods with Model-Driven Engineering
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
This paper briefly presents the TOPCASED project which gathers industrialists, researchers, universities and SMEs, aiming at producing a free/open-source system/software/hardware-engineering toolkit, implemented over the Eclipse platform, using only standard components. An important aspect of TOPCASED is that it enables researchers to plug in their tools easily. TOPCASED is meant to be used on actual industrial projects and may therefore be considered as an important target by researchers working on formal methods and foundations of software engineering for critical systems
[computer science education, open-source system, public domain software, TOPCASED project, Educational institutions, teaching, formal specification, Aerospace industry, Open source software, Automotive engineering, formal verification, software engineering toolkit, Space technology, hardware engineering toolkit, model-driven engineering, formal methods, Computer industry, Software systems, Model driven engineering, software tools, Software tools, Software engineering, Eclipse platform]
UML-based Service Discovery Tool
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
The development of service centric systems has been recognised as an important approach for software system development. In this paper, we present a UML-based tool to identify services that can provide the functionality and satisfy properties and constraints of service centric systems specified during the design phase of the development of these systems and allows for the (re-) formulation of the design models based on the discovered services
[Context-aware services, Process design, Algorithm design and analysis, Visualization, Unified Modeling Language, service centric systems, Unified modeling language, formal specification, Construction industry, software system development, Web services, UML-based service discovery tool, Search engines, Software systems, Systems engineering and theory, service identification]
Automated Verification Tool for DHTML
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Automated verification for client-side programs of DHTML applications has become necessary. This is because DHTML applications are increasingly complicated in order to enhance the functionality and usability of dynamic Web content. We are therefore motivated to create a tool for automatically verifying a JavaScript program of a DHTML application against a specification describing the page flows. The verification is based on a type inference technique focusing on DOM updates
[Tree data structures, Java, client-server systems, Transducers, DHTML applications, client-side programs, Laboratories, dynamic Web content, type theory, formal specification, Computer languages, document object model, Inference algorithms, Internet, reasoning about programs, Logic, JavaScript program verification, Usability, Pattern matching, automated verification tool, Testing, hypermedia markup languages, type inference]
Mock-object generation with behavior
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Unit testing is a popular way to guide software development and testing. Each unit test should target a single feature, but in practice it is difficult to test features in isolation. Mock objects are a well-known technique to substitute parts of a program which are irrelevant for a particular unit test. Today mock objects are usually written manually supported by tools that generate method stubs or distill behavior from existing programs. We have developed a prototype tool based on symbolic execution of .NET code that generates mock objects including their behavior by analyzing all uses of the mock object in a given unit test. It is not required that an actual implementation of the mocked behavior exists. We are working towards an integration of our tool into Visual Studio Team System
[Software testing, .NET code, program debugging, program testing, software development, Instruments, software testing, Debugging, unit testing, Programming, program compilers, Visual Studio Team System, mock-object generation, Prototypes, symbolic execution, Concrete, visual programming, Software engineering]
Domain-specific Model Checking Using The Bogor Framework
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Model checking has proven to be an effective technology for verification and debugging in hardware and more recently in software domains. We believe that recent trends in both the requirements for software systems and the processes by which systems are developed suggest that domain-specific model checking engines may be more effective than general purpose model checking tools. To overcome limitations of existing tools which tend to be monolithic and non-extensible, we have developed an extensible and customizable model checking framework called Bogor. In this tutorial, we give an overview of (a) Bogor's direct support for modeling object-oriented designs and implementations, (b) its facilities for extending and customizing its modeling language and algorithms to create domain-specific model checking engines, and (c) pedagogical materials that we have developed to describe the construction of model checking tools built on top of the Bogor infrastructure
[Algorithm design and analysis, Java, program debugging, object-oriented programming, program verification, Object oriented modeling, Software algorithms, software system requirements, Power system modeling, Yarn, Engines, domain-specific model checking, Bogor infrastructure, object-oriented design, Software systems, Dynamic programming, Context modeling]
Testing Tools and Techniques: A Mini-Tutorial on Evaluation Methods for ASE
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Ever wonder how your tool would fare in the 'real world'? Think your tool excels where others have failed? Is your tool better than a software developer? How can your tool help a software developer? Have you asked these kinds of questions before? If so, then this mini-tutorial is focused on your needs. In the mini-tutorial, I will go over the basics of tool evaluation from the beginning to end, from ethics to theory to analysis. Both qualitative and quantitative approaches will be covered
[Tutorial, software development, Psychology, Programming, Computer science, Ethics, Biographies, Councils, tool testing, software engineering, software tools, Software tools, tool evaluation, Testing, Software engineering]
2nd Asian Workshop on Aspect-Oriented Software Development (AOAsia)
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Separation of concerns is one of the main tenets of software engineering - allowing developers to reason about software systems in sensible portions, regardless which phase of the lifecycle they are working in. Many researchers in software engineering are actually in the field of aspect-orientation without realizing it.
[]
Second International Workshop on Supporting Knowledge Collaboration in Software Development (KCSD2006)
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
The creation of modern software systems requires knowledge from a wide range of domains: application domains, computer hardware and operating systems, algorithms, programming languages, vast amount of component libraries, development environments, the history of the software system, and users. Because few software developers have all the required knowledge, the development of software is no longer confined to an individual but has to rely on distributed cognition by reaching into a complex and networked world of information and computer mediated collaboration. Knowledge collaboration has thus become an important aspect of software development.
[]
Japanese Workshop on Leveraging Web2.0 Technologies in Software Development Environments (WebSDE)
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
This paper briefly describes the theme and goals of the WebSDE Workshop on ASE'2006. This workshop emphasizes next-generation software development environments inspired by Web2.0 technologies and seeks to explore ways of automated support to software development in the Web2.0 era.
[]
Japanese Workshop on Requirements Engineering Tools (JWRET)
21st IEEE/ACM International Conference on Automated Software Engineering
None
2006
Requirements Engineering begins to attract attention as a trump letting a project succeed and software improve its reliability. There already exist a lot of research results in requirements engineering area, and they seem to be helpful for practitioners. However, it is not so easy to use such research results because most of them do not provide stable and scalable tool support. One of the reasons is that tasks in requirements engineering highly depend on human decisions and/or insights and it seems to be hard to achieve such tasks systematically and automatically. Of course, we can find several tools to support requirements engineering process. For example, there is a survey of requirements management tools in INCOSE site [1]. Tools for goal oriented requirements analysis can be found in [2] and [3]. One of the common points of such tools is they are really helpful to achieve human-centric tasks in requirements engineering. Therefore, we have to explore what kinds of systematic or automatic support could be provided to improve humancentric tasks in requirements engineering. That is the reason why we plan this workshop. In this workshop, participants will explore tools to support requirements processes: e.g. requirements elicitation, requirement specification, requirements validation, or requirements management.
[]
ASE'08 Organization
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Provides a listing of current committee members and society officers.
[]
ASE 2008 Committee
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Provides a listing of current committee members.
[]
Forward
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
The papers appearing in these proceedings were subjected to a rigorous and high quality reviewing process. A total of 280 papers were submitted to the conference. Each paper was reviewed by at least 3 members of the Program Committee and Expert Review Panel. The committee accepted 34 for presentation as talks and publication as full papers, and 31 for presentation as posters and publication as short papers. In addition to the technical papers, the conference includes a Doctoral Symposium and a Tool Demonstrations track. A total of 17 tool papers were selected by review for publication within the proceedings, while 6 doctoral symposium papers were accepted. The Doctoral Symposium is a one day forum that seeks to bring together PhD students working on foundations, techniques, tools and applications of automated software engineering. Specifically, the symposium aims to provide a setting whereby students receive feedback on their research and guidance on future directions from a broad group of advisors, foster a supportive community of scholars and a spirit of collaborative research, and contribute to the conference goals through interaction with other researchers and conference events
[]
Swarm Verification
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Reportedly, supercomputer designer Seymour Cray once said that he would sooner use two strong oxen to plow afield than a thousand chickens. Although this is undoubtedly wise when it comes to plowing afield, it is not so clear for other types of tasks. Model checking problems are of the proverbial "search the needle in a haystack" type. Such problems can often be parallelized easily. Alas, none of the usual divide and conquer methods can be used to parallelize the working of a model checker. Given that it has become easier than ever to gain access to large numbers of computers to perform even routine tasks it is becoming more and more attractive to find alternate ways to use these resources to speed up model checking tasks. This paper describes one such method, called swarm verification.
[File systems, program verification, model checking, Computational modeling, Magnetic cores, Memory management, Random access memory, Switches, parallel processing, swarm verification, divide-and-conquer method, Testing]
Reflections on, and Predictions for, Support Systems for the Development of Programs
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
My first attempt to build a "formal development support system" was in IBM in the 1970s; more public is the mural system we built in Manchester; the Rodin (EU) project developed a set of open source tools that are now being used in the (EU) DEPLOY project. These attempts give me some perspective from which to predict what sort of tool will finally make the difference to software developers that CAD systems have made in hardware design.
[Design automation, formal development support system, Reflection, CAD systems, Computer science, program development, Computer languages, hardware design, Software, Hardware, software tools, Artificial intelligence, open source tools]
Increasing Test Granularity by Aggregating Unit Tests
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Unit tests are focused, efficient, and there are many techniques to support their automatic generation. Coarser granularity tests, however, are necessary to validate the behavior of larger software components, and are also likely to be more robust in the presence of program changes. This paper investigates whether coarser granularity tests can be automatically generated by aggregating unit tests. We leverage our Differential Unit Test (DUT) framework to represent unit tests, define a space of potential aggregations of those unit tests, and implement a strategy to traverse that space to generate Aggregated DUTs (A- DUTs) that validate the effects of multiple method calls on a (set of) receiver object(s). An empirical study of A-DUTs on two applications shows their tradeoffs with DUTs and their potential to increase the number of versions for which tests remain usable relative to method level tests.
[object-oriented programming, Law, program testing, Instruments, Receivers, granularity tests, Fault detection, test granularity, Robustness, software components, automatic generation, differential unit test framework, aggregated differential unit test, Legal factors, Testing]
Random Test Run Length and Effectiveness
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
A poorly understood but important factor in random testing is the selection of a maximum length for test runs. Given a limited time for testing, it is seldom clear whether executing a small number of long runs or a large number of short runs maximizes utility. It is generally expected that longer runs are more likely to expose failures - which is certainly true with respect to runs shorter than the shortest failing trace. However, longer runs produce longer failing traces, requiring more effort from humans in debugging or more resources for automated minimization. In testing with feedback, increasing ranges for parameters may also cause the probability of failure to decrease in longer runs. We show that the choice of test length dramatically impacts the effectiveness of random testing, and that the patterns observed in simple models and predicted by analysis are useful in understanding effects observed in a large scale case study of a JPL flight software system.
[program debugging, Protocols, program testing, Probability, Minimization, automated minimization, File systems, Public key, random test run length, debugging, Buffer overflow, JPL flight software system, Testing]
Program Analysis with Dynamic Precision Adjustment
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
We present and evaluate a framework and tool for combining multiple program analyses which allows the dynamic (on-line) adjustment of the precision of each analysis depending on the accumulated results. For example, the explicit tracking of the values of a variable may be switched off in favor of a predicate abstraction when and where the number of different variable values that have been encountered has exceeded a specified threshold. The method is evaluated on verifying the SSH client/server software and shows significant gains compared with predicate abstraction-based model checking.
[Algorithm design and analysis, client-server systems, Shape, program verification, Heuristic algorithms, program diagnostics, Switches, dynamic precision adjustment, Servers, predicate abstraction, Analytical models, model checking, client/server software, program analysis, Performance analysis]
IR-Based Traceability Recovery Processes: An Empirical Comparison of "One-Shot" and Incremental Processes
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
We present the results of a controlled experiment aiming at analysing the role played by the approach adopted during an IR-based traceability recovery process. In particular, we compare the tracing performances achieved by subjects using the "one-shot" process, where the full ranked list of candidate links is proposed, and the incremental process, where a similarity threshold is used to cut the ranked list and the links are classified step-by-step. The analysis of the achieved results shows that, in general, the incremental process improves the tracing accuracy and reduces the effort to analyse the proposed links.
[traceability recovery, Accuracy, one-shot process, Laboratories, Process control, Documentation, information retrieval, Probabilistic logic, Software, Large scale integration, incremental process, system recovery]
Enabling Automated Traceability Maintenance by Recognizing Development Activities Applied to Models
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
For anything but the simplest of software systems, the ease and costs associated with change management can become critical to the success of a project. Establishing traceability initially can demand questionable effort, but sustaining this traceability as changes occur can be a neglected matter altogether. Without conscious effort, traceability relations become increasingly inaccurate and irrelevant as the artifacts they associate evolve. Based upon the observation that there are finite types of development activity that appear to impact traceability when software development proceeds through the construction and refinement of UML models, we have developed an approach to automate traceability maintenance in such contexts. Within this paper, we describe the technical details behind the recognition of these development activities, a task upon which our automated approach depends, and we discuss how we have validated this aspect of the work to date.
[project management, Unified Modeling Language, software development, Unified modeling language, software systems, software development management, Maintenance engineering, Programming, software maintenance, Engines, change management, traceability relations, UML models, Refining, automated traceability maintenance, management of change, Software systems, software engineering, Catalogs]
Incremental Latent Semantic Indexing for Automatic Traceability Link Evolution Management
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Maintaining traceability links among software artifacts is particularly important for many software engineering tasks. Even though automatic traceability link recovery tools are successful in identifying the semantic connections among software artifacts produced during software development, no existing traceability link management approach can effectively and automatically deal with software evolution. We propose a technique to automatically manage traceability link evolution and update the links in evolving software. Our novel technique, called incremental latent semantic indexing (iLSI), allows for the fast and low-cost LSI computation for the update of traceability links by analyzing the changes to software artifacts and by reusing the result from the previous LSI computation before the changes. We present our iLSI technique, and describe a complete automatic traceability link evolution management tool, TLEM, that is capable of interactively and quickly updating traceability links in the presence of evolving software artifacts. We report on our empirical evaluation with various experimental studies to assess the performance and usefulness of our approach.
[Evolution (biology), incremental latent semantic indexing, software artifacts, Documentation, automatic traceability link evolution management, Software systems, Software, Large scale integration, Complexity theory, Matrix decomposition, software maintenance, software evolution]
Automated Verification of Multi-Agent Programs
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
In this paper, we show that the flexible model-checking of multi-agent systems, implemented using agent-oriented programming languages, is viable thus paving the way for the construction of verifiably correct applications of autonomous agents and multi-agent systems. Model checking experiments were carried out on AJPF (agent JPF), our extension of Java PathFinder that incorporates the agent infrastructure layer, our unifying framework for agent programming languages. In our approach, properties are specified in a temporal language extended with (shallow) agent-related modalities. The framework then allows the verification of programs written in a variety of agent programming languages, thus removing the need for individual languages to implement their own verification framework. It even allows the verification of multi-agent systems comprised of agents developed in a variety of different (agent) programming languages. As an example, we also provide model checking results for the verification of a multi-agent system implementing a well-known task sharing protocol.
[Java, Multiagent systems, multi-agent systems, program verification, task sharing protocol, agent-oriented programming language, agent JPF, Java PathFinder, Data structures, Cognition, Specification languages, multiagent system, Computer languages, model checking, agent infrastructure layer, Software, automated program verification, autonomous agents, temporal language]
Validating Real Time Specifications using Real Time Event Queue Modeling
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Interrupt-driven real time control software is difficult to design and validate. It does not line up well with traditional state-based, timed-transition specification formalisms, due to the complexity of timers and the pending interrupt queue. The present work takes a new approach to the problem of modeling and tool-supported reasoning about such systems based on infinite-state modeling of the temporal event queue. This approach, RTEQ, can be used in any formalism or tool set supporting function rich modeling. The present paper describes the approach, explores its expressive power and semantics, and describes a significant industrial case study applying it to the design of a novel network medium access controller for wireless communications.
[Real time systems, wireless communications, Schedules, Computational modeling, real time event queue modeling, temporal event queue, real time specifications, tool-supported reasoning, Cognition, state-based specification formalisms, formal specification, interrupt-driven real time control software, Wireless communication, Unicast, timed-transition specification formalisms, infinite-state modeling, Hardware, network medium access controller]
Automatic Inference of Frame Axioms Using Static Analysis
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Many approaches to software verification are currently semi-automatic: a human must provide key logical insights - e.g., loop invariants, class invariants, and frame axioms that limit the scope of changes that must be analyzed. This paper describes a technique for automatically inferring frame axioms of procedures and loops using static analysis. The technique builds on a pointer analysis that generates limited information about all data structures in the heap. Our technique uses that information to over-approximate a potentially unbounded set of memory locations modified by each procedure/loop; this over- approximation is a candidate frame axiom. We have tested this approach on the buffer-overflow benchmarks from ASE 2007. With manually provided specifications and invariants/axioms, our tool could verify/falsify 226 of the 289 benchmarks. With our automatically inferred frame axioms, the tool could verify/falsify 203 of the 289, demonstrating the effectiveness of our approach.
[storage allocation, program control structures, Shape, program verification, software verification, Scalability, program diagnostics, automatic inference, static analysis, Data structures, loop invariants, Specification languages, memory locations, frame axioms, buffer-overflow benchmarks, class invariants, Software, data structures, pointer analysis, Arrays, Resource management]
Generating and Evaluating Choices for Fixing Inconsistencies in UML Design Models
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Our objective is to provide automated support for assisting designers in fixing inconsistencies in UML models. We have previously developed techniques for efficiently detecting inconsistencies in such models and identifying where changes need to occur in order to fix problems detected by these means. This paper extends previous work by describing a technique for automatically generating a set of concrete changes for fixing inconsistencies and providing information about the impact of each change on all consistency rules. The approach is integrated with the design tool IBM Rational Rose . We demonstrate the computational scalability and usability of the approach through the empirical evaluation of 39 UML models of sizes up to 120,000 elements.
[Unified Modeling Language, Computational modeling, Unified modeling language, Receivers, Manuals, Writing, UML design model, Generators, computational scalability, Servers, IBM Rational Rose]
Refining Real-Time System Specifications through Bounded Model- and Satisfiability-Checking
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
In bounded model checking (BMC) a system is modeled with a finite automaton and various desired properties with temporal logic formulae. Property verification is achieved by translation into boolean logic and the application of SAT-solvers. bounded satisfiability checking (BSC) adopts a similar approach, but both the system and the properties are modeled with temporal logic formulae, without an underlying operational model. Hence, BSC supports a higher-level, descriptive approach to system specification and analysis. We compare the performance of BMC and BSC over a set of case studies, using the Zot tool to translate automata and temporal logic formulae into boolean logic. We also propose a method to check whether an operational model is a correct implementation (refinement) of a temporal logic model, and assess its effectiveness on the same set of case studies. Our experimental results show the feasibility of BSC and refinement checking, with modest performance loss w.r.t. BMC.
[real-time system specifications, finite automata, temporal logic formulae, Radiation detectors, computability, temporal logic, bounded model checking, Encoding, History, formal specification, finite automaton, Analytical models, Zot tool, formal verification, property verification, Pressing, Delta modulation, Distance measurement, satisfiability checking]
Evaluating Models for Model-Based Debugging
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Developing model-based automatic debugging strategies has been an active research area for several years, with the aim of locating defects in a program by utilising fully automated generation of a model of the program from its source code. We provide an overview of current techniques in model-based debugging and assess strengths and weaknesses of the individual approaches. An empirical comparison is presented that investigates the relative accuracy of different models on a set of test programs and fault assumptions, showing that our abstract interpretation based model provides high accuracy at significantly less computational effort than slightly more accurate techniques. We compare a range of model-based debugging techniques with other state-of-the-art automated debugging approaches and outline possible future developments in automatic debugging using model-based reasoning as the central unifying component in a comprehensive framework.
[program debugging, Computational modeling, Object oriented modeling, program diagnostics, Adaptation model, Debugging, source code, Complexity theory, Circuit faults, Analytical models, program defect location, automated program generation, model-based automatic debugging strategy]
Error Reporting Logic
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
When a system fails to meet its specification, it can be difficult to find the source of the error and determine how to fix it. In this paper, we introduce error reporting logic (ERL), an algorithm and tool that produces succinct explanations for why a target system violates a specification expressed in first order predicate logic. ERL analyzes the specification to determine which parts contributed to the failure, and it displays an error message specific to those parts. Additionally, ERL uses a heuristic to determine which object in the target system is responsible for the error. Results from a small user study suggest that the combination of a more focused error message and a responsible object for the error helps users to find the failure in the system more effectively. The study also yielded insights into how the users find and fix errors that may guide future research.
[Connectors, specification analysis, first order predicate logic, USA Councils, Humans, Computer architecture, heuristic, Software, Specification languages, error reporting logic, formal specification, Radio access networks]
Efficient Monitoring of Parametric Context-Free Patterns
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Recent developments in runtime verification and monitoring show that parametric regular and temporal logic specifications can be efficiently monitored against large programs. However, these logics reduce to ordinary finite automata, limiting their expressivity. For example, neither can specify structured properties that refer to the call stack of the program. While context-free grammars (CFGs) are expressive and well-understood, existing techniques for monitoring CFGs generate large runtime overhead in real-life applications. This paper shows, for the first time, that monitoring parametric CFGs is practical (with overhead on the order of 10% or lower for average cases, several times faster than the state-of-the-art). We present a monitor synthesis algorithm for CFGs based on an LR(1) parsing algorithm, modified with stack cloning to account for good prefix matching. In addition, a logic-independent mechanism is introduced to support matching against the suffixes of execution traces.
[Algorithm design and analysis, Java, finite automata, program verification, regular logic specification, CFG, temporal logic, parametric context-free pattern monitoring, Synchronization, formal specification, context-free grammar, stack cloning, Runtime, Production, LR parsing algorithm, context-free grammars, system monitoring, temporal logic specification, Safety, runtime verification, prefix matching, Monitoring]
A Framework for Dynamic Service Discovery
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Service discovery has been recognised as an important activity for service-based systems. In this paper we describe a framework for dynamic service discovery that supports the identification of service during the execution time of service-based systems. In the framework, services are identified based on structural, behavioural, quality, and contextual characteristics of a system represented in query languages. The framework supports both pull and push modes of query execution. In the approach, a service is identified based on the computation of distances between a query and a candidate service. A prototype tool has been implemented in order to illustrate and evaluate the framework. The paper also describes the results of a set of experiments that we have conducted to evaluate the work.
[query execution, XML, Quality of service, dynamic service discovery, Ontologies, Cities and towns, query languages, Distance measurement, software engineering, Servers, Database languages, service-based systems]
A Methodology and Framework for Creating Domain-Specific Development Infrastructures
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Domain-specific architectures, middleware platforms, and analysis techniques leverage domain knowledge to help engineers build systems more effectively. An integrated set of these elements is called a domain-specific development infrastructure (DSDI). DSDIs are commonly created in a costly, ad-hoc fashion because current model-driven engineering (MDE) technologies lack sufficient mechanisms for capturing the semantics of domain concepts. In this paper, we propose a methodology for incorporating semantics within MDE frameworks to simplify and automate DSDI integration. We also present and evaluate a framework, called XTEAM, that implements our approach, resulting in structured processes and enforceable guidelines for DSDI integration. We have applied our approach to several DSDIs, and report on the benefits accrued.
[domain knowledge, Computational modeling, domain-specific development infrastructures, Manuals, XTEAM, semantics, Middleware, Construction industry, Connectors, Analytical models, software architecture, domain-specific architectures, analysis techniques, model-driven engineering, Computer architecture, middleware platforms]
Connecting Programming Environments to Support Ad-Hoc Collaboration
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Physical proximity supports various forms of ad-hoc collaboration among developers such as opportunistic task adaptation and helping co-developers when they are stuck. Connecting the input/output flows of stand-alone programming environments of distributed developers offers the potential to support such collaboration among them. Such a connection has several components including communication sessions, awareness of others' availability and the state of the objects on which they are working, and control channels allowing users to import edits of and share code with others and be notified when a team member has moved away from a program element of interest. It is possible to develop a collaboration-centered design that combines a variety of collaboration streams into a usable and useful user-interface, and implement the design using existing programming environment, communication, and compiler technologies.
[program element, Inspection, user interfaces, distributed developer, Programming environments, stand-alone programming environment, team member, user-interface, Tiles, Collaboration, groupware, Streaming media, Software, ad-hoc collaboration-centered design, programming environments, IEEE Potentials]
Reducing False Positives by Combining Abstract Interpretation and Bounded Model Checking
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Fully automatic source code analysis tools based on abstract interpretation have become an integral part of the embedded software development process in many companies. And although these tools are of great help in identifying residual errors, they still possess a major drawback: analyzing industrial code comes at the cost of many spurious errors that must be investigated manually. The need for efficient development cycles prohibits extensive manual reviews, however. To overcome this problem, the combination of different software verification techniques has been suggested in the literature. Following this direction, we present a novel approach combining abstract interpretation and source code bounded model checking, where the model checker is used to reduce the number of false error reports. We apply our methodology to source code from the automotive industry written in C, and show that the number of spurious errors emitted by an abstract interpretation product can be reduced considerably.
[automotive industry, Filtering, program verification, program diagnostics, abstract interpretation, software reliability, Manuals, Inspection, automobile industry, software verification technique, C language, industrial software project, Runtime, source code bounded model checking, automatic source code analysis tool, embedded systems, Software, Libraries, software tools, false positive error report reduction, Arrays, embedded software development process]
Unit Testing of Flash Memory Device Driver through a SAT-Based Model Checker
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Flash memory has become virtually indispensable in most mobile devices. In order for mobile devices to operate successfully, it is essential that the flash memory be controlled correctly through the device driver software. However, as is typical for embedded software, conventional testing methods often fail to detect hidden flaws in the complex device driver software. This deficiency incurs significant development and operation overheads to the manufacturers. Model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses. These techniques, however, require significant manual efforts to create an abstract target model and, thus, are not widely applied in industry. In this project, we applied a model checking technique based on a Boolean satisfiability (SAT) solver. One advantage of SAT-based model checking is that a target C code can be analyzed directly without an abstract model, thereby enabling automated and bit-level accurate verification. In this project, we have applied CBMC, a SAT-based software model checker, to the unit testing of the Samsung OneNANDtrade device driver. Through this project, we detected several bugs that had not been discovered previously.
[device driver software, program testing, program verification, bit-level accurate verification, Random access memory, device drivers, unit testing, Boolean satisfiability solver, Analytical models, bugs, flash memory device driver, flash memories, SAT-based model checker, embedded software, Flash memory, mobile devices, Software, Mathematical model, Testing, Driver circuits]
Effort Estimation in Capturing Architectural Knowledge
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Capturing and using design rationale is becoming a hot topic for software architects, as architectural design decisions are now considered first class entities that should be recorded and documented explicitly. Capturing such architecture knowledge has been underestimated for several years as architects have been only focused on documenting their architectures and neglecting the rationale that led to them. The importance of recording design rationale becomes enormous for maintenance and evolution activities, as design decisions can be replayed in order to avoid highly cost architecture recovery processes. Hence, in this work we describe how architecture design decisions can be captured and documented with specific tool support. We also provide effort estimation in capturing such knowledge and we compare this with architecture modeling efforts in order to analyze the viability of knowledge capturing strategies.
[Unified modeling language, Documentation, design rationale, Maintenance engineering, software architecture, Evolution (biology), Software architecture, systems analysis, Computer architecture, Software, effort estimation, architectural design decisions, architectural knowledge]
Test-Suite Augmentation for Evolving Software
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
One activity performed by developers during regression testing is test-suite augmentation, which consists of assessing the adequacy of a test suite after a program is modified and identifying new or modified behaviors that are not adequately exercised by the existing test suite and, thus, require additional test cases. In previous work, we proposed MATRIX, a technique for test-suite augmentation based on dependence analysis and partial symbolic execution. In this paper, we present the next step of our work, where we (I) improve the effectiveness of our technique by identifying all relevant change-propagation paths, (2) extend the technique to handle multiple and more complex changes, (3) introduce the first tool that fully implements the technique, and (4) present an empirical evaluation performed on real software. Our results show that our technique is practical and more effective than existing test-suite augmentation approaches in identifying test cases with high fault-detection capabilities.
[Patents, MATRIX, program testing, regression testing, test-suite augmentation, partial symbolic execution, Explosions, software fault tolerance, Fault diagnosis, evolving software, dependence analysis, Distance measurement, Software, fault-detection capabilities, Gain, Testing]
Reducing the Cost of Path Property Monitoring Through Sampling
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Run-time monitoring can provide important insights about a program's behavior and, for simple properties, it can be done efficiently. Monitoring properties describing sequences of program states and events, however, can result in significant run-time overhead. In this paper we present a novel approach to reducing the cost of run-time monitoring of path properties. Properties are composed to form a single integrated property that is then systematically decomposed into a set of properties that encode necessary conditions for property violations. The resulting set of properties forms a lattice whose structure is exploited to select a sample of properties that can lower monitoring cost, while preserving violation detection power relative to the original properties. Preliminary studies for a widely used Java API reveal that our approach produces a rich, structured set of properties that enables control of monitoring overhead, while detecting more violations than alternative techniques.
[Java, application program interfaces, Instruments, Lattices, Documentation, path property monitoring, Java API, run-time overhead, Sockets, run-time monitoring, Automata, system monitoring, Monitoring]
Query-Aware Test Generation Using a Relational Constraint Solver
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
We present a novel approach for black-box testing of database management systems (DBMS) using the Alloy tool-set. Given a database schema and an SQL query as inputs, our approach first formulates Alloy models for both inputs, and then using the Alloy Analyzer, it generates (1) input data to populate test databases, and (2) the expected result of executing the given query on the generated data. The Alloy Analyzer results form a complete test suite (input/oracle) for verifying the execution result of a DBMS query processor. By incorporating both the schema and the query during the analysis, our approach performs query-aware data generation where executing the query on the generated data produces meaningful non-empty results. We developed a prototype tool, ADUSA, and used it to evaluate our approach. Experimental results show the ability of our approach to detect bugs in both open-source as well as commercial database management systems.
[open-source database management system, program debugging, program testing, public domain software, SQL query, query-aware test generation, Metals, Alloy tool-set, Relational databases, query-aware data generation, database management systems, query processing, commercial database management system, Analytical models, Databases, ADUSA, Alloy Analyzer, constraint handling, Testing, relational constraint solver, black-box testing, Generators, relational databases, SQL, bugs detection, Computer bugs]
Inferring Finite-State Models with Temporal Constraints
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Finite state machine-based abstractions of software behaviour are popular because they can be used as the basis for a wide range of (semi-) automated verification and validation techniques. These can however rarely be applied in practice, because the specifications are rarely kept up- to-date or even generated in the first place. Several techniques to reverse-engineer these specifications have been proposed, but they are rarely used in practice because their input requirements (i.e. the number of execution traces) are often very high if they are to produce an accurate result. An insufficient set of traces usually results in a state machine that is either too general, or incomplete. Temporal logic formulae can often be used to concisely express constraints on system behaviour that might otherwise require thousands of execution traces to identify. This paper describes an extension of an existing state machine inference technique that accounts for temporal logic formulae, and encourages the addition of new formulae as the inference process converges on a solution. The implementation of this process is openly available, and some preliminary results are provided.
[software behaviour, program verification, temporal logic formulae, Radiation detectors, program diagnostics, Merging, Reverse engineering, Humans, temporal logic, reverse engineering, finite state machines, inference mechanisms, formal specification, finite state machine-based abstraction, semi automated validation, temporal constraint, reverse engineer, Inference algorithms, Software, Safety, semi automated verification, inference process]
Type-Checking Software Product Lines - A Formal Approach
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
A software product line (SPL) is an efficient means to generate a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test all variants and ensure properties like type-safety for the entire SPL. While first steps to type-check an entire SPL have been taken, they are informal and incomplete. In this paper, we extend the Featherweight Java (FJ) calculus with feature annotations to be used for SPLs. By extending FJ's type system, we guarantee that - given a well-typed SPL - all possible program variants are well- typed as well. We show how results from this formalization reflect and help implementing our own language-independent SPL tool CIDE.
[software product lines, Java, Pediatrics, Frequency modulation, Color, type checking, program variants, Calculus, Cognition, type theory, formal approach, feature annotation, type safety, process algebra, type system, Featherweight Java calculus, Software]
Using Simulation to Investigate Requirements Prioritization Strategies
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Agile and traditional plan-based approaches to software system development both agree that prioritizing requirements is an essential activity. They differ in basic strategy - when to prioritize, to what degree, and how to guide implementation. As with many software engineering methods, verifying the benefit of following a particular approach is a challenge. Industry and student/classroom based experimental studies are generally impractical to use for large numbers of controlled experiments and benefits are difficult to measure directly. We use simulation to validate the fundamental, yet typically intangible benefits of requirements prioritization strategies. Our simulation is directly based on detailed empirical studies of agile and plan-based requirements management studies. Our simulation shows, as many have claimed, that an agile strategy excels when requirements are highly volatile, whereas a plan-based strategy excels when requirements are stable, and that there exist mixed strategies that are better than either for typical development efforts.
[Schedules, TV, simulation, agile-based requirement prioritization strategy, Programming, digital simulation, formal specification, software system development, Upper bound, formal verification, plan-based requirement prioritization strategy, systems analysis, software engineering, Random variables, Software engineering, Business]
Automated Aspect Recommendation through Clustering-Based Fan-in Analysis
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Identifying code implementing a crosscutting concern (CCC) automatically can benefit the maintainability and evolvability of the application. Although many approaches have been proposed to identify potential aspects, a lot of manual work is typically required before these candidates can be converted into refactorable aspects. In this paper, we propose a new aspect mining approach, called clustering-based fan-in analysis (CBFA), to recommend aspect candidates in the form of method clusters, instead of single methods. CBFA uses a new lexical based clustering approach to identify method clusters and rank the clusters using a new ranking metric called cluster fan- in. Experiments on Linux and JHotDraw show that CBFA can provide accurate recommendations while improving aspect mining coverage significantly compared to other state-of-the-art mining approaches.
[Java, automated aspect recommendation, object-oriented programming, program diagnostics, aspect mining, Manuals, refactorable aspects, crosscutting concern, Synchronization, Data mining, Linux, Clustering algorithms, clustering-based fan-in analysis, ranking metric, Kernel, method clusters, lexical based clustering, software metrics]
Predictive Typestate Checking of Multithreaded Java Programs
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Writing correct multithreaded programs is difficult. Existing tools for finding bugs in multithreaded programs primarily focus on finding generic concurrency problems such as data races, atomicity violations, and deadlocks. However, these generic bugs may sometimes be benign and may not help to catch other functional errors in multithreaded programs. In this paper, we focus on a high-level programming error, called typestate error, which happens when a program does not follow the correct usage protocol of an object. We present a novel technique that finds typestate errors in multithreaded programs by looking at a successful execution. An appealing aspect of our technique is that it not only finds typestate errors that occur during a program execution, but also many other typestate errors that could have occurred in a different execution. We have implemented this technique in a prototype tool for Java and have experimented it with a number of real-world Java programs.
[Java, program debugging, multiprocessing programs, multi-threading, predictive typestate checking, Learning automata, Lattices, Programming, generic concurrency problems, protocol, Sockets, high-level programming error, Automata, multithreaded Java programs, protocols, Clocks, generic bugs]
Improving Structural Testing of Object-Oriented Programs via Integrating Evolutionary Testing and Symbolic Execution
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Achieving high structural coverage such as branch coverage in object-oriented programs is an important and yet challenging goal due to two main challenges. First, some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics. Second, covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states. Previous work has developed the symbolic execution technique and the evolutionary testing technique to address these two challenges, respectively. However, neither technique was designed to address both challenges at the same time. To address the respective weaknesses of these two previous techniques, we propose a novel framework called Evacon that integrates evolutionary testing (used to search for desirable method sequences) and symbolic execution (used to generate desirable method arguments). We have implemented our framework and applied it to test 13 classes previously used in evaluating white-box test generation tools. The experimental results show that the tests generated using our framework can achieve higher branch coverage than the ones generated by evolutionary testing, symbolic execution, or random testing within the same amount of time.
[object-oriented programming, program testing, white-box test generation tools, high structural coverage, complex program logics, Receivers, Encoding, evolutionary testing, Biological cells, Construction industry, Genetic algorithms, structural testing, symbolic execution, Evacon, object-oriented programs, Skeleton, branch coverage, Testing, random testing]
Automatic Debugging of Concurrent Programs through Active Sampling of Low Dimensional Random Projections
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Concurrent computer programs are fast becoming prevalent in many critical applications. Unfortunately, these programs are especially difficult to test and debug. Recently, it has been suggested that injecting random timing noise into many points within a program can assist in eliciting bugs within the program. Upon eliciting the bug, it is necessary to identify a minimal set of points that indicate the source of the bug to the programmer. In this paper, we pose this problem as an active feature selection problem. We propose an algorithm called the iterative group sampling algorithm that iteratively samples a lower dimensional projection of the program space and identifies candidate relevant points. We analyze the convergence properties of this algorithm. We test the proposed algorithm on several real-world programs and show its superior performance. Finally, we show the algorithms' performance on a large concurrent program.
[iterative methods, program debugging, sampling methods, automatic debugging, program testing, Instruments, low dimensional random projections, concurrent computer programs, Noise, Debugging, concurrent programs, Equations, real-world programs, Computer bugs, random timing noise, concurrency control, Timing, iterative group sampling algorithm, Testing]
How Program History Can Improve Code Completion
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Code completion is a widely used productivity tool. It takes away the burden of remembering and typing the exact names of methods or classes: As a developer starts typing a name, it provides a progressively refined list of candidates matching the name. However, the candidate list always comes in alphabetic order, i.e., the environment is only second-guessing the name based on pattern matching. Finding the correct candidate can be cumbersome or slower than typing the full name. We present an approach to improve code completion with program history. We define a benchmark measuring the accuracy and usefulness of a code completion engine. Further, we use the change history data to also improve the results offered by code completion tools. Finally, we propose an alternative interface for completion tools.
[codes, code completion, pattern matching, Evolution (biology), Benchmark testing, productivity tool, Prediction algorithms, Software, History, Indexes, Engines]
SpotWeb: Detecting Framework Hotspots and Coldspots via Mining Open Source Code on the Web
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Software developers often face challenges in reusing open source frameworks due to several factors such as the framework complexity and lack of proper documentation. In this paper, we propose a code-search-engine-based approach that detects hotspots in a given framework by mining code examples gathered from open source repositories available on the Web; these hotspots are API classes and methods that are frequently reused. Hotspots can serve as starting points for developers in understanding and reusing the given framework. Our approach also detects coldspots, which are API classes and methods that are rarely used. Coldspots serve as caveats for developers as there can be difficulties in finding relevant code examples and are generally less exercised compared to hotspots. We developed a tool, called SpotWeb, for frameworks or libraries written in Java and used our tool to detect hotspots and coldspots of eight widely used open source frameworks. We show the utility of our detected hotspots by comparing these hotspots with the API classes reused by a real application and compare our results with the results of a previous related approach.
[framework hotspots, Java, application program interfaces, public domain software, SpotWeb, data mining, Documentation, Classification algorithms, Data mining, code-search-engine-based approach, USA Councils, Computer bugs, Web mining, Search engines, software reusability, API, framework coldspots, open source code mining]
Generic Patch Inference
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
A key issue in maintaining Linux device drivers is the need to update drivers in response to evolutions in Linux internal libraries. Currently, there is little tool support for performing and documenting such changes. In this paper we present a tool, spdiff, that identifies common changes made in a set of pairs of files and their updated versions, and extracts a generic patch performing those changes. Library developers can use our tool to extract a generic patch based on the result of manually updating a few typical driver files, and then apply this generic patch to other drivers. Driver developers can use it to extract an abstract representation of the set of changes that others have made. Our experiments on recent changes in Linux show that the inferred generic patches are more concise than the corresponding patches found in commits to the Linux source tree while being safe with respect to the changes performed in the provided pairs of driver files.
[Linux internal library, spdiff tool, generic patch inference, program diagnostics, device drivers, Compounds, software maintenance, software libraries, abstract representation, Linux device driver maintenance, Evolution (biology), Linux, Libraries, Safety, software tools, Pattern matching, Driver circuits]
Configuration Lifting: Verification meets Software Configuration
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Configurable software is ubiquitous, and the term software product line (SPL) has been coined for it lately. It remains a challenge, however, how such software can be verified over all variants. Enumerating all variants and analyzing them individually is inefficient, as knowledge cannot be shared between analysis runs. Instead of enumeration we present a new technique called lifting that converts all variants into a meta-program, and thus facilitates the configuration-aware application of verification techniques like static analysis, model checking and deduction-based approaches. As a side-effect, lifting provides a technique for checking software feature models, which describe software variants, for consistency. We demonstrate the feasibility of our approach by checking configuration dependent hazards for the highly configurable Linux kernel which possesses several thousand of configurable features. Using our techniques, two novel bugs in the kernel configuration system were found.
[operating system kernels, program verification, configuration lifting, configurable Linux kernel, verification techniques, static analysis, software configuration, meta-program, SPL, configuration management, Analytical models, software product line, Runtime, model checking, Linux, product development, software reusability, Software systems, deduction-based approaches, Software, Kernel, Driver circuits]
Product Line Tools are Product Lines Too: Lessons Learned from Developing a Tool Suite
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Tool developers are facing high expectations regarding the capabilities and usability of software engineering tools. Users expect tools which are tailored to their specific needs and integrated in their environment. This increases the complexity of tools and makes their customization more difficult, although numerous mechanisms supporting adaptability and extensibility are available. In this experience paper we report on the lessons we have learned when developing a tool suite for product line engineering. Our experiences suggest that software engineering tools should be designed as product lines.
[product line tools, Visualization, Automation, Computational modeling, Conferences, Adaptation model, Software, software tools, tool suite, software engineering tools, product line engineering, Software engineering]
A specification language for static analysis of student exercises
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
In this paper we use formal software engineering techniques to support one of the most difficult steps in software engineering: learning to use a programming language. In order to support numerous exercises of a large number of students some automatic support for checking the solutions and providing hints for the occurrence of erroneous code fragments is highly desirable to help students doing their work. For such support a specification language to describe search patterns for typical solution patterns in student exercises has been created. This language especially supports the structured description of complex search patterns. The specified patterns are transformed into graph rules, which are applied to student solutions in our testing support environment. The approach extends the work we presented in [5]. In addition to the application areas described here the search for a class of structures can be used in other areas like the check for design patterns or looking for code fragments where refactorings can be applied.
[Algorithm design and analysis, Java, computer science education, erroneous code fragment checking, student exercise, program testing, programming language learning, structured description, program diagnostics, graph theory, Programming, static analysis, formal specification, design pattern, Computer languages, graph transformation, formal software engineering technique, specification languages, testing support environment, Pattern matching, specification language, Software engineering]
A Case Study on the Automatic Composition of Network Application Mashups
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
MaxMash is a tool that can compose select features of networked application and generate the source code for application mashups that can integrate those features. This paper presents a case study that demonstrates how MaxMash is used to combine the Jabber chatting protocol and the Microsoft Maps Web application. The composed mashup is able to answer direction queries via a chat client.
[Protocols, Mashups, chat client, source code, Programming, distributed processing, MaxMash, Microsoft Maps Web application, direction queries, HTML, Servers, Data mining, Jabber chatting protocol, Distance measurement, Internet, automatic network application mashup composition]
Predicting Effectiveness of Automatic Testing Tools
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Automatic white-box test generation is a challenging problem. Many existing tools rely on complex code analyses and heuristics. As a result, structural features of an input program may impact tool effectiveness in ways that tool users and designers may not expect or understand. We develop a technique that uses structural program metrics to predict the test coverage achieved by three automatic test generation tools. We use coverage and structural metrics extracted from 11 software projects to train several decision tree classifiers. Our experiments show that these classifiers can predict high or low coverage with success rates of 82% to 94%.
[automatic testing tools, Java, program testing, structural program metrics, Training, automatic testing, decision tree classifiers, Automatic testing, automatic white-box test generation, decision trees, complex code analyses, Software, software projects, software tools, Decision trees, Testing, Classification tree analysis, software metrics]
Distributed Constraints Maintenance in Collaborative UML Modeling Environments
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Constraints maintenance plays an important role in keeping the integrity and validity of models in UML software modeling. Constraints maintenance capabilities are reasonably adequate in UML modeling applications, but few work has been done to address the distributed constraints maintenance issue in collaborative UML modeling environments. In this paper, we propose a novel solution to the issue, which can retain the effects of all concurrent modeling operations even though they may cause constraints violations. We further contribute a distributed constraints maintenance framework, in which the solution is encapsulated as a generic engine to be mounted in a variety of single-user UML modeling applications for supporting distributed collaborative UML modeling and distributed constraints maintenance.
[distributed constraint maintenance, Java, Unified Modeling Language, Computational modeling, Unified modeling language, Maintenance engineering, concurrent modeling operation, collaborative UML software modeling environment, software maintenance, Engines, Collaboration, concurrency control, groupware, Software, generic engine]
Software Cost Estimation using Fuzzy Decision Trees
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
This paper addresses the issue of software cost estimation through fuzzy decision trees, aiming at acquiring accurate and reliable effort estimates for project resource allocation and control. Two algorithms, namely CHAID and CART, are applied on empirical software cost data recorded in the ISBSG repository. Approximately 1000 project data records are selected for analysis and experimentation, with fuzzy decision trees instances being generated and evaluated based on prediction accuracy. The set of association rules extracted is used for providing mean effort value ranges. The experimental results suggest that the proposed approach may provide accurate cost predictions in terms of effort. In addition, there is strong evidence that the fuzzy transformation of cost drivers contribute to enhancing the estimation process.
[project management, project resource allocation, software reliability, Estimation, data mining, fuzzy set theory, fuzzy logic, Programming, fuzzy decision tree, Classification algorithms, Data mining, resource allocation, decision trees, Software, software cost estimation, association rule extraction, Decision trees, effort estimation, Driver circuits, reliable software cost estimation]
Rhizome: A Feature Modeling and Generation Platform
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Rhizome is an end-to-end feature modeling and code generation platform that includes a feature modeling language (FeatureML), a template language (MarkerML) and a template-based code generator. A software designer creates feature models using FeatureML by selecting and defining design choices. These design choices can be automatically associated with code templates and interpreted as parameter values for code generation. The code generator then replaces markers embedded in the code templates with dynamically generated code blocks to produce source code.
[Computational modeling, Rhizome end-to-end feature modeling, software design, source code, Data structures, Generators, Indexes, software maintenance, code interpretation, program compilers, program interpreters, software product line, FeatureML, MarkerML template language, product development, specification languages, feature modeling language, Software, Data models, Arrays, template-based code generation platform]
Combining the Analysis of Spatial Layout and Text to Support Design Exploration
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
The design exploration (DE) approach allows a large number of probable end users to communicate with software developers by creating mockups of user interfaces and augmenting the partial designs (e.g. windows and widgets) produced with textual descriptions and explanations. Software developers collect and analyze these annotated partial designs to gain an understanding of the user, their activity, and the domain of concern. This paper presents the DE analyzer and its techniques for automatically analyzing the collection of annotated partial designs. The DE Analyzer uses a combination of textual and spatial layout analysis algorithms to assist software developers' navigation and exploration of the information collected using the DE Builder.
[text analysis, Dictionaries, Navigation, design exploration approach, annotated partial design, user interfaces, spatial layout analysis, user interface, Construction industry, Layout, systems analysis, User interfaces, Software, software engineering, Graphical user interfaces]
Using Cluster Analysis to Improve the Design of Component Interfaces
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
For large software systems, interface structure has an important impact on their maintainability and build performance. For example, for complex systems written in C, recompilation due to a change in one central header file can run into hours. In this paper, we explore how automated cluster analysis can be used to refactor interfaces, in order to reduce the number of dependencies and to improve encapsulation, thus improving build performance and maintainability. We implemented our approach in a tool called "Interface Regroup Wizard\
[Encapsulation, application program interfaces, Merging, software systems, software maintainability, interface regroup wizard, Proposals, C language, software maintenance, pattern clustering, industrial embedded system, Software systems, Distance measurement, Software, component interfaces, cluster analysis, Software engineering]
Augmenting Counterexample-Guided Abstraction Refinement with Proof Templates
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Existing software model checkers based on predicate abstraction and refinement typically perform poorly at verifying the absence of buffer overflows, with analyses depending on the sizes of the arrays checked. We observe that many of these analyses can be made efficient by providing proof templates for common array traversal idioms idioms, which guide the model checker towards proofs that are independent of array size. We have integrated this technique into our software model checker, PtYasm, and have evaluated our approach on a set of testcases derived from the Verisec suite, demonstrating that our technique enables verification of the safety of array accesses independently of array size.
[Algorithm design and analysis, program verification, Computational modeling, array traversal idioms, software model checkers, predicate abstraction, buffer overflows verification, Discharges, counterexample-guided abstraction refinement, PtYasm, Verisec suite, Lead, proof templates, Software, Safety, Buffer overflow, safety verification]
An Assume Guarantee Verification Methodology for Aspect-Oriented Programming
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
We propose a modular verification methodology for aspect oriented programs. Aspects are the new modularization units to encapsulate crosscutting concerns and have powerful features whose effects can drastically change software behavior. Having such an impact on behavior requires an automated verification support. In this work we introduce a technique that separately answers two questions: "does the aspect have the provisioned effect?" and "does the base program satisfy the assumptions of the aspect?" To answer these questions modularly, we propose using design contracts and state machines as aspect interfaces. An aspect interface both closes the environment of the aspect and specifies its assumptions on any base code. We show that our approach can be used in verifying AspectJ programs modularly and checking compatibility for aspect reuse.
[Java, object-oriented programming, Programming, Synchronization, finite state machines, formal verification, modular verification methodology, software behavior, state machines, Automata, aspect oriented programs, AspectJ programs, aspect interfaces, Weaving, design contracts, Contracts, Software engineering]
Living with the Law: Can Automation give us Moore with Less?
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Multi-core programming presents developers with a dramatic paradigm shift. Whereas sequential programming largely allowed the decoupling of source from underlying architecture, it is now impossible to develop new patterns and abstractions in isolation from issues of modern hardware utilization. Synchronization and coordination are now manifested at all levels of the software stack, and developers currently lack the essential tools to even partially automate reasoning techniques and system configuration management. As a first stage to addressing this problem, this paper proposes a framework for a tool suite designed to partially automate the acquisition and management of static system visualization in a feedback loop with dynamic execution properties. This model enables developers to find a best fit system configuration, potentially reconciling resource contention and utilization tensions that are critical to multi-core platforms. The application of a prototype of this suite, Deja View, demonstrates how tool support can aid reasoning about causally related sets of changes across system artifacts.
[Deja View, Programming, system configuration management, static system visualization, feedback, sequential programming, feedback loop, Feedback loop, resource allocation, Computer architecture, Benchmark testing, Hardware, dynamic execution property, program control structures, software stack, modern hardware utilization, program diagnostics, NASA, resource contention, reasoning technique automation, utilization tensions, multicore programming, synchronisation, configuration management, multiprogramming, synchronization, Software, best fit system configuration, program visualisation]
VCR: Virtual Capture and Replay for Performance Testing
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
This paper proposes a novel approach to performance testing, called virtual capture and replay (VCR), that couples capture and replay techniques with the checkpointing capabilities provided by the latest virtualization technologies. VCR enables software performance testers to automatically take a snapshot of a running system when certain critical conditions are verified, and to later replay the scenario that led to those conditions. Several in-depth analyses can be separately carried out in the laboratory just by rewinding the captured scenario and replaying it using different probes and analysis tools.
[Checkpointing, virtual reality, program testing, Instruments, VCR, Quality of service, Video recording, virtual capture, checkpointing capabilities, virtualization technologies, software performance testing, virtual replay, Probes, Monitoring, Testing]
A Case for Automatic Exception Handling
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Exception handling mechanisms have been around for more than 30 years. Nevertheless, modern exceptions systems are not very different from the early models. Programming languages designers often neglect the exception mechanism and look at it more like an add-on for their language instead of central part. As a consequence, software quality suffers as programmers feel that the task of writing good error handling code is too complex, unattractive and inefficient. We propose a new model that automates the handling of exceptions by the runtime platform. This model frees the programmer from having to write exception handling code and, at the same time, successfully increases the resilience of programs to abnormal situations.
[memory allocation, Java, Object oriented modeling, exception handling, garbage collector, software quality, Servers, programming languages, error handling code, Computer languages, Runtime, error recovery, Writing, Software, automatic exception handling mechanism]
DiffGen: Automated Regression Unit-Test Generation
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Software programs continue to evolve throughout their lifetime. Maintenance of such evolving programs, including regression testing, is one of the most expensive activities in software development. We present an approach and its implementation called DiffGen for automated regression unit-test generation and checking for Java programs. Given two versions of a Java class, our approach instruments the code by adding new branches such that if these branches can be covered by a test generation tool, behavioral differences between the two class versions are exposed. DiffGen then uses a coverage-based test generation tool to generate test inputs for covering the added branches to expose behavioral differences. We have evaluated DiffGen on finding behavioral differences between 21 classes and their versions. Experimental results show that our approach can effectively expose many behavioral differences that cannot be exposed by state-of-the-art techniques.
[Java, coverage-based test generation tool, program testing, program verification, software development, Instruments, Production facilities, software maintenance, software program maintenance, Detectors, DiffGen automated regression unit-test generation, Software, Java program checking, software tools, Testing, Driver circuits]
An Automated Test Code Generation Method for Web Applications using Activity Oriented Approach
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Automated tests are important for Web applications as they grow more complex day by day. Web application testing frameworks have emerged to help satisfy this need. However, used without a model that is designed for system evolution and realization, maintaining test code becomes cumbersome and inefficient. This paper describes an activity oriented approach to engineer automated tests for Web applications with reference to a Web application developed for grant funding agencies. In this approach, the developer defines a domain model to represent application state, and uses a test activity graph comprised of self-validating user interactions to verify application correctness. We have implemented a test code generator called iTester using activity-oriented approach.
[automatic programming, activity oriented approach, test code maintenance, program testing, program verification, Object oriented modeling, Computational modeling, Unified modeling language, Web application testing framework, test activity graph, automated test code generation method, self-validating user interaction, Fixtures, Web pages, system realization, Software, application correctness verification, Internet, system evolution, Testing]
The Consistency of Web Conversations
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
We describe BPELCheck, a tool for statically analyzing interactions of composite Web services implemented in BPEL. Our algorithm is compositional, and checks each process interacting with an abstraction of its peers, without constructing the product state space. Interactions between pairs of peer processes are modeled using conversation automata which encode the set of valid message exchange sequences between the two processes. A process is consistent if each possible conversation leaves its peer automata in a state labeled as consistent and the overall execution satisfies a user-specified predicate on the automata states. We have implemented BPELCheck in the Enterprise Service Pack of the NetBeans development environment. Our tool handles the major syntactic constructs of BPEL, including sequential and parallel composition, exception handling, flows, and Boolean state variables. We have used BPELCheck to check conversational consistency for a set of BPEL processes, including an industrial example.
[NetBeans development environment, Peer to peer computing, automata theory, composite Web services, conversation automata, exception handling, Credit cards, Authorization, Computer science, Enterprise Service Pack, message exchange sequences, Boolean functions, BPELCheck, Web services, Automata, XML, Web conversations, Boolean state variables, peer automata]
Testing Peers' Volatility
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Peer-to-peer (P2P) is becoming a key technology for software development, but still lacks integrated solutions to build trust in the final software, in terms of correctness and security. Testing such systems is difficult because of the high numbers of nodes which can be volatile. In this paper, we present a framework for testing volatility of P2P systems. The framework is based on the individual control of peers, allowing test cases to precisely control the volatility of peers during execution. We validated our framework through implementation and experimentation on two open-source P2P systems. Through experimentation, we analyze the behavior of both systems on different conditions of volatility and show how the framework is able to detect implementation problems.
[peer-to-peer technology, Schedules, Java, peer-to-peer computing, program testing, software development, Peer to peer computing, Routing, P2P system volatility, Delay, Software, open-source P2P system, Testing]
Automated Continuous Integration of Component-Based Software: An Industrial Experience
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
When a software product is composed of dozens of or even hundreds of components with complicated dependency relationship among each other, one component's change can affect lots of other components' behavior. Sometimes, therefore, the stabilization job with multiple updated components drives the people who are responsible for integration and release the software into an integration hell. To avoid this kind of integration hell, we established integration procedure which encourages the developers frequent and small releases. We also created an automated integration system which continuously runs integration process in an incremental way so as to create and maintain an up-to-minute reasonably stable version of the product release candidate. In this paper, we introduce our integration procedure and automated integration system for a software product with hundreds of components, and a few lessons learned from the implementing and applying the system as well.
[component-based software, complicated dependency relationship, Automation, object-oriented programming, automated integration system, Buildings, Control systems, automated continuous integration, Lead, Feature extraction, Software, software product, Testing]
A Two-Step Approach for Modelling Flexibility in Software Processes
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
In this paper, we present a two-step approach for modelling controlled flexibility in software processes: (1) senior process engineers express which, where and how changes can be applied onto process elements, (2) other process participants can easily identify which changes they are allowed to perform, and act accordingly. To support this, we propose a flexibility meta-model and modelling language, and a software process modelling tool called FlexEPFC. Finally, we compare FlexEPFC with three other prominent process-aware tools: SPADE, JIL/Juliette and Jazz.
[Unified Modeling Language, software process flexibility modelling, Unified modeling language, Process control, FlexEPFC, Modeling, modelling language, Analytical models, Evolution (biology), Software, software engineering, process-aware tools, flexibility meta-model, Software engineering]
A Generic Approach for Class Model Normalization
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Designing and maintaining a huge class model is a very complex task. When an object oriented software or model grows, duplicated elements start to appear, decreasing the readability and the maintainability. In this paper, we present an approach, implemented in a tool and validated by a case study, that helps software architects designing and improving their class models, discarding redundancy and adding relevant abstractions. Since many different languages allow to express class models, this approach has been made generic i.e. capable of dealing with any language described by a meta-model.
[object-oriented programming, Biological system modeling, Object oriented modeling, Unified modeling language, software abstraction, Lattices, object oriented software, Encoding, formal specification, relational concept analysis, software architect design, Analytical models, software architecture, generic approach, class model normalization, software tools, Context modeling]
XE (eXtreme editor) - bridging the aspect-oriented programming usability gap
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
In spite of the modularization benefits supported by the Aspect-Oriented programming paradigm, different usability issues have hindered its adoption. The decoupling between aspect definitions and base code, and the compile-time weaving mechanism adopted by different AOP languages, require developers to manage the consistency between base code and aspect code themselves. These mechanisms create opportunities for errors related to aspect weaving invisibility and non-local control characteristics of AOP languages. This paper describes XE (Extreme Editor), an IDE that supports developers in managing these issues in the functional aspect-oriented programming domain.
[object-oriented programming, functional programming, Banking, compile-time weaving mechanism, Programming, extreme editor, user interfaces, IDE, program compilers, functional aspect-oriented programming domain, Fluids, Evolution (biology), Authentication, usability issue, Weaving, Usability, programming environments]
Rapid: Identifying Bug Signatures to Support Debugging Activities
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Most existing fault-localization techniques focus on identifying and reporting single statements that may contain a fault. Even in cases where a fault involves a single statement, it is generally hard to understand the fault by looking at that statement in isolation. Faults typically manifest themselves in a specific context, and knowing that context is necessary to diagnose and correct the fault. In this paper, we present a novel fault-localization technique that identifies sequences of statements that lead to a failure. The technique works by analyzing partial execution traces corresponding to failing executions and identifying common segments in these traces, incrementally. Our approach provides developers a context that is likely to result in a more directed approach to fault understanding and a lower overall cost for debugging.
[program debugging, software debugging, Instruments, fault localization, Debugging, bug signatures, Data mining, software fault tolerance, Fault diagnosis, USA Councils, Information filters, failing execution, fault understanding, Software engineering]
Heuristics for Scalable Dynamic Test Generation
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Recently there has been great success in using symbolic execution to automatically generate test inputs for small software systems. A primary challenge in scaling such approaches to larger programs is the combinatorial explosion of the path space. It is likely that sophisticated strategies for searching this path space are needed to generate inputs that effectively test large programs (by, e.g., achieving significant branch coverage). We present several such heuristic search strategies, including a novel strategy guided by the control flow graph of the program under test. We have implemented these strategies in CREST, our open source concolic testing tool for C, and evaluated them on two widely-used software tools, grep 2.2 (15 K lines of code) and Vim 5.7 (150 K lines). On these benchmarks, the presented heuristics achieve significantly greater branch coverage on the same testing budget than concolic testing with a traditional depth-first search strategy.
[program testing, Force, software systems, Aerospace electronics, path space, Flow graphs, control flow graph, Benchmark testing, symbolic execution, open source concolic testing tool, Distance measurement, Software, software tools, depth-first search strategy, scalable dynamic test generation, Testing]
Managing Models through Macromodeling
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Software development involves the creation and use of many related models yet there are few tools that address the issue of how to work with and manage such collections of models, or "multimodels." We propose a formal multimodeling framework that allows specialized model relationship types to be defined on existing types of models and provides a new type of model with a formal semantics called a macromodel. Macromodels can be used to enhance multimodel development, comprehension, consistency management and evolution. A preliminary evaluation of the framework is done using a detailed example from the telecommunications domain.
[macromodeling, software development, Computational modeling, Unified modeling language, software development management, Telecommunications, Object recognition, consistency management, formal specification, multimodel development, multimodels, formal semantics, model relationship types, Software, entity-relationship modelling, formal multimodeling framework, Context modeling, Software engineering]
Cleman: Comprehensive Clone Group Evolution Management
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Recent research results have shown more benefits of the management of code clones, rather than detecting and removing them. However, existing management approaches for code clone group evolution are still ad hoc, unsatisfactory, and limited. In this paper, we introduce a novel method for comprehensive code clone group management in evolving software. The core of our method is Cleman, an algorithmic framework that allows for a systematic construction of efficient and accurate clone group management tools. Clone group management is rigorously formulated by a formal model, which provides the foundation for Cleman framework. We use Cleman framework to build a clone group management tool that is able to detect high-quality clone groups and efficiently manage them when the software evolves. We also conduct an empirical evaluation on real-world systems to show the flexibility of Cleman framework and the efficiency, completeness, and incremental updatability of our tool.
[code clone group evolution management, Cleman framework, Cloning, Manuals, software evolution, Evolution (biology), formal model, evolving software, comprehensive code clone group management, Software systems, Feature extraction, Software, Distance measurement, software engineering, algorithmic framework]
Composition of Qualitative Adaptation Policies
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
In a highly dynamic environment, software systems requires a capacity of self-adaptation to fit the environment and the user needs evolution, which increases the software architecture complexity. Despite most current execution platforms include some facilities for handling dynamic adaptation, current design methodologies do not address this issue. One of the requirement for such a design process is to describe adaptation policies in a composable and qualitative fashion in order to cope with complexity. This paper introduces an approach for describing adaptation policies in a qualitative way while keeping the compositionality of adaptation policies. The basic example of a Web server is used to illustrate how to specify and to compose two adaptations policies which handle respectively the use of a cache and the deployment of new data sources.
[Reactive power, software architecture, Adaptation model, software systems, Service oriented architecture, software architecture complexity, Fractals, Servers, qualitative adaptation policies, Web server, Load modeling, software metrics]
A System for Supporting Development of Large Scaled Rich Internet Applications
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Rich Internet Application (RIA) has been proposed in order to solve the problems of current web applications. The user experience of current web applications is not comparable to desktop applications. RIA provides sophisticated interfaces for representing complex processes and data. Therefore, it requires collaboration between designers who design the interface and animation of an application and developers who implement business logics. In the development process of an application, the change of design is usually happened and it requires not only designer's work but also developer's work.Therefore, it costs a lot in large scaled applications. This paper provides a system which can provide complete separation of designer's and developer's work in source code level in order to reduce development costs for RIAs. In addition, we introduce this system into a practical system and evaluate its utility.
[development costs, Flexible printed circuits, business logics, Rich Internet Application, sophisticated interfaces, HTML, user interfaces, current Web applications, animation, computer animation, Runtime, XML, groupware, Software, Internet, Usability, source code level, interface design]
Discovering Patterns of Change Types
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
The reasons why software is changed are manyfold; new features are added, bugs have to be fixed, or the consistency of coding rules has to be re-established. Since there are many types of of source code changes we want to explore whether they appear frequently together in time and whether they describe specific development activities. We describe a semi-automated approach to discover patterns of such change types using agglomerative hierarchical clustering. We extracted source code changes of one commercial and two open-source software systems and applied the clustering. We found that change type patterns do describe development activities and affect the control flow, the exception flow, or change the API.
[Java, application program interfaces, public domain software, software prototyping, data mining, source code change extraction, Encoding, History, Data mining, exception flow, semiautomated approach, agglomerative hierarchical clustering, Guidelines, software evolution, pattern clustering, control flow, Software systems, Software, API, software change type pattern discovery, open-source software system]
Model-Driven Development of Mobile Personal Health Care Applications
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Personal health care applications on mobile devices allow patients to enhance their health via reminders, monitoring and feedback to health care providers. Engineering such applications is challenging with a need for health care plan meta-models, per-patient instantiation of care plans, and development and deployment of supporting Web and mobile device applications. We describe a novel prototype environment for visually modelling health care plans and automated plan and mobile device application code generation.
[Visualization, mobile personal health care, metamodel, Medical services, Mobile communication, Mobile handsets, patient care, patient monitoring, model-driven development, health care plan, mobile device, mobile computing, Web application, application code generation, Internet, Biomedical monitoring, Monitoring, health care, Load modeling]
Enforcing Structural Regularities in Source Code using IntensiVE
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
The design and implementation of a software system is often governed by many different coding conventions, design patterns, architectural design rules, and other so-called structural regularities. To prevent a deterioration of the system's source code, it is important that these regularities are verified and enforced in subsequent evolutions of the system. The Intensional Views Environment (IntensiVE), which is the subject of this demonstration, is a tool suite for documenting such structural regularities in (object-oriented) software systems and verifying their consistency in later versions of those systems.
[object-oriented programming, source coding, Object oriented modeling, object-oriented software systems, Documentation, source code, Programming, IntensiVE, Production facilities, Encoding, software system, architectural design rules, tool suite, software architecture, coding conventions, Intensional Views Environment, design patterns, Software systems, structural regularities, Software, software tools]
QuARS Express - A Tool Demonstration
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Requirements analysis is an important phase in a software project. Automatic evaluation of natural language (NL) requirements documents has been proposed as a means to improve the quality of the system under development. QuARS Express is an automatic analyzer of natural language (NL) requirements able to manage complex and structured requirement documents containing metadata, and to produce an analysis report rich of information that points out linguistic defects and indications about the writing style of NL requirements.
[document handling, meta data, linguistic defects, metadata, natural language processing, Natural languages, computational linguistics, QuARS Express, natural language requirements documents, Indexes, software project, requirements analysis, Analytical models, Fires, systems analysis, Writing, tool demonstration, Software, Graphical user interfaces]
MTSA: The Modal Transition System Analyser
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Modal transition systems (MTS) are operational models that distinguish between required and proscribed behaviour of the system to be and behaviour which it is not yet known whether the system should exhibit. MTS, in contrast with traditional behaviour models, support reasoning about the intended system behaviour in the presence of incomplete knowledge. In this paper, we present MTSA a tool that supports the construction, analysis and elaboration of Modal Transition Systems (MTS).
[tool support, Analytical models, Computational modeling, program diagnostics, system behaviour, Merging, System recovery, Inspection, Cognition, software tools, modal transition system analyser, Construction industry]
MaramaEML: An Integrated Multi-View Business Process Modelling Environment with Tree-Overlays, Zoomable Interfaces and Code Generation
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
MaramaEML is an integrated support tool for the enterprise modelling language (EML) built using the Eclipse based Marama framework. It provides support for multiple visual notations including: the business process modelling notation (BPMN); the EML tree- based, multi-layer hierarchical service representation; fisheye zooming capabilities; automatic BPEL code generation; and inter-notation mapping.
[Visualization, multiple visual notation, Computational modeling, Unified modeling language, trees (mathematics), Eclipse based Marama framework, Complexity theory, MaramaEML, Engines, multiview business process modelling, Web services, code generation, tree-overlays, simulation languages, enterprise modelling language, zoomable interface, business data processing, Business]
PtYasm: Software Model Checking with Proof Templates
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
We describe PTYASM, an enhanced version of the YASM software model checker which uses proof templates. These templates associate correctness arguments with common programming idioms, thus enabling efficient verification. We have used PTYASM to verify the safety of array accesses in programs derived from the Verisec suite. PTYASM is able to verify this property in the majority of testcases, while existing software model checkers fail to do so due to loop unrolling.
[program control structures, program verification, Instruments, Computational modeling, Programming, PTYASM, Indexes, software model checking, formal specification, loop unrolling, YASM software model checker, Verisec suite, proof templates, Software, Safety, Arrays, programming idioms]
Semi-Automating Pragmatic Reuse Tasks
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Developers undertaking a pragmatic reuse task must collect and reason about information that is spread throughout the source code of a system before they can understand the scope of their task. Once they have this understanding, they can undertake the manual process of actually reusing the source code within their system. We have created a tool environment to help developers plan and perform pragmatic reuse tasks enabling them to reason about and perform larger reuse tasks than they generally feel comfortable attempting otherwise.
[Software maintenance, Automation, Navigation, source coding, Conferences, Manuals, source code, tool environment, pragmatic reuse tasks, software reusability, Software, Planning, software tools]
DUALLY: A framework for Architectural Languages and Tools Interoperability
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Nowadays different notations for architectural modeling have been proposed, each one focussing on a specific application domain, analysis type, or modeling environment. No effective interoperability is possible to date. DUALLY is an automated framework that aims to offer an answer to this need allowing architectural languages and tools interoperability. DUALLY has been implemented as an Eclipse plugin and it is based on model transformation techniques. This demonstration paper shows DUALLY by applying its approach to two outstanding architectural description languages.
[open systems, Biological system modeling, Computational modeling, Unified modeling language, Adaptation model, tool interoperability, architectural modeling, DUALLY framework, software architecture, Computer architecture, architectural language, Software, Weaving, software tools]
Automated Mapping from Goal Models to Self-Adaptive Systems
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Self-adaptive systems should autonomously adapt at run time to changes in their operational environment, guided by the goals assigned by their stakeholders. We present a tool that supports goal-oriented modelling and generation of code for goal-directed, self-adaptive systems, supporting Tropos4AS, an extension of the software engineering methodology Tropos.
[Java, automated mapping, Adaptive systems, Adaptation model, Unified modeling language, goal models, program compilers, Analytical models, Tropos, software engineering, self-adaptive systems, Tropos4AS, Robots, Software engineering, goal-oriented modelling]
ADDSS: Architecture Design Decision Support System Tool
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
This paper describes the ADDSS tool which enables capturing and documenting architectural design decisions in order to avoid knowledge vaporization.
[software architecture, Software architecture, Evolution (biology), Conferences, Computer architecture, architectural design decision support system tool, Maintenance engineering, Software, Libraries, software tools, decision support systems]
APPAREIL: A Tool for Building Automated Program Translators Using Annotated Grammars
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Operations languages are used to write spacecraft operations procedures. The APPAREIL tool automates the process of generating program translators between operations languages, from a specification of their language grammar annotated with extra information. From these annotated grammars the tool automatically produces a partial translator that covers most of the translation. This translator needs to be augmented manually with specific transformations, to deal with the more complicated cases. To get more confidence on the correctness of the translation, the tool offers a control-flow equivalence verification module.
[control-flow equivalence verification module, annotated grammar, spacecraft application, Telemetry, Flow graphs, Construction industry, Space vehicles, program interpreters, Computer languages, APPAREIL tool, automated program translator, grammars, Focusing, specification languages, aerospace computing, Libraries, operations language]
Rapid Model-Driven Prototyping and Verification of High-Integrity Real-Time Systems
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Model-driven technologies offer a most attractive framework for rapid, iterative software development cycles by facilitating a productive merge of high-level modeling with automated model transformation and verification. In the high- integrity application domain it is of paramount importance to assure semantics preservation across each view of the system produced as part of the development process, most notably the design model, the analysis model, the executable. In this short paper we present a toolset that assures the preservation of properties of interest across all stages of an MDE-geared development.
[Real time systems, design model, Biological system modeling, Computational modeling, software prototyping, model driven engineering, analysis model, rapid model driven prototyping, Analytical models, MDE-geared development, formal verification, iterative software development cycles, Prototypes, Computer architecture, automated model transformation, Timing, high integrity real-time systems, automated model verification]
Save-IDE: An Integrated Development Environment for Building Predictable Component-Based Embedded Systems
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
In this paper we present an Integrated Development Environment Save-IDE, a toolset that embraces several tools: a tool for designing component-based systems and components, modeling and predicting certain run-time properties, such as timing properties, and transforming the components to real-time execution elements. Save-IDE is specialized for the domain of dependable embedded systems, which in addition to standard design tools requires tool support for analysis and verification of particular properties of such systems.
[object-oriented programming, run-time property, Save-IDE, dependable embedded systems, System analysis and design, Analytical models, Embedded system, Automata, embedded systems, integrated development environment, Skeleton, Software, Timing, object-oriented methods, component-based embedded systems, real-time execution elements]
The Clem Toolkit
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
In this demonstration session, we present a toolkit we have designed around a model-driven language fLEJ. This relies on formal methods to ease the development of applications in an efficient and reusable way. Formal methods have been advocated as a means of increasing the reliability of systems, especially those which are safety or business critical. It is still difficult to develop automatic specification and verification tools due to limitations like state explosion, undecidability, etc. To face these problems, we provide LE with a constructive semantic that allows the modular compilation of programs into software and hardware targets (C code, VHDL code, FPGA synthesis, Verification tools). Moreover, we also provide software to design, compile and verify LE programs. Our approach is pertinent according to the two main requirements of critical realistic applications: the modular compilation allows us to deal with large systems, the model-driven approach provides us with formal validation.
[modular program compilation, codes, field programmable gate arrays, constructive semantic, VHDL code, FPGA synthesis, hardware description languages, C code, formal validation, formal specification, Equations, System analysis and design, Sorting, verification tools, model-driven language fLEJ, automatic specification tools, formal verification, Automata, formal methods, Software, Data models, CLEM toolkit, Mathematical model]
Tool Support for Parametric Analysis of Large Software Simulation Systems
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
The analysis of large and complex parameterized software systems, e.g., systems simulation in aerospace, is very complicated and time-consuming due to the large parameter space, and the complex, highly coupled nonlinear nature of the different system components. Thus, such systems are generally validated only in regions local to anticipated operating points rather than through characterization of the entire feasible operational envelope of the system. We have addressed the factors deterring such an analysis with a tool to support envelope assessment: we utilize a combination of advanced Monte Carlo generation with n-factor combinatorial parameter variations to limit the number of cases, but still explore important interactions in the parameter space in a systematic fashion. Additional test-cases, automatically generated from models (e.g., UML, Simulink, Stateflow) improve the coverage. The distributed test runs of the software system produce vast amounts of data, making manual analysis impossible. Our tool automatically analyzes the generated data through a combination of unsupervised Bayesian clustering techniques (AutoBayes) and supervised learning of critical parameter ranges using the treatment learner TAR3. The tool has been developed around the Trick simulation environment, which is widely used within NASA. We will present this tool with a GN&amp;C (Guidance, Navigation and Control) simulation of a small satellite system.
[Unified modeling language, digital simulation, Simulink, satellite system, Analytical models, Monte Carlo methods, unsupervised Bayesian clustering techniques, complex parameterized software systems, software tools, Object oriented modeling, Atmospheric modeling, program diagnostics, large software simulation systems, NASA, AutoBayes, tool support, n-factor combinatorial parameter variations, advanced Monte Carlo generation, UML, systems simulation, parametric analysis, Stateflow, Data models, Software, Bayes methods]
ReqsCov: A Tool for Measuring Test-Adequacy over Requirements
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
When creating test cases for software, a common approach is to create tests that exercise requirements. Determining the adequacy of test cases, however, is generally done through inspection or indirectly by measuring structural coverage of an executable artifact (such as source code or a software model). We present ReqsCov, a tool to directly measure requirements coverage provided by test cases. ReqsCov allows users to measure Linear Temporal Logic requirements coverage using three increasingly rigorous requirements coverage metrics: naive coverage, antecedent coverage, and Unique First Cause coverage. By measuring requirements coverage, users are given insight into the quality of test suites beyond what is available when solely using structural coverage metrics over an implementation.
[Software testing, Automation, program testing, program verification, software test case, antecedent coverage, NASA, Artificial neural networks, software inspection, temporal logic, linear temporal logic, naive coverage, formal specification, software tool, ReqsCov, unique first cause coverage, Software, requirements coverage measurement, Software measurement, Testing, software metrics]
AspectM: UML-Based Extensible AOM Language
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
AspectM, a UML-based aspect-oriented modeling (AOM) language, provides not only basic modeling constructs but also an extension mechanism called metamodel access protocol (MMAP) that allows a modeler to extend the AspectM metamodel. MMAP enables a modeler to construct domain-specific AOM languages at relatively low cost. In this paper, we show the overview of an AspectM support tool consisting of a reflective model editor and a verifying model weaver.
[Navigation, program verification, Unified Modeling Language, Biological system modeling, Unified modeling language, metamodel access protocol, Access protocols, Programming, AspectM metamodel, AspectM support tool, Reflection, MMAP, model weaver verification, UML, object-oriented languages, software tools, reflective model editor, extensible aspect-oriented modeling language]
Tools for Traceability in Secure Software Development
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
For secure and dependable software system development, one must ensure that security requirements are truly traceable to design and implementation, and the traceability links can be updated accordingly to changed entities. To address this, we present a suite of security requirements analysis and traceability assurance tools and demonstrate how they are effectively integrated.
[Protocols, program verification, Unified Modeling Language, Object oriented modeling, program diagnostics, Unified modeling language, security requirements analysis, Programming, Security, secure software development, UMLSec, security of data, dependable software development, Software, software tools, Cryptography, software traceability assurance tool]
Unifying Analysis Tools with Rosetta
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
The Rosetta system specification language will require a variety of analysis capabilities to assist system designers. The language's generality prohibits the development of a single analysis tool. It is proposed, instead, to leverage the existing analysis tools and create an analysis environment unified around the Rosetta language. A semi-automated tool, the Rosetta Nexus, will generated tool-specific analysis models and a correspondence with the original Rosetta specifications.
[Algorithm design and analysis, Energy consumption, Programming, analysis capability tool, Specification languages, Analytical models, Computer languages, system designer, systems analysis, specification languages, software tools, Rosetta system specification language, Software engineering]
Model-Driven Development of Mobile Applications
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
This research aims to simplify the creation of applications for mobile platforms by developing a high-level and platform independent model of an application, and automatically transforming this high-level model to platform specific code. The research method is a combination of the model-driven development (MDD) approach in software development and application of techniques in the field of human-computer interaction (HCI) particularly on user- centered system design. This research involves developing a graphical modeling language which is specific to mobile applications and coming up with a generic algorithm for the conversion of this graphical model into code. The main focus of the research however, is on the design of the graphical model, and the interaction techniques which will allow non-expert people to create specialized mobile applications with case.
[object-oriented programming, software development, Computational modeling, Unified modeling language, mobile application, graphical modeling language, Programming, Mobile communication, Mobile handsets, user-centered system design, model-driven development, program visualization, computer graphics, mobile computing, human-computer interaction, Prototypes, specification languages, human computer interaction, high-level model, Mathematical model, program visualisation, user centred design]
Automated Web Performance Analysis
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Performance is a key feature in many systems nowadays. There are several tools on the market that ensure and test for adequate performance. They, can be divided into simulation tools and monitoring tools. But only a few automatise and combine both approaches. This paper describes a system capable of automatically creating a web performance simulation and conducting trend analysis of the system under test (SUT). To achieve this the system requires input information, like Monitoring Points and Static-Information about the SUT. The system monitors and analyses the SUT and based on this information generates a simulation model of the system. The simulation model is refinded stepwise e.g. by adding or removing connections between the model components or adjusting the parameters until the aimed accuracy is achieved. With the help of the simulation model a prediction module creates an analysis of the SUT, and thereby can give as much information about the current state of the system and potential trends as possible. This predictive information can be used for pro-active server tuning or other performance optimisations. The focus of my PhD thesis is on the adjustment and prediction part of the system described here. For all other parts, already existing tools and techniques will be used where possible. This initial paper outlines the complete system.
[program testing, Computational modeling, Predictive models, simulation tools, Complexity theory, monitoring tools, Analytical models, system monitoring, Software, Data models, Internet, automated Web performance analysis, Monitoring, software performance evaluation]
Automatic Test Generation for LUSTRE/SCADE Programs
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Lustre is a declarative, data-flow language, which is devoted to the specification of synchronous and real-time applications. It ensures efficient code generation and provides formal specification and verification facilities. A graphical tool dedicated to the development of critical embedded systems and often used by industries and professionals is SCADE (Safety Critical Application Development Environment). SCADE is a graphical environment based on the LUSTRE language and it allows the hierarchical definition of the system components and the automatic code generation. This research work is partially concerned with Lutess, a testing environment which automatically transforms formal specifications into test data generators.
[automatic test pattern generation, program testing, program verification, safety-critical software, LUSTRE program, Generators, Formal specifications, automatic code generation, formal specification, program compilers, Delay, declarative data-flow language, safety critical application development environment, formal verification, SCADE graphical tool, automatic test data generation, Software, Safety, software tools, parallel languages, Testing, Clocks]
Feature Interaction Detection in the Automotive Domain
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
The main goal of our research is to develop techniques for the detection of feature interactions for embedded systems in the automotive domain. Automotive systems are cyber-physical systems (CPS), which are composed of a "cyber" part that consists of software components running on digital hardware and a "physical" part that is the mechanical processes monitored by the software. Feature interactions in automotive systems arise from the activation of two or more features sending requests to the mechanical processes that create contradictory physical forces, possibly at distinct times, that cause unsafe behaviours. An example is having simultaneous requests to apply the brakes and the throttle. While both actions may be "correct" according to the intended behaviour of each feature, their interaction is undesired and potentially dangerous. To deal with the feature interaction problem, we propose to perform analysis at design time by using formal methods to detect all possible interactions.
[Actuators, automotive domain, Telecommunications, Automotive engineering, Vehicles, cyber-physical systems, Analytical models, mechanical engineering computing, embedded systems, automotive engineering, Feature extraction, feature interaction detection, Mathematical model]
Towards Good Enough Testing: A Cognitive-Oriented Approach Applied to Infotainment Systems
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
This contribution outlines a cognitive-oriented approach to construct test systems that can "partially " imitate several cognitive paradigms of skilled human testers. For example, learning, reasoning, optimization, etc. Hence, a reasonable portion of the workload done by a human tester would be shifted to the test system itself. This consequently leads to a substantial reduction in the development time and cost; yet the test efficiency is not sacrificed.
[program testing, Unified modeling language, Knowledge based systems, Humans, infotainment systems, cognitive-oriented approach, test systems, Cognition, Software, substantial reduction, Optimization, Testing]
ARAMIS 2008: The First Int. Workshop on Automated engineeRing of Autonomic and run-tiMe evolvIng Systems
2008 23rd IEEE/ACM International Conference on Automated Software Engineering
None
2008
Provides notice of upcoming conference events of interest to practitioners and researchers.
[]
Foreword
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Presents the welcome message from the conference proceedings.
[Conferences, Visual analytics, Feedback, Cities and towns, Programming, Collaborative work, Computer industry, Application software, Computer Society, Software engineering]
ASE 2009 Organization
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Provides a listing of current committee members and society officers.
[]
Program Committee
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Provides a listing of current committee members.
[]
A Petri Net Based Debugging Environment for QVT Relations
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
In the Model-Driven Architecture (MDA) paradigm the Query/View/Transformation (QVT) standard plays a vital role for model transformations. Especially the high-level declarative QVT Relations language, however, has not yet gained widespread use in practice. This is not least due to missing tool support in general and inadequate debugging support in particular. Transformation engines interpreting QVT Relations operate on a low level of abstraction, hide the operational semantics of a transformation and scatter metamodels, models, QVT code, and trace information across different artifacts. We therefore propose a model-based debugger representing QVT Relations on bases of TROPIC, a model transformation language utilizing a variant of Colored Petri Nets (CPNs). As a prerequisite for convenient debugging, TROPIC provides a homogeneous view on all artifacts of a transformation on basis of a single formalism. Besides that, this formalism also provides a runtime model, thus making the afore hidden operational semantics of the transformation explicit. Using an explicit runtime model allows to employ model-based techniques for debugging, e.g., using the Object Constraint Language (OCL) for simply defining breakpoints and querying the execution state of a transformation.
[Knowledge engineering, program debugging, debugging environment, QVT code, Petri nets, operational semantics, high-level declarative QVT Relations language, runtime model, transformation engines, model-based debugger, formal specification, Engines, software architecture, Runtime, model transformations, Logic, Informatics, Bioinformatics, QVT Relations, Petri net, Object Constraint Language, Scattering, Debugging, data flow analysis, Model Transformations, model transformation language, Query/View/Transformation standard, Colored Petri Nets, model-driven architecture, metamodels, programming environments, Software engineering]
Validating Automotive Control Software Using Instrumentation-Based Verification
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
This paper discusses the results of an application of a formally based verification technique, called Instrumentation-Based Verification (IBV), to a production automotive lighting controller. The goal of the study is to assess, from both a tools as well as a methodological perspective, the performance of IBV in an industrial setting. The insights obtained as a result of the project include a refinement of a previously developed architecture for requirements specifications; observations about changes to model-based design workflows; insights into the role of requirements during development; and the capability of automated verification to detect inconsistencies among requirements as well as between requirements and design models.
[Instruments, automotive control software validation, Educational institutions, Lighting control, Automotive engineering, formal verification, USA Councils, production automotive lighting controller, Production, Automatic control, model-based design workflows, Mathematical model, IBV, instrumentation-based verification, Monitoring, Software engineering]
Semi-automated Test Planning for e-ID Systems by Using Requirements Clustering
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
In acceptance testing, customer requirements as specified in system specifications have to be tested for their successful implementation. This is a time-consuming task due to inherent system complexity and thus a large number of requirements. In order to reduce efforts in acceptance testing, we introduce a novel approach that exploits redundancies and implicit relations in requirements specifications, which are based on multi-viewpoint techniques, in our case the reference model for open distributed processing (RM-ODP). It deploys requirements clustering and linguistic analysis techniques for reducing the total number of test cases. We report on concrete experiences with this approach within joint R&amp;D work of the Software Quality Lab (s-lab) of the University of Paderborn and HJP Consulting, an international consulting company, specialized in planning, procurement and acceptance testing of national electronic identification (e-ID) systems. The paper is concluded with an overview on the current tool support especially for automated detection of the redundancies and implicit relations in requirements. Also the future work on the tool support for the overall test specification process is discussed.
[Software testing, Biometrics, test specification, System testing, program testing, requirements clustering, Test Planning, RM-ODP, software quality, Open Distributed Processing Systems, biometrics (access control), formal specification, Distributed processing, Tree graphs, Software Quality Lab, HJP Consulting, Clustering algorithms, requirements specifications, s-lab, international consulting company, system specifications, national electronic identification systems, identification technology, reference model, open distributed processing, Requirements Clustering, linguistic analysis, Linguistic Analysis, Acceptance Testing, University of Paderborn, tool support, Automatic testing, system complexity, procurement, Software quality, semiautomated test planning, acceptance testing, e-ID systems, Concrete, multiviewpoint techniques, automated redundancy detection, Radiofrequency identification, customer requirements]
A Quantum Algorithm for Software Engineering Search
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Quantum computers can solve a few basic problems, such as factoring an integer and searching a database, much faster than classical computers. However, the complexity of software artifacts, and the types of questions software engineers ask about them, pose significant challenges for applying existing quantum approaches to software engineering search (SES) problems. This paper first describes a new quantum search algorithm, IDGS-FA, whose design is motivated by the characteristics of SES problems. Next, it describes how to apply quantum searching to three SES problems: FSM property checking, software test generation, and library-based software synthesis. Next, the paper gives the main ideas in QSAT, a novel toolkit supporting efficient simulation of the algorithms and applications discussed. Finally, it concludes with a substantial simulation-based study of IDGS-FA, showing that it improves both the reliability and speed of other approaches.
[Software testing, database searching, program testing, program verification, FSM property checking, search algorithm, modeling, quantum searching, simulation, simulation-based study, quantum algorithm, software libraries, software test generation, Concurrent computing, Quantum computing, Space technology, USA Councils, software tools, verification, validation, synthesis, Quantum entanglement, Computational modeling, Software algorithms, software artifact complexity, quantum computers, integer factoring, library-based software synthesis, QSAT, toolkit, Quantum mechanics, quantum computing, software engineering search problem, IDGS-FA, Software engineering, software metrics]
Understanding the Value of Software Engineering Technologies
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
When AI search methods are applied to software process models, then appropriate technologies can be discovered for a software project. We show that those recommendations are greatly affected by the business context of its use. For example, the automatic defect reduction tools explored by the ASE community are only relevant to a subset of software projects, and only according to certain value criteria. Therefore, when arguing for the value of a particular technology, that argument should include a description of the value function of the target user community.
[Project management, Programming, ASE community, business context, Appropriate technology, software economics, Equations, artificial intelligence, automatic defect reduction tools, Technology management, Search methods, Space technology, USA Councils, software projects subset, target user community, software process models, software engineering, software engineering technologies, Artificial intelligence, AI search methods, Software engineering]
Type Inference for Soft-Error Fault-Tolerance Prediction
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Software systems are becoming increasingly vulnerable to a new class of soft errors, originating from voltage spikes produced by cosmic radiation. The standard technique for assessing the source-level impact of these soft errors, fault injection - essentially a black-box testing technique - provides limited high-level information. Since soft errors can occur anywhere, even control-structured white-box techniques offer little insight. We propose a type-based approach, founded on data-flow structure, to classify the usage pattern of registers and memory cells. To capture all soft errors, the type system is defined at the assembly level, close to the hardware, and allows inferring types in the untyped assembly representation. In a case study, we apply our type inference scheme to a prototype brake-by-wire controller, developed by Volvo Technology, and identify a high correlation between types and fault-injection results. The case study confirms that the inferred types are good predictors for soft-error impact.
[Assembly systems, test selection and prioritization, program testing, software systems, type theory, Registers, source level impact, high-level information, Fault tolerance, brake-by-wire controller, Prototypes, memory cells, Voltage, Hardware, fault injection, soft errors, voltage spikes, Testing, type inference, assembly type system, cosmic radiation, black-box testing, untyped assembly representation, software fault tolerance, usage pattern, data flow structure, data flow computing, Computer errors, Software systems, Error correction, reasoning about programs, soft-error fault tolerance prediction]
Evaluating the Accuracy of Fault Localization Techniques
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
We investigate claims and assumptions made in several recent papers about fault localization (FL) techniques. Most of these claims have to do with evaluating FL accuracy. Our investigation centers on a new subject program having properties useful for FL experiments. We find that Tarantula (Jones et al.) works well on the program, and we show weak support for the assertion that coverage-based test suites help Tarantula to localize faults. Baudry et al. used automatically-generated mutants to evaluate the accuracy of an FL technique that generates many distinct scores for program locations. We find no evidence to suggest that the use of mutants for this purpose is invalid. However, we find evidence that the standard method for evaluating FL accuracy is unfairly biased toward techniques that generate many distinct scores, and we propose a fairer method of accuracy evaluation. Finally, Denmat et al. suggest that data mining techniques may apply to FL. We investigate this suggestion with the data mining tool Weka, using standard techniques for evaluating the accuracy of data mining classifiers. We find that standard classifiers suffer from the class imbalance problem. However, we find that adding cost information improves accuracy.
[Software testing, Fault localization, Costs, Data analysis, fault diagnosis, program locations, program testing, Genetic mutations, fault localization, Humans, data mining, coverage based test suites, Data mining, Programming profession, Computer science, mutation analysis, assertion, accuracy evaluation, Software engineering, Tarantula]
Towards a Comprehensive Test Suite for Detectors of Design Patterns
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Detection of design patterns is an important part of reverse engineering. Availability of patterns provides for a better understanding of code and also makes analysis more efficient in terms of time and cost. In recent years, we have observed a continual improvement in the field of automatic detection of design patterns in source code. Existing approaches can detect a fairly broad range of design patterns, targeting both structural and behavioral aspects of patterns. However, it is not straightforward to assess and compare these approaches. There is no common ground on which to evaluate the accuracy of the detection approaches, given the existence of variants and specific code constructs used to implement a design pattern. We propose a systematic approach to constructing a comprehensive test suite for detectors of design patterns. This approach is applied to construct a test suite covering the Singleton pattern. The test suite contains many implementation variants of these patterns, along with such code constructs as method forwarding, access modifiers, and long inheritance paths. Furthermore, we use this test suite to compare three detection tools and to identify their strengths and weaknesses.
[Software testing, code constructs, System testing, Costs, program testing, Reverse engineering, test suite, comprehensive test suite, design patterns, Detectors, object-oriented methods, forwarding method, Pattern analysis, object-oriented programming, detection, source coding, Documentation, design patterns detector, source code, reverse engineering, Automatic testing, inheritance paths, detection tools, Software systems, automatic detection, Singleton pattern, access modifiers, Software engineering]
Improving API Usage through Automatic Detection of Redundant Code
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Software projects often rely on third-party libraries made accessible through Application Programming Interfaces (APIs). We have observed many cases where APIs are used in ways that are not the most effective. We developed a technique and tool support to automatically detect such patterns of API usage in software projects. The main hypothesis underlying our technique is that client code imitating the behavior of an API method without calling it may not be using the API effectively because it could instead call the method it imitates. Our technique involves analyzing software systems to detect cases of API method imitations. In addition to warning developers of potentially re-implemented API methods, we also indicate how to improve the use of the API. Applying our approach on 10 Java systems revealed over 400 actual cases of potentially suboptimal API usage, leading to many improvements to the quality of the code we studied.
[Java, Automatic programming, Java systems, user interfaces, Application software, redundant code, code quality, recommendation system, Computer science, Degradation, code analysis, Software libraries, Software systems, software projects, software tools, automatic detection, Functional programming, Software tools, API usage, Software engineering]
Clone-Aware Configuration Management
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Recent research results show several benefits of the management of code clones. In this paper, we introduce Clever, a novel clone-aware software configuration management (SCM) system. In addition to traditional SCM functionality, Clever provides clone management support, including clone detection and update, clone change management, clone consistency validating, clone synchronizing, and clone merging. Clever represents source code and clones as (sub)trees in Abstract Syntax Trees (ASTs), measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of Clever include the algorithms to compute tree editing scripts; to detect and update code clones and their groups; and to analyze the changes of cloned code to validate their consistency and recommend the relevant synchronization. Our empirical study on many real-world programs shows that Clever is highly efficient and accurate in clone detection and updating, and provides useful analysis of clone changes.
[Algorithm design and analysis, Merging, Conference management, software management, clone synchronization, clone-aware software configuration management, abstract syntax trees, clone consistency validating, Clever, Engineering management, clone change management, clone merging, clone detection, AST, Change detection algorithms, clone synchronizing, Collaborative software, SCM system, tree edit, clone editing consistency, Cloning, clone-aware, tree editing scripts, configuration management, code clones, clone management support, real-world programs, clone management, Collaboration, Software systems, Software engineering]
SMT-Based Bounded Model Checking for Embedded ANSI-C Software
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Propositional bounded model checking has been applied successfully to verify embedded software but is limited by the increasing propositional formula size and the loss of structure during the translation. These limitations can be reduced by encoding word-level information in theories richer than propositional logic and using SMT solvers for the generated verification conditions. Here, we investigate the application of different SMT solvers to the verification of embedded software written in ANSI-C. We have extended the encodings from previous SMT-based bounded model checkers to provide more accurate support for variables of finite bit width, bit-vector operations, arrays, structures, unions and pointers. We have integrated the CVC3, Boolector, and Z3 solvers with the CBMC front-end and evaluated them using both standard software model checking benchmarks and typical embedded software applications from telecommunications, control systems, and medical devices. The experiments show that our approach can analyze larger problems and substantially reduce the verification time.
[Surface-mount technology, propositional logic, computability, Telecommunication control, Boolector, embedded software verification, SMT solvers, formal specification, Embedded software, telecommunications, formal verification, control systems, Control system synthesis, Satisfiability Modulo Theories, ANSI standards, Software standards, CBMC front-end, Logic, CVC3, Z3 solvers, word-level information encoding, embedded ANSI-C software, Medical control systems, SMT-based bounded model checking, Telecommunication standards, Encoding, Application software, encoding, propositional bounded model checking, Bounded Model Checking, medical devices, Embedded ANSI-C Software]
Static Validation of C Preprocessor Macros
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
The widely used C preprocessor (CPP) is generally considered a source of difficulty for understanding and maintaining C/C++ programs. The main reason for this difficulty is CPP's purely lexical semantics, i.e., its treatment of both input and output as token streams. This can easily lead to errors that are difficult to diagnose, and it has been estimated that up to 20% of all macros are erroneous. To reduce such errors, more restrictive, replacement languages for CPP have been proposed to limit expanded macros to be valid C syntactic units. However, there is no practical tool that can effectively validate CPP macros in legacy applications. In this paper, we introduce a novel, general characterization of inconsistent macro usage as a strong indicator of macro errors. Our key insight is that all applications of the same macro should behave similarly. In particular, we map each macro call c in a source file f to c's normalized syntactic constructs within the abstract syntax tree (AST) for f's preprocessed source, and use syntactic similarity as the basis for comparing macro calls of the same macro definition. Utilizing this characterization, we have developed an efficient algorithm to statically validate macro usage in C/C++ programs. We have implemented the algorithm; evaluation results show that our tool is effective in detecting common macro-related errors and reports few false positives, making it a practical tool for validating macro usage.
[program verification, Laboratories, Subcontracting, Research and development, C preprocessor macros, inconsistencies, static validation, macro errors, preprossing, Performance analysis, token streams, Contracts, macros, legacy applications, Engineering profession, lexical semantics, program diagnostics, C syntactic units, Government, abstract syntax tree, C++ language, software maintenance, C++ programs, Programming profession, Computer languages, C programs, Software engineering]
Looper: Lightweight Detection of Infinite Loops at Runtime
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
When a running program becomes unresponsive, it is often impossible for a user to determine if the program is performing some useful computation or if it has entered an infinite loop. We present LOOPER, an automated technique for dynamically analyzing a running program to prove that it is non-terminating. LOOPER uses symbolic execution to produce simple non-termination arguments for infinite loops dependent on both program values and the shape of heap. The constructed arguments are verified with an off-the-shelf SMT solver. We have implemented our technique in a prototype tool for Java applications, and we demonstrate our technique's effectiveness on several non-terminating benchmarks, including a reported infinite loop bug in open-source text editor jEdit. Our tool is able to dynamically detect infinite loops deep in the execution of large Java programs with no false warnings, producing symbolic arguments that can aid in debugging non-termination.
[open-source text editor, Surface-mount technology, Shape, simple nontermination arguments, Open source software, Jacobian matrices, lightweight detection, Runtime, USA Councils, Prototypes, program analysis, infinite loops, Java programs, Java, SMT solver, jEdit, program diagnostics, program values, Debugging, running program, symbolic arguments, Java application, symbolic execution, Software engineering, LOOPER]
Improving the Efficiency of Dependency Analysis in Logical Decision Models
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
To address the problem that existing software dependency extraction methods do not work on higher-level software artifacts, do not express decisions explicitly, and do not reveal implicit or indirect dependencies, our recent work explored the possibility of formally defining and automatically deriving a pairwise dependence relation from an augmented constraint networks (ACN) that models the assumption relation among design decisions. The current approach is difficult to scale, requiring constraint solving and solution enumeration. We observe that the assumption relation among design decisions for most software systems can be abstractly modeled using a special form of ACN. For these more restrictive, but highly representative models, we present an O(n3) algorithm to derive the dependency relation without solving the constraints. We evaluate our approach by computing design structure matrices for existing ACNs that model multiple versions of heterogenous real software designs, often reducing the running time from hours to seconds.
[dependency analysis, constraint solving, Scalability, Unified modeling language, Programming, software dependency extraction method, version modelling, design decisions, Software design, USA Councils, augmented constraint network, Large-scale systems, augmented constraint networks, heterogenous real software design, object-oriented programming, logical decision models, Formal specifications, Computer science, configuration management, Software systems, higher-level software artifacts, pairwise dependence relation, design structure matrices, Software engineering, computational complexity, solution enumeration]
Explicit Concern-Driven Development with ArchEvol
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Supporting developers in examining and evolving a software system in terms of concerns is considered a critical capability in the face of the scale and complexity of today's software. A number of existing approaches make an inroad to providing this support, but they fall short in key ways. This paper introduces ArchEvol, a new programming environment that embodies a new kind of approach, one we term explicit concern-driven development. The vision is threefold: (1) a fine-grained concern model maps concerns to code, (2) concerns are visualized at both the code level, to assist in the actual act of making changes, and the architectural level, to assist in gauging levels of scattering and tangling, and (3) automated support assists developers in maintaining the concern mapping over time. Developers, then, continuously examine, structure, and modify the software they produce in terms of concerns. We introduce our approach, discuss how we have realized it in ArchEvol, and present the results of a first set of evaluations that demonstrate its potential.
[Visualization, Scattering, concern-driven development, Face detection, Programming environments, fine-grained concern model, software evolution, programming environment, USA Councils, development environments, gauging levels, Software systems, ArchEvol, Weaving, software engineering, automated support, software concerns, Informatics, Software engineering]
Design Rule Hierarchies and Parallelism in Software Development Tasks
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
As software projects continue to grow in scale, being able to maximize the work that developers can carry out in parallel as a set of concurrent development tasks, without incurring excessive coordination overhead, becomes increasingly important. Prevailing design models, however, are not explicitly conceived to suggest how development tasks on the software modules they describe can be effectively parallelized. In this paper, we present a design rule hierarchy based on the assumption relations among design decisions. Software modules located within the same layer of the hierarchy suggest independent, hence parallelizable, tasks. Dependencies between layers or within a module suggest the need for coordination during concurrent work. We evaluate our approach by investigating the source code and mailing list of Apache Ant. We observe that technical communication between developers working on different modules within the same hierarchy layer, as predicted, is significantly less than communication between developers working across layers.
[Context, project management, concurrent development tasks, Project management, Programming, Predictive models, software modules, design rule hierarchies, parallel programming, Computer science, software architecture, Software design, USA Councils, collaboration, Parallel processing, software engineering, software development task parallelism, software projects, Large-scale systems, Apache Ant, Software engineering]
Automated Test Order Generation for Software Component Integration Testing
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
The order in which software components are tested can have a significant impact on the number of stubs required during component integration testing. This paper presents an efficient approach that applies heuristics based on a given software component test dependency graph to automatically generate a test order that requires a (near) minimal number of test stubs. Thus, the approach reduces testing effort and cost. The paper describes the proposed approach, analyses its complexity and illustrates its use. Comparison with three well known graph-based approaches, for a real-world software application, shows that only the classic Le Traon et al.'s approach and ours give an optimal number of stubs. However, experiments on randomly simulated dependency models with 100 to 10,000 components show that our approach has a significant performance advantage with a reduction in the average running time of 96.01%.
[Software testing, object-oriented programming, program testing, heuristic algorithms, software testing, simulated dependency model, software component test dependency graph, directed feedback vertex-set problem, automated test order generation, test stubs, component integration, Automatic testing, integrated software, software component integration testing, graph based approach, Software engineering]
A Divergence-Oriented Approach to Adaptive Random Testing of Java Programs
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Adaptive Random Testing (ART) is a testing technique which is based on an observation that a test input usually has the same potential as its neighbors in detection of a specific program defect. ART helps to improve the efficiency of random testing in that test inputs are selected evenly across the input spaces. However, the application of ART to object-oriented programs (e.g., C++ and Java) still faces a strong challenge in that the input spaces of object-oriented programs are usually high dimensional, and therefore an even distribution of test inputs in a space as such is difficult to achieve. In this paper, we propose a divergence-oriented approach to adaptive random testing of Java programs to address this challenge. The essential idea of this approach is to prepare for the tested program a pool of test inputs each of which is of significant difference from the others, and then to use the ART technique to select test inputs from the pool for the tested program. We also develop a tool called ARTGen to support this testing approach, and conduct experiment to test several popular open-source Java packages to assess the effectiveness of the approach. The experimental result shows that our approach can generate test cases with high quality.
[Software testing, Java, System testing, object-oriented programming, program testing, Object oriented modeling, Subspace constraints, ARTGen tool, test inputs distribution, Face detection, specific program defect detection, divergence-oriented approach, Automatic testing, Packaging, object-oriented programs, adaptive random testing, Large-scale systems, Java programs, Software engineering]
Adaptive Random Test Case Prioritization
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the "additional" techniques) and yet involves much less time cost.
[Software testing, Greedy algorithms, Costs, fault diagnosis, Australia Council, program testing, Subspace constraints, regression testing, regression analysis, fault detection, test case prioritization, Programming, Adaptive random testing, adaptive random test case prioritization, coverage based ART techniques, random selection, Computer science, Fault detection, Automatic testing, software engineering, Software engineering]
Model-Based Customization and Deployment of Eclipse-Based Tools: Industrial Experiences
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Developers of software engineering tools are facing high expectations regarding capabilities and usability. Users expect tools tailored to their specific needs and integrated in their working environment. This increases tools' complexity and complicates their customization and deployment despite available mechanisms for adaptability and extensibility. A main challenge lies in understanding and managing the dependencies between different technical mechanisms for realizing tool variability. We report on industrial experiences of applying a model-based and tool-supported product line approach for the customization and deployment of two Eclipse-based tools. We illustrate challenges of customizing these tools to different development contexts: In the first case study we developed variability models of a product line tool suite used by an industry partner and utilized these models for tool customization and deployment. In the second case study we applied the same approach to a maintenance and setup tool of our industry partner. Our experiences suggest to design software tools as product lines; to formally describe the tools' variability in models; and to provide end-user capabilities for customizing and deploying the tools.
[Productivity, tool-supported product line, end-user customization, Laboratories, DP industry, personal computing, product customisation, Programming, software engineering tool developers, eclipse-based tools, model-based deployment, Elicpse-based tools, Software design, industrial experience, Computer industry, model-based customization, Metals industry, software tools, Software tools, Usability, product line engineering, Software engineering, Context modeling, deployment]
Self-Repair through Reconfiguration: A Requirements Engineering Approach
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
High variability software systems can deliver their functionalities in multiple ways by reconfiguring their components. High variability has become important because of current trends towards software systems that come in product families, offer high levels of personalization, and fit well within a service-oriented architecture. The purpose of our research is to propose a framework that exploits such variability to allow a software system to self-repair in cases of failure. We propose an autonomic architecture that consists of monitoring, diagnosis, reconfiguration and execution components. This architecture uses requirements models as a basis for monitoring, diagnosis, and reconfiguration. We illustrate our proposal with a medium-sized publicly available case study (an automated teller machine (ATM) simulation), and evaluate its performance through a series of experiments. Our experimental results demonstrate that it is feasible to scale our approach to software systems with medium-size requirements.
[Adaptive systems, Computational modeling, Service oriented architecture, automated teller machine, Switches, Displays, requirements engineering approach, Requirement Monitoring and Diagnosis, Proposals, Autonomic computing, Condition monitoring, autonomic architecture, software architecture, Web services, Computer architecture, Software systems, Self-reconfiguration, self-repair through reconfiguration, service-oriented architecture, Asynchronous transfer mode, Software engineering]
Model Checking of Domain Artifacts in Product Line Engineering
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
In product line engineering individual products are derived from the domain artifacts of the product line. The reuse of the domain artifacts is constraint by the product line variability. Since domain artifacts are reused in several products, product line engineering benefits from the verification of domain artifacts. For verifying development artifacts, model checking is a well-established technique in single system development. However, existing model checking approaches do not incorporate the product line variability and are hence of limited use for verifying domain artifacts. In this paper we present an extended model checking approach which takes the product line variability into account when verifying domain artifacts. Our approach is thus able to verify that every permissible product (specified with I/O-automata) which can be derived from the product line fulfills the specified properties (specified with CTL). Moreover, we use two examples to validate the applicability of our approach and report on the preliminary validation results.
[Software testing, automata theory, domain artifacts, Variability, formal specification, Automotive engineering, Design engineering, Quality assurance, Model Checking, I/O-automata, Traffic control, product line variability, domain artifact verification, product line engineering, single system development, Rails, Domain Artifact Verification, model checking, Systems engineering and theory, Software systems, Product Line Engineering, CTL, development artifact verification, Software engineering, Formal verification]
Alattin: Mining Alternative Patterns for Detecting Neglected Conditions
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
To improve software quality, static or dynamic verification tools accept programming rules as input and detect their violations in software as defects. As these programming rules are often not well documented in practice, previous work developed various approaches that mine programming rules as frequent patterns from program source code. Then these approaches use static defect-detection techniques to detect pattern violations in source code under analysis. These existing approaches often produce many false positives due to various factors. To reduce false positives produced by these mining approaches, we develop a novel approach, called Alattin, that includes a new mining algorithm and a technique for detecting neglected conditions based on our mining algorithm. Our new mining algorithm mines alternative patterns in example form "P<sub>1</sub> or P<sub>2</sub>\
[application program interfaces, program verification, public domain software, data mining, software quality, neglected condition detection, API method, open source library, software libraries, programming rules, USA Councils, Dynamic programming, software tools, Pattern analysis, Alattin approach, alternative pattern mining, Automatic programming, code search, frequent itemset mining, Programming profession, static verification tool, dynamic verification tool, Computer science, Software libraries, Software quality, alternative patterns, Software tools, Software engineering]
Mining Temporal Specifications from Object Usage
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
A caller must satisfy the callee's precondition--that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Computation Tree Logic (CTL) formulas that describe the operations a parameter goes through: "In parseProperties (String xml), the parameter xml normally stems from getProperties()." Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 189 violations of operational preconditions, uncovering 9 unique defects and 36 unique code smells-with 44% true positives in the 50 top-ranked violations.
[object-oriented programming, program diagnostics, object usage, data mining, trees (mathematics), computation tree logic, Documentation, static analysis, parameter XML, formal specification, program code, Programming profession, mining temporal specifications, parseProperties, Computer science, formal logic, model checking, Prototypes, Data flow computing, getProperties(), Logic, AspectJ, Software engineering]
Inferring Resource Specifications from Natural Language API Documentation
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Typically, software libraries provide API documentation, through which developers can learn how to use libraries correctly. However, developers may still write code inconsistent with API documentation and thus introduce bugs, as existing research shows that many developers are reluctant to carefully read API documentation. To find those bugs, researchers have proposed various detection approaches based on known specifications. To mine specifications, many approaches have been proposed, and most of them rely on existing client code. Consequently, these mining approaches would fail to mine specifications when client code is not available. In this paper, we propose an approach, called Doc2Spec, that infers resource specifications from API documentation. For our approach, we implemented a tool and conducted an evaluation on Javadocs of five libraries. The results show that our approach infers various specifications with relatively high precisions, recalls, and F-scores. We further evaluated the usefulness of inferred specifications through detecting bugs in open source projects. The results show that specifications inferred by Doc2Spec are useful to detect real bugs in existing projects.
[Java, application program interfaces, program testing, natural language API documentation, natural language processing, client code, Natural languages, Laboratories, application programming interface, data mining, Documentation, Educational technology, F-scores, Javadocs, bug detection, formal specification, software libraries, Doc2Spec, Computer science, Software libraries, Databases, Computer bugs, inferring resource specifications, Software engineering]
Specification and Control of Interface Responses to User Input in Rich Internet Applications
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
The complexity of data-intensive business processes is typically reflected in the user interfaces of the information systems supporting them. To facilitate ease of use despite the visual and semantic complexity of dialog forms, users should be guided in ways such as highlighting and describing invalid input, showing/hiding or enabling/disabling particular UI widgets. Manual implementation of the rules governing these user interface responses typically requires considerable effort, as they involve business and presentation concerns, and are often dependent on each other. In addition, users expect the interface to respond immediately to any input, which can be especially challenging in web applications. In this paper, we formalize the aspects that must be considered in formulating input evaluation rules, and present a run-time framework that encapsulates the necessary logic, enabling domain experts to specify the business rules instead of requiring developers to implement them.
[data intensive business processes, dialog forms, visual complexity, semantic complexity, interface responses control, Control systems, business rules, user interfaces, UI widgets, Information systems, Web application, user input, User interfaces, Automatic control, Internet, information systems, Logic, Books, Usability, business data processing, Internet application, Business, Software engineering]
Code Completion from Abbreviated Input
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Abbreviation Completion is a novel technique to improve the efficiency of code-writing by supporting code completion of multiple keywords based on non-predefined abbreviated input - a different approach from conventional code completion that finds one keyword at a time based on an exact character match. Abbreviated input is expanded into keywords by a Hidden Markov Model learned from a corpus of existing code. The technique does not require the user to memorize abbreviations and provides incremental feedback of the most likely completions. This paper presents the algorithm for abbreviation completion, integrated with a new user interface for multiple-keyword completion. We tested the system by sampling 3000 code lines from open source projects and found that more than 98% of the code lines could be resolved from acronym-like abbreviations. A user study found 30% reduction in time usage and 41% reduction of keystrokes over conventional code completion.
[Abbreviation, System testing, Data Mining, user interfaces, Data mining, program compilers, abbreviation completion, hidden Markov models, code-writing, Code Assistants, multiple-keyword completion, Feedback, existing code corpus, Lead, Hidden Markov Model, keyword, open source projects, character match, nonpredefined abbreviated, acronym-like abbreviations, Code Completion, Programming profession, user interface, Multiple Keywords, code completion, Hidden Markov models, User interfaces, Sampling methods, Acceleration, Software engineering]
Task-First or Context-First? Tool Integration Revisited
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
If software engineering tools are not "properly integrated\
[Productivity, task management, tool integration, Communication system control, Scattering, Standardization, Context awareness, information retrieval, Information retrieval, context-based tool integration approach, project artifacts, ubiquitous computing, software engineering tools, task-based tool integration approach, Collaboration, Frequency, software engineering, context awareness, software tools, information need, Joining processes, emperical research, Software engineering]
Mining Hierarchical Scenario-Based Specifications
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Scalability over long traces, as well as comprehensibility and expressivity of results, are major challenges for dynamic analysis approaches to specification mining. In this work we present a novel use of object hierarchies over traces of inter-object method calls, as an abstraction/refinement mechanism that enables user-guided, top-down or bottom-up mining of layered scenario-based specifications, broken down by hierarchies embedded in the system under investigation. We do this using data mining methods that provide statistically significant sound and complete results modulo user-defined thresholds, in the context of Damm and Harel's live sequence charts (LSC); a visual, modal, scenario-based, inter-object language. Thus, scalability, comprehensibility, and expressivity are all addressed. Our technical contribution includes a formal definition of hierarchical inter-object traces, and algorithms for `zooming-out' and `zooming-in', used to move between abstraction levels on the mined specifications. An evaluation of our approach based on several case studies shows promising results.
[Scalability, data mining, Conference management, Mathematics, Information management, modulo user-defined thresholds, scenario-based language, Data mining, modal language, formal specification, user-guided mining, abstraction mechanism, visual language, Engineering management, live sequence charts, Management information systems, refinement mechanism, hierarchical scenario-based specification mining, inter-object language, bottom-up mining, dynamic analysis, specification mining, Computer science, top-down mining, layered scenario-based specifications, hierarchical inter-object traces, system monitoring, Concrete, inter-object method calls, object hierarchies, Software engineering]
Automatic Generation of Object Usage Specifications from Large Method Traces
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Formal specifications are used to identify programming errors, verify the correctness of programs, and as documentation. Unfortunately, producing them is error-prone and time-consuming, so they are rarely used in practice. Inferring specifications from a running application is a promising solution. However, to be practical, such an approach requires special techniques to treat large amounts of runtime data. We present a scalable dynamic analysis that infers specifications of correct method call sequences on multiple related objects. It preprocesses method traces to identify small sets of related objects and method calls which can be analyzed separately. We implemented our approach and applied the analysis to eleven real-world applications and more than 240 million runtime events. The experiments show the scalability of our approach. Moreover, the generated specifications describe correct and typical behavior, and match existing API usage documentation.
[API usage documentation, Law, application program interfaces, program verification, Laboratories, scalable dynamic analysis, Specification inference, formal specification, program correctness verification, Runtime, object-oriented methods, correct method call sequences, Java, program diagnostics, large method traces, Documentation, dynamic analysis, Formal specifications, formal specifications, Computer science, Collaboration, object-oriented applications, Computer errors, temporal properties, programming errors, Legal factors]
Efficient Formalism-Independent Monitoring of Parametric Properties
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Parametric properties provide an effective and natural means to describe object-oriented system behaviors, where the parameters are typed by classes and bound to object instances at runtime. Efficient monitoring of parametric properties, in spite of increasingly growing interest due to applications such as testing and security, imposes a highly non-trivial challenge on monitoring approaches due to the potentially huge number of parameter instances. Existing solutions usually compromise their expressiveness for performance or vice versa. In this paper, we propose a generic, in terms of specification formalism, yet efficient, solution to monitoring parametric specifications. Our approach is based on a general algorithm for slicing parametric traces and makes use of static knowledge about the desired property to optimize monitoring. The needed knowledge is not specific to the underlying formalism and can be easily computed when generating monitoring code from the property. Our approach works with any specification formalism, providing better and extensible expressiveness. Also, a thorough evaluation shows that our technique outperforms other state-of-art techniques optimized for particular logics or properties.
[Software testing, Java, parametric specification monitoring, program testing, Computerized monitoring, program diagnostics, Mechanical factors, parametric properties, Security, Runtime Verification, Logic testing, formal specification, Programming profession, object-oriented system behavior, Condition monitoring, Runtime, optimisation, formalism-independent monitoring, optimization, Software Engineering, object-oriented methods, parametric trace slicing, Monitoring, Software engineering]
Automatically Recommending Triage Decisions for Pragmatic Reuse Tasks
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Planning a complex software modification task imposes a high cognitive burden on developers, who must juggle navigating the software, understanding what they see with respect to their task, and deciding how their task should be performed given what they have discovered. Pragmatic reuse tasks, where source code is reused in a white-box fashion, is an example of a complex and error-prone modification task: the developer must plan out which portions of a system to reuse, extract the code, and integrate it into their own system. In this paper we present a recommendation system that automates some aspects of the planning process undertaken by developers during pragmatic reuse tasks. In a retroactive evaluation, we demonstrate that our technique was able to provide the correct recommendation 64% of the time and was incorrect 25% of the time. Our case study suggests that developer investigative behaviour is positively influenced by the use of the recommendation system.
[Navigation, cost, Shape measurement, Process planning, complex software modification tasks, Software performance, pragmatic software reuse tasks, Inspection, planning process, structural relevance, pragmatic reuse tasks, recommendation system, Computer science, triage decisions, recommendation systems, USA Councils, software reusability, Software systems, Cost function, retroactive evaluation, source code analysis, Software engineering]
Mining Health Models for Performance Monitoring of Services
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Online services such as search and live applications rely on large infrastructures in data centers, consisting of both stateless servers (e.g., web servers) and stateful servers (e.g., database servers). Acceptable performance of such infrastructures, and hence the availability of online services, rely on a very large number of parameters such as per-process resources and configurable system/application parameters. These parameters are available for collection as performance counters distributed across various machines, but services have had a hard time determining which performance counters to monitor and what thresholds to use for performance alarms in a production environment. In this paper, we present a novel framework called PerfAnalyzer, a storage-efficient and pro-active performance monitoring framework for correlating service health with performance counters. PerfAnalyzer automatically infers and builds health models for any service by running the standard suite of predeployment tests for the service and data mining the resulting performance counter data-set. A filtered set of performance counters and thresholds of alarms are produced by our framework. The health model inferred by our framework can then be used to detect performance degradation and collect detailed data for root-cause analysis in a production environment. We have applied PerfAnalyzer on five simple stress scenarios - CPU, memory, I/O, disk, and network, and two real system - Microsoft's SQL Server 2005 and IIS 7.0 Web Server, with promising results.
[performance monitoring, data mining, data centers, service health model, Data mining, storage-efficient performance monitoring framework, PerfAnalyzer, Counting circuits, Condition monitoring, Degradation, Databases, stateful servers, proactive performance monitoring framework, stateless servers, Production, Web server, health care, software performance evaluation, online services, Availability, configurable system, Data analysis, performance alarms, performance degradation, machine learning, computer centres, application parameters, performance counters, Automatic testing, production environment, system monitoring, health model mining, root-cause analysis]
Inferring Method Effect Summaries for Nested Heap Regions
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Effect systems are important for reasoning about the side effects of a program. Although effect systems have been around for decades, they have not been widely adopted in practice because of the large number of annotations that they require. A tool that infers effects automatically can make effect systems practical. We present an effect inference algorithm and an Eclipse plug-in, DPJizer, which alleviate the burden of writing effect annotations for a language called Deterministic Parallel Java (DPJ). The key novel feature of the algorithm is the ability to infer effects on nested heap regions. Besides DPJ, we also illustrate how the algorithm can be used for a different effect system based on object ownership. Our experience shows that DPJizer is both useful and effective: (i) inferring effect annotations automatically saves significant programming burden; and (ii) inferred effects are more precise than those written manually, and are fine-grained enough to enable the compiler to prove determinism of the program.
[Heart, Java, inferring method effect summaries, Eclipse plug-in, inference mechanisms, object ownership, Programming profession, Engines, parallel programming, effect inference algorithm, Program processors, nested heap region, DPJizer, Writing, Lab-on-a-chip, Inference algorithms, Safety, parallel languages, deterministic parallel Java, Software engineering]
ReAssert: Suggesting Repairs for Broken Unit Tests
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Developers often change software in ways that cause tests to fail. When this occurs, developers must determine whether failures are caused by errors in the code under test or in the test code itself. In the latter case, developers must repair failing tests or remove them from the test suite. Repairing tests is time consuming but beneficial, since removing tests reduces a test suite's ability to detect regressions. Fortunately, simple program transformations can repair many failing tests automatically. We present ReAssert, a novel technique and tool that suggests repairs to failing tests' code which cause the tests to pass. Examples include replacing literal values in tests, changing assertion methods, or replacing one assertion with several. If the developer chooses to apply the repairs, ReAssert modifies the code automatically. Our experiments show that ReAssert can repair many common test failures and that its suggested repairs correspond to developers' expectations.
[Software testing, Performance evaluation, assertion method, Software maintenance, program testing, Government, Programming, code under test, Software test maintenance, Application software, software maintenance, Logic testing, test suite, test repair, ReAssert, test code, broken unit tests, Automatic testing, USA Councils, program transformation, Protection, Software tools, Software engineering]
Cache-Based Model Checking of Networked Applications: From Linear to Branching Time
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Many applications are concurrent and communicate over a network. The non-determinism in the thread and communication schedules makes it desirable to model check such systems. However, a simple state space exploration scheme is not applicable, as backtracking results in repeated communication operations. A cache-based approach solves this problem by hiding redundant communication operations from the environment. In this work, we propose a change from a linear-time to a branching-time cache, allowing us to relax restrictions in previous work regarding communication traces that differ between schedules. We successfully applied the new algorithm to real-life programs where a previous solution is not applicable.
[Software testing, System testing, program verification, branching-time cache, software verification, Communication system control, real-life programs, networking, Electronic mail, State-space methods, Application software, Yarn, caching, input/output, cache-based model checking, networked applications, Processor scheduling, linear-time cache, state space exploration scheme, Software model checking, backtracking, Space exploration, Software engineering, redundant communication operations]
State-Space Coverage Estimation
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Software model checking is the process of systematically exploring a program's state space to find hard-to-discover errors. Because of the exponential size of the state space, an exhaustive search of the state space is often impossible given the memory resources. In such cases, an estimate of how much of the state space is covered can help the verifier to decide whether to employ additional computational resources or to use more aggressive abstraction techniques. Our work focuses on coverage estimation for explicit-state model checking of software programs. In this paper, we present an estimation algorithm that is based on Monte Carlo techniques that sample the unexplored portion of the reachability graph. We implemented our algorithm in Java Pathfinder and evaluated our approach on a suite of Java programs, simulating out-of-memory errors after a known percentage of a program's state space had been searched. Our empirical studies show that, on average, our algorithm's coverage estimates differ from the actual coverage by less than 10 percentage points, with a standard deviation of about 5 percentage points - regardless of whether the actual state-space coverage is low (3%) or high (95%).
[program verification, estimation algorithm, formal specification, reachability graph, Monte Carlo methods, Space technology, Prototypes, out-of-memory errors, Model checking, memory resources, State estimation, Java programs, explicit-state model checking, coverage estimation, Java, reachability analysis, state-space coverage estimation, data flow analysis, Explosions, State-space methods, software model checking, aggressive abstraction techniques, software programs, Monte Carlo techniques, Computer science, hard-to-discover errors, computational resources, Java Pathfinder, automatic verification, Computer errors, Software engineering]
A Framework for State-Space Exploration of Java-Based Actor Programs
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
The actor programming model offers a promising model for developing reliable parallel and distributed code. Actors provide flexibility and scalability: local execution may be interleaved, and distributed nodes may operate asynchronously. The resulting nondeterminism is captured by nondeterministic processing of messages. To automate testing, researchers have developed several tools tailored to specific actor systems. As actor languages and libraries continue to evolve, such tools have to be reimplemented. Because many actor systems are compiled to Java bytecode, we have developed Basset, a general framework for testing actor systems compiled to Java bytecode. We illustrate Basset by instantiating it for the Scala programming language and for the ActorFoundry library for Java. Our implementation builds on Java PathFinder, a widely used model checker for Java. Experiments show that Basset can effectively explore executions of actor programs; e.g., it discovered a previously unknown bug in a Scala application.
[Java, System testing, Automatic programming, Basset, Java bytecode, Java PathFinder, programming languages, ActorFoundry library, Scala programming language, Computer science, Java-based actor program, Automatic testing, USA Councils, Computer bugs, Interleaved codes, Libraries, state-space exploration, Software engineering]
Symbolic Deadlock Analysis in Concurrent Libraries and Their Clients
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Methods in object-oriented concurrent libraries hide internal synchronization details. However, information hiding may result in clients causing thread safety violations by invoking methods in an unsafe manner. Given such a library, we present a technique for inferring interface contracts that specify permissible concurrent method calls and patterns of aliasing among method arguments, such that the derived contracts guarantee deadlock free execution for the methods in the library. The contracts also help client developers by documenting required assumptions about the library methods. Alternatively, the contracts can be statically enforced in the client code to detect potential deadlocks in the client. Our technique combines static analysis with a symbolic encoding for tracking lock dependencies, allowing us to synthesize contracts using a SMT solver. Our prototype tool analyzes over a million lines of code for some widely-used Java libraries within an hour, thus demonstrating its scalability and efficiency. Furthermore, the contracts inferred by our approach have been able to pinpoint real deadlocks in clients, i.e. deadlocks that have been a part of bug-reports filed by users and developers of the client code.
[Surface-mount technology, Scalability, Deadlock Detection, internal synchronization details, Concurrent Libraries, SMT solvers, Yarn, system recovery, software libraries, Java libraries, Prototypes, symbolic encoding, Libraries, Safety, Static Analysis, Contracts, object-oriented concurrent libraries, lock dependencies, Java, object-oriented programming, interface contracts, deadlock free execution, program diagnostics, symbolic deadlock analysis, information hiding, static analysis, permissible concurrent method calls, Encoding, synchronisation, Program Analysis, concurrency control, System recovery, aliasing patterns]
Precise Data Race Detection in a Relaxed Memory Model Using Heuristic-Based Model Checking
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Most approaches to reasoning about multithreaded programs, including model checking, make the implicit assumption that the system being considered is sequentially consistent. This is, however, invalid in most modern computer architectures and results in unsound reasoning for programs that contain data races, where data races are defined by the memory model of the programming environment. We describe an extension to the model checker Java PathFinder that incorporates knowledge of the Java Memory Model to precisely detect data races in Java byte code. Our tool incorporates special purpose heuristic algorithms that result in shorter counterexample paths. Once data races have been eliminated from a program, Java PathFinder can be soundly employed to verify additional properties.
[Heuristic algorithms, relaxed memory model, Java PathFinder, Java memory model, Yarn, Concurrent computing, heuristic algorithm, Information science, storage management, programming environment, computer architecture, Computer architecture, data race, multithreaded programs, Java byte code, Java, multi-threading, reasoning for programs, heuristic based model checking, Programming profession, Programming environments, Computer aided instruction, model checking, precise data race detection, reasoning about programs, Software engineering]
A Formal Syntax for Probabilistic Timed Property Sequence Charts
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Probabilistic properties are considered as the most important requirements for a variety of software systems, since they are used to formulate extra-functional requirements such as reliability, availability, safety, security and performance requirements. Currently, several probabilistic logics have been proposed to specify such important properties. However, due to the inherent complexity of the underlying temporal logics, these probabilistic logics are rather complex and software developers have problems using them to correctly specify the intended properties. To overcome this problem, we define a formal and graphical property specification language called probabilistic timed property sequence charts (PTPSC) which is a probabilistic extension of property sequence charts (PSC). We illustrate the use of PTPSC in the context of a vehicle-to-vehicle communication device for avoiding traffic accidents.
[Java, Heuristic algorithms, Probabilistic and Timed Property Sequence Chart, probabilistic logic, software developers, traffic accidents, temporal logics, Yarn, Programming profession, Property Sequence Chart, Programming environments, Concurrent computing, probabilistic timed property sequence charts, Computer aided instruction, Information science, formal syntax, Computer architecture, specification languages, probabilistic logics, Probabilistic Properties, vehicle-to-vehicle communication device, Software engineering, extrafunctional requirements]
Let the Ants Deploy Your Software - An ACO Based Deployment Optimisation Strategy
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Decisions regarding the mapping of software components to hardware nodes affect the quality of the resulting system. Making these decisions is hard when considering the ever-growing complexity of the search space, as well as conflicting objectives and constraints. An automation of the solution space exploration would help not only to make better decisions but also to reduce the time of this process. In this paper, we propose to employ Ant Colony Optmisation (ACO) as a multi-objective optimisation strategy. The constructive approach is compared to an iterative optimisation procedure - a Genetic Algorithm (GA) adaptation - and was observed to perform suprisingly similar, although not quite on a par with the GA, when validated based on a series of experiments.
[Ant colony optimization, Software algorithms, solution space exploration, Reliability engineering, Ant Colony Optimisation, genetic algorithms, multi-objective optimisation, Genetic algorithms, Design engineering, deployment optimisation strategy, Software quality, Component Deployment, Hardware, Space exploration, Safety, Iterative methods, genetic algorithm adaptation, ant colony optmisation]
Using String Distances for Test Case Prioritisation
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Test case prioritisation aims at finding an ordering which enhances a certain property of an ordered test suite. Traditional techniques rely on the availability of code or a specification of the program under test. In this paper, we propose to use string distances on the text of test cases for their comparison and elaborate a prioritisation algorithm. Such a prioritisation does not require code and can be useful for initial testing and in cases when code is difficult to instrument. We also briefly report on preliminary results of an experiment where the proposed prioritisation technique was compared with random permutations and four classical string distance metrics were evaluated.
[Software testing, test case prioritisation algorithm, Costs, Hamming distance, program testing, Instruments, Testing and Debugging, Test case prioritisation, Application software, Software debugging, String distance, string distance metrics, Automatic testing, ordered test suite, Software Engineering, Testing tools, random permutations, Computer industry, software tools, Software tools, Software engineering]
Reggae: Automated Test Generation for Programs Using Complex Regular Expressions
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Test coverage such as branch coverage is commonly measured to assess the sufficiency of test inputs. To reduce tedious manual efforts in generating high-covering test inputs, various automated techniques have been proposed. Some recent effective techniques include Dynamic Symbolic Execution (DSE) based on path exploration. However, these existing DSE techniques cannot generate high-covering test inputs for programs using complex regular expressions due to large exploration space; these complex regular expressions are commonly used for input validation and information extraction. To address this issue, we propose an approach, named Reggae, to reduce the exploration space of DSE in test generation. In our evaluation, we apply Reggae on various input-validation programs that use complex regular expressions. Empirical results show that Reggae helps a test-generation tool generate test inputs to achieve 79% branch coverage of validators, improved from 29% achieved without the help of Reggae.
[Software testing, automatic test software, program testing, program verification, input-validation programs, Manuals, path exploration, Logic testing, automated test generation, Engines, string generation, test generation, information extraction, Automatic testing, USA Councils, Impedance matching, complex regular expressions, Reggae, dynamic symbolic execution, Space exploration, Software measurement, branch coverage, exploration space, Software engineering]
A Methodology and Framework to Simplify Usability Analysis of Mobile Applications
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Usability analysis is an important step in software development in order to improve certain aspects of the system. However, it is often a challenge especially when it comes to evaluating applications running on mobile devices because of the restrictions posed by the device and the lack of supporting tools and software available to collect the necessary usability data. This paper proposes a methodology and framework to aid developers in preparing the mobile system for usability analysis. The focus is on the simplification of the developer's task in preparing the system for evaluation and the processing of the collected usability data by automating some of the tasks involved in the process.
[program testing, software development, software development management, mobile application, Programming, Application software, Data mining, Cost accounting, Guidelines, mobile computing, usability evaluation, framework, mobile applications, mobile devices, Cameras, Usability, Informatics, Software tools, software performance evaluation, methodology, Software engineering, usability analysis]
An Open Source-Based Approach to Software Development Infrastructures
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
As software systems become larger and more complex, automated software engineering tools play a crucial role for effective software development management, which is a key factor to lead quality software systems. In this work, we present TRICA, an open source-based software development infrastructure. The name of TRICA represents its features such as Traceability, Relationship, Informativeness, Cost-effectiveness, and Automation. Essentially, in TRICA, a continuous integration tool is coupled with a software configuration management tool and an issue tracking tool. We provisioned a mechanism to connect the open source tools in TRICA so that project members use the collaborated information to solve various issues and implementation problems efficiently, and easily share forthcoming issues during the course of the project. We show that TRICA can help to decentralize risks throughout the software development cycle and achieve successful software development.
[continuous integration, public domain software, software systems, Programming, tracking tool, software quality, Open source software, open source, groupware, software tools, software development cycle, Testing, Automation, Collaborative tools, Collaborative software, program diagnostics, collaborated information, software development management, software configuration management tool, software engineering tools, Software development management, TRICA, configuration management, automated software engineering tools, continuous integration tool, integrated software, issue tracking, Software systems, SCM, Software tools, Software engineering, open source tools]
EA-Analyzer: Automating Conflict Detection in Aspect-Oriented Requirements
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
One of the aims of aspect-oriented requirements engineering is to address the composability and subsequent analysis of crosscutting and non-crosscutting concerns during requirements engineering. Composing concerns may help to reveal conflicting dependencies that need to be identified and resolved. However, detecting conflicts in a large set of textual aspect-oriented requirements is an error-prone and time-consuming task. This paper presents EA-analyzer, the first automated tool for identifying conflicts in aspect-oriented requirements specified in natural-language text. The tool is based on a novel application of a Bayesian learning method that has been effective at classifying text. We present an empirical evaluation of the tool with three industrial-strength requirements documents from different real-life domains. We show that the tool achieves up to 92.97% accuracy when one of the case study documents is used as a training set and the other two as a validation set.
[text analysis, Aspect-Oriented Requirements Engineering, aspect-oriented requirements engineering, Data security, Programming, Bayesian learning method, natural-language text, Formal specifications, Learning systems, Industrial training, Aspect-Oriented Software Development, Conflicting Dependencies, Bayesian methods, conflict detection, Requirements Analysis, Requirements Composition, Customer relationship management, Computer architecture, software engineering, Cryptography, industrial-strength requirements, Software engineering]
An Automated Passive Testing Approach for the IMS PoC Service
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Although the adoption of the IP Multimedia Subsystem (IMS) keeps growing, IMS applications are often integrated to the system without being formally tested. In this work, we are interested in the IMS Push over Cellular (PoC) service, an OMA standard. We propose a conformance passive testing approach to check that its implementation respects the main standard requirements. This approach is based on a set of formal invariants representing the most relevant expected properties to be tested. Two testing phases are applied: the verification of the invariants against the service specification and their testing on the PoC collected execution traces.
[Software testing, System testing, Ground penetrating radar, program testing, conformance passive testing, Conference management, multimedia systems, IMS push over cellular, Formal model, IP multimedia subsystem, Engineering management, OMA standard, IP networks, automated passive testing, Testing, Multimedia systems, Telecommunications, service specification, IMS, IMS PoC service, Automatic testing, Automata, PoC service, invariants verification, Software engineering]
Adding Examples into Java Documents
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Code examples play an important role to explain the usage of Application Programming Interfaces (APIs), but most API documents do not provide sufficient code examples. For example, for the JDK 5 documents (JavaDocs), only 2% of APIs have code examples. In this paper, we propose a technique that automatically augments API documents with code examples. Our approach finds and embeds code examples for more than 75% of the APIs in JavaDocs 5.
[Productivity, document handling, Java, Automatic programming, application program interfaces, Humans, application programming interfaces, Data mining, Application software, JDK 5 documents, Java documents, Code Clustering, API Documents, Ranking, JavaDocs 5, Search engines, API documents, Feature extraction, Libraries, Structures, Examples, Software engineering]
Enhanced Automation for Managing Model and Metamodel Inconsistency
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Model-driven engineering (MDE) introduces additional challenges for managing evolution. For example, a metamodel change may affect instance models. Existing tool supported approaches for updating models in response to a metamodel change assume extra effort from metamodel developers. When no existing approach is applicable, metamodel users must update their models manually, an error prone and tedious task. In this paper, we describe the technical challenges faced when using the eclipse modeling framework (EMF) and existing approaches for updating models in response to a metamodel change. We then motivate and describe alternative techniques, including: a mechanism for loading, storing and manipulating inconsistent models; a mapping of inconsistent models to a human-usable notation for semi-automated and collaborative co-evolution; and integration with an inter-model reference manager, achieving automatic consistency checking as part of metamodel distribution.
[Java, Automation, automatic consistency checking, Collaborative software, collaborative coevolution, Conference management, eclipse modeling framework, Software development management, Computer science, Engineering management, metacomputing, model-driven engineering, metamodel distribution, Model driven engineering, Error correction, software tools, Software engineering, metamodel inconsistency, human-usable notation]
Generating Fixes from Object Behavior Anomalies
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Advances in recent years have made it possible in some cases to locate a bug (the source of a failure) automatically. But debugging is also about correcting bugs. Can tools do this automatically? The results reported in this paper, from the new PACHIKA tool, suggest that such a goal may be reachable. PACHIKA leverages differences in program behavior to generate program fixes directly. It automatically summarizes executions to object behavior models, determines differences between passing and failing runs, generates possible fixes, and assesses them via the regression test suite. Evaluated on the ASPECTJ bug history, PACHIKA generates a valid fix for 3 out of 18 crashing bugs; each fix pinpoints the bug location and passes the ASPECTJ test suite.
[Java, program debugging, program testing, fixing, object behavior anomalies, bugs correction, Debugging, Vehicle crash testing, program execution, Computer crashes, History, Programming profession, ASPECTJ test suite, Computer science, object usage patterns, Automatic testing, Computer bugs, PACHIKA tool, ASPECTJ bug, program fixes generation, regression test suite, debugging, software tools, Software engineering]
Service Substitution Revisited
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
In this paper, we propose a framework that reduces the complexity of service substitution. The framework is based on two substitution relations and corresponding theorems. The proposed relations and theorems allow organizing available services into groups. Then, the complexity of retrieving candidate substitute services for the target service and generating corresponding adapters scales up with the number of available groups, instead of scaling up with the number of available services.
[Availability, candidate substitute service retrieval, Humans, information retrieval, Information management, evolution, Organizing, Computer science, Web services, service substitution, XML, Concrete, service complexity, maintenance, Software engineering, computational complexity]
A Verification-Driven Approach to Traceability and Documentation for Auto-Generated Mathematical Software
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Automated code generators are increasingly used in safety-critical applications, but since they are typically not qualified, the generated code must still be fully tested, reviewed, and certified. For mathematical and engineering software this requires reviewers to trace subtle details of textbook formulas and algorithms to the code, and to match requirements (e.g., physical units or coordinate frames) not represented explicitly in models or code. We support these tasks by using the AutoCert verification system to identify and verify mathematical concepts in the code, recovering verified traceability links between concepts, code, and verification conditions. We then exploit these links to construct a natural language report that provides a high-level structured argument explaining where the code uses specified assumptions and why and how it complies with the requirements. We have applied our approach to generate review documents for several sub-systems of NASA's Project Constellation.
[program verification, system documentation, verified traceability links, safety-critical software, Software safety, program compilers, auto-generated mathematical software, Automatic control, model-based design, AutoCert verification system, Mathematical model, verification and validation, software documentation, automatic programming, Natural languages, NASA, Documentation, automated code generation, code reviews, Application software, high-level structured argument, engineering software, Automatic testing, Signal generators, Software engineering, automated code generators, natural language report]
Towards Augmenting Requirements Models with Preferences
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
The analysis of stakeholder requirements is a critical aspect of software engineering. A common way of specifying stakeholder requirements is in terms of a hierarchy of goals whose AND/OR decomposition captures a family of software solutions that comply with the goals. In this paper, we extend this goal modeling framework to include the specification of optional user requirements and user preferences, aggregated together into weighted formulae to be optimized. We team this with an automated reasoning tool, adapted from state of the art research in artificial intelligence planning with preferences, in order to synthesize solutions that both comply with the goals and optimize stakeholder preferences and optional requirements.
[preferences, Medical services, Logic design, Specification languages, variability, formal specification, artificial intelligence planning, Design optimization, Computer science, Headphones, planning (artificial intelligence), requirements engineering, Broadcasting, software engineering, Space exploration, stakeholder requirements specification, automated reasoning tool, user preferences specification, Artificial intelligence, Software engineering]
Automated Comprehension Tasks in Software Exploration
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Finding issues in software usually requires a series of comprehension tasks. After every task, an engineer explores the results and decides whether further tasks are required. Software comprehension therefore is a combination of tasks and a supported exploration of the results typically in an adequate visualization. In this paper, we describe how we simplify the combination of existing automated procedures to sequentially solve common software comprehension tasks. Beyond that we improve the understanding of the outcomes with interactive and explorative visualization concepts in a time efficient workflow. We validate the presented concept with basic comprehension tasks in an extended CocoViz tool implementation.
[Visualization, automated comprehension tasks, software exploration, Documentation, Maintenance engineering, reverse engineering, Exploration, Comprehension, Filters, software comprehension tasks, Computer architecture, Automatic control, Software systems, software engineering, explorative visualization, interactive visualization, program visualisation, Informatics, Software engineering, Testing, CocoViz tool]
Pointcut Rejuvenation: Recovering Pointcut Expressions in Evolving Aspect-Oriented Software
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. We present an automated approach that limits fragility problems by providing mechanical assistance in pointcut maintenance. The approach is based on harnessing arbitrarily deep structural commonalities between program elements corresponding to join points selected by a pointcut. The extracted patterns are then applied to later versions to offer suggestions of new join points that may require inclusion. We demonstrate the usefulness of our technique by rejuvenating pointcuts in multiple versions of several open-source AspectJ programs. The results show that our parameterized heuristic algorithm was able to automatically infer new join points in subsequent versions with an average recall of 0.93. Moreover, these join points appeared, on average, in the top 4th percentile of the suggestions, indicating that the results were precise.
[Software maintenance, Java, Automatic programming, Heuristic algorithms, aspect-oriented software, Scattering, software maintenance, Open source software, pointcut expression, pointcut fragility, heuristic algorithm, pointcut maintenance, Packaging, aspect-oriented programming, Robustness, pointcut rejuvenation, open-source AspectJ program, Software tools, Software development environments, Software engineering]
Applications of Simulation and AI Search: Assessing the Relative Merits of Agile vs Traditional Software Development
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
This paper augments Boehm-Turner's model of agile and plan-based software development augmented with an AI search algorithm. The AI search finds the key factors that predict for the success of agile or traditional plan-based software developments. According to our simulations and AI search algorithm: (1) in no case did agile methods perform worse than plan-based approaches; (2) in some cases, agile performed best. Hence, we recommend that the default development practice for organizations be an agile method. The simplicity of this style of analysis begs the question: why is so much time wasted on evidence-less debates on software process when a simple combination of simulation plus automatic search can mature the dialog much faster?
[Boehm-Turner model, Costs, software development, AI search algorithm, plan-based software developments, Object oriented modeling, Computational modeling, Computer simulation, software prototyping, Programming, digital simulation, Application software, agile methods, planning (artificial intelligence), Search engines, Artificial intelligence, Software engineering, Context modeling, software process]
Automating the Implementation of Analysis Concerns in Workflow Applications
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
In workflow management systems, analysis concerns related to monitoring, measurement, and control aim at identifying potential improvements of workflow applications. However, the specification of analysis concerns is done using a specific workflow language and engine, producing entangled code which is detrimental to their maintainability. The purpose of this paper is twofold. First, it presents briefly a domain-specific language to specify analysis concerns, independently of any workflow technology and in a modularized way. Second, it shows a strategy to assist developers to enhance a given workflow technology to support the automated implementation of analysis concerns into its workflow applications. Thus, given a workflow application and its analysis concerns, they are automatically integrated producing an enhanced executable workflow application.
[Automation, Data analysis, Computerized monitoring, languages, analysis concerns, workflow language, Control systems, Application software, programming languages, Engines, measurement, workflow management systems, domain-specific language, code generation, Automatic control, workflow management, Software measurement, workflow management software, Workflow management software, Software engineering, monitors]
Static Typing for Ruby on Rails
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Ruby on Rails (or just "Rails") is a popular web application framework built on top of Ruby, an object-oriented scripting language. While Ruby's powerful features such as dynamic typing help make Rails development extremely lightweight, this comes at a cost. Dynamic typing in particular means that type errors in Rails applications remain latent until run time, making debugging and maintenance harder. In this paper, we describe DRails, a novel tool that brings static typing to Rails applications to detect a range of run time errors. DRails works by translating Rails programs into pure Ruby code in which Rails's numerous implicit conventions are made explicit. We then discover type errors by applying DRuby, a previously developed static type inference system, to the translated program. We ran DRails on a suite of applications and found that it was able to detect several previously unknown errors.
[static type inference system, program debugging, Costs, authoring languages, Web application framework, web frameworks, USA Councils, Rails program translation, Ruby, scripting languages, Object oriented modeling, program diagnostics, Vehicle crash testing, object-oriented scripting language, Educational institutions, Application software, software maintenance, type systems, static typing, Rails, Computer science, program interpreters, Ruby on Rails, Computer bugs, object-oriented languages, Internet, Software engineering]
Towards Automating Class-Splitting Using Betweenness Clustering
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Large, unwieldy classes are a significant maintenance problem. Programmers dislike them because the fundamental logic is often obscured, making them hard to understand and modify. This paper proposes a solution - a semi-automatic technique for splitting large classes into smaller, more cohesive ones. The core of the technique is the use of betweenness clustering to identify the best way of partitioning a class. This turned a tedious manual process into a quick and simple semi-automated one in roughly one third of the cases we examined.
[Algorithm design and analysis, maintenance problem, semiautomatic technique, graph theory, object-oriented, Clustering algorithms, object-oriented methods, Logic, Pattern analysis, Refactoring, Java, Social network services, betweenness clustering, Graph theory, object-oriented systems, software maintenance, Programming profession, Computer science, pattern clustering, class-splitting automation, clustering, betweenness, cohesion, extract class, maintainability, Software engineering]
Reducing Features to Improve Bug Prediction
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.
[program debugging, feature selection technique, Scalability, Software performance, Support Vector Machine, History, Bug prediction, Machine Learning, scalability, Feature Selection, Design engineering, Prediction algorithms, machine learning classifiers, source code file, insufficient accuracy, learning (artificial intelligence), source coding, support vector machines, Support vector machines, software history data, SVM classifiers, Naive Bayes, Computer bugs, Support vector machine classification, Machine learning, Bayes methods, classification-based bug prediction, Reliability, Software engineering]
Generating Vulnerability Signatures for String Manipulating Programs Using Automata-Based Forward and Backward Symbolic Analyses
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Given a program and an attack pattern (specified as a regular expression), we automatically generate string-based vulnerability signatures, i.e., a characterization that includes all malicious inputs that can be used to generate attacks. We use an automata-based string analysis framework. Using forward reachability analysis we compute an over-approximation of all possible values that string variables can take at each program point. Intersecting these with the attack pattern yields the potential attack strings if the program is vulnerable. Using backward analysis we compute an over-approximation of all possible inputs that can generate those attack strings. In addition to identifying existing vulnerabilities and their causes, these vulnerability signatures can be used to filter out malicious inputs. Our approach extends the prior work on automata-based string analysis by providing a backward symbolic analysis that includes a symbolic pre-image computation for deterministic finite automata on common string manipulating functions such as concatenation and replacement.
[web security, finite automata, automata-based string analysis, World Wide Web, Filters, deterministic automata, vulnerability signature, malicious inputs, backward symbolic analysis, automata-based forward and backward symbolic analyses, Pattern analysis, over-approximation, string analysis, reachability analysis, string manipulating functions, data analysis, Doped fiber amplifiers, Reachability analysis, Computer science, string-based vulnerability signatures, Image analysis, Character generation, Automata, attack pattern, vulnerability signature generation, forward reachability analysis, digital signatures, string manipulating programs, deterministic finite automata, symbolic pre-image computation, Software engineering]
Weaving Context Sensitivity into Test Suite Construction
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Context-aware applications capture environmental changes as contexts and self-adapt their behaviors dynamically. Existing testing research has not explored context evolutions or their patterns inherent to individual test cases when constructing test suites. We propose the notation of context diversity as a metric to measure how many changes in contextual values of individual test cases. In this paper, we discuss how this notion can be incorporated in a test case generation process by pairing it with coverage-based test data selection criteria.
[Software testing, context diversity, context sensitivity weaving, program testing, software testing, coverage-based test data selection criteria, Application software, ubiquitous computing, Middleware, Programming profession, test suite construction, Computer science, test case generation process, Automatic testing, Physics computing, Working environment noise, context-aware programe, context-aware applications, Weaving, Software engineering]
Optimizing a Structural Constraint Solver for Efficient Software Checking
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Several static analysis techniques, e.g., symbolic execution or scope-bounded checking, as well as dynamic analysis techniques, e.g., specification-based testing, use constraint solvers as an enabling technology. To analyze code that manipulates structurally complex data, the underlying solver must support structural constraints. Solving such constraints can be expensive due to the large number of aliasing possibilities that the solver must consider. This paper presents a novel technique to selectively reduce the number of test cases to be generated. Our technique applies across a class of structural constraint solvers. Experimental results show that the technique enables an order of magnitude reduction in the number of test cases to be considered.
[Software testing, Marine technology, Performance evaluation, program testing, program diagnostics, dynamic analysis technique, Bounded Exhaustive Testing, Software Testing, Data structures, structural constraint solver, static analysis technique, test cases number reduction, Constraint optimization, Automatic testing, Computer bugs, XML, magnitude reduction, software checking, Constraint theory, Software engineering]
A Case for Automated Debugging Using Data Structure Repair
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Automated debugging is becoming increasingly important as the size and complexity of software increases. This paper makes a case for using constraint-based data structure repair, a recently developed technique for fault recovery, as a basis for automated debugging. Data structure repair uses given structural integrity constraints for key data structures to monitor their correctness during the execution of a program. If a constraint violation is detected, repair performs mutations on the data structures, i.e., corrupt program state, and transforms it into another state, which satisfies the desired constraints. The primary goal of data structure repair is to transform an erroneous state into an acceptable state. Therefore, the mutations performed by repair actions provide a basis of debugging faults in code (assuming the errors are due to bugs). A key challenge to embodying this insight into a mechanical technique arises due to the difference in the concrete level of the program states and the abstract level of the program code: repair actions apply to concrete data structures that exist at runtime, whereas debugging applies to code. We observe that static structures (program variables) hold handles to dynamic structures (heap-allocated data), which allows bridging the gap between the abstract and concrete levels. We envision a tool-chain where a data structure repair tool generates repair logs that are used by a fault localization tool and a repair abstraction tool that apply in synergy to not only identify the location of fault(s) in code but also to synthesize debugging suggestions. An embodiment of our vision can significantly reduce the cost of developing reliable software.
[program debugging, heap allocated data, Genetic mutations, static structures, constraint based data structure repair, Runtime, data structures, constraint handling, Monitoring, Testing, automated debugging, Debugging, Data structures, program variables, Software debugging, Programming profession, fault recovery, program code abstract level, Korat, Computer bugs, Juzi, Concrete, Data structure repair, corrupt program state, Software engineering]
Generation of Simulation Views for Domain Specific Modeling Languages Based on the Eclipse Modeling Framework
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
The generation of tools for domain specific modeling languages (DSMLs) is a key issue in model-driven development. Various tools already support the generation of domain-specific visual editors from models, but tool generation for visual behavior modeling languages is not yet supported in a satisfactory way. In this paper we propose a generic approach to specify DSML environments visually by models and transformation rules based on the Eclipse Modeling Framework (EMF). Editing rules define the behavior of generated visual editors, whereas simulation rules describe a model's operational semantics. From a DSML definition (model and transformation rules), an Eclipse plug-in is generated, implementing a visual DSML environment including an editor and (possibly multiple) simulators for different simulation views on the model. We present the basic components of Tiger2, our EMF-based generation environment, along the environment generation process for a small DSML modeling the behavior of ants in an ant hill.
[domain specific modeling languages, domain specific visual editors, Metamodeling, simulation, Programming, Electronic mail, simulation views generation, eclipse modeling framework, EMF based generation environment, model driven development, graph transformation, EMF transformation, Automatic control, editing rules, visual behavior modeling languages, simulation languages, software engineering, visual environment generation, DSL, Tiger2, Software engineering, EMF]
Using Spectrum-Based Fault Localization for Test Case Grouping
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Model-based test case generation allows one to derive almost arbitrary numbers of test cases from models. If resulting test suites are executed against real implementations, there are often huge numbers of failed test cases. Thus, the analysis of the test execution, i.e. the identification of failures for error reporting, becomes a tedious and time consuming task. In this paper we investigate a technique for grouping test runs that most likely reveal the same failure. This reduces the post analysis time and enables the generation of small regression test suites. The test case grouping is implemented by means of spectrum-based fault localization at the level of the specification. We calculate the grouping by relating the spectra of the test cases. Besides a brief discussion of our approach we present results of applying our approach to the Session Initiation Protocol.
[Software testing, System testing, Protocols, program testing, spectrum based fault localization, spectrum-based fault localization, Specification languages, Application software, system recovery, model based test case generation, post analysis time reduction, session initiation protocol, Automatic testing, small regression test suites, Computer bugs, Failure analysis, failures identification, model-based testing, Software systems, error reporting, test case grouping, test runs grouping, Software engineering, software metrics]
Cluster-Based I/O-Efficient LTL Model Checking
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
I/O-efficient algorithms take the advantage of large capacities of external memories to verify huge state spaces even on a single machine with low-capacity RAM. On the other hand, parallel algorithms are used to accelerate the computation and their usage may significantly increase the amount of available RAM memory if clusters of computers are involved. Since both the large amount of memory and high speed computation are desired in verification of large-scale industrial systems, extending I/O-efficient model checking to work over a network of computers can bring substantial benefits. In this paper we propose an explicit state cluster-based I/O efficient LTL model checking algorithm that is capable to verify systems with approximately 1010 states within hours.
[large scale industrial systems, LTL model checking, LTL, Random access memory, RAM memory, input-output programs, Parallel algorithms, Concurrent computing, formal verification, parallel model checking, Clustering algorithms, Computer networks, low capacity RAM, Large-scale systems, parallel algorithms, random-access storage, external memories, Read-write memory, I/O-efficient model checking, I/O-efficient algorithms, State-space methods, state spaces, high speed computation, Computer industry, Acceleration]
A Linear Programming Approach for Automated Localization of Multiple Faults
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
In this paper, we address the problem of localizing faults by analyzing execution traces of successful and unsuccessful invocations of the application when run against a suite of tests. We present a new algorithm, based on a linear programming model, which is designed to be particularly effective for the case where multiple faults are present in the application under investigation. Through an extensive empirical study, we show that in the case of both single and multiple faults, our approach outperforms a host of prominent fault localization methods from the literature.
[Software testing, Algorithm design and analysis, program testing, automated localization, spectrum-based fault localization, Design methodology, fault localization, Linear programming, linear programming approach, Time measurement, linear programming, Automatic testing, Aggregates, Binary codes, multiple faults, fault localization method, software engineering, Software measurement, Software engineering]
Lost in Translation: Forgetful Semantic Anchoring
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Assigning behavioral semantics to domain-specific languages (DSLs) opens the door for the application of formal methods, yet is largely an unresolved problem. Previously proposed solutions include semantic anchoring, in which a transformation from the DSL to an external framework that can supply both behavioral semantics and apply formal methods is constructed. The drawback of this approach is that it loses the structural constraints of the original DSL along with the details of the transformation, which can lead to erroneous results when formal methods are applied. We demonstrate this problem of forgetful semantic anchoring using existing approaches through a translation from dataflow systems to interface automata. We then describe our modeling tool FORMULA and apply it to the same example, showing how forgetful semantic anchoring can be avoided.
[Composition, automata theory, behavioral semantic, FORMULA modeling tool, Metamodeling, structural constraints, Behavioral Semantics, Application software, formal specification, Domain specific languages, Microwave integrated circuits, Automata, specification languages, forgetful semantic anchoring, formal methods, Software systems, Constraint theory, domain specific languages, dataflow systems, DSL, Logic, interface automata, Software engineering]
An IDE-based, Integrated Solution to Schema Evolution of Object-Oriented Software
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
With the wide support for serialization in object-oriented programming languages, persistent objects have become common place. Retrieving previously persisted objects from classes whose schema changed is however difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses this issues through an IDE-based approach that handles schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of any corrupt objects. This article describes the principles behind invariant-safe schema evolution,and the design and implementation of the ESCHER system.
[Java, object-oriented programming, versioning, Object oriented modeling, transformation functions, ESCHER framework, object oriented software, integrated solution, schema evolution, IDE-based, serialization, Computer languages, Runtime, refactoring, object-oriented programming languages, Packaging, object-oriented languages, persistence, Robustness, Software engineering]
KaitoroBase: Visual Exploration of Software Architecture Documents
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
This paper describes a software architecture documentation tool (KaitoroBase) built within the Thinkbase Visual Wiki to provide support for non-linear navigation and visualization of Software Architecture Documents (SADs) produced using the Attribute-Driven Design (ADD) method. This involves constructing the meta-model for the SAD in Freebase which provides the foundation for the graph-based interactive visualization enabled by Thinkbase. The resulting tool displays a graphical, high-level structure of SAD, allows for exploratory search, non-linear navigation, and at the same time connects to low-level details of SADs in a wiki.
[Visualization, Thinkbase Visual Wiki, Design methodology, Moon, tool display, software architecture, exploratory search, Software architecture, software architecture documentation tool, data visualisation, meta model, Computer architecture, KaitoroBase, Visual Wiki, document handling, Navigation, Documentation, freebase, Knowledge management, attribute driven design method, Software Architecture Document (SAD), high level structure SAD, Concrete, Attribute-Driven Design (ADD), Software engineering, nonlinear navigation, graph based interactive visualization]
phpModeler - A Web Model Extractor
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
This paper presents phpModeler, a tool for reverse engineering of legacy php web applications that generates static UML diagrams showing resources which the current web page is using, its functions and dependencies it has on other web pages. Once the models describing individual web pages have been generated, phpModeler can analyze them and generate dependency models that for each entity in every page model show all entities dependent on it. phpModeler can also be used to highlight the difference between page models - a feature that, when combined with a SVN repository shows the way how the current web page has evolved over time. phpModeler is a plugin for the Eclipse IDE.
[Java, Unified Modeling Language, Unified modeling language, Reverse engineering, Service oriented architecture, reverse engineering, HTML, Eclipse IDE, Web model extractor, web applications, Mechanical engineering, tool, legacy php Web application, phpModeler, Databases, SVN repository, Web pages, Web page, architecture recovery, Libraries, Internet, maintenance, Software engineering, static UML diagram]
Zoltar: A Toolset for Automatic Fault Localization
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Locating software components which are responsible for observed failures is the most expensive, error-prone phase in the software development life cycle. Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important process for the development of dependable software. In this paper we present a toolset for automatic fault localization, dubbed Zoltar, which hosts a range of spectrum-based fault localization techniques featuring BARINEL, our latest algorithm. The toolset provides the infrastructure to automatically instrument the source code of software programs to produce runtime data, which is subsequently analyzed to return a ranked list of diagnosis candidates. Aimed at total automation (e.g., for runtime fault diagnosis), Zoltar has the capability of instrumenting the program under analysis with fault screeners as a run-time replacement for design-time test oracles.
[program debugging, error prone phase, Embedded software, Fault diagnosis, zoltar, Runtime, BARINEL, software engineering, software tools, software components, Testing, software development, Instruments, runtime data production, automatic fault localization, Debugging, Probability, Software debugging, software programs, Zoltar, runtime replacement, software faults, Fault detection, debugging process, spectrum based fault localization techniques, automated diagnosis, Software tools, Fault Localization tool, Software engineering]
Supporting Requirements Validation: The EuRailCheck Tool
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
We present the EuRailCheck tool, which supports the formalization and the validation of requirements, based on the use of formal methods. The tool allows the user to analyze the requirements in natural language and to categorize and structure them. It allows to formalize the requirements into a subset of UML enriched with static and temporal constraints for which we defined a formal semantics. Finally, the tool allows to apply model checking techniques specialized for the validation of formal requirements. The tool has been developed and validated within a project funded by the European Railway Agency for the validation of the European Train Control System specification. By now, the tool has been successfully used by about thirty railway experts of different companies.
[static constraints, Railway engineering, Unified modeling language, EuRailCheck tool, Control systems, temporal constraints, formal semantics, formal verification, European train control system specification, Rail transportation, requirements validation, Unified Modeling Language, model checking techniques, Natural languages, railway industry, EuRailCheck, Application software, railway engineering, UML, formal methods, natural languages, European railway agency, Software tools, Joining processes, Usability, natural language, Software engineering, ETCS]
Loopfrog: A Static Analyzer for ANSI-C Programs
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Practical software verification is dominated by two major classes of techniques. The first is model checking, which provides total precision, but suffers from the state space explosion problem. The second is abstract interpretation, which is usually much less demanding, but often returns a high number of false positives. We present Loopfrog, a static analyzer that combines the best of both worlds: the precision of model checking and the performance of abstract interpretation. In contrast to traditional static analyzers, it also provides `leaping' counterexamples to aid in the diagnosis of errors.
[Art, software verification, program diagnostics, Laboratories, Explosions, State-space methods, state space explosion problem, ANSI-C program, formal verification, model checking precision, Transformers, Approximation algorithms, Loopfrog, abstract interpretation performance, Iterative algorithms, Performance analysis, Iterative methods, static analyzer, Software engineering]
A Tool Suite for the Generation and Validation of Configurations for Software Availability
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
The Availability Management Framework (AMF) is a service responsible for managing the availability of services provided by applications that run under its control. Standardized by the Service Availability Forum (SAF), AMF requires for its operations a complete and compliant AMF configuration of the applications to be managed. In this paper, we describe two complementary and integrated tools for AMF configurations generation and validation. Indeed, writing manually an AMF configuration is a tedious and error prone task as a large number of requirements defined in the standard have to be taken into consideration during the process. One solution for ensuring compliance with the standard is the validation of the configurations against all the AMF requirements. For this, we have designed and implemented a domain model for AMF configurations and use it as a basis for an AMF configuration validator. To further ease the task of a configuration designer, we have devised and implemented a method for generating automatically AMF configurations.
[Availability, Validation, program verification, AMF configurations validation, Redundancy, software reliability, Conference management, Application software, Domain Model, tool suite, AMF configurations generation, Automated Configuration Generation, Engineering management, service availability forum, software availability, Writing, availability management framework, Availability Management Framework, High-Availability, software tools, Resource management, Software tools, Protection, Software engineering]
A Tool for Attributed Goal-Oriented Requirements Analysis
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
This paper presents an integrated supporting tool for Attributed Goal-Oriented Requirements Analysis (AGORA), which is an extended version of goal-oriented analysis. Our tool assists seamlessly requirements analysts and stakeholders in their activities throughout AGORA steps including constructing goal graphs with group work, prioritizing goals, and version control of goal graphs.
[Humans, integrated supporting tool, Goal-Oriented Requirements Analysis, Paper technology, Security, formal specification, AGORA, attributed goal oriented requirements analysis, formal verification, systems analysis, software tools, Usability, Software engineering]
Jtop: Managing JUnit Test Cases in Absence of Coverage Information
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Test case management may make the testing process more efficient and thus accelerate software delivery. With the popularity of using JUnit for testing Java software, researchers have paid attention to techniques to manage JUnit test cases in regression testing of Java software. Typically, most existing test case management tools are based on the coverage information. However, coverage information may need extra efforts to obtain. In this paper, we present an Eclipse IDE plug-in (named Jtop) for managing JUnit test cases in absence of coverage information. Jtop statically analyzes the program under test and its corresponding JUnit test cases to perform the following management tasks: regression test case selection, test suite reduction and test case prioritization. Furthermore, Jtop also enables the programmer to manually manipulate test cases through a graphical user interface.
[Software testing, program testing, graphical user interfaces, test case selection, Electronic equipment testing, test case prioritization, Conference management, test case management, Displays, software management, test suite reduction, Technology management, Engineering management, Java software, Graphical user interfaces, Java, software testing, graphical user interface, coverage Information, Jtop, software delivery, regression test case selection, Automatic testing, managing JUnit test cases, Software engineering]
An Automated Tool for Generating UML Models from Natural Language Requirements
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
This paper describes a domain independent tool, named, UML Model Generator from Analysis of Requirements (UMGAR), which generates UML models like the Use-case Diagram, Analysis class model, Collaboration diagram and Design class model from natural language requirements using efficient Natural Language Processing (NLP) tools. UMGAR implements a set of syntactic reconstruction rules to process complex requirements into simple requirements. UMGAR also provides a generic XMI parser to generate XMI files for visualizing the generated models in any UML modeling tool. With respect to the existing tools in this area, UMGAR provides more comprehensive support for generating models with proper relationships, which can be used for large requirement documents.
[Visualization, Unified modeling language, computational linguistics, Humans, automated tool, NLP, Requirement engineering, groupware, Natural language processing, XMI files, use case diagram, Automation, Unified Modeling Language, Collaborative tools, Collaborative software, natural language processing, Natural languages, syntactic reconstruction rules, Natural Language Processing, International collaboration, UML model generator from analysis of requirements, collaboration diagram, XMI parser, natural language requirements, analysis class model, Software engineering]
AOWP: Web-Specific AOP Framework for PHP
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Aspect-oriented programming (AOP) is a technique for modularizing crosscutting concerns (CCCs). A variety of CCCs can be found in typical Web applications. Most CCCs are scattered over Web-specific events such as page requests. AOWP, a PHP-based AOP framework, provides Web-specific aspect instantiations for dealing with session management and pointcut &amp; advice mechanisms for capturing Web-specific events. CCCs in Web applications can be clearly modularized by introducing AOWP.
[Access control, Web specific events, session management, Web application development, aspect oriented programming, Scattering, Web applications, page requests, Domain-specific pointcuts, Programming profession, Authentication, Writing, crosscutting concerns modularization, aspect-oriented programming, AOWP, Web specific AOP framework, PHP, Weaving, Internet, Aspect-oriented programming, Informatics, Software engineering, Context modeling]
A Modelling Language for Interactive Web Applications
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Web applications are increasingly becoming the most important platform for software applications in industry, with many modelling languages proposed to handle the complexity of developing, documenting and deploying these applications. New technology has allowed for development of Rich Internet Applications (RIAs) which increase usability and reliability; however, existing modelling languages fall short of modelling many of these new concepts. The research in this Ph.D. seeks to identify these new modelling challenges, and develop an approach that is suitable for modelling RIAs.
[modelling languages, Java, Computer aided software engineering, rich Internet applications, rich internet applications, HTML, Application software, Security, modelling language, software applications, interactive Web applications, Computer industry, model-driven architecture, simulation languages, software engineering, Internet, Standards development, Usability, Software engineering]
Automated Software Tool Support for Checking the Inconsistency of Requirements
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Handling inconsistency in software requirements is a complicated task which has attracted the interest of many groups of researchers. Formal and semi-formal specifications often have inconsistencies in the depicted requirements that need to be managed and resolved. This is particularly challenging when refining informal to formalized requirements. We propose an automated tool with traceability and consistency checking techniques to support analysis of requirements and traceability between different representations: textual, visual, informal and formal.
[automated software tool support, Collaborative software, Natural languages, Unified modeling language, semiformal specifications, Requirements Engineering, Formal specifications, Logic testing, formal specification, software requirements, Traceability, Design engineering, Engineering management, inconsistency handling, requirements inconsistency checking, textual and visual requirements representation, Software systems, Inconsistency management, software tools, Software tools, Software engineering]
A Holistic Approach to Mobile Service Provisioning
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Mobile service provisioning is concerned with enabling handheld devices, such as phones and PDAs, to host services. This naturally raises significant challenges that stem from mobile devices' inherent mobility and limited resources. Furthermore, mobile services tend to operate in dynamic, insecure and heterogeneous operating environments. The aim of our research is to develop a novel toolkit for constructing mobile services that addresses such challenges. In getting started, we have taken a middleware-based approach that mediates interaction between clients and mobile services. This work offers a foundation upon which we intend to build using techniques that fall within the domain of automated software engineering.
[Base stations, client-server systems, middleware based approach, Bluetooth, Ground penetrating radar, holistic approach, Power supplies, multihomed devices, automated software engineering, clients interaction, Patient monitoring, software architecture, model driven development, mobile computing, mobile service provisioning, Bandwidth, Streaming media, Personal digital assistants, vertical handover, Mobile computing, Software engineering, middleware]
Secure and Usable Requirements Engineering
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Software security is an increasingly important aspect of computing; however, it is still addressed as an after thought in too many development efforts. While a variety of approaches have been proposed for security requirements engineering, we find many still lacking with respect to their usability. In this proposal I describe my work in the area of security requirements engineering. SURE, Secure and Usable Requirements Engineering, is a new approach that supports non-security experts in order to specify security requirements from which testing artifacts can be derived. In addition, ASSURE, Automated Support for Secure and Usable Requirements Engineering, a system that implements the SURE technique is presented.
[Proposals, formal specification, automated support secure and usable requirements engineering, specifications, software usability, security, formal verification, usability, Robustness, Protection, Computer security, Testing, Availability, requirements, ASSURE, testing, secure and usable requirements engineering, SURE, security of data, Information security, systems analysis, Systems engineering and theory, software security, Usability, Software engineering]
Goal-Based Testing of Semantic Web Services
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Web services, the reusable software components, have brought automation to Internet computing. However, since they are currently described syntactically using XML standards, i.e., SOAP, WSDL and UDDI, the automation of web services tasks, e.g., web service discovery, selection, composition and execution, is still a challenge. In order to make the most of automation in Service Oriented Architecture (SOA), the concept of the semantic web services (SWS), which are described semantically using an ontology language, have been introduced. The research related to testing and quality assurance aspects of web services is not mature. This is especially true for semantic web services, since research to-date has mainly focused the automation of WS tasks. Furthermore, some semantic web service frameworks promote the client-oriented SOA, by formally specifying user requirements, called "goal specification\
[Automation, Service oriented architecture, Ontologies, Internet computing, reusable software component, goal based semantic Web service, Simple object access protocol, formal specification, Semantic Web, software architecture, Web services, formal verification, Web and internet services, XML standard, client oriented SOA, XML, software reusability, goal based testing, ontology language, service oriented architecture, user requirement, Software reusability, Testing]
Migration from Procedural Programming to Aspect Oriented Paradigm
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Separation of Concerns' has long been a key issue in the field of Software Engineering. While developing a large complex and scalable Software, it can be observed that certain concerns have a tendency to get interleaved with the Core-Functionalities in such a way that they become inseparable. As a result of which the coder, who is supposed to be responsible only with the Core-Functionalities, is bound to take extra burden or botheration regarding the proper and accurate handling of these scattered and crosscutting concerns called Aspects. In our work we propose to devise a complete process of migrating a procedural form source-code to an aspect oriented program. We propose to devise a methodology to separate the scattered concerns from source-code through Code-Mining cascaded with a Traceability-Framework also to be framed by us. Thereafter we propose to devise a Design-Level Aspect Oriented Model for refactoring these separated code fragments in the Aspect Oriented Paradigm. Lastly, we propose to verify and validate the complete migration process.
[Software maintenance, Automatic programming, core functionalities, Scalability, Scattering, Debugging, scalable software, aspect oriented program, traceability framework, procedural programming migration, Traceability, Technology management, aspect oriented paradigm, Aspect-Oriented-Model and Verification, aspect-oriented programming, Separation-of-Concerns, software engineering, Functional programming, large complex software, Object oriented programming, code mining, Software engineering, Testing]
Improving Component Dependency Resolution with Soft Constraints, Validation and Verification
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Software components are encapsulated units of execution which express dependencies through explicitly stated requirements and capabilities. These are used within component repositories to resolve sub-systems of deployable components. The main problem with this dependency resolution is not the lack of available solutions but the excess of them. Within current repositories the returned solution may not be optimal for the composers requirements and context. Improving dependency resolved solutions from repositories using soft constraints with validation and verification is the topic of our Ph.D. and discussed within this paper.
[Availability, object-oriented programming, Quality of service, Licenses, NP-complete problem, Delay, component dependency resolution, deployable components, Runtime, formal verification, soft constraints, Packaging, Software systems, Cost function, software engineering, constraint handling, software components, validation, verification, Software engineering]
[Publisher's information]
2009 IEEE/ACM International Conference on Automated Software Engineering
None
2009
Provides a listing of current staff, committee members and society officers.
[]
Welcome Message from the Chairs
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This conference publication contains the proceedings of the 26th International Conference on Automated Software Engineering (ASE 2011), held at The Oread, in Lawrence, Kansas, USA, on November 6&#x2013;12, 2011. The IEEE/ACM International Conference on Automated Software Engineering brings together researchers and practitioners to share ideas on the foundations, techniques, tools, and applications of automated software engineering. The specific topics targeted by ASE 2011 included but were not limited to: Automated reasoning techniques, Component-based systems, Computer-supported cooperative work, Configuration management, Domain modelling and meta-modelling, Empirical software engineering, Human-computer interaction, Knowledge acquisition and management, Maintenance and evolution, Model-based software development, Model-driven engineering and model transformation, Modelling language semantics, Open systems development, Product line architectures, Program understanding, Program synthesis, Program transformation, Reengineering, Requirements engineering, Specification languages, Software architecture and design, Software visualization, Testing, verification, and validation, Tutoring, help, and documentation systems, and Software analysis.
[]
Wikipedia and how to use it for semantic document representation
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Summary form only given. With petascale systems becoming broadly available in high end computing, attention is now focused on the challenges associated with the next major performance milestone: exascale computing. Demand for computational capability grows unabated, with areas of national and commercial interest including global climate change, alternative energy sources, defense and medicine, as well as basic science. Past growth in the high end has relied on a combination of faster clock speeds and larger systems, but the clock speed benefits of Moore's Law have ended, and 200-cabinet petascale machines are near a practical limit. Future system designs will instead be constrained by power density and total system power demand, resulting in radically different architectures. The challenges associated with exascale computing will require broad research activities across computer science, including the development of new algorithms, programming models, system software and computer architecture.
[]
Unifying testing and analysis through behavioral coverage
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Summary form only given. The primary technical goal of this work is the development of a comprehensive approach to perimeter protection for critical infrastructure and industrial sites. The approach taken is to combine both video and non-video sensors so as to produce a real-time system capable of tracking objects of interest and of detecting potential events that may warrant the attention of security officials. A summary of the system architecture is shown in Figure 1. System modules include: &#x2022; The ability to detect and track people using both visual and radar based sensors. &#x2022; The detection of articulated motions as well as complex and abnormal events. &#x2022; The application of object recognition to detected left behind objects.
[]
Automated web application testing using search based software engineering
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This paper introduces three related algorithms and a tool, SWAT, for automated web application testing using Search Based Software Testing (SBST). The algorithms significantly enhance the efficiency and effectiveness of traditional search based techniques exploiting both static and dynamic analysis. The combined approach yields a 54% increase in branch coverage and a 30% reduction in test effort. Each improvement is separately evaluated in an empirical study on 6 real world web applications.
[Algorithm design and analysis, search based software engineering, program testing, Heuristic algorithms, Instruments, program diagnostics, search based software testing, SBSE, static analysis, Web applications, Vectors, HTML, dynamic analysis, Automated Test data generation, SWAT, Internet, Acceleration, automated Web application testing, Testing]
Auto-locating and fix-propagating for HTML validation errors to PHP server-side code
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Checking/correcting HTML validation errors in Web pages is helpful for Web developers in finding/fixing bugs. However, existing validating/fixing tools work well only on static HTML pages and do not help fix the corresponding server code if validation errors are found in HTML pages, due to several challenges with dynamically generated pages in Web development. We propose PhpSync, a novel automatic locating/fixing tool for HTML validation errors in PHP-based Web applications. Given an HTML page produced by a server-side PHP program, PhpSync uses Tidy, an HTML validating/correcting tool to find the validation errors in that HTML page. If errors are detected, it leverages the fixes from Tidy in the given HTML page and propagates them to the corresponding location(s) in PHP code. Our core solutions include 1) a symbolic execution algorithm on the given PHP program to produce a single tree-based model, called D-model, which approximately represents its possible client page outputs, 2) an algorithm mapping any text in the given HTML page to the text(s) in the node(s) of the D-model and then to the PHP code, and 3) a fix-propagating algorithm from the fixes in the HTML page to the PHP code via the D-model and the mapping algorithm. Our empirical evaluation shows that on average, PhpSync achieves 96.7% accuracy in locating the corresponding locations in PHP code from client pages, and 95% accuracy in propagating the fixes to the server-side code.
[Web developers, program verification, fix-propagating algorithm, D-model, PHP Dynamic Web Applications, HTML, Servers, Tidy, Accuracy, HTML validation errors, Three dimensional displays, hypermedia markup languages, Bug Localization, automatic fixing tool, automatic locating tool, Browsers, PHP server-side code, PhpSync, symbolic execution algorithm, single tree-based model, Web pages, Approximation algorithms, Validation Errors, Internet, Fix Propagation]
Scaling up automated test generation: Automatically generating maintainable regression unit tests for programs
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This paper presents an automatic technique for generating maintainable regression unit tests for programs. We found previous test generation techniques inadequate for two main reasons. First. they were designed for and evaluated upon libraries rather than applications. Second, they were designed to find bugs rather than to create maintainable regression test suites: the test suites that they generated were brittle and hard to understand. This paper presents a suite of techniques that address these problems by enhancing an existing unit test generation system. In experiments using an industrial system, the generated tests achieved good coverage and mutation kill score, were readable by the product's developers, and required few edits as the system under test evolved. While our evaluation is in the context of one test generator, we are aware of many research systems that suffer similar limitations, so our approach and observations are more generally relevant.
[Context, program debugging, program testing, regression analysis, maintainable regression unit tests, Observers, Security, software maintenance, program compilers, automated test generation, bugs, mutation kill score, Databases, industrial system, Libraries, Software, programs, Testing]
Heap cloning: Enabling dynamic symbolic execution of java programs
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
The dynamic symbolic-execution technique can automatically perform symbolic execution of programs that use problematic features of Java, such as native methods. However, to compute precise symbolic execution, the technique requires manual effort to specify models for problematic code. Furthermore, existing approaches to perform symbolic execution either cannot be extended to perform dynamic symbolic execution or incur significant imprecision. In this paper, we present a novel program-transformation technique called heap cloning. Heap cloning transforms a program in such a way that dynamic symbolic execution of the transformed program results in the same path constraints as dynamic symbolic execution of the original program. However, symbolic execution of the transformed program produces feedback on where imprecision is introduced, and that feedback can reduce the manual effort required to build models. Furthermore, such transformation can enable existing approaches to perform symbolic execution systems to overcome their limitations. In this paper, we also present a system, called Cinger, that leverages heap cloning, and that we used to perform an empirical evaluation. The empirical evaluation shows that Cinger can compute precise path constraints, and requires little (if any) manual effort for a set of large real-world programs.
[Java, Cinger, Instruments, Computational modeling, program diagnostics, Cloning, Manuals, program interpreters, dynamic symbolic-execution technique, program-transformation technique, heap cloning, program-analysis technique, Libraries, Concrete, Java programs, software performance evaluation]
Automatic generation of load tests
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Load tests aim to validate whether system performance is acceptable under peak conditions. Existing test generation techniques induce load by increasing the size or rate of the input. Ignoring the particular input values, however, may lead to test suites that grossly mischaracterize a system's performance. To address this limitation we introduce a mixed symbolic execution based approach that is unique in how it 1) favors program paths associated with a performance measure of interest, 2) operates in an iterative-deepening beam-search fashion to discard paths that are unlikely to lead to high-load tests, and 3) generates a test suite of a given size and level of diversity. An assessment of the approach shows it generates test suites that induce program response times and memory consumption several times worse than the compared alternatives, it scales to large and complex inputs, and it exposes a diversity of resource consuming program behavior.
[automatic test generation technique, automatic test pattern generation, iterative methods, program testing, memory consumption, Load testing, mixed symbolic execution based approach, program response time, Size measurement, program path, iterative-deepening beam search fashion, Servers, resource consuming program behavior, Databases, Current measurement, Memory management, load test, symbol manipulation, system performance, symbolic execution, Time factors, Testing]
Symbolic search-based testing
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
We present an algorithm for constructing fitness functions that improve the efficiency of search-based testing when trying to generate branch adequate test data. The algorithm combines symbolic information with dynamic analysis and has two key advantages: It does not require any change in the underlying test data generation technique and it avoids many problems traditionally associated with symbolic execution, in particular the presence of loops. We have evaluated the algorithm on industrial closed source and open source systems using both local and global search-based testing techniques, demonstrating that both are statistically significantly more efficient using our approach. The test for significance was done using a one-sided, paired Wilcoxon signed rank test. On average, the local search requires 23.41% and the global search 7.78% fewer fitness evaluations when using a symbolic execution based fitness function generated by the algorithm.
[Algorithm design and analysis, Software testing, automatic test software, fitness functions, symbolic information, Symbolic Execution, program testing, program diagnostics, Software algorithms, information retrieval, Educational institutions, dynamic analysis, Approximation methods, local search-based testing technique, branch adequate test data generation, one-sided paired Wilcoxon signed rank test, symbolic execution, Approximation algorithms, global search-based testing technique, Fitness Functions, Search-Based Testing, symbolic search-based testing, industrial closed source system, open source systems]
Automated documentation inference to explain failed tests
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
A failed test reveals a potential bug in the tested code. Developers need to understand which parts of the test are relevant to the failure before they start bug-fixing. This paper presents a fully-automated technique (and its tool implementation, called FailureDoc) to explain a failed test. FailureDoc augments the failed test with explanatory documentation in the form of code comments. The comments indicate changes to the test that would cause it to pass, helping programmers understand why the test fails. We evaluated FailureDoc on five real-world programs. FailureDoc generated meaningful comments for most of the failed tests. The inferred comments were concise and revealed important debugging clues. We further conducted a user study. The results showed that FailureDoc is useful in bug diagnosis.
[Measurement, document handling, code comments, program debugging, Correlation, program testing, program diagnostics, automated documentation inference, bug diagnosis, Documentation, Debugging, Vectors, bug fixing, inference mechanisms, tested code, FailureDoc, Concrete, Arrays, debugging clues, failed tests]
Generating program inputs for database application testing
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Testing is essential for quality assurance of database applications. Achieving high code coverage of the database application is important in testing. In practice, there may exist a copy of live databases that can be used for database application testing. Using an existing database state is desirable since it tends to be representative of real-world objects' characteristics, helping detect faults that could cause failures in real-world settings. However, to cover a specific program code portion (e.g., block), appropriate program inputs also need to be generated for the given existing database state. To address this issue, in this paper, we propose a novel approach that generates program inputs for achieving high code coverage of a database application, given an existing database state. Our approach uses symbolic execution to track how program inputs are transformed before appearing in the executed SQL queries and how the constraints on query results affect the application's execution. One significant challenge in our problem context is the gap between program-input constraints derived from the program and from the given existing database state; satisfying both types of constraints is needed to cover a specific program code portion. Our approach includes novel query formulation to bridge this gap. Our approach is loosely integrated into Pex, a state-of-the-art white-box testing tool for .NET from Microsoft Research. Empirical evaluations on two real database applications show that our approach assists Pex to generate program inputs that achieve higher code coverage than the program inputs generated by Pex without our approach's assistance.
[query results, Pex, program testing, fault detection, white box testing tool, database management systems, program compilers, query processing, Loans and mortgages, program code portion, Microsoft Research, program input generation, Databases, .NET, program input constraints, Copper, query formulation, Testing, Context, code coverage, database application testing, software fault tolerance, quality assurance, executed SQL queries, symbolic execution, Concrete, Feeds]
Prioritizing tests for fault localization through ambiguity group reduction
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In practically all development processes, regression tests are used to detect the presence of faults after a modification. If faults are detected, a fault localization algorithm can be used to reduce the manual inspection cost. However, while using test case prioritization to enhance the rate of fault detection of the test suite (e.g., statement coverage), the diagnostic information gain per test is not optimal, which results in needless inspection cost during diagnosis. We present RAPTOR, a test prioritization algorithm for fault localization, based on reducing the similarity between statement execution patterns as the testing progresses. Unlike previous diagnostic prioritization algorithms, RAPTOR does not require false negative information, and is much less complex. Experimental results from the Software Infrastructure Repository's benchmarks show that RAPTOR is the best technique under realistic conditions, with average cost reductions of 40% with respect to the next best technique, with negligible impact on fault detection capability.
[program testing, Subspace constraints, software reliability, fault localization algorithm, Estimation, Inspection, software infrastructure repository, Complexity theory, RAPTOR, Bayesian methods, ambiguity group reduction, Software, regression tests, test prioritization algorithm, Testing]
Identifying future field accesses in exhaustive state space traversal
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
One popular approach to detect errors in multi-threaded programs is to systematically explore all possible interleavings. A common algorithmic strategy is to construct the program state space on-the-fly and perform thread scheduling choices at any instruction that could have effects visible to other threads. Existing tools do not look ahead in the code to be executed, and thus their decisions are too conservative. They create unnecessary thread scheduling choices at instructions that do not actually influence other threads, which implies exploring exponentially greater numbers of interleavings. In this paper we describe how information about field accesses that may occur in the future can be used to identify and eliminate unnecessary thread choices. This reduces the number of states that must be processed to explore all possible behaviors and therefore improves the performance of exhaustive state space traversal. We have applied this technique to Java PathFinder, using the WALA library for static analysis. Experiments on several Java programs show big performance gains. In particular, it is now possible to check with Java PathFinder more complex programs than before in reasonable time.
[field accesses, Instruction sets, Companies, state explosion, Java PathFinder, software libraries, thread scheduling choice, WALA, scheduling, Java program, Performance analysis, Context, exhaustive state space traversal, Java, future field access identification, multi-threading, program state space on-the-fly, program diagnostics, Transfer functions, static analysis, Remuneration, concurrency, WALA library, multithreaded program, state-space methods]
Model checking distributed systems by combining caching and process checkpointing
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Verification of distributed software systems by model checking is not a straightforward task due to inter-process communication. Many software model checkers only explore the state space of a single multi-threaded process. Recent work proposes a technique that applies a cache to capture communication between the main process and its peers, and allows the model checker to complete state-space exploration. Although previous work handles non-deterministic output in the main process, any peer program is required to produce deterministic output. This paper introduces a process checkpointing tool. The combination of caching and process checkpointing makes it possible to handle non-determinism on both sides of communication. Peer states are saved as checkpoints and restored when the model checker backtracks and produces a request not available in the cache. We also introduce the concept of strategies to control the creation of checkpoints and the overhead caused by the checkpointing tool.
[Checkpointing, checkpointing, Schedules, process checkpointing tool, peer states, multi-threading, software verification, Instruction sets, Process control, distributed processing, software model checkers, distributed software systems, cache storage, software model checking, caching, formal verification, interprocess communication, distributed systems, Space exploration, state-space exploration, single multithreaded process, Message systems]
Supporting domain-specific state space reductions through local partial-order reduction
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Model checkers offer to automatically prove safety and liveness properties of complex concurrent software systems, but they are limited by state space explosion. Partial-Order Reduction (POR) is an effective technique to mitigate this burden. However, applying existing notions of POR requires to verify conditions based on execution paths of unbounded length, a difficult task in general. To enable a more intuitive and still flexible application of POR, we propose local POR (LPOR). LPOR is based on the existing notion of statically computed stubborn sets, but its locality allows to verify conditions in single states rather than over long paths. As a case study, we apply LPOR to message-passing systems. We implement it within the Java Pathfinder model checker using our general Java-based LPOR library. Our experiments show significant reductions achieved by LPOR for model checking representative message-passing protocols and, maybe surprisingly, that LPOR can outperform dynamic POR.
[Algorithm design and analysis, Java, message passing, domain specific state space reduction, Computational modeling, Heuristic algorithms, message passing protocol, safety-critical software, Java based LPOR library, Complexity theory, Optimization, Java Pathfinder model checker, formal verification, complex concurrent software systems, concurrency control, local partial order reduction, System recovery, protocols]
Scalable and precise symbolic analysis for atomicity violations
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
We present a symbolic testing tool BEST for finding atomicity violations. We automatically infer and generate potential atomicity properties from an observed run of a multi-threaded program, and use precise modeling and constraint-based symbolic search to find atomicity violating schedules in the most generalization of the observed run. We focus mainly on the tool scalability by devising various simplification steps to reduce the formula and the search space by orders-of-magnitude. To that effect, we also introduce a new notion of atomicity that is useful and simple to check. We demonstrate the effectiveness of the combined techniques on several public C/C++/Java benchmarks in finding known/unknown atomicity bugs.
[Schedules, Java, program debugging, multi-threading, program testing, atomicity bugs, tool scalability, Generators, C++ language, constraint based symbolic search, Synchronization, symbolic analysis, Concurrent computing, Runtime, C benchmarks, Computer bugs, symbol manipulation, multithreaded program, symbolic testing tool, Java benchmarks, BEST, C++ benchmarks, atomicity violations]
DC2: A framework for scalable, scope-bounded software verification
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software model checking and static analysis have matured over the last decade, enabling their use in automated software verification. However, lack of scalability makes these tools hard to apply. Furthermore, approximations in the models of program and environment lead to a profusion of false alarms. This paper proposes DC2, a verification framework using scope-bounding to bridge these gaps. DC2 splits the analysis problem into manageable parts, relying on a combination of three automated techniques: (a) techniques to infer useful specifications for functions in the form of pre- and post-conditions; (b) stub inference techniques that infer abstractions to replace function calls beyond the verification scope; and (c) automatic refinement of pre- and post-conditions from false alarms identified by a user. DC2 enables iterative reasoning over the calling environment, to help in finding non-trivial bugs and fewer false alarms. We present an experimental evaluation that demonstrates the effectiveness of DC2 on several open-source and industrial software projects.
[Industries, stub inference techniques, program verification, Scalability, program diagnostics, public domain software, static analysis, automatic refinement, software model checking, open-source projects, Analytical models, iterative reasoning, DC2, industrial software projects, Computer bugs, scope-bounded software verification, Syntactics, Software, automated software verification, Arrays]
Formalizing hardware/software interface specifications
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software drivers are usually developed after hardware devices become available. This dependency can induce a long product cycle. Although co-simulation and co-verification techniques have been utilized to facilitate the driver development, Hardware/Software (HW/SW) interface models, as the test harnesses, are often challenging to specify. Such interface models should have formal semantics, be efficient for testing, and cover all HW/SW behaviors described by HW/SW interface protocols. We present an approach to formalizing HW/SW interface specifications, where we propose a semantic model, relative atomicity, to capture the concurrency model in HW/SW interfaces; demonstrate our approach via a realistic example; elaborate on how we have utilized this approach in device/driver development process; and discuss criteria for evaluating our formal specifications. We have detected fifteen issues in four English specifications. Furthermore, our formal specifications are readily useful as the test harnesses for co-verification, which has discovered twelve real bugs in five industrial driver programs.
[hardware devices, program debugging, Protocols, Barium, HW/SW interface protocols, device drivers, product cycle, Registers, formal specification, HW/SW behaviors, Concurrent computing, formal semantics, formal verification, Semantics, English specifications, industrial driver programs, Hardware, protocols, software drivers, co-verification techniques, semantic model, formal specifications, co-simulation techniques, concurrency model, concurrency control, peripheral interfaces, Software, HW/SW interface models, driver development, hardware/software interface specifications, hardware/software interface models, HW/SW interface specifications, relative atomicity]
Safe asynchronous multicore memory operations
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Asynchronous memory operations provide a means for coping with the memory wall problem in multicore processors, and are available in many platforms and languages, e.g., the Cell Broadband Engine, CUDA and OpenCL. Reasoning about the correct usage of such operations involves complex analysis of memory accesses to check for races. We present a method and tool for proving memory-safety and race-freedom of multicore programs that use asynchronous memory operations. Our approach uses separation logic with permissions, and our tool automates this method, targeting a C-like core language. We describe our solutions to several challenges that arose in the course of this research. These include: syntactic reasoning about permissions and arrays, integration of numerical abstract domains, and utilization of an SMT solver. We demonstrate the feasibility of our approach experimentally by checking absence of DMA races on a set of programs drawn from the IBM Cell SDK.
[parallel memories, program verification, Instruction sets, parallel architectures, Abstract interpretation, Cognition, C language, Software verification, Automated theorem proving, memory access, SMT solver utilization, syntactic reasoning, safe asynchronous multicore memory operation, cell broadband engine, storage management, C-like core language, IBM cell SDK, Microprocessors, DMA races, memory coping, multicore processor, multiprocessing systems, Multicore processing, complex analysis, numerical abstract domain, CUDA, Computer bugs, separation logic, multicore program memory safety, OpenCL, Logic arrays, Concurrent programs]
A model-driven framework for guided design space exploration
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Design space exploration (DSE) aims at searching through various models representing different design candidates to support activities like configuration design of critical systems or automated maintenance of IT systems. In model-driven engineering, DSE is applied to find instance models that are (i) reachable from an initial model with a sequence of transformation rules and (ii) satisfy a set of structural and numerical constraints. Since exhaustive exploration of the design space is infeasible for large models, the traversal is often guided by hints, derived by system analysis, to prioritize the next states to traverse (selection criteria) and to avoid searching unpromising states (cut-off criteria). In this paper, we define an exploration approach where selection and cut-off criteria are defined using dependency analysis of transformation rules and an algebraic abstraction. The approach is evaluated against other exploration techniques and illustrated on a cloud infrastructure configuration problem.
[exploration techniques, model transformation, system configuration design, algebraic abstraction, Servers, system analysis, Analytical models, Databases, numerical constraints, cut-off criteria, model-driven framework, automated IT system maintenance, software engineering, Space exploration, Numerical models, information systems, transformation rule dependency analysis, Labeling, cloud computing, design space exploration, structural constraints, Vectors, systems analysis, model-driven engineering, cloud infrastructure configuration problem, transformation rule sequence, selection criteria, guided design space exploration]
Automated extraction of architecture-level performance models of distributed component-based systems
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Modern enterprise applications have to satisfy increasingly stringent Quality-of-Service requirements. To ensure that a system meets its performance requirements, the ability to predict its performance under different configurations and workloads is essential. Architecture-level performance models describe performance-relevant aspects of software architectures and execution environments allowing to evaluate different usage profiles as well as system deployment and configuration options. However, building performance models manually requires a lot of time and effort. In this paper, we present a novel automated method for the extraction of architecture-level performance models of distributed component-based systems, based on monitoring data collected at run-time. The method is validated in a case study with the industry-standard SPECjEnterprise2010 Enterprise Java benchmark, a representative software system executed in a realistic environment. The obtained performance predictions match the measurements on the real system within an error margin of mostly 10-20 percent.
[architecture-level performance model automated extraction, system deployment, SPECjEnterprise2010 Enterprise Java benchmark, Unified modeling language, software architectures, Predictive models, distributed processing, Servers, Data mining, software architecture, quality-of-service requirements, data monitoring, configuration options, real system measurement, Monitoring, software performance evaluation, electronic commerce, Context, performance requirements, Java, object-oriented programming, usage profile evaluation, enterprise applications, distributed component-based systems, software metrics]
Precomputing possible configuration error diagnoses
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Complex software packages, particularly systems software, often require substantial customization before being used. Small mistakes in configuration can lead to hard-todiagnose error messages. We demonstrate how to build a map from each program point to the options that might cause an error at that point. This can aid users in troubleshooting these errors without any need to install or use additional tools. Our approach relies on static dataflow analysis, meaning all the analysis is done in advance. We evaluate our work in detail on two substantial systems, Hadoop and the JChord program analysis toolkit, using failure injection and also by using log messages as a source of labeled program points. When logs and stack traces are available, they can be incorporated into the analysis. This reduces the number of false positives by nearly a factor of four for Hadoop, at the cost of approximately one minute's work per unique query.
[Context, Java, Instruments, program diagnostics, program point, Debugging, Servers, precomputing possible configuration error diagnoses, Web services, Hadoop program analysis toolkit, data flow computing, software packages, JChord program analysis toolkit, Libraries, static dataflow analysis]
An optimal strategy for algorithmic debugging
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Algorithmic debugging is a technique that uses an internal data structure to represent computations and ask about their correctness. The strategy used to explore this data structure is essential for the performance of the technique. The most efficient strategy in practice is Divide and Query that, until now, has been considered optimal in the worst case. In this paper we first show that the original algorithm is inaccurate and moreover, in some situations it is unable to find all possible solutions, thus it is incomplete. Then, we present a new version of the algorithm that solves these problems. Moreover, we introduce a counterexample showing that Divide and Query is not optimal, and we propose the first optimal strategy for algorithmic debugging with respect to the number of questions asked by the debugger.
[program debugging, Heuristic algorithms, Debugging, Data structures, Search problems, Indexes, Equations, optimal strategy, Algorithmic Debugging, query processing, algorithmic debugging, Divide and Query, internal data structure, USA Councils, data structures, divide and query]
Localizing SQL faults in database applications
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This paper presents a new fault-localization technique designed for applications that interact with a relational database. The technique uses dynamic information specific to the application's database, such as Structured Query Language (SQL) commands, to provide a fault-location diagnosis. By creating statement-SQL tuples and calculating their suspiciousness, the presented method lets the developer identify the database commands and the program statements likely to cause the failures. The technique also calculates suspiciousness for statement-attribute tuples and uses this information to identify SQL fragments that are statistically likely to be responsible for the suspiciousness of that SQL command. The paper reports the results of two empirical studies. The first study compares existing and database-aware fault-localization methods, and reveals the strengths and limitations of prior techniques, while also highlighting the effectiveness of the new approach. The second study demonstrates the benefits of using database information to improve understanding and reduce manual debugging effort.
[Measurement, program statement, Java, program debugging, SQL fault localization, database-aware fault-localization method, Instruments, Relational databases, fault location, manual debugging, relational database, structured query language command, relational databases, SQL, database application, statement-attribute tuple, Marketing and sales, Software, statement-SQL tuple]
Improving automated documentation to code traceability by combining retrieval techniques
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Documentation written in natural language and source code are two of the major artifacts of a software system. Tracking a variety of traceability links between software documentation and source code assists software developers in comprehension, efficient development, and effective management of a system. Automated traceability systems to date have been faced with a major open research challenge: how to extract these links with both high precision and high recall. In this paper we introduce an approach that combines three supporting techniques, Regular Expression, Key Phrases, and Clustering, with a Vector Space Model (VSM) to improve the performance of automated traceability between documents and source code. This combination approach takes advantage of strengths of the three techniques to ameliorate limitations of VSM. Four case studies have been used to evaluate our combined technique approach. Experimental results indicate that our approach improves the performance of VSM, increases the precision of retrieved links, and recovers more true links than VSM alone.
[source coding, retrieval techniques, automated code traceability systems, program diagnostics, Unified modeling language, system documentation, vector space model, Documentation, information retrieval, source code, Vectors, automated software documentation, regular expression, Vector Space Model, Clustering, Engines, Key Phrases, Traceability, Regular Expression, pattern clustering, Clustering algorithms, Software systems, natural languages, natural language]
Iterative mining of resource-releasing specifications
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software systems commonly use resources such as network connections or external file handles. Once finish using the resources, the software systems must release these resources by explicitly calling specific resource-releasing API methods. Failing to release resources properly could result in resource leaks or even outright system failures. Existing verification techniques could analyze software systems to detect defects related to failing to release resources. However, these techniques require resource-releasing specifications for specifying which API method acquires/releases certain resources, and such specifications are not well documented in practice, due to the large amount of manual effort required to document them. To address this issue, we propose an iterative mining approach, called RRFinder, to automatically mining resource-releasing specifications for API libraries in the form of (resource-acquiring, resource-releasing) API method pairs. RRFinder first identifies resource-releasing API methods, for which RRFinder then identifies the corresponding resource-acquiring API methods. To identify resource-releasing API methods, RRFinder performs an iterative process including three steps: model-based prediction, call-graph-based propagation, and class-hierarchy-based propagation. From heterogeneous information (e.g., source code, natural language), the model-based prediction employs a classification model to predict the likelihood that an API method is a resource-releasing method. The call-graph-based and class-hierarchy-based propagation propagates the likelihood information across methods. We evaluated RRFinder on eight open source libraries, and the results show that RRFinder achieved an average recall of 94.0% with precision of 86.6% in mining resource-releasing specifications, and the mined specifications are useful in detecting resource leak defects.
[iterative methods, application program interfaces, program verification, public domain software, graph theory, software systems, data mining, Predictive models, resource releasing specifications, verification techniques, resource-releasing specification, model based prediction, RRFinder, resource releasing API methods, call graph based propagation, Libraries, open source libraries, Iterative methods, resource leak detection, Java, pattern classification, Object oriented modeling, class hierarchy based propagation, API libraries, specification mining, iterative mining, system failures, classification model, Feature extraction, Resource management]
Flexible design pattern detection based on feature types
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Accurately recovered design patterns support development related tasks like program comprehension and reengineering. Researchers proposed a variety of recognition approaches already. Though, much progress was made, there is still a lack of accuracy and flexibility in recognition. A major problem is the large variety of variants for implementing the same pattern. Furthermore, the integration of multiple search techniques is required to provide more accurate and effective pattern detection. In this paper, we propose variable pattern definitions composed of reusable feature types. Each feature type is assigned to one of multiple search techniques that is best fitting for its detection. A prototype implementation was applied to three open source applications. For each system a baseline was determined and used for comparison with the results of previous techniques. We reached very good results with an improved pattern catalog, but also demonstrated the necessity for customizations on new inspected systems. These results demonstrate the importance of customizable pattern definitions and multiple search techniques in order to overcome accuracy and flexibility issues of previous approaches.
[reusable feature types, variable pattern definitions, software prototyping, Feature-Based Pattern Recognition, pattern catalog, multiple search techniques, Production facilities, customizable pattern definitions, Design Pattern Recognition, Analytical models, Accuracy, Pattern Detection, Code Analysis, flexible design pattern detection, search problems, pattern recognition, program comprehension, Program Comprehension, Java, recognition approaches, program diagnostics, reengineering, Pattern recognition, prototype implementation, software reusability, Feature extraction, Pattern Definition, Regular Expressions, Catalogs]
Towards more accurate retrieval of duplicate bug reports
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In a bug tracking system, different testers or users may submit multiple reports on the same bugs, referred to as duplicates, which may cost extra maintenance efforts in triaging and fixing bugs. In order to identify such duplicates accurately, in this paper we propose a retrieval function (REP) to measure the similarity between two bug reports. It fully utilizes the information available in a bug report including not only the similarity of textual content in summary and description fields, but also similarity of non-textual fields such as product, component, version, etc. For more accurate measurement of textual similarity, we extend BM25F - an effective similarity formula in information retrieval community, specially for duplicate report retrieval. Lastly we use a two-round stochastic gradient descent to automatically optimize REP for specific bug repositories in a supervised learning manner. We have validated our technique on three large software bug repositories from Mozilla, Eclipse and OpenOffice. The experiments show 10-27% relative improvement in recall rate@k and 17-23% relative improvement in mean average precision over our previous model. We also applied our technique to a very large dataset consisting of 209,058 reports from Eclipse, resulting in a recall rate@k of 37-71% and mean average precision of 47%.
[program debugging, software bug repositories, information retrieval community, supervised learning, information retrieval, Information retrieval, duplicate bug reports, Tuning, Training, Support vector machines, bug tracking system, retrieval function, Accuracy, duplicate report retrieval, Computer bugs, Software, textual similarity, two-round stochastic gradient descent, learning (artificial intelligence), REP, gradient methods, BM25F]
A topic-based approach for narrowing the search space of buggy files from a bug report
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Locating buggy code is a time-consuming task in software development. Given a new bug report, developers must search through a large number of files in a project to locate buggy code. We propose BugScout, an automated approach to help developers reduce such efforts by narrowing the search space of buggy files when they are assigned to address a bug report. BugScout assumes that the textual contents of a bug report and that of its corresponding source code share some technical aspects of the system which can be used for locating buggy source files given a new bug report. We develop a specialized topic model that represents those technical aspects as topics in the textual contents of bug reports and source files, and correlates bug reports and corresponding buggy files via their shared topics. Our evaluation shows that BugScout can recommend buggy files correctly up to 45% of the cases with a recommended ranked list of 10 files.
[BugScout, program debugging, text analysis, Defect Localization, software development, topic based approach, Software algorithms, buggy files, source code, Vectors, Synchronization, buggy code, Topic Modeling, Training, bug report, textual contents, Prediction algorithms, Software systems, software engineering, search space]
Specifying and detecting meaningful changes in programs
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software developers are often interested in particular changes in programs that are relevant to their current tasks: not all changes to evolving software are equally important. However, most existing differencing tools, such as diff, notify developers of more changes than they wish to see. In this paper, we propose a technique to specify and automatically detect only those changes in programs deemed meaningful, or relevant, to a particular development task. Using four elementary annotations on the grammar of any programming language, namely Ignore, Order, Prefer and Scope, developers can specify, with limited effort, the type of change they wish to detect. Our algorithms use these annotations to transform the input programs into a normalised form, and to remove clones across different normalised programs in order to detect non-trivial and relevant differences. We evaluate our tool on a benchmark of programs to demonstrate its improved precision compared to other differencing approaches.
[Java, Semantics, Cloning, Optimized production technology, Syntactics, meaningful changes detection, software developers, software engineering, programming language, Grammar, normalised programs, meaningful change specification]
Self-adaptive software meets control theory: A preliminary approach supporting reliability requirements
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This paper investigates a novel approach to derive self-adaptive software by automatically modifying the model of the application using a control-theoretical approach. Self adaptation is achieved at the model level to assure that the model-which lives alongside the application at run-time- continues to satisfy its reliability requirements, despite changes in the environment that might lead to a violation. We assume that the model is given in terms of a Discrete Time Markov Chain (DTMC). DTMCs can express reliability concerns by modeling possible failures through transitions to failure states. Reliability requirements may be expressed as reachability properties that constrain the probability to reach certain states, denoted as failure states. We assume that DTMCs describe possible variant behaviors of the adaptive system through transitions exiting a given state that represent alternative choices, made according to certain probabilities. Viewed from a control-theory standpoint, these probabilities correspond to the input variables of a controlled system-i.e., in the control theory lexicon, "control variables". Adopting the same lexicon, such variables are continuously modified at run-time by a feedback controller so as to ensure continuous satisfaction of the requirements despite disturbances, i.e., changes in the environment. Changes at the model level may then be automatically transferred to changes in the running implementation. The approach is methodologically described by providing a translation scheme from DTMCs to discrete-time dynamic systems, the formalism in which the controllers are derived. An initial empirical assessment is described for a case study. Conjectures for extensions to other models and other requirements.
[Adaptation models, run-time verification, discrete time Markov chain, control theory, software reliability, failure modeling, DTMC, reliability, Control systems, control theory lexicon, feedback, formal verification, control variable, Mathematical model, self-adaptive software, reachability analysis, reachability properties, non-functional requirements, discrete time systems, discrete time dynamic system, probability, self-adjusting systems, Adaptive software, Software reliability, Equations, feedback controller, reliability requirement, dynamic systems, Markov processes, Software]
Generalizing evolutionary coupling with stochastic dependencies
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Researchers have leveraged evolutionary coupling derived from revision history to conduct various software analyses, such as software change impact analysis (IA). The problem is that the validity of historical data depends on the recency of changes and varies with different evolution paths-thus, influencing the accuracy of analysis results. In this paper, we formalize evolutionary coupling as a stochastic process using a Markov chain model. By varying the parameters of this model, we define a family of stochastic dependencies that accounts for different types of evolution paths. Each member of this family weighs historical data differently according to their recency and frequency. To assess the utility of this model, we conduct IA on 78 releases of five open source systems, using 16 stochastic dependency types, and compare with the results of several existing approaches. The results show that our stochastic-based IA technique can provide more accurate results than these existing techniques.
[stochastic dependency, Smoothing methods, stochastic-based IA technique, public domain software, evolution paths, Markov chain model, stochastic process, History, software maintenance, software change impact analysis, Couplings, impact analysis, Markov chain, Accuracy, historical data validity, evolutionary coupling, management of change, Markov processes, stochastic dependency types, Software, evolutionary coupling generalization, open source systems]
Differential precondition checking: A lightweight, reusable analysis for refactoring tools
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
One of the most difficult parts of building automated refactorings is ensuring that they preserve behavior. This paper proposes a new technique to check for behavior preservation; we call this technique differential precondition checking. It is simple yet expressive enough to implement the most common refactorings, and the core algorithm runs in linear time. However, the main advantage is that a differential precondition checker can be placed in a library and reused in refactoring tools for many different languages; the core algorithm can be implemented in a way that is completely language independent. We have implemented a differential precondition checker and used it in refactoring tools for Fortran (Photran), PHP, and BC.
[Java, BC, Computational modeling, Image edge detection, Buildings, reusable analysis, software maintenance, refactoring tools, Fortran, Analytical models, behavior preservation, formal verification, refactoring, Semantics, Syntactics, PHP, differential precondition checking, program representation]
A performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
The formalization of variability models (e.g. feature models) is a prerequisite for the automated analysis of these models. The efficient execution of the analysis operations depends on the selection of well-suited solver implementations. Regarding feature models, on the one hand, the formalization with Boolean expressions enables the use of SAT or BDD solvers. On the other hand, feature models can be transformed into a Constraint-Satisfaction Problem (CSP) in order to use CSP solvers for validation. This paper presents a performance comparison regarding nine contemporary high-performance solvers, three for each base problem structure (BDD, CSP, and SAT). Four operations on 90 feature models are run on each solver. The results will in turn clear the way for new improvements regarding the automatic verification of software product lines, since the efficient execution of analysis operations is essential to such automatic verification approaches.
[automated analysis operations, feature models, automated reasoning techniques, Java, program verification, SAT solvers, Computational modeling, BDD solvers, Data structures, variability model formalization, constraint satisfaction problem, contemporary algorithmic approach, performance measurement, Analytical models, software product line, Boolean functions, Runtime, constraint satisfaction problems, Boolean expressions, feature model, Software, software performance evaluation, automatic software product line verification]
Finding relevant answers in software forums
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Online software forums provide a huge amount of valuable content. Developers and users often ask questions and receive answers from such forums. The availability of a vast amount of thread discussions in forums provides ample opportunities for knowledge acquisition and summarization. For a given search query, current search engines use traditional information retrieval approach to extract webpages containing relevant keywords. However, in software forums, often there are many threads containing similar keywords where each thread could contain a lot of posts as many as 1,000 or more. Manually finding relevant answers from these long threads is a painstaking task to the users. Finding relevant answers is particularly hard in software forums as: complexities of software systems cause a huge variety of issues often expressed in similar technical jargons, and software forum users are often expert internet users who often posts answers in multiple venues creating many duplicate posts, often without satisfying answers, in the world wide web. To address this problem, this paper provides a semantic search engine framework to process software threads and recover relevant answers according to user queries. Different from standard information retrieval engine, our framework infer semantic tags of posts in the software forum threads and utilize these tags to recover relevant answer posts. In our case study, we analyze 6,068 posts from three software forums. In terms of accuracy of our inferred tags, we could achieve on average an overall precision, recall and F-measure of 67%, 71%, and 69% respectively. To empirically study the benefit of our overall framework, we also conduct a user-assisted study which shows that as compared to a standard information retrieval approach, our proposed framework could increase mean average precision from 17% to 71% in retrieving relevant answers to various queries and achieve a Normalized Discounted Cumulative Gain (nDCG) @1 score of 91.2% and nDCG@2 score of 71.6%.
[software systems complexities, search engines, World Wide Web, Data mining, Engines, query processing, Semantics, semantic search engine framework, Search engines, software engineering, semantic tags, user assisted study, information retrieval approach, semantic Web, knowledge acquisition, online software forums, search query, F-measure, normalized discounted cumulative gain, Tagging, Feature extraction, social networking (online), Software, expert Internet users, knowledge summarization, computational complexity]
Software process evaluation: A machine learning approach
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.
[Machine learning algorithms, defect management process, sequence classification, software management, software quality, Data mining, sequence classification task, semiautomated approach, software products quality, software process improvement, authority constraint, learning (artificial intelligence), Capability maturity model, pattern classification, machine learning approach, software development, real industrial software project, manual qualitative evaluation, machine learning, Standards organizations, software process evaluation, Organizations, Machine learning, Software, software process]
Local vs. global models for effort estimation and defect prediction
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules. This instability can be explained by data heterogeneity. We show that effort and defect data contain many local regions with markedly different properties to the global space. In other words, what appears to be useful in a global context is often irrelevant for particular local contexts. This result raises questions about the generality of conclusions from empirical SE. At the very least, SE researchers should test if their supposedly general conclusions are valid within subsets of their data. At the very most, empirical SE should become a search for local regions with similar properties (and conclusions should be constrained to just those regions).
[Context, software module defect prediction, program diagnostics, defect/effort estimation, Estimation, data mining, conclusion instability, empirical SE, Data mining, Couplings, Runtime, USA Councils, data heterogeneity, global models, Software, effort estimation, local models, validation, Principal component analysis]
Capacity planning for event-based systems using automated performance predictions
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Event-based communication is used in different domains including telecommunications, transportation, and business information systems to build scalable distributed systems. The loose coupling of components in such systems makes it easy to vary the deployment. At the same time, the complexity to estimate the behavior and performance of the whole system is increased, which complicates capacity planning. In this paper, we present an automated performance prediction method supporting capacity planning for event-based systems. The performance prediction is based on an extended version of the Palladio Component Model - a performance meta-model for component-based systems. We apply this method on a real-world case study of a traffic monitoring system. In addition to the application of our performance prediction techniques for capacity planning, we evaluate the prediction results against measurements in the context of the case study. The results demonstrate the practicality and effectiveness of the proposed approach.
[Adaptation models, Predictive models, traffic monitoring system, telecommunications, Phase change materials, planning (artificial intelligence), event-based communication, traffic information systems, capacity planning, scalable distributed systems, event-based systems, Hardware, automated performance prediction method, Monitoring, object-oriented programming, automated performance predictions, component-based systems, loose coupling, Middleware, transportation, performance meta-model, performance prediction techniques, Palladio component model, business information systems, Capacity planning]
Ecological inference in empirical software engineering
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software systems are decomposed hierarchically, for example, into modules, packages and files. This hierarchical decomposition has a profound influence on evolvability, maintainability and work assignment. Hierarchical decomposition is thus clearly of central concern for empirical software engineering researchers; but it also poses a quandary. At what level do we study phenomena, such as quality, distribution, collaboration and productivity? At the level of files? packages? or modules? How does the level of study affect the truth, meaning, and relevance of the findings? In other fields it has been found that choosing the wrong level might lead to misleading or fallacious results. Choosing a proper level, for study, is thus vitally important for empirical software engineering research; but this issue hasn't thus far been explicitly investigated. We describe the related idea of ecological inference and ecological fallacy from sociology and epidemiology, and explore its relevance to empirical software engineering; we also present some case studies, using defect and process data from 18 open source projects to illustrate the risks of modeling at an aggregation level in the context of defect prediction, as well as in hypothesis testing.
[Measurement, Biological system modeling, Object oriented modeling, hypothesis testing, sociology, Predictive models, diseases, ecological fallacy, software modules, inference mechanisms, hierarchical decomposition, epidemiology, ecology, social sciences, software packages, ecological inference, empirical software engineering, Data models, Software engineering, Testing]
Detection of feature interactions using feature-aware verification
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
A software product line is a set of software products that are distinguished in terms of features (i.e., end-user-visible units of behavior). Feature interactions -situations in which the combination of features leads to emergent and possibly critical behavior- are a major source of failures in software product lines. We explore how feature-aware verification can improve the automatic detection of feature interactions in software product lines. Feature-aware verification uses product-line-verification techniques and supports the specification of feature properties along with the features in separate and composable units. It integrates the technique of variability encoding to verify a product line without generating and checking a possibly exponential number of feature combinations. We developed the tool suite SPLVERIFIER for feature-aware verification, which is based on standard model-checking technology. We applied it to an e-mail system that incorporates domain knowledge of AT&amp;T. We found that feature interactions can be detected automatically based on specifications that have only local knowledge.
[e-mail system, feature interactions, feature-aware verification, Encoding, Electronic mail, model-checking technology, software product line, formal verification, SPLVERIFIER, Automata, product development, product-line-verification techniques, software reusability, Feature extraction, Software, Safety, automatic detection, Cryptography]
Querying source code with natural language
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
One common task of developing or maintaining software is searching the source code for information like specific method calls or write accesses to certain fields. This kind of information is required to correctly implement new features and to solve bugs. This paper presents an approach for querying source code with natural language.
[Context, program debugging, source coding, software development, Natural languages, software maintenance, Engines, query processing, source code querying, Training data, Prototypes, natural languages, Software, natural language, Software engineering]
Coverage rewarded: Test input generation via adaptation-based programming
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This paper introduces a new approach to test input generation, based on reinforcement learning via easy to use adaptation-based programming. In this approach, a test harness can be written with little more effort than is involved in nai&#x0308;ve random testing. The harness will simply map choices made by the adaptation-based programming (ABP) library, rather than pseudo-random numbers, into operations and parameters. Realistic experimental evaluation over three important fine-grained coverage measures (path, shape, and predicate coverage) shows that ABP-based testing is typically competitive with, and sometimes superior to, other effective methods for testing container classes, including random testing and shape-based abstraction.
[Context, test input generation, Java, Shape, program testing, program diagnostics, software testing, Containers, adaptation based programming library, shape based abstraction, reinforcement learning, experimental evaluation, Learning, Libraries, learning (artificial intelligence), Testing, ABP based testing, container classes, random testing]
Mendel: Source code recommendation based on a genetic metaphor
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
When evolving software systems, developers spend a considerable amount of time understanding existing source code. To successfully implement new or alter existing behavior, developers need to answer questions such as: &#x201C;Which types and methods can I use to solve this task?&#x201D; or &#x201C;Should my implementation follow particular naming or structural conventions?&#x201D;. In this paper we present Mendel, a source code recommendation tool that aids developers in answering such questions. Based on the entity the developer currently browses, the tool employs a genetics-inspired metaphor to analyze source-code entities related to the current working context and provides its user with a number of recommended properties (naming conventions, used types, invoked messages, etc.) that the source code entity currently being worked on should exhibit. An initial validation of Mendel seems to confirm the potential of our approach.
[Protocols, object-oriented programming, source coding, software systems, source code entity, Data mining, recommender systems, Mendel, USA Councils, genetic metaphor, source code recommendation, Genetics, Software systems, Concrete]
Optimizing the automatic test generation by SAT and SMT solving for Boolean expressions
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Recent advances in propositional satisfiability (SAT) and Satisfiability Modulo Theories (SMT) solvers are increasingly rendering SAT and SMT-based automatic test generation an attractive alternative to traditional algorithmic test generation methods. The use of SAT/SMT solvers is particularly appealing when testing Boolean expressions: These tools are able to deal with constraints over the models, generate compact test suites, and they support fault-based test generation methods. However, these solvers normally require more time and greater amount of memory than classical test generation algorithms, limiting their applicability. In this paper we propose several ways to optimize the process of test generation and we compare several SAT/SMT solvers and propositional transformation rules. These optimizations promise to make SAT/SMT-based techniques as efficient as standard methods for testing purposes, especially when dealing with Boolean expressions, as proved by our experiments.
[Context, Software testing, satisfiability modulo theories solvers, compact test suites, Limiting, program testing, computability, automatic test generation, Optimization, prepositional satisfiability, Boolean functions, optimisation, Boolean expressions, optimization, Libraries, propositional transformation rules, Context modeling]
Code-based automated program fixing
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Initial research in automated program fixing has generally limited itself to specific areas, such as data structure classes with carefully designed interfaces, and relied on simple approaches. To provide high-quality fix suggestions in a broad area of applicability, the present work relies on the presence of contracts in the code, and on the availability of static and dynamic analyses to gather evidence on the values taken by expressions derived from the code. The ideas have been built into the AutoFix-E2 automatic fix generator. Applications of AutoFix-E2 to general-purpose software, such as a library to manipulate documents, show that the approach provides an improvement over previous techniques, in particular purely model-based approaches.
[program debugging, AutoFix-E2 automatic fix generator, Object oriented modeling, automated debugging, general-purpose software, program synthesis, Indexes, program compilers, data structure classes, code-based automated program fixing, Libraries, Software, data structures, automated debugging and fixing, Arrays, Contracts]
Taming changes With 1.x-Way architecture-implementation mapping
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
A new approach is presented to maintain the conformance between software architecture and code in the presence of changes to both artifacts. Its novel features include suppression of mistaken changes of architecture-prescribed code, explicit recording of architecture changes, and automatic mapping of specific kinds of architecture changes to code in specific ways. In particular, a new code separation mechanism is presented to decouple architecture-prescribed code from user-defined details. It is supported by three important technologies developed in this study to manage architecture changes, including an architecture change model, architecture-based code regeneration, and architecture change notification. The approach is implemented and integrated in ArchStudio, an Eclipse-based architecture development environment.
[Java, 1.x-Way architecture implementation mapping, Manuals, Programming, architecture change notification, architecture prescribed code, consistency management, architecture based code regeneration, software architecture, explicit recording, Software architecture, decouple architecture prescribed code, eclipse based architecture development environment, Computer architecture, taming changes, automatic mapping, code separation mechanism, Software, ArchStudio]
Evaluating test selection strategies for end-user specified flow-based applications
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
An emerging class of end-user programming is the assembly of flow-based applications from a set of reusable components. Testing has become a major challenge, as a very large number of flows can be assembled from a set of components with the expectation of functioning correctly. Faults in assembled flows can create dissatisfaction among users and thereby potentially undermine this end-user programming paradigm We approach this problem as a flow-selection problem, and are interested in ways of testing a subset of flows that provide a high likelihood of revealing faults. We describe a number of flow-selection strategies, which run in the context of a flow pattern, a specification mechanism that constrains the space of assemble-able flows. We evaluate the different strategies on real-world flow patterns in terms of efficiency, i.e., the reduction of flows to test, and effectiveness, measuring of how well the strategies can catch faults.
[program testing, test selection strategies, Programming, formal specification, end user specified flow based application, specification mechanism, assembleable flow fault, flow selection problem, user dissatisfaction, Testing, object-oriented programming, Component Reuse, personal computing, Vectors, Topology, End-user programming, Indexes, Specification-based testing, end user programming, component reuse, directed graphs, flow pattern, software reusability, Coverage Criteria Evaluation, Software, Concrete]
Towards dynamic backward slicing of model transformations
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Model transformations are frequently used means for automating software development in various domains to improve quality and reduce production costs. Debugging of model transformations often necessitates identifying parts of the transformation program and the transformed models that have causal dependence on a selected statement. In traditional programming environments, program slicing techniques are widely used to calculate control and data dependencies between the statements of the program. Here we introduce program slicing for model transformations where the main challenge is to simultaneously assess data and control dependencies over the transformation program and the underlying models of the transformation. In this paper, we present a dynamic backward slicing approach for both model transformation programs and their transformed models based on automatically generated execution trace models of transformations.
[Adaptation models, automatic programming, program debugging, data dependencies, Computational modeling, automating software development, Unified modeling language, dynamic backward slicing approach, program slicing techniques, software quality, software production cost reduction, causal dependence, Servers, program statements, data assess, model transformation debugging, transformation program, Program slicing, Fires, Data models, software cost estimation, automatically generated execution trace models, program slicing, Model transformations]
Mining test oracles of web search engines
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Web search engines have major impact in people's everyday life. It is of great importance to test the retrieval effectiveness of search engines. However, it is labor-intensive to judge the relevance of search results for a large number of queries, and these relevance judgments may not be reusable since the Web data change all the time. In this work, we propose to mine test oracles of Web search engines from existing search results. The main idea is to mine implicit relationships between queries and search results, e.g., some queries may have fixed top 1 result while some may not, and some Web domains may appear together in top 10 results. We define a set of items of queries and search results, and mine frequent association rules between these items as test oracles. Experiments on major search engines show that our approach mines many high-confidence rules that help understand search engines and detect suspicious search results.
[Google, search engines, Itemsets, Web search engine, data mining, Search engines, test oracle, Internet, frequent association rules, Association rules, Engines, Web data]
AutoODC: Automated generation of Orthogonal Defect Classifications
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Orthogonal Defect Classification (ODC), the most influential framework for software defect classification and analysis, provides valuable in-process feedback to system development and maintenance. Conducting ODC classification on existing organizational defect reports is human intensive and requires experts' knowledge of both ODC and system domains. This paper presents AutoODC, an approach and tool for automating ODC classification by casting it as a supervised text classification problem. Rather than merely apply the standard machine learning framework to this task, we seek to acquire a better ODC classification system by integrating experts' ODC experience and domain knowledge into the learning process via proposing a novel Relevance Annotation Framework. We evaluated AutoODC on an industrial defect report from the social network domain. AutoODC is a promising approach: not only does it leverage minimal human effort beyond the human annotations typically required by standard machine learning approaches, but it achieves an overall accuracy of 80.2% when using manual classifications as a basis of comparison.
[program debugging, text classification problem, natural language processing, Humans, Manuals, text classification, AutoODC, machine learning framework, software defect classification, Training, Support vector machines, social network, Accuracy, relevance annotation framework, Text categorization, software defect analysis, Machine learning, software bug, Orthogonal Defect Classification (ODC), learning (artificial intelligence), automated generation of orthogonal defect classifications, influential framework]
Observations on the connectedness between requirements-to-code traces and calling relationships for trace validation
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Traces between requirements and code reveal where requirements are implemented. Such traces are essential for code understanding and change management. Unfortunately, the handling of traces is highly error prone, in part due to the informal nature of requirements. This paper discusses observations on the connectedness between requirements-to-code traces and calling relationships within the source code. These observations are based on the empirical evaluation of four case study systems covering 150 KLOC and 59 sample requirements. We found that certain patterns of connectedness have high or low likelihoods of occurring. These patterns can thus be used to confirm or reject existing traceability - hence they are useful for validating requirements-to-code traces.
[trace handling, error prone, Validation, Java, Gold, program verification, Conferences, Manuals, source code, Information retrieval, requirements-to-code trace validation, change management, Requirements, Traceability, calling relationships, KLOC, Software, Software engineering]
Proximity based weighting of test cases to improve spectrum based fault localization
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Spectrum based fault localization techniques such as Tarantula and Ochiai calculate the suspiciousness score of a program statement using the number of failing and passing test cases that execute the statement. These techniques implicitly assume that all test cases are equally important. However, research on test case generation and selection techniques has shown that using certain test cases can lead to more effective fault localization than others. In this paper, we present an approach to improve the effectiveness of spectrum based fault localization by incorporating the relative importance of different test cases in the calculation of suspiciousness scores.
[Context, program statement, Schedules, program testing, spectrum based fault localization, Computational modeling, fault localization, test cases, suspiciousness scores, weighting test cases, program compilers, proximity based weighting, Equations, software fault tolerance, Ochiai, proximity of test cases, test case generation, Benchmark testing, Mathematical model, Software engineering, Tarantula]
Slicing feature models
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Feature models (FMs) are a popular formalism for describing the commonality and variability of software product lines (SPLs) in terms of features. As SPL development increasingly involves numerous large FMs, scalable modular techniques are required to manage their complexity. In this paper, we present a novel slicing technique that produces a projection of an FM, including constraints. The slicing allows SPL practitioners to find semantically meaningful decompositions of FMs and has been integrated into the FAMILIAR language.
[feature models, software product lines, Frequency modulation, novel slicing technique, FM, Data structures, Slicing, Complexity theory, Feature Models, SPL, Software Product Lines, Boolean functions, Semantics, software reusability, Feature extraction, Software, program slicing]
Using model checking to analyze static properties of declarative models
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
We show how static properties of declarative models can be efficiently analyzed in a symbolic model checker; in particular, we use Cadence SMV to analyze Alloy models by translating Alloy to SMV. The computational paths of the SMV models represent interpretations of the Alloy models. The produced SMV model satisfies its LTL specifications if and only if the original Alloy model is inconsistent with respect to its finite scopes; counterexamples produced by the model checker are valid instances of the Alloy model. Our experiments show that the translation of many frequently used constructs of Alloy to SMV results in optimized models such that their analysis in SMV is much faster than in the Alloy Analyzer. Model checking is faster than SAT solving for static problems when an interpretation can be eliminated by early decisions in the model checking search.
[Alloy models, declarative models, Computational modeling, program diagnostics, Metals, LTL specifications, Educational institutions, static property analysis, formal specification, Optimization, Computer science, Analytical models, formal verification, Semantics, Cadence SMV model, symbolic model checking]
Finding the merits and drawbacks of software resources from comments
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In order to reuse software resources efficiently, developers need necessary quality guarantee on software resources. However, our investigation proved that most software resources on the Internet did not provide enough quality descriptions. In this paper, we propose an approach to help developers judge a software resource's quality based on comments. In our approach, the software resources' comments on the Internet are automatically collected, the sentiment polarity (positive or negative) of a comment is identified and the quality aspects which the comment talks about are extracted. As a result, the merits and drawbacks of software resources are drew out which could help developers judge a software resource's quality in the process of software resource selection and reuse. To evaluate our approach, we applied our method to a group of open source software and the results showed that our method achieved satisfying precision in merits and drawbacks finding.
[public domain software, open source software, Sentiment Polarity, User Comment, sentiment polarity, Software Reuse, software quality, software resource quality, Data mining, Software Quality, Open source software, software resource reuse, Support vector machines, software reusability, Feature extraction, quality guarantee, Internet, Software Resource, Web search]
Combining search-based and constraint-based testing
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Many modern automated test generators are based on either meta-heuristic search techniques or use constraint solvers. Both approaches have their advantages, but they also have specific drawbacks: Search-based methods get stuck in local optima and degrade when the search landscape offers no guidance; constraint-based approaches, on the other hand, can only handle certain domains efficiently. In this paper we describe a method that integrates both techniques and delivers the best of both worlds. On a high-level view, our method uses a genetic algorithm to generate tests, but the twist is that during evolution a constraint solver is used to ensure that mutated offspring efficiently explores different control flow. Experiments on 20 case study examples show that on average the combination improves branch coverage by 28% over search-based techniques and by 13% over constraint-based techniques.
[Software testing, IEEE Computer Society, Java, program testing, search-based methods, automated test generators, constraint solvers, Search problems, meta-heuristic search techniques, search landscape, genetic algorithms, Genetic algorithms, constraint-based techniques, genetic algorithm, USA Councils, search-based testing, constraint-based testing, constraint handling, search problems]
Stateful testing: Finding more errors in code and contracts
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Automated random testing has shown to be an effective approach to finding faults but still faces a major unsolved issue: how to generate test inputs diverse enough to find many faults and find them quickly. Stateful testing, the automated testing technique introduced in this article, generates new test cases that improve an existing test suite. The generated test cases are designed to violate the dynamically inferred contracts (invariants) characterizing the existing test suite. As a consequence, they are in a good position to detect new faults, and also to improve the accuracy of the inferred contracts by discovering those that are unsound. Experiments on 13 data structure classes totalling over 28,000 lines of code demonstrate the effectiveness of stateful testing in improving over the results of long sessions of random testing: stateful testing found 68.4% new faults and improved the accuracy of automatically inferred contracts to over 99%, with just a 7% time overhead.
[stateful testing, program testing, random processes, Search problems, dynamic analysis, Data mining, Databases, automation, automated random testing, Arrays, Contracts, Testing, random testing]
Do software engineers benefit from source code navigation with traceability? &#x2014; An experiment in software change management
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
For decades now, mainstream development environments provide the same basic automations for navigating source code: mainly searching and the tree exploration of files and folders. This may imply that other automations have little additional value or too steep a learning curve for mainstream adoption. This paper investigates whether source code navigation enriched with traceability benefit basic maintenance tasks such as changing features and fixing bugs in code. To test this, we conducted a controlled experiment with 52 subjects performing real maintenance tasks on two third-party development projects: all with the same navigation tool but half of the tasks with and the other half without traceability navigation. We found that the existence of traceability profoundly affected the quality of the change tasks and fundamentally changed how software engineers navigated through source code. We show that software engineers benefit instantly from traceability, without training, which is to show that the current automations available to software engineers are by no means sufficient or the only easy ones to use.
[Industries, Software maintenance, program debugging, software engineers, Automation, Navigation, source coding, program diagnostics, Unified modeling language, Maintenance engineering, software management, traceability benefit basic maintenance tasks, third-party development projects, software maintenance, mainstream development environments, source code navigation, software change management, management of change]
Automating analysis of qualitative preferences in goal-oriented requirements engineering
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In goal-oriented requirements engineering, a goal model graphically represents relationships between the required goals (functional requirements), tasks (realizations of goals), and optional goals (non-functional properties) involved in designing a system. It may, however, be impossible to find a design that fulfills all required goals and all optional goals. In such cases, it is useful to find designs that provide the required functionality while satisfying the most preferred set of optional goals under the goal model's constraints. We present an approach that considers expressive qualitative preferences over optional goals, as these can model interacting and/or mutually exclusive subgoals. Our framework employs a model checking-based method for reasoning with qualitative preferences to identify the most preferred alternative(s). We evaluate our approach using existing goal models from the literature.
[Algorithm design and analysis, goal-oriented requirements engineering, Computational modeling, Documentation, reasoning, qualitative preferences, inference mechanisms, Computer science, Analytical models, formal verification, Robustness, model checking-based method, Testing]
History slicing
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
To perform a number of tasks such as inferring design rationale from past code changes or assessing developer expertise for a software feature or bug, the evolution of a set of lines of code can be assessed by mining software histories. However, determining the evolution of a set of lines of code is a manual and time consuming process. This paper presents a model of this process and an approach for automating it. We call this process History Slicing. We describe the process and options for generating a graph that links every line of code with its corresponding previous revision through the history of the software project. We then explain the method and options for utilizing this graph to determine the exact revisions that contain changes for the lines of interest and their exact position in each revision. Finally, we present some preliminary results which show initial evidence that our automated technique can be several orders of magnitude faster than the manual approach and require that developers examine up to two orders of magnitude less code in extracting such histories.
[program debugging, software feature, Conferences, Manuals, History, Data mining, software histories, software project, USA Councils, software bug, Software, history slicing, program slicing, time consuming process, Software engineering]
Analyzing temporal API usage patterns
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software reuse through Application Programming Interfaces (APIs) is an integral part of software development. As developers write client programs, their understanding and usage of APIs change over time. Can we learn from long-term changes in how developers work with APIs in the lifetime of a client program? We propose Temporal API Usage Mining to detect significant changes in API usage. We describe a framework to extract detailed models representing addition and removal of calls to API methods over the change history of a client program. We apply machine learning technique to these models to semi-automatically infer temporal API usage patterns, i.e., coherent addition of API calls at different phases in the life-cycle of the client program.
[application program interfaces, software reuse, software development, temporal API usage mining, data mining, Documentation, application programming interfaces, Software Reuse, Programming, Usage Pattern, temporal API usage patterns, History, Data mining, API calls, Mining Software Repositories, Machine learning, software reusability, API Usage, API Usability, Software, client program life cycle, Principal component analysis]
Isomorphism in model tools and editors
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Domain-specific languages (DSLs) are modeling languages that are customized for a specific context or project. DSLs allow for fast and precise modeling because the language features and constructs can be precisely tailored based on the needs of the modeling effort. There exist highly customizable model-editing tools that can be easily configured to support DSLs defined by end-users (e.g., system architects, engineers, and analysts). However, to leverage models created using these tools for automated analysis, simulation, and code generation, end-users must build custom analysis tools and code generators. In contrast to model editors, the implementation and maintenance of these analysis and code generation tools can be tedious and hampers the utility of DSLs. In this paper, we posit that analysis and code generation tools for DSLs are, in fact, isomorphic to model editing tools. The implication of this insight is that model editors, analysis tools, and code generators can be treated as analogs conceptually and architecturally, and highly customizable analysis and code generation tools for DSLs can be built using the same approach that has already proven successful for the construction of DSL model editors.
[Unified modeling language, Metamodeling, code generators, program compilers, isomorphism, customizable analysis, Analytical models, Semantics, specification languages, language features, software tools, DSL, domain-specific languages, Object oriented modeling, custom analysis tools, automated simulation, Generators, automated analysis, code generation tools, modeling languages, model editors, end-users, model tools, simulation languages, model-editing tools]
A case for alloy annotations for efficient incremental analysis via domain specific solvers
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Alloy is a declarative modelling language based on first-order logic with sets and relations. Alloy formulas are checked for satisfiability by the fully automatic Alloy Analyzer. The analyzer, given an Alloy formula and a scope, i.e. a bound on the universe of discourse, searches for an instance i.e. a valuation to the sets and relations in the formula, such that it evaluates to true. The analyzer translates the Alloy problem to a propositional formula for which it searches a satisfying assignment via an off-the-shelf propositional satisfiability (SAT) solver. The SAT solver performs an exhaustive search and increasing the scope leads to the combinatorial explosion problem. We envision annotations, a meta-data facility used in imperative languages, as a means of augmenting Alloy models to enable more efficient analysis by specifying the priority, i.e. order of solving, of a given constraint and the slover to be used. This additional information would enable using the solutions to a particular constraint as partial solutions to the next in case constraint priority is specified and using a specific solver for reasoning about a given constraint in case a constraint solver is specified.
[meta data, imperative languages, Metals, first-order logic, Binary search trees, computability, efficient incremental analysis, meta-data facility, Educational institutions, domain specific solvers, SAT solver, declarative modelling language, Analytical models, satisfiability, automatic Alloy analyzer, logic programming, Software, Data models, alloy annotations, Software engineering]
Exploring caching for efficient collection operations
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Many useful programs operate on collection types. Extensive libraries are available in many programming languages, such as the C++ Standard Template Library, which make programming with collections convenient. Extending programming languages to provide collection queries as first class constructs in the language would not only allow programmers to write queries explicitly in their programs but it would also allow compilers to leverage the wealth of experience available from the database domain to optimize such queries. This paper describes an approach to reducing the run time of programs involving explicit collection queries by leveraging a cache to store previously computed results. We propose caching the results of join (sub)queries which allows queries that miss the cache entirely to be answered partially from the cache thereby improving the query execution time. We also describe an effective cache policy to determine which join (sub)queries to cache. The cache is maintained incrementally, when the underlying collections change, and use of the cache space is optimized by a cache replacement policy.
[C++ standard template library, Conferences, cache space, Maintenance engineering, Programming, efficient collection operations, Frequency estimation, cache storage, Selectivity, C++ language, Join caching, programming languages, program compilers, query processing, extending programming languages, Histograms, query collection, Run-time, exploring caching, Query processing, Cache Policy]
Tracing requirements to tests with high precision and recall
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Requirements traceability is linking requirements to software artifacts, such as source code, test-cases and configuration files. For stakeholders of software, it is important to understand which requirements were tested, whether sufficiently, if at all. Hence tracing requirements in test-cases is an important problem. In this paper, we build on existing research and use features, realization of functional requirements in software [15], to automatically create requirements traceability links between requirements and test-cases. We evaluate our approach on a chat system, Apache Pool [21] and Apache Log4j [11]. We obtain precision/recall levels of more than 90%, an improvement upon currently existing Information Retrieval approaches when tested on the same case studies.
[source coding, program diagnostics, requirements traceability, Documentation, information retrieval, source code, testing, Programming, Apache Log4j, Large scale integration, automated analysis, formal specification, requirement traceability, software stakeholders, Apache Pool, information retrieval approaches, formal verification, software artifacts, program understanding, chat system, Software systems, functional software Celal requirements, Software engineering, Testing]
Extracting structured data from natural language documents with island parsing
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
The design and evolution of a software system leave traces in various kinds of artifacts. In software, produced by humans for humans, many artifacts are written in natural language by people involved in the project. Such entities contain structured information which constitute a valuable source of knowledge for analyzing and comprehending a system's design and evolution. However, the ambiguous and informal nature of narrative is a serious challenge in gathering such information, which is scattered throughout natural language text. We present an approach-based on island parsing-to recognize and enable the parsing of structured information that occur in natural language artifacts. We evaluate our approach by applying it to mailing lists pertaining to three software systems. We show that this approach allows us to extract structured data from emails with high precision and recall.
[Java, natural language processing, Natural languages, natural language document, data mining, Electronic mail, Grammar, Data mining, History, structured information, grammars, Production, island parsing]
GRoundTram: An integrated framework for developing well-behaved bidirectional model transformations
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Bidirectional model transformation is useful for maintaining consistency between two models, and has many potential applications in software development including model synchronization, round-trip engineering, and software evolution. Despite these attractive uses, the lack of a practical tool support for systematic development of well-behaved and efficient bidirectional model transformation prevents it from being widely used. In this paper, we solve this problem by proposing an integrated framework called GRoundTram, which is carefully designed and implemented for compositional development of well-behaved and efficient bidirectional model transformations. GRoundTram is built upon a well-founded bidirectional framework, and is equipped with a user-friendly language for coding bidirectional model transformation, a new tool for validating both models and bidirectional model transformations, an optimization mechanism for improving efficiency, and a powerful debugging environment for testing bidirectional behavior. GRoundTram has been used by people of other groups and their results show its usefulness in practice.
[round trip engineering, model synchronization, software development, Computational modeling, Unified modeling language, Debugging, bidirectional model transformation coding, Programming, user friendly language, software maintenance, consistency maintainance, optimization mechanism, software evolution, well behaved bidirectional model transformations, optimisation, GRoundTram, US Department of Transportation, Software engineering, Testing]
Run-time systems failure prediction via proactive monitoring
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In run-time evolving systems, components may evolve while the system is being operated. Unsafe run-time changes may compromise the correct execution of the entire system. Traditional design-time verification techniques difficultly cope with run-time changes, and run-time monitoring may detect disfunctions only too late, when the failure arises. The desire would be to define advanced monitors with the ability to predict and prevent the potential errors happening in the future. In this direction, this paper proposes CASSANDRA, a new approach that by combining design-time and run-time analysis techniques, can &#x201C;look ahead&#x201D; in the near execution future, and predict potential failures. During run-time we on-the-fly construct a model of the future k-step global state space according to design-time specifications and the current execution state. Consequently, we can run-time check whether possible failures might happen in the future.
[Software Evolution, program testing, program verification, run-time system failure prediction, Cassandra, Educational institutions, Synchronization, Component-based Software Engineering, design time verification technique, Component architectures, run-time analysis techniques, design time specifications, Failure Prediction, run-time monitoring, Automata, k-step global state space, Proactive run-time Monitoring, design time analysis technique, Software, Weaving, proactive monitoring, Monitoring]
Towards an approach and framework for test-execution plan derivation
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In industrial test and maintenance projects, test execution plans are important for performing test cycle in a time constrained manner with the objective, of delivering the expected quality by effective utilization of resources. To take advantage of the inherent parallelism in test suites, multiple resources are often deployed to test an application. The resource allocation is however driven more by costs and risks and does not exploit the parallelism. Test execution plans are often static in nature and are not well equipped to handle dynamically occurring events like abends, and changes in resource availability and test requirements. Derivation of test plans is a cumbersome activity, as it also needs to take into account test execution order, violation of which may result in unexpected failures. In this paper, we describe an approach to derive a test execution plan to facilitate parallel execution, given resource availability and test case dependencies. The execution plan provides workload distribution and scheduling of the test cases in a test suite. The case studies on test projects have shown that the derived test plans can contribute significantly towards improving the test execution cycles of the test suites.
[Job shop scheduling, program testing, Optimal scheduling, parallel execution, resource availability, maintenance projects, software maintenance, parallel processing, test suites, test execution plan derivation, industrial test projects, Processor scheduling, Databases, USA Councils, test requirements, Testing, test cycle]
Statistical debugging with elastic predicates
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Traditional debugging and fault localization methods have addressed localization of sources of software failures. While these methods are effective in general, they are not tailored to an important class of software, including simulations and computational models, which employ floating-point computations and continuous stochastic distributions to represent, or support evaluation of, an underlying model. To address this shortcoming, we introduce elastic predicates, a novel approach to predicate-based statistical debugging. Elastic predicates introduce profiling of values assigned to variables within a failing program. These elastic predicates are better predictors of software failure than the static and uniform predicates used in existing techniques such as Cooperative Bug Isolation (CBI). We present experimental results for established fault localization benchmarks and widely used simulations that show improved effectiveness.
[program debugging, Computational modeling, elastic predicates, fault localization, Optimized production technology, Stochastic processes, Debugging, automated debugging, floating point arithmetic, software fault tolerance, statistical debugging, floating point computations, Presses, failing program, Benchmark testing, software failures, cooperative bug isolation, Software, fault localization methods, continuous stochastic distributions]
Diagnosis of software failures using computational geometry
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Complex software systems have become commonplace in modern organizations and are considered critical to their daily operations. They are expected to run on a diverse set of platforms while interoperating with a wide variety of other applications. Although there have been advances in the discipline of software engineering, software faults, and malicious attacks still regularly cause system downtime [1]. Downtime of critical applications can create additional work, cause delays, and lead to financial loss [2]. This paper presents a computational geometry technique to tackle the problem of timely failure diagnosis during the execution of a software application. Our approach to failure diagnosis involves collecting a set of software metrics and building a geometric enclosures corresponding to known classes of faults. The geometric enclosures are then used to partition the state space defined by the metrics.
[Measurement, malicious attacks, computational geometry, Servers, software fault tolerance, software faults, Training, security of data, Training data, software failures, Software systems, failure diagnosis, software engineering, Monitoring, software metrics]
GitBAC: Flexible access control for non-modular concerns
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Today's techniques for controlling access to software artifacts are limited to restricting access to whole files and directories. But when a company's access control policy does not match a project's existing physical modularization, these techniques require either an all-or-nothing approach or re-modularization of the files and directories. The increased maintenance overhead this brings to project administration can lead to unimplemented or insufficient developer access control and an increased risk of insider security incidents (e.g., theft of intellectual property). We have created a tool (GitBAC) to provide access control of software artifacts using a crosscutting concern instead of artifact modularization. Our method provides fine-grained access control of artifacts and accommodates flexible access control policies.
[Access control, Conferences, artifact modularization, nonmodular concerns, fine-grained access control, development tools, Programming, flexible access control, Servers, software maintenance, company access control policy, project administration, authorisation, GitBAC, crosscutting concerns, Software, Monitoring, software artifact access control]
Client-side web application slicing
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Highly interactive web applications that offer user experience and responsiveness of standard desktop applications are becoming prevalent in the web application domain. However, with these benefits come certain drawbacks. For example, the event-based architectural style, and poor support for code organization, often lead to a situation where code responsible for a certain behavior is intermixed with irrelevant code. This makes development, debugging and reuse difficult. One way of locating code implementing a certain behavior is program slicing, a method that, given a subset of a program's behavior, reduces the program to a minimal form that still produces that behavior. In this paper we present a semi-automatic client-side web application slicing method, describe the web page dependency graph, and show how it can be used to extract only the code implementing a certain behavior.
[program debugging, program behavior, web application, Debugging, HTML, Browsers, code reuse, dynamic program slicing, web page dependency graph, Web pages, Fires, client side web application slicing, JavaScript, interactive web applications, Libraries, program slicing, Cascading style sheets, standard desktop applications]
Supporting activity based computing paradigm in global software development
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Global software development (GSD) teams have to use multiple tools to perform both complex and even simple tasks involving many context switches that can be frustrating. To lessen these issues, researchers are looking at providing new plug-ins whereas commercial vendors are flooding the market with comprehensive solutions often in the form of platforms. The current file- and application- oriented desktop metaphor can hardly support the collaborative and distributed nature of GSD teams. We assert that the Activity-Based Computing (ABC) paradigm has the potential for addressing the tool support related challenges of GSD. We have been incrementally designing and developing a flexible middleware (ABC4GSD) for supporting ABC in GSD. In this paper we present the theoretical foundations underpinning our approach and the architectural overview of a middleware for supporting ABC in GSD. Moreover, we briefly present a prototype leveraging the features provided by the middleware as a proof of concept.
[software vendors, Peer to peer computing, software prototyping, global software development team, software prototype, DP industry, software development management, Programming, file-oriented desktop metaphor, Servers, Middleware, team working, tool support, activity based computing paradigm, Collaboration, application-oriented desktop metaphor, software tools, ABC4GSD, software development tools, flexible middleware, middleware]
Inferred dependence coverage to support fault contextualization
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This paper provides techniques for aiding developers' task of familiarizing themselves with the context of a fault. Many fault-localization techniques present the software developer with a subset of the program to inspect in order to aid in the search for faults that cause failures. However, typically, these techniques do not describe how the components of the subset relate to each other in a way that enables the developer to understand how these components interact to cause failures. These techniques also do not describe how the subset relates to the rest of the program in a way that enables the developer to understand the context of the subset. In this paper, we present techniques for providing static and dynamic relations among program elements that can be used as the basis for the exploration of a program when attempting to understand the nature of faults.
[Context, program testing, Instruments, Conferences, program diagnostics, software failure, program subset, program elements, software fault tolerance, fault localization techniques, USA Councils, Software, software developer, Context modeling, Software engineering]
Using model-based assurance to strengthen diagnostic procedures
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In previous work we described Diagnostic Tree for Verification (DTV), a partially automated software engineering technique by which diagnostic trees generated from system models are used to help check out diagnostic procedures. Diagnostic procedures are instructions used to isolate failures during operations. Assuring such procedures manually is time-consuming and costly. This paper reports our recent experience in applying DTV to diagnostic procedures for lighting failures in NASA's Habitat Demonstration Unit (HDU), a prototype for astronauts' living quarters. DTV identified missing and inconsistent instructions, as well as more-efficient sequences of diagnostic steps. Unexpectedly, the most significant benefit was finding assumptions that will not remain true as the system evolves. We describe both the challenges faced in applying DTV and how its independent perspective helped in assuring the procedures' adequacy and quality. Finally, the paper discusses more generally how software systems that are model-based, rapidly evolving and safety-critical appear most likely to benefit from this approach.
[Adaptation models, program verification, NASA habitat demonstration unit, model-based, model-based assurance, software system, astronaut living quarter, safety-critical appear, Analytical models, partially automated software engineering technique, diagnostic procedures, astronomy computing, Lighting, diagnostic tree for verification, Computational modeling, program diagnostics, diagnostic procedure, NASA, trees (mathematics), automated analysis, diagnostic tree, lighting failure, Software, trouble-shooting, Digital TV]
Fault-localization using dynamic slicing and change impact analysis
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Spectrum-based fault-localization tools, such as Tarantula, have been developed to help guide developers towards faulty statements in a system under test. These tools report statements ranked in order of suspiciousness. Unfortunately, the reported statements can often be unrelated to the error. This paper evaluates the impact of several approaches to ignoring such unrelated statements in order to improve the effectiveness of fault-localization tools.
[Measurement, Schedules, spectrum based fault localization tools, Filtering, program testing, Inspection, faulty statements, software fault tolerance, USA Councils, Computational efficiency, program slicing, change impact analysis, dynamic slicing, Testing, Tarantula]
Improving source code search with natural language phrasal representations of method signatures
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
As software continues to grow, locating code for maintenance tasks becomes increasingly difficult. Software search tools help developers find source code relevant to their maintenance tasks. One major challenge to successful search tools is locating relevant code when the user's query contains words with multiple meanings or words that occur frequently throughout the program. Traditional search techniques, which treat each word individually, are unable to distinguish relevant and irrelevant methods under these conditions. In this paper, we present a novel search technique that uses information such as the position of the query word and its semantic role to calculate relevance. Our evaluation shows that this approach is more consistently effective than three other state of the art search techniques.
[query word, Head, source coding, Natural languages, source code search improvement, code search, software search tools, concern location, Information retrieval, software maintenance, Equations, maintenance tasks, query processing, Semantics, natural languages, Software, software tools, Mathematical model, natural language phrasal representations, method signatures, search techniques]
Deviation management during process execution
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software development companies have been putting a lot of effort in adopting process models, however two main issues remain. On the one hand, process models are inherently incomplete, since companies can not capture all possible situations in a single model. On the other hand, managers can not force process participants (agents) to strictly follow these models. The effect of both issues is that companies need to be able to handle deviations during process enactment. In order to make sure that process agents follow the process model and that their deviations get detected and handled, they adopt the so-called Process-centered Software Engineering Environments (PSEEs). Unfortunately, the options proposed by these tools, when it comes to handling a deviation, are rather limited to basically ignoring or forbidding it. In the present work, we face this limitation by presenting an approach for detecting, managing and tolerating agent deviations. Besides, in this paper we present the formal specification for this approach in the Linear Temporal Logic (LTL). It has been used as a the basis of our PSEE prototype.
[Computational modeling, Unified modeling language, Process control, software development management, Companies, temporal logic, linear temporal logic, process enactment, formal specification, process participants, software development companies, process models, Prototypes, deviation management, process execution, process centered software engineering environments, Software, agent deviations]
PRECIS: Inferring invariants using program path guided clustering
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
We propose PRECIS, a methodology for automatically generating invariants at function and loop boundaries through program path guided clustering. We instrument function inputs and outputs together with predicates for branch conditions and record their values during each execution. Program runs that share the same path are grouped together based on predicate words. For each group with sufficient data we use linear regression to express the output as a function of the inputs. Groups with insufficient data are examined as candidates for clustering with neighboring groups. Candidates that share the same output function are merged into a cluster. For each cluster, we write an invariant that summarizes the behavior of the corresponding set of paths. We evaluate our technique using Siemens benchmarks. When compared to Daikon, we find that our method has significant advantages.
[Context, automatic invariant generation, Instruments, program diagnostics, Linear regression, Siemens benchmarks, regression analysis, linear regression, Data mining, Indexes, PRECIS, program path guided clustering, pattern clustering, Benchmark testing]
Automated planning for feature model configuration based on stakeholders' business concerns
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In Software Product Line Engineering, concrete products of a family can be generated through a configuration process over a feature model. The configuration process selects features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from all the available features in the feature model is a cumbersome task because 1) the stakeholders may have diverse business concerns and limited resources that they can spend on a product and 2) features may have negative and positive contributions on different business concern. Many configurations techniques have been proposed to facilitate software developers' tasks through automated product derivation. However, most of the current proposals for automatic configuration are not devised to cope with business oriented requirements and stakeholders' resource limitations. We propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy the stakeholders' business concerns and resource limitations. We also provide tooling support to facilitate the use of our framework.
[software product line engineering, stakeholders business concerns, Configuration, software development management, Artificial Intelligence, Muscles, Educational institutions, Cognition, automatic configuration, artificial intelligence planning technique, planning (artificial intelligence), automated planning, configuration process, USA Councils, Feature Model, Software, Planning, feature model configuration, business data processing, Business]
An adaptive approach to impact analysis from change requests to source code
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
The paper presents an adaptive approach to perform impact analysis from a given change request (e.g., a bug report) to source code. Given a textual change request, a single snapshot (release) of source code, indexed using Latent Semantic Indexing, is used to estimate the impact set. Additionally, the approach configures the best-fit combination of information retrieval, dynamic analysis, and data mining of past source code commits to produce an improved impact set. The tandem operation of the three techniques sets it apart from other related solutions.
[indexing, program diagnostics, latent semantic indexing, data mining, information retrieval, source code, Maintenance engineering, dynamic analysis, History, Association rules, program compilers, impact set, Couplings, impact analysis, Software, textual change request, Indexing]
Domain and value checking of web application invocation arguments
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Invocations are widely used by many web applications, but have been found to be a common source of errors. This paper presents a new technique that can statically verify that an invocation's set of argument names, types, and request method match the constraints of a target interface. An empirical evaluation of the technique shows that it is successful at identifying previously unknown errors in web applications.
[Java, Crawlers, domain checking, HTML, Approximation methods, Equations, invocation set, formal verification, value checking, Web application invocation arguments, Web pages, Web sites, Software engineering]
Mixed constraints for test input generation - An initial exploration
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
The use of specifications provides an effective technique to automate testing. A form of specification that automates generation of test inputs is logical constraints that define properties of desired inputs. Recent advances in constraint solving technology have made the use of constraints particularly attractive. However, manually writing constraints to define complex inputs to real-world programs can pose a significant burden on the user and restrict their wider use. We envision a novel approach to facilitate the use of constraints: to provide a mixed notation for writing the properties. Our key insight is that different properties can lend to easier formulation using different programming paradigms. Thus, a notation that supports more than one paradigm, e.g., declarative and imperative paradigms, can enable achieving a sweet-spot in minimizing the manual effort required in constraint formulation. Moreover, solving such constraints is also likely to be more efficient as different properties may require different paradigms for more direct and accurate representation. This paper presents our vision and gives an illustration to make a case for the usefulness of such a notation.
[test input generation, Java, program testing, Metals, Programming, formal specification, specification usage, testing automation, constraint formulation, Vegetation, Writing, constraint solving technology, programming paradigms, constraint handling, Testing, Software engineering]
Enhancing architectural recovery using concerns
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Architectures of implemented software systems tend to drift and erode as they are maintained and evolved. To properly understand such systems, their architectures must be recovered from implementation-level artifacts. Many techniques for architectural recovery have been proposed, but their degrees of automation and accuracy remain unsatisfactory. To alleviate these shortcomings, we present a machine learning-based technique for recovering an architectural view containing a system's components and connectors. Our approach differs from other architectural recovery work in that we rely on recovered software concerns to help identify components and connectors. A concern is a software system's role, responsibility, concept, or purpose. We posit that, by recovering concerns, we can improve the correctness of recovered components, increase the automation of connector recovery, and provide more comprehensible representations of architectures.
[software architecture, architectural recovery enhancement, Sockets, Supervised learning, software systems, machine learning based technique, Computer architecture, Software, Libraries, learning (artificial intelligence), system recovery, Meteorology]
Search-based fault localization
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Many spectrum-based fault localization measures have been proposed in the literature. However, no single fault localization measure completely outperforms others: a measure which is more accurate in localizing some bugs in some programs is less accurate in localizing other bugs in other programs. This paper proposes to compose existing spectrum-based fault localization measures into an improved measure. We model the composition of various measures as an optimization problem and present a search-based approach to explore the space of many possible compositions and output a heuristically near optimal composite measure. We employ two search-based strategies including genetic algorithm and simulated annealing to look for optimal solutions and compare the effectiveness of the resulting composite measures on benchmark software systems. Compared to individual spectrum-based fault localization techniques, our composite measures perform statistically significantly better.
[search based approach, program debugging, optimization problem, simulated annealing, search based fault localization, spectrum based fault localization measures, Search problems, genetic algorithms, Biological cells, software fault tolerance, Genetic algorithms, Training, genetic algorithm, Accuracy, Computer bugs, Simulated annealing, search problems, program bugs]
Towards requirements aware systems: Run-time resolution of design-time assumptions
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In earlier work we proposed the idea of requirements-aware systems that could introspect about the extent to which their goals were being satisfied at runtime. When combined with requirements monitoring and self adaptive capabilities, requirements awareness should help optimize goal satisfaction even in the presence of changing run-time context. In this paper we describe initial progress towards the realization of requirements-aware systems with REAssuRE. REAssuRE focuses on explicit representation of assumptions made at design time. When such assumptions are shown not to hold, REAssuRE can trigger system adaptations to alternative goal realization strategies.
[Context, Adaptation models, Adaptive systems, Uncertainty, Computational modeling, Conferences, goal realization strategies, requirements awareness, requirements monitoring, Cognition, self adaptive capabilities, formal specification, self adaptive systems, design time assumptions, formal verification, requirements aware systems, run-time resolution, trigger system, claims, goals]
Generating essential user interface prototypes to validate requirements
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Requirements need to be validated at an early stage of analysis to address inconsistency and incompleteness issues. Capturing requirements usually involves natural language analysis, which is often imprecise and error prone, or translation into formal models, which are difficult for non-technical stakeholders to understand and use. Users often best understand proposed software systems from the likely user interface they will present. To this end we describe novel automated tool support for capturing requirements as Essential Use Cases and translating these into &#x201C;Essential User Interface&#x201D; low-fidelity rapid prototypes. We describe our automated tool supporting requirements capture, lo-fi user interface prototype generation and consistency management.
[inconsistency address, formal model translation, software prototyping, Unified modeling language, Natural languages, natural language interfaces, rapid prototyping, low fidelity rapid prototype, consistency management, software system, natural language analysis, formal verification, lo-fi user interface prototype generation, essential user interface, Prototypes, User interfaces, nontechnical stakeholder, Concrete, Libraries, requirements validation, Software engineering, automated tool support]
Automatically exploring how uncertainty impacts behavior of dynamically adaptive systems
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
A dynamically adaptive system (DAS) monitors itself and its execution environment to evaluate requirements satisfaction at run time. Unanticipated environmental conditions may produce sensory inputs that alter the self-assessment capabilities of a DAS in unpredictable and undesirable ways. Moreover, it is impossible for a human to know or enumerate all possible combinations of system and environmental conditions that a DAS may encounter throughout its lifetime. This paper introduces Loki, an approach for automatically discovering combinations of environmental conditions that produce requirements violations and latent behaviors in a DAS. By anticipating adverse environmental conditions that might arise at run time, Loki facilitates the identification of goals with inadequate obstacle mitigations or insufficient constraints to prevent such unwanted behaviors. We apply Loki to an autonomous vehicle system and describe several undesirable behaviors discovered.
[novelty search, Adaptation models, collision avoidance, Adaptive systems, dynamically adaptive systems, Unified modeling language, Noise, Evolutionary computation, mobile robots, goal-oriented requirements modeling, formal verification, road vehicles, execution environment, obstacle mitigations, LOKI approach, Monitoring, environmental uncertainty, self-assessment capabilities, requirements satisfaction evaluation, Noise measurement, adaptive systems, evolutionary algorithm, evolutionary computation, autonomous vehicle system, systems analysis, DAS]
iDiff: Interaction-based program differencing tool
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
When a software system evolves, its program entities such as classes/methods are also changed. System comprehension, maintenance, and other tasks require the detection of the changed entities between two versions. However, existing differencing tools are file-based and cannot handle well the common cases in which the methods/classes are reordered/moved or even renamed/modified. Moreover, many tools show the program changes at the text line level. In this demo, we present iDiff, a program differencing tool that is able to display the changes to classes/methods between two versions and to track the corresponding classes/methods even they were reordered/moved/renamed and/or modified. The key idea is that during software evolution, an entity could change its location, name, order, and even its internal implementation. However, its interaction with other entities would be more stable. iDiff represents a system at a version as an attributed graph, in which the nodes represent program entities, the edges represent the interactions between the nodes. Entities between two versions are matched via an incremental matching algorithm, which takes into account the similarity of interactions for matching. The differences of two versions of the entire system including its program entities are detected based on the matched entities.
[Algorithm design and analysis, Computers, software recovery, interaction based program differencing tool, Tracking, graph theory, iDiff, software maintenance, software system, software evolution, attributed graph, Semantics, Collaboration, Syntactics, Software]
CloneDifferentiator: Analyzing clones by differentiation
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Clone detection provides a scalable and efficient way to detect similar code fragments. But it offers limited explanation of differences of functions performed by clones and variations of control and data flows of clones. We refer to such differences as semantic differences of clones. Understanding these semantic differences is essential to correctly interpret cloning information and perform maintenance tasks on clones. Manual analysis of semantic differences of clones is complicated and error-prone. In the paper, we present our clone analysis tool, called Clone-Differentiator. Our tool automatically characterizes clones returned by a clone detector by differentiating Program Dependence Graphs (PDGs) of clones. CloneDifferentiator is able to provide a precise characterization of semantic differences of clones. It can provide an effective means of analyzing clones in a task oriented manner.
[Java, clone detection, program diagnostics, graph theory, Cloning, Maintenance engineering, clone analysis tool, Graph differencing, Clone analysis, code fragments, Program dependence graph, Matched filters, task oriented manner, Semantics, semantic differences, program dependence graphs, CloneDifferentiator, Software, Libraries]
Implementing efficient model validation in EMF tools
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Model-driven development tools built on industry standard platforms, such as the Eclipse Modeling Framework (EMF), heavily use model queries in various use cases, such as model transformation, well-formedness constraint validation and domain-specific model execution. As these queries are executed rather frequently in interactive modeling applications, they have a significant impact on the runtime performance of the tool, and also on the end user experience. However, due to their complexity, they can be time consuming to implement and optimize on a case-by-case basis. To address these shortcomings, we developed the EMF-INCQUERY framework for defining declarative queries over EMF models and executing them effectively using a caching mechanism. In the current paper, we demonstrate how our framework can be easily integrated with other EMF tools. We describe a case study in which EMF-INCQUERY is integrated into the open source Papyrus UML environment to provide on-the-fly validation of well-formedness criteria in UML models.
[Adaptation models, program verification, public domain software, Unified modeling language, model transformation, EMF tools, eclipse modeling framework, model-driven development, query processing, runtime performance, Runtime, model validation, incremental evaluation, EMF-INCQUERY framework, software tools, industry standard platforms, Monitoring, software performance evaluation, model queries, EMF, open source Papyrus UML environment, Unified Modeling Language, Computational modeling, model query, declarative queries, domain-specific model execution, Query processing, Pattern matching, constraint validation]
JPF-AWT: Model checking GUI applications
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Verification of Graphical User Interface (GUI) applications presents many challenges. GUI applications are open systems that are driven by user events. Verification of such applications by means of model checking therefore requires a user model in order to close the state space. In addition, GUIs rely extensively on complex and inherently concurrent framework libraries such as AWT/Swing, for which the application code merely provides callbacks. Software model checking of GUI applications therefore needs abstractions of such frameworks that faithfully preserve application behavior. This paper presents JPF-AWT, an extension of the Java PathFinder software model checker, which addresses these challenges. JPF-AWT has been successfully applied to a GUI front end of a NASA ground data system.
[Java, GUI front end, open systems, graphical user interfaces, state space, Data acquisition, concurrent framework library, model checking GUI applications, software model checking, NASA ground data system, JPF-AWT, Java PathFinder software model checker, formal verification, concurrency control, user events, Swing, Libraries, Software, Robots, Graphical user interfaces, Testing, state-space methods, graphical user interface applications]
The CORE system: Animation and functional correctness of pointer programs
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Pointers are a powerful and widely used programming mechanism, but developing and maintaining correct pointer programs is notoriously hard. Here we describe the CORE1 system, which supports the development of provably correct pointer programs. The CORE system combines data structure animation with functional correctness. While the animation component allows a programmer to explore and debug their algorithms, the functional correctness component provides a stronger guarantee via formal proof. CORE builds upon two external reasoning tools, i.e. the Smallfoot family of static analysers and the IsaPlanner proof planner. At the heart of the CORE functional correctness capability lies an integration of planning and term synthesis. The planning subsystem bridges the gap between shape analysis, provided by Smallfoot, and functional correctness. Key to this process is the generation of functional invariants, i.e. both loop and frame invariants. We use term synthesis, coupled with IsaPlanner's capability for reasoning about inductively defined structures, to automate the generation of functional invariants. The formal guarantees constructed by the CORE system take the form of proof tactics.
[program debugging, Shape, IsaPlanner proof planner, formal guarantees, Cognition, shape analysis, functional invariants, computer animation, planning (artificial intelligence), formal verification, CORE system, theorem proving, reasoning tools, program diagnostics, formal proof, Data structures, Educational institutions, data structure animation, Smallfoot, planning subsystem, static analysers, term synthesis, animation, Computer science, CORE functional correctness capability, automatic verification, separation logic, proof planning, Animation, Planning, pointer programs]
APIExample: An effective web search based usage example recommendation system for java APIs
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Programmers often learn how to use an API by studying its usage examples. There are many usage examples scattered in web pages on the Internet. However, it often takes programmers much effort to find out the desired examples from a large number of web pages by web search. This paper proposes a tool named APIExample that can extract usage examples for java APIs from web pages on the Internet and recommend them to programmers. Given a java API, the tool collects its related web pages from the Internet, extracts java code snippets and their surrounding descriptive texts embedded in the pages, then assembles them into usage examples for programmers. Furthermore, in order to help programmers capture more kinds of usages of the target API by browsing fewer examples, our tool clusters and ranks the listed examples based on the target API's usage. Besides, as a practical tool, APIExample provides multiple aspects of frequently-used information about using the target API in a concise user interface with friendly user experience. Two kinds of user-interaction style, a web search portal and an Eclipse plug-in, are now both publicly available.
[Java, search engines, application program interfaces, usage example, portals, Programming, Eclipse plug-in, Java API, friendly user experience, recommendation, user interfaces, example recommendation system, Data mining, web search, user interface, Web search portal, Web pages, Java code, API, Internet, Web search, Portals, tool clusters]
BEST: A symbolic testing tool for predicting multi-threaded program failures
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
We present a tool BEST (Binary instrumentation-based Error-directed Symbolic Testing) for predicting concurrency violations.1 We automatically infer potential concurrency violations such as atomicity violations from an observed run of a multi-threaded program, and use precise modeling and constraint-based symbolic (non-enumerative) search to find feasible violating schedules in a generalization of the observed run. We specifically focus on tool scalability by devising POR-based simplification steps to reduce the formula and the search space by several orders-of-magnitude. We have successfully applied the tool to several publicly available C/C++/Java programs and found several previously known/unknown concurrency related bugs. The tool also has extensive visual support for debugging.
[Context, Schedules, Java, program debugging, constraint-based symbolic search, multithreaded program failure prediction, multi-threading, program testing, Instruments, public domain software, C++ language, Synchronization, Concurrent computing, binary instrumentation based error-directed symbolic testing, Runtime, Computer bugs, concurrency control, POR-based simplification steps, publicly available C++ programs, debugging visual support, symbolic testing tool, BEST, concurrency related bugs, publicly available Java programs]
Decomposing feature models: language, environment, and applications
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Variability in software product lines is often expressed through feature models (FMs). To handle the complexity of increasingly larger FMs, we propose semantically meaningful decomposition support through a slicing operator. We describe how the slicing operator is integrated into the FAMILIAR environment and how it can be combined with other operators to support complex tasks over FMs in different case studies.
[Context, software product lines, feature models, Frequency modulation, Model Composition, Reverse engineering, FM, Slicing, Cognition, Feature Models, familiar environment, Software Product Lines, Semantics, slicing operator, product development, software reusability, Feature extraction, Software, program slicing, Separation of Concerns]
SAUML: A tool for symbolic analysis of UML-RT models
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Model Driven Development (MDD) is an approach to software development built around the notion of models. One of its implementation is the IBM RSA RTE, which uses the UML-RT modeling language. In this paper we introduce the tool SAUML (Symbolic Analysis of UML-RT Models) that enhances the current practice of MDD with the analyses of UML-RT models. The implemented technique is based on symbolic execution, features modularity and supports the reuse of analysis results. The paper gives an overview of this technique and its implementation in the IBM RSA RTE tool.
[Java, Unified Modeling Language, software development, Computational modeling, Heuristic algorithms, Unified modeling language, Educational institutions, symbolic analysis, Analytical models, UML-RT models, model driven development, IBM RSA RTE, SAUML, software reusability, symbol manipulation, Software, analysis result reuse]
TestEra: A tool for testing Java programs using alloy specifications
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This tool paper presents an embodiment of TestEra - a framework developed in previous work for specification-based testing of Java programs. To test a Java method, TestEra uses the method's pre-condition specification to generate test inputs and the post-condition to check correctness of outputs. TestEra supports specifications written in Alloy - a first-order, declarative language based on relations - and uses the SAT-based back-end of the Alloy tool-set for systematic generation of test suites. Each test case is a JUnit test method, which performs three key steps: (1) initialization of pre-state, i.e., creation of inputs to the method under test; (2) invocation of the method; and (3) checking the correctness of post-state, i.e., checking the method output. The tool supports visualization of inputs and outputs as object graphs for graphical illustration of method behavior. TestEra is available for download to be used as a library or as an Eclipse plug-in.
[specification-based testing, Java, program testing, Metals, declarative language, Alloy tool-set, Receivers, Eclipse plug-in, TestEra, Systematics, graphs, Java program testing, object graph, SAT-based backend, JUnit test method, alloy specification, specification languages, Libraries, graphical illustration, Testing, Software engineering]
MAJOR: An efficient and extensible tool for mutation analysis in a Java compiler
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Mutation analysis is an effective, yet often time-consuming and difficult-to-use method for the evaluation of testing strategies. In response to these and other challenges, this paper presents MAJOR, a fault seeding and mutation analysis tool that is integrated into the Java Standard Edition compiler as a non-invasive enhancement for use in any Java-based development environment. MAJOR reduces the mutant generation time and enables efficient mutation analysis. It has already been successfully applied to large applications with up to 373,000 lines of code and 406,000 mutants. Moreover, MAJOR's domain specific language for specifying and adapting mutation operators also makes it extensible. Due to its ease-of-use, efficiency, and extensibility, MAJOR is an ideal platform for the study and application of mutation analysis.
[mutation analysis tool, Java, Automation, MAJOR, fault seeding, program testing, program diagnostics, Educational institutions, formal specification, program compilers, software fault tolerance, Java Standard Edition compiler, Runtime, Memory management, mutation operators, testing strategy evaluation, Testing, mutant generation time reduction]
jCT: A Java Code Tomograph
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
We are concerned with analyzing software, in particular, with its nature and how developer decisions and behavior impact the quality of the product they produce. This is the domain of empirical software engineering where measurement seeks to capture attributes affecting the product, process, and resources of software development. A well-established means to study software attributes is metrics data mining. But, even though a variety of frameworks have emerged that can distill desired measures from software systems (e.g., JHawk or SonarJ), a systematic approach to collecting measures from large data sets has still eluded us. For this reason, we have developed the Java Code Tomograph (jCT), a novel framework for metrics extraction and processing. jCT offers an extensible measurement infrastructure with built-in support for the curated repositories Qualitas Corpus and Helix. With jCT, large-scale empirical studies of code within the same software system or across different software systems become feasible. In this paper, we provide an overview of jCT's main design features and discuss its operation in relation to the effectiveness of the framework.
[Java, Java code tomograph, program diagnostics, curated repositories, Qualitas Corpus, data mining, metrics extraction, Reasoning about programs, Data mining, Software metrics, software analysis, metrics data mining, Helix, Software systems, empirical software engineering, Software measurement, Software engineering, software metrics]
Generating realistic test models for model processing tools
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Test models are needed to evaluate and benchmark algorithms and tools in model driven development. Most model generators randomly apply graph operations on graph representations of models. This approach leads to test models of poor quality. Some approaches do not guarantee the basic syntactic correctness of the created models. Even if so, it is almost impossible to guarantee, or even control, the creation of complex structures, e.g. a subgraph which implements an association between two classes. Such a subgraph consists of an association node, two association end nodes, and several edges, and is normally created by one user command. This paper presents the SiDiff Model Generator, which can generate models, or sets of models, which are syntactically correct, contain complex structures, and exhibit defined statistical characteristics.
[Context, Adaptation models, complex structures, program testing, Biological system modeling, Computational modeling, Unified modeling language, graph theory, Tools for Model-based Development, Generators, Model-based Testing, generating realistic test models, model driven development, graph representations, processing tool model, syntactic correctness, Generating Synthetic Models, statistical characteristics, software engineering, statistical analysis, graph operations, SiDiff Model, benchmark algorithms, Context modeling]
Guided test visualization: Making sense of errors in concurrent programs
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
This paper describes a tool to help debug error traces found by the Java Pathfinder model checker in concurrent Java programs. It does this by abstracting out thread interactions and program locations that are not obviously pertinent to the error through control flow or data dependence. The tool then iteratively refines the abstraction by adding thread interactions at critical locations until the error is reachable. The tool visualizes the entire process and enables the user to systematically analyze each abstraction and execution. Such an approach explicitly identifies specific context switch locations and thread interactions needed to debug a concurrent error trace in small to moderate programs that can be managed by the Java Pathfinder Tool.
[Context, Java, Visualization, program debugging, debug, test, guided test visualization, Switches, thread interactions, concurrent Java programs, concurrency, Concurrent computing, Java Pathfinder model checker, formal verification, control flow, Data visualization, concurrency control, debug error, Model checking, Concrete, switch locations, program visualisation, data dependence]
The Capture Calculus Toolset
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Distributed embedded systems, such as multi-player smartphone games, training instrumentation systems, and &#x201C;smart&#x201D; homes, naturally have complex requirements. These are difficult to elicit, represent, validate, and verify. For high confidence, one demands that the represented requirements reflect realistic uses of the system; however, such uses, often representing human actions in complex environments, can have hundreds to thousands of steps and be impractical to elicit and manage using only declarative or intensional (computed) representations. Non-functional requirements like scalability increase this complexity further. In this paper, I show how one can bootstrap requirements using data captured from initial prototypes deployed in small scale real world tests. Using such captures as seeds, I show how a calculus of transformations on captures, from captures to scenarios, among scenarios, and from scenarios back to captures can be used in several requirements engineering tasks. I describe a novel ecosystem of tools and transformations that implement this capture calculus and illustrate its use on data obtained from the domain of multi-player outdoor smartphone games.
[requirement engineering, software prototyping, capture calculus toolset, Humans, distributed processing, transformation calculus, Calculus, scenario, geocast, Prototypes, computer games, embedded systems, small scale real world test, bootstrap requirement, training instrumentation system, requirements, Computational modeling, multiplayer outdoor smart phone game, testing, distributed embedded system, capture, smart phones, complex requirement, computer bootstrapping, Games, Animation, Clocks, functional requirement]
A model checking framework for hierarchical systems
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
BDD-based symbolic model checking is capable of verifying systems with a large number of states. In this work, we report an extensible framework to facilitate symbolic encoding and checking of hierarchical systems. Firstly, a novel library of symbolic encoding functions for compositional operators (e.g., parallel composition, sequential composition, choice operator, etc.) are developed so that users can apply symbolic model checking techniques to hierarchical systems with little knowledge of symbolic encoding techniques (like BDD or CUDD). Secondly, as the library is language-independent, we build an extensible framework with various symbolic model checking algorithms so that the library can be easily applied to encode and verify different modeling languages. Lastly, the applicability and scalability of our framework are demonstrated by applying the framework in the development of symbolic model checkers for three modeling languages as well as a comparison with the NuSMV model checker.
[choice operator, BDD, Hierarchical systems, symbolic encoding functions, NuSMV model checker, symbolic model checking techniques, compositional operators, software libraries, binary decision diagrams, Boolean functions, formal verification, parallel composition, Libraries, symbolic model checking algorithms, verifying systems, Computational modeling, Data structures, sequential composition, Encoding, symbolic encoding techniques, hierarchical systems, Synchronization, CUDD, modeling languages, language-independent library, simulation languages, symbolic checking, symbolic model checking framework]
Automatically detecting the quality of the query and its implications in IR-based concept location
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Concept location is an essential task during software maintenance and in particular program comprehension activities. One of the approaches to this task is based on leveraging the lexical information found in the source code by means of Information Retrieval techniques. All IR-based approaches to concept location are highly dependent on the queries written by the users. An IR approach, even though good on average, might fail when the input query is poor. Currently there is no way to tell when a query leads to poor results for IR-based concept location, unless a considerable effort is put into analyzing the results after the fact. We propose an approach based on recent advances in the field of IR research, which aims at automatically determining the difficulty a query poses to an IR-based concept location technique. We plan to evaluate several models and relate them to IR performance metrics.
[Measurement, information retrieval-based concept location, program comprehension, concept location, Correlation, query quality, Conferences, Estimation, source code, information retrieval, query, lexical information, software maintenance, query processing, search, Search engines, program comprehension activities, Prediction algorithms]
Using Formal Concept Analysis to support change analysis
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Software needs to be maintained and changed to cope with new requirement, existing faults and change requests as software evolves. One particular issue in software maintenance is how to deal with a change proposal before change implementation? Changes to software often cause unexpected ripple effects. To avoid this and alleviate the risk of performing undesirable changes, some predictive measurement should be conducted and a change scheme of the change proposal should be presented. This research intends to provide a unified framework for change analysis, which includes dependencies extraction, change impact analysis, changeability assessment, etc. We expect that our change analysis framework will contribute directly to the improvement of the accuracy of these predictive measures before change implementation, and thus provide more accurate change analysis results for software maintainers, improve quality of software evolution and reduce the software maintenance effort and cost.
[Measurement, Software maintenance, predictive measurement, software fault, Lattices, Maintenance engineering, software evolution quality, ripple effects, Proposals, software maintenance, formal concept analysis, changeability assessment, software fault tolerance, dependency extraction, formal verification, change impact analysis, Software engineering]
A framework for managing uncertainty in self-adaptive software systems
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Self-adaptation endows a software system with the ability to satisfy certain objectives by automatically modifying its behavior. While many promising approaches for the construction of self-adaptive software systems have been developed, the majority of them ignore the uncertainty underlying the adaptation decisions. This has been one of the key inhibitors to widespread adoption of self-adaption techniques in risk-averse real-world settings. In this research abstract I outline my ongoing effort in the development of a framework for managing uncertainty in self-adaptation. This framework employs state-of-the-art mathematical approaches to model and assess uncertainty in adaptation decisions. Preliminary results show that knowledge about uncertainty allows self-adaptive software systems to make better decisions.
[Analytical models, software architecture, Uncertainty, Runtime, uncertainty management framework, self-adaptive software systems, Software systems, self-adaptation, software engineering, uncertainty, self-adaption techniques, Robots]
Toward consistency checking of natural language temporal requirements
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
We discuss the problem of identifying inconsistencies in temporal requirements expressed as natural language text. We propose a partially automated approach that aims to minimize analysts' workload and improve accuracy. As one of the ingredients of the approach, we introduce a formal language to represent temporal requirements precisely and unambiguously. We call this language Temporal Action Language (TAL).
[text analysis, formal languages, natural language processing, Natural languages, Unified modeling language, Cognition, natural language text, Servers, consistency checking, temporal action language, formal verification, Semantics, Syntactics, natural language temporal requirements, Software, temporal requirement, TAL, formal language]
Analyzing temporal properties of abstract models
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Models are created and changed throughout the development process of software systems. The cost of repairing the errors that are due to mistakes in models is very high. In this research, we address this problem by developing model checking techniques that can be applied to abstract models that guide designers throughout the evolution of models and systems. Abstract models are declarative, expressed as a set of constraints, and this declarative aspect is the main challenge in model checking them. Our main idea for solving this problem is to express the model checking problem as a constraint solving problem. This approach enables designers to use current state-of-the-art constraint solvers for analysis. We have implemented this idea for Alloy models and we are further extending it for automatic model repairing. To achieve scalability, we have developed BDD-based methods for analysis of declarative models and we are studying model checking methods that are based on satisfiability modulo theories. We plan to extend these methods to infinite state space models.
[constraint solving problem, software development process, program verification, Computational modeling, software systems, Metals, computability, Data structures, BDD based methods, abstract models, software maintenance, scalability, Analytical models, model checking, Semantics, Software, infinite state space models, Mathematical model, automatic model repairing, satisfiability modulo theory]
Improving spectrum-based fault localization using proximity-based weighting of test cases
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
Spectrum based fault localization techniques such as Tarantula and Ochiai calculate the suspiciousness score of a program statement using the number of failing and passing test cases that execute the statement. These techniques implicitly assume that all test cases are equally important. However, research on test case generation and selection techniques has shown that using certain test cases can lead to more effective fault localization than others. The proposed research aims to improve the effectiveness of spectrum based fault localization by incorporating the relative importance of different test cases in the calculation of suspiciousness scores.
[Context, Weight measurement, program statement, program testing, spectrum-based fault localization, fault localization, Debugging, weighting test cases, Equations, software fault tolerance, Tarantula techniques, selection techniques, proximity of test cases, test case generation, proximity-based test case weighting, Benchmark testing, Ochiai techniques, Mathematical model, Software engineering]
Automatic assessment of software documentation quality
2011 26th IEEE/ACM International Conference on Automated Software Engineering
None
2011
In this paper I describe the concept of my dissertation, which aims at adapting the Continuous Quality Monitoring Method (CQMM) for the automated quality assessment of software documentation using a developed document quality analysis framework and software document quality rules which represent best practices for software documentation.
[document handling, Adaptation models, continuous quality monitoring method, tool-based approach, ISO standards, Documentation, Inspection, software quality, quality model, document quality analysis framework, Best practices, automatic software documentation quality assessment, software document quality rules, Software, continuous quality monitoring, software documentation, Software engineering]
The GISMOE challenge: constructing the pareto program surface using genetic programming to find better programs (keynote paper)
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Optimising programs for non-functional properties such as speed, size, throughput, power consumption and bandwidth can be demanding; pity the poor programmer who is asked to cater for them all at once! We set out an alternate vision for a new kind of software development environment inspired by recent results from Search Based Software Engineering (SBSE). Given an input program that satisfies the functional requirements, the proposed programming environment will automatically generate a set of candidate program implementations, all of which share functionality, but each of which differ in their non-functional trade offs. The software designer navigates this diverse Pareto surface of candidate implementations, gaining insight into the trade offs and selecting solutions for different platforms and environments, thereby stretching beyond the reach of current compiler technologies. Rather than having to focus on the details required to manage complex, inter-related and conflicting, non-functional trade offs, the designer is thus freed to explore, to understand, to control and to decide rather than to construct.
[GISMOE challenge, Pareto program surface, Pareto optimisation, genetic programming, bandwidth property, search based software engineering, Compilation, SBSE, power consumption property, Pareto Surface, genetic algorithms, program optimization, throughput property, genetic improvement of software for multiple objective exploration, software development environment, Non-functional Properties, software engineering, Search Based Optimization, Genetic Programming, speed property, size property]
Re-founding software engineering -- SEMAT at the age of three (keynote abstract)
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Software engineering is gravely hampered by immature practices. Specific problems include: The prevalence of fads more typical of the fashion industry than an engineering discipline; a huge number of methods and method variants, with differences little understood and artificially magnified; the lack of credible experimental evaluation and validation; and the split between industry practice and academic research. At the root of the problems we lack a sound, widely accepted theoretical basis. A prime example of such a basis is Maxwell's equations in electrical engineering. It is difficult to fathom what electrical engineering would be today without those four concise equations. They are a great example to the statement &#x201C;There is nothing so practical as a good theory&#x201D;. In software engineering we have nothing similar, and there is widespread doubt whether it is needed. This talk will argue for the need of a basic theory in software engineering, a theory identifying its pure essence, its common ground or its kernel. The Semat (Software Engineering Methods and Theory) community addresses this huge challenge. It supports a process to refound software engineering based on a kernel of widely-agreed elements, extensible for specific uses, addressing both technology and people issues. This kernel represents the essence of software engineering. This talk promises to make you see the light in the tunnel.
[practice, status of SEMAT, education, method, electrical engineering, Maxwell equations, Software Engineering Methods and Theory, fashion industry, software engineering theory, fad prevalence, alpha, software engineering, SEMAT, Kernel, activity space]
Practical isolation of failure-inducing changes for debugging regression faults
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
During software evolution, new released versions still contain many bugs. One common scenario is that end users encounter regression faults and submit them to bug tracking systems. Different from in-house regression testing, typically only one test input is available, which passes the old version and fails the modified new version. To address the issue, delta debugging has been proposed for failure-inducing changes identification between two versions. Despite promising results, there are two practical factors that thwart the application of delta debugging: a large number of tests and misleading false positives. In this work, we present a combination of coverage analysis and delta debugging that automatically isolates failure-inducing changes. Evaluations on twelve real regression faults in GNU software demonstrate both the speed gain and effectiveness improvements. Moreover, a case study on libPNG and TCPflow indicates that our technique is comparable to peer techniques in debugging regressions faults.
[field failure, program debugging, coverage analysis, libPNG, regression testing, regression analysis, automated debugging, speed gain, effectiveness improvement, software fault tolerance, software evolution, delta debugging, bug tracking system, Regression fault, regression fault debugging, TCPflow, failure-inducing change identification, GNU software]
Diversity maximization speedup for fault localization
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Fault localization is useful for reducing debugging effort. However, many fault localization techniques require non-trivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (DMS). DMS orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than 250 bugs from the Software-artifact Infrastructure Repository show (1) that DMS can help existing fault localization techniques to achieve comparable accuracy with on average 67% fewer labeled test cases than previously best test case prioritization techniques, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), DMS can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant.
[program debugging, test case prioritization techniques, program testing, debugging effort reduction, DMS, C language, diversity maximization speedup, Test Case Prioritization, software-artifact infrastructure repository, fault localization techniques, manual labeling, test case selection strategy, Fault Localization, C programs, test oracle creation]
Improving the effectiveness of spectra-based fault localization using specifications
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Fault localization i.e., locating faulty lines of code, is a key step in removing bugs and often requires substantial manual effort. Recent years have seen many automated localization techniques, specifically using the program's passing and failing test runs, i.e., test spectra. However, the effectiveness of these approaches is sensitive to factors such as the type and number of faults, and the quality of the test-suite. This paper presents a novel technique that applies spectra-based localization in synergy with specification-based analysis to more accurately locate faults. Our insight is that unsatisfiability analysis of violated specifications, enabled by SAT technology, could be used to (1) compute unsatisfiable cores that contain likely faulty statements and (2) generate tests that help spectra-based localization. Our technique is iterative and driven by a feedback loop that enables more precise fault localization. SAT-TAR is a framework that embodies our technique for Java programs, including those with multiple faults. An experimental evaluation using a suite of widely-studied data structure programs, including the ANTLR and JTopas parser applications, shows that our technique localizes faults more accurately than state-of-the-art approaches.
[Java, program debugging, Automated Debugging, program testing, data structure program, specification-based analysis, Kodkod, iterative technique, formal specification, Alloy, software fault tolerance, feedback loop, SAT-TAR framework, SAT technology, Minimal UNSAT cores, ANTLR parser, satisfiability, Fault Localization, Java program, test spectra technique, JTopas parser, spectra-based fault localization, bug removal, Tarantula]
To what extent could we detect field defects? an empirical study of false negatives in static bug finding tools
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Software defects can cause much loss. Static bug-finding tools are believed to help detect and remove defects. These tools are designed to find programming errors; but, do they in fact help prevent actual defects that occur in the field and reported by users? If these tools had been used, would they have detected these field defects, and generated warnings that would direct programmers to fix them? To answer these questions, we perform an empirical study that investigates the effectiveness of state-of-the-art static bug finding tools on hundreds of reported and fixed defects extracted from three open source programs: Lucene, Rhino, and AspectJ. Our study addresses the question: To what extent could field defects be found and detected by state-of-the-art static bug-finding tools? Different from past studies that are concerned with the numbers of false positives produced by such tools, we address an orthogonal issue on the numbers of false negatives. We find that although many field defects could be detected by static bug finding tools, a substantial proportion of defects could not be flagged. We also analyze the types of tool warnings that are more effective in finding field defects and characterize the types of missed defects.
[Static bug-finding tools, program debugging, field defect detection, field defects, AspectJ program, public domain software, software reliability, software defect, programming error, Lucene program, false negatives, open source program, false negative, Rhino program, static bug finding tool]
Diagnosys: automatic generation of a debugging interface to the Linux kernel
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
The Linux kernel does not export a stable, well-defined kernel interface, complicating the development of kernel-level services, such as device drivers and file systems. While there does exist a set of functions that are exported to external modules, this set of functions frequently changes, and the functions have implicit, ill-documented preconditions. No specific debugging support is provided. We present Diagnosys, an approach to automatically constructing a debugging interface for the Linux kernel. First, a designated kernel maintainer uses Diagnosys to identify constraints on the use of the exported functions. Based on this information, developers of kernel services can then use Diagnosys to generate a debugging interface specialized to their code. When a service including this interface is tested, it records information about potential problems. This information is preserved following a kernel crash or hang. Our experiments show that the generated debugging interface provides useful log information and incurs a low performance penalty.
[operating system kernels, program debugging, kernel maintainer, Linux kernel, Debugging, user interfaces, Diagnosys, external modules, Device drivers, log information, Wrappers, Linux, kernel crash, debugging interface automatic generation, kernel-level services]
Duplicate bug report detection with a combination of information retrieval and topic modeling
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Detecting duplicate bug reports helps reduce triaging efforts and save time for developers in fixing the same issues. Among several automated detection approaches, text-based information retrieval (IR) approaches have been shown to outperform others in term of both accuracy and time efficiency. However, those IR-based approaches do not detect well the duplicate reports on the same technical issues written in different descriptive terms. This paper introduces DBTM, a duplicate bug report detection approach that takes advantage of both IR-based features and topic-based features. DBTM models a bug report as a textual document describing certain technical issue(s), and models duplicate bug reports as the ones about the same technical issue(s). Trained with historical data including identified duplicate reports, it is able to learn the sets of different terms describing the same technical issues and to detect other not-yet-identified duplicate ones. Our empirical evaluation on real-world systems shows that DBTM improves the state-of-the-art approaches by up to 20% in accuracy.
[program debugging, text analysis, Duplicate Bug Reports, information retrieval, topic modeling, topic-based feature, duplicate bug report detection, IR-based feature, IR approach, text-based information retrieval, textual document, Information Retrieval, Topic Model, DBTM approach]
User-aware privacy control via extended static-information-flow analysis
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Applications in mobile-marketplaces may leak private user information without notification. Existing mobile platforms provide little information on how applications use private user data, making it difficult for experts to validate applications and for users to grant applications access to their private data. We propose a user-aware privacy control approach, which reveals how private information is used inside applications. We compute static information flows and classify them as safe/unsafe based on a tamper analysis that tracks whether private data is obscured before escaping through output channels. This flow information enables platforms to provide default settings that expose private data only for safe flows, thereby preserving privacy and minimizing decisions required from users. We built our approach into TouchDevelop, an application-creation environment that allows users to write scripts on mobile devices and install scripts published by other users. We evaluate our approach by studying 546 scripts published by 194 users.
[TouchDevelop environment, user script writing, information analysis, user interfaces, Information Flow Analysis, mobile marketplace, mobile platform, Mobile Application, mobile computing, Privacy Control, static-information-flow analysis, user-aware privacy control, data privacy, private user information, tamper analysis]
Automatic query performance assessment during the retrieval of software artifacts
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Text-based search and retrieval is used by developers in the context of many SE tasks, such as, concept location, traceability link retrieval, reuse, impact analysis, etc. Solutions for software text search range from regular expression matching to complex techniques using text retrieval. In all cases, the results of a search depend on the query formulated by the developer. A developer needs to run a query and look at the results before realizing that it needs reformulating. Our aim is to automatically assess the performance of a query before it is executed. We introduce an automatic query performance assessment approach for software artifact retrieval, which uses 21 measures from the field of text retrieval. We evaluate the approach in the context of concept location in source code. The evaluation shows that our approach is able to predict the performance of queries with 79% accuracy, using very little training data.
[concept location, text analysis, software text search, source code, text retrieval, text-based retrieval, query processing, regular expression matching, text-based search, automatic query performance assessment, software engineering, SE tasks, Query performance, software artifact retrieval]
Supporting automated vulnerability analysis using formalized vulnerability signatures
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Adopting publicly accessible platforms such as cloud computing model to host IT systems has become a leading trend. Although this helps to minimize cost and increase availability and reachability of applications, it has serious implications on applications' security. Hackers can easily exploit vulnerabilities in such publically accessible services. In addition to, 75% of the total reported application vulnerabilities are web application specific. Identifying such known vulnerabilities as well as newly discovered vulnerabilities is a key challenging security requirement. However, existing vulnerability analysis tools cover no more than 47% of the known vulnerabilities. We introduce a new solution that supports automated vulnerability analysis using formalized vulnerability signatures. Instead of depending on formal methods to locate vulnerability instances where analyzers have to be developed to locate specific vulnerabilities, our approach incorporates a formal vulnerability signature described using OCL. Using this formal signature, we perform program analysis of the target system to locate signature matches (i.e. signs of possible vulnerabilities). A newly-discovered vulnerability can be easily identified in a target program provided that a formal signature for it exists. We have developed a prototype static vulnerability analysis tool based on our formalized vulnerability signatures specification approach. We have validated our approach in capturing signatures of the OWSAP Top10 vulnerabilities and applied these signatures in analyzing a set of seven benchmark applications.
[formal signature, information technology, formal specification approach, formalized vulnerability signature, Formal vulnerability specification, formal specification, IT system, vulnerability instance, signature match, security of data, OWSAP Top10 vulnerability, security requirement, application security, Common weaknesses enumeration (CWE), Web application, Vulnerability analysis, Internet, automated vulnerability analysis, cloud computing, Software security]
A qualitative study on user guidance capabilities in product configuration tools
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Software systems are nowadays often configured by sales people, domain experts, or even customers instead of engineers. Configuration tools communicate the systems' variability to these end users and provide guidance for selecting and customizing the available features. However, even if a configuration tool creates technically correct systems, addressing the specific needs of business-oriented users remains challenging. We analyze existing configuration tools to identify key capabilities for guiding end users and discuss these capabilities using the cognitive dimensions of notations framework. We present an implementation of the capabilities in our configuration tool DOPLER CW. We performed a qualitative investigation on the usefulness of the tool's capabilities for user guidance in product configuration by involving nine business-oriented experts of two industry partners from the domain of industrial automation. We present key results and derive general implications for tool developers.
[user guidance capability, qualitative study, user interfaces, software system, user customization, end user guidance, product configuration tool, business-oriented user, DOPLER CW configuration tool, software engineering, user selection, cognitive dimension-of-notation framework, cognitive dimensions of notations, Configuration tools]
Structured merge with auto-tuning: balancing precision and performance
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Software-merging techniques face the challenge of finding a balance between precision and performance. In practice, developers use unstructured-merge (i.e., line-based) tools, which are fast but imprecise. In academia, many approaches incorporate information on the structure of the artifacts being merged. While this increases precision in conflict detection and resolution, it can induce severe performance penalties. Striving for a proper balance between precision and performance, we propose a structured-merge approach with auto-tuning. In a nutshell, we tune the merge process on-line by switching between unstructured and structured merge, depending on the presence of conflicts. We implemented a corresponding merge tool for Java, called JDime. Our experiments with 8 real-world Java projects, involving 72 merge scenarios with over 17 million lines of code, demonstrate that our approach indeed hits a sweet spot: While largely maintaining a precision that is superior to the one of unstructured merge, structured merge with auto-tuning is up to 12 times faster than purely structured merge, 5 times on average.
[Java, software-merging techniques, conflict resolution, Structured Merge, merging, conflict detection, JDime, structured-merge approach, unstructured-merge, Version Control, Software Merging, auto-tuning]
An automated approach to forecasting QoS attributes based on linear and non-linear time series modeling
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Predicting future values of Quality of Service (QoS) attributes can assist in the control of software intensive systems by preventing QoS violations before they happen. Currently, many approaches prefer Autoregressive Integrated Moving Average (ARIMA) models for this task, and assume the QoS attributes' behavior can be linearly modeled. However, the analysis of real QoS datasets shows that they are characterized by a highly dynamic and mostly nonlinear behavior to the extent that existing ARIMA models cannot guarantee accurate QoS forecasting, which can introduce crucial problems such as proactively triggering unrequired adaptations and thus leading to follow-up failures and increased costs. To address this limitation, we propose an automated forecasting approach that integrates linear and nonlinear time series models and automatically, without human intervention, selects and constructs the best suitable forecasting model to fit the QoS attributes' dynamic behavior. Using real-world QoS datasets of 800 web services we evaluate the applicability, accuracy, and performance aspects of the proposed approach, and results show that the approach outperforms the popular existing ARIMA models and improves the forecasting accuracy by on average 35.4%.
[autoregressive integrated moving average model, Automated QoS Forecasting, QoS violation prevention, autoregressive moving average processes, time series, quality of service, Runtime Adaptation, ARIMA model, Quality of Service (QoS), formal verification, Web services, time series modeling, model checking, Time Series Models, QoS attribute forecasting, software intensive system]
Using unfoldings in automated testing of multithreaded programs
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
In multithreaded programs both environment input data and the nondeterministic interleavings of concurrent events can affect the behavior of the program. One approach to systematically explore the nondeterminism caused by input data is dynamic symbolic execution. For testing multithreaded programs we present a new approach that combines dynamic symbolic execution with unfoldings, a method originally developed for Petri nets but also applied to many other models of concurrency. We provide an experimental comparison of our new approach with existing algorithms combining dynamic symbolic execution and partial-order reductions and show that the new algorithm can explore the reachable control states of each thread with a significantly smaller number of test runs. In some cases the reduction to the number of test runs can be even exponential allowing programs with long test executions or hard-to-solve constrains generated by symbolic execution to be tested more efficiently.
[multi-threading, program testing, program behavior, Petri nets, automated testing, unfoldings, test execution, Dynamic symbolic execution, nondeterministic interleaving, concurrency model, concurrency control, multithreaded program, dynamic symbolic execution, unfoldings method, partial-order reduction]
Runtime monitoring of software energy hotspots
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
GreenIT has emerged as a discipline concerned with the optimization of software solutions with regards to their energy consumption. In this domain, most of the state-of-the-art solutions concentrate on coarse-grained approaches to monitor the energy consumption of a device or a process. However, none of the existing solutions addresses in-process energy monitoring to provide in-depth analysis of a process energy consumption. In this paper, we therefore report on a fine-grained runtime energy monitoring framework we developed to help developers to diagnose energy hotspots with a better accuracy than the state-of-the-art. Concretely, our approach adopts a 2-layer architecture including OS-level and process-level energy monitoring. OS-level energy monitoring estimates the energy consumption of processes according to different hardware devices (CPU, network card). Process-level energy monitoring focuses on Java-based applications and builds on OS-level energy monitoring to provide an estimation of energy consumption at the granularity of classes and methods. We argue that this per-method analysis of energy consumption provides better insights to the application in order to identify potential energy hotspots. In particular, our preliminary validation demonstrates that we can monitor energy hotspots of Jetty web servers and monitor their variations under stress scenarios.
[Java, Jetty Web servers, Profiling, process-level energy monitoring, Power Model, Bytecode Instrumentation, coarse-grained approach, Power Monitoring, software energy hotspots, OS-level energy monitoring, Java-based applications, GreenIT, file servers, 2-layer architecture, software solution optimization, operating systems (computers), software engineering, green computing, process energy consumption, energy consumption, fine-grained runtime energy monitoring framework]
Predicting recurring crash stacks
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Software crash is one of the most severe bug manifestations and developers want to fix crash bugs quickly and efficiently. The Crash Reporting System (CRS) is widely deployed for this purpose. Even with the help of CRS, fixes are largely by manual effort, which is error-prone and results in recurring crashes even after the fixes. Our empirical study reveals that 48% of fixed crashes in Firefox CRS are recurring mostly due to incomplete or missing fixes. It is desirable to automatically check if a crash fix misses some reported crash traces at the time of the first fix. This paper proposes an automatic technique to predict recurring crash traces. We first extract stack traces and then compare them with bug fix locations to predict recurring crash traces. Evaluation using the real Firefox crash data shows that the approach yields reasonable accuracy in prediction of recurring crashes. Had our technique been deployed earlier, more than 2,225 crashes in Firefox 3.6 could have been avoided.
[crash bugs, program debugging, stack trace extraction, Firefox crash data, Crash, software crash, crash reporting system, Firefox CRS, bug, recurring crash stack prediction, bug fix locations, bug manifestations]
Automated inference of goal-oriented performance prediction functions
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Understanding the dependency between performance metrics (such as response time) and software configuration or usage parameters is crucial in improving software quality. However, the size of most modern systems makes it nearly impossible to provide a complete performance model. Hence, we focus on scenario-specific problems where software engineers require practical and efficient approaches to draw conclusions, and we propose an automated, measurement-based model inference method to derive goal-oriented performance prediction functions. For the practicability of the approach it is essential to derive functional dependencies with the least possible amount of data. In this paper, we present different strategies for automated improvement of the prediction model through an adaptive selection of new measurement points based on the accuracy of the prediction model. In order to derive the prediction models, we apply and compare different statistical methods. Finally, we evaluate the different combinations based on case studies using SAP and SPEC benchmarks.
[measurement point adaptive selection, Model Inference, software configuration, usage parameters, functional dependency, measurement-based model inference method, software quality, SPEC benchmark, configuration management, SAP benchmark, performance metrics, automated inference, scenario-specific problems, Performance Prediction, goal-oriented performance prediction functions, statistical analysis, statistical methods]
Code patterns for automatically validating requirements-to-code traces
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Traces between requirements and code reveal where requirements are implemented. Such traces are essential for code understanding and change management. Unfortunately, traces are known to be error prone. This paper introduces a novel approach for validating requirements-to-code traces through calling relationships within the code. As input, the approach requires an executable system, the corresponding requirements, and the requirements-to-code traces that need validating. As output, the approach identifies likely incorrect or missing traces by investigating patterns of traces with calling relationships. The empirical evaluation of four case study systems covering 150 KLOC and 59 requirements demonstrates that the approach detects most errors with 85-95% precision and 82-96% recall and is able to handle traces of varying levels of correctness and completeness. The approach is fully automated, tool supported, and scalable.
[requirements-to-code trace automatic validation, code patterns, code understanding, requirements, program verification, calling relationships, reverse engineering, feature location, traceability, change management, validation]
Unbounded data model verification using SMT solvers
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
The growing influence of web applications in every aspect of society makes their dependability an immense concern. A fundamental building block of web applications that use the Model-View-Controller (MVC) pattern is the data model, which specifies the object classes and the relations among them. We present an approach for unbounded, automated verification of data models that 1) extracts a formal data model from an Object Relational Mapping, 2) converts verification queries about the data model to queries about the satisfiability of formulas in the theory of uninterpreted functions, and 3) uses a Satisfiability Modulo Theories (SMT) solver to check the satisfiability of the resulting formulas. We implemented this approach and applied it to five open-source Rails applications. Our results demonstrate that the proposed approach is feasible, and is more efficient than SAT-based bounded verification.
[data model, program verification, Unbounded verification, object relational mapping, MVC frameworks, computability, Web applications, verification query conversion, open-source rails applications, SMT solvers, software architecture, unbounded data model verification, formula satisfiability, object class, formal data model, MVC pattern, Internet, Satisfiability Modulo Theories solver, model-view-controller pattern, SAT-based bounded verification]
Computing repair trees for resolving inconsistencies in design models
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Resolving inconsistencies in software models is a complex task because the number of repairs grows exponentially. Existing approaches thus emphasize on selected repairs only but doing so diminishes their usefulness. This paper copes with the large number of repairs by focusing on what caused an inconsistency and presenting repairs as a linearly growing repair tree. The cause is computed by examining the run-time evaluation of the inconsistency to understand where and why it failed. The individual changes that make up repairs are then modeled in a repair tree as alternatives and sequences reflecting the syntactic structure of the inconsistent design rule. The approach is automated and tool supported. Its scalability was empirically evaluated on 29 UML models and 18 OCL design rules where we show that the approach computes repair trees in milliseconds on average. We believe that the approach is applicable to arbitrary modeling and constraint languages.
[Unified Modeling Language, trees (mathematics), repair trees computation, inconsistency resolving, OCL design rules, Inconsistency Management, design models, UML models, design rule syntactic structure, constraint languages, software models, inconsistency run-time evaluation, arbitrary modeling, software engineering, Repairing Inconsistencies]
Supporting automated software re-engineering using re-aspects
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
System maintenance, including omitting an existing system feature e.g. buggy or vulnerable code, or modifying existing features, e.g. replacing them, is still very challenging. To address this problem we introduce the &#x201C;re-aspect&#x201D; (re-engineering aspect), inspired from traditional AOP. A re-aspect captures system modification details including signatures of entities to be updated; actions to apply including remove, modify, replace, or inject new code; and code to apply. Re-aspects locate entities to update, entities that will be impacted by the given update, and finally propagate changes on the system source code. We have applied our re-aspects technique to the security re-engineering problem and evaluated it on a set of open source .NET applications to demonstrate its usefulness.
[Software Evolution, AOP, Re-Aspects, code injection, software re-aspect, open source application, systems re-engineering, system maintenance, .NET application, automated software re-engineering, System Reengineering, code modification, Change Impact Analysis, re-engineering aspect, aspect-oriented programming, code replacement, code removal]
Supporting operating system kernel data disambiguation using points-to analysis
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Generic pointers scattered around operating system (OS) kernels make the kernel data layout ambiguous. This limits current kernel integrity checking research to covering a small fraction of kernel data. Hence, there is a great need to obtain an accurate kernel data definition that resolves generic pointer ambiguities, in order to formulate a set of constraints between structures to support precise integrity checking. In this paper, we present KDD, a new tool for systematically generating a sound kernel data definition for any C-based OS e.g. Windows and Linux, without any prior knowledge of the kernel data layout. KDD performs static points-to analysis on the kernel's source code to infer the appropriate candidate types for generic pointers. We implemented a prototype of KDD and evaluated it to prove its scalability and effectiveness.
[checkpointing, operating system kernels, data disambiguation, data analysis, operating system kernel, generic pointer, Windows, KDD tool, Systematic kernel data integrity checking, C-based OS, Linux, points-to analysis, kernel integrity checking research, OS kernel, pointer ambiguity]
Can I clone this piece of code here?
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
While code cloning is a convenient way for developers to reuse existing code, it may potentially lead to negative impacts, such as degrading code quality or increasing maintenance costs. Actually, some cloned code pieces are viewed as harmless since they evolve independently, while some other cloned code pieces are viewed as harmful since they need to be changed consistently, thus incurring extra maintenance costs. Recent studies demonstrate that neither the percentage of harmful code clones nor that of harmless code clones is negligible. To assist developers in leveraging the benefits of harmless code cloning and/or in avoiding the negative impacts of harmful code cloning, we propose a novel approach that automatically predicts the harmfulness of a code cloning operation at the point of performing copy-and-paste. Our insight is that the potential harmfulness of a code cloning operation may relate to some characteristics of the code to be cloned and the characteristics of its context. Based on a number of features extracted from the cloned code and the context of the code cloning operation, we use Bayesian Networks, a machine-learning technique, to predict the harmfulness of an intended code cloning operation. We evaluated our approach on two large-scale industrial software projects under two usage scenarios: 1) approving only cloning operations predicted to be very likely of no harm, and 2) blocking only cloning operations predicted to be very likely of harm. In the first scenario, our approach is able to approve more than 50% cloning operations with a precision higher than 94.9% in both subjects. In the second scenario, our approach is able to avoid more than 48% of the harmful cloning operations by blocking only 15% of the cloning operations for the first subject, and avoid more than 67% of the cloning operations by blocking only 34% of the cloning operations for the second subject.
[machine learning technique, software reuse, Code cloning, harmless code cloning, cloning operation, large-scale industrial software project, code quality degradation, copy-and-paste operation, maintenance cost, Harmfulness prediction, software reusability, harmful code cloning, Programming aid, Bayesian networks, belief networks]
Automatic recovery of statecharts from procedural code
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
We have developed a static-analysis algorithm that extracts statecharts from procedural implementations of state machines. The extracted statecharts are semantically-equivalent to the original program, and can be used for further development instead of the procedural code. We have implemented this algorithm in a tool called StatRec. We report on the results of running StatRec on a number of examples, including an implementation of the TCP protocol.
[statecharts, Java, transport control protocol, program diagnostics, procedural code, TCP protocol, static analysis, C language, finite state machines, State machines, statechart extraction, StatRec tool, automatic statechart recovery, static-analysis algorithm, state machine]
Locating distinguishing features using diff sets
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
In this paper, we focus on the problem of feature location for families of related software products realized via code cloning. Locating code that corresponds to features in such families is an important task in many software development activities, such as support for sharing features between different products of the family or refactoring the code into product line representations that eliminate duplications and facilitate reuse. We suggest two heuristics for improving the accuracy of existing feature location techniques when locating distinguishing features - those that are present in one product variant while absent in another. Our heuristics are based on identifying code regions that have a high potential to implement a feature of interest. We refer to these regions as diff sets and compute them by comparing product variants to each other. We exemplify our approach on a small but realistic example and describe initial evaluation results.
[feature location techniques, software development activities, software products, code refactoring, product variants, feature location, Software product lines, software maintenance, product line representations, code cloning, diff sets, software reusability, code region identification]
Slicing and replaying code change history
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Change-aware development environments have recently become feasible and reasonable. These environments can automatically record fine-grained code changes on a program and allow programmers to replay the recorded changes in chronological order. However, they do not always need to replay all the code changes to investigate how a particular entity of the program has been changed. Therefore, they often skip several code changes of no interest. This skipping action is an obstacle that makes many programmers hesitate in using existing replaying tools. This paper proposes a slicing mechanism that can extract only code changes necessary to construct a particular class member of a Java program from the whole history of past code changes. In this mechanism, fine-grained code changes are represented by edit operations recorded on source code of a program. The paper also presents a running tool that implements the proposed slicing and replays its resulting slices. With this tool, programmers can avoid replaying edit operations nonessential to the construction of class members they want to understand.
[Code change, Java, Program comprehension, edit operation, running tool, Software maintenance and evolution, code change history, change-aware development environment, software maintenance, slicing mechanism, Integrated development environments, Program slicing, fine-grained code change, Java program, skipping action, program source code]
Generating model transformation rules from examples using an evolutionary algorithm
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
We propose an evolutionary approach to automatically generate model transformation rules from a set of examples. To this end, genetic programming is adapted to the problem of model transformation in the presence of complex input/output relationships (i.e., models conforming to meta-models) by generating declarative programs (i.e., transformation rules in this case). Our approach does not rely on prior transformation traces for the model-example pairs, and directly generates executable, many-to-many rules with complex conditions. The applicability of the approach is illustrated with the well-known problem of transforming UML class diagrams into relational schemas, using examples collected from the literature.
[genetic programming, Model transformation by example, Unified Modeling Language, input-output relationship, genetic algorithms, model-example pair, declarative program generation, evolutionary algorithm, relational schema, UML class diagram, model transformation rule, software engineering, many-to-many rule, transformation trace]
Augmented dynamic symbolic execution
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Dynamic symbolic execution (DSE) can efficiently explore all simple paths through a program, reliably determining whether there are any program crashes or violations of assertions or code contracts. However, if such automated oracles do not exist, the traditional approach is to present the developer a small and representative set of tests in order to let him/her determine their correctness. Customer feedback on Microsoft's Pex tool revealed that users expect different values and also more values than those produced by Pex, which threatens the applicability of DSE in a scenario without automated oracles. Indeed, even though all paths might be covered by DSE, the resulting tests are usually not sensitive enough to make a good regression test suite. In this paper, we present augmented dynamic symbolic execution, which aims to produce representative test sets by augmenting path conditions with additional conditions that enforce target criteria such as boundary or mutation adequacy, or logical coverage criteria.
[code contracts, program testing, boundary values, augmented dynamic symbolic execution, mutation testing, Test generation, program crash, logical coverage criteria, mutation adequacy, assertion violation, DSE, boundary adequacy, representative test sets, Microsoft Pex tool, dynamic symbolic execution, path conditions]
Using GUI ripping for automated testing of Android applications
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
We present AndroidRipper, an automated technique that tests Android apps via their Graphical User Interface (GUI). AndroidRipper is based on a user-interface driven ripper that automatically explores the app's GUI with the aim of exercising the application in a structured manner. We evaluate AndroidRipper on an open-source Android app. Our results show that our GUI-based test cases are able to detect severe, previously unknown, faults in the underlying code, and the structured exploration outperforms a random approach.
[program testing, graphical user interfaces, public domain software, graphical user interface, user-interface driven ripper, automated testing, GUI ripping, Android, open-source Android, Testing Tools, mobile computing, Testing Automation, operating systems (computers), AndroidRipper technique, Android application]
kbe-anonymity: test data anonymization for evolving programs
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
High-quality test data that is useful for effective testing is often available on users' site. However, sharing data owned by users with software vendors may raise privacy concerns. Techniques are needed to enable data sharing among data owners and the vendors without leaking data privacy. Evolving programs bring additional challenges because data may be shared multiple times for every version of a program. When multiple versions of the data are cross-referenced, private information could be inferred. Although there are studies addressing the privacy issue of data sharing for testing and debugging, little work has explicitly addressed the challenges when programs evolve. In this paper, we examine kb-anonymity that is recently proposed for anonymizing data for a single version of a program, and identify a potential privacy risk if it is repeatedly applied for evolving programs. We propose kbe-anonymity to address the insufficiencies of kb-anonymity and evaluate our model on three Java programs. We demonstrate that kbe -anonymity can successfully address the potential risk of kb-anonymity, maintain sufficient path coverage for testing, and be as efficient as kb-anonymity.
[Java, program debugging, evolving program, program testing, test data anonymization, privacy preservation, software vendor, private information, testing and debugging, privacy risk, behavior preservation, kbe-anonymity, privacy concern, data privacy, Java programs, data sharing, k-anonymity]
Selection of regression system tests for security policy evolution
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
As security requirements of software often change, developers may modify security policies such as access control policies (policies in short) according to evolving requirements. To increase confidence that the modification of policies is correct, developers conduct regression testing. However, rerunning all of existing system test cases could be costly and time-consuming. To address this issue, we develop a regression-test-selection approach, which selects every system test case that may reveal regression faults caused by policy changes. Our evaluation results show that our test-selection approach reduces a substantial number of system test cases efficiently.
[policy modification, Test Selection, Regression Testing, regression-test-selection approach, security policy evolution, regression analysis, access control policy, regression fault, Security Policy, authorisation, software security requirement, regression system test, statistical testing]
Fast and precise points-to analysis with incremental CFL-reachability summarisation: preliminary experience
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
We describe our preliminary experience in the design and implementation of a points-to analysis for Java, called EMU, that enables developers to perform pointer-related queries in programs undergoing constant changes in IDEs. EMU achieves fast response times by adopting a modular approach to incrementally updating method summaries upon small code changes: the points-to information in a method is summarised indirectly by CFL reachability rather than directly by points-to sets. Thus, the impact of a small code change made in a method is localised, requiring only its affected part to be re-summarised just to reflect the change. EMU achieves precision by being context-sensitive (for both method invocation and heap abstraction) and field-sensitive. Our evaluation shows that EMU can be promisingly deployed in IDEs where the changes are small.
[Java, reachability analysis, context free-language, program diagnostics, incremental CFL-reachability summarisation, Summarisation, IDE, EMU, Points-to analysis, context-free languages, pointer-related queries, points-to analysis, heap abstraction, modular approach, method invocation, CFL reachability]
Automatically securing permission-based software by reducing the attack surface: an application to Android
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
In the permission-based security model (used e.g. in Android and Blackberry), applications can be granted more permissions than they actually need, what we call a &#x201C;permission gap&#x201D;. Malware can leverage the unused permissions for achieving their malicious goals, for instance using code injection. In this paper, we present an approach to detecting permission gaps using static analysis. Using our tool on a dataset of Android applications, we found out that a non negligible part of applications suffers from permission gaps, i.e. does not use all the permissions they declare.
[call-graph, invasive software, malware, permission-based software, program diagnostics, Soot, code injection, static analysis, attack surface reduction, permission-based software security, Android, security, permission gap, Android applications, Permissions, operating systems (computers)]
Support vector machines for anti-pattern detection
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Developers may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and--or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns in a whole software system may be infeasible because of the required parsing time and of the subsequent needed manual validation. Detecting anti-patterns on subsets of a system could reduce costs, effort, and resources. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently some limitations: they require extensive knowledge of anti-patterns, they have limited precision and recall, and they cannot be applied on subsets of systems. To overcome these limitations, we introduce SVMDetect, a novel approach to detect anti-patterns, based on a machine learning technique---support vector machines. Indeed, through an empirical study involving three subject systems and four anti-patterns, we showed that the accuracy of SVMDetect is greater than of DETEX when detecting anti-patterns occurrences on a set of classes. Concerning, the whole system, SVMDetect is able to find more anti-patterns occurrences than DETEX.
[program comprehension, antipattern detection, manual validation, support vector machines, machine learning technique, Anti-pattern, software systems, source code, parsing time, software maintenance, SVMDetect, program maintenance, DETEX, empirical software engineering, learning (artificial intelligence)]
Detection of embedded code smells in dynamic web applications
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
In dynamic Web applications, there often exists a type of code smells, called embedded code smells, that violate important principles in software development such as software modularity and separation of concerns, resulting in much maintenance effort. Detecting and fixing those code smells is crucial yet challenging since the code with smells is embedded and generated from the server-side code. We introduce WebScent, a tool to detect such embedded code smells. WebScent first detects the smells in the generated code, and then locates them in the server-side code using the mapping between client-side code fragments and their embedding locations in the server program, which is captured during the generation of those fragments. Our empirical evaluation on real-world Web applications shows that 34%-81% of the tested server files contain embedded code smells. We also found that the source files with more embedded code smells are likely to have more defects and scattered changes, thus potentially require more maintenance effort.
[dynamic Web applications, WebScent, software development, Code Smells, server-side code, maintenance effort, embedded code smell detection, Dynamic Web Applications, Internet, Embedded Code, software maintenance, client-side code fragments]
Boreas: an accurate and scalable token-based approach to code clone detection
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Detecting code clones in a program has many applications in software engineering and other related fields. In this paper, we present Boreas, an accurate and scalable token-based approach for code clone detection. Boreas introduces a novel counting-based method to define the characteristic matrices, which are able to describe the program segments distinctly and effectively for the purpose of clone detection. We conducted experiments on JDK 7 and Linux kernel 2.6.38.6 source code. Experimental results show that Boreas is able to match the detecting accuracy of a recently proposed syntactic-based tool Deckard, with the execution time reduced by more than an order of magnitude.
[count matrix, Java, program debugging, program testing, Boreas, characteristic matrices, scalable token-based approach, counting-based method, execution time reduction, count vector, matrix algebra, JDK 7 source code, Linux, Linux kernel 2.6.38.6 source code, Code clone detection, Deckard syntactic-based tool, software engineering, program code clone detection]
Refactorings without names
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
As with design patterns before, the naming and cataloguing of refactorings has contributed significantly to the recognition of the discipline. However, in practice concrete refactoring needs may deviate from what has been distilled as a named refactoring, and mapping these needs to a series of such refactorings - if at all possible - can be difficult. To address this, we propose a framework of specifying refactorings in an ad hoc fashion, and demonstrate its feasibility by presenting an implementation. Evaluation is done by simulating application through a user on a set of given sample programs. Results suggest that our proposal of ad hoc refactoring is, for the investigated scenarios at least, viable.
[ad hoc refactoring, refactoring cataloguing, program restructuring, refactoring specification, software maintenance, formal specification, refactoring naming, Refactoring]
Automated API migration in a user-extensible refactoring tool for Erlang programs
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Wrangler is a refactoring and code inspection tool for Erlang programs. Apart from providing a set of built-in refactorings and code inspection functionalities, Wrangler allows users to define refactorings, code inspections, and general program transformations for themselves to suit their particular needs. These are defined using a template- and rule-based program transformation and analysis framework built into Wrangler. This paper reports an extension to Wrangler's extension framework, supporting the automatic generation of API migration refactorings from a user-defined adapter module.
[template, code inspection tool, application program interfaces, rewrite rule, program diagnostics, user-extensible refactoring tool, Erlang programs, automated API migration refactoring, Wrangler, API migration, programming languages, software maintenance, Erlang, general program transformations, template-and rule-based program analysis framework, template-and rule-based program transformation, refactoring, knowledge based systems, software engineering, user-defined adapter module]
Using mobile devices for collaborative requirements engineering
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
In requirements engineering, CRC modeling and use case analysis are established techniques and are often performed as a group work activity. In particular, role play is used to involve different stakeholders into the use case analysis. To support this kind of co-located collaboration we developed CREW-Space, which allows several users to simultaneously interact through Android-enabled mobile devices with the same model displayed on a shared screen. Furthermore, it keeps track of the current state of the role play and, in addition, each mobile device serves as a private workspace; it actually turns into a tangible digital CRC card.
[Android-enabled mobile device, group work activity, CRC modeling, CREW-Space software, formal specification, mobile device, use case analysis, mobile computing, formal verification, collaborative requirements engineering, groupware, operating systems (computers), Requirements engineering, computer-supported collaborative work, tools and environments]
Automatically generating and adapting model constraints to support co-evolution of design models
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Design models must abide by constraints that can come from diverse sources, like their metamodels, requirements, or the problem domain. Software modelers expect these constraints to be enforced on their models and receive instant error feedback if they fail. This works well when constraints are stable. However, constraints may evolve much like their models do. This evolution demands efficient constraint adaptation mechanisms to ensure that models are always validated against the correct constraints. In this paper, we present an idea based on constraint templates that tackles this evolution scenario by automatically generating and updating constraints.
[model constraints, constraint adaptation mechanisms, metamodeling, constraint templates, error feedback, Co-evolution, software maintenance, design model coevolution, software modelers, consistency checking]
Adaptability of model comparison tools
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Modern model-based development methodologies require a large number of efficient, high-quality model comparison tools. They must be carefully adapted to the specific model type, user preferences and application context. Implementing a large number of dedicated, monolithic tools is infeasible, the only viable approach are generic, adaptable tools. Generic tools currently available provide only partial or low-quality solutions to this challenge; their results are not satisfactory for model types such as state machines or block diagrams. This paper presents the SiDiff approach to model comparison which includes a set of highly configurable incremental matchers and a specification language to control their application.
[model-based development methodology, model comparsion, model configuration management, user preferences, incremental matchers, Model versioning, model matching, specific model type, application context, specification languages, SiDiff approach, model comparison tool adaptability, software engineering, specification language]
Predicting common web application vulnerabilities from input validation and sanitization code patterns
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Software defect prediction studies have shown that defect predictors built from static code attributes are useful and effective. On the other hand, to mitigate the threats posed by common web application vulnerabilities, many vulnerability detection approaches have been proposed. However, finding alternative solutions to address these risks remains an important research problem. As web applications generally adopt input validation and sanitization routines to prevent web security risks, in this paper, we propose a set of static code attributes that represent the characteristics of these routines for predicting the two most common web application vulnerabilities-SQL injection and cross site scripting. In our experiments, vulnerability predictors built from the proposed attributes detected more than 80% of the vulnerabilities in the test subjects at low false alarm rates.
[sanitization code pattern, static code attributes, cross site scripting vulnerability, web application vulnerabilities, input validation routing, sanitization routine, vulnerability detection approach, empirical study, Web application vulnerability prediction, Web security risk, Defect prediction, SQL injection vulnerability, false alarm rate, security of data, input validation and sanitization, Internet, static code attribute, software defect prediction]
Software defect prediction using semi-supervised learning with dimension reduction
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.
[semisupervised learning, software fault prediction, model training, fault prone module detection, software quality, random forest, dimension reduction, software modules availability, high quality software products, preprocessing strategy, multidimensional scaling, semi-supervised learning, learning (artificial intelligence), software metrics dimensional complexity, software defect prediction, software metrics, fault content, Software fault prediction]
Healing online service systems via mining historical issue repositories
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Online service systems have been increasingly popular and important nowadays, with an increasing demand on the availability of services provided by these systems, while significant efforts have been made to strive for keeping services up continuously. Therefore, reducing the MTTR (Mean Time to Restore) of a service remains the most important step to assure the user-perceived availability of the service. To reduce the MTTR, a common practice is to restore the service by identifying and applying an appropriate healing action (i.e., a temporary workaround action such as rebooting a SQL machine). However, manually identifying an appropriate healing action for a given new issue (such as service down) is typically time consuming and error prone. To address this challenge, in this paper, we present an automated mining-based approach for suggesting an appropriate healing action for a given new issue. Our approach generates signatures of an issue from its corresponding transaction logs and then retrieves historical issues from a historical issue repository. Finally, our approach suggests an appropriate healing action by adapting healing actions for the retrieved historical issues. We have implemented a healing suggestion system for our approach and applied it to a real-world product online service that serves millions of online customers globally. The studies on 77 incidents (severe issues) over 3 months showed that our approach can effectively provide appropriate healing actions to reduce the MTTR of the service.
[MTTR, data mining, historical issue repository mining, transaction logs, service user-perceived availability, information services, automated mining-based approach, Online service system, healing action, healing suggestion system, fault tolerant computing, healing online service systems, mean time to restore, historical issue retrieval]
Automated evaluation of syntax error recovery
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Evaluation of parse error recovery techniques is an open problem. The community lacks objective standards and methods to measure the quality of recovery results. This paper proposes an automated technique for recovery evaluation that offers a solution for two main problems in this area. First, a representative testset is generated by a mutation based fuzzing technique that applies knowledge about common syntax errors. Secondly, the quality of the recovery results is automatically measured using an oracle-based evaluation technique. We evaluate the validity of our approach by comparing results obtained by automated evaluation with results obtained by manual inspection. The evaluation shows a clear correspondence between our quality metric and human judgement.
[representative testset, quality metric, Evaluation, human judgement, syntax error recovery, Parsing, computational linguistics, automated evaluation, parse error recovery techniques, IDE, mutation based fuzzing technique, system recovery, Error Recovery, recovery evaluation, oracle-based evaluation technique, Test Generation]
MaramaAI: tool support for capturing and managing consistency of multi-lingual requirements
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Requirements captured by Requirements Engineers are commonly inconsistent with their client's intended requirements and are often error prone especially if the requirements are written in multiple languages. We demonstrate the use of our automated inconsistency-checking tool MaramaAI to capture and manage the consistency of multi-lingual requirements in both the English and Malay languages for requirements engineers and clients using a round-trip, rapid prototyping approach.
[natural language processing, software prototyping, Consistency, Multi-lingual Requirements, consistency capturing, Rapid-prototyping, consistency management, formal specification, Malay languages, tool support, multilingual requirements, MaramaAI, automated inconsistency-checking tool, round-trip approach, English languages, rapid prototyping approach, requirements engineers]
GUITest: a Java library for fully automated GUI robustness testing
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Graphical User Interfaces (GUIs) are substantial parts of today's applications, no matter whether these run on tablets, smartphones or desktop platforms. Since the GUI is often the only component that humans interact with, it demands for thorough testing to ensure an efficient and satisfactory user experience. Being the glue between almost all of an application's components, GUIs also lend themselves for system level testing. However, GUI testing is inherently difficult and often involves great manual labor, even with modern tools which promise automation. This paper introduces a Java library called GUITest, which allows to generate fully automated GUI robustness tests for complex applications, without the need to manually generate models or input sequences. We will explain how it operates and present first results on its applicability and effectivity during a test involving Microsoft Word.
[Java, program testing, graphical user interfaces, graphical user interface, Automated Testing, system level testing, smart phone, GUI robustness testing, Java library, user experience, Microsoft Word, Gui Testing, Robustness Testing, tablet computer, desktop computer platform, GUITest]
Observatory of trends in software related microblogs
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Microblogging has recently become a popular means to disseminate information among millions of people. Interestingly, software developers also use microblog to communicate with one another. Different from traditional media, microblog users tend to focus on recency and informality of content. Many tweet contents are relatively more personal and opinionated, compared to that of traditional news report. Thus, by analyzing microblogs, one could get the up-to-date information about what people are interested in or feel toward a particular topic. In this paper, we describe our microblog observatory that aggregates more than 70,000 Twitter feeds, captures software-related tweets, and computes trends from across topics and time points. Finally, we present the results to the end users via a web interface available at http://research.larc.smu.edu.sg/palanteer/swdev.
[Web interface, Visualization, Twitter feeds, software related microblogs, software developers, Exploration, Twitter, user interfaces, microblog observatory, Software Development, microblogging, software-related tweets, tweet contents, social networking (online)]
Arcade.PLC: a verification platform for programmable logic controllers
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
This paper introduces Arcade.PLC, a verification platform for programmable logic controllers (PLCs). The tool supports static analysis as well as ACTL and past-time LTL model checking using counterexample-guided abstraction refinement for different programming languages used in industry. In the underlying principles of the framework, knowledge about the hardware platform is exploited so as to provide efficient techniques. The effectiveness of the approach is evaluated on programs implemented using a combination of programming languages.
[PLC, static analysis, programming languages, verification platform, formal verification, past-time LTL model checking, model checking, counterexample-guided abstraction refinement, Arcade.PLC platform, programming language, ACTL model checking, control engineering computing, programmable logic controllers, programmable controllers]
Test suite selection based on traceability annotations
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
This paper describes the Tobias tool. Tobias is a combinatorial test generator which unfolds a test pattern provided by the test engineer, and performs various combinations and repetitions of test parameters and methods. Tobias is available on-line at tobias.liglab.fr . This website features recent improvements of the tool including a new input language, a traceability mechanism, and the definition of various ``selectors'' which achieve test suite reduction.
[test suite selection, combinatorial test generator, test method, program testing, input language, Tobias tool, test parameter repetition, test parameter combination, test suite reduction, Combinatorial testing, Tobias, traceability mechanism, traceability annotation]
PuMoC: a CTL model-checker for sequential programs
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
In this paper, we present PuMoC, a CTL model checker for Pushdown systems (PDSs) and sequential C/C++ and Java programs. PuMoC allows to do CTL model-checking w.r.t simple valuations, where the atomic propositions depend on the control locations of the PDSs, and w.r.t. regular valuations, where atomic propositions are regular predicates over the stack content. Our tool allowed to (1) check 500 randomly generated PDSs against several CTL formulas; (2) check around 1461 versions of 30 Windows drivers taken from SLAM benchmarks; (3) check several C and Java programs; and (4) perform data flow analysis of real-world Java programs. Our results show the efficiency and the applicability of our tool.
[Java, atomic proposition, Pushdown Systems, Branching-time Temporal Logic, Model-Checking, data flow analysis, CTL model checker, C++ language, C-C++ language, PuMoC model checker, formal verification, pushdown system, Java program, Software Model-Checking, Windows driver]
Weave droid: aspect-oriented programming on Android devices: fully embedded or in the cloud
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Weave Droid is an Android application that makes Aspect-Oriented Programming (AOP) on Android devices possible and user-friendly. It allows to retrieve applications and aspects and weave them together in several ways. Applications and aspects can be loaded from Google Play, personal repositories, or the local memory of a device. Then, two complementary weaving modes are provided: local or remote, using the embedded aspect compiler or the compiler in the cloud, respectively. This provides flexibility and preserves the mobility of the target devices. Weave Droid opens a world of possible applications, not only by benefiting from the already existing uses of AOP on standard machines, but also by the various uses related to the mobile devices. Effectiveness of Weave Droid is demonstrated by weaving aspects with off-the-shelf applications from Google Play.
[complementary weaving modes, AOP, local mode, Android devices, remote mode, Aspect-Oriented Programming, Weave droid, program compilers, Android, cloud, device local memory, mobile computing, Cloud Computing, Google Play, aspect-oriented programming, operating systems (computers), embedded aspect compiler, Mobile Devices, Weaving, personal repositories, Embedded Systems, cloud computing, Android application]
Caprice: a tool for engineering adaptive privacy
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
In a dynamic environment where context changes frequently, users' privacy requirements can also change. To satisfy such changing requirements, there is a need for continuous analysis to discover new threats and possible mitigation actions. A frequently changing context can also blur the boundary between public and personal space, making it difficult for users to discover and mitigate emerging privacy threats. This challenge necessitates some degree of self-adaptive privacy management in software applications. This paper presents Caprice - a tool for enabling software engineers to design systems that discover and mitigate context-sensitive privacy threats. The tool uses privacy policies, and associated domain and software behavioural models, to reason over the contexts that threaten privacy. Based on the severity of a discovered threat, adaptation actions are then suggested to the designer. We present the Caprice architecture and demonstrate, through an example, that the tool can enable designers to focus on specific privacy threats that arise from changing context and the plausible category of adaptation action, such as ignoring, preventing, reacting, and terminating interactions that threaten privacy.
[changing context, user privacy requirements, context-sensitive privacy threat mitigation, adaptive software, privacy policies, software behavioural models, Caprice, self-adaptive privacy management, mitigation actions, Privacy, context-sensitive privacy threat discovery, selective disclosure, engineering adaptive privacy, data privacy, software engineering, software tools]
JStereoCode: automatically identifying method and class stereotypes in Java code
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Object-Oriented (OO) code stereotypes are low-level patterns that reveal the design intent of a source code artifact, such as, a method or a class. They are orthogonal to the problem domain of the software and they reflect the role of a method or class from the OO problem solving point of view. However, the research community in automated reverse engineering has focused more on higher-level design information, such as design patterns. Existing work on reverse engineering code stereotypes is scarce and focused on C++ code, while no tools are freely available as of today. We present JStereoCode, a tool that automatically identifies the stereotypes of methods and classes in Java systems. The tool is integrated with Eclipse and for a given Java project will classify each method and class in the system based on their stereotypes. Applications of JStereoCode include: program comprehension, defect prediction, etc.
[program comprehension, Java, Program comprehension, object-oriented programming, OO code stereotypes, identifying method, Java project, C++ code, Eclipse, low-level patterns, reverse engineering code stereotypes, Java systems, reverse engineering, source code artifact, class stereotypes, C++ language, JStereoCode, Code stereotypes, defect prediction, design patterns, automated reverse engineering, object-oriented code stereotypes]
CHESS: a model-driven engineering tool environment for aiding the development of complex industrial systems
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Modern software systems require advanced design support especially capable of mastering rising complexity, as well as of automating as many development tasks as possible. Model-Driven Engineering (MDE) is earning consideration as a solid response to those challenges on account of its support for abstraction and domain specialisation. However, MDE adoption often shatters industrial practice because its novelty opposes the need to preserve vast legacy and to not disband the skills matured in pre-MDE or alternative development solutions. This work presents the CHESS tool environment, a novel approach for cross-domain modelling of industrial complex systems. It leverages on UML profiling and separation of concerns realised through the specification of well-defined design views, each of which addresses a particular aspect of the problem. In this way, extra-functional, functional, and deployment descriptions of the system can be given in a focused manner, avoiding issues pertaining to distinct concerns to interfere with one another.
[Separation of concerns, UML separation, complex industrial system, Unified Modeling Language, system deployment description, system extra-functional description, back propagation, industrial complex system, model-driven engineering tool, UML profiling, CHESS tool, code generation, system functional description, MDE adoption, software engineering]
SYMake: a build code analysis and refactoring tool for makefiles
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Software building is an important task during software development. However, program analysis support for build code is still limited, especially for build code written in a dynamic language such as Make. We introduce SYMake, a novel program analysis and refactoring tool for build code in Makefiles. SYMake is capable of detecting several types of code smells and errors such as cyclic dependencies, rule inclusion, duplicate prerequisites, recursive variable loops, etc. It also supports automatic build code refactoring, e.g. rule extraction/removal, target creation, target/variable renaming, prerequisite extraction, etc. In addition, SYMake provides the analysis on defined rules, targets, prerequisites, and associated information to help developers better understand build code in a Makefile and its included files.
[dynamic language, duplicate prerequisites, Makefiles, software development, program diagnostics, Code Smells, software building, recursive variable loops, Build Code Analysis, Maintenance, software maintenance, refactoring tool, automatic build code refactoring, cyclic dependencies, rule extraction-removal, target-variable renaming, prerequisite extraction, code analysis, code smells, rule inclusion, program analysis, target creation, SYMake, Refactoring]
Quokka: visualising interactions of enterprise software environment emulators
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Enterprise software systems operate in large-scale, heterogeneous, distributed environments which makes assessment of non-functional properties, such as scalability and robustness, of those systems particularly challenging. Enterprise environment emulators can provide test-beds representative of real environments using only a few physical hosts thereby allowing assessment of the non-functional properties of enterprise software systems. To date, analysing outcomes of these tests has been an ad hoc and somewhat tedious affair; largely based on manual and/or script-assisted inspection of interaction logs. Quokka visualises emulations significantly aiding analysis and comprehension. Emulated interactions can be viewed live (in real-time) as well as be replayed at a later stage, furthermore, basic charts are used to aggregate and summarise emulations, helping to identify performance and scalability issues.
[Scalability, Enterprise Software Emulation, Quokka tool, data visualisation, emulation visualization, enterprise software system, interaction log, robustness property, Service Virtualization, Visualisation, enterprise software environment emulator, business data processing, scalability property]
Communicating continuous integration servers for increasing effectiveness of automated testing
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Automated testing and continuous integration are established concepts in today's software engineering landscape, but they work in a kind of isolated environment as they do not fully take into consideration the complexity of dependencies between code artifacts in different projects. In this paper, we demonstrate the Continuous Change Impact Analysis Process (CCIP) that breaks up the isolation by actively taking into account project dependencies. The implemented CCIP approach extends the traditional continuous integration (CI) process by enforcing communication between CI servers whenever new artifact updates are available. We show that the exchange of CI process results contribute to improving effectiveness of automated testing.
[Software testing, program testing, software engineering landscape, continuous change impact analysis process, automated testing, code artifacts, software libraries, dependency management, project dependency, CCIP, test coverage, software engineering, communicating continuous integration servers, software project dependency]
GZoltar: an eclipse plug-in for testing and debugging
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Testing and debugging is the most expensive, error-prone phase in the software development life cycle. Automated testing and diagnosis of software faults can drastically improve the efficiency of this phase, this way improving the overall quality of the software. In this paper we present a toolset for automatic testing and fault localization, dubbed GZoltar, which hosts techniques for (regression) test suite minimization and automatic fault diagnosis (namely, spectrum-based fault localization). The toolset provides the infrastructure to automatically instrument the source code of software programs to produce runtime data. Subsequently the data was analyzed to both minimize the test suite and return a ranked list of diagnosis candidates. The toolset is a plug-and-play plug-in for the Eclipse IDE to ease world-wide adoption.
[program debugging, toolset, fault diagnosis, RZoltar, program testing, automated software fault testing, Eclipse plug-in, GZoltar, Eclipse IDE, software program source code, software quality, plug-and-play plug-in, regression test suite minimization, automatic software faults diagnosis, software development life cycle, runtime data, Automatic Testing, software fault localization, Automatic Debugging, debugging, statistical testing, error-prone phase]
Semantic patch inference
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
We propose a tool for inferring transformation specifications from a few examples of original and updated code. These transformation specifications may contain multiple code fragments from within a single function, all of which must be present for the transformation to apply. This makes the inferred transformations context sensitive. Our algorithm is based on depth-first search, with pruning. Because it is applied locally to a collection of functions that contain related changes, it is efficient in practice. We illustrate the approach on an example drawn from recent changes to the Linux kernel.
[operating system kernels, Linux kernel, semantic patch, inference mechanisms, tree searching, formal specification, collateral evolution, transformation specification, depth-first search algorithm, code fragment, Software evolution, Linux, pruning algorithm, semantic patch inference]
REInDetector: a framework for knowledge-based requirements engineering
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Requirements engineering (RE) is a coordinated effort to allow clients, users, and software engineers to jointly formulate assumptions, constraints, and goals about a software solution. However, one of the most challenging aspects of RE is the detection of inconsistencies between requirements. To address this issue, we have developed REInDetector, a knowledge-based requirements engineering tool, supporting automatic detection of a range of inconsistencies. It provides facilities to elicit, structure, and manage requirements with distinguished capabilities for capturing the domain knowledge and the semantics of requirements. This permits an automatic analysis of both consistency and realizability of requirements. REInDetector finds implicit consequences of explicit requirements and offers all stakeholders an additional means to identify problems in a more timely fashion than existing RE tools. In this paper, we describe the Description Logic used to capture requirements, the REInDetector tool, its support for inconsistency detection, and its efficacy as applied to several RE examples. An important feature of REInDetector is also its ability to generate comprehensive explanations to provide more insights into the detected inconsistencies.
[requirement management, domain knowledge, knowledge-based requirements engineering, requirement elicitation, requirement semantics, REInDetector, Consistency, Requirements Engineering, formal specification, knowledge representation languages, description logic, Description Logic, RE, inconsistency detection, knowledge based systems, requirement structuring]
Formal verification techniques for model transformations specified by-demonstration
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Model transformations play an essential role in many aspects of model-driven development. By-demonstration approaches provide a user-friendly tool for specifying reusable model transformations. Here, a modeler performs the model transformation only once by hand and an executable transformation is automatically derived. Such a transformation is characterized by the set of pre- and postconditions that are required to be satisfied prior and after the execution of the transformation. However, the automatically derived conditions are usually too restrictive or incomplete and need to be refined manually to obtain the intended model transformation. As model transformations may be specified improperly despite the use of by-demonstration development approaches, we propose to employ formal verification techniques to detect inconsistent and erroneous transformations. In particular, we conjecture that methods drawn from software model checking and theorem proving might be employed to verify certain correctness properties of model transformations.
[formal verification techniques, by-demonstration development approaches, user-friendly tool, software model checking, model-driven development, reusable model transformations, formal verification, model checking, correctness properties, modeler, software reusability, by-demonstration specification, theorem proving, Model transformations]
A model-driven parser generator with reference resolution support
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
ModelCC is a model-based parser generator. Model-based parser generators decouple language specification from language processing. This model-driven approach avoids the limitations imposed by parser generators whose language specifications must conform to specific grammar constraints. Moreover, ModelCC supports reference resolution within the language specification. Therefore, it does not parse just trees but it can also efficiently deal with abstract syntax graphs. These graphs can even include cycles (i.e. they are not constrained to directed acyclic graphs).
[grammar constraint, ModelCC parser generator, directed acyclic graph, parser generators, Model-based software development, model-driven parser generator, formal specification, program compilers, abstract syntax graph, language processing, directed graphs, abstract syntax graphs, language specification]
Property-preserving program refinement
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
During the development and maintenance process, a program changes form, often being refined as specifications and implementation decisions are realized. A correctness proof built in parallel with an original program can be extended to a proof of refined program by showing equivalences between the original and refined program. This paper illustrates two examples of property-preserving refinement, partial evaluation and generalization, and explores the correctness-preserving equivalences underpinning those refinement techniques. We plan to explore ways in which the informal reasoning behind these and similar program refinement tasks may be captured to extend the proof for an original program into a proof of the refined program.
[property-preserving program refinement, software development process, Coq, informal reasoning, Program equivalence, inference mechanisms, software maintenance, formal specification, correctness proof, refined program, formal verification, program equivalence, partial generalization, optimization, software maintenance process, partial evaluation, correctness-preserving equivalence]
Predicting software complexity by means of evolutionary testing
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
One characteristic that impedes software from achieving good levels of maintainability is the increasing complexity of software. Empirical observations have shown that typically, the more complex the software is, the bigger the test suite is. Thence, a relevant question, which originated the main research topic of our work, has raised: "Is there a way to correlate the complexity of the test cases utilized to test a software product with the complexity of the software under test?". This work presents a new approach to infer software complexity with basis on the characteristics of automatically generated test cases. From these characteristics, we expect to create a test case profile for a software product, which will then be correlated to the complexity, as well as to other characteristics, of the software under test. This research is expected to provide developers and software architects with means to support and validate their decisions, as well as to observe the evolution of a software product during its life-cycle. Our work focuses on object-oriented software, and the corresponding test suites will be automatically generated through an emergent approach for creating test data named as Evolutionary Testing.
[object-oriented programming, program testing, developers, object-oriented software, software maintainability, Complexity Measurement, software product testing, software maintenance, evolutionary testing, software architects, Evolutionary Testing, Object-Oriented Software, software product test case profile, automatically generated test cases, software product lifecycle, software complexity prediction]
Identifying refactoring sequences for improving software maintainability
2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering
None
2012
Refactoring is a well-known technique that preserves software behaviors and improves its bad structures or bad smells. In most cases, more than one bad smell is found in a program. Consequently, developers frequently apply refactorings more than once. Applying appropriate refactoring sequences, an ordered list of refactorings, developers can remove bad smells as well as reduce improvement time and produce highly maintainable software. According to our 2011 survey, developers consider four main criteria to select an optimal refactoring sequence: 1) the number of removed bad smells, 2) maintainability, 3) the size of refactoring sequence and 4) the number of modified program elements. A refactoring sequence that satisfies these four criteria produces code without bad smells, with higher maintainability, using the least improvement effort and time, and providing more traceability. Some existing works suggest a list of refactorings without ordering, and others suggest refactoring sequences. However, these works do not consider the four criteria discussed earlier. Therefore, our research proposes an approach to identify an optimal refactoring sequence that meets these criteria. In addition, it is expected that the findings will reduce maintenance time and cost, increase maintainability and enhance software quality.
[modified program elements, Refactoring sequence, refactoring sequence identification, bad smells, bad smell, software maintainability, software quality, bad structures, software maintenance, software behaviors, refactoring sequence size, maintainability and software maintenance]
Message from the Chairs
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Presents the welcome message from the conference proceedings.
[]
BOOM: Experiences in language and tool design for distributed systems (keynote)
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
With the rapid expansion of cloud infrastructure and mobile devices, distributed systems have quickly emerged as a dominant computing platform. Distributed systems bring significant complexity to programming, due to platform issues including asynchrony, concurrency, and partial failure. Meanwhile, scalable distributed infrastructure&#x2014;notably &#x201C;NoSQL&#x201D; systems&#x2014;have put additional burdens on programmers by sacrificing traditional infrastructure contracts like linearizable or transactional I/O in favor of high availability. A growing segment of the developer community needs to deal with these issues today, and for the most part developers are still using languages and tools designed for sequential computation on tightly coupled architectures. This has led to software that is increasingly hard to test and hard to trust. Over the past 5 years, the BOOM project at Berkeley has focused on making it easier to write correct and maintainable code for distributed systems. Our work has taken a number of forms, including the development of the Bloom programming language for distributed systems, tools for testing and checking distributed programs, and the CALM Theorem, which connects programmer level concerns of determinism to system-level concerns about the need for distributed coordination. This talk will reflect on this work, and highlight opportunities for improved collaboration between the software engineering and distributed systems research communities.
[]
The challenges of verification and validation of automated planning systems (keynote)
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Mission planning is central to space mission operations, and has benefited from advances in model-based planning software. A model is a description of the objects, actions, constraints and preferences that the planner reasons over to generate plans. Developing, verifying and validating a planning model is, however, a difficult task. Mission planning constraints and preferences arise from many sources, including simulators and engineering specification documents. As mission constraints evolve, planning domain modelers must add and update model constraints efficiently using the available source data, catching errors quickly, and correcting the model. The consequences of erroneous models are very high, especially in the space operations environment. We first describe the space operations environment, particularly the role of the mission planning system. We then describe model-based planning, and briefly review the current state of the practice in designing model-based mission planning tools and the challenges facing model developers. We then describe an Interactive Model Development Environment (IMDE) approach to developing mission planning systems. This approach integrates modeling and simulation environments to reduce model editing time, generate simulations automatically to evaluate plans, and identify modeling errors automatically by evaluating simulation output. The IMDE approach was tested on a small subset of the Lunar Atmosphere and Dust Environment Explorer (LADEE) flight software to demonstrate how to develop the LADEE mission planning system.
[]
Big problems in industry (panel)
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Software Engineering in practice deals with scale in a variety of dimensions. We build large scale systems operating on vast amount of data. We have millions of customers with billions of queries and transactions. We have distributed teams making thousands of changes, running millions of tests and releasing multiple times per day. These dimensions of scale interact to provide challenges for software development tools and processes. The panelists will describe the challenging aspects of scale in their specific problem domains and discuss which software engineering methods work and which leave room for improvement.
[]
Round-up: Runtime checking quasi linearizability of concurrent data structures
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We propose a new method for runtime checking of a relaxed consistency property called quasi linearizability for concurrent data structures. Quasi linearizability generalizes the standard notion of linearizability by intentionally introducing nondeterminism into the parallel computations and exploiting such nondeterminism to improve the performance. However, ensuring the quantitative aspects of this correctness condition in the low level code is a difficult task. Our method is the first fully automated method for checking quasi linearizability in the unmodified C/C++ code of concurrent data structures. It guarantees that all the reported quasi linearizability violations are real violations. We have implemented our method in a software tool based on LLVM and a concurrency testing tool called Inspect. Our experimental evaluation shows that the new method is effective in detecting quasi linearizability violations in the source code of concurrent data structures.
[linearizability notion, Inspect tool, Law, program testing, Instruction sets, quasilinearizability property, runtime checking, concurrent data structures, parallel computations, Data structures, History, parallel processing, Standards, nondeterminism notion, Runtime, software tool, formal verification, concurrency testing tool, concurrency control, C-C++ code, data structures, correctness condition]
Constraint-based automatic symmetry detection
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We present an automatic approach to detecting symmetry relations for general concurrent models. Despite the success of symmetry reduction in mitigating state explosion problem, one essential step towards its soundness and effectiveness, i.e., how to discover sufficient symmetries with least human efforts, is often either overlooked or oversimplified. In this work, we show how a concurrent model can be viewed as a constraint satisfaction problem (CSP), and present an algorithm capable of detecting symmetries arising from the CSP which induce automorphisms of the model. To the best of our knowledge, our method is the first approach that can automatically detect both process and data symmetries as demonstrated via a number of systems.
[general concurrent models, CSP, Protocols, data symmetries, symmetry reduction, Color, Transforms, concurrency theory, constraint satisfaction problem, automorphisms, Cost accounting, constraint-based automatic symmetry detection, constraint satisfaction problems, state explosion problem, human efforts, Lead, Space exploration, Arrays]
Proving MCAPI executions are correct using SMT
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Asynchronous message passing is an important paradigm in writing applications for embedded heterogeneous multicore systems. The Multicore Association (MCA), an industry consortium promoting multicore technology, is working to standardize message passing into a single API, MCAPI, for bare metal implementation and portability across platforms. Correctness in such an API is difficult to reason about manually, and testing against reference solutions is equally difficult as reference solutions implement an unknown set of allowed behaviors, and programmers have no way to directly control API internals to expose or reproduce errors. This paper provides a way to encode an MCAPI execution as a Satisfiability Modulo Theories (SMT) problem, which if satisfiable, yields a feasible execution schedule on the same trace, such that it resolves non-determinism in the MCAPI runtime in a way that it now fails user provided assertions. The paper proves the problem is NP-complete. The encoding is useful for test, debug, and verification of MCAPI program execution. The novelty in the encoding is the direct use of match pairs (potential send and receive couplings). Match-pair encoding for MCAPI executions, when compared to other encoding strategies, is simpler to reason about, results in significantly fewer terms in the SMT problem, and captures feasible behaviors that are ignored in previously published techniques. Further, to our knowledge, this is the first SMT encoding that is able to run in infinite-buffer semantics, meaning the runtime has unlimited internal buffering as opposed to no internal buffering. Results demonstrate that the SMT encoding, restricted to zero-buffer semantics, uses fewer clauses when compared to another zero-buffer technique, and it runs faster and uses less memory. As a result the encoding scales well for programs with high levels of non-determinism in how sends and receives may potentially match.
[Schedules, application program interfaces, program verification, multicore technology, execution schedule, reference solutions, computability, MCAPI program execution testing, Refinement, Complexity theory, portability, satisfiability modulo theories problem, Runtime, asynchronous message passing, Semantics, match-pair encoding, embedded systems, API internals, industry consortium, zero-buffer technique, unlimited internal buffering, MCAPI runtime, MCAPI program execution verification, Message Passing, message passing, multiprocessing systems, MCAPI executions, infinite-buffer semantics, Multicore processing, embedded heterogeneous multicore systems, MCAPI program execution debugging, bare metal implementation, encoding strategies, Encoding, NP-complete problem, Abstraction, Multicore Association, SMT encoding, Message passing, SMT, zero-buffer semantics, single API]
Efficient data race prediction with incremental reasoning on time-stamped lock history
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We present an efficient data race prediction algorithm that uses lock-reordering based incremental search on time-stamped lock histories for solving multiple races effectively. We balance prediction accuracy, coverage, and performance with a specially designed pairwise reachability algorithm that can store and re-use past search results, thereby, amortizing the cost of reasoning over redundant and overlapping search space. Compared to graph-based search algorithms, our algorithm incurs much smaller overhead due to amortization, and can potentially be used while a program under test is executing. To demonstrate such a possibility, we implemented our approach as an incremental Predictive Analysis (iPA) module in a predictive testing framework. Our approach can handle traces with a few hundreds to half a million events, and predict known/unknown real data races with a performance penalty of less than 4% in addition to what is incurred by runtime race detectors.
[program debugging, program testing, pairwise reachability algorithm, Instruction sets, software reliability, incremental reasoning, Cognition, amortization, History, Accuracy, lock reordering based incremental search, efficient data race prediction algorithm, incremental predictive analysis, multi-threaded programs, redundancy, search problems, Testing, reachability analysis, multi-threading, overlapping search space, redundant search space, Vectors, Synchronization, prediction theory, predictive testing, iPA module, program under test execution, reasoning about programs, time stamped lock history, runtime race detector]
PIEtrace: Platform independent executable trace
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
To improve software dependability, a large number of software engineering tools have been developed over years. Many of them are difficult to apply in practice because their system and library requirements are incompatible with those of the subject software. We propose a technique called platform independent executable trace. Our technique traces and virtualizes a regular program execution that is platform dependent, and generates a stand-alone program called the trace program. Running the trace program re-generates the original execution. More importantly, trace program execution is completely independent of the underlying operating system and libraries such that it can be compiled and executed on arbitrary platforms. As such, it can be analyzed by a third party tool on a platform preferred by the tool. We have implemented the technique on x86 and sensor platforms. We show that buggy executions of 10 real-world Windows and sensor applications can be traced and virtualized, and later analyzed by existing Linux tools. We also demonstrate how the technique can be used in cross-platform malware analysis.
[invasive software, PIEtrace, sensor platforms, program execution, Registers, software engineering tools, subject software, malware analysis, Linux, Operating systems, platform independent executable trace, software dependability, Libraries, Malware, software engineering, Linux tools, Virtualization, library requirements]
Improving efficiency of dynamic analysis with dynamic dependence summaries
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Modern applications make heavy use of third-party libraries and components, which poses new challenges for efficient dynamic analysis. To perform such analyses, transitive dependent components at all layers of the call stack must be monitored and analyzed, and as such may be prohibitively expensive for systems with large libraries and components. As an approach to address such expenses, we record, summarize, and reuse dynamic dataflows between inputs and outputs of components, based on dynamic control and data traces. These summarized dataflows are computed at a fine-grained instruction level; the result of which, we call &#x201C;dynamic dependence summaries.&#x201D; Although static summaries have been proposed, to the best of our knowledge, this work presents the first technique for dynamic dependence summaries. The benefits to efficiency of such summarization may be afforded with losses of accuracy. As such, we evaluate the degree of accuracy loss and the degree of efficiency gain when using dynamic dependence summaries of library methods. On five large programs from the DaCapo benchmark (for which no existing whole-program dynamic dependence analyses have been shown to scale) and 21 versions of NANOXML, the summarized dependence analysis provided 90% accuracy and a speed-up of 100% (i.e., &#x00D7;2), on average, when compared to traditional exhaustive dynamic dependence analysis.
[DaCapo benchmark, NANOXML, summarized dependence analysis, third-party libraries, dynamic control, degree of accuracy loss evaluation, Dynamic analysis, software libraries, dynamic dependence summaries, Accuracy, Data-flow, Dynamic Slicing, library methods, Abstracts, Libraries, transitive dependent components, Summarization, instruction sets, Java, dynamic program analysis, dynamic dataflows, program diagnostics, fine-grained instruction level, Program analysis, Indexes, data traces, exhaustive dynamic dependence analysis, Concrete, degree of efficiency gain evaluation, large object-oriented libraries, Arrays]
Efficient parametric runtime verification with deterministic string rewriting
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Early efforts in runtime verification show that parametric regular and temporal logic specifications can be monitored efficiently. These approaches, however, have limited expressiveness: their specifications always reduce to monitors with finite state. More recent developments showed that parametric context-free properties can be efficiently monitored with overheads generally lower than 12-15%. While context-free grammars are more expressive than finite-state languages, they still do not allow every computable safety property. This paper presents a monitor synthesis algorithm for string rewriting systems (SRS). SRSs are well known to be Turing complete, allowing for the formal specification of any computable safety property. Earlier attempts at Turing complete monitoring have been relatively inefficient. This paper demonstrates that monitoring parametric SRSs is practical. The presented algorithm uses a modified version of Aho-Corasick string searching for quick pattern matching with an incremental rewriting approach that avoids reexamining parts of the string known to contain no redexes.
[Algorithm design and analysis, parametric regular specifications, temporal logic, string rewriting systems, finite state machines, formal specification, incremental rewriting approach, Runtime, formal verification, Turing machines, SRS, context-free grammars, quick pattern matching, Safety, Monitoring, Java, rewriting systems, temporal logic specifications, Aho-Corasick string searching, Runtime Verification, Turing complete monitoring, deterministic string rewriting, Automata, String Rewriting, parametric runtime verification, Pattern matching, finite-state languages]
Operator-based and random mutant selection: Better together
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Mutation testing is a powerful methodology for evaluating the quality of a test suite. However, the methodology is also very costly, as the test suite may have to be executed for each mutant. Selective mutation testing is a well-studied technique to reduce this cost by selecting a subset of all mutants, which would otherwise have to be considered in their entirety. Two common approaches are operator-based mutant selection, which only generates mutants using a subset of mutation operators, and random mutant selection, which selects a subset of mutants generated using all mutation operators. While each of the two approaches provides some reduction in the number of mutants to execute, applying either of the two to medium-sized, real-world programs can still generate a huge number of mutants, which makes their execution too expensive. This paper presents eight random sampling strategies defined on top of operator-based mutant selection, and empirically validates that operator-based selection and random selection can be applied in tandem to further reduce the cost of mutation testing. The experimental results show that even sampling only 5% of mutants generated by operator-based selection can still provide precise mutation testing results, while reducing the average mutation testing time to 6.54% (i.e., on average less than 5 minutes for this study).
[cost reduction, test suite quality evaluation, Java, Correlation, program testing, random sampling strategies, Educational institutions, mutation testing, software quality, Standards, Power measurement, operator-based mutant selection, random mutant selection, Libraries, Testing]
Testing properties of dataflow program operators
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Dataflow programming languages, which represent programs as graphs of data streams and operators, are becoming increasingly popular and being used to create a wide array of commercial software applications. The dependability of programs written in these languages, as well as the systems used to compile and run these programs, hinges on the correctness of the semantic properties associated with operators. Unfortunately, these properties are often poorly defined, and frequently are not checked, and this can lead to a wide range of problems in the programs that use the operators. In this paper we present an approach for improving the dependability of dataflow programs by checking operators for necessary properties. Our approach is dynamic, and involves generating tests whose results are checked to determine whether specific properties hold or not. We present empirical data that shows that our approach is both effective and efficient at assessing the status of properties.
[semantic properties, program testing, Ports (Computers), dataflow programming languages, data streams, data flow analysis, Optimization, testing properties, commercial software applications, Program processors, Aggregates, Semantics, System recovery, dataflow program operators, Testing]
Bita: Coverage-guided, automatic testing of actor programs
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Actor programs are concurrent programs where concurrent entities communicate asynchronously by exchanging messages. Testing actor programs is challenging because the order of message receives depends on the non-deterministic scheduler and because exploring all schedules does not scale to large programs. This paper presents Bita, a scalable, automatic approach for testing non-deterministic behavior of actor programs. The key idea is to generate and explore schedules that are likely to reveal concurrency bugs because these schedules increase the schedule coverage. We present three schedule coverage criteria for actor programs, an algorithm to generate feasible schedules that increase coverage, and a technique to force a program to comply with a schedule. Applying Bita to real-world actor programs implemented in Scala reveals eight previously unknown concurrency bugs, of which six have already been fixed by the developers. Furthermore, we show our approach to find bugs 122&#x00D7; faster than random scheduling, on average.
[Schedules, program debugging, message passing, message exchange, multiprocessing programs, program testing, Receivers, Programming, Bita, concurrent programs, actor programs, Postal services, Concurrent computing, automatic testing, Computer bugs, scheduling, coverage-guided testing, concurrency bugs, Testing]
SABRINE: State-based robustness testing of operating systems
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
The assessment of operating systems robustness with respect to unexpected or anomalous events is a fundamental requirement for mission-critical systems. Robustness can be tested by deliberately exposing the system to erroneous events during its execution, and then analyzing the OS behavior to evaluate its ability to gracefully handle these events. Since OSs are complex and stateful systems, robustness testing needs to account for the timing of erroneous events, in order to evaluate the robust behavior of the OS under different states. This paper presents SABRINE (StAte-Based Robustness testIng of operatiNg systEms), an approach for state-aware robustness testing of OSs. SABRINE automatically extracts state models from execution traces, and generates a set of test cases that cover different OS states. We evaluate the approach on a Linux-based Real-Time Operating System adopted in the avionic domain. Experimental results show that SABRINE can automatically identify relevant OS states, and find robustness vulnerabilities while keeping low the number of test cases.
[program testing, Linux-based real-time operating system, Linux kernel, execution traces, erroneous event timing, avionic domain, Robustness Testing, Robustness, Hardware, operating systems robustness assessment, Kernel, Probes, Monitoring, Fault Injection, Testing, mission-critical systems, operating system kernels, state-based robustness testing of operating systems, program diagnostics, OS, Dependability Benchmarking, Operating Systems, Linux, SABRINE, Fault Tolerance]
BLITZ: Compositional bounded model checking for real-world programs
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Bounded Model Checking (BMC) for software is a precise bug-finding technique that builds upon the efficiency of modern SAT and SMT solvers. BMC currently does not scale to large programs because the size of the generated formulae exceeds the capacity of existing solvers. We present a new, compositional and property-sensitive algorithm that enables BMC to automatically find bugs in large programs. A novel feature of our technique is to decompose the behaviour of a program into a sequence of BMC instances and use a combination of satisfying assignments and unsatisfiability proofs to propagate information across instances. A second novelty is to use the control- and data-flow of the program as well as information from proofs to prune the set of variables and procedures considered and hence, generate smaller instances. Our tool BLITZ outperforms existing tools and scales to programs with over 100,000 lines of code. BLITZ automatically and efficiently discovers bugs in widely deployed software including new vulnerabilities in Internet infrastructure software.
[BLITZ, program debugging, compositional algorithm, data-flow, bug-finding technique, SMT solver, control-flow, program verification, Input variables, computability, property-sensitive algorithm, SAT solver, Indium phosphide, real-world programs, Computer bugs, Semantics, data flow computing, Model checking, Internet infrastructure software, BMC, Software, Internet, compositional bounded model checking]
Ranger: Parallel analysis of alloy models by range partitioning
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We present a novel approach for parallel analysis of models written in Alloy, a declarative extension of first-order logic based on relations. The Alloy language is supported by the fully automatic Alloy Analyzer, which translates models into propositional formulas and uses off-the-shelf SAT technology to solve them. Our key insight is that the underlying constraint satisfaction problem can be split into subproblems of lesser complexity by using ranges of candidate solutions, which partition the space of all candidate solutions. Conceptually, we define a total ordering among the candidate solutions, split this space of candidates into ranges, and let independent SAT searches take place within these ranges' endpoints. Our tool, Ranger, embodies our insight. Experimental evaluation shows that Ranger provides substantial speedups (in several cases, superlinear ones) for a variety of hard-to-solve Alloy models, and that adding more hardware reduces analysis costs almost linearly.
[Scalability, Metals, first-order logic, SAT, computability, analysis cost reduction, Alloy language models, parallel processing, Ranger tool, range partitioning, Analytical models, range endpoints, parallel analysis, specification languages, Static analysis, Hardware, candidate solution space partitioning, Alloy Analyzer, Computational modeling, constraint satisfaction problem, Vectors, Partitioning algorithms, Alloy, Parallel analysis, constraint satisfaction problems, propositional formulas, off-the-shelf SAT technology]
Automated verification of pattern-based interaction invariants in Ajax applications
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
When developing asynchronous JavaScript and XML (Ajax) applications, developers implement Ajax design patterns for increasing the usability of the applications. However, unpredictable contexts of running applications might conceal faults that will break the design patterns, which decreases usability. We propose a support tool called JSVerifier that auto-matically verifies interaction invariants; the applications handle their interactions in invariant occurrence and order. We also present a selective set of interaction invariants derived from Ajax design patterns, as input. If the application behavior breaks the design patterns, JSVerifier automatically outputs faulty execution paths for debugging. The results of our case studies show that JSVerifier can verify the interaction invariants in a feasible amount of time, and we conclude that it can help developers increase the usability of Ajax applications.
[program debugging, program verification, pattern-based interaction invariants, Servers, Reverse Engineering, Design Pattern, Model Checking, XML applications, debugging, JSVerifier, software tools, Ajax applications, faulty execution paths, Testing, Ajax, Context, Java, object-oriented programming, Debugging, Educational institutions, asynchronous JavaScript applications, Web pages, XML, Usability, automated verification, Ajax design patterns]
Software model checking for distributed systems with selector-based, non-blocking communication
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Many modern software systems are implemented as client/server architectures, where a server handles multiple clients concurrently. Testing does not cover the outcomes of all possible thread and communication schedules reliably. Software model checking, on the other hand, covers all possible outcomes but is often limited to subsets of commonly used protocols and libraries. Earlier work in cache-based software model checking handles implementations using socket-based TCP/IP networking, with one thread per client connection using blocking input/output. Recently, servers using non-blocking, selector-based input/output have become prevalent. This paper describes our work extending the Java PathFinder extension net-iocache to such software, and the application of our tool to modern server software.
[transport control protocol, software verification, server software, Servers, caching, client-server architectures, non-blocking input/output, formal verification, Computer architecture, Model checking, distributed systems, Libraries, selector-based nonblocking communication, blocking input-output, selector-based input/output, Message systems, Java PathFinder extension, Java, client-server systems, Internet protocol, communication schedule, cache-based software model checking, input-output cache, thread schedule, software model checking, socket-based TCP-IP networking, Software]
A study of repetitiveness of code changes in software evolution
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
In this paper, we present a large-scale study of repetitiveness of code changes in software evolution. We collected a large data set of 2,841 Java projects, with 1.7 billion source lines of code (SLOC) at the latest revisions, 1.8 million code change revisions (0.4 million fixes), 6.2 million changed files, and 2.5 billion changed SLOCs. A change is considered repeated within or cross-project if it matches another change having occurred in the history of the project or another project, respectively. We report the following important findings. First, repetitiveness of changes could be as high as 70-100% at small sizes and decreases exponentially as size increases. Second, repetitiveness is higher and more stable in the cross-project setting than in the within-project one. Third, fixing changes repeat similarly to general changes. Importantly, learning code changes and recommending them in software evolution is beneficial with accuracy for top-1 recommendation of over 30% and top-3 of nearly 35%. Repeated fixing changes could also be useful for automatic program repair.
[source code (software), source lines of code, automatic programming, Java, Software Evolution, Maintenance engineering, Programming, Repetitive Code Changes, code change revisions, SLOC, History, software maintenance, Java projects, software evolution, Databases, Vegetation, code change repetitiveness, Software, Libraries, code change learning, automatic program repair]
Consistency-preserving edit scripts in model versioning
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
In model-based software development, models are iteratively evolved. To optimally support model evolution, developers need adequate tools for model versioning tasks, including comparison, patching, and merging of models. A significant disadvantage of tools currently available is that they display, and operate with, low-level model changes which refer to internal model representations and which can lead to intermediate inconsistent states. Higher-level consistency-preserving edit operations including refactorings are better suited to explain changes or to resolve conflicts. This paper presents an automatic procedure which transforms a low-level difference into an executable edit script which uses consistency-preserving edit operations only. Edit scripts support consistent model patching and merging on a higher abstraction level. Our approach to edit script generation has been evaluated in a larger real-world case study.
[Adaptation models, automatic procedure, software development, Unified modeling language, Merging, model versioning, model merging, model comparison, model patching, configuration management, Semantics, Abstracts, Syntactics, internal model representations, Concrete, software tools, consistency preserving edit scripts]
JFlow: Practical refactorings for flow-based parallelism
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Emerging applications in the domains of recognition, mining and synthesis (RMS); image and video processing; data warehousing; and automatic financial trading admit a particular style of parallelism termed flow-based parallelism. To help developers exploit flow-based parallelism, popular parallel libraries such as Groovy's GPars, Intel's TBB Flow Graph and Microsoft's TPL Dataflow have begun introducing many new and useful constructs. However, to reap the benefits of such constructs, developers must first use them. This involves refactoring their existing sequential code to incorporate these constructs - a manual process that overwhelms even experts. To alleviate this burden, we introduce a set of novel analyses and transformations targeting flow-based parallelism. We implemented these ideas in JFlow, an interactive refactoring tool integrated into the Eclipse IDE. We used JFlow to parallelize seven applications: four from a previously known benchmark and three from a suite of large open source projects. JFlow, with minimal interaction from the developer, can successfully parallelize applications from the aforementioned domains with good performance (offering up to 3.45x speedup on a 4-core machine) and is fast enough to be used interactively as part of a developer's workflow.
[Groovy GPars, public domain software, Pipelines, recognition, data warehousing, Eclipse IDE, parallel programming, mining and synthesis domain, video processing, parallel libraries, interactive refactoring tool, Databases, Parallel processing, interactive systems, Libraries, automatic financial trading, flow-based parallelism, image processing, Java, large open source projects, sequential code refactoring, software maintenance, JFlow, Sensitivity, RMS domain, Microsoft TPL dataflow, Feature extraction, Intel TBB flow graph]
Automated planning for software architecture evolution
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
In previous research, we have developed a theoretical framework to help software architects make better decisions when planning software evolution. Our approach is based on representation and analysis of candidate evolution paths-sequences of transitional architectures leading from the current system to a desired target architecture. One problem with this kind of approach is that it imposes a heavy burden on the software architect, who must explicitly define and model these candidate paths. In this paper, we show how automated planning techniques can be used to support automatic generation of evolution paths, relieving this burden on the architect. We illustrate our approach by applying it to a data migration scenario, showing how this architecture evolution problem can be translated into a planning problem and solved using existing automated planning tools.
[Measurement, Availability, data migration scenario, automated planning techniques, automated planning tools, target architecture, Connectors, planning, software architecture, candidate evolution paths, Software architecture, Computer architecture, transitional architectures, Software, Planning, automatic generation, architecture evolution problem, software architecture evolution]
Automatically synthesizing SQL queries from input-output examples
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Many computer end-users, such as research scientists and business analysts, need to frequently query a database, yet lack enough programming knowledge to write a correct SQL query. To alleviate this problem, we present a programming by example technique (and its tool implementation, called SQLSynthesizer) to help end-users automate such query tasks. SQLSynthesizer takes from users an example input and output of how the database should be queried, and then synthesizes a SQL query that reproduces the example output from the example input. If the synthesized SQL query is applied to another, potentially larger, database with a similar schema, the synthesized SQL query produces a corresponding result that is similar to the example output. We evaluated SQLSynthesizer on 23 exercises from a classic database textbook and 5 forum questions about writing SQL queries. SQLSynthesizer synthesized correct answers for 15 textbook exercises and all 5 forum questions, and it did so from relatively small examples.
[database query, SQL query writing, automatic SQL query synthesis, SQLSynthesizer tool, database textbook exercises, programming-by-example technique, input-output examples, Standards, SQL, query processing, Databases, Aggregates, computer end-users, Writing, Syntactics, Skeleton, forum questions, Graphical user interfaces]
SEDGE: Symbolic example data generation for dataflow programs
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Exhaustive, automatic testing of dataflow (esp. mapreduce) programs has emerged as an important challenge. Past work demonstrated effective ways to generate small example data sets that exercise operators in the Pig platform, used to generate Hadoop map-reduce programs. Although such prior techniques attempt to cover all cases of operator use, in practice they often fail. Our SEDGE system addresses these completeness problems: for every dataflow operator, we produce data aiming to cover all cases that arise in the dataflow program (e.g., both passing and failing a filter). SEDGE relies on transforming the program into symbolic constraints, and solving the constraints using a symbolic reasoning engine (a powerful SMT solver), while using input data as concrete aids in the solution process. The approach resembles dynamic-symbolic (a.k.a. &#x201C;concolic&#x201D;) execution in a conventional programming language, adapted to the unique features of the dataflow domain. In third-party benchmarks, SEDGE achieves higher coverage than past techniques for 5 out of 20 PigMix benchmarks and 7 out of 11 SDSS benchmarks and (with equal coverage for the rest of the benchmarks). We also show that our targeting of the high-level dataflow language pays off: for complex programs, state-of-the-art dynamic-symbolic execution at the level of the generated map-reduce code (instead of the original dataflow program) requires many more test cases or achieves much lower coverage than our approach.
[conventional programming language, program testing, Programming, Hadoop map-reduce programs, high-level dataflow language, test cases, Cognition, symbolic constraints, programming languages, automatic testing, complex programs, specification languages, dataflow domain, Benchmark testing, pig platform, dataflow programs, mapreduce programs, SDSS benchmarks, SMT solver, state-of-the-art dynamic-symbolic execution, symbolic reasoning engine, operator use, data flow analysis, Educational institutions, Data processing, Extraterrestrial measurements, dataflow operator, concolic execution, SEDGE system, Concrete, map-reduce code, reasoning about programs, symbolic example data generation]
Characteristic studies of loop problems for structural test generation via symbolic execution
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Dynamic Symbolic Execution (DSE) is a state-of-the-art test-generation approach that systematically explores program paths to generate high-covering tests. In DSE, the presence of loops (especially unbound loops) can cause an enormous or even infinite number of paths to be explored. There exist techniques (such as bounded iteration, heuristics, and summarization) that assist DSE in addressing loop problems. However, there exists no literature-survey or empirical work that shows the pervasiveness of loop problems or identifies challenges faced by these techniques on real-world open-source applications. To fill this gap, we provide characteristic studies to guide future research on addressing loop problems for DSE. Our proposed study methodology starts with conducting a literature-survey study to investigate how technical problems such as loop problems compromise automated software-engineering tasks such as test generation, and which existing techniques are proposed to deal with such technical problems. Then the study methodology continues with conducting an empirical study of applying the existing techniques on real-world software applications sampled based on the literature-survey results and major open-source project hostings. This empirical study investigates the pervasiveness of the technical problems and how well existing techniques can address such problems among real-world software applications. Based on such study methodology, our two-phase characteristic studies identify that bounded iteration and heuristics are effective in addressing loop problems when used properly. Our studies further identify challenges faced by these techniques and provide guidelines for effectively addressing these challenges.
[Software testing, loop problems, program testing, software testing, Debugging, unbound loops, Search problems, Security, real world open source applications, open source project hostings, Open source software, infinite number, software applications, real-world software applications, characteristic studies, DSE, dynamic symbolic execution, structural test generation]
Entropy-based test generation for improved fault localization
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Spectrum-based Bayesian reasoning can effectively rank candidate fault locations based on passing/failing test cases, but the diagnostic quality highly depends on the size and diversity of the underlying test suite. As test suites in practice often do not exhibit the necessary properties, we present a technique to extend existing test suites with new test cases that optimize the diagnostic quality. We apply probability theory concepts to guide test case generation using entropy, such that the amount of uncertainty in the diagnostic ranking is minimized. Our ENTBUG prototype extends the search-based test generation tool EVOSUITE to use entropy in the fitness function of its underlying genetic algorithm, and we applied it to seven real faults. Empirical results show that our approach reduces the entropy of the diagnostic ranking by 49% on average (compared to using the original test suite), leading to a 91% average reduction of diagnosis candidates needed to inspect to find the true faulty one.
[Fault localization, Uncertainty, program testing, fitness function, Entropy, Cognition, diagnostic quality, EVOSUITE, entropy-based test generation, test suite, Genetic algorithms, entropy, Sociology, test case generation, improved fault localization, belief networks, passing-failing test cases, search-based test generation tool, probability theory concepts, program diagnostics, probability, Debugging, inference mechanisms, Statistics, software fault tolerance, Test case generation, ENTBUG prototype, diagnostic ranking, spectrum-based Bayesian reasoning]
Detecting bad smells in source code using change history information
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately. We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis.
[Measurement, source code (software), feature envy, HIST, Code Smells, software management, History, bad smells detection, code smells, Surgery, Detectors, structural information, code elements, software projects, Java, divergent change, fault proneness, Historical Information for Smell deTection, Change History Information, Association rules, software maintenance, smell detectors, versioning systems, blob, change history information, Feature extraction, fault tolerant computing, parallel inheritance, source code analysis, shotgun surgery]
Personalized defect prediction
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction-building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java-the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.
[software reliability, Linux kernel, PostgreSQL, Eclipse, Predictive models, program compilers, Lucene, Training, coding styles, Mars, different defect patterns, java software projects, Change classification, Jackrabbit, Java, separate prediction model, Xorg, Vectors, C software projects, experience levels, machine learning, personalized defect prediction, Linux, Computer bugs, Syntactics, Feature extraction, commit frequencies, software defect prediction]
Automatic recommendation of API methods from feature requests
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Developers often receive many feature requests. To implement these features, developers can leverage various methods from third party libraries. In this work, we propose an automated approach that takes as input a textual description of a feature request. It then recommends methods in library APIs that developers can use to implement the feature. Our recommendation approach learns from records of other changes made to software systems, and compares the textual description of the requested feature with the textual descriptions of various API methods. We have evaluated our approach on more than 500 feature requests of Axis2/Java, CXF, Hadoop Common, HBase, and Struts 2. Our experiments show that our approach is able to recommend the right methods from 10 libraries with an average recall-rate@5 of 0.690 and recall-rate@10 of 0.779 respectively. We also show that the state-of-the-art approach by Chan et al., that recommends API methods based on precise text phrases, is unable to handle feature requests.
[Java, application program interfaces, software systems, Documentation, Control systems, Vectors, Axis2/Java, software libraries, API methods, HBase, CXF, Struts 2, Databases, textual description, library APIs, Hadoop Common, feature requests, Software systems, Libraries, automatic recommendation]
Variability-aware performance prediction: A statistical learning approach
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Configurable software systems allow stakeholders to derive program variants by selecting features. Understanding the correlation between feature selections and performance is important for stakeholders to be able to derive a program variant that meets their requirements. A major challenge in practice is to accurately predict performance based on a small sample of measured variants, especially when features interact. We propose a variability-aware approach to performance prediction via statistical learning. The approach works progressively with random samples, without additional effort to detect feature interactions. Empirical results on six real-world case studies demonstrate an average of 94% prediction accuracy based on small random samples. Furthermore, we investigate why the approach works by a comparative analysis of performance distributions. Finally, we compare our approach to an existing technique and guide users to choose one or the other in practice.
[Measurement, Correlation, statistical learning, Predictive models, program variants, variability-aware performance prediction, configurable software systems, configuration management, Accuracy, Feature extraction, Software systems, Silicon, learning (artificial intelligence), statistical analysis, software performance evaluation]
A scalable approach for malware detection through bounded feature space behavior modeling
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
In recent years, malware (malicious software) has greatly evolved and has become very sophisticated. The evolution of malware makes it difficult to detect using traditional signature-based malware detectors. Thus, researchers have proposed various behavior-based malware detection techniques to mitigate this problem. However, there are still serious shortcomings, related to scalability and computational complexity, in existing malware behavior modeling techniques. This raises questions about the practical applicability of these techniques. This paper proposes and evaluates a bounded feature space behavior modeling (BOFM) framework for scalable malware detection. BOFM models the interactions between software (which can be malware or benign) and security-critical OS resources in a scalable manner. Information collected at run-time according to this model is then used by machine learning algorithms to learn how to accurately classify software as malware or benign. One of the key problems with simple malware behavior modeling (e.g., n-gram model) is that the number of malware features (i.e., signatures) grows proportional to the size of execution traces, with a resulting malware feature space that is so large that it makes the detection process very challenging. On the other hand, in BOFM, the malware feature space is bounded by an upper limit N, a constant, and the results of our experiments show that its computation time and memory usage are vastly lower than in currently reported, malware detection techniques, while preserving or even improving their high detection accuracy.
[invasive software, Scalability, Instruction sets, BOFM framework, Malware behavior modeling, Malware detection, scalable malware detection techniques, malicious software, malware feature space, software classification, File systems, malware behavior modeling techniques, Malware, learning (artificial intelligence), machine learning algorithms, behavior-based malware detection techniques, pattern classification, Computational modeling, signature-based malware detectors, execution trace size, bounded feature space behavior modeling, security-critical OS resources, Feature extraction, operating systems (computers), computational complexity]
Automatically partition software into least privilege components using dynamic data dependency analysis
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
The principle of least privilege requires that software components should be granted only necessary privileges, so that compromising one component does not lead to compromising others. However, writing privilege separated software is difficult and as a result, a large number of software is monolithic, i.e., it runs as a whole without separation. Manually rewriting monolithic software into privilege separated software requires significant effort and can be error prone. We propose ProgramCutter, a novel approach to automatically partitioning monolithic software using dynamic data dependency analysis. ProgramCutter works by constructing a data dependency graph whose nodes are functions and edges are data dependencies between functions. The graph is then partitioned into subgraphs where each subgraph represents a least privilege component. The privilege separated software runs each component in a separated process with confined system privileges. We evaluate it by applying it on four open source software. We can reduce the privileged part of the program from 100% to below 22%, while having a reasonable execution time overhead. Since ProgramCutter does not require any expert knowledge of the software, it not only can be used by its developers for software refactoring, but also by end users or system administrators. Our contributions are threefold: (i) we define a quantitative measure of the security and performance of privilege separation; (ii) we propose a graph-based approach to compute the optimal separation based on dynamic information flow analysis; and (iii) the separation process is automatic and does not require expert knowledge of the software.
[data analysis, open source software, data flow graphs, dynamic information flow analysis, Educational institutions, ProgramCutter, Databases, data dependency graph, Authentication, dynamic data dependency analysis, Writing, monolithic software automatic partitioning, Software, Performance analysis, software refactoring, program slicing, least privilege components]
Finding architectural flaws using constraints
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
During Architectural Risk Analysis (ARA), security architects use a runtime architecture to look for security vulnerabilities that are architectural flaws rather than coding defects. The current ARA process, however, is mostly informal and manual. In this paper, we propose Scoria, a semi-automated approach for finding architectural flaws. Scoria uses a sound, hierarchical object graph with abstract objects and dataflow edges, where edges can refer to nodes in the graph. The architects can augment the object graph with security properties, which can express security information unavailable in code. Scoria allows architects to write queries on the graph in terms of the hierarchy, reachability, and provenance of a dataflow object. Based on the query results, the architects enhance their knowledge of the system security and write expressive constraints. The expressiveness is richer than previous approaches that check only for the presence or absence of communication or do not track a dataflow as an object. To evaluate Scoria, we apply these constraints to several extended examples adapted from the CERT standard for Java to confirm that Scoria can detect injected architectural flaws. Next, we write constraints to enforce an Android security policy and find one architectural flaw in one Android application.
[Encryption, security architects, semiautomated approach, architectural risk analysis, security property, system security, software architecture, Runtime, Android (operating system), ARA process, Abstracts, reachability, dataflow edges, runtime architecture, Java, reachability analysis, Scoria, data flow analysis, security information, CERT standard, Encoding, Standards, dataflow object, Connectors, architectural flaws, security of data, object graph, coding defects, Android security policy, security vulnerability]
Improving bug localization using structured information retrieval
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Locating bugs is important, difficult, and expensive, particularly for large-scale systems. To address this, natural language information retrieval techniques are increasingly being used to suggest potential faulty source files given bug reports. While these techniques are very scalable, in practice their effectiveness remains low in accurately localizing bugs to a small number of files. Our key insight is that structured information retrieval based on code constructs, such as class and method names, enables more accurate bug localization. We present BLUiR, which embodies this insight, requires only the source code and bug reports, and takes advantage of bug similarity data if available. We build BLUiR on a proven, open source IR toolkit that anyone can use. Our work provides a thorough grounding of IR-based bug localization research in fundamental IR theoretical and empirical knowledge and practice. We evaluate BLUiR on four open source projects with approximately 3,400 bugs. Results show that BLUiR matches or outperforms a current state-of-the-art tool across applications considered, even when BLUiR does not use bug similarity data used by the other tool.
[Measurement, code constructs, Java, program debugging, structured information retrieval, natural language processing, public domain software, open source IR toolkit, information retrieval, source code, Information retrieval, natural language information retrieval, large-scale systems, bug localization, BLUiR, Accuracy, search, Computer bugs, bug similarity data, bug reports, Mathematical model, Bug localization, Indexing]
Leveraging program equivalence for adaptive program repair: Models and first results
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Software bugs remain a compelling problem. Automated program repair is a promising approach for reducing cost, and many methods have recently demonstrated positive results. However, success on any particular bug is variable, as is the cost to find a repair. This paper focuses on generate-and-validate repair methods that enumerate candidate repairs and use test cases to define correct behavior. We formalize repair cost in terms of test executions, which dominate most test-based repair algorithms. Insights from this model lead to a novel deterministic repair algorithm that computes a patch quotient space with respect to an approximate semantic equivalence relation. This allows syntactic and dataflow analysis techniques to dramatically reduce the repair search space. Generate-and-validate program repair is shown to be a dual of mutation testing, suggesting several possible cross-fertilizations. Evaluating on 105 real-world bugs in programs totaling 5MLOC and involving 10,000 tests, our new algorithm requires an order-of-magnitude fewer test evaluations than the previous state-of-the-art and is over three times more efficient monetarily.
[Algorithm design and analysis, deterministic repair algorithm, Adaptation models, program debugging, generate-and-validate program repair methods, Automated program repair, program testing, test cases, Search problems, syntactic analysis techniques, Optimization, adaptive program repair, search-based software engineering, Testing, cost reduction, test-based repair algorithms, software bugs, data flow analysis, Maintenance engineering, mutation testing, repair search space reduction, software maintenance, dataflow analysis techniques, deterministic algorithms, program equivalence, automated program repair, cross-fertilizations, patch quotient space, Approximation algorithms, repair cost, approximate semantic equivalence relation]
Detecting and characterizing semantic inconsistencies in ported code
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Adding similar features and bug fixes often requires porting program patches from reference implementations and adapting them to target implementations. Porting errors may result from faulty adaptations or inconsistent updates. This paper investigates (1) the types of porting errors found in practice, and (2) how to detect and characterize potential porting errors. Analyzing version histories, we define five categories of porting errors, including incorrect control- and data-flow, code redundancy, inconsistent identifier renamings, etc. Leveraging this categorization, we design a static control- and data-dependence analysis technique, SPA, to detect and characterize porting inconsistencies. Our evaluation on code from four open-source projects shows that SPA can detect porting inconsistencies with 65% to 73% precision and 90% recall, and identify inconsistency types with 58% to 63% precision and 92% to 100% recall. In a comparison with two existing error detection tools, SPA improves precision by 14 to 17 percentage points.
[Context, program debugging, data-flow, data-dependence analysis technique, OFDM, program diagnostics, public domain software, Cloning, error detection tools, History, error detection, open-source projects, semantic inconsistency characterization, static control, bug fixes, incorrect control, Linux, semantic inconsistency detection, Semantics, code redundancy, inconsistent identifier renamings, Syntactics, porting errors, ported code]
Lightweight control-flow instrumentation and postmortem analysis in support of debugging
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Debugging is difficult and costly. As a human programmer looks for a bug, it would be helpful to see a complete trace of events leading to the point of failure. Unfortunately, full tracing is simply too slow to use in deployment, and may even be impractical during testing. We aid post-deployment debugging by giving programmers additional information about program activity shortly before failure. We use latent information in post-failure memory dumps, augmented by low-overhead, tunable run-time tracing. Our results with a realistically-tuned tracing scheme show low enough overhead (0-5%) to be used in production runs. We demonstrate several potential uses of this enhanced information, including a novel postmortem static slice restriction technique and a reduced view of potentially-executed code. Experimental evaluation shows our approach to be very effective, such as shrinking stack-sensitive interprocedural static slices by 49-78% in larger applications.
[Algorithm design and analysis, program debugging, program activity, Instruments, realistically-tuned tracing scheme, potentially-executed code, Debugging, lightweight control-flow instrumentation, shrinking stack-sensitive interprocedural static slices, Computer crashes, Core dumps, postmortem analysis, post-deployment debugging, latent information, post-failure memory dumps, Production, postmortem static slice restriction technique, run-time tracing, Arrays, program slicing]
Characterizing and detecting resource leaks in Android applications
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Android phones come with a host of hardware components embedded in them, such as Camera, Media Player and Sensor. Most of these components are exclusive resources or resources consuming more memory/energy than general. And they should be explicitly released by developers. Missing release operations of these resources might cause serious problems such as performance degradation or system crash. These kinds of defects are called resource leaks. This paper focuses on resource leak problems in Android apps, and presents our lightweight static analysis tool called Relda, which can automatically analyze an application's resource operations and locate the resource leaks. We propose an automatic method for detecting resource leaks based on a modified Function Call Graph, which handles the features of event-driven mobile programming by analyzing the callbacks defined in Android framework. Our experimental data shows that Relda is effective in detecting resource leaks in real Android apps.
[resource leaks characterization, Humanoid robots, Mobile communication, callbacks, sensor component, Android apps, media player component, function call graph, Android (operating system), mobile computing, resource allocation, Android applications, resource leak, Java, event-driven mobile programming, program diagnostics, Media, static analysis, camera component, resource leaks detection, resource leak problems, Relda tool, application resource operations, Cameras, lightweight static analysis tool, Androids, Smart phones]
Dangling references in multi-configuration and dynamic PHP-based Web applications
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
PHP is a dynamic language popularly used in Web development for writing server-side code to dynamically create multiple versions of client-side pages at run time for different configurations. A PHP program contains code to be executed or produced for multiple configurations/versions. That dynamism and multi-configuration nature leads to dangling references. Specifically, in the execution for a configuration, a reference to a variable or a call to a function is dangling if its corresponding declaration cannot be found. We conducted an exploratory study to confirm the existence of such dangling reference errors including dangling cross-language and embedded references in the client-side HTML/JavaScript code and in data-accessing SQL code that are embedded in scattered PHP code. Dangling references have caused run-time fatal failures and security vulnerabilities. We developed DRC, a static analysis method to detect such dangling references. DRC uses symbolic execution to collect PHP declarations/references and to approximate all versions of the generated output, and then extracts embedded declarations/references. It associates each detected declaration/reference with a conditional constraint that represents the execution paths (i.e. configurations/versions) containing that declaration/reference. It then validates references against declarations via a novel dangling reference detection algorithm. Our empirical evaluation shows that DRC detects dangling references with high accuracy. It revealed 83 yet undiscovered defects caused by dangling references.
[dynamic language, run-time fatal failures, HTML code, data-accessing SQL code, Dangling References, Web development, dangling references, HTML, PHP program, Security, Servers, Reactive power, Databases, dangling cross-language, scattered PHP code, multiple versions, multiple configurations, program diagnostics, embedded references, Thumb, dynamic PHP-based Web applications, JavaScript code, SQL, writing server-side code, client-side pages, Internet, Web Code Analysis, multiconfiguration nature, Detection algorithms, static analysis method]
Dynamically transforming data structures
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Fine-tuning which data structure implementation to use for a given problem is sometimes tedious work since the optimum solution depends on the context, i.e., on the operation sequences, actual parameters as well as on the hardware available at run time. Sometimes a data structure with higher asymptotic time complexity performs better in certain contexts because of lower constants. The optimal solution may not even be possible to determine at compile time. We introduce transformation data structures that dynamically change their internal representation variant based on a possibly changing context. The most suitable variant is selected at run time rather than at compile time. We demonstrate the effect on performance with a transformation ArrayList data structure using an array variant and a linked hash bag variant as alternative internal representations. Using our transformation ArrayList, the standard DaCapo benchmark suite shows a performance gain of 5.19% in average.
[Context, Algorithm design and analysis, DaCapo benchmark suite, transformation ArrayList data structure, internal representation variant, Abstracts, Switches, data structures, hash bag variant, Complexity theory, Arrays, possibly changing context]
Towards precise metrics for predicting graph query performance
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Queries are the foundations of data intensive applications. In model-driven software engineering (MDSE), model queries are core technologies of tools and transformations. As software models are rapidly increasing in size and complexity, most MDSE tools frequently exhibit scalability issues that decrease developer productivity and increase costs. As a result, choosing the right model representation and query evaluation approach is a significant challenge for tool engineers. In the current paper, we aim to provide a benchmarking framework for the systematic investigation of query evaluation performance. More specifically, we experimentally evaluate (existing and novel) query and instance model metrics to highlight which provide sufficient performance estimates for different MDSE scenarios in various model query tools. For that purpose, we also present a comparative benchmark, which is designed to differentiate model representation and graph query evaluation approaches according to their performance when using large models and complex queries.
[Measurement, Model queries, Query metrics, MDSE tools, graph query performance prediction, Scalability, model representation, Unified modeling language, graph theory, software reliability, Resource description framework, Engines, instance model metrics, data models, query processing, Model metrics, Query processing, model-driven software engineering, software models, query evaluation approach, Benchmark testing, Performance benchmark, data intensive applications, model queries]
TzuYu: Learning stateful typestates
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Behavioral models are useful for various software engineering tasks. They are, however, often missing in practice. Thus, specification mining was proposed to tackle this problem. Existing work either focuses on learning simple behavioral models such as finite-state automata, or relies on techniques (e.g., symbolic execution) to infer finite-state machines equipped with data states, referred to as stateful typestates. The former is often inadequate as finite-state automata lack expressiveness in capturing behaviors of data-rich programs, whereas the latter is often not scalable. In this work, we propose a fully automated approach to learn stateful typestates by extending the classic active learning process to generate transition guards (i.e., propositions on data states). The proposed approach has been implemented in a tool called TzuYu and evaluated against a number of Java classes. The evaluation results show that TzuYu is capable of learning correct stateful typestates more efficiently.
[Java, data-rich programs, Learning automata, data mining, Educational institutions, specification mining, formal specification, behavioral model learning, Support vector machines, fully automated approach, TzuYu, stateful typestate learning, transition guards, Automata, data state propositions, Java classes, Concrete, software engineering tasks, learning (artificial intelligence), classic active learning process, Testing]
Mining branching-time scenarios
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Specification mining extracts candidate specification from existing systems, to be used for downstream tasks such as testing and verification. Specifically, we are interested in the extraction of behavior models from execution traces. In this paper we introduce mining of branching-time scenarios in the form of existential, conditional Live Sequence Charts, using a statistical data-mining algorithm. We show the power of branching scenarios to reveal alternative scenario-based behaviors, which could not be mined by previous approaches. The work contrasts and complements previous works on mining linear-time scenarios. An implementation and evaluation over execution trace sets recorded from several real-world applications shows the unique contribution of mining branching-time scenarios to the state-of-the-art in specification mining.
[Context, Weight measurement, program testing, conditional live sequence charts, data mining, Educational institutions, Data mining, branching-time scenarios mining, statistical data-mining algorithm, formal verification, Semantics, Abstracts, behavior models extraction, statistical analysis, Testing]
Measuring the structural complexity of feature models
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.
[Measurement, software product lines, feature models, Frequency modulation, structural complexity metric, NP complete problems, practical relevant analysis, state-of-the-art analysis tools, graph theory, Data structures, Encoding, Complexity theory, SOTA analysis tools, automated analysis, performance measurement, Analytical models, software product line, Boolean functions, optimisation, heuristics, feature model, exponential worst case execution time, computational complexity]
Scalable product line configuration: A straw to break the camel's back
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a &#x201C;seed&#x201D; in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.
[competing objectives, Linux kernel, evolutionary algorithms, SMT solvers, Optimization, scalable product line configuration, precomputed solution, seed solution, multiobjective optimization, Analytical models, feature-rich seed, Sociology, IBEA, automated configuration, static learning, learning (artificial intelligence), Variability models, model structure, evolutionary learning, software product lines, operating system kernels, indicator-based evolutionary algorithm, Biological system modeling, Statistics, evolutionary computation, Linux, randomly-generated initial population, Software]
Software analytics for incident management of online services: An experience report
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
As online services become more and more popular, incident management has become a critical task that aims to minimize the service downtime and to ensure high quality of the provided services. In practice, incident management is conducted through analyzing a huge amount of monitoring data collected at runtime of a service. Such data-driven incident management faces several significant challenges such as the large data scale, complex problem space, and incomplete knowledge. To address these challenges, we carried out two-year software-analytics research where we designed a set of novel data-driven techniques and developed an industrial system called the Service Analysis Studio (SAS) targeting real scenarios in a large-scale online service of Microsoft. SAS has been deployed to worldwide product datacenters and widely used by on-call engineers for incident management. This paper shares our experience about using software analytics to solve engineers' pain points in incident management, the developed data-analysis techniques, and the lessons learned from the process of research development and technology transfer.
[Measurement, service incident diagnosis, Service Analysis Studio, data-driven techniques, Servers, large data scale, Runtime, Microsoft, data-driven incident management, data-analysis techniques, Monitoring, software analytics, worldwide product datacenters, online services, software-analytics research, Radiation detectors, program diagnostics, large-scale online service, technology transfer, research development, Synthetic aperture sonar, computer centres, monitoring data, incident management, Software, data handling, Internet, Online service]
A comparative analysis of software architecture recovery techniques
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Many automated techniques of varying accuracy have been developed to help recover the architecture of a software system from its implementation. However, rigorously assessing these techniques has been hampered by the lack of architectural &#x201C;ground truths&#x201D;. Over the past several years, we have collected a set of eight architectures that have been recovered from open-source systems and independently, carefully verified. In this paper, we use these architectures as ground truths in performing a comparative analysis of six state-of-the-art software architecture recovery techniques. We use a number of metrics to assess each technique for its ability to identify a system's architectural components and overall architectural structure. Our results suggest that two of the techniques routinely outperform the rest, but even the best of the lot has surprisingly low accuracy. Based on the empirical data, we identify several avenues of future research in software architecture recovery.
[Algorithm design and analysis, ground truths, Java, software system architecture recovery techniques, Vectors, software maintenance, system architectural components, architectural structure, software architecture, Accuracy, Software architecture, formal verification, Clustering algorithms, Computer architecture, open source systems]
Towards contextual and on-demand code clone management by continuous monitoring
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Effective clone management is essential for developers to recognize the introduction and evolution of code clones, to judge their impact on software quality, and to take appropriate measures if required. Our previous study shows that cloning practice is not simply a technical issue. It must be interpreted and considered in a larger context from technical, personal, and organizational perspectives. In this paper, we propose a contextual and on-demand code clone management approach called CCEvents (Code Cloning Events). Our approach provides timely notification about relevant code cloning events for different stakeholders through continuous monitoring of code repositories. It supports on-demand customization of clone monitoring strategies in specific technical, personal, and organizational contexts using a domain-specific language. We implemented the proposed approach and conducted an empirical study with an industrial project. The results confirm the requirements for contextual and on-demand code clone management and show the effectiveness of CCEvents in providing timely code cloning notifications and in helping to achieve effective clone management.
[Context, source code (software), continuous monitoring, code cloning notifications, code repositories continuous monitoring, Navigation, contextual and on-demand code clone management approach, CCEvents, Cloning, software quality, code cloning events, domain-specific language, Organizations, Detectors, clone monitoring strategies, Outsourcing, Monitoring, organizational contexts]
The potential of polyhedral optimization: An empirical study
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Present-day automatic optimization relies on powerful static (i.e., compile-time) analysis and transformation methods. One popular platform for automatic optimization is the polyhedron model. Yet, after several decades of development, there remains a lack of empirical evidence of the model's benefits for real-world software systems. We report on an empirical study in which we analyzed a set of popular software systems, distributed across various application domains. We found that polyhedral analysis at compile time often lacks the information necessary to exploit the potential for optimization of a program's execution. However, when conducted also at run time, polyhedral analysis shows greater relevance for real-world applications. On average, the share of the execution time amenable to polyhedral optimization is increased by a factor of nearly 3. Based on our experimental results, we discuss the merits and potential of polyhedral optimization at compile time and run time.
[present-day automatic optimization, optimising compilers, program diagnostics, static analysis, program execution, Time measurement, Multimedia communication, polyhedral analysis, Optimization, Analytical models, real-world software systems, Program processors, optimisation, polyhedral optimization, Benchmark testing, transformation methods, Arrays, polyhedron model]
Automated unit testing of large industrial embedded software using concolic testing
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Current testing practice in industry is often ineffective and slow to detect bugs, since most projects utilize manually generated test cases. Concolic testing alleviates this problem by automatically generating test cases that achieve high coverage. However, specialized execution platforms and resource constraints of embedded software hinder application of concolic testing to embedded software. To overcome these limitations, we have developed CONcrete and symBOLic (CONBOL) testing framework to unit test large size industrial embedded software automatically. To address the aforementioned limitations, CONBOL tests target units on a host PC platform by generating symbolic unit testing drivers/stubs automatically and applying heuristics to reduce false alarms caused by the imprecise drivers/stubs. We have applied CONBOL to four million lines long industrial embedded software and detected 24 new crash bugs. Furthermore, the development team of the target software adopted CONBOL to their development process to apply CONBOL to the revised target software regularly.
[automated unit testing, concrete and symbolic testing framework, symbolic unit testing drivers, program testing, CONBOL testing framework, false alarm reduction, embedded software resource constraints, large size industrial embedded software, specialized execution platforms, imprecise driver-stubs, Embedded software, concolic testing, crash bugs detection, heuristics, Computer bugs, embedded systems, Hardware, Arrays, host PC platform, Testing]
Minimizing CPU time shortage risks in integrated embedded software
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
A major activity in many industries is to integrate software artifacts such that the functional and performance requirements are properly taken care of. In this paper, we focus on the problem of minimizing the risk of CPU time shortage in integrated embedded systems. In order to minimize this risk, we manipulate the start time (offset) of the software executables such that the system real-time constraints are satisfied, and further, the maximum CPU time usage is minimized. We develop a number of search-based optimization algorithms, specifically designed to work for large search spaces, to compute offsets for concurrent software executables with the objective of minimizing CPU usage. We evaluated and compared our algorithms by applying them to a large automotive software system. Our experience shows that our algorithms can automatically generate offsets such that the maximum CPU usage is very close to the known lower bound imposed by the domain constraints. Further, our approach finds limits on the maximum CPU usage lower than those found by a random strategy, and is not slower than a random strategy. Finally, our work achieves better results than the CPU usage minimization techniques devised by domain experts.
[Industries, Software algorithms, search-based optimization algorithms, Synchronization, random strategy, integrated embedded software, integrated embedded systems, Automotive engineering, embedded systems, integrated software, Software, Real-time systems, CPU time shortage risks minimization, automotive software system, search problems]
Model based test validation and oracles for data acquisition systems
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
This paper presents an automated, model based test validation and oracle approach for systems with complex input and output structures, such as Data Acquisition (DAQ) systems, which are common in many sectors including the satellite communications industry. We present a customised modelling methodology for such systems and a tool that automatically validates test inputs and, after test execution, applies an oracle that is based on mappings between the input and output. We also apply our proposed approach and tool to a complex industrial DAQ system and investigate the scalability and effectiveness of the approach in validating test cases, the DAQ system, or its specifications (captured as models). The results of the case study show that the approach is indeed scalable with respect to two dimensions: (1) model size and (2) test validation and oracle execution time. The size of the model for the DAQ system under study remains within practical bounds, and far below that of typical system models, as it includes a class diagram with 68 classes and 49 constraints. The developed test validation and oracles tool can handle satellite transmission files up to two GB within practical time constraints, taking, on a standard PC, less than three minutes for test validation and less than 50 minutes for applying the oracle. The approach was also effective in automatically applying the oracle successfully for the actual test suite of the DAQ system, accurately identifying all issues and violations that were expected, thus showing that an approach based on models can be sufficiently accurate.
[program testing, program verification, time constraints, Unified modeling language, Complexity theory, automated model based test validation, test execution, customised modelling methodology, automatic testing, complex input structures, oracle execution time, complex output structures, data acquisition systems, data acquisition, Testing, Context, class diagram, Unified Modeling Language, Data acquisition, standard PC, complex industrial DAQ system, satellite transmission file handling, file organisation, Data models, satellite communication industry, Context modeling]
Automated verification of interactive rule-based configuration systems
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Rule-based specifications of systems have again become common in the context of product line variability modeling and configuration systems. In this paper, we define a logical foundation for rule-based specifications that has enough expressivity and operational behavior to be practically useful and at the same time enables decidability of important overall properties such as consistency or cycle-freeness. Our logic supports rule-based interactive user transitions as well as the definition of a domain theory via rule transitions. As a running example, we model DOPLER, a rule-based configuration system currently in use at Siemens.
[Context, rule-based specification, Redundancy, Calculus, formal specification, interactive rule-based configuration system, formal logic, Computer languages, product line variability modeling, formal verification, Semantics, knowledge based systems, interactive systems, rule transition, Silicon, DOPLER, Slabs, automated verification, rule-based interactive user transition, domain theory]
AutoComment: Mining question and answer sites for automatic comment generation
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Code comments improve software maintainability. To address the comment scarcity issue, we propose a new automatic comment generation approach, which mines comments from a large programming Question and Answer (Q&amp;A) site. Q&amp;A sites allow programmers to post questions and receive solutions, which contain code segments together with their descriptions, referred to as code-description mappings.We develop AutoComment to extract such mappings, and leverage them to generate description comments automatically for similar code segments matched in open-source projects. We apply AutoComment to analyze Java and Android tagged Q&amp;A posts to extract 132,767 code-description mappings, which help AutoComment to generate 102 comments automatically for 23 Java and Android projects. The user study results show that the majority of the participants consider the generated comments accurate, adequate, concise, and useful in helping them understand the code.
[automatic comment generation, public domain software, Humanoid robots, data mining, code-description mapping extraction, natural language processing for software engineering, programming question-and-answer site mining, open-source projects, code segments, software maintainability improvement, Android (operating system), Databases, Java tagged Q&amp;A post analysis, Natural language processing, software engineering, Android tagged Q&amp;A post analysis, code comments, program comprehension, Java, automated comment generation, natural language processing, Cloning, documentation, comment scarcity issue, software maintenance, AutoComment, Software, Androids, question answering (information retrieval)]
Detecting system use cases and validations from documents
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Identifying system use cases and corresponding validations involves analyzing large requirement documents to understand the descriptions of business processes, rules and policies. This consumes a significant amount of effort and time. We discuss an approach to automate the detection of system use cases and corresponding validations from documents. We have devised a representation that allows for capturing the essence of rule statements as a composition of atomic `Rule intents' and key phrases associated with the intents. Rule intents that co-occur frequently constitute `Rule acts' analogous to the Speech acts in Linguistics. Our approach is based on NLP techniques designed around this Rule Model. We employ syntactic and semantic NL analyses around the model to identify and classify rules and annotate them with Rule acts. We map the Rule acts to business process steps and highlight the combinations as potential system use cases and validations for human supervision.
[Access control, atomic rule intents, linguistics, program verification, automatic system use case detection, validation detection, Switches, Manuals, rule acts, business processes, business rules, rules annotation, business policies, Rule intents, Databases, knowledge based systems, Requirement documents, syntactic NL analysis, NLP techniques, rule model, validations, requirement document analysis, Business, document handling, Rule types, natural language processing, Rule acts, human supervision, Rules, rule classification, semantic NL analysis, Insurance, User interfaces, NL analyses, speech acts, rule statements, System use cases, business data processing, rule identification]
Multi-user variability configuration: A game theoretic approach
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Multi-user configuration is a neglected problem in variability-intensive systems area. The appearance of conflicts among user configurations is a main concern. Current approaches focus on avoiding such conflicts, applying the mutual exclusion principle. However, this perspective has a negative impact on users satisfaction, who cannot make any decision fairly. In this work, we propose an interpretation of multi-user configuration as a game theoretic problem. Game theory is a well-known discipline which analyzes conflicts and cooperation among intelligent rational decision-makers. We present a taxonomy of multi-user configuration approaches, and how they can be interpreted as different problems of game theory. We focus on cooperative game theory to propose and automate a tradeoff-based bargaining approach, as a way to solve the conflicts and maximize user satisfaction at the same time.
[Frequency modulation, Video on demand, intelligent rational decision-makers, Smart homes, game theory, Proposals, Game theory, Games, game theoretic approach, multiuser configuration taxonomy, tradeoff-based bargaining approach, human computer interaction, Internet, multiuser variability configuration, cooperative game theory]
From comparison matrix to Variability Model: The Wikipedia case study
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Product comparison matrices (PCMs) provide a convenient way to document the discriminant features of a family of related products and now abound on the internet. Despite their apparent simplicity, the information present in existing PCMs can be very heterogeneous, partial, ambiguous, hard to exploit by users who desire to choose an appropriate product. Variability Models (VMs) can be employed to formulate in a more precise way the semantics of PCMs and enable automated reasoning such as assisted configuration. Yet, the gap between PCMs and VMs should be precisely understood and automated techniques should support the transition between the two. In this paper, we propose variability patterns that describe PCMs content and conduct an empirical analysis of 300+ PCMs mined from Wikipedia. Our findings are a first step toward better engineering techniques for maintaining and configuring PCMs.
[Electronic publishing, Encyclopedias, Color, variability patterns, Wikipedia, Electronic mail, discriminant feature documentation, matrix algebra, PCM mining, Phase change materials, automated reasoning, variability model, assisted configuration, Internet, Web sites, product comparison matrices]
Learning effective query transformations for enhanced requirements trace retrieval
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
In automated requirements traceability, significant improvements can be realized through incorporating user feedback into the trace retrieval process. However, existing feedback techniques are designed to improve results for individual queries. In this paper we present a novel technique designed to extend the benefits of user feedback across multiple trace queries. Our approach, named Trace Query Transformation (TQT), utilizes a novel form of Association Rule Mining to learn a set of query transformation rules which are used to improve the efficacy of future trace queries. We evaluate TQT using two different kinds of training sets. The first represents an initial set of queries directly modified by human analysts, while the second represents a set of queries generated by applying a query optimization process based on initial relevance feedback for trace links between a set of source and target documents. Both techniques are evaluated using requirements from theWorldVista Healthcare system, traced against certification requirements for the Commission for Healthcare Information Technology. Results show that the TQT technique returns significant improvements in the quality of generated trace links.
[text analysis, software engineering activities, requirements traceability, data mining, Medical services, Manuals, automated requirements traceability, association rules, source documents, Training, query processing, Itemsets, formal verification, text mining, learning (artificial intelligence), certification requirements, health care, effective query transformation learning, query optimization process, TQT technique, program diagnostics, Commission for Healthcare Information Technology, Educational institutions, WorldVista Healthcare system, Association rules, machine learning, Standards, association rule mining, target documents, trace query transformation, contractual requirements, relevance feedback, requirement trace retrieval enhancement process, query replacement, training sets, user feedback, medical computing]
Environment rematching: Toward dependability improvement for self-adaptive applications
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Self-adaptive applications can easily contain faults. Existing approaches detect faults, but can still leave some undetected and manifesting into failures at runtime. In this paper, we study the correlation between occurrences of application failure and those of consistency failure. We propose fixing consistency failure to reduce application failure at runtime. We name this environment rematching, which can systematically reconnect a self-adaptive application to its environment in a consistent way. We also propose enforcing atomicity for application semantics during the rematching to avoid its side effect. We evaluated our approach using 12 self-adaptive robot-car applications by both simulated and real experiments. The experimental results confirmed our approach's effectiveness in improving dependability for all applications by 12.5-52.5%.
[Legged locomotion, Correlation, environment rematching, automobiles, consistency failure, self-adjusting systems, application failure, mobile robots, Noise measurement, dependability improvement, self-adaptive robot-car applications, application semantics, Semantics, Consistency failure, Robot sensing systems]
Cloud Twin: Native execution of android applications on the Windows Phone
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
To successfully compete in the software marketplace, modern mobile applications must run on multiple competing platforms, such as Android, iOS, and Windows Phone. Companies producing mobile applications spend substantial amounts of time, effort, and money to port applications across platforms. Creating individual program versions for different platforms further exacerbates the maintenance burden. This paper presents Cloud Twin, a novel approach to natively executing the functionality of a mobile application written for another platform. The functionality is accessed by means of dynamic cross-platform replay, in which the source application's execution in the cloud is mimicked natively on the target platform. The reference implementation of Cloud Twin natively emulates the behavior of Android applications on a Windows Phone. Specifically, Cloud Twin transmits, via web sockets, the UI actions performed on the Windows Phone to the cloud server, which then mimics the received actions on the Android emulator. The UI updates on the emulator are efficiently captured by means of Aspect Oriented Programming and sent back to be replayed on the Windows Phone. Our case studies with third-party applications indicate that the Cloud Twin approach can become a viable solution to the heterogeneity of the mobile application market.
[Windows phone, cloud server, aspect oriented programming, software marketplace, DP industry, Mobile communication, Servers, cloud twin, mobile computing, dynamic cross-platform replay, Sockets, Layout, XML, market opportunities, mobile applications, Android applications, aspect-oriented programming, operating systems (computers), Androids, cloud computing, Smart phones]
SBFR: A search based approach for reproducing failures of programs with grammar based input
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Reproducing field failures in-house, a step developers must perform when assigned a bug report, is an arduous task. In most cases, developers must be able to reproduce a reported failure using only a stack trace and/or some informal description of the failure. The problem becomes even harder for the large class of programs whose input is highly structured and strictly specified by a grammar. To address this problem, we present SBFR, a search-based failure-reproduction technique for programs with structured input. SBFR formulates failure reproduction as a search problem. Starting from a reported failure and a limited amount of dynamic information about the failure, SBFR exploits the potential of genetic programming to iteratively find legal inputs that can trigger the failure.
[search based approach, genetic programming, search based failure reproduction technique, search problem, dynamic programming, Search problems, informal description, Grammar, genetic algorithms, Statistics, system recovery, program failures, Genetic algorithms, bug report, grammars, dynamic information, Sociology, Production, field failures, Trajectory, SBFR, search problems, grammar based input]
PYTHIA: Generating test cases with oracles for JavaScript applications
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Web developers often write test cases manually using testing frameworks such as Selenium. Testing JavaScript-based applications is challenging as manually exploring various execution paths of the application is difficult. Also JavaScript's highly dynamic nature as well as its complex interaction with the DOM make it difficult for the tester to achieve high coverage. We present a framework to automatically generate unit test cases for individual JavaScript functions. These test cases are strengthened by automatically generated test oracles capable of detecting faults in JavaScript code. Our approach is implemented in a tool called Pythia. Our preliminary evaluation results point to the efficacy of the approach in achieving high coverage and detecting faults.
[Measurement, automatic test software, faults detection, program testing, PYTHIA, complex interaction, JavaScript applications, Reactive power, Runtime, oracles, Web developer, JavaScript functions, JavaScript, document object model, software tools, Testing, Java, DOM, Instruments, software testing, JavaScript code, Browsers, automatic unit test case generation, software fault tolerance, automatic test oracles generation, test generation, manually execution path exploration, software tool, Web services]
Randomizing regression tests using game theory
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
As software evolves, the number of test-cases in the regression test suites continues to increase, requiring testers to prioritize their execution. Usually only a subset of the test cases is executed due to limited testing resources. This subset is often known to the developers who may try to &#x201C;game&#x201D; the system by committing insufficiently tested code for parts of the software that will not be tested. In this new ideas paper, we propose a novel approach for randomizing regression test scheduling, based on Stackelberg games for deployment of scarce resources. We apply this approach to randomizing test cases in such a way as to maximize the testers' expected payoff when executing the test cases. Our approach accounts for resource limitations (e.g., number of testers) and provides a probabilistic distribution for scheduling test cases. We provide an example application of our approach showcasing the idea of using Stackelberg games for randomized regression test scheduling.
[randomizing regression tests, Schedules, program testing, game theory, regression analysis, randomizing regression test scheduling, Vectors, Stackelberg games, Security, statistical distributions, Game theory, Equations, processor scheduling, software evolution, regression test suites, randomized regression test scheduling, Games, scheduling test cases, probabilistic distribution, Testing]
Automated inference of classifications and dependencies for combinatorial testing
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Even for small programs, the input space is huge - often unbounded. Partition testing divides the input space into disjoint equivalence classes and combinatorial testing selects a subset of all possible input class combinations, according to criteria such as pairwise coverage. The down side of this approach is that the partitioning of the input space into equivalence classes (input classification) is done manually. It is expensive and requires deep domain and implementation understanding. In this paper, we propose a novel approach to classify test inputs and their dependencies automatically. Firstly, random (or automatically generated) input vectors are sent to the system under test (SUT). For each input vector, an observed &#x201C;hit vector&#x201D; is produced by monitoring the execution of the SUT. Secondly, hit vectors are grouped into clusters using machine learning. Each cluster contains similar hit vectors, i.e., similar behaviors, and from them we obtain corresponding clusters of input vectors. Input classes are then extracted for each input parameter straightforwardly. Our experiments with a number of subjects show good results as the automatically generated classifications are the same or very close to the expected ones.
[program testing, Servers, invariant inference, input parameter, Systematics, disjoint equivalence classes, Clustering algorithms, automated inference, Automated input classifications, learning (artificial intelligence), Testing, input class combinations, input classification, pattern classification, hit vector, implementation understanding, pairwise coverage, reverse engineering, Vectors, partition testing, system under test, inference mechanisms, machine learning, deep domain understanding, combinatorial testing, SUT, Support vector machine classification, Concrete]
Adding context to fault localization with integration coverage
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Fault localization is a costly task in the debugging process. Several techniques to automate fault localization have been proposed aiming at reducing effort and time spent. Some techniques use heuristics based on code coverage data. The goal is to indicate program code excerpts more likely to contain faults. The coverage data mostly used in automated debugging is based on white-box unit testing (e.g., statements, basic blocks, predicates). This paper presents a technique which uses integration coverage data to guide the fault localization process. By ranking most suspicious pairs of method invocations, roadmaps-sorted lists of methods to be investigated-are created. At each method, unit coverage (e.g., basic blocks) is used to locate the fault site. Fifty-five bugs of four programs containing 2K to 80K lines of code (LOC) were analyzed. The results indicate that, by using the roadmaps, the effectiveness of the fault localization process is improved: 78% of all the faults are reached within a fixed amount of basic blocks; 40% more than an approach based on the Tarantula technique. Furthermore, fewer blocks have to be investigated until reaching the fault.
[Context, Integration coverage, program debugging, Fault localization, Statistical analysis, program testing, Debugging, Educational institutions, Coverage-based debugging, program code excerpts, fault localization process, fault localization automation, Computer bugs, debugging process, Libraries, Tarantula technique, integration coverage, Testing, white-box unit testing]
Using automatically generated invariants for regression testing and bug localization
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We present Preambl, an approach that applies automatically generated invariants to regression testing and bug localization. Our invariant generation methodology is Precis, an automatic and scalable engine that uses program predicates to guide clustering of dynamically obtained path information. In this paper, we apply it for regression testing and for capturing program predicates information to guide statistical analysis based bug localization. We present a technique to localize bugs in paths of variable lengths. We are able to map the localized post-deployment bugs on a path to pre-release invariants generated along that path. Our experimental results demonstrate the efficacy of the use of PRECIS for regression testing, as well as the ability of Preambl to zone in on relevant segments of program paths.
[program debugging, Target tracking, Statistical analysis, Instruments, regression testing, regression analysis, path information, automatically generated invariants, scalable engine, PRECIS, Preambl, Measurement units, statistical analysis based bug localization, Computer bugs, Software, invariant generation methodology, localized post deployment bugs, Testing]
Class level fault prediction using software clustering
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Defect prediction approaches use software metrics and fault data to learn which software properties associate with faults in classes. Existing techniques predict fault-prone classes in the same release (intra) or in a subsequent releases (inter) of a subject software system. We propose an intra-release fault prediction technique, which learns from clusters of related classes, rather than from the entire system. Classes are clustered using structural information and fault prediction models are built using the properties of the classes in each cluster. We present an empirical investigation on data from 29 releases of eight open source software systems from the PROMISE repository, with predictors built using multivariate linear regression. The results indicate that the prediction models built on clusters outperform those built on all the classes of the system.
[Measurement, program debugging, public domain software, regression analysis, subject software system, Predictive models, Fault Prediction, Open source software, Accuracy, defect prediction, Clustering algorithms, class level fault prediction, open source software systems, Empirical Study, fault-prone class prediction, software properties, Linear regression, multivariate linear regression, software fault tolerance, software clustering, pattern clustering, PROMISE repository, Software Clustering, intrarelease fault prediction technique, software metrics]
ExPort: Detecting and visualizing API usages in large source code repositories
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
This paper presents a technique for automatically mining and visualizing API usage examples. In contrast to previous approaches, our technique is capable of finding examples of API usage that occur across several functions in a program. This distinction is important because of a gap between what current API learning tools provide and what programmers need: current tools extract relatively small examples from single files/functions, even though programmers use APIs to build large software. The small examples are helpful in the initial stages of API learning, but leave out details that are helpful in later stages. Our technique is intended to fill this gap. It works by representing software as a Relational Topic Model, where API calls and the functions that use them are modeled as a document network. Given a starting API, our approach can recommend complex API usage examples mined from a repository of over 14 million Java methods.
[source code (software), Visualization, visualization, application program interfaces, data mining, ExPort, Databases, document network, relational topic model, Prototypes, learning (artificial intelligence), API usage, Portfolios, API usage visualization, Java, code search, automatic API usage mining, large source code repositories, Java methods, Software, Concrete, API learning tools, program visualisation, call graph, API usage detection]
Flow Permissions for Android
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
This paper proposes Flow Permissions, an extension to the Android permission mechanism. Unlike the existing permission mechanism our permission mechanism contains semantic information based on information flows. Flow Permissions allow users to examine and grant explicit information flows within an application (e.g., a permission for reading the phone number and sending it over the network) as well as implicit information flows across multiple applications (e.g., a permission for reading the phone number and sending it to another application already installed on the user's phone). Our goal with Flow Permissions is to provide visibility into the holistic behavior of the applications installed on a user's phone. Our evaluation compares our approach to dynamic flow tracking techniques; our results with 600 popular applications and 1,200 malicious applications show that our approach is practical and effective in deriving Flow Permissions statically.
[Java, Android (operating system), Humanoid robots, Seals, information flows, MySpace, Browsers, flow permissions, Android permission mechanism, Androids, semantic information, Smart phones]
A pattern-based approach to parametric specification mining
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
This paper presents a technique for using execution traces to mine parametric temporal specifications in the form of quantified event automata (QEA) - previously introduced as an expressive and efficient formalism for runtime verification. We consider a pattern-based mining approach that uses a pattern library to generate and check potential properties over given traces, and then combines successful patterns. By using predefined models to measure the tool's precision and recall we demonstrate that our approach can effectively and efficiently extract specifications in realistic scenarios.
[pattern-based mining approach, pattern library, program verification, automata theory, data mining, execution traces, Complexity theory, parametric specification mining, QEA, formal specification, software libraries, quantified event automata, Training, Satellites, Runtime, Automata, Libraries, pattern-based approach, parametric temporal specifications, runtime verification, Pattern matching]
Semi-automatic generation of metamodels from model sketches
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Traditionally, metamodeling is an upfront activity performed by experts for defining modeling languages. Modeling tools then typically restrict modelers to using only constructs defined in the metamodel. This is inappropriate when users want to sketch graphical models without any restrictions and only later assign meanings to the sketched elements. Upfront metamodeling also complicates the creation of domain-specific languages, as it requires experts with both domain and metamodeling expertise. In this paper we present a new approach that supports modelers in creating metamodels for diagrams they have sketched or are currently sketching. Metamodels are defined in a semi-automatic, interactive way by annotating diagram elements and automated model analysis. Our approach requires no metamodeling expertise and supports the co-evolution of models and meta-models.
[Context, automated model analysis, Adaptation models, domain-specific languages, metamodel, Computational modeling, inference, end-user, Unified modeling language, model sketches, Metamodeling, SysML, diagram elements annotation, metamodels semiautomatic generation, semiautomated, computer graphics, graphical models, Sketch, Libraries, model, DSL]
Assessing the maturity of requirements through argumentation: A good enough approach
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Requirements engineers need to be confident that enough requirements analysis has been done before a project can move forward. In the context of KAOS, this information can be derived from the soundness of the refinements: sound refinements indicate that the requirements in the goal-graph are mature enough or good enough for implementation. We can estimate how close we are to `good enough' requirements using the judgments of experts and other data from the goals. We apply Toulmin's model of argumentation to evaluate how sound refinements are. We then implement the resulting argumentation model using Bayesian Belief Networks and provide a semi-automated way aided by Natural Language Processing techniques to carry out the proposed evaluation. We have performed an initial validation on our work using a small case-study involving an electronic document management system.
[Context, document handling, natural language processing, goal graph, electronic document management system, Educational institutions, Probability distribution, Soundness of refinements, requirements analysis, formal verification, maturity of requirements, Tagging, natural language processing techniques, Natural language processing, KAOS context, Bayes methods, sound refinements, Reliability, requirements engineers, Bayesian belief networks]
Natural language requirements quality analysis based on business domain models
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Quality of requirements written in natural language has always been a critical concern in software engineering. Poorly written requirements lead to ambiguity and false interpretation in different phases of a software delivery project. Further, incomplete requirements lead to partial implementation of the desired system behavior. In this paper, we present a model for harvesting domain (functional or business) knowledge. Subsequently we present natural language processing and ontology based techniques for leveraging the model to analyze requirements quality and for requirements comprehension. The prototype also provides an advisory to business analysts so that the requirements can be aligned to the expected domain standard. The prototype developed is currently being used in practice, and the initial results are very encouraging.
[domain knowledge, ontology based techniques, natural language processing, business domain models, business analysts, Ontology, Natural languages, OWL, Natural Language Processing, Ontologies, Requirements Engineering, software delivery project, Standards, requirements comprehension, Analytical models, natural language requirements quality analysis, Business Domain Modeling, systems analysis, ontologies (artificial intelligence), domain standard, software engineering, Portals, business data processing, Business]
Model/code co-refactoring: An MDE approach
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Model-driven engineering suggests that models are the primary artefacts of software development. This means that models may be refactored even after code has been generated from them, in which case the code must be changed to reflect the refactoring. However, as we show neither re-generating the code from the refactored model nor applying an equivalent refactoring to the generated code is sufficient to keep model and code in sync - rather, model and code need to be refactored jointly. To enable this, we investigate the technical requirements of model/code co-refactoring, and implement a model-driven solution that we evaluate using a set of open-source programs and their structural models. Results suggest that our approach is feasible.
[equivalent refactoring, Model-driven engineering, Java, Adaptation models, software development, Biological system modeling, Unified modeling language, model driven engineering, refactored model, code regeneration, Synchronization, program compilers, constraints, MDE, structural models, refactoring, model driven solution, software engineering, open source programs, DSL, code corefactoring]
Recovering model transformation traces using multi-objective optimization
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Model Driven Engineering (MDE) is based on a large set of models that are used and manipulated throughout the development cycle. These models are manually or automatically produced and/or exploited using model transformations. To allow engineers to maintain the models and track their changes, recovering transformation traces is essential. In this paper, we propose an automated approach, based on multi-objective optimization, to recover transformation traces between models. Our approach takes as input a source model in the form of a set of fragments (fragments are defined using the source meta-model cardinalities and OCL constraints), and a target model. The recovered transformation traces take the form of many-to-many mappings between the constructs of the two models.
[Context, program diagnostics, Unified modeling language, target model, development cycle, model driven engineering, many-to-many mappings, Vectors, Statistics, source meta-model cardinalities, Optimization, MDE, multiobjective optimization, optimisation, Sociology, model transformation trace recovery, Genetics, OCL constraints, software engineering]
Model repair and transformation with Echo
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Models are paramount in model-driven engineering. In a software project many models may coexist, capturing different views of the system or different levels of abstraction. A key and arduous task in this development method is to keep all such models consistent, both with their meta-models (and the respective constraints) and among themselves. This paper describes Echo, a tool that aims at simplifying this task by automating inconsistency detection and repair using a solver based engine. Consistency between different models can be specified by bidirectional model transformations, and is guaranteed to be recovered by minimal updates on the inconsistent models. The tool is freely available as an Eclipse plugin, developed on top of the popular EMF framework, and supports constraints and transformations specified in the OMG standard languages OCL and QVT-R, respectively.
[inconsistency detection automation, meta-models, Object oriented modeling, Computational modeling, Unified modeling language, Echo, OMG standard languages, Metals, Maintenance engineering, QVT-R, model repair, software maintenance, software project, Standards, bidirectional model transformations, Semantics, model-driven engineering, development method, EMF framework, OCL, Eclipse plugin, solver based engine]
Smart Cloud Broker: Finding your home in the clouds
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
As the rate of cloud computing adoption grows, so does the need for consumption assistance. Enterprises looking to migrate their IT systems to the cloud require assistance in identifying providers that offer resources with the most appropriate pricing and performance levels to match their specific business needs. In this paper, we present Smart Cloud Broker - a suite of software tools that allows cloud infrastructure consumers to evaluate and compare the performance of different Infrastructure as a Service (IaaS) offerings from competing cloud service providers, and consequently supports selection of the cloud configuration and provider with the specifications that best meet the user's requirements. Using Smart Cloud Broker, prospective cloud users can estimate the performance of the different cloud platforms by running live tests against representative benchmark applications under representative load conditions.
[Cloud computing, cloud service providers, program testing, IaaS, cloud configuration, Servers, Smart Cloud Broker, Databases, cloud infrastructure consumers, Pricing, Benchmark testing, cloud platform performance, software tools, cloud computing, Catalogs, software performance evaluation, Infrastructure as a Service]
OCRA: A tool for checking the refinement of temporal contracts
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Contract-based design enriches a component model with properties structured in pairs of assumptions and guarantees. These properties are expressed in term of the variables at the interface of the components, and specify how a component interacts with its environment: the assumption is a property that must be satisfied by the environment of the component, while the guarantee is a property that the component must satisfy in response. Contract-based design has been recently proposed in many methodologies for taming the complexity of embedded systems. In fact, contract-based design enables stepwise refinement, compositional verification, and reuse of components. However, only few tools exist to support the formal verification underlying these methods. OCRA (Othello Contracts Refinement Analysis) is a new tool that provides means for checking the refinement of contracts specified in a linear-time temporal logic. The specification language allows to express discrete as well as metric real-time constraints. The underlying reasoning engine allows checking if the contract refinement is correct. OCRA has been used in different projects and integrated in CASE tools.
[program verification, Unified modeling language, metric constraints, temporal logic, Cognition, formal specification, formal verification, specification languages, Model checking, contract-based design, software tools, linear-time temporal logic, Contracts, specification language, Context, compositional verification, object-oriented programming, OCRA tool, component interface model, stepwise refinement, CASE tools, Embedded systems, component reuse, reasoning engine, discrete constraints, Automata, temporal contract refinement checking, software reusability, Othello Contracts Refinement Analysis]
The bounded model checker LLBMC
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
This paper presents LLBMC, a tool for finding bugs and runtime errors in sequential C/C++ programs. LLBMC employs bounded model checking using an SMT-solver for the theory of bitvectors and arrays and thus achieves precision down to the level of single bits. The two main features of LLBMC that distinguish it from other bounded model checking tools for C/C++ are (i) its bit-precise memory model, which makes it possible to support arbitrary type conversions via stores and loads; and (ii) that it operates on a compiler intermediate representation and not directly on the source code.
[program debugging, runtime errors, SMT solver, source code, bounded model checking, bounded model checker LLBMC, Encoding, finding bugs, Decoding, C++ language, program compilers, Runtime, Program processors, formal verification, Computer bugs, sequential C/C++ programs, Model checking]
CSeq: A concurrency pre-processor for sequential C verification tools
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Sequentialization translates concurrent programs into equivalent nondeterministic sequential programs so that the different concurrent schedules no longer need to be handled explicitly. It can thus be used as a concurrency preprocessing technique for automated sequential program verification tools. Our CSeq tool implements a novel sequentialization for C programs using pthreads, which extends the Lal/Reps sequentialization to support dynamic thread creation. CSeq now works with three different backend tools, CBMC, ESBMC, and LLBMC, and is competitive with state-of-the-art verification tools for concurrent programs.
[program verification, Instruction sets, Programming, LLBMC, C language, parallel programming, automated sequential program verification tools, Concurrent computing, CSeq tool, Benchmark testing, ESBMC, concurrency preprocessing technique, dynamic thread creation, Context, concurrent program translation, C program sequentialization, CBMC, concurrency preprocessor, program interpreters, sequential C verification tools, backend tools, Lal-Reps sequentialization, concurrency control, equivalent nondeterministic sequential programs, pthreads]
Automated testing of cloud-based elastic systems with AUToCLES
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Cloud-based elastic computing systems dynamically change their resources allocation to provide consistent quality of service and minimal usage of resources in the face of workload fluctuations. As elastic systems are increasingly adopted to implement business critical functions in a cost-efficient way, their reliability is becoming a key concern for developers. Without proper testing, cloud-based systems might fail to provide the required functionalities with the expected service level and costs. Using system testing techniques, developers can expose problems that escaped the previous quality assurance activities and have a last chance to fix bugs before releasing the system in production. System testing of cloud-based systems accounts for a series of complex and time demanding activities, from the deployment and configuration of the elastic system, to the execution of synthetic clients, and the collection and persistence of execution data. Furthermore, clouds enable parallel executions of the same elastic system that can reduce the overall test execution time. However, manually managing the concurrent testing of multiple system instances might quickly overwhelm developers' capabilities, and automatic support for test generation, system test execution, and management of execution data is needed. In this demo we showcase AUToCLES, our tool for automatic testing of cloud-based elastic systems. Given specifications of the test suite and the system under test, AUToCLES implements testing as a service (TaaS): It automatically instantiates the SUT, configures the testing scaffoldings, and automatically executes test suites. If required, AUToCLES can generate new test inputs. Designers can inspect executions both during and after the tests.
[System testing, Cloud computing, testing as a service, program testing, Elasticity, cloud-based elastic computing system, automated testing, quality of service, resources allocation, Standards, test generation, SUT, business critical function, system testing technique, concurrent testing, AUToCLES, quality assurance, system test execution, TaaS, cloud computing, Monitoring]
Tool support for automatic model transformation specification using concrete visualisations
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Complex model transformation is crucial in several domains, including Model-Driven Engineering (MDE), information visualisation and data mapping. Most current approaches use meta-model-driven transformation specification via coding in textual scripting languages. This paper demonstrates a novel approach and tool support that instead provides for specification of correspondences between models using concrete visualisations of source and target models, and generates transformation scripts from these by-example model correspondence specifications.
[Solid modeling, Visualization, Design automation, Computational modeling, data mapping, formal specification, Data visualization, data visualisation, automatic model transformation specification, model-driven engineering, meta-model-driven transformation specification, concrete visualisations, Concrete, information visualisation, textual scripting languages]
CCmutator: A mutation generator for concurrency constructs in multithreaded C/C++ applications
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We introduce CCmutator, a mutation generation tool for multithreaded C/C++ programs written using POSIX threads and the recently standardized C++11 concurrency constructs. CCmutator is capable of performing partial mutations and generating higher order mutants, which allow for more focused and complex combinations of elementary mutation operators leading to higher quality mutants. We have implemented CCmutator based on the popular Clang/LLVM compiler framework, which allows CCmutator to be extremely scalable and robust in handling real-world C/C++ applications. CCmutator is also designed in such a way that all mutants of the same order can be generated in parallel, which allows the tool to be easily parallelized on commodity multicore hardware to improve performance.
[Software testing, elementary mutation operators, software development process, program testing, Instruction sets, commodity multicore hardware, program compilers, performance improvement, Concurrent computing, CCmutator, multithreaded C applications, software performance evaluation, Java, multiprocessing programs, multi-threading, mutation generation tool, software testing, multithreaded C++ applications, Clang-LLVM compiler framework, C++ language, POSIX threads, Synchronization, higher order mutant generation, standardized C++11 concurrency constructs, partial mutations, Computer bugs]
Crushinator: A framework towards game-independent testing
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Testing game applications relies heavily on beta testing methods. The effectiveness of beta testing depends on how well beta testers represent the common game-application users and if users are willing to participate in the beta test. An automated testing tool framework could reduce the dependence upon beta testing by most companies to analyze their game applications. This paper presents the Crushinator as one such framework. This framework provides a game-independent testing tool that implements multiple testing methods that can assist and possibly replace the use of beta testing.
[program testing, Unified modeling language, Servers, Engines, exploratory testing, event-driven applications, computer games, Games, Computer architecture, game-independent testing, automated testing tool, model-based testing, Crushinator, beta testing method, Testing, Load modeling]
Pex4Fun: A web-based environment for educational gaming via automated test generation
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Pex4Fun (http://www.pex4fun.com/) is a web-based educational gaming environment for teaching and learning programming and software engineering. Pex4Fun can be used to teach and learn programming and software engineering at many levels, from high school all the way through graduate courses. With Pex4Fun, a student edits code in any browser - with Intellisense - and Pex4Fun executes it and analyzes it in the cloud. Pex4Fun connects teachers, curriculum authors, and students in a unique social experience, tracking and streaming progress updates in real time. In particular, Pex4Fun finds interesting and unexpected input values (with Pex, an advanced test-generation tool) that help students understand what their code is actually doing. The real fun starts with coding duels where a student writes code to implement a teacher's secret specification (in the form of sample-solution code not visible to the student). Pex4Fun finds any discrepancies in behavior between the student's code and the secret specification. Such discrepancies are given as feedback to the student to guide how to fix the student's code to match the behavior of the secret specification. This tool demonstration shows how Pex4Fun can be used in teaching and learning, such as solving coding duels, exploring course materials in feature courses, creating and teaching a course, creating and publishing coding duels, and learning advanced topics behind Pex4Fun.
[high school, software engineering teaching, secret specification, teaching, cloud, Pex4Fun, coding duels, Education, computer games, graduate courses, software engineering, cloud computing, programming, Testing, Intellisense, computer science education, Encoding, automated test generation, Programming profession, programming teaching, educational courses, course materials, Games, advanced test-generation tool, Web-based educational gaming environment, computer aided instruction, Software engineering]
Developing self-verifying service-based systems
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We present a tool-supported framework for the engineering of service-based systems (SBSs) capable of self-verifying their compliance with developer-specified reliability requirements. These self-verifying systems select their services dynamically by using a combination of continual quantitative verification and online updating of the verified models. Our framework enables the practical exploitation of recent theoretical advances in the development of self-adaptive SBSs through (a) automating the generation of the software components responsible for model updating, continual verification and service selection; and (b) employing standard SBS development processes.
[Adaptation models, software component generation, model updating, object-oriented programming, developer-specified reliability requirements, Unified modeling language, software reliability, tool-supported framework, Scattering, Quality of service, online updating, self-verifying service-based system development, Analytical models, Web services, formal verification, Reliability, service selection, continual quantitative verification, self-adaptive SBSs development]
TRAM: A tool for transforming textual requirements into analysis models
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Tool support for automatically constructing analysis models from the natural language specification of requirements (NLR) is critical to model driven development (MDD), as it can bring forward the use of precise formal languages from the coding to the specification phase in the MDD lifecycle. TRAM provides such a support through a novel approach. By using a set of conceptual patterns to facilitate the transformation of an NLR to its target software model, TRAM has shown its potential as an automated tool to support the earliest phase of MDD. This paper describes TRAM and evaluates the tool against three benchmark approaches.
[formal languages, conceptual patterns, Object oriented modeling, natural language processing, program diagnostics, Unified modeling language, Natural languages, analysis models, software model, Model transformation, textual requirement transformation, formal specification, TRAM, tool support, Analytical models, model driven development, semantic object models, Semantics, natural language specification of requirements, Software, software tools]
iProbe: A lightweight user-level dynamic instrumentation tool
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We introduce a new hybrid instrumentation tool for dynamic application instrumentation called iProbe, which is flexible and has low overhead. iProbe takes a novel 2-stage design, and offloads much of the dynamic instrumentation complexity to an offline compilation stage. It leverages standard compiler flags to introduce &#x201C;place-holders&#x201D; for hooks in the program executable. Then it utilizes an efficient user-space &#x201C;HotPatching&#x201D; mechanism which modifies the functions to be traced and enables execution of instrumented code in a safe and secure manner. In its evaluation on a micro-benchmark and SPEC CPU2006 benchmark applications, the iProbe prototype achieved the instrumentation overhead an order of magnitude lower than existing state-of-the-art dynamic instrumentation tools like SystemTap and DynInst.
[program debugging, lightweight user-level dynamic instrumentation tool, dynamic application instrumentation, Scalability, Hotpatching, hybrid instrumentation tool, Production Systems, Complexity theory, SystemTap, DynInst, program compilers, iProbe, HotPatching mechanism, Low-Overhead, Linux, Benchmark testing, Tracing, Probes, Kernel, Monitoring]
Detecting and fixing emergent behaviors in Distributed Software Systems using a message content independent method
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
This research is intended to automatically detect emergent behaviors of scenario based Distributed Software Systems (DSS) in design phase. The direct significance of our work is reducing the cost of verifying DSS for unexpected behavior in execution time. Existing approaches have some drawbacks which we try to cover in our work. The main contributions are modeling the DSS components as a social network and not using behavioral modeling, detecting components with no emergent behavior, and investigating the interactions of instances of one type.
[Decision support systems, message content independent method, distributed software system, Conferences, Social network services, Unified modeling language, distributed processing, DSS component, Educational institutions, multiagent system, social network, emergent behavior, formal verification, Emergent behavior, Model checking, social networking (online), Multi-agent systems]
Synthesizing fault-tolerant programs from deontic logic specifications
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
We study the problem of synthesizing fault-tolerant components from specifications, i.e., the problem of automatically constructing a fault-tolerant component implementation from a logical specification of the component, and the system's required level of fault-tolerance. In our approach, the logical specification of the component is given in dCTL, a branching time temporal logic with deontic operators, especially designed for fault-tolerant component specification. The synthesis algorithm takes the component specification, and a user-defined level of fault-tolerance (masking, nonmasking, failsafe), and automatically determines whether a component with the required fault-tolerance is realizable. Moreover, if the answer is positive, then the algorithm produces such a fault-tolerant implementation. Our technique for synthesis is based on the use of (bi)simulation algorithms for capturing different fault-tolerance classes, and the extension of a synthesis algorithm for CTL to cope with dCTL specifications.
[Algorithm design and analysis, Temporal Logics, temporal logic, Cognition, fault-tolerant component specification, bisimulation algorithm, formal specification, Fault tolerance, deontic operators, Deontic logics, Fault tolerant systems, branching time temporal logic, Formal specification, Model checking, Safety, synthesis algorithm, dCTL specifications, logical specification, software fault tolerance, Correctness by construction, Writing, fault-tolerance required level, fault-tolerant program synthesis, Fault-tolerance, deontic logic specifications, Program Synthesis]
Supporting bug investigation using history analysis
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
In my research, I propose an automated technique to support bug investigation by using a novel analysis of the history of the source code. During the bug-fixing process, developers spend a high amount of manual effort investigating the bug in order to answer a series of questions about it. My research will support developers in answering the following questions about a bug: Who is the most suitable developer to fix the bug?, Where is the bug located?, When was the bug inserted? and Why was the bug inserted?
[bug-fixing process, Visualization, program debugging, Conferences, program diagnostics, Debugging, source code, History, bug investigation support, Computer bugs, Software, automated technique, history analysis, Software engineering]
Context-aware task allocation for distributed agile team
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
The philosophy of Agile software development advocates the spirit of open discussion and coordination among team members to adapt to incremental changes encountered during the process. Based on our observations from 20 agile student development teams over an 8-week study in Beihang University, China, we found that the task allocation strategy as a result of following the Agile process heavily depends on the experience of the users, and cannot be guaranteed to result in efficient utilization of team resources. In this research, we propose a context-aware task allocation decision support system that balances the considerations for quality and timeliness to improve the overall utility derived from an agile software development project.We formulate the agile process as a distributed constraint optimization problem, and propose a technology framework that assesses individual developers' situations based on data collected from a Scrum-based agile process, and helps individual developers make situation-aware decisions on which tasks from the backlog to select in real-time. Preliminary analysis and simulation results show that it can achieve close to optimally efficient utilization of the developers' collective capacity. We plan to build the framework into a computer-supported collaborative development platform and refine the method through more realistic projects.
[context-aware task allocation, distributed agile, software prototyping, Scrum-based agile process, team member coordination, distributed constraint optimization problem, team working, optimisation, technology framework, China, groupware, Variable speed drives, agile software development project, developer collective capacity, project management, computer-supported collaborative development platform, software development management, distributed agile team, Educational institutions, Indexes, task allocation strategy, team member discussion, Beihang University, task allocation, Software, Planning, Delays, Resource management, agile software development, context-aware task allocation decision support system]
Preventing erosion of architectural tactics through their strategic implementation, preservation, and visualization
2013 28th IEEE/ACM International Conference on Automated Software Engineering
None
2013
Nowadays, a successful software production is increasingly dependent on how the final deployed system addresses customers' and users' quality concerns such as security, reliability, availability, interoperability, performance and many other types of such requirements. In order to satisfy such quality concerns, software architects are accountable for devising and comparing various alternate solutions, assessing the trade-offs, and finally adopting strategic design decisions which optimize the degree to which each of the quality concerns is satisfied. Although designing and implementing a good architecture is necessary, it is not usually enough. Even a good architecture can deteriorate in subsequent releases and then fail to address those concerns for which it was initially designed. In this work, we present a novel traceability approach for automating the construction of traceabilty links for architectural tactics and utilizing those links to implement a change impact analysis infrastructure to mitigate the problem of architecture degradation. Our approach utilizes machine learning methods to detect tactic-related classes. The detected tactic-related classes are then mapped to a Tactic Traceability Pattern. We train our trace algorithm using code extracted from fifty performance-centric and safety-critical open source software systems and then evaluate it against a real case study.
[safety-critical software, reliability, software quality, software production, performance-centric open source software system, software architecture, security, change impact analysis infrastructure, Heart beat, Software architecture, software architect, architectural tactic, Computer architecture, tactic-related class, learning (artificial intelligence), tactic traceability pattern, Architecture, interoperability, Software reliability, machine learning, traceability, safety-critical open source software system, tactics, traceability patterns, Software, traceabilty link]
Message from the Chairs
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
ASE 2015 Organization
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Provides a listing of current committee members and society officers.
[]
Program Committee
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Provides a listing of current committee members and society officers.
[]
Doctoral Symposium and Tool Demonstrations Committees
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Provides a listing of current committee members and society officers.
[]
ASE Steering Committee and ASE Fellows
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Provides a listing of current committee members and society officers.
[]
Keynotes
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Provides an abstract for each of the keynote presentations and may include a brief professional biography of each
[]
Learning to Rank for Question-Oriented Software Text Retrieval (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Question-oriented text retrieval, aka natural language-based text retrieval, has been widely used in software engineering. Earlier work has concluded that questions with the same keywords but different interrogatives (such as how, what) should result in different answers. But what is the difference? How to identify the right answers to a question? In this paper, we propose to investigate the "answer style" of software questions with different interrogatives. Towards this end, we build classifiers in a software text repository and propose a re-ranking approach to refine search results. The classifiers are trained by over 16,000 answers from the StackOverflow forum. Each answer is labeled accurately by its question's explicit or implicit interrogatives. We have evaluated the performance of our classifiers and the refinement of our re-ranking approach in software text retrieval. Our approach results in 13.1% and 12.6% respectively improvement with respect to text retrieval criteria nDCG@1 and nDCG@10 compared to the baseline. We also apply our approach to FAQs of 7 open source projects and show 13.2% improvement with respect to nDCG@1. The results of our experiments suggest that our approach could find answers to FAQs more precisely.
[text analysis, software text repository, Buildings, StackOverflow forum, information retrieval, interrogative, text retrieval, Indexes, natural language-based text retrieval, Training, Search engines, learning-to-rank, classifier, Feature extraction, rank, Software, software engineering, learning (artificial intelligence), Software engineering, question-oriented software text retrieval, re-ranking approach]
Development Emails Content Analyzer: Intention Mining in Developer Discussions (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Written development communication (e.g. mailing lists, issue trackers) constitutes a precious source of information to build recommenders for software engineers, for example aimed at suggesting experts, or at redocumenting existing source code. In this paper we propose a novel, semi-supervised approach named DECA (Development Emails Content Analyzer) that uses Natural Language Parsing to classify the content of development emails according to their purpose (e.g. feature request, opinion asking, problem discovery, solution proposal, information giving etc), identifying email elements that can be used for specific tasks. A study based on data from Qt and Ubuntu, highlights a high precision (90%) and recall (70%) of DECA in classifying email content, outperforming traditional machine learning strategies. Moreover, we successfully used DECA for re-documenting source code of Eclipse and Lucene, improving the recall, while keeping high precision, of a previous approach based on ad-hoc heuristics.
[source code (software), DECA, Taxonomy, data mining, electronic mail, development emails content analyzer, Electronic mail, Proposals, program compilers, semisupervised approach, content classification, intention mining, Bandwidth, Empirical Study, software engineering, learning (artificial intelligence), pattern classification, software development, natural language processing, Natural languages, source code, Natural Language Processing, natural language parsing, Unstructured Data Mining, Pragmatics, recommender systems, recommender system, grammars, Software]
CodeExchange: Supporting Reformulation of Internet-Scale Code Queries in Context (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Programming today regularly involves searching for source code online, whether through a general search engine such as Google or a specialized code search engine such as SearchCode, Ohloh, or GitHub. Searching typically is an iterative process, with develop-ers adjusting the keywords they use based on the results of the previous query. However, searching in this manner is not ideal, because just using keywords places limits on what developers can express as well as the overall interaction that is required. Based on the observation that the results from one query create a con-text in which a next is formulated, we present CodeExchange, a new code search engine that we developed to explicitly leverage this context to support fluid, expressive reformulation of queries. We motivate the need for CodeExchange, highlight its key design decisions and overall architecture, and evaluate its use in both a field deployment and a laboratory study.
[source code (software), iterative methods, search engines, GitHub, Manuals, Programming, interface, query processing, field deployment, query reformulation, Search engines, context, CodeExchange, Face, Code search, Context, Google, Java, SearchCode, Ohloh, source code, iterative process, specialized code search engine, Internet-scale code queries, internet-scale, code search engine, Internet]
How do Developers Document Database Usages in Source Code? (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Database-centric applications (DCAs) usually contain a large number of tables, attributes, and constraints describing the underlying data model. Understanding how database tables and attributes are used in the source code along with the constraints related to these usages is an important component of DCA maintenance. However, documenting database-related operations and their constraints in the source code is neither easy nor common in practice. In this paper, we present a two-fold empirical study aimed at identifying how developers document database usages at source code method level. In particular, (i) we surveyed open source developers to understand their practices on documenting database usages in source code, and (ii) we mined a large set of open source projects to measure to what extent database-related methods are commented and if these comments are updated during evolution. Although 58% of the developers claimed to find value in method comments describing database usages, our findings suggest that 77% of 33K+ methods in 3.1K+ open-source Java projects with database accesses were completely undocumented.
[document handling, source code (software), Java, open source projects, database centric applications, public domain software, database related methods, Documentation, DCA maintenance, Electronic mail, database tables, documenting database related operations, Data mining, database management systems, software maintenance, underlying data model, source code method level, Databases, Software, Data models]
Efficient Data Model Verification with Many-Sorted Logic (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Misuse or loss of web application data can have catastrophic consequences in today's Internet oriented world. Hence, verification of web application data models is of paramount importance. We have developed a framework for verification of web application data models via translation to First Order Logic (FOL), followed by automated theorem proving. Due to the undecidability of FOL, this automated approach does not always produce a conclusive answer. In this paper, we investigate the use of many-sorted logic in data model verification in order to improve the effectiveness of this approach. Many-sorted logic allows us to specify type information explicitly, thus lightening the burden of reasoning about type information during theorem proving. Our experiments demonstrate that using many-sorted logic improves the verification performance significantly, and completely eliminates inconclusive results in all cases over 7 real world web applications, down from an 17% inconclusive rate.
[Java, FOL, Encoding, Verification, Complexity theory, Web application data model verification, Rails, formal verification, Many-Sorted Logic, Semantics, automated theorem proving, many-sorted logic, first order logic, Data models, Libraries, Internet, theorem proving, Logic, Data Models]
Synthesising Interprocedural Bit-Precise Termination Proofs (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Proving program termination is key to guaranteeing absence of undesirable behaviour, such as hanging programs and even security vulnerabilities such as denial-of-service attacks. To make termination checks scale to large systems, interprocedural termination analysis seems essential, which is a largely unexplored area of research in termination analysis, where most effort has focussed on difficult single-procedure problems. We present a modular termination analysis for C programs using template-based interprocedural summarisation. Our analysis combines a context-sensitive, over-approximating forward analysis with the inference of under-approximating preconditions for termination. Bit-precise termination arguments are synthesised over lexicographic linear ranking function templates. Our experimental results show that our tool 2LS outperforms state-of-the-art alternatives, and demonstrate the clear advantage of interprocedural reasoning over monolithic analysis in terms of efficiency, while retaining comparable precision.
[Algorithm design and analysis, bit-precise termination argument, monolithic analysis, lexicographic linear ranking function template, template-based interprocedural summarisation, inference, synthesising interprocedural bit-precise termination proof, C language, Computer crime, program termination, Semantics, termination analysis, program analysis, interprocedural analysis, theorem proving, denial-of-service attack, Context, synthesis, modular termination analysis, program diagnostics, abstract interpretation, large system, interprocedural reasoning, Encoding, context-sensitive forward analysis, over-approximating forward analysis, inference mechanisms, computer network security, interprocedural termination analysis, Computer bugs, C program, security vulnerability]
Interpolation Guided Compositional Verification (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Model checking suffers from the state space explosion problem. Compositional verification techniques such as assume-guarantee reasoning (AGR) have been proposed to alleviate the problem. However, there are at least three challenges in applying AGR. Firstly, given a system M1 ? M2, how do we automatically construct and refine (in the presence of spurious counterexamples) an assumption A2, which must be an abstraction of M2? Previous approaches suggest to incrementally learn and modify the assumption through multiple invocations of a model checker, which could be often time consuming. Secondly, how do we keep the state space small when checking M1 ? A2 = f if multiple refinements of A2 are necessary? Lastly, in the presence of multiple parallel components, how do we partition the components? In this work, we propose interpolation-guided compositional verification. The idea is to tackle three challenges by using interpolations to generate and refine the abstraction of M2, to abstract M1 at the same time (so that the state space is reduced even if A2 is refined all the way to M2), and to find good partitions. Experimental results show that the proposed approach outperforms existing approaches consistently.
[Radiation detectors, parallel components, automatic compositional verification, Cognition, Explosions, compositional verification techniques, AGR, parallel processing, state space explosion problem, Interpolation, Reactive power, interpolation, formal verification, model checking, interpolation guided compositional verification, satisfiability, Model checking, Computer security, assume-guarantee reasoning]
Crust: A Bounded Verifier for Rust (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Rust is a modern systems language that provides guaranteed memory safety through static analysis. However, Rust includes an escape hatch in the form of "unsafe code," which the compiler assumes to be memory safe and to preserve crucial pointer aliasing invariants. Unsafe code appears in many data structure implementations and other essential libraries, and bugs in this code can lead to memory safety violations in parts of the program that the compiler otherwise proved safe. We present CRUST, a tool combining exhaustive test generation and bounded model checking to detect memory safety errors, as well as violations of Rust's pointer aliasing invariants within unsafe library code. CRUST requires no programmer annotations, only an indication of the modules to check. We evaluate CRUSTon data structures from the Rust standard library. It detects memory safety bugs that arose during the library's development and remained undetected for several months.
[program debugging, data structure implementations, program testing, bounded model checking, program compilers, software libraries, formal verification, memory safety violations, Libraries, data structures, SMT-based verification, memory safety, Safety, exhaustive test generation, compiler, memory safety bugs, pointer aliasing invariants, Rust, program diagnostics, unsafe library code, static analysis, Indexes, Standards, test generation, CRUST, Computer bugs, Arrays]
Have We Seen Enough Traces? (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Dynamic specification mining extracts candidate specifications from logs of execution traces. Existing algorithms differ in the kinds of traces they take as input and in the kinds of candidate specification they present as output. One challenge common to all approaches relates to the faithfulness of the mining results: how can we be confident that the extracted specifications faithfully characterize the program we investigate? Since producing and analyzing traces is costly, how would we know we have seen enough traces? And, how would we know we have not wasted resources and seen too many of them?In this paper we address these important questions by presenting a novel, black box, probabilistic framework based on a notion of log completeness, and by applying it to three different well-known specification mining algorithms from the literature: k-Tails, Synoptic, and mining of scenario-based triggers and effects. Extensive evaluation over 24 models taken from 9 different sources shows the soundness, generalizability, and usefulness of the framework and its contribution to the state-of-the-art in dynamic specification mining.
[Adaptation models, program verification, Heuristic algorithms, Computational modeling, probabilistic framework, Estimation, data mining, probability, Specification Mining, Probabilistic logic, dynamic specification mining algorithm, Data mining, Servers, formal specification, program specification, k-Tails, black box, Synoptic]
Extracting Visual Contracts from Java Programs (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Visual contracts model the operations of components or services by pre-and post-conditions formalised as graph transformation rules. They provide a precise intuitive notation to support testing, understanding and analysis of software. However, due to their detailed specification of data states and transformations, modelling real applications is an error-prone process. In this paper we propose a dynamic approach to reverse engineering visual contracts from Java based on tracing the execution of Java operations. The resulting contracts give an accurate description of the observed object transformations, their effects and preconditions in terms of object structures, parameter and attribute values, and their generalised specification by universally quantified (multi) objects. While this paper focusses on the fundamental technique rather than a particular application, we explore potential uses in our evaluation, including in program understanding, review of test reports and debugging.
[Context, Visualization, Java, rule learning, software, Unified modeling language, reverse engineering visual contracts, object transformations, Java operations, multi objects, universally quantified objects, generalised specification, error-prone process, graph transformation, extraction of visual contracts, Cities and towns, graph transformation rules, Java programs, Contracts, Testing, visual contracts model]
Synergizing Specification Miners through Model Fissions and Fusions (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Software systems are often developed and released without formal specifications. For those systems that are formally specified, developers have to continuously maintain and update the specifications or have them fall out of date. To deal with the absence of formal specifications, researchers have proposed techniques to infer the missing specifications of an implementation in a variety of forms, such as finite state automaton (FSA). Despite the progress in this area, the efficacy of the proposed specification miners needs to improve if these miners are to be adopted. We propose SpecForge, a new specification mining approach that synergizes many existing specification miners. SpecForge decomposes FSAs that are inferred by existing miners into simple constraints, through a process we refer to as model fission. It then filters the outlier constraints and fuses the constraints back together into a single FSA (i.e., model fusion). We have evaluated SpecForge on execution traces of 10 programs, which includes 5 programs from DaCapo benchmark, to infer behavioral models of 13 library classes. Our results show that SpecForge achieves an average precision, recall and F-measure of 90.57%, 54.58%, and 64.21% respectively. SpecForge outperforms the best performing baseline by 13.75% in terms of F-measure.
[DaCapo benchmark, software systems, data mining, Specification Mining, Manuals, specification mining approach, specification maintenance, SpecForge, finite state machines, formal specification, FSA, Benchmark testing, outlier constraints, Libraries, finite state automaton, Model Fission, Synergizing Miners, formal specifications, F-measure, specification miners, behavioral models, Model Fusion, model fusions, model fissions, Automata, Inference algorithms, Software, Software engineering]
Evolutionary Robustness Testing of Data Processing Systems Using Models and Data Mutation (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
System level testing of industrial data processing software poses several challenges. Input data can be very large, even in the order of gigabytes, and with complex constraints that define when an input is valid. Generating the right input data to stress the system for robustness properties (e.g. to test how faulty data is handled) is hence very complex, tedious and error prone when done manually. Unfortunately, this is the current practice in industry. In previous work, we defined a methodology to model the structure and the constraints of input data by using UML class diagrams and OCL constraints. Tests were automatically derived to cover predefined fault types in a fault model. In this paper, to obtain more effective system level test cases, we developed a novel search-based test generation tool. Experiments on a real-world, large industrial data processing system show that our automated approach can not only achieve better code coverage, but also accomplishes this using significantly smaller test suites.
[industrial data processing system, program testing, Unified modeling language, data mutation, evolutionary robustness testing, Data processing, Search problems, automated approach, Data models, Robustness, Software, data handling, search problems, Testing, search-based test generation tool]
Dynamically Testing GUIs Using Ant Colony Optimization (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
In this paper we introduce a dynamic GUI test generator that incorporates ant colony optimization. We created two ant systems for generating tests. Our first ant system implements the normal ant colony optimization algorithm in order to traverse the GUI and find good event sequences. Our second ant system, called AntQ, implements the antq algorithm that incorporates Q-Learning, which is a behavioral reinforcement learning technique. Both systems use the same fitness function in order to determine good paths through the GUI. Our fitness function looks at the amount of change in the GUI state that each event causes. Events that have a larger impact on the GUI state will be favored in future tests. We compared our two ant systems to random selection. We ran experiments on six subject applications and report on the code coverage and fault finding abilities of all three algorithms.
[Context, Ant colony optimization, program testing, dynamic GUI test generator, code coverage, Heuristic algorithms, graphical user interfaces, fitness function, Q-Learning, Generators, fault finding abilities, ant colony optimization algorithm, behavioral reinforcement learning technique, AntQ, Computer architecture, learning (artificial intelligence), ant colony optimisation, Graphical user interfaces, Testing]
Test Analysis: Searching for Faults in Tests (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Tests are increasingly specified as programs. Expressing tests as code is advantageous in that developers are comfortable writing and running code, and tests can be automated and reused as the software evolves. Tests expressed as code, however, can also contain faults. Some test faults are similar to those found in application code, while others are more subtle, caused by incorrect implementation of testing concepts and processes. These faults may cause a test to fail when it should not, or allow program faults to go undetected. In this work we explore whether lightweight static analyses can be cost-effective in pinpointing patterns associated with faults tests. Our exploration includes a categorization and explanation of test patterns, and their application to 12 open source projects that include over 40K tests. We found that several patterns, detectable through simple and efficient static analyses of just the test code, can detect faults with a low false positive rate, while other patterns would require a more sophisticated and extensive code analysis to be useful.
[source code (software), program testing, program diagnostics, Dynamic scheduling, Encoding, coding patterns, lightweight static analysis, software fault tolerance, Fault diagnosis, application code, static analyses, Detectors, Syntactics, Software, test faults, test analysis, program faults, Testing]
Array Shadow State Compression for Precise Dynamic Race Detection (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Precise dynamic race detectors incur significant time and space overheads, particularly for array-intensive programs, due to the need to store and manipulate analysis (or shadow) state for every element of every array. This paper presents SlimState, a precise dynamic race detector that uses an adaptive, online algorithm to optimize array shadow state representations. SlimState is based on the insight that common array access patterns lead to analogous patterns in array shadow state, enabling optimized, space efficient representations of array shadow state with no loss in precision. We have implemented SlimState for Java. Experiments on a variety of benchmarks show that array shadow compression reduces the space and time overhead of race detection by 27% and 9%, respectively. It is particularly effective for array-intensive programs, reducing space and time overheads by 35% and 17%, respectively, on these programs.
[data race detection, Java, space overhead, program testing, Instruction sets, Heuristic algorithms, array shadow state compression, dynamic analysis, Synchronization, array-intensive programs, concurrency, adaptive online algorithm, array access patterns, time overhead, Detectors, precise dynamic race detection, system monitoring, SLIMSTATE, space efficient representations, Arrays, array shadow state representations, analogous patterns, Clocks]
Fast and Precise Symbolic Analysis of Concurrency Bugs in Device Drivers (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Concurrency errors, such as data races, make device drivers notoriously hard to develop and debug without automated tool support. We present Whoop, a new automated approach that statically analyzes drivers for data races. Whoop is empowered by symbolic pairwise lockset analysis, a novel analysis that can soundly detect all potential races in a driver. Our analysis avoids reasoning about thread interleavings and thus scales well. Exploiting the race-freedom guarantees provided by Whoop, we achieve a sound partial-order reduction that significantly accelerates Corral, an industrial-strength bug-finder for concurrent programs. Using the combination of Whoop and Corral, we analyzed 16 drivers from the Linux 4.0 kernel, achieving 1.5 -- 20&#x00D7; speedups over standalone Corral.
[program debugging, CORRAL, Instruction sets, device drivers, Programming, race-freedom guarantees, concurrent programs, automated approach, industrial-strength bug-finder, Concurrent computing, WHOOP, symbol manipulation, debugging, concurrency bugs, Linux 4.0 kernel, Kernel, concurrency (computers), partial-order reduction, symbolic pairwise lockset analysis, statical analysis, Context, data races all, thread interleavings, concurrent errors, Linux, Computer bugs, data races]
JaConTeBe: A Benchmark Suite of Real-World Java Concurrency Bugs (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Researchers have proposed various approaches to detect concurrency bugs and improve multi-threaded programs, but performing evaluations of the effectiveness of these approaches still remains a substantial challenge. We survey the existing evaluations and find out that they often use code or bugs not representative of real world. To improve representativeness, we have prepared JaConTeBe, a benchmark suite of 47 confirmed concurrency bugs from 8 popular open-source projects, supplemented with test cases for reproducing buggy behaviors. Running three approaches on JaConTeBe shows that our benchmark suite confirms some limitations of the three approaches. We submitted JaConTeBe to the SIR repository (a software-artifact repository for rigorous controlled experiments), and it was included as a part of SIR.
[Java concurrency bugs, Java, program debugging, software-artifact repository, multi-threading, evaluations, public domain software, JaConTeBe, real-world Java concurrency bugs, SIR repository, test cases, open-source projects, Open source software, Concurrent computing, benchmark suite, Computer bugs, Benchmark testing, System recovery, multithreaded programs, buggy behaviors, SIR]
Generating Fixtures for JavaScript Unit Testing (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
In today's web applications, JavaScript code interacts with the Document Object Model (DOM) at runtime. This runtime interaction between JavaScript and the DOM is error-prone and challenging to test. In order to unit test a JavaScript function that has read/write DOM operations, a DOM instance has to be provided as a test fixture. This DOM fixture needs to be in the exact structure expected by the function under test. Otherwise, the test case can terminate prematurely due to a null exception. Generating these fixtures is challenging due to the dynamic nature of JavaScript and the hierarchical structure of the DOM. We present an automated technique, based on dynamic symbolic execution, which generates test fixtures for unit testing JavaScript functions. Our approach is implemented in a tool called ConFix. Our empirical evaluation shows that ConFix can effectively generate tests that cover DOM-dependent paths. We also find that ConFix yields considerably higher coverage compared to an existing JavaScript input generation technique.
[Web applications, HTML, JavaScript Unit Testing, Runtime, DOM fixture, JavaScript functions, JavaScript, document object model, Testing, object-oriented programming, DOM, fixtures generation, data flow analysis, Generators, JavaScript code, runtime interaction, web applications, null exception, concolic execution, ConFix, test generation, Fixtures, Computer bugs, Concrete, dynamic symbolic execution, Internet, Test fixture]
Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Rather than tediously writing unit tests manually, tools can be used to generate them automatically - sometimes even resulting in higher code coverage than manual testing. But how good are these tests at actually finding faults? To answer this question, we applied three state-of-the-art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. Although the automatically generated test suites detected 55.7% of the faults overall, only 19.9% of all the individual test suites detected a fault. By studying the effectiveness and problems of the individual tools and the tests they generate, we derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. These insights include 1) improving the obtained code coverage so that faulty statements are executed in the first instance, 2) improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, and 3) improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time.
[automatic test software, fault diagnosis, program testing, regression testing, Manuals, fault detection, unit testing, execution environment simulation, unit test generation tools, test suites, faulty program state propagation, EvoSuite, test effectiveness, Testing, Java, code coverage, observable output, Agitar, sensitive assertions, Randoop, empirical study, Generators, faulty statements, Defects4J dataset, automated test generation, automated unit test generators, Writing, Software]
GRT: Program-Analysis-Guided Random Testing (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
We propose Guided Random Testing (GRT), which uses static and dynamic analysis to include information on program types, data, and dependencies in various stages of automated test generation. Static analysis extracts knowledge from the system under test. Test coverage is further improved through state fuzzing and continuous coverage analysis. We evaluated GRT on 32 real-world projects and found that GRT outperforms major peer techniques in terms of code coverage (by 13 %) and mutation score (by 9 %). On the four studied benchmarks of Defects4J, which contain 224 real faults, GRT also shows better fault detection capability than peer techniques, finding 147 faults (66 %). Furthermore, in an in-depth evaluation on the latest versions of ten popular real-world projects, GRT successfully detects over 20 unknown defects that were confirmed by developers.
[object-oriented programming, program testing, GRT, static analysis, dynamic analysis, Frequency measurement, Data mining, automated test generation, fault detection capability, program types, Impurities, Computer bugs, program analysis guided random testing, peer techniques, Tin, Software, continuous coverage analysis, Automatic test generation, Testing, random testing]
Study and Refactoring of Android Asynchronous Programming (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
To avoid unresponsiveness, a core part of mobile development is asynchronous programming. Android providesseveral async constructs that developers can use. However, developers can still use the inappropriate async constructs, which result in memory leaks, lost results, and wasted energy. Fortunately, refactoring tools can eliminate these problems by transforming async code to use the appropriate constructs. In this paper we conducted a formative study on a corpusof 611 widely-used Android apps to map the asynchronouslandscape of Android apps, understand how developers retrofit asynchrony, and learn about barriers encountered by developers. Based on this study, we designed, implemented, and evaluated ASYNCDROID, a refactoring tool which enables Android developers to transform existing improperly-used async constructs into correct constructs. Our empirical evaluation shows that ASYNCDROID is applicable, accurate, and saves developers effort. We submitted 45 refactoring patches, and developers consider that the refactorings are useful.
[improperly-used async constructs, Asynchronous, Humanoid robots, Receivers, Transforms, Programming, Android asynchronous programming, Registers, software maintenance, refactoring tool, Android, Android (operating system), mobile computing, mobile development, refactoring patches, asynchronous landscape, Androids, Android application, Android developers, ASYNCDROID, Refactoring, Graphical user interfaces]
Tracking the Software Quality of Android Applications Along Their Evolution (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Mobile apps are becoming complex software systems that must be developed quickly and evolve continuously to fit new user requirements and execution contexts. However, addressing these requirements may result in poor design choices, also known as antipatterns, which may incidentally degrade software quality and performance. Thus, the automatic detection and tracking of antipatterns in this apps are important activities in order to ease both maintenance and evolution. Moreover, they guide developers to refactor their applications and thus, to improve their quality. While antipatterns are well-known in object-oriented applications, their study in mobile applications is still in its infancy. In this paper, we analyze the evolution of mobile apps quality on 3, 568 versions of 106 popular Android applications downloaded from the Google Play Store. For this purpose, we use a tooled approach, called PAPRIKA, to identify 3 object-oriented and 4 Android-specific antipatterns from binaries of mobile apps, and to analyze their quality along evolutions.
[Measurement, Java, mobile apps quality, antipattern, Humanoid robots, PAPRIKA, object-oriented antipatterns, Mobile communication, software quality, software maintenance, software evolution, Android, mobile computing, Software quality, Android applications, object-oriented methods, Google Play Store, Androids, mobile app, Android-specific antipatterns]
Reverse Engineering Mobile Application User Interfaces with REMAUI (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
When developing the user interface code of a mobile application, in practice a big gap exists between the digital conceptual drawings of graphic artists and working user interface code. Currently, programmers bridge this gap manually, by reimplementing the conceptual drawings in code, which is cumbersome and expensive. To bridge this gap, we introduce the first technique to automatically Reverse Engineer Mobile Application User Interfaces (REMAUI). On a given input bitmap REMAUI identifies user interface elements such as images, texts, containers, and lists, via computer vision and optical character recognition (OCR) techniques. In our experiments on 488 screenshots of over 100 popular third-party Android and iOS applications, REMAUI-generated user interfaces were similar to the originals, both pixel-by-pixel and in terms of their runtime user interface hierarchies. REMAUI's average overall runtime on a standard desktop computer was 9 seconds.
[third-party Android, standard desktop computer, reverse engineering mobile application user interfaces, Containers, user interfaces, OCR techniques, iOS (operating system), optical character recognition techniques, Android (operating system), mobile computing, optical character recognition, input bitmap, digital conceptual drawings, REMAUI-generated user interfaces, Graphical user interfaces, reverse engineering, working user interface code, Mobile applications, Optical character recognition software, user interface elements identification, graphic artists, Layout, computer vision, runtime user interface hierarchies, iOS applications, Smart phones]
CodeHow: Effective Code Search Based on API Understanding and Extended Boolean Model (E)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Over the years of software development, a vast amount of source code has been accumulated. Many code search tools were proposed to help programmers reuse previously-written code by performing free-text queries over a large-scale codebase. Our experience shows that the accuracy of these code search tools are often unsatisfactory. One major reason is that existing tools lack of query understanding ability. In this paper, we propose CodeHow, a code search technique that can recognize potential APIs a user query refers to. Having understood the potentially relevant APIs, CodeHow expands the query with the APIs and performs code retrieval by applying the Extended Boolean model, which considers the impact of both text similarity and potential APIs on code search. We deploy the backend of CodeHow as a Microsoft Azure service and implement the front-end as a Visual Studio extension. We evaluate CodeHow on a large-scale codebase consisting of 26K C# projects downloaded from GitHub. The experimental results show that when the top 1 results are inspected, CodeHow achieves a precision score of 0.794 (i.e., 79.4% of the first returned results are relevant code snippets). The results also show that CodeHow outperforms conventional code search tools. Furthermore, we perform a controlled experiment and a survey of Microsoft developers. The results confirm the usefulness and effectiveness of CodeHow in programming practices.
[API understanding, Visualization, text analysis, precision score, application program interfaces, code retrieval, user query, GitHub, Programming, large-scale codebase, query processing, Microsoft Azure service, text similarity, CodeHow, Extended Boolean model, Libraries, programming practices, extended Boolean model, software development, free-text queries, code search technique, software reuse, code search, Documentation, Boolean algebra, Indexes, Standards, Visual Studio extension, Microsoft developers, C# projects, Software, API]
SpyREST: Automated RESTful API Documentation Using an HTTP Proxy Server (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
RESTful API documentation is expensive to produce and maintain due to the lack of reusable tools and automated solutions. Most RESTful APIs are documented manually and the API developers are responsible for keeping the documentation up to date as the API evolves making the process both costly and error-prone. In this paper we introduce a novel technique using an HTTP proxy server that can be used to automatically generate RESTful API documentation and demonstrate SpyREST, an example implementation of the proposed technique. SpyREST uses a proxy to intercept example API calls and intelligently produces API documentation for RESTful Web APIs by processing the request and response data. Using the proposed HTTP proxy server based technique, RESTful API developers can significantly reduce the cost of producing and maintaining API documentation by replacing a large manual process with an automated process.
[RESTful API, RESTful Web API, Web API, Automation, application program interfaces, SpyREST, Software as a service, system documentation, Documentation, Manuals, hypermedia, reusable tools, HTTP proxy server, Servers, Uniform resource locators, automated RESTful API documentation, Databases, software reusability, Libraries, Internet, software tools, Example based documentation]
Tracking and Analyzing Cross-Cutting Activities in Developers' Daily Work (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Developers use many software applications to process large amounts of diverse information in their daily work. The information is usually meaningful beyond the context of an application that manages it. However, as different applications function independently, developers have to manually track, correlate and re-find cross-cutting information across separate applications. We refer to this difficulty as information fragmentation problem. In this paper, we present ActivitySpace, an interapplication activity tracking and analysis framework for tackling information fragmentation problem in software development. ActivitySpace can monitor the developer's activity in many applications at a low enough level to obviate application-specific support while accounting for the ways by which low-level activity information can be effectively aggregated to reflect the developer's activity at higher-level of abstraction. A system prototype has been implemented on Microsoft Windows. Our preliminary user study showed that the ActivitySpace system is promising in supporting interapplication information needs in developers' daily work.
[Context, cross-cutting activities, Java, software development, software development management, ActivitySpace, History, Databases, interapplication activity tracking, Software, Mice, software application, information fragmentation problem, Monitoring]
An Automated Framework for Recommending Program Elements to Novices (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Novice programmers often learn programming by implementing well-known algorithms. There are several challenges in the process. Recommendation systems in software currently focus on programmer productivity and ease of development. Teaching aides for such novice programmers based on recommendation systems still remain an under-explored area. In this paper, we present a general framework for recognizing the desired target for partially-written code and recommending a reliable series of edits to transform the input program into the target solution. Our code analysis is based on graph matching and tree edit algorithms. Our experimental results show that efficient graph comparison techniques can accurately match two portions of source code and produce an accurate set of source code edits. We provide details on implementation of our framework, which is developed as a plugin for Java in Eclipse IDE.
[Algorithm design and analysis, source code (software), novice programmer, pattern matching, partially-written code, Target recognition, graph theory, graph matching, Transforms, Programming, Eclipse IDE, recommendation system, code analysis, pq-Gram Algorithm, teaching aide, source code edit, programmer productivity, programming, Java, automated framework, Knowledge based systems, tree edit algorithm, recommender systems, recommending program element, Recommendation Framework, graph comparison technique, Software engineering]
Automated Tagging of Software Projects Using Bytecode and Dependencies (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Several open and closed source repositories group software systems and libraries to allow members of particular organizations or the open source community to take advantage of them. However, to make this possible, it is necessary to have effective ways of searching and browsing the repositories. Software tagging is the process of assigning terms (i.e., tags or labels) to software assets in order to describe features and internal details, making the task of understanding software easier and potentially browsing and searching through a repository more effective. We present Sally, an automatic software tagging approach that is able to produce meaningful tags for Maven-based software projects by analyzing their bytecode and dependency relations without any special requirements from developers. We compared tags generated by Sally to the ones in two widely used online repositories, and the tags generated by a state-of-the-art categorization approach. The results suggest that Sally is able to generate expressive tags without relying on machine learning-based models.
[bytecode, project management, public domain software, Software algorithms, software assets, automatic software tagging approach, online repositories, information retrieval, software management, term assignment, Data mining, software tagging, Maven-based software projects, Support vector machines, Sally, categorization approach, Tagging, Feature extraction, Software systems, Internet, closed source repository group software systems, open source community, dependency relations, open source repository group software systems]
Repairing Programs with Semantic Code Search (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Automated program repair can potentially reduce debugging costs and improve software quality but recent studies have drawn attention to shortcomings in the quality of automatically generated repairs. We propose a new kind of repair that uses the large body of existing open-source code to find potential fixes. The key challenges lie in efficiently finding code semantically similar (but not identical) to defective code and then appropriately integrating that code into a buggy program. We present SearchRepair, a repair technique that addresses these challenges by(1) encoding a large database of human-written code fragments as SMT constraints on input-output behavior, (2) localizing a given defect to likely buggy program fragments and deriving the desired input-output behavior for code to replace those fragments, (3) using state-of-the-art constraint solvers to search the database for fragments that satisfy that desired behavior and replacing the likely buggy code with these potential patches, and (4) validating that the patches repair the bug against program testsuites. We find that SearchRepair repairs 150 (19%) of 778 benchmark C defects written by novice students, 20 of which are not repaired by GenProg, TrpAutoRepair, and AE. We compare the quality of the patches generated by the four techniques by measuring how many independent, not-used-during-repairtests they pass, and find that SearchRepair-repaired programs pass 97.3% ofthe tests, on average, whereas GenProg-, TrpAutoRepair-, and AE-repaired programs pass 68.7%, 72.1%, and 64.2% of the tests, respectively. We concludethat SearchRepair produces higher-quality repairs than GenProg, TrpAutoRepair, and AE, and repairs some defects those tools cannot.
[program debugging, C defects, SearchRepair technique, program diagnostics, fault localization, GenProg-repaired program, Maintenance engineering, program repair, repair quality, automated repair, software quality, buggy code, AE-repaired programs, Semantics, Computer bugs, semantic code search, SMT constraints, Benchmark testing, human-written code fragments, SearchRepair, debugging, Software, TrpAutoRepair-repaired program, Indexing]
Fixing Recurring Crash Bugs via Analyzing Q&amp;A Sites (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Recurring bugs are common in software systems, especially in client programs that depend on the same framework. Existing research uses human-written templates, and is limited to certain types of bugs. In this paper, we propose a fully automatic approach to fixing recurring crash bugs via analyzing Q&amp;A sites. By extracting queries from crash traces and retrieving a list of Q&amp;A pages, we analyze the pages and generate edit scripts. Then we apply these scripts to target source code and filter out the incorrect patches. The empirical results show that our approach is accurate in fixing real-world crash bugs, and can complement existing bug-fixing approaches.
[Context, source code (software), program debugging, query extraction, crash traces, software systems, recurring crash bugs, source code, Registers, Q&amp;A sites, query processing, human-written templates, Computer bugs, edit scripts, bug-fixing approaches, Web pages, Search engines, real-world crash bugs, Software]
Search-Based Synthesis of Probabilistic Models for Quality-of-Service Software Engineering (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
The formal verification of finite-state probabilistic models supports the engineering of software with strict quality-of-service (QoS) requirements. However, its use in software design is currently a tedious process of manual multiobjective optimisation. Software designers must build and verify probabilistic models for numerous alternative architectures and instantiations of the system parameters. When successful, they end up with feasible but often suboptimal models. The EvoChecker search-based software engineering approach and tool introduced in our paper employ multiobjective optimisation genetic algorithms to automate this process and considerably improve its outcome. We evaluate EvoChecker for six variants of two software systems from the domains of dynamic power management and foreign exchange trading. These systems are characterised by different types of design parameters and QoS requirements, and their design spaces comprise between 2E+14 and 7.22E+86 relevant alternative designs. Our results provide strong evidence that EvoChecker significantly outperforms the current practice and yields actionable insights for software designers.
[program testing, program verification, finite-state probabilistic models, software design, Quality of service, Optimization, suboptimal models, quality-of-service requirements, formal verification, quality-of-service software engineering, Model Repair, foreign exchange trading, Probabilistic Model Checking, search-based synthesis, Model Synthesis, Search-Based Software Engineering, probability, design parameters, Probabilistic logic, genetic algorithms, quality of service, probabilistic models, EvoChecker search-based software engineering approach, dynamic power management, multiobjective optimisation genetic algorithms, Markov processes, Software systems, QoS requirements, design space, Genetic Algorithms, Software engineering]
Synthesizing Web Element Locators (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
To programmatically interact with the user interface of a web application, element locators are used to select and retrieve elements from the Document Object Model (DOM). Element locators are used in JavaScript code, Cascading stylesheets, and test cases to interact with the runtime DOM of the webpage. Constructing these element locators is, however, challenging due to the dynamic nature of the DOM. We find that locators written by web developers can be quite complex, and involve selecting multiple DOM elements. We present an automated technique for synthesizing DOM element locators using examples provided interactively by the developer. The main insight in our approach is that the problem of synthesizing complex multi-element locators can be expressed as a constraint solving problem over the domain of valid DOM states in a web application. We implemented our synthesis technique in a tool called LED, which provides an interactive drag and drop support inside the browser for selecting positive and negative examples. We find that LED supports at least 86% of the locators used in the JavaScript code of deployed web applications, and that the locators synthesized by LED have a recall of 98% and a precision of 63%. LED is fast, taking only 0.23 seconds on average to synthesize a locator.
[program testing, Programming, Web applications, HTML, Element locators, user interfaces, Program synthesis, DOM element locator synthesis, document object model, CSS selectors, Mathematical model, Cascading style sheets, Java, Navigation, DOM, live editor-for-DOM, program synthesis, Light emitting diodes, JavaScript code, LED, user interface, Web element locator synthesis, Webpage, Web application, online front-ends, Writing, Programming by example, Internet]
Cost-Efficient Sampling for Performance Prediction of Configurable Systems (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
A key challenge of the development and maintenanceof configurable systems is to predict the performance ofindividual system variants based on the features selected. It isusually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predictperformance based on small samples of measured variants, butit is still open how to dynamically determine an ideal samplethat balances prediction accuracy and measurement effort. Inthis paper, we adapt two widely-used sampling strategies forperformance prediction to the domain of configurable systemsand evaluate them in terms of sampling cost, which considersprediction accuracy and measurement effort simultaneously. Togenerate an initial sample, we introduce a new heuristic based onfeature frequencies and compare it to a traditional method basedon t-way feature coverage. We conduct experiments on six realworldsystems and provide guidelines for stakeholders to predictperformance by sampling.
[Measurement, performance prediction accuracy, sampling methods, sampling strategies, Buildings, sampling, Predictive models, configurable systems, Electronic mail, feature combinatorics, software maintenance, configurable system development, measurement effort, sampling cost, feature frequencies, Training, performance prediction, configurable system maintenance, Mathematical model, software performance evaluation, Testing]
Predicting Delays in Software Projects Using Networked Classification (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Software projects have a high risk of cost and schedule overruns, which has been a source of concern for the software engineering community for a long time. One of the challenges in software project management is to make reliable prediction of delays in the context of constant and rapid changes inherent in software projects. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether a subset of software tasks (among the hundreds to thousands of ongoing tasks) in a software project have a risk of being delayed. Our approach makes use of not only features specific to individual software tasks (i.e. local data) -- as done in previous work -- but also their relationships (i.e. networked data). In addition, using collective classification, our approach can simultaneously predict the degree of delay for a group of related tasks. Our evaluation results show a significant improvement over traditional approaches which perform classification on each task independently: achieving 46% -- 97% precision (49% improved), 46% -- 97% recall (28% improved), 56% -- 75% F-measure (39% improved), and 78% -- 95% Area Under the ROC Curve (16% improved).
[Networked classification, pattern classification, project management, Predictive models, software management, Data mining, Information technology, Machine Learning, F-measure, software engineering community, Networked Classification, ROC Curve, Software, software engineering, Software analytics, Delays, Risk management, software project management, Software engineering]
Performance Prediction of Configurable Software Systems by Fourier Learning (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Understanding how performance varies across a large number of variants of a configurable software system is important for helping stakeholders to choose a desirable variant. Given a software system with n optional features, measuring all its 2n possible configurations to determine their performances is usually infeasible. Thus, various techniques have been proposed to predict software performances based on a small sample of measured configurations. We propose a novel algorithm based on Fourier transform that is able to make predictions of any configurable software system with theoretical guarantees of accuracy and confidence level specified by the user, while using minimum number of samples up to a constant factor. Empirical results on the case studies constructed from real-world configurable systems demonstrate the effectiveness of our algorithm.
[Algorithm design and analysis, Fourier transforms, Software algorithms, Estimation, Fourier learning, Fourier transform, confidence level, configurable software system, configuration management, performance prediction, Boolean functions, possible configuration, optional feature, Software systems, Prediction algorithms, learning (artificial intelligence), software performance, software performance evaluation]
Model-Driven Allocation Engineering (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Cyber-physical systems (CPSs) provide sophisticated functionality and are controlled by networked electronic control units (ECUs). Nowadays, software engineers use component-based development approaches to develop their software. Moreover, software components have to be allocated to an ECU to be executed. Engineers have to cope with topology-, software-, and timing-dependencies and memory-, scheduling-, and routing-constraints. Currently, engineers use linear programs to specify allocation constraints and to derive a feasible allocation automatically. However, encoding the allocation problem as a linear program is a complex and error-prone task. This paper contributes a model-driven, OCL-based allocation engineering approach for reducing the engineering effort and to avoid failures. We validate our approach with an automotive case study modeled with MechatronicUML. Our validation shows that we can specify allocation constraints with less engineering effort and are able to derive feasible allocations automatically.
[Actuators, model-driven allocation engineering, CPS, timing-dependencies, model-driven OCL-based allocation engineering approach, Deployment, linear programming, Automotive, Automotive engineering, software-dependencies, resource allocation, component-based development approach, constraint allocation problem, Allocation, Computer architecture, Hardware, Brakes, constraint handling, topology-dependencies, linear programs, software engineers, object-oriented programming, Unified Modeling Language, networked electronic control units, software component allocation, MechatronicUML, Cyber-physical systems, memory-constraints, scheduling-constraints, cyber-physical systems, Constraints, ECU, Software, routing-constraints, Resource management]
Configuration-Aware Change Impact Analysis (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This paper presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining possibly impacted products when changing source code of a product family. The approach further supports engineers, who are adapting specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanism, it provides more precise results than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. Using an industrial product line we report evaluation results on the benefit and performance of the approach.
[source code (software), product variants, configuration, data flow analysis, Maintenance engineering, control flow analysis, Mechanical factors, configuration-aware change impact analysis, product family source code, Standards, Runtime, variability mechanism, program analysis, industrial product line, Software systems, CIA, change impact analysis, maintenance, Testing]
Automating the Extraction of Model-Based Software Product Lines from Model Variants (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
We address the problem of automating 1) the analysis of existing similar model variants and 2) migrating them into a software product line. Our approach, named MoVaPL, considers the identification of variability and commonality in model variants, as well as the extraction of a CVL-compliant Model-based Software Product Line (MSPL) from the features identified on these variants. MoVaPL builds on a generic representation of models making it suitable to any MOF-based models. We apply our approach on variants of the open source ArgoUML UML modeling tool as well as on variants of an In-flight Entertainment System. Evaluation with these large and complex case studies contributed to show how our feature identification with structural constraints discovery and the MSPL generation process are implemented to make the approach valid (i.e., the extracted software product line can be used to regenerate all variants considered) and sound (i.e., derived variants We address the problem of automating 1) the analysis of existing similar model variants and 2) migrating them into a software product line. Our approach, named MoVaPL, considers the identification of variability and commonality in model variants, as well as the extraction of a CVL-compliant Model-based Software Product Line (MSPL) from the features identified on these variants. MoVaPL builds on a generic representation of models making it suitable to any MOF-based models. We apply our approach on variants of the open source ArgoUML UML modeling tool as well as on variants of an In-flight Entertainment System. Evaluation with these large and complex case studies contributed to show how our feature identification with structural constraints discovery and the MSPL generation process are implemented to make the approach valid (i.e., the extracted software product line can be used to regenerate all variants considered) and sound (i.e., derived variants which did not exist are at least structurally valid).which did not exist are at least structurally valid).
[software product lines, MSPL generation process, Unified Modeling Language, common variability language, Computational modeling, public domain software, Unified modeling language, Buildings, open source ArgoUML UML modeling tool, in-flight entertainment system, model driven engineering, reverse engineering, model variants, MoVaPL approach, Software product lines, Analytical models, MOF-based models, Upper bound, CVL-compliant model-based software product lines, Feature extraction, model variant analysis]
Scaling Size and Parameter Spaces in Variability-Aware Software Performance Models (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
In software performance engineering, what-if scenarios, architecture optimization, capacity planning, run-time adaptation, and uncertainty management of realistic models typically require the evaluation of many instances. Effective analysis is however hindered by two orthogonal sources of complexity. The first is the infamous problem of state space explosion -- the analysis of a single model becomes intractable with its size. The second is due to massive parameter spaces to be explored, but such that computations cannot be reused across model instances. In this paper, we efficiently analyze many queuing models with the distinctive feature of more accurately capturing variability and uncertainty of execution rates by incorporating general (i.e., non-exponential) distributions. Applying product-line engineering methods, we consider a family of models generated by a core that evolves into concrete instances by applying simple delta operations affecting both the topology and the model's parameters. State explosion is tackled by turning to a scalable approximation based on ordinary differential equations. The entire model space is analyzed in a family-based fashion, i.e., at once using an efficient symbolic solution of a super-model that subsumes every concrete instance. Extensive numerical tests show that this is orders of magnitude faster than a naive instance-by-instance analysis.
[software product lines, Uncertainty, queueing theory, Computational modeling, software performance engineering, Unified modeling language, run-time adaptation, Software performance, queuing models, differential equations, Analytical models, ordinary differential equations, capacity planning, product-line engineering methods, Numerical models, Mathematical model, architecture optimization, uncertainty management, software performance evaluation, variability-aware software performance models, what-if scenarios, state space explosion]
Executing Model-Based Tests on Platform-Specific Implementations (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Model-based testing of embedded real-time systems is challenging because platform-specific details are often abstracted away to make the models amenable to various analyses. Testing an implementation to expose non-conformance to such a model requires reconciling differences arising from these abstractions. Due to stateful behavior, naive comparisons of model and system behaviors often fail causing numerous false positives. Previously proposed approaches address this by being reactively permissive: passing criteria are relaxed to reduce false positives, but may increase false negatives, which is particularly bothersome for safety-critical systems. To address this concern, we propose an automated approach that is proactively adaptive: test stimuli and system responses are suitably modified taking into account platform-specific aspects so that the modified test when executed on the platform-specific implementation exercises the intended scenario captured in the original model-based test. We show that the new framework eliminates false negatives while keeping the number of false positives low for a variety of platform-specific configurations.
[program testing, Computational modeling, embedded real-time system, Model-based testing, safety-critical software, platform-specific configuration, embedded systems, safety-critical system, platform-specific implementation, model-based testing, Real-time systems, Software, Hardware, Timing, Monitoring, Testing]
Automated Test Input Generation for Android: Are We There Yet? (E)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Like all software, mobile applications ("apps") must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: ease of use, ability to work on multiple platforms, code coverage, and ability to detect faults. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android.
[Java, Test input generation, program testing, graphical user interfaces, automatic application testing, Humanoid robots, Receivers, fault detection ability metric, automated test input generation, automated testing, Android apps, open source Android operating systems, work ability metric, Systematics, Android (operating system), mobile computing, mobile applications, Software, Androids, ease-of-use metric, Testing, code coverage metric]
Testing Cross-Platform Mobile App Development Frameworks (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Mobile app developers often wish to make their apps available on a wide variety of platforms, e.g., Android, iOS, and Windows devices. Each of these platforms uses a different programming environment, each with its own language and APIs for app development. Small app development teams lack the resources and the expertise to build and maintain separate code bases of the app customized for each platform. As a result, we are beginning to see a number of cross-platform mobile app development frameworks. These frameworks allow the app developers to specify the business logic of the app once, using the language and APIs of a home platform (e.g., Windows Phone), and automatically produce versions of the app for multiple target platforms (e.g., iOS and Android). In this paper, we focus on the problem of testing cross-platform app development frameworks. Such frameworks are challenging to develop because they must correctly translate the home platform API to the (possibly disparate) target platform API while providing the same behavior. We develop a differential testing methodology to identify inconsistencies in the way that these frameworks handle the APIs of the home and target platforms. We have built a prototype testing tool, called X-Checker, and have applied it to test Xamarin, a popular framework that allows Windows Phone apps to be cross-compiled into native Android (and iOS) apps. To date, X-Checker has found 47 bugs in Xamarin, corresponding to inconsistencies in the way that Xamarin translates between the semantics of the Windows Phone and the Android APIs. We have reported these bugs to the Xamarin developers, who have already committed patches for twelve of them.
[application program interfaces, Windows devices, Mobile communication, Mobile apps, Android apps, cross-platform mobile app development frameworks, mobile app developers, differential testing methodology, programming environment, Android (operating system), mobile computing, home platform API, Windows Phone apps, Libraries, Android API, Testing, iOS apps, app development teams, Cross-platform, multiple target platforms, prototype testing tool, X-Checker, Computer bugs, Xamarin developers, Porting, Software, Smart phones]
CLAMI: Defect Prediction on Unlabeled Datasets (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort. In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.
[Measurement, defect information collection, supervised learning, Manuals, Predictive models, software quality, CLA approach, software fault tolerance, CLAMI approach, unsupervised learning, Training, Supervised learning, Software, Data models, software engineering, cross-project defect prediction]
Mutation-Based Fault Localization for Real-World Multilingual Programs (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Programmers maintain and evolve their software in a variety of programming languages to take advantage of various control/data abstractions and legacy libraries. The programming language ecosystem has diversified over the last few decades, and non-trivial programs are likely to be written in more than a single language. Unfortunately, language interfaces such as Java Native Interface and Python/C are difficult to use correctly and the scope of fault localization goes beyond language boundaries, which makes debugging multilingual bugs challenging. To overcome the aforementioned limitations, we propose a mutation-based fault localization technique for real-world multilingual programs. To improve the accuracy of locating multilingual bugs, we have developed and applied new mutation operators as well as conventional mutation operators. The results of the empirical evaluation for six non-trivial real-world multilingual bugs are promising in that the proposed technique identifies the buggy statements as the most suspicious statements for all six bugs.
[real-world multilingual programs, program debugging, program testing, suspicious statement identification, Mutation Based Fault Localization, Programming, data abstraction, buggy statement identification, programming languages, empirical evaluation, mutation-based fault localization technique, mutation operators, Fault Localization, Libraries, Safety, Testing, Multilingual Programs, Java, programming language ecosystem, legacy libraries, fault localization, Debugging, multilingual bug debugging, nontrivial real-world multilingual bugs, control abstraction, Computer bugs, Mutation Analysis, multilingual bug location]
Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Bug localization refers to the automated process of locating the potential buggy files for a given bug report. To help developers focus their attention to those files is crucial. Several existing automated approaches for bug localization from a bug report face a key challenge, called lexical mismatch, in which the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. This paper presents a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files and documentation if they appear frequently enough in the pairs of reports and buggy files. Our empirical evaluation on real-world projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, HyLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. Two out of three cases, a correct buggy file is in the list of three suggested files.
[program debugging, DNN, Metadata, projec bug-fixing history, rVSM, History, machine learning techniques, code tokens, information retrieval technique, deep learning, buggy file localization, bug reports, learning (artificial intelligence), deep neural network, HyLoc, Bug Localization, support vector machines, lexical mismatch, source files, information retrieval, IR technique, Information retrieval, Deep Learning, Bridges, Deep Neural Network, Computer bugs, Bug Reports, Feature extraction, Software, Information Retrieval]
Fuzzing the Rust Typechecker Using CLP (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Language fuzzing is a bug-finding technique for testing compilers and interpreters, its effectiveness depends upon the ability to automatically generate valid programs in the language under test. Despite the proven success of language fuzzing, there is a severe lack of tool support for fuzzing statically-typed languages with advanced type systems because existing fuzzing techniques cannot effectively and automatically generate well-typed programs that use sophisticated types. In this work we describe how to automatically generate well-typed programs that use sophisticated type systems by phrasing the problem of well-typed program generation in terms of Constraint Logic Programming (CLP). In addition, we describe how to specifically target the typechecker implementation for testing, unlike all existing work which ignores the typechecker. We focus on typechecker precision bugs, soundness bugs, and consistency bugs. We apply our techniques to Rust, a complex, industrial-strength language with a sophisticated type system.
[Java, program debugging, bug-finding technique, Logic programming, program testing, language fuzzing, CLP, typechecker precision bugs, soundness bugs, Grammar, consistency bugs, Engines, rust typechecker, well-typed program generation, Computer bugs, Syntactics, logic programming, Rust language, interpreter testing, constraint logic programming, compiler testing, Testing]
TCA: An Efficient Two-Mode Meta-Heuristic Algorithm for Combinatorial Test Generation (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Covering arrays (CAs) are often used as test suites for combinatorial interaction testing to discover interaction faults of real-world systems. Most real-world systems involve constraints, so improving algorithms for covering array generation (CAG) with constraints is beneficial. Two popular methods for constrained CAG are greedy construction and meta-heuristic search. Recently, a meta-heuristic framework called two-mode local search has shown great success in solving classic NPhard problems. We are interested whether this method is also powerful in solving the constrained CAG problem. This work proposes a two-mode meta-heuristic framework for constrained CAG efficiently and presents a new meta-heuristic algorithm called TCA. Experiments show that TCA significantly outperforms state-of-the-art solvers on 3-way constrained CAG. Further experiments demonstrate that TCA also performs much better than its competitors on 2-way constrained CAG.
[Algorithm design and analysis, program testing, Heuristic algorithms, Software algorithms, CAG, Search problems, two-mode metaheuristic algorithm, metaheuristic search method, TCA algorithm, Computer science, two-mode local search, covering array generation, combinatorial interaction testing, NP-hard problems, Software, combinatorial test generation, search problems, CA, greedy construction method, Testing]
Automatically Generating Test Templates from Test Names (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Existing specification-based testing techniques require specifications that either do not exist or are too difficult to create. As a result, they often fall short of their goal of helping developers test expected behaviors. In this paper we present a novel, natural language-based approach that exploits the descriptive nature of test names to generate test templates. Similar to how modern IDEs simplify development by providing templates for common constructs such as loops, test templates can save time and lower the cognitive barrier for writing tests. The results of our evaluation show that the approach is feasible: despite the difficulty of the task, when test names contain a sufficient amount of information, the approach's accuracy is over 80% when parsing the relevant information from the test name and generating the template.
[descriptive test names, program testing, cognitive barrier, natural language processing, natural language-based approach, writing tests, automatic test template generation, Encoding, test names, Semantics, Writing, Speech, Concrete, Software, Testing]
Region and Effect Inference for Safe Parallelism (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
In this paper, we present the first full regions-and-effects inference algorithm for explicitly parallel fork-join programs. We infer annotations inspired by Deterministic Parallel Java (DPJ) for a type-safe subset of C++. We chose the DPJ annotations because they give the strongest safety guarantees of any existing concurrency-checking approach we know of, static or dynamic, and it is also the most expressive static checking system we know of that gives strong safety guarantees. This expressiveness, however, makes manual annotation difficult and tedious, which motivates the need for automatic inference, but it also makes the inference problem very challenging: the code may use region polymorphism, imperative updates with complex aliasing, arbitrary recursion, hierarchical region specifications, and wildcard elements to describe potentially infinite sets of regions. We express the inference as a constraint satisfaction problem and develop, implement, and evaluate an algorithm for solving it. The region and effect annotations inferred by the algorithm constitute a checkable proof of safe parallelism, and it can be recorded both for documentation and for fast and modular safety checking.
[safe parallelism, Heuristic algorithms, region-and-effect inference algorithm, Manuals, region polymorphism, parallel programming, Annotation Inference, type-safe subset, Checkable Proof, Parallel processing, Safety, concurrency-checking approach, static checking system, Static Analysis, deterministic parallel Java, arbitrary recursion, Java, C++, complex aliasing, program diagnostics, constraint satisfaction problem, C++ language, modular safety checking, Safe Parallelism, hierarchical region specification, parallel fork-join program, Inference algorithms, Yttrium]
Optimistic Shared Memory Dependence Tracing (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Inter-thread shared memory dependences are crucial to understanding the behavior of concurrent systems, as such dependences are the cornerstone of time-travel debugging and further predictive trace analyses. To enable effective and efficient shared memory dependence tracing, we present an optimistic scheme addressing the challenge of capturing exact dependences between unsynchronized events to reduce the probe effect of program instrumentation. Specifically, our approach achieved a wait-free fast path for thread-local reads on x86-TSO relaxed memory systems, and simultaneously achieved precise tracing of exact read-after-write, write-after-write and write-after-read dependences on the fly. We implemented an open-source RWTrace tool, and evaluation results show that our approach not only achieves efficient shared memory dependence tracing, but also scales well on a multi-core computer system.
[Algorithm design and analysis, program debugging, x86-TSO relaxed memory system, Instruction sets, interthread shared memory dependence, Debugging, program instrumentation, shared memory dependence, dynamic analysis, Synchronization, predictive trace analysis, optimistic shared memory dependence tracing, concurrency, Computer science, time-travel debugging, Memory management, open-source RWTrace tool, Benchmark testing, shared memory systems, multicore computer system]
Model Checking Task Parallel Programs Using Gradual Permissions (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Habanero is a task parallel programming model that provides correctness guarantees to the programmer. Even so, programs may contain data races that lead to non-determinism, which complicates debugging and verification. This paper presents a sound algorithm based on permission regions to prove data race and deadlock freedom in Habanero programs. Permission regions are user annotations to indicate the use of shared variables over spans of code. The verification algorithm restricts scheduling to permission region boundaries and isolation to reduce verification cost. The effectiveness of the algorithm is shown in benchmarks with an implementation in the Java Pathfinder (JPF) model checker. The implementation uses a verification specific library for Habanero that is tested using JPF for correctness. The results show significant reductions in cost, where cost is controlled with the size of the permission regions, at the risk of rejecting programs that are actually free of any data race or deadlock.
[Habanero task parallel programming model, permission region, Schedules, Java, program debugging, program verification, deadlock, Synchronization, parallel programming, Java Pathfinder model checker, formal verification, model checking, JPF model checker, Java Pathfinder, task parallel languages, Data race, System recovery, Model checking, Writing, Data models, verification algorithm, Habanero, gradual permission]
Practically Tunable Static Analysis Framework for Large-Scale JavaScript Applications (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
We present a novel approach to analyze large-scale JavaScript applications statically by tuning the analysis scalability possibly giving up its soundness. For a given sound static baseline analysis of JavaScript programs, our framework allows users to define a sound approximation of selected executions that they are interested in analyzing, and it derives a tuned static analysis that can analyze the selected executions practically. The selected executions serve as parameters of the framework by taking trade-off between the scalability and the soundness of derived analyses. We formally describe our framework in abstract interpretation, and implement two instances of the framework. We evaluate them by analyzing large-scale real-world JavaScript applications, and the evaluation results show that the framework indeed empowers users to experiment with different levels of scalability and soundness. Our implementation provides an extra level of scalability by deriving sparse versions of derived analyses, and the implementation is publicly available.
[Java, Scalability, program diagnostics, abstract interpretation, Approximation methods, JavaScript programs, Tuning, Reactive power, Sensitivity, large-scale JavaScript applications, sparse versions, practically tunable static analysis framework, Libraries, Concrete, tuned static analysis, selected executions approximation]
Static Analysis of JavaScript Web Applications in the Wild via Practical DOM Modeling (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
We present SAFE<sub>Wapp</sub>, an open-source static analysis framework for JavaScript web applications. It provides a faithful (partial) model of web application execution environments of various browsers, based on empirical data from the main web pages of the 9,465 most popular websites. A main feature of SAFE<sub>Wapp</sub> is the configurability of DOM tree abstraction levels to allow users to adjust a trade-off between analysis performance and precision depending on their applications. We evaluate SAFEWapp on the 5 most popular JavaScript libraries and the main web pages of the 10 most popular websites in terms of analysis performance, precision, and modeling coverage. Additionally, as an application of SAFE<sub>Wapp</sub>, we build a bug detector for JavaScript web applications that uses static analysis results from SAFE<sub>Wapp</sub>. Our bug detector found previously undiscovered bugs including ones from wikipedia.org and amazon.com.
[Java, program debugging, javascript Web application, program diagnostics, Encyclopedias, HTML, Browsers, bug detector, Analytical models, DOM tree abstraction level, open-source static analysis, Web pages, SAFE<sub>Wapp</sub>, Internet, Web sites]
Variable Feature Usage Patterns in PHP (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
PHP allows the names of variables, classes, functions, methods, and properties to be given dynamically, as expressions that, when evaluated, return an identifier as a string. While this provides greater flexibility for programmers, it also makes PHP programs harder to precisely analyze and understand. In this paper we present a number of patterns designed to recognize idiomatic uses of these features that can be statically resolved to a precise set of possible names. We then evaluate these patterns across a corpus of 20 open-source systems totaling more than 3.7 million lines of PHP, showing how often these patterns occur in actual PHP code, demonstrating their effectiveness at statically determining the names that can be used at runtime, and exploring anti-patterns that indicate when the identifier computation is truly dynamic.
[public domain software, object-oriented language, PHP code, anti-patterns, Pattern recognition, open-source systems, Open source software, Engines, dynamic language features, Reactive power, Runtime, Static analysis, object-oriented languages, Feature extraction, PHP, usage patterns, object-oriented methods, Arrays, PHP programs, variable feature usage patterns]
Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.
[statistical machine translation, natural language processing, Natural languages, source code, SMT framework, Generators, Programming profession, Computer languages, Algorithms, Education, Japanese pseudocode, Software, programming language, statistical analysis, Statistical Approach, natural language, Python statement, Software engineering, language translation, English pseudocode]
Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Prior research shows that directly applying phrase-based SMT on lexical tokens to migrate Java to C# produces much semantically incorrect code. A key limitation is the use of sequences in phrase-based SMT to model and translate source code with well-formed structures. We propose mppSMT, a divide-and-conquer technique to address that with novel training and migration algorithms using phrase-based SMT in three phases. First, mppSMT treats a program as a sequence of syntactic units and maps/translates such sequences in two languages to one another. Second, with a syntax-directed fashion, it deals with the tokens within syntactic units by encoding them with semantic symbols to represent their data and token types. This encoding via semantic symbols helps better migration of API usages. Third, the lexical tokens corresponding to each sememe are mapped or migrated. The resulting sequences of tokens are merged together to form the final migrated code. Such divide-and-conquer and syntax-direction strategies enable phrase-based SMT to adapt well to syntactical structures in source code, thus, improving migration accuracy. Our empirical evaluation on several real-world systems shows that 84.8 -- 97.9% and 70 -- 83% of the migrated methods are syntactically and semantically correct, respectively. 26.3 -- 51.2% of total migrated methods are exactly matched to the human-written C# code in the oracle. Compared to Java2CSharp, a rule-based migration tool, it achieves higher semantic accuracy from 6.6 -- 57.7% relatively. Importantly, it does not require manual labeling for training data or manual definition of rules.
[source code (software), divide and conquer methods, divide-and-conquer approach, human-written C# code, Statistical Machine Translation, multiphase statistical migration, Training, phrase-based SMT, Semantics, Training data, Syntax-directed Translation, migration algorithms, Java, oracle, statistical machine translation, Computational modeling, source code, lexical tokens, Language Migration, Decoding, semantic symbols, mppSMT, API usages, Syntactics, language translation]
Experiences from Designing and Validating a Software Modernization Transformation (E)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Software modernization often involves complex code transformations that convert legacy code to new architectures or platforms, while preserving the semantics of the original programs. We present the lessons learnt from an industrial software modernization project of considerable size. This includes collecting requirements for a code-to-model transformation, designing and implementing the transformation algorithm, and then validating correctness of this transformation for the code-base at hand. Our transformation is implemented in the TXL rewriting language and assumes specifically structured C++ code as input, which it translates to a declarative configuration model. The correctness criterion for the transformation is that the produced model admits the same configurations as the input code. The transformation converts C++ functions specifying around a thousand configuration parameters. We verify the correctness for each run individually, using translation validation and symbolic execution. The technique is formally specified and is applicable automatically for most of the code-base.
[Industries, Switches, Experience Report, transformation algorithm, Complexity theory, Program Transformation, formal specification, Semantics, TXL rewriting language, correctness criterion, configuration parameter, legacy code, rewriting systems, code-to-model transformation, Symbolic Execution, complex code transformation, declarative configuration model, Object oriented modeling, input code, C++ code, Power electronics, C++ language, software maintenance, C++ function, translation validation, symbolic execution, Software, code-base, software modernization transformation, Functional Equivalence, industrial software modernization project]
Exploiting Domain and Program Structure to Synthesize Efficient and Precise Data Flow Analyses (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
A key challenge in implementing an efficient and precise data flow analysis is determining how to abstract the domain of values that a program variable can take on and how to update abstracted values to reflect program semantics. Such updates are performed by a transfer function and recent work by Thakur, Elder and Reps defined the bilateral algorithm for computing the most precise transfer function for a given abstract domain. In this paper, we identify and exploit the special case where abstract domains are comprised of disjoint subsets. For such domains, transfer functions computed using a customized algorithm can improve performance and in combination with symbolic modeling of block-level transfer functions improve precision as well. We implemented these algorithms in Soot and used them to perform data flow analysis on more than 100 non-trivial Java methods drawn from open source projects. Our experimental data are promising as they demonstrate that a 25-fold reduction in analysis time can be achieved and precision can be increased relative to existing methods.
[Algorithm design and analysis, transfer function, Java, block-level transfer functions, open source projects, Computational modeling, Transfer functions, Lattices, Soot, data flow analysis, domain structure, abstract domain, abstract domains, program structure, Computer science, bilateral algorithm, program semantics, Semantics, symbolic modeling, Java methods, disjoint subsets, Concrete, transfer functions, Data flow analysis]
Access-Path Abstraction: Scaling Field-Sensitive Data-Flow Analysis with Unbounded Access Paths (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Precise data-flow analyses frequently model field accesses through access paths with varying length. While using longer access paths increases precision, their size must be bounded to assure termination, and should anyway be small to enable a scalable analysis. We present Access-Path Abstraction, which for the first time combines efficiency with maximal precision. At control-flow merge points Access-Path Abstraction represents all those access paths that are rooted at the same base variable through this base variable only. The full access paths are reconstructed on demand where required. This makes it unnecessary to bound access paths to a fixed maximal length. Experiments with Stanford SecuriBench and the Java Class Library compare our open-source implementation against a field-based approach and against a field-sensitive approach that uses bounded access paths. The results show that the proposed approach scales as well as a field-based approach, whereas the approach using bounded access paths runs out of memory.
[Context, Stanford SecuriBench, Java, access-path abstraction, control-flow merge points, Target tracking, scalable analysis, field accesses, Scalability, Computational modeling, public domain software, data flow analysis, static analysis, Explosions, Open source software, Analytical models, field-based approach, open-source implementation, field sensitive, Java class library, access path, unbounded access paths, field-sensitive data-flow analysis]
Copy and Paste Redeemed (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Modern software development relies on code reuse, which software engineers typically realise through handwritten abstractions, such as functions, methods, or classes. However, such abstractions can be challenging to develop and maintain. One alternative form of re-use is copy-paste-modify, a methodology in which developers explicitly duplicate source code to adapt the duplicate for a new purpose. We observe that copy-paste-modify can be substantially faster to use than manual abstraction, and past research strongly suggests that it is a popular technique among software developers. We therefore propose that software engineers should forego hand-written abstractions in favour of copying and pasting. However, empirical evidence also shows that copy-paste-modify complicates software maintenance and increases the frequency of bugs. To address this concern, we propose a software tool that merges together similar pieces of code and automatically creates suitable abstractions. This allows software developers to get the best of both worlds: custom abstraction together with easy re-use. To demonstrate the feasibility of our approach, we have implemented and evaluated a prototype merging tool for C++ on a number of near-miss clones (clones with some modifications) in popular Open Source packages. We found that maintainers find our algorithmically created abstractions to be largely preferable to existing duplicated code.
[Algorithm design and analysis, source code (software), public domain software, Merging, C++ listings, Manuals, copy-paste-modify, software developers, prototype merging tool, open source packages, software tools, Face, hand-written abstractions, Refactoring, Static Analysis, Clone management, handwritten abstractions, software engineers, source code duplication, C++, modern software development, Software algorithms, Cloning, code reuse, software tool, Software evolution, Software Engineering, software reusability, Software]
Detecting Broken Pointcuts Using Structural Commonality and Degree of Interest (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Pointcut fragility is a well-documented problem in Aspect-Oriented Programming, changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. Deciding which pointcuts have broken due to base-code changes is a daunting venture, especially in large and complex systems. We present an automated approach that recommends pointcuts that are likely to require modification due to a particular base-code change, as well as ones that do not. Our hypothesis is that join points selected by a pointcut exhibit common structural characteristics. Patterns describing such commonality are used to recommend pointcuts that have potentially broken to the developer. The approach is implemented as an extension to the popular Mylyn Eclipse IDE plug-in, which maintains focused contexts of entities relevant to the task at hand using a Degree of Interest (DOI) model.
[Context, Mylyn Eclipse IDE plug-in, Java, program debugging, DOI model, Programming, Complex systems, structural commonality, broken pointcut detection, pointcut fragility, software evolution, complex systems, structural characteristics, Cities and towns, aspect-oriented programming, Software, Aspect-Oriented programming, degree of interest model, Software engineering]
Covert Communication in Mobile Applications (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
This paper studies communication patterns in mobile applications. Our analysis shows that 63% of the external communication made by top-popular free Android applications from Google Play has no effect on the user-observable application functionality. To detect such covert communication in an efficient manner, we propose a highly precise and scalable static analysis technique: it achieves 93% precision and 61% recall compared to the empirically determined "ground truth\
[Google, Visualization, ground truth, human evaluators, top-popular free Android applications, program diagnostics, Humanoid robots, Interference, communication patterns, scalable static analysis technique, Mobile applications, Servers, covert communication, application experience, Android, application analysis, Android (operating system), mobile computing, Google Play, mobile applications, user-observable application functionality, Androids, communication]
Static Window Transition Graphs for Android (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
This work develops a static analysis to create a model of the behavior of an Android application's GUI. We propose the window transition graph (WTG), a model representing the possible GUI window sequences and their associated events and callbacks. A key component and contribution of our work is the careful modeling of the stack of currently-active windows, the changes to this stack, and the effects of callbacks related to these changes. To the best of our knowledge, this is the first detailed study of this important static analysis problem for Android. We develop novel analysis algorithms for WTG construction and traversal, based on this modeling of the window stack. We also describe an application of the WTG for GUI test generation, using path traversals. The evaluation of the proposed algorithms indicates their effectiveness and practicality.
[graphical user interfaces, program diagnostics, graph theory, window stack modelling, Humanoid robots, graphical user interface, testing, static analysis, smart phones, WTG, Android, Analytical models, mobile computing, Pressing, control-flow analysis, Hardware, GUI, window transition graph, Androids, GUI window sequence, Smart phones, Graphical user interfaces]
Static Analysis of Implicit Control Flow: Resolving Java Reflection and Android Intents (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Implicit or indirect control flow is a transfer of control between procedures using some mechanism other than an explicit procedure call. Implicit control flow is a staple design pattern that adds flexibility to system design. However, it is challenging for a static analysis to compute or verify properties about a system that uses implicit control flow. This paper presents static analyses for two types of implicit control flow that frequently appear in Android apps: Java reflection and Android intents. Our analyses help to resolve where control flows and what data is passed. This information improves the precision of downstream analyses, which no longer need to make conservative assumptions about implicit control flow. We have implemented our techniques for Java. We enhanced an existing security analysis with a more precise treatment of reflection and intents. In a case study involving ten real-world Android apps that use both intents and reflection, the precision of the security analysis was increased on average by two orders of magnitude. The precision of two other downstream analyses was also improved.
[Context, Java, security analysis, Dictionaries, program diagnostics, Type systems, Humanoid robots, static analysis, Android intents, Security, Android, Android (operating system), security of data, implicit control flow, Java reflection, indirect control flow, Androids, Static Analysis, Implicit Control Flow]
String Analysis of Android Applications (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
The desire to understand mobile applications has resulted in researchers adapting classical static analysis techniques to the mobile domain. Examination of data and control flows in Android apps is now a common practice to classify them. Important to these analyses is a fine-grained examination and understanding of strings, since in Android they are heavily used in intents, URLs, reflection, and content providers. Rigorous analysis of string creation, usage, and value characteristics offers additional information to increase precision of app classification. This paper shows that inter-procedural static analysis that specifically targets string construction and usage can be used to reveal valuable insights for classifying Android apps. To this end, we first present case studies to illustrate typical uses of strings in Android apps. We then present the results of our analysis on real-world malicious and benign apps. Our analysis examines how strings are created and used for URL objects, Java reflection, and Android intents, and infers the actual string values used as much as possible. Our results demonstrate that string disambiguation based on creation, usage, and value indeed provides additional information that may be used to improve precision of classifying application behaviors.
[Java, Humanoid robots, fine-grained examination, Android intents, Servers, string disambiguation, Uniform resource locators, Android (operating system), Android applications, Java reflection, Malware, Androids, URL objects, Smart phones, string analysis]
Semantic Slicing of Software Version Histories (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Software developers often need to transfer func-tionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a specific subset of the change history, "inheriting" additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We refer to our approach, CSLICER, as semantic slicing of version histories. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and instantiate it in a specific implementation for Java projects managed in Git. We evaluate the correctness and effectiveness of our approach on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones.
[low-level version history, program debugging, public domain software, Java project, History, Semantics, CSLICER, software version history, configuration management system, program slicing, open-source software repository, Context, semantic slicing problem, Java, change history, high-level semantic functionality, Software algorithms, dependency, Software changes, bug fix, configuration management, version history, unwanted functionality, Syntactics, Software, automated support, software developer]
Development History Granularity Transformations (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Development histories can simplify some software engineering tasks, butdifferent tasks require different history granularities. For example, a history that includes every edit that resulted in compiling code is needed when searching for the cause of a regression, whereas a history that contains only changes relevant to a feature is needed for understanding the evolution of the feature. Unfortunately, today, both manual and automated history generation result in a single-granularity history. This paper introduces the concept of multi-grained development history views and the architecture of Codebase Manipulation, a tool that automatically records a fine-grained history and manages its granularity by applying granularity transformations.
[Codebase Manipulation, Manuals, Transforms, regression analysis, Maintenance engineering, development history granularity transformations, history rewriting, History, Compounds, multigrained development history views, fine-grained development history, regression, automated history generation, Computer architecture, history transformation, Software, software engineering, software engineering tasks, single-granularity history, automated version control, compiling code]
Quantification of Software Changes through Probabilistic Symbolic Execution (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Characterizing software changes is fundamental for software maintenance. However existing techniques are imprecise leading to unnecessary maintenance efforts. We introduce a novel approach that computes a precise numeric characterization of program changes, which quantifies the likelihood of reaching target program events (e.g., assert violations or successful termination) and how that evolves with each program update, together with the percentage of inputs impacted by the change. This precise characterization leads to a natural ranking of different program changes based on their probability of execution and their impact on target events. The approach is based on model counting over the constraints collected with a symbolic execution of the program, and exploits the similarity between program versions to reduce cost and improve the quality of analysis results. We implemented our approach in the Symbolic PathFinder tool and illustrate it on several Java case studies, including the evaluation of different program repairs, mutants used in testing, or incremental analysis after a change.
[Java, program testing, Computational modeling, probability, software change quantification, Maintenance engineering, Probability, Probabilistic logic, software maintenance, program update, probabilistic symbolic execution, Software, IP networks, Symbolic PathFinder tool, program repairs]
Automatic Detection of Potential Layout Faults Following Changes to Responsive Web Pages (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Due to the exponential increase in the number ofmobile devices being used to access the World Wide Web, it iscrucial that Web sites are functional and user-friendly across awide range of Web-enabled devices. This necessity has resulted in the introduction of responsive Web design (RWD), which usescomplex cascading style sheets (CSS) to fluidly modify a Web site's appearance depending on the viewport width of the device in use. Although existing tools may support the testing of responsive Web sites, they are time consuming and error-prone to use because theyrequire manual screenshot inspection at specified viewport widths. Addressing these concerns, this paper presents a method thatcan automatically detect potential layout faults in responsively designed Web sites. To experimentally evaluate this approach, weimplemented it as a tool, called ReDeCheck, and applied itto 5 real-world web sites that vary in both their approach toresponsive design and their complexity. The experiments revealthat ReDeCheck finds 91% of the inserted layout faults.
[fault diagnosis, program testing, World Wide Web, responsive web, HTML, Mobile handsets, REDECHECK tool, responsive Web site testing, responsive Web design, Web site appearance, Web-enabled devices, manual screenshot inspection, automatic layout fault detection, inserted layout faults, RWD, Cascading style sheets, CSS, software testing, functional user-friendly Web sites, empirical studies, complex cascading style sheets, viewport widths, Web design, Layout, Web pages, mobile devices, human computer interaction]
Developing a DSL-Based Approach for Event-Based Monitoring of Systems of Systems: Experiences and Lessons Learned (E)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Complex software-intensive systems are often described as systems of systems (SoS) comprising heterogeneous architectural elements. As SoS behavior fully emerges during operation only, runtime monitoring is needed to detect deviations from requirements. Today, diverse approaches exist to define and check runtime behavior and performance characteristics. However, existing approaches often focus on specific types of systems and address certain kinds of checks, thus impeding their use in industrial SoS. Furthermore, as many SoS need to run continuously for long periods, the dynamic definition and deployment of constraints needs to be supported. In this paper we describe experiences of developing and applying a DSL-based approach for monitoring an SoS in the domain of industrial automation software. We evaluate both the expressiveness of our DSL as well as the scalability of the constraint checker. We also describe lessons learned.
[complex software-intensive system, requirements monitoring, Iron, factory automation, constraint checker scalability, industrial automation software, DSL-based approach, Runtime, runtime monitoring, SoS, performance characteristics, constraint handling, Systems of systems, Monitoring, event-based monitoring, domain-specific languages, Automation, runtime behavior checking, Steel, constraint checking, Casting, systems of systems, heterogeneous architectural elements, computerised monitoring, system monitoring, Software, visual programming, constraint deployment]
Generating Qualifiable Avionics Software: An Experience Report (E)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.
[qualifiable avionics software, product-line technology, Helicopters, Aerospace electronics, Encoding, avionics, program compilers, helicopters, data-management component, NH90 helicopter, model-based technology, Hardware, software engineering, Airbus helicopter, System software, Interviews, legally-binding certification process, software repository analysis]
How Verified is My Code? Falsification-Driven Verification (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Formal verification has advanced to the point that developers can verify the correctness of small, critical modules. Unfortunately, despite considerable efforts, determining if a "verification" verifies what the author intends is still difficult. Previous approaches are difficult to understand and often limited in applicability. Developers need verification coverage in terms of the software they are verifying, not model checking diagnostics. We propose a methodology to allow developers to determine (and correct) what it is that they have verified, and tools to support that methodology. Our basic approach is based on a novel variation of mutation analysis and the idea of verification driven by falsification. We use the CBMC model checker to show that this approach is applicable not only to simple data structures and sorting routines, and verification of a routine in Mozilla's JavaScript engine, but to understanding an ongoing effort to verify the Linux kernel Read-Copy-Update (RCU) mechanism.
[Java, Linux kernel read-copy-update mechanism, falsification-driven verification, falsification, Sorting, CBMC model checker, mutation, formal verification, model checking, Linux, oracles, Computer bugs, mutation analysis, Model checking, Software, Arrays, JavaScript engine, test harnesses, verification, Software engineering]
Mining User Opinions in Mobile App Reviews: A Keyword-Based Approach (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
User reviews of mobile apps often contain complaints or suggestions which are valuable for app developers to improve user experience and satisfaction. However, due to the large volume and noisy-nature of those reviews, manually analyzing them for useful opinions is inherently challenging. To address this problem, we propose MARK, a keyword-based framework for semi-automated review analysis. MARK allows an analyst describing his interests in one or some mobile apps by a set of keywords. It then finds and lists the reviews most relevant to those keywords for further analysis. It can also draw the trends over time of those keywords and detect their sudden changes, which might indicate the occurrences of serious issues. To help analysts describe their interests more effectively, MARK can automatically extract keywords from raw reviews and rank them by their associations with negative reviews. In addition, based on a vector-based semantic representation of keywords, MARK can divide a large set of keywords into more cohesive subsets, or suggest keywords similar to the selected ones.
[keyword-based approach, app developer, Energy consumption, Dictionaries, vector-based semantic representation, Review Analysis, Keyword, data mining, Mobile communication, Batteries, keyword-based framework, Data mining, user experience, MARK, mobile app review, mobile computing, Opinion Mining, mining user opinion, Facebook, semiautomated review analysis, user satisfaction]
"What Parts of Your Apps are Loved by Users?" (T)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Recently, Begel et al. found that one of the most important questions software developers ask is "what parts of software are used/loved by users." User reviews provide an effective channel to address this question. However, most existing review summarization tools treat reviews as bags-of-words (i.e., mixed review categories) and are limited to extract software aspects and user preferences. We present a novel review summarization framework, SUR-Miner. Instead of a bags-of-words assumption, it classifies reviews into five categories and extracts aspects for sentences which include aspect evaluation using a pattern-based parser. Then, SUR-Miner visualizes the summaries using two interactive diagrams. Our evaluation on seventeen popular apps shows that SUR-Miner summarizes more accurate and clearer aspects than state-of-the-art techniques, with an F1-score of 0.81, significantly greater than that of ReviewSpotlight (0.56) and Guzmans' method (0.55). Feedback from developers shows that 88% developers agreed with the usefulness of the summaries from SUR-Miner.
[Visualization, User Feedback, data mining, software aspect extraction, Data Mining, diagrams, Sentiment Analysis, Data mining, Semantics, feature extraction, review summarization tool, Market research, user preference, software engineering, pattern-based parser, pattern classification, software development, Review Summarization, SUR-Miner, software reviews, review classification, grammars, Feature extraction, Software, interactive diagram, Software engineering, bags-of-words]
Ensemble Methods for App Review Classification: An Approach for Software Evolution (N)
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
App marketplaces are distribution platforms for mobile applications that serve as a communication channel between users and developers. These platforms allow users to write reviews about downloaded apps. Recent studies found that such reviews include information that is useful for software evolution. However, the manual analysis of a large amount of user reviews is a tedious and time consuming task. In this work we propose a taxonomy for classifying app reviews into categories relevant for software evolution. Additionally, we describe an experiment that investigates the performance of individual machine learning algorithms and its ensembles for automatically classifying the app reviews. We evaluated the performance of the machine learning techniques on 4550 reviews that were systematically labeled using content analysis methods. Overall, the ensembles had a better performance than the individual classifiers, with an average precision of 0.74 and 0.59 recall.
[Google, pattern classification, User Feedback, Software Evolution, individual classifiers, Taxonomy, automatic application review classification, Manuals, communication channel, App Reviews, software evolution, Support vector machines, mobile computing, Text Classification, mobile applications, ensemble methods, Prediction algorithms, Software, software engineering, content analysis methods, Labeling, learning (artificial intelligence), machine learning algorithms]
The ReMinds Tool Suite for Runtime Monitoring of Systems of Systems
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
The behavior of systems of systems (SoS) emerges only fully during operation and is hard to predict. SoS thus need to be monitored at runtime to detect deviations from important requirements. However, existing approaches for checking runtime behavior and performance characteristics are limited with respect to the kinds of checks and the types of technologies supported, which impedes their use in industrial SoS. In this tool demonstration paper we describe the ReMinds tool suite for runtime monitoring of SoS developed in response to industrial monitoring scenarios. ReMinds provides comprehensive tool support for instrumenting systems, extracting events and data at runtime, defining constraints to check expected behavior and properties, and visualizing constraint violations to facilitate diagnosis.
[Java, program verification, runtime behavior checking, systems of systems runtime monitoring, industrial monitoring scenarios, tool support, Runtime, Aggregates, runtime monitoring, Data visualization, system monitoring, industrial SoS, software tools, ReMinds tool suite, Probes, Monitoring, System of systems]
DRIVER -- A Platform for Collaborative Framework Understanding
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Application frameworks are a powerful technique for large-scale reuse but often very hard to learn from scratch. Although good documentation helps on reducing the learning curve, it is often found lacking, and costly, as it needs to attend different audiences with disparate learning needs. When code and documentation prove insufficient, developers turn to their network of experts. The lack of awareness about the experts, interrupting the wrong people, and experts unavailability are well known hindrances to effective collaboration. This paper presents the DRIVER platform, a collaborative learning environment for framework users to share their knowledge. It provides the documentation on a wiki, where the learning paths of the community of learners can be captured, shared, rated, and recommended, thus tapping into the collective knowledge of the community of framework users. The tool can be obtained at http://bit.ly/driverTool.
[Electronic publishing, computer science education, Knowledge based systems, collaborative, Documentation, DRIVER, learning, Best practices, collaborative framework understanding, large-scale reuse, tool, collaborative learning environment, framework, Information services, Collaboration, groupware, software reusability, Internet, computer aided instruction, learning needs, learning curve]
Tool Support for Analyzing Mobile App Reviews
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Mobile app reviews often contain useful user opinions for app developers. However, manual analysis of those reviews is challenging due to their large volume and noisynature. This paper introduces MARK, a supporting tool for review analysis of mobile apps. With MARK, an analyst can describe her interests of one or more apps via a set of keywords. MARK then lists the reviews most relevant to those keywords for further analyses. It can also draw the trends over time of the selected keywords, which might help the analyst to detect sudden changes in the related user reviews. To help the analyst describe her interests more effectively, MARK can automatically extract and rank the keywords by their associations with negative reviews, divide a large set of keywords into more cohesive subgroups, or expand a small set into a broader one.
[Energy consumption, Google, text analysis, App Review, Keyword, data mining, Mobile communication, Batteries, software reviews, MARK, Mining and Analyzing Reviews by Keywords tool, mobile computing, online reviews, Opinion Mining, Semantics, Market research, Facebook, mobile app reviews]
Recommending API Usages for Mobile Apps with Hidden Markov Model
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Mobile apps often rely heavily on standard API frameworks and libraries. However, learning to use those APIs is often challenging due to the fast-changing nature of API frameworks and the insufficiency of documentation and code examples. This paper introduces DroidAssist, a recommendation tool for API usages of Android mobile apps. The core of DroidAssist is HAPI, a statistical, generative model of API usages based on Hidden Markov Model. With HAPIs trained from existing mobile apps, DroidAssist could perform code completion for method calls. It can also check existing call sequences to detect and repair suspicious (i.e. unpopular) API usages.
[Android mobile applications, application program interfaces, hidden Markov model, Humanoid robots, suspicious API usage detection, Mobile communication, Data mining, API usage recommendation, hidden Markov models, mobile computing, HAPI, suspicious API usage repair, API usage, statistical generative model, Statistical code completion, Documentation, Maintenance engineering, method calls, DroidAssist recommendation tool, recommender systems, code completion, call sequences, Hidden Markov models, Androids, statistical analysis]
FLYAQ: Enabling Non-expert Users to Specify and Generate Missions of Autonomous Multicopters
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Multicopters are increasingly popular since they promise to simplify a myriad of everyday tasks. Currently, vendors provide low-level APIs and basic primitives to program multicopters, making mission development a task-specific and error-prone activity. As a consequence, current approaches are affordable only for users that have a strong technical expertise. Then, software engineering techniques are needed to support the definition, development, and realization of missions at the right level of abstraction and involving teams of autonomous multicopters that guarantee the safety today's users expect. In this paper we describe a tool that enables end-users with no technical expertise, e.g., firefighters and rescue workers, to specify missions for a team of multicopters. The detailed flight plan that each multicopter must perform to accomplish the specified mission is automatically generated by preventing collisions between multicopters and obstacles, and ensuring the preservation of no-fly zones.
[application program interfaces, autonomous multicopters, Multicopter, Solar panels, error-prone activity, no-fly zones, low-level API, task-specific activity, Model-Driven Engineering, helicopters, Earthquakes, Domain-specific Languages, aerospace computing, software engineering techniques, Software, software engineering, Safety, nonexpert users, control engineering computing, FLYAQ, program multicopters, Monitoring, Drones, Software engineering]
Lazy-CSeq: A Context-Bounded Model Checking Tool for Multi-threaded C-Programs
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Lazy-CSeq is a context-bounded verification tool for sequentially consistent C programs using POSIX threads. It first translates a multi-threaded C program into a bounded nondeterministic sequential C program that preserves bounded reachability for all round-robin schedules up to a given number of rounds. It then reuses existing high-performance bounded model checkers as sequential verification backends. Lazy-CSeq handles the full C language and the main parts of the POSIX thread API, such as dynamic thread creation and deletion, and synchronization via thread join, locks, and condition variables. It supports assertion checking and deadlock detection, and returns counterexamples in case of errors. Lazy-CSeq outperforms other concurrency verification tools and has won the concurrency category of the last two SV-COMP verification competitions.
[context-bounded, Schedules, multi-thread, application program interfaces, Programming, C language, deadlock detection, Concurrent computing, context-bounded model checking tool, formal verification, multithreaded C-programs, sequential verification backends, Lazy-CSeq, multi-threaded, round-robin schedules, concurrency verification tools, Context, bounded reachability, multi-threading, SV-COMP verification competitions, context-bounded verification tool, bounded nondeterministic sequential C program, concurrency, bounded model-checking, assertion checking, model checkers, C programs, System recovery, Arrays, POSIX thread API]
SpyREST in Action: An Automated RESTful API Documentation Tool
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
RESTful APIs are often manually documented. As a result, the process of maintaining the documentation of RESTful APIs is both expensive and error-prone. In this demonstration paper, we present SpyREST as an automated software as a service tool that can be used to document RESTful APIs. SpyREST leverages an HTTP Proxy server to intercept real API calls to automatically collect and generate RESTful API documentation by processing HTTP traffic involved in API calls. SpyREST provides an automated yet customizable example based documentation solution for RESTful APIs. RESTful API developers can use SpyREST to automatically generate and maintain updated API documentation.
[Context, RESTful API, automated RESTful API documentation tool, HTTP traffic, Web API, Automation, application program interfaces, SpyREST, Software as a service, Documentation, HTTP proxy server, Servers, Uniform resource locators, Collaboration, cloud computing, Payloads, software as a service tool, Example based documentation]
Clone Merge -- An Eclipse Plugin to Abstract Near-Clone C++ Methods
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Software clones are prevalent. In the work of Lague&#x0308; et al. [2], they observe that 6.4% and 7.5% of the source code in different versions of a large, mature code base are clones. The work of Baxter et al. [1] reports even higher numbers, sometimes exceeding 25%. We consider the prevalence of such near miss clones to be strong indicators that copy-paste-modify is a wide-spread development methodology. Even though clones are prevalent, they are a significant development headache. Specially, if bugs arise in one of the clones, they need to be fixed in all of the clones. This problem is acknowledged in the work of Juergens et al. [4] who say in their work that "cloning can be a substantial problem during development and maintenance\
[Industries, source code (software), program debugging, Eclipse, Switches, Eclipse CDT plugin, clone merge, bugs, near miss software clones, CDT, Prototypes, Evolution, Refactoring, Clone, copy paste merge refactoring, software development, Cloning, source code, Maintenance engineering, C++ language, software maintenance, Abstraction, near-clone C++ methods, Syntactics, Software engineering]
Pseudogen: A Tool to Automatically Generate Pseudo-Code from Source Code
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Understanding the behavior of source code written in an unfamiliar programming language is difficult. One way to aid understanding of difficult code is to add corresponding pseudo-code, which describes in detail the workings of the code in a natural language such as English. In spite of its usefulness, most source code does not have corresponding pseudo-code because it is tedious to create. This paper demonstrates a tool Pseudogen that makes it possible to automatically generate pseudo-code from source code using statistical machine translation (SMT). Pseudogen currently supports generation of English or Japanese pseudo-code from Python source code, and the SMT framework makes it easy for users to create new generators for their preferred source code/pseudo-code pairs.
[source code (software), English pseudocode generation, English language, statistical machine translation, Natural languages, Japanese pseudocode generation, Pseudogen tool, Programming, SMT framework, Generators, automatic pseudocode generation, program compilers, Training, Computer languages, Python source code, Syntactics, machine translation, programming language, Arrays, natural language, language translation]
CIVL: Formal Verification of Parallel Programs
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
CIVL is a framework for static analysis and verification of concurrent programs. One of the main challenges to practical application of these techniques is the large number of ways to express concurrency: MPI, OpenMP, CUDA, and Pthreads, for example, are just a few of many "concurrency dialects" in wide use today. These dialects are constantly evolving and it is increasingly common to use several of them in a single "hybrid" program. CIVL addresses these problems by providing a concurrency intermediate verification language, CIVL-C, as well as translators that consume C programs using these dialects and produce CIVL-C. Analysis and verification tools which operate on CIVL-C can then be applied easily to a wide variety of concurrent C programs. We demonstrate CIVL's error detection and verification capabilities on (1) an MPI+OpenMP program that estimates &#x03C0; and contains a subtle race condition, and (2) an MPI-based 1d-wave simulator that fails to conform to a simple sequential implementation.
[program verification, CIVL, software verification, Graphics processing units, MPI, Pthread, Electronic mail, C language, parallel programming, parallel programs, Concurrent computing, hybrid program, formal verification, MPI-plus-OpenMP program, Libraries, program diagnostics, Government, OpenMP, Maintenance engineering, static analysis, concurrency intermediate verification language, Standards, function equivalence, CUDA, concurrent program, model checking, symbolic execution, MPI-based 1d-wave simulator, C program, parallel program]
Refactorings for Android Asynchronous Programming
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Running compute-intensive or blocking I/O operationsin the UI event thread of smartphone apps can severelydegrade responsiveness. Despite the fact that Android provides several async constructs that developers can use, developers can still miss opportunities to encapsulate long-running operations in async constructs. On the other hand, they can use the inappropriate async constructs, which result in memory leaks, lost results, and wasted energy. Fortunately, refactoring tools can eliminate these problems by retrofitting asynchrony to sequential code and transforming async code to use the appropriate constructs. This demo presents two refactoring tools for Android apps: (i) ASYNCHRONIZER, a refactoring tool that enables developers to extract long-running operations into Android AsyncTask. (ii) ASYNCDROID, a refactoring tool which enables developers to transform existing improperly-used AsyncTask into Android IntentService.
[Android IntentService, long-running operation encapsulation, Asynchronous, Instruction sets, Humanoid robots, smartphone apps, Programming, user interfaces, refactoring tools, UI event thread, Android AsyncTask, Android (operating system), sequential code, I/O operations, improperly-used AsyncTask, Safety, software tools, compute-intensive operations, Refactoring, Graphical user interfaces, Message systems, async code, Android asynchronous programming, smart phones, software maintenance, Android, memory leaks, asynchronizer, Androids, ASYNCDROID]
GRT: An Automated Test Generator Using Orchestrated Program Analysis
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
While being highly automated and easy to use, existing techniques of random testing suffer from low code coverage and defect detection ability for practical software applications. Most tools use a pure black-box approach, which does not use knowledge specific to the software under test. Mining and leveraging the information of the software under test can be promising to guide random testing to overcome such limitations. Guided Random Testing (GRT) implements this idea. GRT performs static analysis on software under test to extract relevant knowledge and further combines the information extracted at run-time to guide the whole test generation procedure. GRT is highly configurable, with each of its six program analysis components implemented as a pluggable module whose parameters can be adjusted. Besides generating test cases, GRT also automatically creates a test coverage report. We show our experience in GRT tool development and demonstrate its practical usage using two concrete application scenarios.
[program analysis components, program testing, program diagnostics, automated test generator, GRT, static analysis, Generators, dynamic analysis, bug detection, Data mining, black-box approach, Impurities, test coverage report, low code coverage, defect detection ability, software under test, Software systems, Libraries, orchestrated program analysis, Automatic test generation, Testing, random testing]
LED: Tool for Synthesizing Web Element Locators
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Web applications are growing fast in popularity and complexity. One of the major problems faced by web developers is writing JavaScript code that can retrieve Document Object Model (DOM) tree elements, and is consistent among multiple DOM states. We attempt to solve this problem by automatically synthesizing JavaScript code that interacts with the DOM. We present an automated tool called LED, to analyze the DOM elements, and synthesize code to select the DOM elements based on the DOM hierarchy as well as the nature of task that the user wants to perform. LED provides an interactive drag and drop support inside the browser for selecting positive and negative examples of DOM elements. We find that LED supports at least 86% of the locators used in the JavaScript code of deployed web applications, and that the locators synthesized by LED have a recall of 98% and a precision of 63%. LED is fast, taking only 0.23 seconds on average to synthesize a locator.
[DOM state, Web applications, Web element locator, automated tool, Element locators, Program synthesis, Web developer, CSS selectors, Selenium, object-oriented methods, DOM tree element, Cascading style sheets, Java, Automation, program diagnostics, trees (mathematics), Light emitting diodes, LED, JavaScript code, Browsers, document object model tree element, Web application, Writing, DOM element, Programming by example, Mice, Internet]
SiPL -- A Delta-Based Modeling Framework for Software Product Line Engineering
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Model-based development has become a widely-used approach to implement software, e.g. for embedded systems. Models replace source code as primary executable artifacts in these cases. Software product line technologies for these domains must be able to generate models as instances of an SPL. This need is addressed among others by an implementation technology for SPLs known as delta modeling. Current approaches to delta modeling require deltas to be written manually using delta languages, and they offer only very limited support for creating and testing a network of deltas. This paper presents a new approach to delta modeling and a supporting tool suite: the abstract notion of a delta is refined to be a consistency-preserving edit script which is generated by comparing two models. The rich structure of edit scripts allows us to detect conflicts and further relations between deltas statically and to implement restructurings in delta sets such as the merging of two deltas. We illustrate the tooling using a case study.
[software product lines, Visualization, software product line engineering, delta modeling, Unified modeling language, source code, Software product lines, Standards, SPL, Software packages, SiPL, delta-based modeling framework, model-based development, software product line technology, delta language, Graphical user interfaces, model differencing]
Model-Based Testing of Stateful APIs with Modbat
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Modbat makes testing easier by providing a user-friendly modeling language to describe the behavior of systems, from such a model, test cases are generated and executed. Modbat's domain-specific language is based on Scala, its features include probabilistic and non-deterministic transitions, component models with inheritance, and exceptions. We demonstrate the versatility of Modbat by finding a confirmed defect in the currently latest version of Java, and by testing SAT solvers.
[Java, application program interfaces, program testing, Modbat, SAT solvers, component-based systems, exception testing, Scala, computability, probabilistic transitions, extended finite-state machines, user-friendly modeling language, domain-specific language, nondeterministic transitions, software test tools, model-based testing, stateful API, Data models, Libraries, simulation languages, Arrays, DSL, Testing]
ActivitySpace: A Remembrance Framework to Support Interapplication Information Needs
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Developers' daily work produces, transforms, and communicates cross-cutting information across applications, including IDEs, emails, Q&amp;A sites, Twitter, and many others. However, these applications function independently of one another. Even though each application has their own effective information management mechanisms, cross-cutting information across separate applications creates a problem of information fragmentation, forcing developers to manually track, correlate, and re-find cross-cutting information across applications. In this paper, we present ActivitySpace, a remembrance framework that unobtrusively tracks and analyze a developer's daily work in separate applications, and provides various semantic and episodic UIs that help developers correlate and re-find cross-cutting information across applications based on information content, time and place of his/her activities. Through a user study of 8 participants, we demonstrate how ActivitySpace helps to tackle information fragmentation problem in developers' daily work.
[Computers, software development management, ActivitySpace, user interfaces, History, information management mechanism, remembrance framework, information fragmentation, semantic UI, interapplication information needs, episodic UI, Databases, Image color analysis, Semantics, Software, Mice]
Investigating Program Behavior Using the Texada LTL Specifications Miner
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Temporal specifications, relating program events through time, are useful for tasks ranging from bug detection to program comprehension. Unfortunately, such specifications are often lacking from system descriptions, leading researchers to investigate methods for inferring these specifications from code, execution traces, code comments, and other artifacts. This paper describes Texada, a tool to dynamically mine temporal specifications in LTL from traces of program activity. We review Texada's key features and demonstrate how it can be used to investigate program behavior through two scenarios: validating an implementation that solves the dining philosophers problem and supporting comprehension of a stack implementation. We also detail Texada's other, more advanced, usage options. Texada is an open source tool: https://bitbucket.org/bestchai/texada.
[program comprehension, code comments, Java, program debugging, program activity, program behavior, public domain software, data mining, program events, execution traces, linear temporal logic, Data structures, Texada LTL specification miner, bug detection, specification mining, formal specification, texada, Concurrent computing, Runtime, dynamic temporal specification mining, Distance measurement, open source tool]
The iMPAcT Tool: Testing UI Patterns on Mobile Applications
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
This paper presents the iMPAcT tool that tests recurring behaviour, i.e., UI Patterns, on mobile applications. This tool is implemented in Java and makes use of Android's APIs UI Automator and UiAutomation. The tool automatically explores a mobile application in order to automatically identify and test UI Patterns. Each UI Pattern has a test strategy, Test Patterns, associated, which is applied when an UI Pattern is found. The approach works on top of a catalogue of UI Patterns, which determines which UI Patterns are to be tested, and what should their correct behaviour be, and may be used for any application.
[Java, UI pattern testing, UI Patterns, application program interfaces, Humanoid robots, data flow analysis, Mobile communication, Mobile applications, recurring behaviour testing, Reverse Engineering, Android (operating system), mobile computing, UiAutomation, Layout, mobile applications, Mobile Testing, Pattern-based testing, Android API, Androids, UI Automator, Pattern matching, iMPAcT tool, Testing]
Measuring Object-Oriented Design Principles
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
The idea of automatizing the assessment of object-oriented design is not new. Different approaches define and apply their own quality models, which are composed of single metrics or combinations thereof, to operationalize software design. However, single metrics are too fine-grained to identify core design flaws and they cannot provide hints for making design improvements. In order to deal with these weaknesses of metric-based models, rules-based approaches have proven successful in the realm of source-code quality. Moreover, for developing a well-designed software system, design principles play a key role, as they define fundamental guidelines and help to avoid pitfalls. Therefore, this thesis will enhance and complete a rule-based quality reference model for operationalizing design principles and will provide a measuring tool that implements these rules. The validation of the design quality model and the measurement tool will be based on various industrial projects. Additionally, quantitative and qualitative surveys will be conducted in order to get validated results on the value of object-oriented design principles for software development.
[Context, source code (software), object-oriented programming, software-design quality, software development, design model, Object oriented modeling, software design, software quality, rule-based quality reference model, softwaredesign assessment method, Software design, object-oriented design, knowledge based systems, source-code quality, object-oriented methods, design principles, Software measurement, Software engineering, metric-based model]
Stability of Self-Adaptive Software Architectures
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Stakeholders and organisations are increasingly looking for long-lived software. As architectures have a profound effect on the operational life-time of the software and the quality of the service provision, architectural stability could be considered a primary criterion towards achieving the long-livety of the software. Architectural stability is envisioned as the next step in quality attributes, combining many inter-related qualities. This research suggests the notion of behavioural stability as a primary criterion for evaluating whether the architecture maintains achieving the expected quality attributes, maintaining architecture robustness, and evaluating how well the architecture accommodates run-time evolutionary changes. The research investigates the notion of architecture stability at run-time in the context of self-adaptive software architectures. We expect to define, characterise and analyse this intuitive concept, as well as identify the consequent trade-offs to be dynamically managed and enhance the self-adaptation process for a long-lived software.
[self-adaptive software architecture, Adaptation models, service provision quality, software quality, software operational life-time, software architecture, Software architecture, Stability criteria, Computer architecture, Software, Robustness, architectural stability, stability]
MetaMod: A Modeling Formalism with Modularity at Its Core
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Because modern engineering products require more and more functionality, the models used in the design of these products get larger and more complex. A way to handle this complexity would be a suitable mechanism to modularize models. However, current approaches in the Model Driven Engineering field have limited support for modularity. This is the gap that our research addresses. We want to tackle the gap by designing and creating a modeling formalism with modularity at its core - MetaMod. We are including the modeling formalism into a prototype such that we can experiment with it. Our evaluation plan includes bootstrapping MetaMod (defining MetaMod in MetaMod) and creating an industrial DSL in MetaMod.
[modeling, Object oriented modeling, Computational modeling, bootstrapping MetaMod, Documentation, Calculus, Complexity theory, formal specification, modeling formalism, modularity, Prototypes, model driven engineering field, DSLs, Software, statistical analysis, industrial DSL]
A Generic Framework for Concept-Based Exploration of Semi-Structured Software Engineering Data
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Software engineering meta-data (SE data), such as revision control data, Github project data or test reports, is typically semi-structured, it comprises a mixture of formatted and free-text fields and is often self-describing. Semi-structured SE data cannot be queried in a SQL-like manner because of its lack of structure. Consequently, there are a variety of customized tools built to analyze specific datasets but these do not generalize. We propose to develop a generic framework for exploration and querying of semi-structured SE data. Our approach investigates the use of a formal concept lattice as a universal data structure and a tag cloud as an intuitive interface to support data exploration.
[universal data structure, SQL-like manner, Lattices, user interfaces, semistructured software engineering data, tag clouds, query processing, formatted fields, Prototypes, tag cloud, data structures, software engineering, data exploration, semistructured SE data querying, browsing software repositories, Context, meta data, Navigation, free-text fields, Github project data, test reports, formal concept analysis, SQL, intuitive interface, Data visualization, customized tools, Tag clouds, Software engineering, revision control data]
Understanding, Refactoring, and Fixing Concurrency in C#
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Industry leaders provide concurrent libraries because asynchronous &amp; parallel programming are increasingly in demand: responsiveness, scalability, and high-throughput are key elements of all modern applications. However, we know little about how developers use these concurrent libraries in practice and the developer's toolbox for concurrency is very limited. We present the first study that analyzes the usage of concurrent libraries in large codebases, such as 2258 open-source C# apps comprising 54M SLOC and 1378 open-source Windows Phone apps comprising 12M SLOC. Using this data, we find important problems about use and misuse of concurrency. Inspired by our findings, we designed, evaluated, and implemented several static analyses and refactoring tools.
[Java, 12M SLOC, program diagnostics, public domain software, open-source Windows Phone apps, software maintenance, Open source software, parallel programming, 54M SLOC, refactoring tools, Concurrent computing, Reactive power, concurrent libraries, Parallel programming, static analyses, open-source C# apps, Libraries, software refactoring, concurrency (computers), asynchronous programming]
A Message-Passing Architecture without Public Ids Using Send-to-Behavior
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
We explore a novel model of computation based on nodes that have no public addresses (ids). We define nodes as concurrent, message-passing computational entities in an abstract communication medium, similar to the Actor model, but with all public node ids elided. Instead, drawing inspiration from biological systems, we postulate a send-to-behavior language construct to enable anonymous one-way communication. A behavior, defined as a function of input to actions, is also an intensional definition of the subset of nodes that express it. Sending to a behavior is defined to deliver the message to one or more nodes that implement that behavior.
[Computers, communication medium, Protocols, message passing, application program interfaces, send-to-behavior language, behavior composition, Computational modeling, message-passing computational entity, Programming, multi node, Topology, parallel programming, message-passing architecture, actor model, Computer architecture, Hardware, public node]
Publisher's Information
2015 30th IEEE/ACM International Conference on Automated Software Engineering
None
2015
Provides a listing of current committee members and society officers.
[]
Program generation for performance
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
It has become extraordinarily difficult to write software that performs close to optimally on complex modern microarchitectures. Particularly plagued are domains that require complex mathematical computations such as multimedia processing, communication, control, graphics, and machine learning. In these domains, performance-critical components are usually written in C (with possible extensions) and often even in assembly, carefully &#x201C;tuned&#x201D; to the platform's architecture and microarchitecture. The result is usually long, rather unreadable code that needs to be re-written or re-tuned with every platform upgrade. On the other hand, the performance penalty for relying on straightforward, non-tuned, &#x201C;more elegant&#x201D; implementations can be often a factor of 10, 100, or even more. The overall problem is one of productivity, maintainability, and quality (namely performance), i.e., software engineering. However, even though a large set of sophisticated software engineering theory and tools exist, it appears that to date this community has not focused much on mathematical computations nor performance in the detailed, close-to-optimal sense above. The reason for the latter may be that performance, unlike various aspects of correctness, is not syntactic in nature (and in reality is often even unpredictable and, well, messy). The aim of this talk is to draw attention to the performance/productivity problem for mathematical applications and to make the case for a more interdisciplinary attack. As a set of thoughts in this direction we offer some of the lessons we have learned in the last decade in our own research on Spiral (www.spiral.net), a program generation framework for numerical kernels. Key techniques used in Spiral include staged declarative domain-specific languages to express algorithm knowledge and algorithm transformations, the use of platform-cognizant rewriting systems for parallelism and locality optimizations, and the use of search and machine learning techniques to navigate possible spaces of choices. Experimental results show that the codegenerated by Spiral competes with, and sometimes outperforms, the best available human-written code. Spiral has been used to generate part of Intel's commercial libraries IPP and MKL.
[rewriting systems, automatic programming, software maintainability, Fourier transform, Program generation, software quality, software maintenance, machine learning, parallelization, program compilers, matrix algebra, performance-critical component, software architecture, vectorization, program generation framework, platform architecture, software engineering, learning (artificial intelligence), software performance, software performance evaluation, rewriting system]
Changing microsoft's build: Revolution or evolution
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Tens of thousands of Microsoft engineers build and test hundreds of software products several times a day. It is essential that this continuous integration scales, guarantees short feedback cycles, and functions reliably with minimal human intervention. During the past three years TSE's charter has been to shorten this cycle time. We went after this goal in two ways: Evolution via CloudBuild and Revolution via Concord. CloudBuild is a build service infrastructure, now being used by all major product groups in Microsoft, like Azure, Bing, Office, SQL except for Windows. CloudBuild addresses all aspects of a continuous integration workflow, like builds, test and code analysis, but also drops, package and symbol creation and storage. CloudBuild supports multiple build languages as long as they fulfill a coarse grained IO based contract. CloudBuild uses content based caching to run build-related tasks only when needed. Lastly, it builds on many machines in parallel. The speed ups of build and testing range from 1.2x to 10x. CloudBuild aims to rapidly onboard teams and hence has to support non-deterministic build tools and specification languages that under-declare dependencies. CloudBuild, being a reliable build service in the presence of unreliable components, currently achieves service availability better than 99%. Windows went a different path. Their past build exhaust was so massive that building Windows in the cloud and bringing the build results back for testing on corp.-net. was considered infeasible. So they decided to move to a new build language, codename Concord. By construction, Concord guarantees reliable builds, no over-build, and allows for efficient distribution. Adopting Concord has led to immense performance improvements, we have seen up to 100X speedup for Windows builds. But the path has been long and rocky, since it not only requires a substantial rewrite of existing build logic, but also all related developer and build lab processes have to change. Whether evolution or revolution is the right path forward - the verdict is still out.
[cloudbuild evolution, non deterministic build tools, program testing, short feedback cycles, software product testing, cache storage, build-as-a-service, parallel machines, build-automation, microsoft build changing, continuous integration scales, Build-system, concord revolution, specification languages, content based caching, minimal human intervention, functions reliably, cloud computing, build-language, performance improvements, software performance evaluation, software product building]
The power of probabilistic thinking
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Traditionally, software engineering has dealt in absolutes. For instance, we talk about a system being &#x201C;correct&#x201D; or &#x201C;incorrect&#x201D;, with the shades of grey in between occasionally acknowledged but rarely dealt with explicitly. And we typically employ logical, algebraic, relational and other representations and techniques that help us reason about software in such absolute terms. There of course have been notable exceptions to this, such as the use of statistical techniques in testing and debugging. But by and large, both researchers and practitioners have favored the relative comfort of an absolutist viewpoint in all aspects of development. In this talk, I will argue the benefits of taking a more thoroughly probabilistic approach in software engineering. Software engineering is rife with stochastic phenomena, and the vast majority of software systems operate in an environment of uncertain, random behavior, which suits an explicit probabilistic characterization. Furthermore, this uncertainty is becoming ever more pronounced in new software systems and platforms, such as the Internet of Things and autonomous vehicles, with their frequent imprecise outputs and heavy reliance on machine learning. To illustrate more deeply some of the considerations involved in taking a probabilistic approach, I will talk about some recent research I have been doing in probabilistic verification.
[program debugging, stochastic phenomena, program testing, program verification, stochastic behavior, testing, uncertainty handling, machine learning, Probabilistic reasoning, probabilistic thinking, probabilistic verification, debugging, software engineering, learning (artificial intelligence)]
An empirical investigation into the nature of test smells
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Test smells have been defined as poorly designed tests and, as reported by recent empirical studies, their presence may negatively affect comprehension and maintenance of test suites. Despite this, there are no available automated tools to support identification and repair of test smells. In this paper, we firstly investigate developers' perception of test smells in a study with 19 participants. The results show that developers generally do not recognize (potentially harmful) test smells, highlighting that automated tools for identifying such smells are much needed. However, to build effective tools, deeper insights into the test smells phenomenon are required. To this aim, we conducted a large-scale empirical investigation aimed at analyzing (i) when test smells occur in source code, (ii) what their survivability is, and (iii) whether their presence is associated with the presence of design problems in production code (code smells). The results indicate that test smells are usually introduced when the corresponding test code is committed in the repository for the first time, and they tend to remain in a system for a long time. Moreover, we found various unexpected relationships between test and code smells. Finally, we show how the results of this study can be used to build effective automated tools for test smell detection and refactoring.
[source code (software), Software Evolution, test suites maintenance, test smells, source code, Maintenance engineering, Data mining, History, software maintenance, production code, Mining Software Repositories, code smells, Test Smells, Production, Software systems, Testing, automated tools]
Evaluating non-adequate test-case reduction
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Given two test cases, one larger and one smaller, the smaller test case is preferred for many purposes. A smaller test case usually runs faster, is easier to understand, and is more convenient for debugging. However, smaller test cases also tend to cover less code and detect fewer faults than larger test cases. Whereas traditional research focused on reducing test suites while preserving code coverage, recent work has introduced the idea of reducing individual test cases, rather than test suites, while still preserving code coverage. Other recent work has proposed non-adequately reducing test suites by not even preserving all the code coverage. This paper empirically evaluates a new combination of these two ideas, non-adequate reduction of test cases, which allows for a wide range of trade-offs between test case size and fault detection. Our study introduces and evaluates C%-coverage reduction (where a test case is reduced to retain at least C% of its original coverage) and N-mutant reduction (where a test case is reduced to kill at least N of the mutants it originally killed). We evaluate the reduction trade-offs with varying values of C% and N for four real-world C projects: Mozilla's SpiderMonkey JavaScript engine, the YAFFS2 flash file system, Grep, and Gzip. The results show that it is possible to greatly reduce the size of many test cases while still preserving much of their fault-detection capability.
[program debugging, software debugging, fault diagnosis, program testing, fault detection, N-mutant reduction, YAFFS2 flash file system, Engines, test adequacy, test suite reduction, Grep, Gzip, Electrical engineering, coverage, code coverage, software testing, Debugging, nonadequate test-case reduction, Size measurement, mutation testing, Computer science, Mozilla SpiderMonkey JavaScript engine, Fault detection, test case size, test reduction, Software, fault tolerant computing]
Optimizing customized program coverage
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Program coverage is used across many stages of software development. While common during testing, program coverage has also found use outside the test lab, in production software. However, production software has stricter requirements on run-time overheads, and may limit possible program instrumentation. Thus, optimizing the placement of probes to gather program coverage is important. We introduce and study the problem of customized program coverage optimization. We generalize previous work that optimizes for complete coverage instrumentation with a system that adapts optimization to customizable program coverage requirements. Specifically, our system allows a user to specify desired coverage locations and to limit legal instrumentation locations. We prove that the problem of determining optimal coverage probes is NP-hard, and we present a solution based on mixed integer linear programming. Due to the computational complexity of the problem, we also provide two practical approximation approaches. We evaluate the effectiveness of our approximations across a diverse set of benchmarks, and show that our techniques can substantially reduce instrumentation while allowing the user immense freedom in defining coverage requirements. When naive instrumentation is dense or expensive, our optimizations succeed in lowering execution time overheads.
[Context, NP-hard, program testing, software development, customized program coverage optimization, mixed integer linear programming, program coverage, integer programming, Debugging, testing, linear programming, mixed integer linear optimization, legal instrumentation locations, Optimization, run-time overheads, Software, software engineering, production software, Probes, Monitoring, computational complexity]
What makes killing a mutant hard
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Mutation operators have been studied at length to determine which ones are the &#x201C;best&#x201D; at some metric (for example creates the least equivalent mutants, creates hard-to-kill mutants, etc.). These studies though have focused on specific test suites, where the test inputs and oracles are fixed, which leads to results that are strongly influenced by the test suites and thus makes the conclusions potentially less general. In this paper we consider all test inputs and we assume we have no prior knowledge about the likelihood of any specific inputs. We will also show how varying the strength of the oracle have a big impact on the results. We only consider a few mutation operators (mostly relational), only a handful of programs to mutate (amenable to probabilistic symbolic execution), and only consider how likely it is that a mutant is killed. A core finding is that the likelihood of reaching the source line where the mutation is applied, is an important contributor to the likelihood of killing the mutant and when we control for this we can see which operators create mutations that are too easy versus very hard to kill.
[Measurement, source code (software), source line, Java, program testing, Probabilistic Symbolic Execution, Radiation detectors, mutation operator, probability, Probabilistic logic, test input, test suite, probabilistic symbolic execution, Mutation Testing, Software, Testing]
Test case permutation to improve execution time
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
With the growing complexity of software, the number of test cases needed for effective validation is extremely large. Executing these large test suites is expensive, both in terms of time and energy. Cache misses are known to be one of the main factors contributing to execution time of a software. Cache misses are reduced by increasing the locality of memory references. For a single program run, compiler optimisations help improve data locality and code layout optimisations help improve spatial locality of instructions. Nevertheless, cache locality optimisations have not been proposed and explored across several program runs, which is the case when we run several test cases. In this paper, we propose and evaluate a novel approach to improve instruction locality across test case runs. Our approach measures the distance between test case runs (number of different instructions). We then permute the test cases for execution so that the distance between neighboring test cases is minimised. We hypothesize that test cases executed in this new order for improved instruction locality will reduce time consumed. We conduct a preliminary evaluation with four subject programs and test suites from the SIR repository to answer the following questions, 1. Is execution time of a test suite affected by the order in which test cases are executed? and 2. How does time consumed in executing our permutation compare to random test case permutations? We found that the order in which test cases are executed has a definite impact on execution time. The extent of impact varies, based on program characteristics and test cases. Our approach outperformed more than 97% of random test case permutations on 3 of the 4 subject programs and did better than 93% of the random orderings on the remaining subject program. Using the optimised permutation, we saw a maximum reduction of 7.4% over average random permutation execution time and 34.7% over the worst permutation.
[Software testing, cache misses, optimised permutation, SIR repository, cache storage, Complexity theory, program compilers, Optimization, memory references, average random permutation execution time, test case permutations, optimisation, instruction locality, Layout, data locality, code layout optimisations, cache locality optimisations, spatial locality, random orderings, Instruction locality, Software, compiler optimisations, Informatics, Cache misses]
Predicting semantically linkable knowledge in developer online forums via convolutional neural network
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Consider a question and its answers in Stack Overflow as a knowledge unit. Knowledge units often contain semantically relevant knowledge, and thus linkable for different purposes, such as duplicate questions, directly linkable for problem solving, indirectly linkable for related information. Recognising different classes of linkable knowledge would support more targeted information needs when users search or explore the knowledge base. Existing methods focus on binary relatedness (i.e., related or not), and are not robust to recognize different classes of semantic relatedness when linkable knowledge units share few words in common (i.e., have lexical gap). In this paper, we formulate the problem of predicting semantically linkable knowledge units as a multiclass classification problem, and solve the problem using deep learning techniques. To overcome the lexical gap issue, we adopt neural language model (word embeddings) and convolutional neural network (CNN) to capture word- and document-level semantics of knowledge units. Instead of using human-engineered classifier features which are hard to design for informal user-generated content, we exploit large amounts of different types of user-created knowledge-unit links to train the CNN to learn the most informative wordlevel and document-level features for the multiclass classification task. Our evaluation shows that our deep-learning based approach significantly and consistently outperforms traditional methods using traditional word representations and human-engineered classifier features.
[Knowledge engineering, CNN, human-engineered classifier features, data mining, Mining software repositories, Uniform resource locators, Deep learning, Semantics, semantically linkable knowledge prediction, Complex networks, developer online forums, document-level semantics, informal user-generated content, learning (artificial intelligence), knowledge unit, Link prediction, pattern classification, word-level semantics, word representation, Social network services, convolutional neural network, multiclass classification problem, neural language model, word embeddings, Multiclass classification, Stack Overflow, Machine learning, Semantic relatedness, social networking (online), deep learning techniques, Software, information needs, binary relatedness, neural nets]
Testing advanced driver assistance systems using multi-objective search and neural networks
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Recent years have seen a proliferation of complex Advanced Driver Assistance Systems (ADAS), in particular, for use in autonomous cars. These systems consist of sensors and cameras as well as image processing and decision support software components. They are meant to help drivers by providing proper warnings or by preventing dangerous situations. In this paper, we focus on the problem of design time testing of ADAS in a simulated environment. We provide a testing approach for ADAS by combining multi-objective search with surrogate models developed based on neural networks. We use multi-objective search to guide testing towards the most critical behaviors of ADAS. Surrogate modeling enables our testing approach to explore a larger part of the input search space within limited computational resources. We characterize the condition under which the multi-objective search algorithm behaves the same with and without surrogate modeling, thus showing the accuracy of our approach. We evaluate our approach by applying it to an industrial ADAS system. Our experiment shows that our approach automatically identifies test cases indicating critical ADAS behaviors. Further, we show that combining our search algorithm with surrogate modeling improves the quality of the generated test cases, especially under tight and realistic computational resources.
[program testing, Surrogate Modeling, neural networks, surrogate models, Predictive models, digital simulation, mobile robots, advanced driver assistance system testing, Advanced driver assistance systems, cameras, driver information systems, decision support software components, simulated environment, search problems, Testing, image processing, Computational modeling, Multi-Objective Search Optimization, automobiles, multiobjective search algorithm, warnings, Neural Networks, Automobiles, sensors, computational resources, Simulation, critical ADAS behaviors, Software, Advanced Driver Assistance Systems, design time testing, neural nets, dangerous situation prevention, autonomous cars]
Privacy preserving via interval covering based subclass division and manifold learning based bi-directional obfuscation for effort estimation
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
When a company lacks local data in hand, engineers can build an effort model for the effort estimation of a new project by utilizing the training data shared by other companies. However, one of the most important obstacles for data sharing is the privacy concerns of software development organizations. In software engineering, most of existing privacy-preserving works mainly focus on the defect prediction, or debugging and testing, yet the privacy-preserving data sharing problem has not been well studied in effort estimation. In this paper, we aim to provide data owners with an effective approach of privatizing their data before release. We firstly design an Interval Covering based Subclass Division (ICSD) strategy. ICSD can divide the target data into several subclasses by digging a new attribute (i.e., class label) from the effort data. And the obtained class label is beneficial to maintaining the distribution of the target data after obfuscation. Then, we propose a manifold learning based bi-directional data obfuscation (MLBDO) algorithm, which uses two nearest neighbors, which are selected respectively from the previous and next subclasses by utilizing the manifold learning based nearest neighbor selector, as the disturbances to obfuscate the target sample. We call the entire approach as ICSD&amp;MLBDO. Experimental results on seven public effort datasets show that: 1) ICSD&amp;MLBDO can guarantee the privacy and maintain the utility of obfuscated data. 2) ICSD&amp;MLBDO can achieve better privacy and utility than the compared privacy-preserving methods.
[Decision support systems, software development organization privacy, Data privacy, privacy-preserving data sharing problem, estimation theory, training data utilization, data privatization, Privacy-preserving, target data distribution, Servers, Radio frequency, target sample obfuscation, interval covering based subclass division, manifold learning based bidirectional obfuscation, software engineering, learning (artificial intelligence), effort estimation, MLBDO algorithm, effort data digging, ICSD&amp;MLBDO, class label, ICSD strategy, Subclass division, nearest neighbor selection, public effort datasets, Effort estimation, data privacy, Locality preserving projection, data handling, manifold learning based nearest neighbor selector]
Deep learning code fragments for code clone detection
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Code clone detection is an important problem for software maintenance and evolution. Many approaches consider either structure or identifiers, but none of the existing detection techniques model both sources of information. These techniques also depend on generic, handcrafted features to represent code fragments. We introduce learning-based detection techniques where everything for representing terms and fragments in source code is mined from the repository. Our code analysis supports a framework, which relies on deep learning, for automatically linking patterns mined at the lexical level with patterns mined at the syntactic level. We evaluated our novel learning-based approach for code clone detection with respect to feasibility from the point of view of software maintainers. We sampled and manually evaluated 398 file- and 480 method-level pairs across eight real-world Java systems; 93% of the file- and method-level samples were evaluated to be true positives. Among the true positives, we found pairs mapping to all four clone types. We compared our approach to a traditional structure-oriented technique and found that our learning-based approach detected clones that were either undetected or suboptimally reported by the prominent tool Deckard. Our results affirm that our learning-based approach is suitable for clone detection and a tenable technique for researchers.
[source code (software), Java, Cloning, Transforms, source code, Java systems, neural networks, Programming, software maintenance, machine learning, software evolution, Deckard tool, abstract syntax trees, deep learning, pattern mined linking, deep learning code fragments, Machine learning, Syntactics, Feature extraction, Software, learning (artificial intelligence), code clone detection, language models]
Automatically recommending code reviewers based on their expertise: An empirical comparison
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Code reviews are an essential part of quality assurance in Free, Libre, and Open Source Software (FLOSS) projects. However, finding a suitable reviewer can be difficult, and delayed or forgotten reviews are the consequence. Automating reviewer selection with suitable algorithms can mitigate this problem. We compare empirically six algorithms based on modification expertise and two algorithms based on review expertise on four major FLOSS projects. Our results indicate that the algorithms based on review expertise yield better recommendations than those based on modification expertise. The algorithm Weighted Review Count (WRC) recommends at least one out of five reviewers correctly in 69 % to 75 % of all cases, which is one of the best results achieved in the comparison.
[Measurement, Algorithm design and analysis, Machine learning algorithms, public domain software, modification expertise, FLOSS projects, WRC, software quality, History, recommendation system, expertise metrics, open source, Prediction algorithms, code reviewer automatic recommendation, libre, and open source software projects, free, project management, Software algorithms, review expertise, weighted review count, patches, code reviews, issue tracker, quality assurance, Software, Code reviewer recommendation, reviewer selection automation]
Evaluating the evaluations of code recommender systems: A reality check
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reflect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reflected in artificial evaluations.
[Context, program testing, code-snippet completion, MIMICs, code search, Proposals, History, software maintenance, Artificial Evaluation, method-call completion, IDE Interaction Data, Benchmark testing, Empirical Study, Software, code recommender system, Recommender systems]
Too much automation? The bellwether effect and its implications for transfer learning
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
&#x201C;Transfer learning&#x201D;: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple &#x201C;bellwether&#x201D; transfer learner. Given N data sets, we find which one produces the best predictions on all the others. This &#x201C;bellwether&#x201D; data set is then used for all subsequent predictions (or, until such time as its predictions start failing-at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) &#x201C;bellwethers&#x201D; are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.
[bellwether effect, transfer learning, Buildings, Transfer learning, data mining, Manuals, Predictive models, Data Mining, Complexity theory, software quality, Learning systems, software quality predictor, Software, learning (artificial intelligence), Defect Prediction, Testing]
Automatic microbenchmark generation to prevent dead code elimination and constant folding
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Microbenchmarking evaluates, in isolation, the execution time of small code segments that play a critical role in large applications. The accuracy of a microbenchmark depends on two critical tasks: wrap the code segment into a pay-load that faithfully recreates the execution conditions of the large application; build a scaffold that runs the payload a large number of times to get a statistical estimate of the execution time. While recent frameworks such as the Java Microbenchmark Harness (JMH) address the scaffold challenge, developers have very limited support to build a correct payload. This work focuses on the automatic generation of pay-loads, starting from a code segment selected in a large application. Our generative technique prevents two of the most common mistakes made in microbenchmarks: dead code elimination and constant folding. A microbenchmark is such a small program that can be &#x201C;over-optimized&#x201D; by the JIT and result in distorted time measures, if not designed carefully. Our technique automatically extracts the segment into a compilable payload and generates additional code to prevent the risks of &#x201C;over-optimization&#x201D;. The whole approach is embedded in a tool called AutoJMH, which generates payloads for JMH scaffolds. We validate the capabilities AutoJMH, showing that the tool is able to process a large percentage of segments in real programs. We also show that AutoJMH can match the quality of payloads handwritten by performance experts and outperform those written by professional Java developers without experience in microbenchmarking.
[Java, Performace evaluation, text tagging, AutoJMH tool, JMH, over-optimization risk prevention, Time measurement, Optimization, microbenchmark generation, dead code elimination prevention, microbencharking, code segment, Benchmark testing, Software, Distortion measurement, Java microbenchmark harness, software performance evaluation, Payloads]
Visualization of combinatorial models and test plans
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Combinatorial test design (CTD) is an effective and widely used test design technique. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. One challenge for successful application of CTD in practice relates to this manual model definition and maintenance process. Another challenge relates to the comprehension and use of the test plan generated by CTD for prioritization purposes. In this work we introduce the use of visualizations as a means to address these challenges. We apply three different forms of visualization, matrices, graphs, and treemaps, to visualize the relationships between the different elements of the model, and to visualize the strength of each test in the test plan and the relationships between the different tests in terms of combinatorial coverage. We evaluate our visualizations via a user survey with 19 CTD practitioners, as well as via two industrial projects in which our visualization was used and allowed test designers to get vital insight into their models and into the coverage provided through CTD generated test plans.
[Visualization, program testing, CTD generated test plans, Atmospheric modeling, Computational modeling, Software Visualization, Manuals, combinatorial test design, software visualization, graphs, Image color analysis, test plans visualization, Data visualization, combinatorial coverage, program visualisation, combinatorial models visualization, matrices, Combinatorial Testing, Testing, automatic test plan generation, treemaps]
Finding access control bugs in web applications with CanCheck
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Access control bugs in web applications can have dire consequences since many web applications store private and sensitive data. In this paper we present an automated verification technique for access control in Ruby on Rails (Rails) applications. Our technique starts by automatically extracting a model that captures 1) the ways the data is accessed and modified by the application, 2) the access control policy of the application, and 3) the authorization checks used for access control policy enforcement. Then, it automatically translates this model to first order logic and uses automated theorem provers to check whether the declared access control policy is correctly enforced by the implementation. We implemented our technique in a tool called CanCheck. Using CanCheck on open source Rails applications, we found numerous previously unknown exploitable access control bugs as well as several deficiencies in access control policies.
[program debugging, public domain software, Web applications, private data, Authorization, formal logic, Databases, formal verification, access control bugs, authorisation, Logic-based Verification, theorem proving, Access Control, Rails, Ruby on Rails, Computer bugs, sensitive data, first order logic, open source Rails applications, CanCheck, Data models, Software, data privacy, Internet, automated theorem provers, Web Applications, automated verification]
SOFIA: An automated security oracle for black-box testing of SQL-injection vulnerabilities
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Security testing is a pivotal activity in engineering secure software. It consists of two phases: generating attack inputs to test the system, and assessing whether test executions expose any vulnerabilities. The latter phase is known as the security oracle problem. In this work, we present SOFIA, a Security Oracle for SQL-Injection Vulnerabilities. SOFIA is programming-language and source-code independent, and can be used with various attack generation tools. Moreover, because it does not rely on known attacks for learning, SOFIA is meant to also detect types of SQLi attacks that might be unknown at learning time. The oracle challenge is recast as a one-class classification problem where we learn to characterise legitimate SQL statements to accurately distinguish them from SQLi attack statements. We have carried out an experimental validation on six applications, among which two are large and widely-used. SOFIA was used to detect real SQLi vulnerabilities with inputs generated by three attack generation tools. The obtained results show that SOFIA is computationally fast and achieves a recall rate of 100% (i.e., missing no attacks) with a low false positive rate (0.6%).
[program testing, Security, Servers, automated security oracle, attack generation tools, SQL-injection, SQLi attack statements, Databases, programming-language, security testing, legitimate SQL statements, Testing, Context, SQL-injection vulnerabilities, pattern classification, black-box testing, SQL, test executions, Security oracle, security of data, SOFIA, one-class classification problem, Security testing, Software, Payloads]
Supporting oracle construction via static analysis
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
In software testing, the program under test is usually executed with test inputs and checked against a test oracle, which is a mechanism to verify whether the program behaves as expected. Selecting the right oracle data to observe is crucial in test oracle construction. In the literature, researchers have proposed two dynamic approaches to oracle data selection by analyzing test execution information (e.g., variables' values or interaction information). However, collecting such information during program execution may incur extra cost. In this paper, we present the first static approach to oracle data selection, SODS (Static Oracle Data Selection). In particular, SODS first identifies the substitution relationships between candidate oracle data by constructing a probabilistic substitution graph based on the definition-use chains of the program under test, then estimates the fault-observing capability of each candidate oracle data, and finally selects a subset of oracle data with strong fault-observing capability. For programs with analyzable test code, we further extend SODS via pruning the probabilistic substitution graph based on 0-1-CFA call graph analysis. The experimental study on 11 subject systems written in C or Java demonstrates that our static approach is more effective and much more efficient than state-of-the-art dynamic approaches in most cases.
[Software testing, source code (software), program testing, Test oracle, graph theory, dynamic approach, test oracle, test execution information analysis, C language, SODS, fault-observing capability estimation, variable values, 0-1-CFA call graph analysis, Fault diagnosis, program under test, probabilistic substitution graph, substitution relationships, Java, program diagnostics, oracle construction, software testing, probability, definition-use chains, static analysis, program execution, Probabilistic logic, test code analysis, oracle data selection, Java language, interaction information, Software, static oracle data selection]
Local-based active classification of test report to assist crowdsourced testing
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
In crowdsourced testing, an important task is to identify the test reports that actually reveal fault - true fault, from the large number of test reports submitted by crowd workers. Most existing approaches towards this problem utilized supervised machine learning techniques, which often require users to manually label a large amount of training data. Such process is time-consuming and labor-intensive. Thus, reducing the onerous burden of manual labeling while still being able to achieve good performance is crucial. Active learning is one potential technique to address this challenge, which aims at training a good classifier with as few labeled data as possible. Nevertheless, our observation on real industrial data reveals that existing active learning approaches generate poor and unstable performances on crowdsourced testing data. We analyze the deep reason and find that the dataset has significant local biases. To address the above problems, we propose LOcal-based Active ClassiFication (LOAF) to classify true fault from crowdsourced test reports. LOAF recommends a small portion of instances which are most informative within local neighborhood, and asks user their labels, then learns classifiers based on local neighborhood. Our evaluation on 14,609 test reports of 34 commercial projects from one of the Chinese largest crowdsourced testing platforms shows that our proposed LOAF can generate promising results. In addition, its performance is even better than existing supervised learning approaches which built on large amounts of labelled historical data. Moreover, we also implement our approach and evaluate its usefulness using real-world case studies. The feedbacks from testers demonstrate its practical value.
[program debugging, program testing, Test Report Classification, Manuals, LOAF, Crowdsourced Testing, crowdsourced testing, Active Learning, local-based active classification, supervised machine learning technique, active learning, Training data, test report, Speech recognition, Feature extraction, Software, Labeling, learning (artificial intelligence), Testing]
Multi-objective test report prioritization using image understanding
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
In crowdsourced software testing, inspecting the large number of test reports is an overwhelming but inevitable software maintenance task. In recent years, to alleviate this task, many text-based test-report classification and prioritization techniques have been proposed. However in the mobile testing domain, test reports often consist of more screenshots and shorter descriptive text, and thus text-based techniques may be ineffective or inapplicable. The shortage and ambiguity of natural-language text information and the well defined screenshots of activity views within mobile applications motivate our novel technique based on using image understanding for multi-objective test-report prioritization. In this paper, by taking the similarity of screenshots into consideration, we present a multi-objective optimization-based prioritization technique to assist inspections of crowdsourced test reports. In our technique, we employ the Spatial Pyramid Matching (SPM) technique to measure the similarity of the screenshots, and apply the natural-language processing technique to measure the distance between the text of test reports. Furthermore, to validate our technique, an experiment with more than 600 test reports and 2500 images is conducted. The experimental results show that image-understanding techniques can provide benefit to test-report prioritization for most applications.
[spatial pyramid matching, text analysis, program testing, image classification, Mobile communication, image understanding, screenshots, Multi-Objective Optimization, SPM, mobile computing, Image color analysis, mobile applications, Testing, natural-language text information, natural language processing, text-based test-report classification, Inspection, Mobile applications, mobile testing domain, software maintenance task, Crowdsourced Testing, image matching, descriptive text, crowdsourced software testing, Computer bugs, Image Understanding, Software, Test Report Prioritization, multiobjective test report prioritization]
CrowdService: Serving the individuals through mobile crowdsourcing and service composition
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Some user needs in real life can only be accomplished by leveraging the intelligence and labor of other people via crowdsourcing tasks. For example, one may want to confirm the validity of the description of a secondhand laptop by asking someone else to inspect the laptop on site. To integrate these crowdsourcing tasks into user applications, it is required that crowd intelligence and labor be provided as easily accessible services (e.g., Web services), which can be called crowd services. In this paper, we develop a framework named CrowdService which supplies crowd intelligence and labor as publicly accessible crowd services via mobile crowd-sourcing. We implement the proposed framework on the Android platform and evaluate its usability with a user study.
[Crowdsourcing, mobile crowdsourcing, Portable computers, publicly accessible crowd services, CrowdService, reliability, Inspection, Mobile communication, Mobile handsets, labor, Android (operating system), mobile computing, crowd intelligence, Web services, outsourcing, crowdsourcing tasks, Android platform, service composition, Business]
Taming Android fragmentation: Characterizing and detecting compatibility issues for Android apps
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Android ecosystem is heavily fragmented. The numerous combinations of different device models and operating system versions make it impossible for Android app developers to exhaustively test their apps. As a result, various compatibility issues arise, causing poor user experience. However, little is known on the characteristics of such fragmentation-induced compatibility issues and no mature tools exist to help developers quickly diagnose and fix these issues. To bridge the gap, we conducted an empirical study on 191 real-world compatibility issues collected from popular open-source Android apps. Our study characterized the symptoms and root causes of compatibility issues, and disclosed that the patches of these issues exhibit common patterns. With these findings, we propose a technique named FicFinder to automatically detect compatibility issues in Android apps. FicFinder performs static code analysis based on a model that captures Android APIs as well as their associated context by which compatibility issues are triggered. FicFinder reports actionable debugging information to developers when it detects potential issues. We evaluated FicFinder with 27 large-scale open-source Android apps. The results show that FicFinder can precisely detect compatibility issues in these apps and uncover previously-unknown issues.
[program debugging, application program interfaces, program testing, public domain software, Ecosystems, Humanoid robots, app testing, operating system version, Open source software, actionable debugging information, Android (operating system), Android ecosystem, Hardware, Android API, device model, open-source Android apps, fragmentation-induced compatibility issue, compatibility issue detection, program diagnostics, Android fragmentation, FicFinder, static code analysis, Computer bugs, compatibility issues, Androids]
Automated model-based Android GUI testing using multi-level GUI comparison criteria
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Automated Graphical User Interface (GUI) testing is one of the most widely used techniques to detect faults in mobile applications (apps) and to test functionality and usability. GUI testing exercises behaviors of an application under test (AUT) by executing events on GUIs and checking whether the app behaves correctly. In particular, because Android leads in market share of mobile OS platforms, a lot of research on automated Android GUI testing techniques has been performed. Among various techniques, we focus on model-based Android GUI testing that utilizes a GUI model for systematic test generation and effective debugging support. Since test inputs are generated based on the underlying model, accurate GUI modeling of an AUT is the most crucial factor in order to generate effective test inputs. However, most modern Android apps contain a number of dynamically constructed GUIs that make accurate behavior modeling more challenging. To address this problem, we propose a set of multi-level GUI Comparison Criteria (GUICC) that provides the selection of multiple abstraction levels for GUI model generation. By using multilevel GUICC, we conducted empirical experiments to identify the influence of GUICC on testing effectiveness. Results show that our approach, which performs model-based testing with multi-level GUICC, achieved higher effectiveness than activity-based GUI model generation. We also found that multi-level GUICC can alleviate the inherent state explosion problems of existing a single-level GUICC for behavior modeling of real-world Android apps by flexibly manipulating GUICC.
[program debugging, fault diagnosis, program testing, graphical user interfaces, Android application testing, Humanoid robots, fault detection, Mobile communication, automated model-based Android GUI testing, GUI model generation, mobile OS platforms, GUI comparison criteria, usability testing, Analytical models, Android (operating system), mobile computing, Android apps behavior modeling, single-level GUICC, multilevel GUICC, mobile applications, Space exploration, functionality testing, Graphical user interfaces, Testing, application under test, systematic test generation, multilevel GUI comparison criteria, AUT GUI modeling, debugging support, automated graphical user interface testing, GUI testing, Androids, Model-based test input generation]
HybriDroid: Static analysis framework for Android hybrid applications
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Mobile applications (apps) have long invaded the realm of desktop apps, and hybrid apps become a promising solution for supporting multiple mobile platforms. Providing both platform-specific functionalities via native code like native apps and user interactions via JavaScript code like web apps, hybrid apps help developers build multiple apps for different platforms without much duplicated efforts. However, most hybrid apps are developed in multiple programming languages with different semantics, which may be vulnerable to programmer errors. Moreover, because untrusted JavaScript code may access device-specific features via native code, hybrid apps may be vulnerable to various security attacks. Unfortunately, no existing tools can help hybrid app developers by detecting errors or security holes. In this paper, we present HybriDroid, a static analysis framework for Android hybrid apps. We investigate the semantics of Android hybrid apps especially for the interoperation mechanism of Android Java and JavaScript. Then, we design and implement a static analysis framework that analyzes inter-communication between Android Java and JavaScript. As example analyses supported by HybriDroid, we implement a bug detector that identifies programmer errors due to the hybrid semantics, and a taint analyzer that finds information leaks cross language boundaries. Our empirical evaluation shows that the tools are practically usable in that they found previously uncovered bugs in real-world Android hybrid apps and possible information leaks via a widely-used advertising platform.
[program debugging, hybrid applications, Humanoid robots, interoperation mechanism, Mobile communication, programmer error identification, HybriDroid, Android (operating system), mobile computing, security attack, Semantics, Android Java, JavaScript, Java, program diagnostics, static analysis framework, analysis framework, mobile application, static analysis, smart phones, Android hybrid application, bug detector, taint analyzer, Android, multi-language analysis, Bridges, security of data, Web pages, Androids]
Locus: Locating bugs from software changes
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20.1% and 20.5%, respectively. Locus is also capable of locating the inducing changes within top 5 for 41.0% of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques.
[program debugging, source file level localization, Natural languages, bug-fixing, Debugging, Manuals, information retrieval, Information retrieval, IR-based bug localization, History, MRR, software changes, bug inducing changes, bug localization, Computer bugs, Software, Locus, MAP, software analytics]
Fine-tuning spectrum based fault localisation with frequent method item sets
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Continuous integration is a best practice adopted in modern software development teams to identify potential faults immediately upon project build. Once a fault is detected it must be repaired immediately, hence continuous integration provides an ideal testbed for experimenting with the state of the art in fault localisation. In this paper we propose a variant of what is known as spectrum based fault localisation, which leverages patterns of method calls by means of frequent itemset mining. We compare our variant (we refer to it as patterned spectrum analysis) against the state of the art and demonstrate on 351 real faults drawn from five representative open source java projects that patterned spectrum analysis is more effective in localising the fault.
[program debugging, continuous integration, Automated developer tests, data mining, fault detection, ideal testbed, Servers, Itemsets, item sets, software engineering, pattern spectrum analysis, Context, Statistical debugging, fine-tuning spectrum, software development teams, spectrum based fault localisation, Debugging, fault location, Maintenance engineering, Continuous integration, frequent itemset mining, Spectral analysis, statistical debugging, project build, Software, Spectrum based fault localisation]
Recommending relevant classes for bug reports using multi-objective search
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Developers may follow a tedious process to find the cause of a bug based on code reviews and reproducing the abnormal behavior. In this paper, we propose an automated approach to finding and ranking potential classes with the respect to the probability of containing a bug based on a bug report description. Our approach finds a good balance between minimizing the number of recommended classes and maximizing the relevance of the proposed solution using a multi-objective optimization algorithm. The relevance of the recommended classes (solution) is estimated based on the use of the history of changes and bug-fixing, and the lexical similarity between the bug report description and the API documentation. We evaluated our system on 6 open source Java projects, using the version of the project before fixing the bug of many bug reports. The experimental results show that the search-based approach significantly outperforms three state-of-the-art methods in recommending relevant files for bug reports. In particular, our multi-objective approach is able to successfully locate the true buggy methods within the top 10 recommendations for over 87% of the bug reports.
[program debugging, application program interfaces, estimation theory, bug report description, public domain software, Search problems, multiobjective optimization algorithm, API documentation, History, Optimization, Search-based software engineering, class recommendation, optimisation, bug reports, search problems, Java, probability, Documentation, relevance estimation, software maintenance, multiobjective search, Computer bugs, Software, multi-objective optimization, open source Java project, Software engineering]
An empirical study on dependence clusters for effort-aware fault-proneness prediction
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
A dependence cluster is a set of mutually inter-dependent program elements. Prior studies have found that large dependence clusters are prevalent in software systems. It has been suggested that dependence clusters have potentially harmful effects on software quality. However, little empirical evidence has been provided to support this claim. The study presented in this paper investigates the relationship between dependence clusters and software quality at the function-level with a focus on effort-aware fault-proneness prediction. The investigation first analyzes whether or not larger dependence clusters tend to be more fault-prone. Second, it investigates whether the proportion of faulty functions inside dependence clusters is significantly different from the proportion of faulty functions outside dependence clusters. Third, it examines whether or not functions inside dependence clusters playing a more important role than others are more fault-prone. Finally, based on two groups of functions (i.e., functions inside and outside dependence clusters), the investigation considers a segmented fault-proneness prediction model. Our experimental results, based on five well-known open-source systems, show that (1) larger dependence clusters tend to be more fault-prone; (2) the proportion of faulty functions inside dependence clusters is significantly larger than the proportion of faulty functions outside dependence clusters; (3) functions inside dependence clusters that play more important roles are more fault-prone; (4) our segmented prediction model can significantly improve the effectiveness of effort-aware fault-proneness prediction in both ranking and classification scenarios. These findings help us better understand how dependence clusters influence software quality.
[pattern classification, open-source system, public domain software, fault-proneness, Predictive models, software quality, Servers, classification, mutually interdependent program element, software fault tolerance, Computer science, dependence cluster, pattern clustering, Software quality, Syntactics, Data models, ranking, network analysis, Dependence clusters, fault prediction, effort-aware fault-proneness prediction]
StraightTaint: Decoupled offline symbolic taint analysis
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Taint analysis has been widely applied in ex post facto security applications, such as attack provenance investigation, computer forensic analysis, and reverse engineering. Unfortunately, the high runtime overhead imposed by dynamic taint analysis makes it impractical in many scenarios. The key obstacle is the strict coupling of program execution and taint tracking logic code. To alleviate this performance bottleneck, recent work seeks to offload taint analysis from program execution and run it on a spare core or a different CPU. However, since the taint analysis has heavy data and control dependencies on the program execution, the massive data in recording and transformation overshadow the benefit of decoupling. In this paper, we propose a novel technique to allow very lightweight logging, resulting in much lower execution slowdown, while still permitting us to perform full-featured offline taint analysis. We develop StraightTaint, a hybrid taint analysis tool that completely decouples the program execution and taint analysis. StraightTaint relies on very lightweight logging of the execution information to reconstruct a straight-line code, enabling an offline symbolic taint analysis without frequent data communication with the application. While StraightTaint does not log complete runtime or input values, it is able to precisely identify the causal relationships between sources and sinks, for example. Compared with traditional dynamic taint analysis tools, StraightTaint has much lower application runtime overhead.
[lightweight logging, program diagnostics, Reverse engineering, taint tracking logic code, hybrid taint analysis tool, Taint analysis, reverse engineering, Offline, Registers, Security, attack provenance investigation, decoupled offline symbolic taint analysis, computer forensic analysis, program execution strict coupling, Symbolic taint analysis, Runtime, security of data, Decoupling, ex post facto security applications, Malware, Performance analysis, straight-line code reconstruction, full-featured offline taint analysis, StraightTaint]
IncA: A DSL for the definition of incremental program analyses
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Program analyses support software developers, for example, through error detection, code-quality assurance, and by enabling compiler optimizations and refactorings. To provide real-time feedback to developers within IDEs, an analysis must run efficiently even if the analyzed code base is large. To achieve this goal, we present a domain-specific language called IncA for the definition of efficient incremental program analyses that update their result as the program changes. IncA compiles analyses into graph patterns and relies on existing incremental matching algorithms. To scale IncA analyses to large programs, we describe optimizations that reduce caching and prune change propagation. Using IncA, we have developed incremental control flow and points-to analysis for C, well-formedness checks for DSLs, and 10 FindBugs checks for Java. Our evaluation demonstrates significant speedups for all analyses compared to their non-incremental counterparts.
[Java, software development, program diagnostics, static program analysis, Domain-specific Language, Language Workbench, Optimization, incremental program analysis, Runtime, Program processors, domain-specific language, specification languages, software engineering, DSL, Static Analysis, Pattern matching, IncA analysis, Incremental Computation]
What developers want and need from program analysis: An empirical study
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Program Analysis has been a rich and fruitful field of research for many decades, and countless high quality program analysis tools have been produced by academia. Though there are some well-known examples of tools that have found their way into routine use by practitioners, a common challenge faced by researchers is knowing how to achieve broad and lasting adoption of their tools. In an effort to understand what makes a program analyzer most attractive to developers, we mounted a multi-method investigation at Microsoft. Through interviews and surveys of developers as well as analysis of defect data, we provide insight and answers to four high level research questions that can help researchers design program analyzers meeting the needs of software developers. First, we explore what barriers hinder the adoption of program analyzers, like poorly expressed warning messages. Second, we shed light on what functionality developers want from analyzers, including the types of code issues that developers care about. Next, we answer what non-functional characteristics an analyzer should have to be widely used, how the analyzer should fit into the development process, and how its results should be reported. Finally, we investigate defects in one of Microsoft's flagship software services, to understand what types of code issues are most important to minimize, potentially through program analysis.
[high quality program analysis tools, program diagnostics, Ecosystems, Companies, code defects, software developers, Security, multimethod investigation, Pain, software defect analysis, code issues, program analysis, Microsoft software services, Software, software engineering, functionality developers, Interviews, Software engineering]
DistIA: A cost-effective dynamic impact analysis for distributed programs
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Dynamic impact analysis is a fundamental technique for understanding the impact of specific program entities, or changes to them, on the rest of the program for concrete executions. However, existing techniques are either inapplicable or of very limited utility for distributed programs running in multiple concurrent processes. This paper presents DISTIA, a dynamic analysis of distributed systems that predicts impacts propagated both within and across process boundaries by partially ordering distributed method-execution events, inferring causality from the ordered events, and exploiting message-passing semantics. We applied DISTIA to large distributed systems of various architectures and sizes, for which it on average finishes the entire analysis within one minute and safely reduces impact-set sizes by over 43% relative to existing options with run-time overhead less than 8%. Moreover, two case studies initially demonstrated the precision of DISTIA and its utility in distributed system understanding. While conservative thus subject to false positives, DistIA balances precision and efficiency to offer cost-effective options for evolving distributed programs.
[Algorithm design and analysis, dynamic impact analysis, Heuristic algorithms, distributed processing, causality inference, DISTIA, Servers, distributed method-execution event, concurrent process, Impact analysis, software architecture, distributed system architecture, Message passing, dynamic partial ordering, concurrency control, system monitoring, distributed systems, Software, Performance analysis, distributed program, Clocks]
Radius aware probabilistic testing of deadlocks with guarantees
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Concurrency bugs only occur under certain interleaving. Existing randomized techniques are usually ineffective. PCT innovatively generates scheduling, before executing a program, based on priorities and priority change points. Hence, it provides a probabilistic guarantee to trigger concurrency bugs. PCT randomly selects priority change points among all events, which might be effective for non-deadlock concurrency bugs. However, deadlocks usually involve two or more threads and locks, and require more ordering constraints to be triggered. We interestingly observe that, every two events of a deadlock usually occur within a short range. We generally formulate this range as the bug Radius, to denote the max distance of every two events of a concurrency bug. Based on the bug radius, we propose RPro (Radius aware Probabilistic testing) for triggering deadlocks. Unlike PCT, RPro selects priority change points within the radius of the targeted deadlocks but not among all events. Hence, it guarantees larger probabilities to trigger deadlocks. We have implemented RPro and PCT and evaluated them on a set of real-world benchmarks containing 10 unique deadlocks. The experimental results show that RPro triggered all deadlocks with higher probabilities (i.e., &gt;7.7x times larger on average) than that by PCT. We also evaluated RPro with radius varying from 1 to 150 (or 300). The result shows that the radius of a deadlock is much smaller (i.e., from 2 to 114 in our experiment) than the number of all events. This further confirms our observation and makes RPro meaningful in practice.
[program debugging, program testing, concurrency bug, Instruction sets, probability, Probabilistic logic, radius aware probabilistic testing, deadlock triggering, bug radius, system recovery, Concurrent computing, Deadlock, probabilistic guarantee, Computer bugs, concurrency control, System recovery, Benchmark testing, multithreaded program, RPro, random testing]
LockPeeker: Detecting latent locks in Java APIs
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Detecting lock-related defects has long been a hot research topic in software engineering. Many efforts have been spent on detecting such deadlocks in concurrent software systems. However, latent locks may be hidden in application programming interface (API) methods whose source code may not be accessible to developers. Many APIs have latent locks. For example, our study has shown that J2SE alone can have 2,000+ latent locks. As latent locks are less known by developers, they can cause deadlocks that are hard to perceive or diagnose. Meanwhile, the state-of-the-art tools mostly handle API methods as black boxes, and cannot detect deadlocks that involve such latent locks. In this paper, we propose a novel black-box testing approach, called LockPeeker, that reveals latent locks in Java APIs. The essential idea of LockPeeker is that latent locks of a given API method can be revealed by testing the method and summarizing the locking effects during testing execution. We have evaluated LockPeeker on ten real-world Java projects. Our evaluation results show that (1) LockPeeker detects 74.9% of latent locks in API methods, and (2) it enables state-of-the-art tools to detect deadlocks that otherwise cannot be detected.
[source code (software), application program interfaces, program testing, application programming interface, Java API, API method, deadlock detection, Analytical models, latent locks detection, lock-related defects detection, J2SE, locking effects, Latent lock, software engineering, testing execution, Cryptography, deadlocks, Testing, Java, black-box testing, source code, concurrent software systems, software fault tolerance, Computer bugs, concurrency control, System recovery, LockPeeker, Software]
Sound static deadlock analysis for C/Pthreads
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
We present a static deadlock analysis for C/Pthreads. The design of our method has been guided by the requirement to analyse real-world code. Our approach is sound (i.e., misses no deadlocks) for programs that have defined behaviour according to the C standard and the Pthreads specification, and is precise enough to prove deadlock-freedom for a large number of such programs. The method consists of a pipeline of several analyses that build on a new context- and thread-sensitive abstract interpretation framework. We further present a lightweight dependency analysis to identify statements relevant to deadlock analysis and thus speed up the overall analysis. In our experimental evaluation, we succeeded to prove deadlock-freedom for 292 programs from the Debian GNU/Linux distribution with in total 2.3 MLOC in 4 hours.
[C/Pthreads, Instruction sets, Scalability, Pipelines, context-sensitive abstract interpretation framework, real-world code analysis, sound static deadlock analysis, C language, formal specification, Concurrent computing, Debian GNU/Linux distribution, deadlock analysis, lightweight dependency analysis, program diagnostics, abstract interpretation, static analysis, Standards, Pthreads specification, deadlock-freedom, time 4 hour, thread-sensitive abstract interpretation framework, Linux, Computer bugs, System recovery, C standard]
Static race detection for device drivers: The Goblint approach
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Device drivers rely on fine-grained locking to ensure safe access to shared data structures. For human testers, concurrency makes such code notoriously hard to debug; for automated reasoning, dynamically allocated memory and low-level pointer manipulation poses significant challenges. We present a flexible approach to data race analysis, implemented in the open source Goblint static analysis framework that combines different pointer and value analyses in order to handle a wide range of locking idioms, including locks allocated dynamically as well as locks stored in arrays. To the best of our knowledge, this is the most ambitious effort, having lasted well over ten years, to create a fully automated static race detection tool that can deal with most of the intricate locking schemes found in Linux device drivers. Our evaluation shows that these analyses are sufficiently precise, but practical use of these techniques requires inferring environmental and domain-specific assumptions.
[Decision support systems, program debugging, shared data structures, human testers, public domain software, device drivers, static race detection, Servers, safe access, low-level pointer manipulation, automated reasoning, Concurrency, authorisation, data structures, concurrency (computers), Linux device drivers, race condition, debug, program diagnostics, abstract interpretation, inference mechanisms, fine-grained locking, concurrency, open source Goblint static analysis, Linux]
An empirical evaluation of two user interfaces of an interactive program verifier
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Theorem provers have highly complex interfaces, but there are not many systematic studies of their usability and effectiveness. Specifically, for interactive theorem provers the ability to quickly comprehend intermediate proof situations is of pivotal importance. In this paper we present the (as far as we know) first empirical study that systematically compares the effectiveness of different user interfaces of an interactive theorem prover. We juxtapose two different user interfaces of the interactive verifier KeY: the traditional one which focuses on proof objects and a more recent one that provides a view akin to an interactive debugger. We carefully designed a controlled experiment where users were given various proof understanding tasks that had to be solved with alternating interfaces. We provide statistical evidence that the conjectured higher effectivity of the debugger-like interface is not just a hunch.
[Java, program debugging, program verification, Inspection, Verification, user interfaces, Empirical Evaluation, Standards, user interface, proof object, empirical evaluation, theorem prover, Proof Understanding, User interfaces, interactive systems, interactive debugger, theorem proving, Usability, interactive program verifier]
Traceability maintenance: Factors and guidelines
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Traceability is an important concern for numerous software engineering activities. Establishing traceability links is a challenging and cost-intensive task, which is uneconomical without suitable strategies for maintaining high link quality. Current approaches to Traceability Management (TM), however, often make important assumptions and choices without ensuring that the consequences and implications for trace-ability maintenance are feasible and desirable in practice. In this paper, therefore, we identify a set of core factors that influence how the quality of traceability links can be maintained. For each factor, we discuss relevant challenges and provide guidelines on how best to ensure viable traceability maintenance in a practical TM approach. Our guidelines are meant to be used by tool developers and users to select the most appropriate TM approach for their needs. Our results are based on and supported by data collected from interviews conducted with: (i) 9 of our industrial and academic project partners to elicit requirements for a TM tool, and (ii) 24 software development stakeholders from 15 industrial cases to provide a broader overview of the current state of the practice on TM. To evaluate the feasibility of our guidelines, we investigate a set of existing TM approaches used in industry with respect to our guidelines.
[Computational modeling, Unified modeling language, Manuals, Maintenance engineering, software management, traceability management, software quality, software maintenance, consistency, Guidelines, traceability quality, TM, Software, software engineering, traceability maintenance, Interviews]
Usage, costs, and benefits of continuous integration in open-source projects
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and benefits associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 open-source projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and benefits of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as finding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI.
[CI benefits, continuous integration, Automation, program testing, public domain software, software testing, Buildings, GitHub, mining software repositories, software building, costing, continuous integration systems, History, open-source projects, automated software engineering, program compilers, Open source software, CI usage, software compilation, CI costs, Tunneling, CI systems, software engineering, Testing]
DSL-maps: From requirements to design of domain-specific languages
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Domain-Specific Languages (DSLs) are central to Model-Driven Engineering, where they are used for creating models for particular domains. However, current research and tools for building DSLs focus on the design and implementation aspects of the DSL, while the requirements analysis phase, and its automated transition to design is largely neglected. In order to alleviate this situation, we propose DSL-maps, a notation inspired by mind-maps, to represent requirements for DSLs. The notation is supported by a tool, which helps in the automated transition into an initial meta-model design, using a customizable transformation and recommendations from a catalogue of meta-model design patterns.
[domain-specific languages, Computational modeling, Meta-Modelling Patterns, Unified modeling language, high level languages, requirements analysis phase, mind-maps, Connectors, Model-Driven Engineering, DSL-maps, Domain Specific Languages, systems analysis, model-driven engineering, Syntactics, Domain Analysis, Software, Concrete, DSL, meta-model design patterns, pattern recognition]
The IDE as a scriptable information system
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Software engineering is extremely information-intensive. Every day developers work with source code, version repositories, issue trackers, documentation, web-based and other information resources. However, three key aspects of information work lack good support: (i) combining information from different sources; (ii) flexibly presenting collected information to enable easier comprehension; and (iii) automatically acting on collected information, for example to perform a refactoring. Poor support for these activities makes many common development tasks time-consuming and error-prone. We propose an approach that directly addresses these three issues by integrating a flexible query mechanism into the development environment. Our approach enables diverse ways to process and visualize information and can be extended via scripts. We demonstrate how an implementation of the approach can be used to rapidly write queries that meet a wide range of information needs.
[source code (software), information processing, Visualization, scriptable information system, version repositories, issue trackers, IDE, information visualization, software visualization, Engines, code queries, query processing, query mechanism, refactoring, data visualisation, software engineering, information resources, source code, documentation, Web-based system, software maintenance, development environment, Heating, Computer bugs, Data visualization, Information services, Software, information needs, programming environments]
Inferring annotations for device drivers from verification histories
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
This paper studies and optimizes automated program verification. Detailed reasoning about software behavior is often facilitated by program invariants that hold across all program executions. Finding program invariants is in fact an essential step in automated program verification. Automatic discovery of precise invariants, however, can be very difficult in practice. The problem can be simplified if one has access to a candidate set of assertions (or annotations) and the search for invariants is limited over the space defined by these annotations. Then, the main challenge is to automatically generate quality program annotations. We present an approach that infers program annotations automatically by leveraging the history of verifying related programs. Our algorithm extracts high-quality annotations from previous verification attempts, and then applies them for verifying new programs. We present a case study where we applied our algorithm to Microsoft's Static Driver Verifier (SDV). SDV is an industrial-strength tool for verification of Windows device drivers that uses manually-tuned heuristics for obtaining a set of annotations. Our technique inferred program annotations comparable in performance to the existing annotations used in SDV that were devised manually by human experts over years. Additionally, the inferred annotations together with the existing ones improved the performance of SDV overall, proving correct 47% of drivers more while running 22% faster in our experiments.
[Performance evaluation, program verification, program diagnostics, SDV, Manuals, device drivers, reasoning, Learning invariants, software quality, History, inference mechanisms, Program verification, program invariant, Big Code, Simultaneous localization and mapping, Microsoft Static Driver Verifier, software behavior, program annotation inference, high-quality annotation, Invariant generation, Windows device driver, Kernel, Verification history, Contracts]
Array length inference for C library bindings
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Simultaneous use of multiple programming languages (polyglot programming) assists in creating efficient, coherent, modern programs in the face of legacy code. However, manually creating bindings to low-level languages like C is tedious and error-prone. We offer relief in the form of an automated suite of analyses, designed to enhance the quality of automatically produced bindings. These analyses recover high-level array length information that is missing from C's type system. We emit annotations in the style of GObject-Introspection, which produces bindings from annotations on function signatures. We annotate each array argument as terminated by a special sentinel value, fixed-length, or of length determined by another argument. These properties help produce more idiomatic, efficient bindings. We correctly annotate at least 70% of all arrays with these length types, and our results are comparable to those produced by human annotators, but take far less time to produce.
[bindings, Programming, FFI, libraries, static analysis, Data mining, High level languages, Memory management, Production, foreign function interfaces, Libraries, Safety, type inference]
APEx: Automated inference of error specifications for C APIs
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Although correct error handling is crucial to software robustness and security, developers often inadvertently introduce bugs in error handling code. Moreover, such bugs are hard to detect using existing bug-finding tools without correct error specifications. Creating error specifications manually is tedious and error-prone. In this paper, we present a new technique that automatically infers error specifications of API functions based on their usage patterns in C programs. Our key insight is that error-handling code tend to have fewer branching points and program statements than the code implementing regular functionality. Our scheme leverages this property to automatically identify error handling code at API call sites and infer the corresponding error constraints. We then use the error constraints from multiple call sites for robust inference of API error specifications. We evaluated our technique on 217 API functions from 6 different libraries across 28 projects written in C and found that it can identify error-handling paths with an average precision of 94% and recall of 66%. We also found that our technique can infer correct API error specifications with an average precision of 77% and recall of 47%. To further demonstrate the usefulness of the inferred error specifications, we used them to find 118 previously unknown potential bugs (including several security flaws that are currently being fixed by the corresponding developers) in the 28 tested projects.
[Algorithm design and analysis, C APIs, program debugging, software reliability, software developers, bug-finding tools, API errors, Security, C language, formal specification, program statements, branching points, automated inference, Libraries, Robustness, software tools, APEx, software robustness, error handling bugs, Documentation, specification mining, error handling code, Computer bugs, C programs, API functions, Software, software security, error handling, error specifications]
On essential configuration complexity: Measuring interactions in highly-configurable systems
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Quality assurance for highly-configurable systems is challenging due to the exponentially growing configuration space. Interactions among multiple options can lead to surprising behaviors, bugs, and security vulnerabilities. Analyzing all configurations systematically might be possible though if most options do not interact or interactions follow specific patterns that can be exploited by analysis tools. To better understand interactions in practice, we analyze program traces to characterize and identify where interactions occur on control flow and data. To this end, we developed a dynamic analysis for Java based on variability-aware execution and monitor executions of multiple small to medium-sized programs. We find that the essential configuration complexity of these programs is indeed much lower than the combinatorial explosion of the configuration space indicates. However, we also discover that the interaction characteristics that allow scalable and complete analyses are more nuanced than what is exploited by existing state-of-the-art quality assurance strategies.
[Java, program diagnostics, Feature Interaction, Extraterrestrial measurements, dynamic analysis, Complexity theory, software quality, configuration complexity, Quality assurance, variability-aware execution, execution monitoring, Computer bugs, quality assurance, program trace analysis, Variability-Aware Execution, Configurable Software, highly-configurable system, Meteorology, Testing]
Precise semantic history slicing through dynamic delta refinement
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Semantic history slicing solves the problem of extracting changes related to a particular high-level functionality from the software version histories. State-of-the-art techniques combine static program analysis and dynamic execution tracing to infer an over-approximated set of changes that can preserve the functional behaviors captured by a test suite. However, due to the conservative nature of such techniques, the sliced histories may contain irrelevant changes. In this paper, we propose a divide-and-conquer-style partitioning approach enhanced by dynamic delta refinement to produce minimal semantic history slices. We utilize deltas in dynamic invariants generated from successive test executions to learn significance of changes with respect to the target functionality. Empirical results indicate that these measurements accurately rank changes according to their relevance to the desired test behaviors and thus partition history slices in an efficient and effective manner.
[Heuristic algorithms, dynamic delta refinement, static program analysis, History, precise semantic history slicing, divide and conquer style partitioning approach, dynamic invariants, software configuration management, configuration management, Semantic history slicing, Runtime, Semantics, Organizations, program analysis, Software, Performance analysis, program slicing, software version histories, dynamic execution tracing]
Goal-conflict detection based on temporal satisfiability checking
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Goal-oriented requirements engineering approaches propose capturing how a system should behave through the specification of high-level goals, from which requirements can then be systematically derived. Goals may however admit subtle situations that make them diverge, i.e., not be satisfiable as a whole under specific circumstances feasible within the domain, called boundary conditions. While previous work allows one to identify boundary conditions for conflicting goals written in LTL, it does so through a pattern-based approach, that supports a limited set of patterns, and only produces pre-determined formulations of boundary conditions. We present a novel automated approach to compute boundary conditions for general classes of conflicting goals expressed in LTL, using a tableaux-based LTL satisfiability procedure. A tableau for an LTL formula is a finite representation of all its satisfying models, which we process to produce boundary conditions that violate the formula, indicating divergence situations. We show that our technique can automatically produce boundary conditions that are more general than those obtainable through existing previous pattern-based approaches, and can also generate boundary conditions for goals that are not captured by these patterns.
[Methane, goal-oriented requirements engineering, Goal Conflicts, Satisfiability Checking, Computational modeling, computability, temporal logic, Boundary conditions, temporal satisfiability checking, tableaux-based LTL satisfiability, formal specification, Cost accounting, Standards, finite representation, goal-conflict detection, Software, pattern-based approach, Requirements engineering, boundary conditions, Tableaux Method, pattern recognition]
Symbolic execution of stored procedures in database management systems
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Stored procedures in database management systems are often used to implement complex business logic. Correctness of these procedures is critical for correct working of the system. However, testing them remains difficult due to many possible states of data and database constraints. This leads to mostly manual testing. Newer tools offer automated execution for unit testing of stored procedures but the test cases are still written manually. In this paper, we propose a novel approach of using dynamic symbolic execution to automatically generate test cases and corresponding database states for stored procedures. We treat values in database tables as symbolic, model the constraints on data imposed by the schema and by the SQL statements executed by the stored procedure. We use an SMT solver to find values that will drive the stored procedure on a particular execution path. We instrument the internal execution plans generated by PostgreSQL database management system to extract constraints and use the Z3 SMT solver to generate test cases consisting of table data and procedure inputs. Our evaluation using stored procedures from a large business application shows that this technique can uncover bugs that lead to schema constraint violations and user defined exceptions.
[Symbolic Execution, program testing, Instruments, PostgreSQL, Z3 SMT solver, unit testing, test cases, SQL statements, database tables, Remuneration, Servers, database management systems, execution path, database constraints, complex business logic, SQL, stored procedures, Analytical models, automated execution, internal execution plans, Stored Procedures, Database systems, dynamic symbolic execution, Testing]
Conc-iSE: Incremental symbolic execution of concurrent software
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Software updates often introduce new bugs to existing code bases. Prior regression testing tools focus mainly on test case selection and prioritization whereas symbolic execution tools only handle code changes in sequential software. In this paper, we propose the first incremental symbolic execution method for concurrent software to generate new tests by exploring only the executions affected by code changes between two program versions. Specifically, we develop an inter-thread and inter-procedural change-impact analysis to check if a statement is affected by the changes and then leverage the information to choose executions that need to be re-explored. We also check if execution summaries computed in the previous program can be used to avoid redundant explorations in the new program. We have implemented our method in an incremental symbolic execution tool called Conc-iSE and evaluated it on a large set of multithreaded C programs. Our experiments show that the new method can significantly reduce the overall symbolic execution time when compared with state-of-the-art symbolic execution tools such as KLEE.
[Algorithm design and analysis, program debugging, program testing, sequential software, code bases, test case selection, test case prioritization, Programming, regression testing tools, C language, interprocedural change-impact analysis, multithreaded C programs, Partial order reduction, Concurrent computing, bugs, software updates, Concurrency, program versions, concurrent software incremental symbolic execution, Symbolic execution, concurrency (computers), Testing, multi-threading, Software algorithms, Weakest precondition, Conc-iSE, Computer bugs, Software, interthread change-impact analysis, execution summaries]
Model-based whitebox fuzzing for program binaries
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Many real-world programs take highly structured and complex files as inputs. The automated testing of such programs is non-trivial. If the test does not adhere to a specific file format, the program returns a parser error. For symbolic execution-based whitebox fuzzing the corresponding error handling code becomes a significant time sink. Too much time is spent in the parser exploring too many paths leading to trivial parser errors. Naturally, the time is better spent exploring the functional part of the program where failure with valid input exposes deep and real bugs in the program. In this paper, we suggest to leverage information about the file format and data chunks of existing, valid files to swiftly carry the exploration beyond the parser code. We call our approach Modelbased Whitebox Fuzzing (MoWF) because the file format input model of blackbox fuzzers can be exploited as a constraint on the vast input space to rule out most invalid inputs during path exploration in symbolic execution. We evaluate on 13 vulnerabilities in 8 large program binaries with 6 separate file formats and found that MoWF exposes all vulnerabilities while both, traditional whitebox fuzzing and model-based blackbox fuzzing, expose only less than half, respectively. Our experiments also demonstrate that MoWF exposes 70% vulnerabilities without any seed inputs.
[program debugging, program testing, parser error, path exploration, program compilers, bugs, model-based whitebox fuzzing, symbolic execution-based whitebox fuzzing, Libraries, program binaries, Testing, model-based blackbox fuzzing, Symbolic Execution, Media, Grammar, Browsers, error handling code, security of data, Computer bugs, MoWF, program vulnerabilities, Data models, Program Binaries, error handling, automated program testing]
Symbolic execution of complex program driven by machine learning based constraint solving
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Symbolic execution is a widely-used program analysis technique. It collects and solves path conditions to guide the program traversing. However, due to the limitation of the current constraint solvers, it is difficult to apply symbolic execution on programs with complex path conditions, like nonlinear constraints and function calls. In this paper, we propose a new symbolic execution tool MLB to handle such problem. Instead of relying on the classical constraint solving, in MLB, the feasibility problems of the path conditions are transformed into optimization problems, by minimizing some dissatisfaction degree. The optimization problems are then handled by the underlying optimization solver through machine learning guided sampling and validation. MLB is implemented on the basis of Symbolic PathFinder and encodes not only the simple linear path conditions, but also nonlinear arithmetic operations, and even black-box function calls of library methods, into symbolic path conditions. Experiment results show that MLB can achieve much better coverage on complex real-world programs.
[Algorithm design and analysis, optimization solver, nonlinear arithmetic operations, Machine learning algorithms, constraint solving, MLB, constraint solvers, Transforms, arithmetic, Machine Learning, Optimization, Engines, Constraint Solving, complex program, optimisation, library methods, program analysis, Libraries, constraint handling, learning (artificial intelligence), Java, Symbolic Execution, program diagnostics, linear path conditions, nonlinear constraints, machine learning, symbolic execution tool, complex path conditions, black-box function calls, dissatisfaction degree, symbolic PathFinder, symbolic path conditions, optimization problems, Complicated Path Condition]
Towards bounded model checking using nonlinear programming solver
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Due to their complexity, currently available bounded model checking techniques based on Boolean Satisfiability and Satisfiability Modulo Theories inadequately handle non-linear floating-point and integer arithmetic. Using a numerical approach, we reduce a bounded model checking problem to a constraint satisfaction problem. Currently available techniques attempt to solve the constraint problem but can guarantee neither global convergence nor correctness. Using the IPOPT and ANTIGONE non-linear programming (NLP) solvers, we transform the original constraint satisfaction problem from one having disjunctions of constraints into one having conjunctions of constraints with a few introduced auxiliary variables. The transformation lowers the computing cost and preserves the Boolean structure of the original problem while complying with limits of NLP solvers.
[ANTIGONE nonlinear programming, nonlinear programming, SAT, Programming, computability, auxiliary variables, nonlinear programming solver, Boolean satisfiability, Convergence, Nonlinear Programming, formal verification, constraint problem, Boolean structure, Model checking, Satisfiability Modulo Theories, Mathematical model, global convergence, IPOPT, constraint satisfaction problem, Encoding, Boolean algebra, NLP solvers, Bounded Model Checking, nonlinear floating point, integer arithmetic, bounded model checking problem, Software]
Identifying domain elements from textual specifications
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Analysis modeling refers to the task of identifying domain objects, their attributes and operations, and the relationships between these objects from software requirements specifications which are usually written in some natural language. There have been a few efforts to automate this task, but they seem to be largely constrained by the language related issues as well as the lack of a systematic transformation process. In this paper, we propose a systematic, automated transformation approach which first interprets the specification sentences based on the Hornby's verb patterns, and then uses semantic relationships between the words in the sentences, obtained from Type Dependencies using Stanford NL Parser, to identify the domain elements from them. With the help of a controlled experiment, we show that the analysis class diagrams generated by the proposed approach are far more correct, far more complete and less redundant than those generated by the exiting automated approaches.
[text analysis, Unified modeling language, software requirements specifications, diagrams, Analysis modeling, Analysis class diagram, formal specification, systematic automated transformation approach, Stanford NL parser, Analytical models, Semantics, Natural language processing, Marine vehicles, textual specifications, type dependencies, Automated approach, natural language processing, Natural Language Processing, Model transformation, Object recognition, domain objects identification, domain elements, analysis modeling, specification sentences, grammars, analysis class diagrams, Software, semantic relationships, natural language, Hornby verb patterns]
Continuous detection of design flaws in evolving object-oriented programs using incremental multi-pattern matching
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Design flaws in object-oriented programs may seriously corrupt code quality thus increasing the risk for introducing subtle errors during software maintenance and evolution. Most recent approaches identify design flaws in an ad-hoc manner, either focusing on software metrics, locally restricted code smells, or on coarse-grained architectural anti-patterns. In this paper, we utilize an abstract program model capturing high-level object-oriented code entities, further augmented with qualitative and quantitative design-related information such as coupling/cohesion. Based on this model, we propose a comprehensive methodology for specifying object-oriented design flaws by means of compound rules integrating code metrics, code smells and anti-patterns in a modular way. This approach allows for efficient, automated design-flaw detection through incremental multi-pattern matching, by facilitating systematic information reuse among multiple detection rules as well as between subsequent detection runs on continuously evolving programs. Our tool implementation comprises well-known anti-patterns for Java programs. The results of our experimental evaluation show high detection precision, scalability to real-size programs, as well as a remarkable gain in efficiency due to information reuse.
[Software maintenance, locally restricted code smell, pattern matching, abstract program model, software quality, formal specification, evolving object-oriented program, code quality, software evolution, software architecture, Software metrics, Java program, Motion pictures, systematic information reuse, continuous software evolution, coarse-grained architectural antipattern, high-level object-oriented code entities, Java, object-oriented programming, Object oriented modeling, object-oriented software architecture, software maintenance, incremental multipattern matching, object-oriented design flaw specification, code metrics, design-flaw detection, software metrics, continuous design flaw detection]
Efficient detection of inconsistencies in a multi-developer engineering environment
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Software developers work concurrently on different kinds of development artifacts such as requirements, architecture, design, or source code. To keep these development artifacts consistent, developers have a wide range of consistency checking approaches available. However, most existing consistency checkers work best in context of single tools and they are not well suited when development artifacts are distributed among different tools and are being modified concurrently by many developers. This paper presents a novel, cloud-based approach to consistency checking in a multi-developer/-tool engineering environment. It allows instant consistency checking even if developers and their tools are distributed and even if they do not have access to all artifacts. It does this by systematically reusing consistency checking knowledge to keep the memory/CPU cost of consistency checking to a small constant overhead per developer. The feasibility and scalability of our approach is demonstrated through an empirical validation with 22 partly industrial system models. A prototype implementation implementation is available through the DesignSpace Engineering Cloud.
[Context, software development, Scalability, Multi-Developer Engineering, Unified modeling language, consistency checker, software tool, Model-Driven Engineering, formal verification, cloud-based approach, inconsistency detection, Software systems, multideveloper engineering environment, software tools, cloud computing, Incremental Consistency Checking, Context modeling]
How good are the specs? A study of the bug-finding effectiveness of existing Java API specifications
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Runtime verification can be used to find bugs early, during software development, by monitoring test executions against formal specifications (specs). The quality of runtime verification depends on the quality of the specs. While previous research has produced many specs for the Java API, manually or through automatic mining, there has been no large-scale study of their bug-finding effectiveness. We present the first in-depth study of the bug-finding effectiveness of previously proposed specs. We used JavaMOP to monitor 182 manually written and 17 automatically mined specs against more than 18K manually written and 2.1M automatically generated tests in 200 open-source projects. The average runtime overhead was under 4.3x. We inspected 652 violations of manually written specs and (randomly sampled) 200 violations of automatically mined specs. We reported 95 bugs, out of which developers already fixed 74. However, most violations, 82.81% of 652 and 97.89% of 200, were false alarms. Our empirical results show that (1) runtime verification technology has matured enough to incur tolerable runtime overhead during testing, and (2) the existing API specifications can find many bugs that developers are willing to fix; however, (3) the false alarm rates are worrisome and suggest that substantial effort needs to be spent on engineering better specs and properly evaluating their effectiveness.
[program debugging, tolerable runtime overhead, runtime overhead, application program interfaces, program testing, program verification, public domain software, open-source projects, Open source software, Runtime, runtime verification, Monitoring, Testing, Java, test executions monitoring, software development, bug-finding effectiveness, empirical study, Synchronization, formal specifications, automatically mined specs, Java API specifications, Computer bugs, Java-MOP, manually written specs, false alarm rates, specification quality]
Greedy combinatorial test case generation using unsatisfiable cores
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Combinatorial testing aims at covering the interactions of parameters in a system under test, while some combinations may be forbidden by given constraints (forbidden tuples). In this paper, we illustrate that such forbidden tuples correspond to unsatisfiable cores, a widely understood notion in the SAT solving community. Based on this observation, we propose a technique to detect forbidden tuples lazily during a greedy test case generation, which significantly reduces the number of required SAT solving calls. We further reduce the amount of time spent in SAT solving by essentially ignoring constraints while constructing each test case, but then &#x201C;amending&#x201D; it to obtain a test case that satisfies the constraints, again using unsatisfiable cores. Finally, to complement a disturbance due to ignoring constraints, we implement an efficient approximative SAT checking function in the SAT solver Lingeling. Through experiments we verify that our approach significantly improves the efficiency of constraint handling in our greedy combinatorial testing algorithm.
[Software testing, forbidden tuples, program testing, greedy algorithms, Software algorithms, SAT solving, computability, SAT solving calls, Browsers, parameter interactions, SAT checking function, Linux, Combinatorial testing, test case generation, Approximation algorithms, Software, unsatisfiable cores, constraint handling, greedy combinatorial test case generation, SAT solving community, SAT solver Lingeling]
Towards automatically generating descriptive names for unit tests
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
During maintenance, developers often need to understand the purpose of a test. One of the most potentially useful sources of information for understanding a test is its name. Ideally, test names are descriptive in that they accurately summarize both the scenario and the expected outcome of the test. Despite the benefits of being descriptive, test names often fall short of this goal. In this paper we present a new approach for automatically generating descriptive names for existing test bodies. Using a combination of natural-language program analysis and text generation, the technique creates names that summarize the test's scenario and the expected outcome. The results of our evaluation show that, (1) compared to alternative approaches, the names generated by our technique are significantly more similar to human-generated names and are nearly always preferred by developers, (2) the names generated by our technique are preferred over or are equivalent to the original test names in 83% of cases, and (3) our technique is several orders of magnitude faster than manually writing test names.
[Descriptive names, Software maintenance, program testing, natural language processing, Natural languages, unit testing, Maintenance engineering, Unit testing, Maintenance, software maintenance, text generation, descriptive name, Semantics, Prototypes, test name, natural-language program analysis, Testing]
Applying combinatorial test data generation to big data applications
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Big data applications (e.g., Extract, Transform, and Load (ETL) applications) are designed to handle great volumes of data. However, processing such great volumes of data is time-consuming. There is a need to construct small yet effective test data sets during agile development of big data applications. In this paper, we apply a combinatorial test data generation approach to two real-world ETL applications at Medidata. In our approach, we first create Input Domain Models (IDMs) automatically by analyzing the original data source and incorporating constraints manually derived from requirements. Next, the IDMs are used to create test data sets that achieve t-way coverage, which has shown to be very effective in detecting software faults. The generated test data sets also satisfy all the constraints identified in the first step. To avoid creating IDMs from scratch when there is a change to the original data source or constraints, our approach extends the original IDMs with additional information. The new IDMs, which we refer to as Adaptive IDMs (AIDMs), are updated by comparing the changes against the additional information, and are then used to generate new test data sets. We implement our approach in a tool, called comBinatorial big daTa Test dAta Generator (BIT-TAG). Our experience shows that combinatorial testing can be effectively applied to big data applications. In particular, the test data sets created using our approach for the two ETL applications are only a small fraction of the original data source, but we were able to detect all the faults found with the original data source.
[program testing, Big Data Testing, Big Data, Data mining, Medidata, Adaptive Input Domain Model, Research and development, AIDM, input domain models, Test Data Generation, Input Domain Model, Databases, real-world ETL applications, adaptive IDM, BIT-TAG, Data models, combinatorial big data test data generator, Combinatorial Testing, Testing]
Generating test cases to expose concurrency bugs in android applications
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Mobile systems usually support an event-based model of concurrent programming. This model, although advantageous to maintain responsive user interfaces, may lead to subtle concurrency errors due to unforeseen threads interleaving coupled with non-deterministic reordering of asynchronous events. These bugs are very difficult to reproduce even by the same user action sequences that trigger them, due to the undetermined schedules of underlying events and threads. In this paper, we proposed RacerDroid, a novel technique that aims to expose concurrency bugs in android applications by actively controlling event schedule and thread interleaving, given the test cases that have potential data races. By exploring the state model of the application constructed dynamically, our technique starts first to generate a test case that has potential data races based on the results obtained from existing static or dynamic race detection technique. Then it reschedules test cases execution by actively controlling event dispatching and thread interleaving to determine whether such potential races really lead to thrown exceptions or assertion violations. Our preliminary experiments show that RacerDroid is effective, and it confirms real data races, while at the same time eliminates false warnings for Android apps found in the wild.
[program debugging, program testing, concurrency bug, Humanoid robots, concurrent programming, user interfaces, Concurrent computing, Android (operating system), mobile computing, RacerDroid technique, test case generation, data race, Android application, record/replay, Message systems, event-based model, Instruments, mobile application, testing, smart phone, smart phones, user interface, Android, Computer bugs, concurrency control, mobile system, Data models, Androids]
Automatic test image generation using procedural noise
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
It is difficult to test programs that input images, due to the large number of (pixel) values that must be chosen and the complex ways these values interact. Typically, such programs are tested manually, using images that have known results. However, this is a laborious process and limited in the range of tests that can be applied. We introduce a new approach for testing programs that input images automatically, using procedural noise and spatial statistics to create inputs that are both realistic and can easily be tuned to have specific properties. The effectiveness of our approach is illustrated on an epidemiological simulation of a recently introduced tree pest in Great Britain: Oriental Chestnut Gall Wasp. Our approach produces images that match the real landscapes more closely than other techniques and can be used (alongside metamorphic relations) to detect smaller (artificially introduced) errors with greater accuracy.
[image processing, real landscapes, program testing, image pixels, software testing, test data generation, image denoising, image matching, epidemiological simulation, Genetic algorithms, Histograms, Great Britain, spatial statistics, procedural noise, Imaging, tree pest, Oriental chestnut gall wasp, Image generation, White noise, Software, automatic test image generation, Testing]
Move-optimized source code tree differencing
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
When it is necessary to express changes between two source code files as a list of edit actions (an edit script), modern tree differencing algorithms are superior to most text-based approaches because they take code movements into account and express source code changes more accurately. We present 5 general optimizations that can be added to state-of-the-art tree differencing algorithms to shorten the resulting edit scripts. Applied to Gumtree, RTED, JSync, and ChangeDistiller, they lead to shorter scripts for 1898% of the changes in the histories of 9 open-source software repositories. These optimizations also are parts of our novel Move-optimized Tree DIFFerencing algorithm (MTD-IFF) that has a higher accuracy in detecting moved code parts. MTDIFF (which is based on the ideas of ChangeDistiller) further shortens the edit script for another 20% of the changes in the repositories. MTDIFF and all the benchmarks are available under an open-source license.
[source code (software), text-based approaches, Source Code, public domain software, RTED, Programming, Gumtree, Complexity theory, History, moved code parts, Optimization, Open source software, edit script, source code files, Runtime, optimisation, MTD-IFF, edit actions, move-optimized source code tree differencing algorithm, ChangeDistiller, code movements, Tree Differencing, software maintenance, source code changes, open-source software repositories, open-source license, Optimizations, optimizations, JSync]
Migrating cascading style sheets to preprocessors by introducing mixins
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Cascading Style Sheets (CSS) is the standard language for styling web documents and is extensively used in the industry. However, CSS lacks constructs that would allow code reuse (e.g., functions). Consequently, maintaining CSS code is often a cumbersome and error-prone task. Preprocessors (e.g., Less and Sass) have been introduced to fill this gap, by extending CSS with the missing constructs. Despite the clear maintainability benefits coming from the use of preprocessors, there is currently no support for migrating legacy CSS code to preprocessors. In this paper, we propose a technique for automatically detecting duplicated style declarations in CSS code that can be migrated to preprocessor functions (i.e., mixins). Our technique can parameterize differences in the style values of duplicated declarations, and ensure that the migration will not change the presentation semantics of the web documents. The evaluation has shown that our technique is able to detect 98% of the mix-ins that professional developers introduced in websites and Style Sheet libraries, and can safely migrate real CSS code.
[document handling, CSS, preprocessor function, code maintenance, HTML, Browsers, programming languages, software maintenance, cascading style sheet, code reuse, duplication, mixin, hypermedia markup language, refactoring, Semantics, Web document styling, Syntactics, software reusability, migration, Software, Libraries, Internet, Cascading style sheets, hypermedia markup languages]
Automatic runtime recovery via error handler synthesis
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Software systems are often subject to unexpected runtime errors. Automatic runtime recovery (ARR) techniques aim to recover them from erroneous states and maintain them functional in the field. This paper proposes Ares, a novel, practical approach for ARR. Our key insight is leveraging a system's inherent error handling support to recover from unexpected errors. To this end, we synthesize error handlers in two ways: error transformation and early return. We also equip Ares with a lightweight in-vivo testing infrastructure to select the promising synthesis method and avoid potentially dangerous error handlers. Unlike existing ARR techniques with heavyweight mechanisms (e.g., checkpoint-restart and runtime monitoring), our approach expands the intrinsic capability of runtime error resilience in software systems to handle unexpected errors. Ares's lightweight mechanism makes it practical and easy to be integrated into production environments. We have implemented Ares on top of both the Java HotSpot VM and Android ART, and applied it to recover from 52 real-world bugs. The results are promising - Ares successfully recovers from 39 of them and incurs negligible overhead.
[Android ART, JVM, program debugging, program testing, software systems, exception handling, real-world bugs, Runtime, erroneous states, early return, automatic runtime recovery, runtime error resilience, Testing, Ares, Java, error handler synthesis, testing infrastructure, Resilience, production environments, Java HotSpot VM, Computer bugs, virtual machines, ARR techniques, Software systems, Androids, error handling, error transformation]
Mining revision histories to detect cross-language clones without intermediates
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
To attract more users on different platforms, many projects release their versions in multiple programming languages (e.g., Java and C#). They typically have many code snippets that implement similar functionalities, i.e., cross-language clones. Programmers often need to track and modify cross-language clones consistently to maintain similar functionalities across different language implementations. In literature, researchers have proposed approaches to detect cross-language clones, mostly for languages that share a common intermediate language (such as the .NET language family) so that techniques for detecting single-language clones can be applied. As a result, those approaches cannot detect cross-language clones for many projects that are not implemented in a .NET language. To overcome the limitation, in this paper, we propose a novel approach, CLCMiner, that detects cross-language clones automatically without the need of an intermediate language. Our approach mines such clones from revision histories, which reflect how programmers maintain cross-language clones in practice. We have implemented a prototype tool for our approach and conducted an evaluation on five open source projects that have versions in Java and C#. The results show that CLCMiner achieves high accuracy and point to promising future work.
[Java, open source projects, revision histories mining, single-language clones detection, cross-language clones detection, Cloning, data mining, diff, C# languages, Grammar, History, programming languages, software maintenance, CLCMiner, revision history, Java language, .NET language family, cross-language clone, Software, intermediate language, C# language]
Battery-aware transformations in mobile applications
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
We present an adaptive binary transformation system for reducing the energy impact of advertisements and analytics in mobile applications. Our approach accommodates both the needs of mobile app developers to obtain income from advertisements and the desire of mobile device users for longer battery life. Our technique automatically identifies recurrent advertisement and analytics requests and throttles these requests based on a mobile device's battery status. Of the Android applications we analyzed, 75% have at least one connection that exhibits such recurrent requests. Our automated detection scheme classifies these requests with 100% precision and 80.5% recall. Applying the proposed battery-aware transformations to a representative mobile application reduces the power consumption of the mobile device by 5.8%, without the negative effect of completely removing advertisements.
[Instruments, Humanoid robots, Mobile communication, Batteries, advertising, battery-aware transformations, mobile device battery status, analytics, Android (operating system), mobile computing, power aware computing, advertisements, mobile applications, battery lifetime, mobile advertisements, program analysis, Android applications, Energy efficiency, Androids, Smart phones, adaptive binary transformation system, energy impact reduction]
Bugram: Bug detection with n-gram language models
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
To improve software reliability, many rule-based techniques have been proposed to infer programming rules and detect violations of these rules as bugs. These rule-based approaches often rely on the highly frequent appearances of certain patterns in a project to infer rules. It is known that if a pattern does not appear frequently enough, rules are not learned, thus missing many bugs. In this paper, we propose a new approach - Bugram - that leverages n-gram language models instead of rules to detect bugs. Bugram models program tokens sequentially, using the n-gram language model. Token sequences from the program are then assessed according to their probability in the learned model, and low probability sequences are marked as potential bugs. The assumption is that low probability token sequences in a program are unusual, which may indicate bugs, bad practices, or unusual/special uses of code of which developers may want to be aware. We evaluate Bugram in two ways. First, we apply Bugram on the latest versions of 16 open source Java projects. Results show that Bugram detects 59 bugs, 42 of which are manually verified as correct, 25 of which are true bugs and 17 are code snippets that should be refactored. Among the 25 true bugs, 23 cannot be detected by PR-Miner. We have reported these bugs to developers, 7 of which have already been confirmed by developers (4 of them have already been fixed), while the rest await confirmation. Second, we further compare Bugram with three additional graph- and rule-based bug detection tools, i.e., JADET, Tikanga, and GrouMiner. We apply Bugram on 14 Java projects evaluated in these three studies. Bugram detects 21 true bugs, at least 10 of which cannot be detected by these three tools. Our results suggest that Bugram is complementary to existing rule-based bug detection approaches.
[Java, program debugging, programming rule inference, natural language processing, public domain software, Buildings, software reliability, probability, probability sequence, Programming, rule-based technique, n-gram language model, Software reliability, Bugram, bug detection, inference mechanisms, N-gram Language Model, Static Code Analysis, Computer bugs, Semantics, knowledge based systems, Software, Bug Detection, open source Java project]
Mining input grammars from dynamic taints
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Knowing which part of a program processes which parts of an input can reveal the structure of the input as well as the structure of the program. In a URL http://www.example.com/path/, for instance, the protocol http, the host www.example.com, and the path path would be handled by different functions and stored in different variables. Given a set of sample inputs, we use dynamic tainting to trace the data flow of each input character, and aggregate those input fragments that would be handled by the same function into lexical and syntactical entities. The result is a context-free grammar that reflects valid input structure. In its evaluation, our AUTOGRAM prototype automatically produced readable and structurally accurate grammars for inputs like URLs, spreadsheets or configuration files. The resulting grammars not only allow simple reverse engineering of input formats, but can also directly serve as input for test generators.
[Java, fuzzing, Protocols, Instruments, Ports (Computers), data mining, data flow analysis, dynamic taints, AUTOGRAM prototype, spreadsheet programs, Grammar, program structure, URL, context-free grammar, Uniform resource locators, input grammars mining, spreadsheets, Input formats, context-free grammars, file organisation, Software, dynamic tainting, data flow, configuration files]
Phrase-based extraction of user opinions in mobile app reviews
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Mobile app reviews often contain useful user opinions like bug reports or suggestions. However, looking for those opinions manually in thousands of reviews is ineffective and time-consuming. In this paper, we propose PUMA, an automated, phrase-based approach to extract user opinions in app reviews. Our approach includes a technique to extract phrases in reviews using part-of-speech (PoS) templates; a technique to cluster phrases having similar meanings (each cluster is considered as a major user opinion); and a technique to monitor phrase clusters with negative sentiments for their outbreaks over time. We used PUMA to study two popular apps and found that it can reveal severe problems of those apps reported in their user reviews.
[phrase clustering, PoS template, Review Analysis, sentiment analysis, Artificial neural networks, Mobile communication, Batteries, software reviews, mobile app review, mobile computing, Phrase Extraction, pattern clustering, part-of-speech template, Opinion Mining, Computer bugs, feature extraction, phrase-based extraction, Games, PUMA, Facebook, Monitoring, user opinion]
Practical guidelines for change recommendation using association rule mining
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Association rule mining is an unsupervised learning technique that infers relationships among items in a data set. This technique has been successfully used to analyze a system's change history and uncover evolutionary coupling between system artifacts. Evolutionary coupling can, in turn, be used to recommend artifacts that are potentially affected by a given set of changes to the system. In general, the quality of such recommendations is affected by (1) the values selected for various parameters of the mining algorithm, (2) characteristics of the set of changes used to derive a recommendation, and (3) characteristics of the system's change history for which recommendations are generated. In this paper, we empirically investigate the extent to which certain choices for these factors affect change recommendation. Specifically, we conduct a series of systematic experiments on the change histories of two large industrial systems and eight large open source systems, in which we control the size of the change set for which to derive a recommendation, the measure used to assess the strength of the evolutionary coupling, and the maximum size of historical changes taken into account when inferring these couplings. We use the results from our study to derive a number of practical guidelines for applying association rule mining for change recommendation.
[Algorithm design and analysis, data mining, change recommendations, Size measurement, parameter tuning, History, Data mining, Guidelines, unsupervised learning, change recommendation, association rule mining, Couplings, recommender systems, recommendation quality, evolutionary coupling, Evolutionary coupling, Software, unsupervised learning technique, change impact analysis, open source systems]
Learning a dual-language vector space for domain-specific cross-lingual question retrieval
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
The lingual barrier limits the ability of millions of non-English speaking developers to make effective use of the tremendous knowledge in Stack Overflow, which is archived in English. For cross-lingual question retrieval, one may use translation-based methods that first translate the non-English queries into English and then perform monolingual question retrieval in English. However, translation-based methods suffer from semantic deviation due to inappropriate translation, especially for domain-specific terms, and lexical gap between queries and questions that share few words in common. To overcome the above issues, we propose a novel cross-lingual question retrieval based on word embed-dings and convolutional neural network (CNN) which are the state-of-the-art deep learning techniques to capture word- and sentence-level semantics. The CNN model is trained with large amounts of examples from Stack Overflow duplicate questions and their corresponding translation by machine, which guides the CNN to learn to capture informative word and sentence features to recognize and quantify semantic similarity in the presence of semantic deviations and lexical gaps. A uniqueness of our approach is that the trained CNN can map documents in two languages (e.g., Chinese queries and English questions) in a dual-language vector space, and thus reduce the cross-lingual question retrieval problem to a simple k-nearest neighbors search problem in the dual-language vector space, where no query or question translation is required. Our evaluation shows that our approach significantly outperforms the translation-based method, and can be extended to dual-language documents retrieval from different sources.
[text analysis, Dual-Language Vector Space, dual-language vector space, Predictive models, deep learning technique, English questions, semantic similarity, Cross-lingual question retrieval, document mapping, k-nearest neighbor search problem, query processing, lingual barrier, Chinese queries, Convolution, Semantics, domain-specific terms, machine translation, domain-specific cross-lingual question retrieval, learning (artificial intelligence), search problems, Context, Google, Convolutional Neural Network, nonEnglish speaking developers, natural language processing, convolutional neural network, Inspection, CNN model training, word embeddings, sentence-level semantics capture, dual-language document retrieval, Stack Overflow, lexical gap, monolingual question retrieval, translation-based method, word-level semantics capture, Neural networks, semantic deviation, question answering (information retrieval), neural nets, Word embeddings, language translation]
Reflection-aware static analysis of Android apps
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
We demonstrate the benefits of DroidRA, a tool for taming reflection in Android apps. DroidRA first statically extracts reflection-related object values from a given Android app. Then, it leverages the extracted values to boost the app in a way that reflective calls are no longer a challenge for existing static analyzers. This is achieved through a bytecode instrumentation approach, where reflective calls are supplemented with explicit traditional Java method calls which can be followed by state-of-the-art analyzers which do not handle reflection. Instrumented apps can thus be completely analyzed by existing static analyzers, which are no longer required to be modified to support reflection-aware analysis. The video demo of DroidRA can be found at https://youtu.be/-HW0V68aAWc.
[Java, reflection-related object values extraction, reflection-aware static analysis, DroidRA benefits, Instruments, program diagnostics, reflection, Humanoid robots, Reflection, Android apps, bytecode instrumentation approach, Android, Runtime, Android (operating system), reflective calls, DroidRA, Coal, taming reflection tool, Java method calls, Androids, reflection-aware analysis, Static Analysis, Smart phones]
Relda2: An effective static analysis tool for resource leak detection in Android apps
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Resource leak is a common bug in Android applications (apps for short). In general, it is caused by missing release operations of the resources provided by Android (like Camera, Media Player and Sensors) that require programmers to explicitly release them. It might lead to several serious problems for the app and system, such as performance degradation and system crash. This paper presents Relda2, a light-weight, scalable and practical static analysis tool, for detecting resource leaks in the byte-code of Android apps automatically. It supports two analysis techniques (flow-insensitive for quick scanning and flow-sensitive for accurate scanning), and performs inter-procedural analysis to get more precise bug reports. In addition, our tool is practical to analyze real-world apps, and has been applied to 103 Android apps, including industry applications and open source programs. We have found 67 real resource leaks in these apps, which we confirmed manually. A demo video of our tool can be found at the website: https://www.youtube.com/watch?v=Mk-MFcHpTds.
[resource leak detection, program diagnostics, Humanoid robots, light-weight static analysis, byte-code, static analysis, Android apps, Leak detection, scalable static analysis, Android (operating system), open source program, Relda2, Computer bugs, interprocedural analysis, Software, static analysis tool, Androids, resource leak, Smart phones]
An end-user oriented tool suite for development of mobile applications
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
In this paper, we show an end-user oriented tool suite for mobile application development. The advantages of this tool suite are that the graphical user interface (GUI), as well as the application logic can both be developed in a rapid and simple way, and web-based services on the Internet can be integrated into our platform by end-users. This tool suite involves three sub-systems, namely ServiceAccess, EasyApp and LSCE. ServiceAccess takes charge of the registration and management of heterogeneous services, and can export different form of services according to the requirements of the other sub-systems. EasyApp is responsible for developing GUI in the form of mobile app. LSCE takes charge of creating the application logic that can be invoked by mobile app directly. Finally, a development case is presented to illustrate the development process using this tool suite. The URL of demo video: https://youtu.be/mM2WkU1_k-w.
[Visualization, end-user oriented tool suite, cross-platform, graphical user interfaces, graphical user interface, Mobile communication, Mobile applications, Mobile application, Servers, mobile application development, formal logic, application logic, mobile computing, end-user development, Web services, visual development environment, Libraries, GUI, software tools, Graphical user interfaces]
Model driven design of heterogeneous synchronous embedded systems
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Synchronous embedded systems are becoming more and more complicated and are usually implemented with integrated hardware/software solutions. This implementation manner brings new challenges to the traditional model-driven design environments such as SCADE and STATEMATE, that supports pure hardware or software design. In this paper, we propose a co-design tool Tsmart-Edola to facilitate the system developers, and automatically generate the executable VHDL code and C code from the formal verified SyncBlock computation model. SyncBlock is a lightweight high-level system specification model with well defined syntax, simulation and formal semantics. Based on which, the graphical model editor, graphical simulator, verification translator, and code generator are implemented and seamlessly integrated into the Tsmart-Edola. For evaluation, we apply Tsmart-Edola to the design of a real-world train controller based on the international standard IEC 61375. Several critical ambiguousness or bugs in the standard are detected during formal verification of the constructed system model. Furthermore, the generated VHDL code and C code of Tsmart-Edola outperform that of the state-of-the-art tools in terms of synthesized gate array resource consumption and binary code size. The abstract demo video address is : https://youtu.be/D9ROyJmKZ4s The tool, user manual and examples can be downloaded: http://sts.thss.tsinghua.edu.cn/Tsmart-Edola/.
[SCADE, Ports (Computers), hardware-software codesign, VHDL code, formal specification, lightweight high-level system specification, model driven development, Graphical models, SyncBlock computation model, formal semantics, formal verification, Semantics, embedded systems, Hardware, code generator, model driven design, Computational modeling, hardwaresoftware co-design, Generators, C code, graphical simulator, heterogeneous synchronous embedded system, graphical model editor, computation model, Tsmart-Edola, verification translator, STATEMATE, Software, real-world train controller]
MACKE: Compositional analysis of low-level vulnerabilities with symbolic execution
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Concolic (concrete+symbolic) execution has recently gained popularity as an effective means to uncover non-trivial vulnerabilities in software, such as subtle buffer overflows. However, symbolic execution tools that are designed to optimize statement coverage often fail to cover potentially vulnerable code because of complex system interactions and scalability issues of constraint solvers. In this paper, we present a tool (MACKE) that is based on the modular interactions inferred by static code analysis, which is combined with symbolic execution and directed inter-procedural path exploration. This provides an advantage in terms of statement coverage and ability to uncover more vulnerabilities. Our tool includes a novel feature in the form of interactive vulnerability report generation that helps developers prioritize bug fixing based on severity scores. A demo of our tool is available at https://youtu.be/icC3jc3mHEU.
[program debugging, program testing, scalability issues, Scalability, severity scores, nontrivial software vulnerabilities, constraint solvers, bug fixing, Security, compositional analysis, Engines, interactive vulnerability report generation, modular interactions, concrete+symbolic execution, software tools, low-level vulnerabilities, Symbolic execution, MACKE tool, buffer overflows, program diagnostics, directed interprocedural path exploration, complex system interactions, Complex systems, concolic execution, static code analysis, Compositional analysis, Computer bugs, Memory management, Software]
BovInspector: Automatic inspection and repair of buffer overflow vulnerabilities
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Buffer overflow is one of the most common types of software vulnerabilities. Various static analysis and dynamic testing techniques have been proposed to detect buffer overflow vulnerabilities. With automatic tool support, static buffer overflow detection technique has been widely used in academia and industry. However, it tends to report too many false positives fundamentally due to the lack of software execution information. Currently, static warnings can only be validated by manual inspection, which significantly limits the practicality of the static analysis. In this paper, we present BovInspector, a tool framework for automatic static buffer overflow warnings inspection and validated bugs repair. Given the program source code and static buffer overflow vulnerability warnings, BovInspector first performs warning reachability analysis. Then, BovInspector executes the source code symbolically under the guidance of reachable warnings. Each reachable warning is validated and classified by checking whether all the path conditions and the buffer overflow constraints can be satisfied simultaneously. For each validated true warning, BovInspector fix it with three predefined strategies. BovInspector is complementary to prior static buffer overflow discovery schemes. Experimental results on real open source programs show that BovInspector can automatically inspect on average of 74.9% of total warnings, and false warnings account for about 25% to 100% (on average of 59.9%) of the total inspected warnings. In addition, the automatically generated patches fix all target vulnerabilities. Further information regarding the implementation and experimental results of BovInspector is available at http://bovinspectortool.github.io/project/. And a short video for demonstrating the capabilities of BovInspector is now available at https://youtu.be/IMdcksROJDg.
[source code (software), program debugging, program testing, buffer overflow vulnerabilities repair, public domain software, static buffer overflow detection technique, Engines, dynamic testing techniques, BovInspector, software vulnerabilities, automatic static buffer overflow warnings inspection, Testing, inspection, Validation, warning reachability analysis, Symbolic Execution, program diagnostics, Maintenance engineering, Inspection, static analysis, buffer overflow constraints, static buffer overflow discovery schemes, Explosions, Reachability analysis, bugs repair, Buffer Overflow, Automatic Repair, Software, open source programs, tool framework, program source code, path conditions]
CORRECT: Code reviewer recommendation at GitHub for Vendasta technologies
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Peer code review locates common coding standard violations and simple logical errors in the early phases of software development, and thus, reduces overall cost. Unfortunately, at GitHub, identifying an appropriate code reviewer for a pull request is challenging given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation tool-CORRECT-that considers not only the relevant cross-project work experience (e.g., external library experience) of a developer but also her experience in certain specialized technologies (e.g., Google App Engine) associated with a pull request for determining her expertise as a potential code reviewer. We design our tool using client-server architecture, and then package the solution as a Google Chrome plug-in. Once the developer initiates a new pull request at GitHub, our tool automatically analyzes the request, mines two relevant histories, and then returns a ranked list of appropriate code reviewers for the request within the browser's context. Demo: https://www.youtube.com/watch?v=rXU1wTD6QQ0.
[source code (software), GitHub, Vendasta Technologies, History, peer code review, Google Chrome plug-in, software architecture, code reviewer recommendation tool, Libraries, software tools, client-server architecture, client-server systems, software development, DP industry, Encoding, Browsers, software reviews, CORRECT, recommender systems, Collaboration, Authentication, specialized technology experience, pull request, Software, Internet, Code reviewer recommendation, cross-project experience]
ProcessPAIR: A tool for automated performance analysis and improvement recommendation in software development
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
High-maturity software development processes can generate significant amounts of data that can be periodically analyzed to identify performance problems, determine their root causes and devise improvement actions. However, conducting that analysis manually is challenging because of the potentially large amount of data to analyze and the effort and expertise required. In this paper, we present ProcessPAIR, a novel tool designed to help developers analyze their performance data with less effort, by automatically identifying and ranking performance problems and potential root causes, so that subsequent manual analysis for the identification of deeper causes and improvement actions can be properly focused. The analysis is based on performance models defined manually by process experts and calibrated automatically from the performance data of many developers. We also show how ProcessPAIR was successfully applied for the Personal Software Process (PSP). A video about ProcessPAIR is available in https://youtu.be/dEk3fhhkduo.
[data analysis, software development, Unified modeling language, Estimation, Calibration, PSP, ProcessPAIR, Sensitivity, software tool, personal software process, Statistical distributions, software process improvement, Software, Performance analysis, software tools, software performance evaluation, performance analysis, improvement recommendation, software process]
CVExplorer: Identifying candidate developers by mining and exploring their open source contributions
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Open source code contributions contain a large amount of technical skill information about developers, which can help to identify suitable candidates for a particular development job and therefore impact the success of a development team. We develop CVExplorer as a tool to extract, visualize, and explore relevant technical skills data from GitHub, such as languages and libraries used. It allows non-technical users to filter and identify developers according to technical skills demonstrated across all of their open source contributions, in order to support more accurate candidate identification. We demonstrate the usefulness of CVExplorer by using it to recommend candidates for open positions in two companies. A video demonstration of the tool is available at https:// youtu.be/xRxK-wa7PME.
[source code (software), Java, public domain software, technical skills data visualization, CVExplorer, GitHub, Mining software repositories, Developer skills identification, open source code contributions, human resource management, Data mining, Identifying candidate developers, Aggregates, Data visualization, data visualisation, Tag clouds, Software, software engineering, candidate developers identification]
Lightweight collection and storage of software repository data with DataRover
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
The ease of setting up collaboration infrastructures for software engineering projects creates a challenge for researchers that aim to analyze the resulting data. As teams can choose from various available software-as-a-service solutions and can configure them with a few clicks, researchers have to create and maintain multiple implementations for collecting and aggregating the collaboration data in order to perform their analyses across different setups. The DataRover system presented in this paper simplifies this task by only requiring custom source code for API authentication and querying. Data transformation and linkage is performed based on mappings, which users can define based on sample responses through a graphical front end. This allows storing the same input data in formats and databases most suitable for the intended analysis without requiring additional coding. Furthermore, API responses are continuously monitored to detect changes and allow users to update their mappings and data collectors accordingly. A screencast of the described use cases is available at https: //youtu.be/mt4ztff4SfU.
[source code (software), application program interfaces, Programming, API querying, software management, Data Collection, API monitoring, database management systems, query processing, storage management, software repository data collection, Databases, API authentication, software engineering, data transformation, Monitoring, Data Storage, graphical front end, project management, Link Discovery, source code, DataRover system, software repository data storage, Couplings, Collaboration, Data collection, Data Mapping Definition, Software, data linkage, software engineering project]
Visual contract extractor: A tool for reverse engineering visual contracts using dynamic analysis
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Visual contracts model the operations of classes, components or services by pre- and post-conditions formalised as graph transformation rules. They provide a precise but intuitive notation to test, document and analyse software systems. However, due to their detailed level of specification of data states and transformations, modelling a real application is a complex and error-prone process. Rather than adopting a top-down modelling approach, we follow a dynamic bottom-up approach to reverse engineer visual contracts from object-oriented programs based on tracing the execution of operations. We developed the Visual Contract Extractor (VCE), a dynamic analysis tool which supports the reverse engineering of visual operation contracts from Java programs. We explore the main features of the tool using two case studies and discuss usage scenarios ranging from traditional program understanding to novel applications in the field of model-based engineering. A screencast demonstrating the tool is provided at https://www.youtube.com/watch?v=VtTx8UHgRGo.
[Visualization, program testing, dynamic analysis tool, Unified modeling language, Reverse engineering, Visual contracts, Analytical models, postconditions, graph transformation rules, model extraction, object-oriented programs, software tools, Java programs, Contracts, Java, object-oriented programming, software system testing, Object oriented modeling, dynamic bottom-up approach, reverse engineering, operation execution tracing, dynamic analysis, specification mining, visual contract extractor, model-based engineering, graph transformation, preconditions, software system documentation, VCE, software system analysis]
SuperMod: Tool support for collaborative filtered model-driven software product line engineering
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
The increase in productivity implied by model-driven software product line engineering is weakened by the complexity exposed to the user having to manage a multi-variant model. Recently, a new paradigm has emerged: filtered software product line engineering transfers the established check-out/modify/commit workflow from version control to variability management, allowing to iteratively develop the multi-variant model in a single-variant view. This paper demonstrates SuperMod, a tool that supports collaborative filtered model-driven product line engineering, implemented for and with the Eclipse Modeling Framework. Concerning variability management, the tool offers capabilities for editing feature models and specifying feature configurations, both being well-known formalisms in product line engineering. Furthermore, collaborative editing of product lines is provided through distributed version control. The accompanying video shows that SuperMod seamlessly integrates into existing tool landscapes, reduces the complexity of multi-variant editing, automates a large part of variability management, and ensures consistency. A tool demonstration video is available here: http://youtu.be/5XOk3x5kjFc.
[software product lines, model-driven software product line engineering, collaborative filtering, software product line engineering, variability management, Eclipse modeling framework, filtered editing, Computational modeling, Unified modeling language, version control, Metadata, Control systems, SuperMod tool support, Software product lines, check-out-modify-commit workflow, distributed version control, Collaboration, Software, Model-driven software engineering]
AnModeler: A tool for generating domain models from textual specifications
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
This paper presents AnModeler, a tool for generating analysis models from software requirements specified using use cases. The tool uses the Stanford natural language parser to extract type dependencies (TDs) and parts of speech tags (POS-tags) of sentences from input Use Case Specification (UCS). Then, it identifies sentence structures using a set of rules framed based on Hornby's verb patterns. With the information of the TDs, POS tags, and the identified sentence structures, the tool discovers domain elements, viz.: domain objects (including their attributes and operations) and interactions between them; it consolidates the domain information as a class diagram (as well as a sequence diagram). An experiment conducted on 10 UCSs with two industry experts as subjects showed that the analysis class diagrams generated by AnModeler were remarkably close to those generated by the two industry experts. Being lightweight and easy to use, the tool can also be used to assist students and young developers in acquiring object-oriented domain modeling skills quickly. Link to a short demonstration video: https://youtu.be/_Ct-qF4Y1fU.
[Industries, Visualization, text analysis, UCS, Unified modeling language, Problem level sequence diagram, identified sentence structures, TD extraction, domain model generating tool, diagrams, parts of speech tags, Analysis class diagram, software requirements, Analytical models, analysis class diagrams generated, analysis model generation, Template code, software engineering, object-oriented methods, textual specifications, sequence diagram, Java, domain information, Automated approach, Object oriented modeling, type dependency extraction, Model transformation, POS-tags, use case specification, domain elements, AnModeler, grammars, Automated tool for analysis modeling, natural languages, Stanford natural language parser, Software, object-oriented domain modeling, Hornby verb patterns]
SimilarTech: Automatically recommend analogical libraries across different programming languages
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Third-party libraries are an integral part of many software projects. It often happens that developers need to find analogical libraries that can provide comparable features to the libraries they are already familiar with. Existing methods to find analogical libraries are limited by the community-curated list of libraries, blogs, or Q&amp;A posts, which often contain overwhelming or out-of-date information. This paper presents our tool SimilarTech (https://graphofknowledge. appspot.com/similartech) that makes it possible to automatically recommend analogical libraries by incorporating tag embeddings and domain-specific relational and categorical knowledge mined from Stack Overflow. SimilarTech currently supports recommendation of 6,715 libraries across 6 different programming languages. We release our SimilarTech website for public use. The SimilarTech website attracts more than 2,400 users in the past 6 months. We observe two typical usage patterns of our website in the website visit logs which can satisfy different information needs of developers. The demo video can be seen at https://youtu.be/ubx8h4D4ieE.
[Java, analogical libraries, third-party libraries, Knowledge based systems, Mobile communication, programming languages, software libraries, tag embeddings, Operating systems, Tagging, categorical knowledge, Libraries, software projects, domain-specific relational knowledge, SimilarTech tool]
TeeVML: Tool support for semi-automatic integration testing environment emulation
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Software environment emulation provides a means for simulating an operational environment of a system. This process involves approximation of systems' external behaviors and their communications with a system to be tested in the environment. Development of such an environment is a tedious task and involves complex low level coding. Model driven engineering is an avenue to raise the level of abstraction beyond programming by specifying solution directly using problem domain concepts. In this paper we propose a novel domain-specific modeling tool to generate complex testing environments. Our tool employs a suite of domain-specific visual modeling languages for modeling emulation environment at a high level of abstraction. These high level specifications are then automatically transformed to runtime environment for application integration testing, boosting development productivity and ease of use. The tool demonstration video can be accessed here: https://youtu.be/H3Vg20Juq80.
[Model-driven engineering, domain-specific visual modeling language, Visualization, Protocols, program testing, testing environment emulation, Unified modeling language, Banking, high level specification, formal specification, tool support, domain-specific visual modelling language, application integration testing, TeeVML, Emulation, model-driven engineering, specification languages, software component interface description, Software, software environment emulation, software tools, Testing]
The interactive verification debugger: Effective understanding of interactive proof attempts
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
The Symbolic Execution Debugger (SED) is an extension of the Eclipse debug platform for interactive symbolic execution. Like a traditional debugger, the SED can be used to locate the origin of a defect and to increase program understanding. However, as it is based on symbolic execution, all execution paths are explored simultaneously. We demonstrate an extension of the SED called Interactive Verification Debugger (IVD) for inspection and understanding of formal verification attempts. By a number of novel views, the IVD allows to quickly comprehend interactive proof situations and to debug the reasons for a proof attempt that got stuck. It is possible to perform interactive proofs completely from within the IVD. It can be experimentally demonstrated that the IVD is more effective in understanding proof attempts than a conventional prover user interface. A screencast explaining proof attempt inspection with the IVD is available at youtu.be/8e-q9Jf1h_w.
[Java, Visualization, program debugging, interactive proof attempt, Symbolic Execution, program verification, symbolic execution debugger, Debugging, Inspection, Verification, user interfaces, Indexes, formal verification attempt, Eclipse debug platform, interactive verification debugger, SED, Proof Understanding, IVD, User interfaces, interactive systems, Software, theorem proving, interactive symbolic execution, Program Execution Visualization]
Verifying Simulink Stateflow model: Timed automata approach
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Simulink Stateflow is widely used for the model-driven development of software. However, the increasing demand of rigorous verification for safety critical applications brings new challenge to the Simulink Stateflow because of the lack of formal semantics. In this paper, we present STU, a self-contained toolkit to bridge the Simulink Stateflow and a well-defined rigorous verification. The tool translates the Simulink Stateflow into the Uppaal timed automata for verification. Compared to existing work, more advanced and complex modeling features in Stateflow such as the event stack, conditional action and timer are supported. Then, with the strong verification power of Uppaal, we can not only find design defects that are missed by the Simulink Design Verifier, but also check more important temporal properties. The evaluation on artificial examples and real industrial applications demonstrates the effectiveness.
[Uppaal timed automata, program verification, automata theory, Switches, Verification, Synchronization, model-driven development, Simulink Stateflow model verification, Software packages, Uppaal Timed Automaton, Semantics, Automata, software tools, STU toolkit, Junctions, Simulink Stateflow, Clocks]
GUICat: GUI testing as a service
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
GUIs are event-driven applications where the flow of the program is determined by user actions such as mouse clicks and key presses. GUI testing is a challenging task not only because of the combinatorial explosion in the number of event sequences, but also because of the difficulty to cover the large number of data values. We propose GUICat, the first cloud-based GUI testing framework that simultaneously generates event sequences and data values. It is a white-box GUI testing tool that augments traditional sequence generation techniques with concolic execution. We also propose a cloud-based parallel algorithm for mitigating both event sequence explosion and data value explosion, by distributing the con-colic execution tasks over public clouds such as Amazon EC2. We have evaluated the tool on standard GUI testing benchmarks and showed that GUICat significantly outperforms state-of-the-art GUI testing tools. The video demo URL is https://youtu.be/rfnnQOmZqj4.
[Cloud computing, Java, parallel algorithms, program testing, GUICat, Instruments, Computational modeling, graphical user interfaces, Explosions, combinatorial explosion, event sequences, concolic execution, Test generation, data values, cloud-based parallel algorithm, Cloud, cloud-based GUI testing framework, GUI testing, cloud computing, sequence generation, Symbolic execution, con-colic execution tasks, Graphical user interfaces, Testing]
An automated collaborative requirements engineering tool for better validation of requirements
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
This demo introduces an automated collaborative requirements engineering tool, called TestMEReq, which is used to promote effective communication and collaboration between client-stakeholders and requirements engineers for better requirements validation. Our tool is augmented with real time communication and collaboration support to allow multiple stakeholders to collaboratively validate the same set of requirements. We have conducted a user study focusing on validating requirements using TestMEReq with a few groups of requirements engineers and client stakeholders. The study shows that our automated tool support is able to assist requirements engineers to effectively communicate with client-stakeholders to better validate the requirements virtually in real time. (Demo video: https://www.youtube.com/watch?v=7sWLOx-N4Jo).
[program testing, Essential Use Cases, formal specification, formal verification, TestMEReq, Collaboration, Prototypes, groupware, User interfaces, Abstract test, Software, Libraries, Stakeholders, Essential User Interface, requirements validation, requirement-based testing, communication and collaboration, Testing, automated collaborative requirement engineering tool]
An extensible framework for variable-precision data-flow analyses in MPS
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Data-flow analyses are used as part of many software engineering tasks: they are the foundations of program understanding, refactorings and optimized code generation. Similar to general-purpose languages (GPLs), state-of-the-art domain-specific languages (DSLs) also require sophisticated data-flow analyses. However, as a consequence of the different economies of DSL development and their typically relatively fast evolution, the effort for developing and evolving such analyses must be lowered compared to GPLs. This tension can be resolved with dedicated support for data-flow analyses in language workbenches. In this tool paper we present MPS-DF, which is the component in the MPS language workbench that supports the definition of data-flow analyses for DSLs. Language developers can define data-flow graph builders declaratively as part of a language definition and compute analysis results efficiently based on these data-flow graphs. MPS-DF is extensible such that it does not compromise the support for language composition in MPS. Additionally, clients of MPS-DF analyses can run the analyses with variable precision thus trading off precision for performance. This allows clients to tailor an analysis to a particular use case.
[Algorithm design and analysis, GPL, extensible framework, Lattices, Switches, data flow graphs, general-purpose language, Data-flow Analysis, Domain-specific Language, program compilers, Inter-procedural Analysis, specification languages, software engineering, data flow graph, DSL, software refactoring, data flow analysis, MPS-DF analysis, Encoding, software maintenance, Language Workbench, code generation, domain-specific language, Syntactics, Software]
Towards efficient and effective automatic program repair
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Automatic Program Repair (APR) has recently been an emerging research area, addressing an important challenge in software engineering. APR techniques, if effective and efficient, can greatly help software debugging and maintenance. Recently proposed APR techniques can be generally classified into two families, namely search-based and semantics-based APR methods. To produce repairs, search based APR techniques generate huge populations of possible repairs, i.e., search space, and lazily search for the best one among the search space. Semantics-based APR techniques utilize constraint solving and program synthesis to make search space more tractable, and find those repairs that conform to semantics constraints extracted via symbolic execution. Despite recent advances in APR, search-based APR still suffers from search space explosion problem, while the semantics-based APR could be hindered by limited capability of constraint solving and program synthesis. Furthermore, both APR families may be subject to overfitting, in which generated repairs do not generalize to other test sets. This thesis works towards enhancing both effectiveness and efficiency in order for APR to be practically adopted in foreseeable future. To achieve this goal, other than using test cases as the primary criteria for traversing the search space, we designed a new feature used for a new search-based APR technique to effectively traverse the search space, wherein bug fix history is used to evaluate the quality of repair candidates. We also developed a deductive-reasoning-based repair technique that combines search-based and semantics-based approaches to enhance the repair capability, while ensuring the soundness of generated repairs. We also leveraged machine-learning techniques to build a predictive model that predicts whether an APR technique is effective in fixing particular bugs. In the future, we plan to synergize many existing APR techniques, improve our predictive model, and adopt the advances of other fields such as test case generation and program synthesis for APR.
[program debugging, software debugging, Search problems, bug fixing, History, semantic constraint extraction, Semantics, software engineering, semantics-based APR method, learning (artificial intelligence), Genetic Programming, search-based APR method, Automatic Program Repair, Maintenance engineering, Mining Software Repository, inference mechanisms, software maintenance, machine learning, deductive reasoning, Deductive Reasoning, Computer bugs, semantic networks, Syntactics, symbolic execution, Software, automatic program repair]
Automated testing and notification of mobile app privacy leak-cause behaviours
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
I describe the design, implementation and evaluation of a novel hybrid static/dynamic analysis system for automatically uncovering and testing for the user-triggered causes and paths of privacy leaks in Android applications (privacy `leak-causes'). I describe how I plan to further evaluate and demonstrate improvements in accuracy, coverage and testing speed of my hybrid testing approach against other currently available systems. I also show how user privacy is improved by the presentation of information on leak-causes in a field study as privacy notices. I present plans to investigate which of the commonly utilized mobile notification mechanisms is best suited to the presentation of leak-causes, as well as how users may set better privacy control policies with the information provided.
[Data privacy, program testing, program diagnostics, Humanoid robots, mobile application, hybrid static-dynamic analysis system, mobile app privacy leak-cause behaviour, Mobile communication, Privacy, mobile computing, user-triggered causes, privacy control policy, user privacy, mobile notification mechanism, Android applications, data privacy, privacy notices, Androids, Smart phones, Testing]
Factoring requirement dependencies in software requirement selection using graphs and integer programming
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Software requirement selection is to find a subset of requirements (so-called optimal set) that gives the highest customer value for a release of software while keeping the cost within the budget. Several industrial studies however, have demonstrated that requirements of software projects are intricately interdependent and these interdependencies impact the values of requirements. Furthermore, the strengths of dependency relations among requirements vary in the context of real-world projects. For instance, requirements can be strongly or weakly interdependent. Therefore, it is important to consider both the existence and the strengths of dependency relations during requirement selection. The existing selection models however, have ignored either requirement dependencies altogether or the strengths of those dependencies. This research proposes an Integer programming model for requirement selection which considers both the existence and strengths of requirement dependencies. We further contribute a graph-based dependency modeling technique for capturing requirement dependencies and the their corresponding strengths. Automated/semi-automated techniques will also be devised to identify requirement dependencies and the strengths of those dependencies.
[integer programming, graph theory, software management, Complexity theory, formal specification, Optimization, graphs, formal verification, graph-based dependency modeling technique, Software Requirement Selection, Requirements engineering, software project requirements, dependency relations, automated techniques, project management, Software Requirement Dependencies, Computational modeling, software selection, budgeted cost, Integer Programming, Linear programming, integer programming model, Graph, systems analysis, requirement dependencies, software requirement selection, Software, Planning, optimal set]
Statistical analysis of large sets of models
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.
[Model-driven engineering, repository management, Merging, information filtering, Analytical models, large dataset analysis, generic approach, model large set, Natural language processing, software engineering, learning (artificial intelligence), data analysis, Biological system modeling, Computational modeling, natural language processing, model comparison, vector space model, information retrieval, model searching scenarios, domain analysis, machine learning, filtering activities, model-driven engineering, Feature extraction, Data models, clustering, statistical analysis]
Developer targeted analytics: Supporting software development decisions with runtime information
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Runtime information of deployed software has been used by business and operations units to make informed decisions under the term &#x201C;analytics&#x201D;. However, decisions made by software engineers in the course of evolving software have, for the most part, been based on personal belief and gut-feeling. This could be attributed to software development being, for the longest time, viewed as an activity that is detached from the notion of operating software in a production environment. In recent years, this view has been challenged with the emergence of the DevOps movement, which aim is to promote cross-functional capabilities of development and operations activities within teams. This shift in process and mindset requires analytics tools that specifically target software developers. In this research, I investigate approaches to support developers in their decision-making by incorporating runtime information in source code and provide live feedback in IDEs by predicting the impact of code changes.
[source code (software), Uncertainty, operation unit, Performance Engineering, Computational modeling, Software Analytics, Decision making, Software performance, source code, software development decisions, performance engineering, runtime information, developer targeted analytics, Runtime, business unit, Developer Targeted Analytics, Production, decision making, software engineering, business data processing, software analytics]
API recommendation system for software development
2016 31st IEEE/ACM International Conference on Automated Software Engineering
None
2016
Nowadays, software developers often utilize existing third party libraries and make use of Application Programming Interface (API) to develop a software. However, it is not always obvious which library to use or whether the chosen library will play well with other libraries in the system. Furthermore, developers need to spend some time to understand the API to the point that they can freely use the API methods and putting the right parameters inside them. In this work, I plan to automatically recommend relevant APIs to developers. This API recommendation can be divided into multiple stages. First, we can recommend relevant libraries provided a given task to complete. Second, we can recommend relevant API methods that developer can use to program the required task. Third, we can recommend correct parameters for a given method according to its context. Last but not least, we can recommend how different API methods can be combined to achieve a given task. In effort to realize this API recommendation system, I have published two related papers. The first one deals with recommending additional relevant API libraries given known useful API libraries for the target program. This system can achieve recall rate@5 of 0.852 and recall rate@10 of 0.894 in recommending additional relevant libraries. The second one deals with recommending relevant API methods a given target API and a textual description of the task. This system can achieve recall-rate@5 of 0.690 and recall-rate@10 of 0.779. The results for both system indicate that the systems are useful and capable in recommending the right API/library reasonably well. Currently, I am working on another system which can recommend web APIs (i.e., libraries) given a description of the task. I am also working on a system that recommends correct parameters given an API method. In the future, I also plan to realize API composition recommendation for the given task.
[Context, application program interfaces, software development, API recommendation system, application programming interface, Recommendation System, recall-rate, History, API libraries, software libraries, Training, recommender systems, Databases, textual description, Feature extraction, Libraries, Software, API, Library]
Message from the chairs
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Organization committee
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.
[]
Silver sponsors
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The conference organizers greatly appreciate the support of the various corporate sponsors listed.
[]
Cobra &#x2014; An interactive static code analyzer
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Sadly we know that virtually all software of any significance has residual errors. Some of those errors can be traced back to requirements flaws or faulty design assumptions; others are just plain coding mistakes. Static analyzers have become quite good at spotting these types of errors, but they don't scale very well. If, for instance, you need to check a code base of a few million lines you better be prepared to wait for the result; sometimes hours. Eyeballing a large code base to find flaws is clearly not an option, so what is missing is a static analysis capability that can be used to answer common types of queries interactively, even for large code bases. I will describe the design and use of such a tool in this talk.
[]
Mining structures from massive text data: Will it help software engineering?
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The real-world big data are largely unstructured, interconnected text data. One of the grand challenges is to turn such massive unstructured text data into structured, actionable knowledge. We propose a text mining approach that requires only distant or minimal supervision but relies on massive text data. We show quality phrases can be mined from such massive text data, types can be extracted from massive text data with distant supervision, and entities/attributes/values can be discovered by meta-path directed pattern discovery. We show text-rich and structure-rich networks can be constructed from massive unstructured data. Finally, we speculate whether such a paradigm could be useful for turning massive software repositories into multi-dimensional structures to help searching and mining software repositories.
[]
Software engineering without borders
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
DevOps approaches software engineering by advocating the removal of borders between development and operations. DevOps emphasizes operational resilience, continuous feedback from operations back to development, and rapid deployment of features developed. In this talk we will look at selected (automation) aspects related to DevOps, based on our collaborations with various industrial partners. For example, we will explore (automated) methods for analyzing log data to support deployments and monitor REST API integrations, (search-based) test input generation for reproducing crashes and testing complex database queries, and zero downtime database schema evolution and deployment. We will close by looking at borders beyond those between development and operations, in order to see whether there are other borders we need to remove in order to strengthen the impact of software engineering research.
[]
Systematically testing background services of mobile apps
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Contrary to popular belief, mobile apps can spend a large fraction of time running "hidden" as background services. And, bugs in services can translate into crashes, energy depletion, device slow-down, etc. Unfortunately, without necessary testing tools, developers can only resort to telemetries from user devices in the wild. To this end, Snowdrop is a testing framework that systematically identifies and automates background services in Android apps. Snowdrop realizes a service-oriented approach that does not assume all inter-component communication messages are explicitly coded in the app bytecode. Furthermore, to improve the completeness of test inputs generated, Snowdrop infers field values by exploiting the similarity in how developers name variables. We evaluate Snowdrop by testing 848 commercially available mobile apps. Empirical results show that Snowdrop can achieve 20.91% more code path coverage than pathwise test input generators, and 64.11% more coverage than random test input generators.
[Android Intents, program testing, Humanoid robots, Snowdrop, Mobile communication, Android apps, Android (operating system), mobile computing, testing tools, app bytecode, code path coverage, App background services, service-oriented architecture, Testing, intercomponent communication messages, test input generation, Tools, pathwise test input generators, Generators, Telemetry, random test input generators, service-oriented approach, user devices, Androids, background services]
Crowd intelligence enhances automated mobile testing
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
We show that information extracted from crowd-based testing can enhance automated mobile testing. We introduce Polariz, which generates replicable test scripts from crowd-based testing, extracting cross-app `motif' events: automatically-inferred reusable higher-level event sequences composed of lower-level observed event actions. Our empirical study used 434 crowd workers from Mechanical Turk to perform 1,350 testing tasks on 9 popular Google Play apps, each with at least 1 million user installs. The findings reveal that the crowd was able to achieve 60.5% unique activity coverage and proved to be complementary to automated search-based testing in 5 out of the 9 subjects studied. Our leave-one-out evaluation demonstrates that coverage attainment can be improved (6 out of 9 cases, with no disimprovement on the remaining 3) by combining crowd-based and search-based testing.
[program testing, Humanoid robots, Mobile communication, Mobile handsets, crowd-based testing, Data mining, Crowdsourced Software Engineering, automatic testing, reusable higher-level event sequences, mobile computing, automated mobile testing, lower-level observed event actions, search-based testing, Test Generation, Testing, crowd workers, testing tasks, replicable test scripts, Tools, Google Play apps, crowd intelligence, Androids, automated search, cross-app motif events, Mobile App Testing]
Sketch-guided GUI test generation for mobile applications
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Mobile applications with complex GUIs are very popular today. However, generating test cases for these applications is often tedious professional work. On the one hand, manually designing and writing elaborate GUI scripts requires expertise. On the other hand, generating GUI scripts with record and playback techniques usually depends on repetitive work that testers need to interact with the application over and over again, because only one path is recorded in an execution. Automatic GUI testing focuses on exploring combinations of GUI events. As the number of combinations is huge, it is still necessary to introduce a test interface for testers to reduce its search space. This paper presents a sketch-guided GUI test generation approach for testing mobile applications, which provides a simple but expressive interface for testers to specify their testing purposes. Testers just need to draw a few simple strokes on the screenshots. Then our approach translates the strokes to a testing model and initiates a model-based automatic GUI testing. We evaluate our sketch-guided approach on a few real-world Android applications collected from the literature. The results show that our approach can achieve higher coverage than existing automatic GUI testing techniques with just 10-minute sketching for an application.
[GUI events, Shape, program testing, graphical user interfaces, automatic GUI testing techniques, test interface, real-world Android applications, Mobile applications, Grammar, playback techniques, Connectors, mobile computing, elaborate GUI scripts, GUI test generation approach, Layout, mobile applications, professional work, Graphical user interfaces, Testing]
Saying &#x2018;Hi!&#x2019; is not enough: Mining inputs for effective test generation
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Automatically generating unit tests is a powerful approach to exercise complex software. Unfortunately, current techniques often fail to provide relevant input values, such as strings that bypass domain-specific sanity checks. As a result, state-of-the-art techniques are effective for generic classes, such as collections, but less successful for domain-specific software. This paper presents TestMiner, the first technique for mining a corpus of existing tests for input values to be used by test generators for effectively testing software not in the corpus. The main idea is to extract literals from thousands of tests and to adapt information retrieval techniques to find values suitable for a particular domain. Evaluating the approach with 40 Java classes from 18 different projects shows that TestMiner improves test coverage by 21% over an existing test generator. The approach can be integrated into various test generators in a straightforward way, increasing their effectiveness on previously difficult-to-test classes.
[program testing, relevant input values, data mining, effective test generation, Data mining, complex software, domain-specific software, state-of-the-art techniques, domain-specific sanity checks, test generators, Java classes, difficult-to-test classes, Testing, Java, unit tests automatic generation, information retrieval, Generators, Indexes, Computer science, TestMiner, information retrieval techniques, test coverage, Software, mining inputs, generic classes]
Learn&amp;Fuzz: Machine learning for input fuzzing
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Fuzzing consists of repeatedly testing an application with modified, or fuzzed, inputs with the goal of finding security vulnerabilities in input-parsing code. In this paper, we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques. We present a detailed case study with a complex input format, namely PDF, and a large complex security-critical parser for this format, namely, the PDF parser embedded in Microsoft's new Edge browser. We discuss and measure the tension between conflicting learning and fuzzing goals: learning wants to capture the structure of well-formed inputs, while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs. We also present a new algorithm for this learn&amp;fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs.
[program debugging, Recurrent neural networks, program testing, input-parsing code, fuzzy set theory, PDF parser, Probability distribution, Grammar Learning, Training, complex security-critical parser, Fuzzing, learning (artificial intelligence), neural-network-based statistical machine-learning techniques, security vulnerabilities, probability, Portable document format, Grammar, machine learning, complex input format, Deep Learning, Grammar-based Fuzzing, input fuzzing, security of data, grammars, Microsoft's new Edge browser, learnt input probability distribution, neural nets, conflicting learning]
The impact of continuous integration on other software development practices: A large-scale empirical study
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Continuous Integration (CI) has become a disruptive innovation in software development: with proper tool support and adoption, positive effects have been demonstrated for pull request throughput and scaling up of project sizes. As any other innovation, adopting CI implies adapting existing practices in order to take full advantage of its potential, and "best practices" to that end have been proposed. Here we study the adaptation and evolution of code writing and submission, issue and pull request closing, and testing practices as TRAVIS CI is adopted by hundreds of established projects on GITHUB. To help essentialize the quantitative results, we also survey a sample of GITHUB developers about their experiences with adopting TRAVIS CI. Our findings suggest a more nuanced picture of how GITHUB teams are adapting to, and benefiting from, continuous integration technology than suggested by prior work.
[Automation, project management, project sizes, pull request throughput, program testing, software development, public domain software, Tools, Open source software, Best practices, GITHUB, software engineering, TRAVIS CI, continuous integration technology, Testing]
Perceived language complexity in GitHub issue discussions and their effect on issue resolution
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Modern software development is increasingly collaborative. Open Source Software (OSS) are the bellwether; they support dynamic teams, with tools for code sharing, communication, and issue tracking. The success of an OSS project is reliant on team communication. E.g., in issue discussions, individuals rely on rhetoric to argue their position, but also maintain technical relevancy. Rhetoric and technical language are on opposite ends of a language complexity spectrum: the former is stylistically natural; the latter is terse and concise. Issue discussions embody this duality, as developers use rhetoric to describe technical issues. The style mix in any discussion can define group culture and affect performance, e.g., issue resolution times may be longer if discussion is imprecise. Using GitHub, we studied issue discussions to understand whether project-specific language differences exist, and to what extent users conform to a language norm. We built project-specific and overall GitHub language models to study the effect of perceived language complexity on multiple responses. We find that experienced users conform to project-specific language norms, popular individuals use overall GitHub language rather than project-specific language, and conformance to project-specific language norms reduces issue resolution times. We also provide a tool to calculate project-specific perceived language complexity.
[technical issues, public domain software, team communication, Complexity theory, issue resolution times, code sharing, Open Source Software, Employment, specification languages, technical language, Rhetoric, project-specific perceived language complexity, GitHub issue discussions, GitHub language, OSS project, project-specific language differences, rhetoric language, modern software development, software maintenance, language complexity spectrum, Standards, Pragmatics, issue tracking, language norm, Speech, Software, technical relevancy, Internet]
Can automated pull requests encourage software developers to upgrade out-of-date dependencies?
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Developers neglect to update legacy software dependencies, resulting in buggy and insecure software. One explanation for this neglect is the difficulty of constantly checking for the availability of new software updates, verifying their safety, and addressing any migration efforts needed when upgrading a dependency. Emerging tools attempt to address this problem by introducing automated pull requests and project badges to inform the developer of stale dependencies. To understand whether these tools actually help developers, we analyzed 7,470 GitHub projects that used these notification mechanisms to identify any change in upgrade behavior. Our results find that, on average, projects that use pull request notifications upgraded 1.6&#x00D7; as often as projects that did not use any tools. Badge notifications were slightly less effective: users upgraded 1.4&#x00D7; more frequently. Unfortunately, although pull request notifications are useful, developers are often overwhelmed by notifications: only a third of pull requests were actually merged. Through a survey, 62 developers indicated that their most significant concerns are breaking changes, understanding the implications of changes, and migration effort. The implications of our work suggests ways in which notifications can be improved to better align with developers' expectations and the need for new mechanisms to reduce notification fatigue and improve confidence in automated pull requests.
[Google, project management, insecure software, migration effort, Tools, legacy software dependencies, software developers, Security, software maintenance, automated pull requests, project badges, notification fatigue, stale dependencies, upgrade behavior, software updates, Software, Libraries, software engineering, Safety, developers neglect, out-of-date dependencies, notification mechanisms]
Are developers aware of the architectural impact of their changes?
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Although considered one of the most important decisions in a software development lifecycle, empirical evidence on how developers perform and perceive architectural changes is still scarce. Given the large implications of architectural decisions, we do not know whether developers are aware of their changes' impact on the software's architecture, whether awareness leads to better changes, and whether automatically making developers aware would prevent degradation. Therefore, we use code review data of 4 open source systems to investigate the intent and awareness of developers when performing changes. We extracted 8,900 reviews for which the commits are available. 2,152 of the commits have changes in their computed architectural metrics, and 338 present significant changes to the architecture. We manually inspected all reviews for commits with significant changes and found that only in 38% of the time developers are discussing the impact of their changes on the architectural structure, suggesting a lack of awareness. Finally, we observed that developers tend to be more aware of the architectural impact of their changes when the architectural structure is improved, suggesting that developers should be automatically made aware when their changes degrade the architectural structure.
[Measurement, computed architectural metrics, Java, architectural decisions, Software Architecture, software quality, History, software maintenance, Code Reviews, Couplings, Degradation, architectural structure, architectural impact, software architecture, software development lifecycle, architectural changes, Computer architecture, Software systems, software engineering, time developers, software metrics]
SentiCR: A customized sentiment analysis tool for code review interactions
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Sentiment Analysis tools, developed for analyzing social media text or product reviews, work poorly on a Software Engineering (SE) dataset. Since prior studies have found developers expressing sentiments during various SE activities, there is a need for a customized sentiment analysis tool for the SE domain. On this goal, we manually labeled 2000 review comments to build a training dataset and used our dataset to evaluate seven popular sentiment analysis tools. The poor performances of the existing sentiment analysis tools motivated us to build SentiCR, a sentiment analysis tool especially designed for code review comments. We evaluated SentiCR using one hundred 10-fold cross-validations of eight supervised learning algorithms. We found a model, trained using the Gradient Boosting Tree (GBT) algorithm, providing the highest mean accuracy (83%), the highest mean precision (67.8%), and the highest mean recall (58.4%) in identifying negative review comments.
[Algorithm design and analysis, negative review comments, Sentiment analysis, pattern classification, text analysis, Dictionaries, SentiCR, Social network services, natural language processing, GBT algorithm, code review interactions, customized sentiment analysis tool, trees (mathematics), code review comments, Tools, social media text, SE domain, Training, gradient boosting tree algorithm, Supervised learning, SE activities, software engineering dataset, social networking (online), learning (artificial intelligence)]
Detecting fragile comments
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Refactoring is a common software development practice and many simple refactorings can be performed automatically by tools. Identifier renaming is a widely performed refactoring activity. With tool support, rename refactorings can rely on the program structure to ensure correctness of the code transformation. Unfortunately, the textual references to the renamed identifier present in the unstructured comment text cannot be formally detected through the syntax of the language, and are thus fragile with respect to identifier renaming. We designed a new rule-based approach to detect fragile comments. Our approach, called Fraco, takes into account the type of identifier, its morphology, the scope of the identifier and the location of comments. We evaluated the approach by comparing its precision and recall against hand-annotated benchmarks created for six target Java systems, and compared the results against the performance of Eclipse's automated in-comment identifier replacement feature. Fraco performed with near-optimal precision and recall on most components of our evaluation data set, and generally outperformed the baseline Eclipse feature. As part of our evaluation, we also noted that more than half of the total number of identifiers in our data set had fragile comments after renaming, which further motivates the need for research on automatic comment refactoring.
[Java, text analysis, object-oriented programming, Tools, fragile comments, software maintenance, program structure, renamed identifier present, Fraco, automatic comment refactoring, software development practice, Software evolution, refactoring, Semantics, unstructured comment text, Morphology, Benchmark testing, Syntactics, in-comment identifier replacement feature, rename refactorings, inconsistent code, source code comments]
Improving software text retrieval using conceptual knowledge in source code
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
A large software project usually has lots of various textual learning resources about its API, such as tutorials, mailing lists, user forums, etc. Text retrieval technology allows developers to search these API learning resources for related documents using free-text queries, but it suffers from the lexical gap between search queries and documents. In this paper, we propose a novel approach for improving the retrieval of API learning resources through leveraging software-specific conceptual knowledge in software source code. The basic idea behind this approach is that the semantic relatedness between queries and documents could be measured according to software-specific concepts involved in them, and software source code contains a large amount of software-specific conceptual knowledge. In detail, firstly we extract an API graph from software source code and use it as software-specific conceptual knowledge. Then we discover API entities involved in queries and documents, and infer semantic document relatedness through analyzing structural relationships between these API entities. We evaluate our approach in three popular open source software projects. Comparing to the state-of-the-art text retrieval approaches, our approach lead to at least 13.77% improvement with respect to mean average precision (MAP).
[text analysis, related documents, application program interfaces, public domain software, API graph, conceptual knowledge, Open source software, software-specific conceptual knowledge, software text retrieval, API learning resources, semantic relatedness, Semantics, textual learning resources, learning (artificial intelligence), mean average precision, API entities, Software text retrieval, free-text queries, Knowledge based systems, software source code, popular open source software projects, Tutorials, information retrieval, semantic document relatedness, software-specific concepts, software project, search queries, Internet, MAP, text retrieval technology, Software engineering]
Automatically generating commit messages from diffs using neural machine translation
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically "translate" diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.
[Algorithm design and analysis, program debugging, public domain software, Natural languages, Software algorithms, software maintenance, software evolution, configuration management, automatic commit message generation, NMT, Computer bugs, Machine learning, Prediction algorithms, Software, diffs, neural machine translation, language translation]
Improving missing issue-commit link recovery using positive and unlabeled data
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Links between issue reports and corresponding fix commits are widely used in software maintenance. The quality of links directly affects maintenance costs. Currently, such links are mainly maintained by error-prone manual efforts, which may result in missing links. To tackle this problem, automatic link recovery approaches have been proposed by building traditional classifiers with positive and negative links. However, these traditional classifiers may not perform well due to the inherent characteristics of missing links. Positive links, which can be used to build link recovery model, are quite limited as the result of missing links. Since the construction of negative links depends on the number of positive links in many existing approaches, the available negative links also become restricted. In this paper, we point out that it is better to consider the missing link problem as a model learning problem by using positive and unlabeled data, rather than the construction of traditional classifier. We propose PULink, an approach that constructs the link recovery model with positive and unlabeled links. Our experiment results show that compared to existing state-of-the-art technologies built on traditional classifier, PULink can achieve competitive performance by utilizing only 70% positive links that are used in those approaches.
[Software maintenance, pattern classification, unlabeled link data, Metadata, link recovery model, Indexes, software maintenance, improving missing issue-commit link recovery, positive data, missing links, Training, traditional classifier, Feature extraction, automatic link recovery approaches, learning (artificial intelligence)]
APIBot: Question answering bot for API documentation
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
As the carrier of Application Programming Interfaces (APIs) knowledge, API documentation plays a crucial role in how developers learn and use an API. It is also a valuable information resource for answering API-related questions, especially when developers cannot find reliable answers to their questions online/offline. However, finding answers to API-related questions from API documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page, and then read and understand the information inside the relevant page to figure out the answers. To deal with this challenge, we develop APIBot, a bot that can answer API questions given API documentation as an input. APIBot is built on top of SiriusQA, the QA system from Sirius, a state of the art intelligent personal assistant. To make SiriusQA work well under software engineering scenario, we make several modifications over SiriusQA by injecting domain specific knowledge. We evaluate APIBot on 92 API questions, answers of which are known to be present in Java 8 documentation. Our experiment shows that APIBot can achieve a Hit@5 score of 0.706.
[application program interfaces, software engineering scenario, API-related question answering, question answering bot, application programming interface knowledge, API Documentation, API documentation, Training, Knowledge discovery, APIBot, software engineering, Hit@5 score, domain specific knowledge, Java, intelligent personal assistant, Natural languages, Bot, Documentation, Question Answering, Probabilistic logic, SiriusQA, Java 8 documentation, Software, question answering (information retrieval), Software engineering]
Automatic summarization of API reviews
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
With the proliferation of online developer forums as informal documentation, developers often share their opinions about the APIs they use. However, given the plethora of opinions available for an API in various online developer forums, it can be challenging for a developer to make informed decisions about the APIs. While automatic summarization of opinions have been explored for other domains (e.g., cameras, cars), we found little research that investigates the benefits of summaries of public API reviews. In this paper, we present two algorithms (statistical and aspect-based) to summarize opinions about APIs. To investigate the usefulness of the techniques, we developed, Opiner, an online opinion summarization engine that presents summaries of opinions using both our proposed techniques and existing six off-the-shelf techniques. We investigated the usefulness of Opiner using two case studies, both involving professional software engineers. We found that developers were interested to use our proposed summaries much more frequently than other summaries (daily vs once a year) and that while combined with Stack Overflow, Opiner helped developers to make the right decision with more accuracy and confidence and in less time.
[summary quality, Java, text analysis, study, application program interfaces, Heuristic algorithms, API informal documentation, informal documentation, public API reviews, Engines, opinion summaries, informed decisions, time 11.0 year, online opinion summarization engine, online developer forums, Cameras, Opiner, Software, software engineering, Internet, automatic summarization, Opinion mining, Message systems]
ICoq: Regression proof selection for large-scale verification projects
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Proof assistants such as Coq are used to construct and check formal proofs in many large-scale verification projects. As proofs grow in number and size, the need for tool support to quickly find failing proofs after revising a project increases. We present a technique for large-scale regression proof selection, suitable for use in continuous integration services, e.g., Travis CI. We instantiate the technique in a tool dubbed ICOQ. ICOQ tracks fine-grained dependencies between Coq definitions, propositions, and proofs, and only checks those proofs affected by changes between two revisions. ICOQ additionally saves time by ignoring changes with no impact on semantics. We applied ICOQ to track dependencies across many revisions in several large Coq projects and measured the time savings compared to proof checking from scratch and when using Coq's timestamp-based toolchain for incremental checking. Our results show that proof checking with ICOQ is up to 10 times faster than the former and up to 3 times faster than the latter.
[Java, project management, large-scale verification projects, proof assistants, program testing, program verification, formal proofs, regression analysis, Tools, tool dubbed ICOQ, Time measurement, proof checking, Coq projects, Standards, Semantics, incremental checking, Software, theorem proving, large-scale regression proof selection, ICOQ additionally]
More effective interpolations in software model checking
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
An approach to CEGAR-based model checking which has proved to be successful on large models employs Craig interpolation to efficiently construct parsimonious abstractions. Following this design, we introduce new applications, universal safety interpolant and existential error interpolant, of Craig interpolation that can systematically reduce the program state space to be explored for safety verification. Whenever the universal safety interpolant is implied by the current path, all paths emanating from that location are guaranteed to be safe. Dually whenever the existential error interpolant is implied by the current path, there is guaranteed to be an unsafe path from the location. We show how these interpolants are computed and applied in safety verification. We have implemented our approach in a tool named InterpChecker by building on an open source software model checker. Experiments on a large number of benchmark programs show that both the interpolations and the auxiliary optimization strategies are effective in improving scalability of software model checking.
[open source software model checking, InterpChecker, program verification, Craig interpolation, program diagnostics, public domain software, Subspace constraints, Tools, CEGAR-based model checking, universal safety interpolant, unsafe path, Interpolation, interpolation, program state space, formal verification, Model checking, Software, Safety, Space exploration, theorem proving, existential error interpolant, safety verification]
Proof-based coverage metrics for formal verification
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
When using formal verification on critical software, an important question involves whether we have we specified enough properties for a given implementation model. To address this question, coverage metrics for property-based formal verification have been proposed. Existing metrics are usually based on mutation, where the implementation model is repeatedly modified and re-analyzed to determine whether mutant models are "killed" by the property set. These metrics tend to be very expensive to compute, as they involve many additional verification problems. This paper proposes an alternate family of metrics that can be computed using the recently introduced idea of Inductive Validity Cores (IVCs). IVCs determine a minimal set of model elements necessary to establish a proof. One of the proposed metrics is both rigorous and substantially cheaper to compute than mutation-based metrics. In addition, unlike the mutation-based techniques, the design elements marked as necessary by the metric are guaranteed to preserve provability. We demonstrate the metrics on a large corpus of examples.
[Measurement, critical software, proof-based coverage metrics, requirements completeness, Computational modeling, inductive proofs, inductive validity cores, Coverage, Analytical models, mutation-based techniques, formal verification, IVC, Software, Safety, Mathematical model, mutant models, Testing, additional verification problems]
Model checker execution reports
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Software model checking constitutes an undecidable problem and, as such, even an ideal tool will in some cases fail to give a conclusive answer. In practice, software model checkers fail often and usually do not provide any information on what was effectively checked. The purpose of this work is to provide a conceptual framing to extend software model checkers in a way that allows users to access information about incomplete checks. We characterize the information that model checkers themselves can provide, in terms of analyzed traces, i.e. sequences of statements, and safe canes, and present the notion of execution reports (ERs), which we also formalize. We instantiate these concepts for a family of techniques based on Abstract Reachability Trees and implement the approach using the software model checker CPAchecker. We evaluate our approach empirically and provide examples to illustrate the ERs produced and the information that can be extracted.
[reachability analysis, program verification, incomplete checks, Computational modeling, Atmospheric modeling, trees (mathematics), Tools, software model checking, formal specification, abstract reachability trees, Model checking, Software, model checker execution reports, CPAchecker software model checker, Safety, Erbium]
Modular verification of interrupt-driven software
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Interrupts have been widely used in safety-critical computer systems to handle outside stimuli and interact with the hardware, but reasoning about interrupt-driven software remains a difficult task. Although a number of static verification techniques have been proposed for interrupt-driven software, they often rely on constructing a monolithic verification model. Furthermore, they do not precisely capture the complete execution semantics of interrupts such as nested invocations of interrupt handlers. To overcome these limitations, we propose an abstract interpretation framework for static verification of interrupt-driven software that first analyzes each interrupt handler in isolation as if it were a sequential program, and then propagates the result to other interrupt handlers. This iterative process continues until results from all interrupt handlers reach a fixed point. Since our method never constructs the global model, it avoids the up-front blowup in model construction that hampers existing, non-modular, verification techniques. We have evaluated our method on 35 interrupt-driven applications with a total of 22,541 lines of code. Our results show the method is able to quickly and more accurately analyze the behavior of interrupts.
[static verification techniques, program verification, Instruction sets, program diagnostics, Tools, interrupt handlers, interrupt-driven applications, formal verification, Computer bugs, Semantics, safety-critical computer systems, Model checking, interrupts, interrupt-driven software]
BProVe: A formal verification framework for business process models
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Business Process Modelling has acquired increasing relevance in software development. Available notations, such as BPMN, permit to describe activities of complex organisations. On the one hand, this shortens the communication gap between domain experts and IT specialists. On the other hand, this permits to clarify the characteristics of software systems introduced to provide automatic support for such activities. Nevertheless, the lack of formal semantics hinders the automatic verification of relevant properties. This paper presents a novel verification framework for BPMN 2.0, called BProVe. It is based on an operational semantics, implemented using MAUDE, devised to make the verification general and effective. A complete tool chain, based on the Eclipse modelling environment, allows for rigorous modelling and analysis of Business Processes. The approach has been validated using more than one thousand models available on a publicly accessible repository. Besides showing the performance of BProVe, this validation demonstrates its practical benefits in identifying correctness issues in real models.
[Business Process Modelling, program verification, formal verification framework, operational semantics, rigorous modelling analysis, software systems, novel verification framework, available notations, formal specification, Software Verification, Analytical models, formal semantics, Semantics, Business Processes, business process models, MAUDE, automatic support, Eclipse modelling environment, Business, software development, Tools, relevant properties, BPMN 2, Structural Operational Semantics, BPMN, domain experts, Collaboration, automatic verification, Logic gates, Software systems, business data processing]
Static detection of asymptotic resource side-channel vulnerabilities in web applications
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Web applications can leak confidential user information due to the presence of unintended side-channel vulnerabilities in code. One particularly subtle class of side-channel vulnerabilities arises due to resource usage imbalances along different execution paths of a program. Such side-channel vulnerabilities are especially severe if the resource usage imbalance is asymptotic. This paper formalizes the notion of asymptotic resource side-channels and presents a lightweight static analysis algorithm for automatically detecting them. Based on these ideas, we have developed a tool called SCANNER that detects resource-related side-channel vulnerabilities in PHP applications. SCANNER has found 18 zero-day security vulnerabilities in 10 different web applications and reports only 2 false positives. The vulnerabilities uncovered by SCANNER can be exploited using cross-site search attacks to extract various kinds of confidential information, such as a user's medications or purchase history.
[Algorithm design and analysis, confidential user information, program diagnostics, zero-day security vulnerabilities, Tools, Web applications, cryptography, execution paths, Security, SCANNER, static detection, unintended side-channel vulnerabilities, Databases, resource usage imbalance, asymptotic resource side-channel vulnerabilities, lightweight static analysis algorithm, Timing, Internet, asymptotic resource side-channels, Time factors, PHP applications]
PAD: Programming third-party web advertisement censorship
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
In the current online advertisement delivery, an ad slot on a publisher's website may go through multiple layers of bidding and reselling until the final ad content is delivered. The publishers have little control on the ads being displayed on their web pages. As a result, website visitors may suffer from unwanted ads such as malvertising, intrusive ads, and information disclosure ads. Unfortunately, the visitors often blame the publisher for their unpleasant experience and switch to competitor websites. In this paper, we propose a novel programming support system for ad delivery, called PAD, for publisher programmers, who specify their policies on regulating third-party ads shown on their websites. PAD features an expressive specification language and a novel persistent policy enforcement runtime that can self-install and self-protect throughout the entire ad delegation chain. It also provides an ad-specific memory protection scheme that prevents malvertising by corrupting malicious payloads. Our experiments show that PAD has negligible runtime overhead. It effectively suppresses a set of malvertising cases and unwanted ad behaviors reported in the real world, without affecting normal functionalities and regular ads.
[ad-specific memory protection scheme, third-party ads, expressive specification language, bidding, malvertising cases, unwanted ad behaviors, unpleasant experience, Runtime, persistent policy enforcement runtime, Trojan horses, Advertising, unwanted ads, programming support system, advertising data processing, current online advertisement delivery, programming third-party Web advertisement censorship, Website visitors, Browsers, publisher programmers, information disclosure ads, competitor Websites, PAD, security of data, Web pages, intrusive ads, Internet, negligible runtime overhead, Web sites]
All about activity injection: Threats, semantics, and detection
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Android supports seamless user experience by maintaining activities from different apps in the same activity stack. While such close inter-app communication is essential in the Android framework, the powerful inter-app communication contains vulnerabilities that can inject malicious activities into a victim app's activity stack to hijack user interaction flows. In this paper, we demonstrate activity injection attacks with a simple malware, and formally specify the activity activation mechanism using operational semantics. Based on the operational semantics, we develop a static analysis tool, which analyzes Android apps to detect activity injection attacks. Our tool is fast enough to analyze real-world Android apps in 6 seconds on average, and our experiments found that 1,761 apps out of 129,756 real-world Android apps inject their activities into other apps' tasks.
[invasive software, real-world Android application, program diagnostics, operational semantics, Humanoid robots, activity activation mechanism, interapplication communication, user interaction, Electronic mail, Android framework, activity injection attacks, Android (operating system), mobile computing, Semantics, malicious activities, Malware, victim application activity stack, Androids, Smart phones]
Detecting information flow by mutating input data
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Analyzing information flow is central in assessing the security of applications. However, static and dynamic analyses of information flow are easily challenged by non-available or obscure code. We present a lightweight mutation-based analysis that systematically mutates dynamic values returned by sensitive sources to assess whether the mutation changes the values passed to sensitive sinks. If so, we found a flow between source and sink. In contrast to existing techniques, mutation-based flow analysis does not attempt to identify the specific path of the flow and is thus resilient to obfuscation. In its evaluation, our MUTAFLOW prototype for Android programs showed that mutation-based flow analysis is a lightweight yet effective complement to existing tools. Compared to the popular FlowDroid static analysis tool, MutaFlow requires less than 10% of source code lines but has similar accuracy; on 20 tested real-world apps, it is able to detect 75 flows that FlowDroid misses.
[Java, systematically mutate dynamic values, source coding, Instruments, program diagnostics, sensitive sources, sensitive sinks, data flow analysis, Tools, MUTAFLOW prototype, lightweight yet effective complement, obscure code, Android (operating system), security of data, static analyses, FlowDroid static analysis tool, Prototypes, dynamic analyses, Concrete, lightweight mutation, source code lines, information flow analysis, Smart phones, flow analysis]
Automatically assessing crashes from heap overflows
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Heap overflow is one of the most widely exploited vulnerabilities, with a large number of heap overflow instances reported every year. It is important to decide whether a crash caused by heap overflow can be turned into an exploit. Efficient and effective assessment of exploitability of crashes facilitates to identify severe vulnerabilities and thus prioritize resources. In this paper, we propose the first metrics to assess heap overflow crashes based on both the attack aspect and the feasibility aspect. We further present HCSIFTER, a novel solution to automatically assess the exploitability of heap overflow instances under our metrics. Given a heap-based crash, HCSIFTER accurately detects heap overflows through dynamic execution without any source code or debugging information. Then it uses several novel methods to extract program execution information needed to quantify the severity of the heap overflow using our metrics. We have implemented a prototype HCSIFTER and applied it to assess nine programs with heap overflow vulnerabilities. HCSIFTER successfully reports that five heap overflow vulnerabilities are highly exploitable and two overflow vulnerabilities are unlikely exploitable. It also gave quantitatively assessments for other two programs. On average, it only takes about two minutes to assess one heap overflow crash. The evaluation result demonstrates both effectiveness and efficiency of HC Sifter.
[Measurement, program debugging, program diagnostics, widely exploited vulnerabilities, Tools, Computer crashes, Indexes, Data mining, heap overflow vulnerabilities, Heap overflow, heap overflow crash, HCSIFTER, security of data, Layout, HC Sifter, crashes facilitates, metrics, Memory error, Vulnerability assessment, Payloads]
Learning to share: Engineering adaptive decision-support for online social networks
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Some online social networks (OSNs) allow users to define friendship-groups as reusable shortcuts for sharing information with multiple contacts. Posting exclusively to a friendship-group gives some privacy control, while supporting communication with (and within) this group. However, recipients of such posts may want to reuse content for their own social advantage, and can bypass existing controls by copy-pasting into a new post; this cross-posting poses privacy risks. This paper presents a learning to share approach that enables the incorporation of more nuanced privacy controls into OSNs. Specifically, we propose a reusable, adaptive software architecture that uses rigorous runtime analysis to help OSN users to make informed decisions about suitable audiences for their posts. This is achieved by supporting dynamic formation of recipient-groups that benefit social interactions while reducing privacy risks. We exemplify the use of our approach in the context of Facebook.
[engineering adaptive decision-support, Adaptation models, privacy control, Computational modeling, recipient-groups, online social networks, social interactions, social advantage, reusable shortcuts, Privacy, Sensitivity, OSN users, informed decisions, reusable software architecture, adaptive software architecture, social networking (online), cross-posting pose privacy risks, data privacy, Facebook, Monitoring, friendship-group, nuanced privacy controls]
UI driven Android application reduction
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
While smartphones and mobile apps have been an integral part of our life, modern mobile apps tend to contain a lot of rarely used functionalities. For example, applications contain advertisements and offer extra features such as recommended news stories in weather apps. While these functionalities are not essential to an app, they nonetheless consume power, CPU cycles and bandwidth. In this paper, we design a UI driven approach that allows customizing an Android app by removing its unwanted functionalities. In particular, our technique displays the UI and allows the user to select elements denoting functionalities that she wants to remove. Using this information, our technique automatically removes all the code elements related to the selected functionalities, including all the relevant background tasks. The underlying analysis is a type system, in which each code element is tagged with a type indicating if it should be removed. From the UI hints, our technique infers types for all other code elements and reduces the app accordingly. We implement a prototype and evaluate it on 10 real-world Android apps. The results show that our approach can accurately discover the removable code elements and lead to substantial resource savings in the reduced apps.
[removable code elements, Humanoid robots, Mobile communication, modern mobile apps, recommended news stories, UI driven approach, CPU, smart phones, user interfaces, UI hints, Android (operating system), mobile computing, advertisements, UI, Prototypes, offer extra features, real-world Android apps, Real-time systems, smartphones, Androids, UI driven Android application reduction, integral part, code element, Meteorology]
SimplyDroid: Efficient event sequence simplification for android application
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
To ensure the quality of Android applications, many automatic test case generation techniques have been proposed. Among them, the Monkey fuzz testing tool and its variants are simple, effective and widely applicable. However, one major drawback of those Monkey tools is that they often generate many events in a failure-inducing input trace, which makes the follow-up debugging activities hard to apply. It is desirable to simplify or reduce the input event sequence while triggering the same failure. In this paper, we propose an efficient event trace representation and the SimplyDroid tool with three hierarchical delta-debugging algorithms each operating on this trace representation to simplify crash traces. We have evaluated SimplyDroid on a suite of real-life Android applications with 92 crash traces. The empirical result shows that our new algorithms in SimplyDroid are both efficient and effective in reducing these event traces.
[Algorithm design and analysis, program debugging, event trace representation, program testing, input event sequence, hierarchical delta-debugging algorithms, Test case reduction, debugging activities, event sequence simplification, failure-inducing input trace, delta debugging, Android (operating system), software tools, Monkey fuzz testing tool, event sequence reduction, Graphical user interfaces, crash traces, automatic test case generation techniques, Debugging, Tools, Computer crashes, Android, android application, real-life Android applications, Monkey tools, Androids, SimplyDroid tool, Smart phones]
Automated cross-platform inconsistency detection for mobile apps
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Testing of Android apps is particularly challenging due to the fragmentation of the Android ecosystem in terms of both devices and operating system versions. Developers must in fact ensure not only that their apps behave as expected, but also that the apps' behavior is consistent across platforms. To support this task, we propose DiffDroid, a new technique that helps developers automatically find cross-platform inconsistencies (CPIs) in mobile apps. DiffDroid combines input generation and differential testing to compare the behavior of an app on different platforms and identify possible inconsistencies. Given an app, DiffDroid (1) generates test inputs for the app, (2) runs the app with these inputs on a reference device and builds a model of the app behavior, (3) runs the app with the same inputs on a set of other devices, and (4) compares the behavior of the app on these different devices with the model of its behavior on the reference device. We implemented DiFFDRoiD and performed an evaluation of our approach on 5 benchmarks and over 130 platforms. our results show that DiFFDRoiD can identify CPis on real apps efficiently and with a limited number of false positives. DiFFDRoiD and our experimental infrastructure are publicly available.
[Performance evaluation, mobile apps, DiFFDRoiD, program testing, automated cross-platform inconsistency detection, Humanoid robots, Encoding, Android apps, cross-platform inconsistencies, Analytical models, DiffDroid, Android (operating system), mobile computing, Androids, Testing]
In-memory fuzzing for binary code similarity analysis
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Detecting similar functions in binary executables serves as a foundation for many binary code analysis and reuse tasks. By far, recognizing similar components in binary code remains a challenge. Existing research employs either static or dynamic approaches to capture program syntax or semantics-level features for comparison. However, there exist multiple design limitations in previous work, which result in relatively high cost, low accuracy and scalability, and thus severely impede their practical use. In this paper, we present a novel method that leverages in-memory fuzzing for binary code similarity analysis. Our prototype tool IMF-SIM applies in-memory fuzzing to launch analysis towards every function and collect traces of different kinds of program behaviors. The similarity score of two behavior traces is computed according to their longest common subsequence. To compare two functions, a feature vector is generated, whose elements are the similarity scores of the behavior trace-level comparisons. We train a machine learning model through labeled feature vectors; later, for a given feature vector by comparing two functions, the trained model gives a final score, representing the similarity score of the two functions. We evaluate IMF-SIM against binaries compiled by different compilers, optimizations, and commonly-used obfuscation methods, in total over one thousand binary executables. Our evaluation shows that IMF-SIM notably outperforms existing tools with higher accuracy and broader application scopes.
[similar components, in-memory fuzzing, behavior trace-level comparisons, code similarity, fuzzy set theory, similar functions, Tools, reverse engineering, Indexes, program compilers, binary code similarity analysis, program syntax, IMF-SIM, taint analysis, Runtime, feature extraction, binary code analysis, Binary codes, Syntactics, learning (artificial intelligence), similarity score, In-memory fuzzing]
DSIbin: Identifying dynamic data structures in C/C&#x002B;&#x002B; binaries
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Reverse engineering binary code is notoriously difficult and, especially, understanding a binary's dynamic data structures. Existing data structure analyzers are limited wrt. program comprehension: they do not detect complex structures such as skip lists, or lists running through nodes of different types such as in the Linux kernel's cyclic doubly-linked list. They also do not reveal complex parent-child relationships between structures. The tool DSI remedies these shortcomings but requires source code, where type information on heap nodes is available. We present DSIbin, a combination of DSI and the type excavator Howard for the inspection of C/C++ binaries. While a naive combination already improves upon related work, its precision is limited because Howard's inferred types are often too coarse. To address this we auto-generate candidates of refined types based on speculative nested-struct detection and type merging; the plausibility of these hypotheses is then validated by DSI. We demonstrate via benchmarking that DSIbin detects data structures with high precision.
[type excavator Howard, complex parent-child relationships, complex structures, Shape, C-C++ binaries, Merging, data structure analysis, Linux kernel, dynamic data structure identification, dynamic data structures, DSIbin, tool DSI, Benchmark testing, skip lists, data structures, Kernel, program comprehension, nested-struct detection, binary codes, Howards inferred types, program diagnostics, source code, Tools, Data structures, reverse engineering, C++ language, reverse engineering binary code, Data structure identification, type recovery, Linux, pointer programs, excavators]
Towards robust instruction-level trace alignment of binary code
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Program trace alignment is the process of establishing a correspondence between dynamic instruction instances in executions of two semantically similar but syntactically different programs. In this paper we present what is, to the best of our knowledge, the first method capable of aligning realistically long execution traces of real programs. To maximize generality, our method works entirely on the machine code level, i.e. it does not require access to source code. Moreover, the method is based entirely on dynamic analysis, which avoids the many challenges associated with static analysis of binary code, and which additionally makes our approach inherently resilient to e.g. static code obfuscation. Therefore, we believe that our trace alignment method could prove to be a useful aid in many program analysis tasks, such as debugging, reverse-engineering, investigating plagiarism, and malware analysis. We empirically evaluate our method on 11 popular Linux programs, and show that it is capable of producing meaningful alignments in the presence of various code transformations such as optimization or obfuscation, and that it easily scales to traces with tens of millions of instructions.
[invasive software, program debugging, Linux programs, program compilers, Optimization, malware analysis, Semantics, Computer architecture, program trace alignment, Malware, program analysis tasks, dynamic instruction instances, machine code level, trace alignment method, robust instruction-level trace alignment, program diagnostics, Time series analysis, static code obfuscation, code transformations, source code, static analysis, dynamic analysis, Linux, Syntactics, Concrete, binary code]
Testing intermediate representations for binary analysis
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Binary lifting, which is to translate a binary executable to a high-level intermediate representation, is a primary step in binary analysis. Despite its importance, there are only few existing approaches to testing the correctness of binary lifters. Furthermore, the existing approaches suffer from low test coverage, because they largely depend on random test case generation. In this paper, we present the design and implementation of the first systematic approach to testing binary lifters. We have evaluated the proposed system on 3 state-of-the-art binary lifters, and found 24 previously unknown semantic bugs. Our result demonstrates that writing a precise binary lifter is extremely difficult even for those heavily tested projects.
[program debugging, program testing, binary analysis, C++ languages, Tools, binary lifting, Semantics, Computer bugs, Binary codes, high-level intermediate representation, random test case generation, Software, systematic approach, precise binary lifter, Testing, binary executable]
Comprehensive failure characterization
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
There is often more than one way to trigger a fault. Standard static and dynamic approaches focus on exhibiting a single witness for a failing execution. In this paper, we study the problem of computing a comprehensive characterization which safely bounds all failing program behavior while exhibiting a diversity of witnesses for those failures. This information can be used to facilitate software engineering tasks ranging from fault localization and repair to quantitative program analysis for reliability. Our approach combines the results of overapproximating and underapproximating static analyses in an alternating iterative framework to produce upper and lower bounds on the failing input space of a program, which we call a comprehensive failure characterization (CFC). We evaluated a prototype implementation of this alternating framework on a set of 168 C programs from the SV-COMP benchmarks, and the data indicate that it is possible to efficiently, accurately, and safely characterize failure spaces.
[iterative methods, quantitative program analysis, program verification, program behavior, program diagnostics, fault localization, underapproximating static analyses, Manuals, Tools, Maintenance engineering, upper bounds, Standards, lower bounds, comprehensive failure characterization, overapproximating analyses, dynamic approaches focus, Upper bound, comprehensive characterization, alternating iterative framework, Software, software engineering, software engineering tasks]
TrEKer: Tracing error propagation in operating system kernels
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Modern operating systems (OSs) consist of numerous interacting components, many of which are developed and maintained independently of one another. In monolithic systems, the boundaries of and interfaces between such components are not strictly enforced at runtime. Therefore, faults in individual components may directly affect other parts of the system in various ways. Software fault injection (SFI) is a testing technique to assess the resilience of a software system in the presence of faulty components. Unfortunately, SFI tests of OSs are inconclusive if they do not lead to observable failures, as corruptions of the internal software state may not be visible at its interfaces and, yet, affect the subsequent execution of the OS beyond the duration of the test. In this paper we present TrEKer, a fully automated approach for identifying how faulty OS components affect other parts of the system. TrEKer combines static and dynamic analyses to achieve efficient tracing on the granularity of memory accesses. We demonstrate TrEKer's ability to support SFI oracles by accurately tracing the effects of faults injected into three widely used Linux kernel modules.
[SFI tests, fault diagnosis, Linux kernel modules, monolithic systems, TrEKer, Fault diagnosis, faulty OS components, Execution Tracing, Robustness Testing, Hardware, tracing error propagation, Kernel, internal software state, numerous interacting components, observable failures, Testing, operating system kernels, efficient tracing, Software Fault Injection, Test Oracles, Instruments, OS, software fault tolerance, SFI oracles, software fault injection, Operating Systems, faulty components, fully automated approach, testing technique, Linux, modern operating systems, software system resilience]
RuntimeSearch: Ctrl&#x002B;F for a running program
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Developers often try to find occurrences of a certain term in a software system. Traditionally, a text search is limited to static source code files. In this paper, we introduce a simple approach, RuntimeSearch, where the given term is searched in the values of all string expressions in a running program. When a match is found, the program is paused and its runtime properties can be explored with a traditional debugger. The feasibility and usefulness of RuntimeSearch is demonstrated on a medium-sized Java project.
[program comprehension, concept location, Java, program debugging, ctrl+f, text search, Instruments, program diagnostics, Debugging, medium-sized Java project, debugger, Tools, string expressions, dynamic analysis, program compilers, software system, running program, traditional debugger, Runtime, static source code files, RuntimeSearch, Graphical user interfaces]
Mining implicit design templates for actionable code reuse
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
In this paper, we propose an approach to detecting project-specific recurring designs in code base and abstracting them into design templates as reuse opportunities. The mined templates allow programmers to make further customization for generating new code. The generated code involves the code skeleton of recurring design as well as the semi-implemented code bodies annotated with comments to remind programmers of necessary modification. We implemented our approach as an Eclipse plugin called MICoDe. We evaluated our approach with a reuse simulation experiment and a user study involving 16 participants. The results of our simulation experiment on 10 open source Java projects show that, to create a new similar feature with a design template, (1) on average 69% of the elements in the template can be reused and (2) on average 60% code of the new feature can be adopted from the template. Our user study further shows that, compared to the participants adopting the copy-paste-modify strategy, the ones using MICoDe are more effective to understand a big design picture and more efficient to accomplish the code reuse task.
[Java, Object oriented modeling, public domain software, Unified modeling language, Cloning, data mining, reuse simulation experiment, actionable code reuse, code skeleton, generated code, semiimplemented code bodies, code base, software reusability, code reuse task, Feature extraction, Skeleton, implicit design template mining, open source Java projects, project-specific recurring design detection]
Exploring regular expression comprehension
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The regular expression (regex) is a powerful tool employed in a large variety of software engineering tasks. However, prior work has shown that regexes can be very complex and that it could be difficult for developers to compose and understand them. This work seeks to identify code smells that impact comprehension. We conduct an empirical study on 42 pairs of behaviorally equivalent but syntactically different regexes using 180 participants and evaluate the understandability of various regex language features. We further analyze regexes in GitHub to find the community standards or the common usages of various features. We found that some regex expression representations are more understandable than others. For example, using a range (e.g., [0-9]) is often more understandable than a default character class (e.g., [\\d]). We also found that the DFA size of a regex significantly affects comprehension for the regexes studied. The larger the DFA of a regex (up to size eight), the more understandable it was. Finally, we identify smelly and non-smelly regex representations based on a combination of community standards and understandability metrics.
[Measurement, formal languages, nonsmelly regex representations, regular expression comprehension, understandability metrics, equivalence class, Tools, smelly regex representations, software maintenance, Standards, regexes, community standards, regex representations, Automata, Syntactics, Concrete, software engineering tasks, Regular expression comprehension, Pattern matching, regex expression representations, software metrics, regex language features]
Automatically assessing code understandability: How far are we?
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Program understanding plays a pivotal role in software maintenance and evolution: a deep understanding of code is the stepping stone for most software-related activities, such as bug fixing or testing. Being able to measure the understandability of a piece of code might help in estimating the effort required for a maintenance activity, in comparing the quality of alternative implementations, or even in predicting bugs. Unfortunately, there are no existing metrics specifically designed to assess the understandability of a given code snippet. In this paper, we perform a first step in this direction, by studying the extent to which several types of metrics computed on code, documentation, and developers correlate with code understandability. To perform such an investigation we ran a study with 46 participants who were asked to understand eight code snippets each. We collected a total of 324 evaluations aiming at assessing the perceived understandability, the actual level of understanding, and the time needed to understand a code snippet. Our results demonstrate that none of the (existing and new) metrics we considered is able to capture code understandability, not even the ones assumed to assess quality attributes strongly related with it, such as code readability and complexity.
[Measurement, program debugging, Correlation, public domain software, software-related activities, code snippet, perceived understandability, Documentation, Maintenance engineering, Complexity theory, software quality, software maintenance, code complexity, Negative result, program understanding, Software metrics, Computer bugs, automatic code understandability assessibility, quality attributes, Empirical study, Code understandability, Software, code readability, software metrics]
Improved query reformulation for concept location using CodeRank and document structures
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique-ACER-that takes an initial query, identifies appropriate search terms from the source code using a novel term weight-CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique.
[Measurement, source code (software), Software maintenance, text analysis, weight-CodeRank, CodeRank, baseline queries, software system, request texts, query processing, search terms, term weighting, initial query, learning (artificial intelligence), query formulation, relevant source code locations, concept location, Java, Query reformulation, improved query reformulation, Natural languages, data resampling, source document structures, software maintenance, machine learning, software change requests, query quality analysis, Periodic structures]
Understanding feature requests by leveraging fuzzy method and linguistic analysis
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
In open software development environment, a large number of feature requests with mixed quality are often posted by stakeholders and usually managed in issue tracking systems. Thoroughly understanding and analyzing the real intents that feature requests imply is a labor-intensive and challenging task. In this paper, we introduce an approach to understand feature requests automatically. We generate a set of fuzzy rules based on natural language processing techniques that classify each sentence in feature requests into a set of categories: Intent, Explanation, Benefit, Drawback, Example and Trivia. Consequently, the feature requests can be automatically structured based on the classification results. We conduct experiments on 2,112 sentences taken from 602 feature requests of nine popular open source projects. The results show that our method can reach a high performance on classifying sentences from feature requests. Moreover, when applying fuzzy rules on machine learning methods, the performance can be improved significantly.
[Terminology, example category, public domain software, fuzzy set theory, software quality, Learning systems, fuzzy method, Semantics, knowledge based systems, natural language processing techniques, Natural language processing, learning (artificial intelligence), intent category, understanding feature requests, open source projects, natural language processing, drawback category, fuzzy rules, linguistic analysis, machine learning methods, explanation category, classification, Pragmatics, benefit category, sentence classification, Syntactics, Software, trivia category, open software development environment]
O2O service composition with social collaboration
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
In Online-to-Offline (O2O) commerce, customer services may need to be composed from online and offline services. Such composition is challenging, as it requires effective selection of appropriate services that, in turn, support optimal combination of both online and offline services. In this paper, we address this challenge by proposing an approach to O2O service composition which combines offline route planning and social collaboration to optimize service selection. We frame general O2O service composition problems using timed automata and propose an optimization procedure that incorporates: (1) a Markov Chain Monte Carlo (MCMC) algorithm to stochastically select a concrete composite service, and (2) a model checking approach to searching for an optimal collaboration plan with the lowest cost given certain time constraint. Our procedure has been evaluated using the simulation of a rich scenario on effectiveness and scalability.
[Printing, offline route planning, automata theory, concrete composite service, online-to-offline coomerce, Quality of service, optimal collaboration plan, Optimization, customer services, optimal combination, Monte Carlo methods, Markov chain Monte Carlo algorithm, formal verification, model checking, Collaboration, O2O service composition, Markov processes, Concrete, Libraries, Planning, service selection, timed automata, electronic commerce, social collaboration]
Gremlin-ATL: A scalable model transformation framework
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Industrial use of Model Driven Engineering techniques has emphasized the need for efficiently store, access, and transform very large models. While scalable persistence frameworks, typically based on some kind of NoSQL database, have been proposed to solve the model storage issue, the same level of performance improvement has not been achieved for the model transformation problem. Existing model transformation tools (such as the well-known ATL) often require the input models to be loaded in memory prior to the start of the transformation and are not optimized to benefit from lazy-loading mechanisms, mainly due to their dependency on current low-level APIs offered by the most popular modeling frameworks nowadays. In this paper we present Gremlin-ATL, a scalable and efficient model-to-model transformation framework that translates ATL transformations into Gremlin, a query language supported by several NoSQL databases. With Gremlin-ATL, the transformation is computed within the database itself, bypassing the modeling framework limitations and improving its performance both in terms of execution time and memory consumption. Tool support is available online.
[OCL Scalability, scalable persistence frameworks, application program interfaces, memory consumption, Unified modeling language, model transformation, Transforms, query languages, Database languages, query processing, low-level API, storage management, NoSQL database, Databases, NoSQL, model storage issue, Gremlin-ATL, software engineering, Gremlin, model transformation tools, Load modeling, model-to-model transformation framework, object-oriented programming, Computational modeling, NoSQL databases, ATL transformations, input models, Tools, Persistence Framework, scalable model transformation framework, Model Driven Engineering techniques, execution time, ATL, query language]
Diagnosing assumption problems in safety-critical products
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Problems with the correctness and completeness of environmental assumptions contribute to many accidents in safety-critical systems. The problem is exacerbated when products are modified in new releases or in new products of a product line. In such cases existing sets of environmental assumptions are often carried forward without sufficiently rigorous analysis. This paper describes a new technique that exploits the traceability required by many certifying bodies to reason about the likelihood that environmental assumptions are omitted or incorrectly retained in new products. An analysis of over 150 examples of environmental assumptions in historical systems informs the approach. In an evaluation on three safety-related product lines the approach caught all but one of the assumption-related problems. It also provided clearly defined steps for mitigating the identified issues. The contribution of the work is to arm the safety analyst with useful information for assessing the validity of environmental assumptions for a new product.
[Safety-critical systems, Software traceability, production engineering computing, safety-critical software, Inspection, Hazards, Environmental assumptions, Software product lines, safety-related product lines, Computer science, environmental assumptions, assumption-related problems, safety-critical systems, safety-critical products, Software, Product lines, Accidents]
Software performance self-adaptation through efficient model predictive control
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
A key challenge in software systems that are exposed to runtime variabilities, such as workload fluctuations and service degradation, is to continuously meet performance requirements. In this paper we present an approach that allows performance self-adaptation using a system model based on queuing networks (QNs), a well-assessed formalism for software performance engineering. Software engineers can select the adaptation knobs of a QN (routing probabilities, service rates, and concurrency level) and we automatically derive a Model Predictive Control (MPC) formulation suitable to continuously configure the selected knobs and track the desired performance requirements. Previous MPC approaches have two main limitations: i) high computational cost of the optimization, due to nonlinearity of the models; ii) focus on long-run performance metrics only, due to the lack of tractable representations of the QN's time-course evolution. As a consequence, these limitations allow adaptations with coarse time granularities, neglecting the system's transient behavior. Our MPC adaptation strategy is efficient since it is based on mixed integer programming, which uses a compact representation of a QN with ordinary differential equations. An extensive evaluation on an implementation of a load balancer demonstrates the effectiveness of the adaptation and compares it with traditional methods based on probabilistic model checking.
[model predictive control, Adaptation models, probabilistic model checking, integer programming, software performance engineering, queuing networks, software systems, service rates, Quality of service, Throughput, Optimization, Performance requirements, differential equations, Concurrent computing, Model predictive control, Runtime, resource allocation, routing probabilities, software performance self-adaptation, software performance evaluation, concurrency level, queueing theory, Computational modeling, runtime variabilities, probability, adaptation knobs, workload fluctuations, Adaptive software, system model, ordinary differential equations, Control-theory, service degradation, mixed integer programming, MPC adaptation strategy, predictive control]
Transfer learning for performance modeling of configurable systems: An exploratory analysis
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.
[transfer learning, program diagnostics, software systems, Predictive models, performance model, software configurations, Mobile communication, configurable systems, Analytical models, performance behavior, performance modeling, Software systems, Hardware, software engineering, Performance analysis, Reliability, learning (artificial intelligence), software performance evaluation, highly dimensional configuration space]
A comprehensive study of real-world numerical bug characteristics
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Numerical software is used in a wide variety of applications including safety-critical systems, which have stringent correctness requirements, and whose failures have catastrophic consequences that endanger human life. Numerical bugs are known to be particularly difficult to diagnose and fix, largely due to the use of approximate representations of numbers such as floating point. Understanding the characteristics of numerical bugs is the first step to combat them more effectively. In this paper, we present the first comprehensive study of real-world numerical bugs. Specifically, we identify and carefully examine 269 numerical bugs from five widely-used numerical software libraries: NumPy, SciPy, LAPACK, GNU Scientific Library, and Elemental. We propose a categorization of numerical bugs, and discuss their frequency, symptoms and fixes. Our study opens new directions in the areas of program analysis, testing, and automated program repair of numerical software, and provides a collection of real-world numerical bugs.
[program debugging, SciPy, program testing, numerical software libraries, program diagnostics, public domain software, safety-critical software, LAPACK, Tools, Maintenance engineering, software maintenance, software libraries, NumPy, Elemental, safety-critical systems, numerical bug characteristics, automated program repair, Computer bugs, Semantics, GNU Scientific Library, program analysis, Libraries, Software, Roundoff errors]
A comprehensive study on real world concurrency bugs in Node.js
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Node.js becomes increasingly popular in building server-side JavaScript applications. It adopts an event-driven model, which supports asynchronous I/O and non-deterministic event processing. This asynchrony and non-determinism can introduce intricate concurrency bugs, and leads to unpredictable behaviors. An in-depth understanding of real world concurrency bugs in Node.js applications will significantly promote effective techniques in bug detection, testing and fixing for Node.js. In this paper, we present NodeCB, a comprehensive study on real world concurrency bugs in Node.js applications. Specifically, we have carefully studied 57 real bug cases from open-source Node.js applications, and have analyzed their bug characteristics, e.g., bug patterns and root causes, bug impacts, bug manifestation, and fix strategies. Through this study, we obtain several interesting findings, which may open up many new research directions in combating concurrency bugs in Node.js. For example, one finding is that two thirds of the bugs are caused by atomicity violation. However, due to lack of locks and transaction mechanism, Node.js cannot easily express and guarantee the atomic intention.
[Java, program debugging, program testing, concurrency bug, Instruction sets, program diagnostics, NodeCB, event-driven, empirical study, bug patterns, Node.js, bug detection, bug manifestation, Open source software, Concurrent computing, bug impacts, building server-side JavaScript applications, Databases, Computer bugs, concurrency control, JavaScript, concurrency bugs, open-source Node.js applications, Testing]
Generating simpler AST edit scripts by considering copy-and-paste
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
In software development, there are many situations in which developers need to understand given source code changes in detail. Until now, a variety of techniques have been proposed to support understanding source code changes. Tree-based differencing techniques are expected to have better understandability than text-based ones, which are widely used nowadays (e.g., diff in Unix). In this paper, we propose to consider copy-and-paste as a kind of editing action forming tree-based edit script, which is an editing sequence that transforms a tree to another one. Software developers often perform copy- and-paste when they are writing source code. Introducing copy- and-paste action into edit script contributes to not only making simpler (more easily understandable) edit scripts but also making edit scripts closer to developers' actual editing sequences. We conducted experiments on an open dataset. As a result, we confirmed that our technique made edit scripts shorter for 18% of the code changes with a little more computational time. For the other 82% code changes, our technique generated the same edit scripts as an existing technique. We also confirmed that our technique provided more helpful visualizations.
[source code (software), Visualization, Java, text editing, software development, copy-and-paste action, Cloning, Transforms, source code change, AST edit scripts, editing sequence, configuration management, tree-based differencing techniques, Computer bugs, XML, Writing, Syntactics, Software, software engineering, tree-based edit script, tree data structures, computational time]
Renaming and shifted code in structured merging: Looking ahead for precision and performance
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Diffing and merging of source-code artifacts is an essential task when integrating changes in software versions. While state-of-the-art line-based merge tools (e.g., git merge) are fast and independent of the programming language used, they have only a low precision. Recently, it has been shown that the precision of merging can be substantially improved by using a language-aware, structured approach that works on abstract syntax trees. But, precise structured merging is NP hard, especially, when considering the notoriously difficult scenarios of renamings and shifted code. To address these scenarios without compromising scalability, we propose a syntax-aware, heuristic optimization for structured merging that employs a lookahead mechanism during tree matching. The key idea is that renamings and shifted code are not arbitrarily distributed, but their occurrence follows patterns, which we address with a syntax-specific lookahead. Our experiments with 48 real-world open-source projects (4,878 merge scenarios with over 400 million lines of code) demonstrate that we can significantly improve matching precision in 28 percent of cases while maintaining performance.
[shifted code, merging, syntax-aware heuristic optimization, public domain software, Merging, trees (mathematics), syntax-specific lookahead, source-code artifacts, Tools, software versions, software maintenance, Optimization, Open source software, abstract syntax trees, tree matching, optimisation, lookahead mechanism, NP hard problem, Syntactics, programming language, renaming code, language-aware structured approach, structured merging]
Semantics-assisted code review: An efficient tool chain and a user study
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Code changes are often reviewed before they are deployed. Popular source control systems aid code review by presenting textual differences between old and new versions of the code, leaving developers with the difficult task of determining whether the differences actually produced the desired behavior. Fortunately, we can mine such information from code repositories. We propose aiding code review with inter-version semantic differential analysis. During review of a new commit, a developer is presented with summaries of both code differences and behavioral differences, which are expressed as diffs of likely invariants extracted by running the system's test cases. As a result, developers can more easily determine that the code changes produced the desired effect. We created an invariant-mining tool chain, Getty, to support our concept of semantically-assisted code review. To validate our approach, 1) we applied Getty to the commits of 6 popular open source projects, 2) we assessed the performance and cost of running Getty in different configurations, and 3) we performed a comparative user study with 18 developers. Our results demonstrate that semantically-assisted code review is feasible, effective, and that real programmers can leverage it to improve the quality of their reviews.
[dynamic impact analysis, invariant-mining tool chain, project management, Navigation, semantic-assisted code review, software testing, code differences, data mining, Tools, Software behavior, software maintenance, inter-version semantic differential analysis, code changes, mining software repository, scalability, Computer bugs, Semantics, code repositories, Software, code review, Internet, likely invariants, Testing]
Detecting unknown inconsistencies in web applications
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Although there has been increasing demand for more reliable web applications, JavaScript bugs abound in web applications. In response to this issue, researchers have proposed automated fault detection tools, which statically analyze the web application code to find bugs. While useful, these tools either only target a limited set of bugs based on predefined rules, or they do not detect bugs caused by cross-language interactions, which occur frequently in web application code. To address this problem, we present an anomaly-based inconsistency detection approach, implemented in a tool called HOLOCRON. The main novelty of our approach is that it does not look for hard-coded inconsistency classes. Instead, it applies subtree pattern matching to infer inconsistency classes and association rule mining to detect inconsistencies that occur both within a single language, and between two languages. We evaluated HOLOCRON, and it successfully detected 51 previously unreported inconsistencies - including 18 bugs and 33 code smells - in 12 web applications.
[inconsistency detection approach, program debugging, fault diagnosis, program diagnostics, data mining, Transforms, fault detection, Tools, Encoding, unreported inconsistencies, cross-language interactions, association rule mining, Web application code, Computer languages, hard-coded inconsistency classes, Computer bugs, JavaScript, JavaScript bugs, fault detection tools, Testing]
Why and how JavaScript developers use linters
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Automatic static analysis tools help developers to automatically spot code issues in their software. They can be of extreme value in languages with dynamic characteristics, such as JavaScript, where developers can easily introduce mistakes which can go unnoticed for a long time, e.g. a simple syntactic or spelling mistake. Although research has already shown how developers perceive such tools for strongly-typed languages such as Java, little is known about their perceptions when it comes to dynamic languages. In this paper, we investigate what motivates and how developers make use of such tools in JavaScript projects. To that goal, we apply a qualitative research method to conduct and analyze a series of 15 interviews with developers responsible for the linter configuration in reputable OSS JavaScript projects that apply the most commonly used linter, ESLint. The results describe the benefits that developers obtain when using ESLint, the different ways one can configure the tool and prioritize its rules, and the existing challenges in applying linters in the real world. These results have direct implications for developers, tool makers, and researchers, such as tool improvements, and a research agenda that aims to increase our knowledge about the usefulness of such analyzers.
[Java, program diagnostics, ESLint, Tools, JavaScript developers, Encoding, type theory, Standards, linters, strongly-typed languages, automatic static analysis tools, reputable OSS JavaScript projects, dynamic languages, Software, Face, Interviews]
Automatic testing of symbolic execution engines via program generation and differential testing
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Symbolic execution has attracted significant attention in recent years, with applications in software testing, security, networking and more. Symbolic execution tools, like CREST, KLEE, FuzzBALL, and Symbolic PathFinder, have enabled researchers and practitioners to experiment with new ideas, scale the technique to larger applications and apply it to new application domains. Therefore, the correctness of these tools is of critical importance. In this paper, we present our experience extending compiler testing techniques to find errors in both the concrete and symbolic execution components of symbolic execution engines. The approach used relies on a novel way to create program versions, in three different testing modes-concrete, single-path and multi-path-each exercising different features of symbolic execution engines. When combined with existing program generation techniques and appropriate oracles, this approach enables differential testing within a single symbolic execution engine. We have applied our approach to the KLEE, CREST and FuzzBALL symbolic execution engines, where it has discovered 20 different bugs exposing a variety of important errors having to do with the handling of structures, division, modulo, casting, vector instructions and more, as well as issues related to constraint solving, compiler optimisations and test input replay.
[constraint solving, program testing, program verification, concrete testing mode, symbolic execution components, Engines, test input replay, automatic testing, Program processors, CREST symbolic execution engine, multipath testing mode, program generation techniques, symbolic execution tools, compiler optimisations, compiler testing techniques, Testing, single-path testing mode, optimising compilers, single symbolic execution engine, Instruments, program diagnostics, software testing, Tools, KLEE execution engine, Symbolic PathFinder, Computer bugs, differential testing, Concrete, FuzzBALL symbolic execution engine]
Floating-point symbolic execution: A case study in N-version programming
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Symbolic execution is a well-known program analysis technique for testing software, which makes intensive use of constraint solvers. Recent support for floating-point constraint solving has made it feasible to support floating-point reasoning in symbolic execution tools. In this paper, we present the experience of two research teams that independently added floating-point support to KLEE, a popular symbolic execution engine. Since the two teams independently developed their extensions, this created the rare opportunity to conduct a rigorous comparison between the two implementations, essentially a modern case study on N-version programming. As part of our comparison, we report on the different design and implementation decisions taken by each team, and show their impact on a rigorously assembled and tested set of benchmarks, itself a contribution of the paper.
[research teams, program testing, program diagnostics, public domain software, floating-point reasoning, constraint solvers, Tools, Programming, symbolic execution engine, Cognition, Encoding, floating-point support, floating point arithmetic, floating-point constraint solving, program analysis technique, N-version programming symbolic execution, floating-point symbolic execution, implementation decisions, Benchmark testing, Concrete, symbolic execution tools, constraint handling, testing software]
Rethinking pointer reasoning in symbolic execution
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Symbolic execution is a popular program analysis technique that allows seeking for bugs by reasoning over multiple alternative execution states at once. As the number of states to explore may grow exponentially, a symbolic executor may quickly run out of space. For instance, a memory access to a symbolic address may potentially reference the entire address space, leading to a combinatorial explosion of the possible resulting execution states. To cope with this issue, state-of-the-art executors concretize symbolic addresses that span memory intervals larger than some threshold. Unfortunately, this could result in missing interesting execution states, e.g., where a bug arises. In this paper we introduce MEMSIGHT, a new approach to symbolic memory that reduces the need for concretization, hence offering the opportunity for broader state explorations and more precise pointer reasoning. Rather than mapping address instances to data as previous tools do, our technique maps symbolic address expressions to data, maintaining the possible alternative states resulting from the memory referenced by a symbolic address in a compact, implicit form. A preliminary experimental investigation on prominent benchmarks from the DARPA Cyber Grand Challenge shows that MemSight enables the exploration of states unreachable by previous techniques.
[program debugging, program testing, program verification, program diagnostics, address instances, Merging, symbolic address expressions, Cognition, symbolic memory, pointer reasoning, Indexes, execution states, memory access, span memory intervals, bugs, address space, program analysis technique, Weapons, Computer bugs, symbolic execution, Concrete, symbolic executor, Load modeling]
Leveraging abstract interpretation for efficient dynamic symbolic execution
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Dynamic Symbolic Execution (DSE) is a technique to automatically generate test inputs by executing a program with concrete and symbolic values simultaneously. A key challenge in DSE is scalability; executing all feasible program paths is not possible, owing to the potentially exponential or infinite number of paths. Loops are a main source of path explosion, in particular where the number of iterations depends on a program's input. Problems arise because DSE maintains symbolic values that capture only the dependencies on symbolic inputs. This ignores control dependencies, including loop dependencies that depend indirectly on the inputs. We propose a method to increase the coverage achieved by DSE in the presence of input-data dependent loops and loop dependent branches. We combine DSE with abstract interpretation to find indirect control dependencies, including loop and branch indirect dependencies. Preliminary results show that this results in better coverage, within considerably less time compared to standard DSE.
[program testing, program verification, Scalability, standard DSE, input-data dependent loops, indirect control dependencies, DSE, dependent branches, loop dependencies, Testing, test inputs, program control structures, program diagnostics, abstract interpretation, symbolic inputs, Tools, Explosions, Dynamic symbolic execution, feasible program paths, branch indirect dependencies, test generation, symbolic values, efficient dynamic symbolic execution, Concrete, path explosion]
Tortoise: Interactive system configuration repair
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
System configuration languages provide powerful abstractions that simplify managing large-scale, networked systems. Thousands of organizations now use configuration languages, such as Puppet. However, specifications written in configuration languages can have bugs and the shell remains the simplest way to debug a misconfigured system. Unfortunately, it is unsafe to use the shell to fix problems when a system configuration language is in use: a fix applied from the shell may cause the system to drift from the state specified by the configuration language. Thus, despite their advantages, configuration languages force system administrators to give up the simplicity and familiarity of the shell. This paper presents a synthesis-based technique that allows administrators to use configuration languages and the shell in harmony. Administrators can fix errors using the shell and the technique automatically repairs the higher-level specification written in the configuration language. The approach (1) produces repairs that are consistent with the fix made using the shell; (2) produces repairs that are maintainable by minimizing edits made to the original specification; (3) ranks and presents multiple repairs when relevant; and (4) supports all shells the administrator may wish to use. We implement our technique for Puppet, a widely used system configuration language, and evaluate it on a suite of benchmarks under 42 repair scenarios. The top-ranked repair is selected by humans 76% of the time and the human-equivalent repair is ranked 1.31 on average.
[program debugging, human-equivalent repair, Maintenance engineering, misconfigured system, Web servers, formal specification, large-scale systems, configuration languages force system administrators, system configuration language, synthesis-based technique, Computer bugs, Organizations, specification languages, Benchmark testing, interactive system configuration repair, large-scale networked systems, Tortoise, Puppet]
Contract-based program repair without the contracts
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Automated program repair (APR) is a promising approach to automatically fixing software bugs. Most APR techniques use tests to drive the repair process; this makes them readily applicable to realistic code bases, but also brings the risk of generating spurious repairs that overfit the available tests. Some techniques addressed the overfitting problem by targeting code using contracts (such as pre- and postconditions), which provide additional information helpful to characterize the states of correct and faulty computations; unfortunately, mainstream programming languages do not normally include contract annotations, which severely limits the applicability of such contract-based techniques. This paper presents JAID, a novel APR technique for Java programs, which is capable of constructing detailed state abstractions-similar to those employed by contract-based techniques-that are derived from regular Java code without any special annotations. Grounding the repair generation and validation processes on rich state abstractions mitigates the overfitting problem, and helps extend APR's applicability: in experiments with the DEFECTS4J benchmark, a prototype implementation of JAID produced genuinely correct repairs, equivalent to those written by programmers, for 25 bugs-improving over the state of the art of comparable Java APR techniques in the number and kinds of correct fixes.
[program debugging, validation processes, spurious repairs, contracts, formal specification, mainstream programming languages, comparable Java APR techniques, realistic code bases, overfitting problem, detailed state abstractions, Java programs, Contracts, Monitoring, Java, APR technique, program diagnostics, regular Java code, Maintenance engineering, Tools, contract annotations, repair generation, Indexes, software maintenance, automated program repair, Computer bugs]
Elixir: Effective object-oriented program repair
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
This work is motivated by the pervasive use of method invocations in object-oriented (OO) programs, and indeed their prevalence in patches of OO-program bugs. We propose a generate-and-validate repair technique, called ELIXIR designed to be able to generate such patches. ELIXIR aggressively uses method calls, on par with local variables, fields, or constants, to construct more expressive repair-expressions, that go into synthesizing patches. The ensuing enlargement of the repair space, on account of the wider use of method calls, is effectively tackled by using a machine-learnt model to rank concrete repairs. The machine-learnt model relies on four features derived from the program context, i.e., the code surrounding the potential repair location, and the bug report. We implement ELIXIR and evaluate it on two datasets, the popular Defects4J dataset and a new dataset Bugs.jar created by us, and against 2 baseline versions of our technique, and 5 other techniques representing the state of the art in program repair. Our evaluation shows that ELIXIR is able to increase the number of correctly repaired bugs in Defects4J by 85% (from 14 to 26) and by 57% in Bugs.jar (from 14 to 22), while also significantly out-performing other state-of-the-art repair techniques including ACS, HD-Repair, NOPOL, PAR, and jGenProg.
[PAR, program debugging, object-oriented program repair, correctly repaired bugs, concrete repairs, repair space, machine-learnt model, popular Defects4J dataset, learning (artificial intelligence), dataset Bugs.jar, Java, object-oriented programming, Object oriented modeling, synthesizing patches, program context, HD-Repair, state-of-the-art repair techniques, Maintenance engineering, Tools, expressive repair-expressions, -validate repair technique, bug report, Computer bugs, generate- validate repair technique, potential repair location, Software, Concrete, OO-program bugs, called ELIXIR]
Leveraging syntax-related code for automated program repair
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
We present our automated program repair technique ssFix which leverages existing code (from a code database) that is syntax-related to the context of a bug to produce patches for its repair. Given a faulty program and a fault-exposing test suite, ssFix does fault localization to identify suspicious statements that are likely to be faulty. For each such statement, ssFix identifies a code chunk (or target chunk) including the statement and its local context. ssFix works on the target chunk to produce patches. To do so, it first performs syntactic code search to find candidate code chunks that are syntax-related, i.e., structurally similar and conceptually related, to the target chunk from a code database (or codebase) consisting of the local faulty program and an external code repository. ssFix assumes the correct fix to be contained in the candidate chunks, and it leverages each candidate chunk to produce patches for the target chunk. To do so, ssFix translates the candidate chunk by unifying the names used in the candidate chunk with those in the target chunk; matches the chunk components (expressions and statements) between the translated candidate chunk and the target chunk; and produces patches for the target chunk based on the syntactic differences that exist between the matched components and in the unmatched components. ssFix finally validates the patched programs generated against the test suite and reports the first one that passes the test suite. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results show that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java.
[program debugging, candidate code chunks, Automated program repair, program testing, translated candidate chunk, computational linguistics, syntax-related code, chunk components, Fault diagnosis, target chunk, Databases, Semantics, matched components, code transfer, unmatched components, external code repository, Java, program diagnostics, code chunk, code search, Maintenance engineering, software maintenance, Computer bugs, code database, syntactic code search, Syntactics, automated program repair technique ssFix, local faulty program, Defects4J bug dataset]
Boosting complete-code tool for partial program
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
To improve software quality, researchers and practitioners have proposed static analysis tools for various purposes (e.g., detecting bugs, anomalies, and vulnerabilities). Although many such tools are powerful, they typically need complete programs where all the code names (e.g., class names, method names) are resolved. In many scenarios, researchers have to analyze partial programs in bug fixes (the revised source files can be viewed as a partial program), tutorials, and code search results. As a partial program is a subset of a complete program, many code names in partial programs are unknown. As a result, despite their syntactical correctness, existing complete-code tools cannot analyze partial programs, and existing partial-code tools are limited in both their number and analysis capability. Instead of proposing another tool for analyzing partial programs, we propose a general approach, called GRAPA, that boosts existing tools for complete programs to analyze partial programs. Our major insight is that after unknown code names are resolved, tools for complete programs can analyze partial programs with minor modifications. In particular, GRAPA locates Java archive files to resolve unknown code names, and resolves the remaining unknown code names from resolved code names. To illustrate GRAPA, we implement a tool that leverages the state-of-the-art tool, WALA, to analyze Java partial programs. We thus implemented the first tool that is able to build system dependency graphs for partial programs, complementing existing tools. We conduct an evaluation on 8,198 partial-code commits from four popular open source projects. Our results show that GRAPA fully resolved unknown code names for 98.5% bug fixes, with an accuracy of 96.1% in total. Furthermore, our results show the significance of GRAPA's internal techniques, which provides insights on how to integrate with more complete-code tools to analyze partial programs.
[Java, program debugging, Partial program, program diagnostics, public domain software, graph theory, Tools, Boosting, GRAPA, software quality, Data mining, unknown code names, partial program, boosting complete-code tool, Java partial program analysis, software quality improvement, Computer bugs, partial-code tools, program analysis, Syntactics, Software, complete-code tool, Java archive files]
A language model for statements of software code
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Building language models for source code enables a large set of improvements on traditional software engineering tasks. One promising application is automatic code completion. State-of-the-art techniques capture code regularities at token level with lexical information. Such language models are more suitable for predicting short token sequences, but become less effective with respect to long statement level predictions. In this paper, we have proposed PCC to optimize the token-level based language modeling. Specifically, PCC introduced an intermediate representation (IR) for source code, which puts tokens into groups using lexeme and variable relative order. In this way, PCC is able to handle long token sequences, i.e., group sequences, to suggest a complete statement with the precise synthesizer. Further more, PCC employed a fuzzy matching technique which combined genetic and longest common subsequence algorithms to make the prediction more accurate. We have implemented a code completion plugin for Eclipse and evaluated it on open-source Java projects. The results have demonstrated the potential of PCC in generating precise long statement level predictions. In 30%-60% of the cases, it can correctly suggest the complete statement with only six candidates, and 40%-90% of the cases with ten candidates.
[source code (software), pattern matching, Synthesizers, public domain software, complete statement, automatic code completion, short token sequences, longest common subsequence algorithms, fuzzy set theory, Predictive models, Training, Training data, group sequences, software engineering, fuzzy matching technique, variable relative order, Java, open-source Java projects, genetic subsequence algorithms, source code, lexical information, IR, genetic algorithms, long token sequences, Code Completion, Language Model, code completion plugin, PCC, precise long statement level predictions, Software, software code, token-level based language modeling, Context modeling]
Context-aware integrated development environment command recommender systems
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Integrated development environments (IDEs) are complex applications that integrate multiple tools for creating and manipulating software project artifacts. To improve users' knowledge and the effectiveness of usage of the available functionality, the inclusion of recommender systems into IDEs has been proposed. We present a novel IDE command recommendation algorithm that, by taking into account the contexts in which a developer works and in which different commands are usually executed, is able to provide relevant recommendations. We performed an empirical comparison of the proposed algorithm with state-of-the-art IDE command recommenders on a real-world data set. The algorithms were evaluated in terms of precision, recall, F1, k-tail, and with a new evaluation metric that is specifically measuring the usefulness of contextual recommendations. The experiments revealed that in terms of the contextual relevance and usefulness of recommendations the proposed algorithm outperforms existing algorithms.
[Algorithm design and analysis, complex applications, Java, software project artifacts, project management, Software algorithms, contextual recommendations, software management, integrated development environments, ubiquitous computing, recommender systems, IDE command recommenders, Prediction algorithms, IDE command recommendation algorithm, Software, software engineering, available functionality, Recommender systems, context-aware integrated development environment command recommender systems, Context modeling]
Predicting relevance of change recommendations
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Software change recommendation seeks to suggest artifacts (e.g., files or methods) that are related to changes made by a developer, and thus identifies possible omissions or next steps. While one obvious challenge for recommender systems is to produce accurate recommendations, a complimentary challenge is to rank recommendations based on their relevance. In this paper, we address this challenge for recommendation systems that are based on evolutionary coupling. Such systems use targeted association-rule mining to identify relevant patterns in a software system's change history. Traditionally, this process involves ranking artifacts using interestingness measures such as confidence and support. However, these measures often fall short when used to assess recommendation relevance. We propose the use of random forest classification models to assess recommendation relevance. This approach improves on past use of various interestingness measures by learning from previous change recommendations. We empirically evaluate our approach on fourteen open source systems and two systems from our industry partners. Furthermore, we consider complimenting two mining algorithms: Co-Change and Tarmaq. The results find that random forest classification significantly outperforms previous approaches, receives lower Brier scores, and has superior trade-off between precision and recall. The results are consistent across software system and mining algorithm.
[Tarmaq mining algorithm, data mining, random forests, Data mining, History, software system, precision value, evolutionary coupling, association-rule mining, software engineering, Software measurement, learning (artificial intelligence), software change recommendation, artifacts, pattern classification, relevance prediction, Software algorithms, random forest classification models, recommendation relevance, Brier scores, Couplings, recall value, targeted association rule mining, recommender systems, relevance feedback, Co-Change mining algorithm, recommendation confidence, Software systems, open source systems, interestingness measures]
AnswerBot: Automated generation of answer summary to developers' technical questions
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The prevalence of questions and answers on domain-specific Q&amp;A sites like Stack Overflow constitutes a core knowledge asset for software engineering domain. Although search engines can return a list of questions relevant to a user query of some technical question, the abundance of relevant posts and the sheer amount of information in them makes it difficult for developers to digest them and find the most needed answers to their questions. In this work, we aim to help developers who want to quickly capture the key points of several answer posts relevant to a technical question before they read the details of the posts. We formulate our task as a query-focused multi-answer-posts summarization task for a given technical question. Our proposed approach AnswerBot contains three main steps : 1) relevant question retrieval, 2) useful answer paragraph selection, 3) diverse answer summary generation. To evaluate our approach, we build a repository of 228,817 Java questions and their corresponding answers from Stack Overflow. We conduct user studies with 100 randomly selected Java questions (not in the question repository) to evaluate the quality of the answer summaries generated by our approach, and the effectiveness of its relevant question retrieval and answer paragraph selection components. The user study results demonstrate that answer summaries generated by our approach are relevant, useful and diverse; moreover, the two components are able to effectively retrieve relevant questions and select salient answer paragraphs for summarization.
[search engines, salient answer paragraphs, answer paragraph selection components, user query, question retrieval, answer posts, randomly selected Java questions, query processing, core knowledge asset, answer summary, question repository, Search engines, Knowledge discovery, automated generation, software engineering domain, Java, Google, multianswer-posts summarization task, relevant posts, Tools, relevant question retrieval, AnswerBot, Synchronization, Summary generation, Stack Overflow, domain-specific Q&amp;A, question answering (information retrieval)]
Recommending crowdsourced software developers in consideration of skill improvement
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Finding suitable developers for a given task is critical and challenging for successful crowdsourcing software development. In practice, the development skills will be improved as developers accomplish more development tasks. Prior studies on crowdsourcing developer recommendation do not consider the changing of skills, which can underestimate developers' skills to fulfill a task. In this work, we first conducted an empirical study of the performance of 74 developers on Topcoder. With a difficulty-weighted algorithm, we re-compute the scores of each developer by eliminating the effect of task difficulty from the performance. We find out that the skill improvement of Topcoder developers can be fitted well with the negative exponential learning curve model. Second, we design a skill prediction method based on the learning curve. Then we propose a skill improvement aware framework for recommending developers for software development with crowdsourcing.
[Crowdsourcing, Algorithm design and analysis, crowdsourcing developer recommendation, Topcoder developers, Correlation, development tasks, crowdsourcing, software development, crowdsourcing software development, development skills, Topcoder, skill improvement aware framework, skill prediction method, professional aspects, recommender systems, negative exponential learning curve model, Prediction algorithms, Software, software engineering, Reliability]
The rise of the (modelling) bots: Towards assisted modelling via social networks
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
We are witnessing a rising role of mobile computing and social networks to perform all sorts of tasks. This way, social networks like Twitter or Telegram are used for leisure, and they frequently serve as a discussion media for work-related activities. In this paper, we propose taking advantage of social networks to enable the collaborative creation of models by groups of users. The process is assisted by modelling bots that orchestrate the collaboration and interpret the users' inputs (in natural language) to incrementally build a (meta-)model. The advantages of this modelling approach include ubiquity of use, automation, assistance, natural user interaction, traceability of design decisions, possibility to incorporate coordination protocols, and seamless integration with the user's normal daily usage of social networks. We present a prototype implementation called SOCIO, able to work over several social networks like Twitter and Telegram, and a preliminary evaluation showing promising results.
[collaborative creation, Telegram, Computational modeling, natural language processing, social networks, natural user interaction, Twitter, Windows, user interfaces, assisted modelling, mobile computing, Collaborative modelling, meta-modelling, Collaboration, Prototypes, groupware, modelling bots, natural languages, social networking (online), SOCIO, natural language]
UNDEAD: Detecting and preventing deadlocks in production software
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Deadlocks are critical problems afflicting parallel applications, causing software to hang with no further progress. Existing detection tools suffer not only from significant recording performance overhead, but also from excessive memory and/or storage overhead. In addition, they may generate numerous false alarms. Subsequently, after problems have been reported, tremendous manual effort is required to confirm and fix these deadlocks. This paper designs a novel system, UnDead, that helps defeat deadlocks in production software. Different from existing detection tools, UnDead imposes negligible runtime performance overhead (less than 3 % on average) and small memory overhead (around 6%), without any storage consumption. After detection, UnDead automatically strengthens erroneous programs to prevent future occurrences of both existing and potential deadlocks, which is similar to the existing work-Dimmunix. However, UnDead exceeds Dimmunix with several orders of magnitude lower performance overhead, while eliminating numerous false positives. Extremely low runtime and memory overhead, convenience, and automatic prevention make UnDead an always-on detection tool, and a "band-aid" prevention system for production software.
[program debugging, automatic prevention, program diagnostics, production engineering computing, UnDead, Tools, UNDEAD, false positives, storage consumption, Concurrent computing, performance overhead, Runtime, runtime performance overhead, Computer bugs, band-aid prevention system, concurrency control, Production, System recovery, Dimmunix, memory overhead, Software, storage overhead, production software]
Promoting secondary orders of event pairs in randomized scheduling using a randomized stride
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Because of the wide use of randomized scheduling in concurrency testing research, it is important to understand randomized scheduling and its limitations. This work analyzes how randomized scheduling discovers concurrency bugs by focusing on the probabilities of the two possible orders of a pair of events. Analysis shows that the disparity between probabilities can be large for programs that encounter a large number of events during execution. Because sets of ordered event pairs define conditions for discovering concurrency bugs, this disparity can make some concurrency bugs highly unlikely. The complementary nature of the two possible orders also indicates a potential trade-off between the probability of discovering frequently-occurring and infrequently-occurring concurrency bugs. To help address this trade-off in a more balanced way, randomized-stride scheduling is proposed, where scheduling granularity for each thread is adjusted using a randomized stride calculated based on thread length. With some assumptions, strides can be calculated to allow covering the least likely event pair orders. Experiments confirm the analysis results and also suggest that randomized-stride scheduling is more effective for discovering concurrency bugs compared to the original randomized scheduling implementation, and compared to other algorithms in recent literature.
[program debugging, software debugging, Instruction sets, scheduling granularity, software quality, processor scheduling, parallel programming, concurrency testing research, Concurrent computing, scheduling algorithms, Scheduling algorithms, ordered event pairs, probabilities, Testing, Message systems, randomized-stride scheduling, probability, secondary orders, Scheduling, frequently-occurring concurrency bugs, infrequently-occurring concurrency bugs, Multithreading, Computer bugs, concurrency control, event pair orders]
Parallel bug-finding in concurrent programs via reduced interleaving instances
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Concurrency poses a major challenge for program verification, but it can also offer an opportunity to scale when subproblems can be analysed in parallel. We exploit this opportunity here and use a parametrizable code-to-code translation to generate a set of simpler program instances, each capturing a reduced set of the original program's interleavings. These instances can then be checked independently in parallel. Our approach does not depend on the tool that is chosen for the final analysis, is compatible with weak memory models, and amplifies the effectiveness of existing tools, making them find bugs faster and with fewer resources. We use Lazy-CSeq as an off-the-shelf final verifier to demonstrate that our approach is able, already with a small number of cores, to find bugs in the hardest known concurrency benchmarks in a matter of minutes, whereas other dynamic and static tools fail to do so in hours.
[program debugging, program verification, Instruction sets, off-the-shelf final verifier, Programming, simpler program instances, concurrent programs, dynamic tools, swarm verification, Concurrent computing, Model checking, original program, weak memory models, concurrency benchmarks, multi-threading, Tools, reduced interleaving instances, capturing a reduced set, Verification, concurrency, parallel bug-finding, static tools, sequentialization, Computer bugs, concurrency control, parametrizable code-to-code translation]
Understanding and overcoming parallelism bottlenecks in ForkJoin applications
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
ForkJoin framework is a widely used parallel programming framework upon which both core concurrency libraries and real-world applications are built. Beneath its simple and user-friendly APIs, ForkJoin is a sophisticated managed parallel runtime unfamiliar to many application programmers: the framework core is a work-stealing scheduler, handles fine-grained tasks, and sustains the pressure from automatic memory management. ForkJoin poses a unique gap in the compute stack between high-level software engineering and low-level system optimization. Understanding and bridging this gap is crucial for the future of parallelism support in JVM-supported applications. This paper describes a comprehensive study on parallelism bottlenecks in ForkJoin applications, with a unique focus on how they interact with underlying system-level features, such as work stealing and memory management. We identify 6 bottlenecks, and found that refactoring them can significantly improve performance and energy efficiency. Our field study includes an in-depth analysis of Akka - a real-world actor framework - and 30 additional open-source ForkJoin projects. We sent our patches to the developers of 15 projects, and 7 out of the 9 projects that replied to our patches have accepted them.
[application program interfaces, open-source ForkJoin, high-level software engineering, Programming, automatic memory management, parallel programming framework, parallelism support, Optimization, parallel programming, processor scheduling, Runtime, real-world actor framework, energy efficiency, system-level features, Parallel processing, software engineering, JVM-supported applications, Java, compute stack, parallelism bottlenecks, core concurrency libraries, real-world applications, Synchronization, ForkJoin applications, work-stealing scheduler, user-friendly API, application programmers, low-level system optimization]
Quick verification of concurrent programs by iteratively relaxed scheduling
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The most prominent advantage of software verification over testing is a rigorous check of every possible software behavior. However, large state spaces of concurrent systems, due to non-deterministic scheduling, result in a slow automated verification process. Therefore, verification introduces a large delay between completion and deployment of concurrent software. This paper introduces a novel iterative approach to verification of concurrent programs that drastically reduces this delay. By restricting the execution of concurrent programs to a small set of admissible schedules, verification complexity and time is drastically reduced. Iteratively adding admissible schedules after their verification eventually restores non-deterministic scheduling. Thereby, our framework allows to find a sweet spot between a low verification delay and sufficient execution time performance. Our evaluation of a prototype implementation on well-known benchmark programs shows that after verifying only few schedules of the program, execution time overhead is competitive to existing deterministic multi-threading frameworks.
[Schedules, iterative methods, program verification, software verification, Programming, verification complexity, slow automated verification process, Concurrent computing, execution time overhead, iterative approach, sufficient execution time performance, Model checking, scheduling, quick verification, multi-threading, admissible schedules, low verification delay, deterministic multithreading frameworks, concurrent software, iteratively relaxed scheduling, software behavior, concurrency control, concurrent systems, rigorous check, nondeterministic scheduling, Software, Delays, concurrent program verification]
Automatic loop-invariant generation anc refinement through selective sampling
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Automatic loop-invariant generation is important in program analysis and verification. In this paper, we propose to generate loop-invariants automatically through learning and verification. Given a Hoare triple of a program containing a loop, we start with randomly testing the program, collect program states at run-time and categorize them based on whether they satisfy the invariant to be discovered. Next, classification techniques are employed to generate a candidate loop-invariant automatically. Afterwards, we refine the candidate through selective sampling so as to overcome the lack of sufficient test cases. Only after a candidate invariant cannot be improved further through selective sampling, we verify whether it can be used to prove the Hoare triple. If it cannot, the generated counterexamples are added as new tests and we repeat the above process. Furthermore, we show that by introducing a path-sensitive learning, i.e., partitioning the program states according to program locations they visit and classifying each partition separately, we are able to learn disjunctive loop-invariants. In order to evaluate our idea, a prototype tool has been developed and the experiment results show that our approach complements existing approaches.
[program control structures, automatic loop-invariant generation, program locations, Loop-invariant, program verification, program diagnostics, Hoare triple, candidate loop-invariant, program state partitioning, Tools, disjunctive loop-invariants, Indexes, classification, Cost accounting, Computer science, selective sampling, active learning, path-sensitive learning, Prototypes, program analysis, Software, learning (artificial intelligence), program states, Testing]
FiB: Squeezing loop invariants by interpolation between forward/backward predicate transformers
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Loop invariant generation is a fundamental problem in program analysis and verification. In this work, we propose a new approach to automatically constructing inductive loop invariants. The key idea is to aggressively squeeze an inductive invariant based on Craig interpolants between forward and backward reachability analysis. We have evaluated our approach by a set of loop benchmarks, and experimental results show that our approach is promising.
[program control structures, reachability analysis, fundamental problem, program verification, program diagnostics, Tools, Craig interpolants, Reachability analysis, Sun, Computer science, Interpolation, invariant generation, interpolation, formal verification, loop benchmarks, program analysis, Syntactics, Benchmark testing, inductive invariant, inductive loop invariants, FiB]
Symlnfer: Inferring program invariants using symbolic states
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
We introduce a new technique for inferring program invariants that uses symbolic states generated by symbolic execution. Symbolic states, which consist of path conditions and constraints on local variables, are a compact description of sets of concrete program states and they can be used for both invariant inference and invariant verification. Our technique uses a counterexample-based algorithm that creates concrete states from symbolic states, infers candidate invariants from concrete states, and then verifies or refutes candidate invariants using symbolic states. The refutation case produces concrete counterexamples that prevent spurious results and allow the technique to obtain more precise invariants. This process stops when the algorithm reaches a stable set of invariants. We present Symlnfer, a tool that implements these ideas to automatically generate invariants at arbitrary locations in a Java program. The tool obtains symbolic states from Symbolic PathFinder and uses existing algorithms to infer complex (potentially nonlinear) numerical invariants. Our preliminary results show that Symlnfer is effective in using symbolic states to generate precise and useful invariants for proving program safety and analyzing program runtime complexity. We also show that Symlnfer outperforms existing invariant generation systems.
[program testing, program verification, concrete program states, program runtime complexity analysis, Complexity theory, Runtime, formal verification, program safety, Benchmark testing, Java program, symbolic states, Java, program diagnostics, Tools, Encoding, invariant verification, Symbolic PathFinder, Symlnfer, local variable constraints, symbolic execution, program invariant inference, Concrete, Inference algorithms, concrete states, path conditions]
Parsimony: An IDE for example-guided synthesis of lexers and parsers
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
We present Parsimony, a programming-by-example development environment for synthesizing lexers and parsers by example. Parsimony provides a graphical interface in which the user presents examples simply by selecting and labeling sample text in a text editor. An underlying synthesis engine then constructs syntactic rules to solve the system of constraints induced by the supplied examples. Parsimony is more expressive and usable than prior programming-by-example systems for parsers in several ways: Parsimony can (1) synthesize lexer rules in addition to productions, (2) solve for much larger constraint systems over multiple examples, rather than handling examples one-at-a-time, and (3) infer much more complex sets of productions, such as entire algebraic expression grammars, by detecting instances of well-known grammar design patterns. The results of a controlled user study across 18 participants show that users are able to perform lexing and parsing tasks faster and with fewer mistakes when using Parsimony as compared to a traditional parsing workflow.
[graphical user interfaces, computational linguistics, program synthesis, Data structures, Grammar, synthesize lexer rules, program compilers, programming-by-example development environment, synthesis engine, Standards, example-guided synthesis, supplied examples, Lexer, grammars, parser, Automata, Parsimony, Syntactics, programming-by-example systems, programming-by-example]
Mining constraints for event-based monitoring in systems of systems
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.
[Constraint mining, Heuristic algorithms, event logs, data mining, software-intensive systems of systems, temporal logic, mined constraints, monitored events, real-world industrial SoS, Data mining, formal specification, event timing, heterogeneous systems, Runtime, SoS environments, deep domain knowledge, deviation detection, process mining, learning (artificial intelligence), event data, Monitoring, event-based monitoring, Automation, runtime monitoring approaches, declarative specifications, multiple development teams, specification mining, expected behavior, systems of systems, Feature extraction, system monitoring, event occurrence, constraint language, System of systems]
Programming bots by synthesizing natural language expressions into API invocations
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. Moreover, bots will derive immeasurable value by programming them to invoke APIs for their results. Today, within the Web and Mobile development community, complex applications are being stringed together with a few lines of code - all made possible by APIs. Yet, developers today are not as empowered to program bots in much the same way. To overcome this, we introduce BotBase, a bot programming platform that dynamically synthesizes natural language user expressions into API invocations. Our solution is two faceted: Firstly, we construct an API knowledge graph to encode and evolve APIs; secondly, leveraging the above we apply techniques in NLP, ML and Entity Recognition to perform the required synthesis from natural language user expressions into API calls.
[fixed set, object-oriented programming, machine-learning classifiers, application program interfaces, API knowledge graph, Computational modeling, natural language processing, Natural languages, users expressions, user utterances, Programming, world conversations, API calls, bot programming platform, specific use-case, Machine learning, Concrete, API, natural language user expression synthesis, learning (artificial intelligence), API invocations, Meteorology, Business, BotBase]
Test suite parallelization in open-source projects: A study on its usage and impact
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Dealing with high testing costs remains an important problem in Software Engineering. Test suite parallelization is an important approach to address this problem. This paper reports our findings on the usage and impact of test suite parallelization in open-source projects. It provides recommendations to practitioners and tool developers to speed up test execution. Considering a set of 468 popular Java projects we analyzed, we found that 24% of the projects contain costly test suites but parallelization features still seem underutilized in practice - only 19.1% of costly projects use parallelization. The main reported reason for adoption resistance was the concern to deal with concurrency issues. Results suggest that, on average, developers prefer high predictability than high performance in running tests.
[test suite parallelization, Java, program testing, Instruction sets, Electrical resistance measurement, open-source projects, test execution, Java projects, parallel processing, Open source software, Resistance, parallelization features, Parallel processing, testing costs, software engineering, adoption resistance, concurrency (computers), Testing]
Systematic reduction of GUI test sequences
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Graphic user interface (GUI) is an integral part of many software applications. However, GUI testing remains a challenging task. The main problem is to generate a set of high-quality test cases, i.e., sequences of user events to cover the often large input space. Since manually crafting event sequences is labor-intensive and automated testing tools often have poor performance, we propose a new GUI testing framework to efficiently generate progressively longer event sequences while avoiding redundant sequences. Our technique for identifying the redundancy among these sequences relies on statically checking a set of simple and syntactic-level conditions, whose reduction power matches and sometimes exceeds that of classic techniques based on partial order reduction. We have evaluated our method on 17 Java Swing applications. Our experimental results show the new technique, while being sound and systematic, can achieve more than 10X reduction in the number of test sequences compared to the state-of-the-art GUI testing tools.
[Java, partial order reduction, program testing, graphical user interfaces, GUI test sequences, graphic user interface, Redundancy, systematic reduction, Tools, Systematics, software applications, automated testing tools, syntactic-level condition, redundant sequences, high-quality test cases, Java Swing applications, Graphical user interfaces, Testing]
Automatically reducing tree-structured test inputs
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Reducing the test input given to a program while preserving some property of interest is important, e.g., to localize faults or to reduce test suites. The well-known delta debugging algorithm and its derivatives automate this task by repeatedly reducing a given input. Unfortunately, these approaches are limited to blindly removing parts of the input and cannot reduce the input by restructuring it. This paper presents the Generalized Tree Reduction (GTR) algorithm, an effective and efficient technique to reduce arbitrary test inputs that can be represented as a tree, such as program code, PDF files, and XML documents. The algorithm combines tree transformations with delta debugging and a greedy backtracking algorithm. To reduce the size of the considered search space, the approach automatically specializes the tree transformations applied by the algorithm based on examples of input trees. We evaluate GTR by reducing Python files that cause interpreter crashes, JavaScript files that cause browser inconsistencies, PDF documents with malicious content, and XML files used to tests an XML validator. The GTR algorithm reduces the trees of these files to 45.3%, 3.6%, 44.2%, and 1.3% of the original size, respectively, outperforming both delta debugging and another state-of-the-art algorithm.
[program debugging, tree-structured test inputs, program testing, XML documents, tree transformations, generalized tree reduction algorithm, XML files, XML validator, GTR algorithm, greedy backtracking algorithm, tree data structures, delta debugging algorithm, greedy algorithms, Debugging, Portable document format, program code, arbitrary test inputs, Computer bugs, XML, online front-ends, Syntactics, JavaScript files, Python files, PDF files, Internet]
Synthetic data generation for statistical testing
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Usage-based statistical testing employs knowledge about the actual or anticipated usage profile of the system under test for estimating system reliability. For many systems, usage-based statistical testing involves generating synthetic test data. Such data must possess the same statistical characteristics as the actual data that the system will process during operation. Synthetic test data must further satisfy any logical validity constraints that the actual data is subject to. Targeting data-intensive systems, we propose an approach for generating synthetic test data that is both statistically representative and logically valid. The approach works by first generating a data sample that meets the desired statistical characteristics, without taking into account the logical constraints. Subsequently, the approach tweaks the generated sample to fix any logical constraint violations. The tweaking process is iterative and continuously guided toward achieving the desired statistical characteristics. We report on a realistic evaluation of the approach, where we generate a synthetic population of citizens' records for testing a public administration IT system. Results suggest that our approach is scalable and capable of simultaneously fulfilling the statistical representativeness and logical validity requirements.
[usage-based statistical testing, program testing, Unified modeling language, software reliability, system reliability, formal logic, Test Data Generation, Histograms, Model-Driven Engineering, Usage-based Statistical Testing, public administration IT system, statistical testing, synthetic test data, statistical representativeness, Statistical analysis, data analysis, data-intensive systems, Probabilistic logic, Generators, logical validity requirements, public administration, synthetic data generation, UML, logical constraints, OCL, Data models, Reliability, usage profile]
SEALANT: A detection and visualization tool for inter-app security vulnerabilities in androic
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Android's flexible communication model allows interactions among third-party apps, but it also leads to inter-app security vulnerabilities. Specifically, malicious apps can eavesdrop on interactions between other apps or exploit the functionality of those apps, which can expose a user's sensitive information to attackers. While the state-of-the-art tools have focused on detecting inter-app vulnerabilities in Android, they neither accurately analyze realistically large numbers of apps nor effectively deliver the identified issues to users. This paper presents SEALANT, a novel tool that combines static analysis and visualization techniques that, together, enable accurate identification of inter-app vulnerabilities as well as their systematic visualization. SEALANT statically analyzes architectural information of a given set of apps, infers vulnerable communication channels where inter-app attacks can be launched, and visualizes the identified information in a compositional representation. SEALANT has been demonstrated to accurately identify inter-app vulnerabilities from hundreds of real-world Android apps and to effectively deliver the identified information to users. (Demo Video: https://youtu.be/E4lLQonOdUw)
[Sealing materials, Visualization, visualization tool, program diagnostics, third-party apps, Humanoid robots, Tools, inter-app security vulnerabilities, static analysis, Data mining, Android flexible communication model, Analytical models, detecting inter-app vulnerabilities, Android (operating system), mobile computing, security of data, real-world Android apps, SEALANT statically analyzes architectural information, infers vulnerable communication channels, Androids]
Visualization support for requirements monitoring in systems of systems
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Industrial software systems are often systems of systems (SoS) whose full behavior only emerges at runtime. The systems and their interactions thus need to be continuously monitored and checked during operation to determine compliance with requirements. Many requirements monitoring approaches have been proposed. However, only few of these come with tools that present and visualize monitoring results and details on requirements violations to end users such as industrial engineers. In this tool demo paper we present visualization capabilities we have been developing motivated by industrial scenarios. Our tool complements ReMinds, an existing requirements monitoring framework, which supports collecting, aggregating, and analyzing events and event data in architecturally heterogeneous SoS. Our visualizations support a `drill-down' scenario for monitoring and diagnosis: starting from a graphical status overview of the monitored systems and their relations, engineers can view trends and statistics about performed analyses and diagnose the root cause of problems by inspecting the events and event data that led to a specific violation. Initial industry feedback we received confirms the usefulness of our tool support. Demo video: https://youtu.be/iv7kWzeNkdk..
[visualization, formal specification, industrial engineers, Runtime, tool demo paper, data visualisation, tool complements ReMinds, event data, Probes, Monitoring, Automation, architecturally heterogeneous SoS, monitored systems, Requirements monitoring, Tools, visualization support, industrial software systems, visualization capabilities, visualize monitoring results, industrial scenarios, systems of systems, drill-down scenario, Data visualization, system monitoring, Internet, initial industry feedback, System of systems]
A demonstration of simultaneous execution and editing in a development environment
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
We introduce a tool within the Code Bubbles development environment that allows for continuous execution as the programmer edits. The tool, SEEDE, shows both the intermediate and final results of execution in terms of variables, control flow, output, and graphics. These results are updated as the user edits. The user can explore the execution to find or fix bugs or use the intermediate values to help write appropriate code. A demonstration video is available at https://www.you-tube.com/watch?v=GpibSxX3Wlw.
[Java, program debugging, live coding, Navigation, public domain software, SEEDE, Debugging, Tools, Data structures, integrated development environments, Encoding, editing, user edits, Code Bubbles development environment, control flow, programmer edits, Writing, Continuous execution, debugging, continuous execution, intermediate values]
TREM: A tool for mining timed regular specifications from system traces
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Software specifications are useful for software validation, model checking, runtime verification, debugging, monitoring, etc. In context of safety-critical real-time systems, temporal properties play an important role. However, temporal properties are rarely present due to the complexity and evolutionary nature of software systems. We propose Timed Regular Expression Mining (TREM) a hosted tool for specification mining using timed regular expressions (TREs). It is designed for easy and robust mining of dominant temporal properties. TREM uses an abstract structure of the property; the framework constructs a finite state machine to serve as an acceptor. TREM is scalable, easy to access/use, and platform independent specification mining framework. The tool is tested on industrial strength software system traces such as the QNX real-time operating system using traces with more than 1.5 Million entries. The tool demonstration video can be accessed here: youtu.be/cSd_aj3_LH8.
[TREM, tool demonstration video, program verification, software systems, data mining, Specification Mining, safety-critical software, Timed Regular Expressions, Data mining, finite state machines, formal specification, software validation, dominant temporal properties, Timed Regular Expression Mining, regular expressions, platform independent specification mining framework, Real-time systems, robust mining, runtime verification, Monitoring, Debugging, Tools, software specifications, model checking, QNX real-time operating system, Automata, safety-critical real-time systems, operating systems (computers), Software, finite state machine, industrial strength software system traces]
ModelWriter: Text and model-synchronized document engineering platform
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The ModelWriter platform provides a generic framework for automated traceability analysis. In this paper, we demonstrate how this framework can be used to trace the consistency and completeness of technical documents that consist of a set of System Installation Design Principles used by Airbus to ensure the correctness of aircraft system installation. We show in particular, how the platform allows the integration of two types of reasoning: reasoning about the meaning of text using semantic parsing and description logic theorem proving; and reasoning about document structure using first-order relational logic and finite model finding for traceability analysis.
[document handling, text analysis, Atmospheric modeling, program diagnostics, Hydraulic systems, Tools, Cognition, Grammar, aircraft system installation, inference mechanisms, semantic parsing, aerospace engineering, formal logic, system installation design principles, Analytical models, Airbus, document engineering platform, ModelWriter platform, description logic theorem, Semantics, finite model, theorem proving, document structure, automated traceability analysis]
Incrementally slicing editable submodels
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Model slicers are tools which provide two services: (a) finding parts of interest in a model and (b) displaying these parts somehow or extract these parts as a new, autonomous model, which is referred to as slice or sub-model. This paper focuses on the creation of editable slices, which can be processed by model editors, analysis tools, model management tools etc. Slices are useful if, e.g., only a part of a large model shall be analyzed, compared or processed by time-consuming algorithms, or if sub-models shall be modified independently. We present a new generic incremental slicer which can slice models of arbitrary type and which creates slices which are consistent in the sense that they are editable by standard editors. It is built on top of a model differencing framework and does not require additional configuration data beyond those available in the differencing framework. The slicer can incrementally extend or reduce an existing slice if model elements shall be added or removed, even if the slice has been edited meanwhile. We demonstrate the usefulness of our slicer in several scenarios using a large UML model. A screencast of the demonstrated scenarios is provided at http://pi.informatik.uni-siegen.de/projects/SiLift/ase2017.
[Adaptation models, UML model, object-oriented programming, Unified Modeling Language, Computational modeling, model differencing framework, Unified modeling language, model elements, Tools, autonomous model, Servers, formal specification, incrementally sliced editable submodels, editable slices, model editors, generic incremental slicer, analysis tools, Data models, program visualisation, program slicing, Load modeling, model management tools]
DSSynth: An automated digital controller synthesis tool for physical plants
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
We present an automated MATLAB Toolbox, named DSSynth (Digital-System Synthesizer), to synthesize sound digital controllers for physical plants that are represented as linear timeinvariant systems with single input and output. In particular, DSSynth synthesizes digital controllers that are sound w.r.t. stability and safety specifications. DSSynth considers the complete range of approximations, including time discretization, quantization effects and finite-precision arithmetic (and its rounding errors). We demonstrate the practical value of this toolbox by automatically synthesizing stable and safe controllers for intricate physical plant models from the digital control literature. The resulting toolbox enables the application of program synthesis to real-world control engineering problems. A demonstration can be found at https://youtu.be_hLQslRcee8.
[linear timeinvariant systems, intricate physical plant models, MATLAB, Engines, Formal Synthesis, Mathematical model, control engineering computing, stability, Finite-Word Length, automated MATLAB toolbox, Transfer functions, digital control, Tools, digital control literature, sound digital controller synthesis, control system synthesis, linear systems, Digital Control Systems, Verification, physical plants, digital-system synthesizer, Digital control, automated digital controller synthesis tool, DSSynth, real-world control engineering problems, MATLAB Toolbox]
A static analysis tool with optimizations for reachability determination
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
To reduce the false positives of static analysis, many tools collect path constraints and integrate SMT solvers to filter unreachable execution paths. However, the accumulated calling and computing of SMT solvers are time and resource consuming. This paper presents TsmartLW, an alternate static analysis tool in which we implement a path constraint solving engine to speed up reachability determination. Within the engine, typical types of constraint-patterns are firstly defined based on an empirical study of a large number of code repositories. For each pattern, a constraint solving algorithm is designed and implemented. For each program, the engine predicts the most suitable strategy and then applies the strategy to solve path constraints. The experimental results on some well-known benchmarks and real-world applications show that TsmartLW is faster than some state-of-the-art static analysis tools. For example, it is 1.32&#x00D7; faster than CPAchecker and our engine is 369&#x00D7; faster than SMT solvers in solving path constraints. The demo video is available at https://www.youtube.com/watch?v=5c3ARhFclHA&amp;t=2s.
[Algorithm design and analysis, constraint-patterns, path constraint solving, Visualization, unreachable execution paths, program testing, program verification, program diagnostics, constraint solving algorithm, Tools, SMT solvers, Reachability determination, Engines, Optimization, alternate static analysis tool, formal verification, path constraint, Benchmark testing, constraint pattern, reachability determination, TsmartLW]
CogniCrypt: Supporting developers in using cryptography
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Previous research suggests that developers often struggle using low-level cryptographic APIs and, as a result, produce insecure code. When asked, developers desire, among other things, more tool support to help them use such APIs. In this paper, we present CogniCrypt, a tool that supports developers with the use of cryptographic APIs. CogniCrypt assists the developer in two ways. First, for a number of common cryptographic tasks, CogniCrypt generates code that implements the respective task in a secure manner. Currently, CogniCrypt supports tasks such as data encryption, communication over secure channels, and long-term archiving. Second, CogniCrypt continuously runs static analyses in the background to ensure a secure integration of the generated code into the developer's workspace. This video demo showcases the main features of CogniCrypt: youtube.com/watch?v=JUq5mRHfAWY.
[Ciphers, Java, application program interfaces, data encryption, CogniCrypt, Tools, static analysis, cryptography, Encryption, long-term archiving, cryptographic tasks, low-level cryptographic API, Code Analysis, Code Generation, Variability Modeling, Cryptography]
BProVe: Tool support for business process verification
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
This demo introduces BProVe, a tool supporting automated verification of Business Process models. BProVe analysis is based on a formal operational semantics defined for the BPMN 2.0 modelling language, and is provided as a freely accessible service that uses open standard formats as input data. Furthermore a plug-in for the Eclipse platform has been developed making available a tool chain supporting users in modelling and visualising, in a friendly manner, the results of the verification. Finally we have conducted a validation through more than one thousand models, showing the effectiveness of our verification tool in practice. (Demo video: https://youtu.be/iF5OM7vKtDA).
[freely accessible service, open standard formats, program verification, Atmospheric modeling, Tools, BPMN 2, business process verification, Structural Operational Semantics, formal specification, BPMN, Software Verification, Analytical models, formal operational semantics, Semantics, Collaboration, Business Processes, business process models, MAUDE, BProVe tool support, automated verification, business data processing, Business, Load modeling, Eclipse platform]
Taco: A tool to generate tensor algebra kernels
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Tensor algebra is an important computational abstraction that is increasingly used in data analytics, machine learning, engineering, and the physical sciences. However, the number of tensor expressions is unbounded, which makes it hard to develop and optimize libraries. Furthermore, the tensors are often sparse (most components are zero), which means the code has to traverse compressed formats. To support programmers we have developed taco, a code generation tool that generates dense, sparse, and mixed kernels from tensor algebra expressions. This paper describes the taco web and command-line tools and discusses the benefits of a code generator over a traditional library. See also the demo video at tensor-compiler.org/ase2017.
[code generation tool, mathematics computing, tensors, program compilers, software libraries, physical sciences, compressed formats, Libraries, sparse kernels, learning (artificial intelligence), tensor algebra kernels, code generator, compiler, Kernel, linear algebra, tensor expressions, computational abstraction, data analysis, data analytics, Tensor algebra, Tools, Indexes, machine learning, taco web, command-line tools, dense kernels, Tensile stress, mixed kernels, sparse, Linear algebra, tensor algebra expressions]
STARTS: STAtic regression test selection
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Regression testing is an important part of software development, but it can be very time consuming. Regression test selection (RTS) aims to speed up regression testing by running only impacted tests-the subset of tests that can change behavior due to code changes. We present STARTS, a tool for STAtic Regression Test Selection. Unlike dynamic RTS, STARTS requires no code instrumentation or runtime information to find impacted tests; instead, STARTS uses only compile-time information. Specifically, STARTS builds a dependency graph of program types and finds, as impacted, tests that can reach some changed type in the transitive closure of the dependency graph. STARTS is a Maven plugin that can be easily integrated into any Maven-based Java project. We find that STARTS selects on average 35.2% of tests, leading to an end-to-end runtime that is on average 81.0% of running all the tests. A video demo of STARTS can be found at https://youtu.be/PCNtk8jphrM.
[Java, program testing, program diagnostics, program type dependency graph, regression analysis, Tools, dynamic RTS, program compilers, STARTS selects, Runtime, code instrumentation, Prototypes, static regression test selection, Software, Libraries, Testing]
EventFlowSlicer: A tool for generating realistic goal-driven GUI tests
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Most automated testing techniques for graphical user interfaces (GUIs) produce test cases that are only concerned with covering the elements (widgets, menus, etc.) on the interface, or the underlying program code, with little consideration of test case semantics. This is effective for functional testing where the aim is to find as many faults as possible. However, when one wants to mimic a real user for evaluating usability, or when it is necessary to extensively test important end-user tasks of a system, or to generate examples of how to use an interface, this generation approach fails. Capture and replay techniques can be used, however there are often multiple ways to achieve a particular goal, and capturing all of these is usually too time consuming and unrealistic. Prior work on human performance regression testing introduced a constraint based method to filter test cases created by a functional test case generator, however that work did not capture the specifications, or directly generate only the required tests and considered only a single type of test goal. In this paper we present EventFlowSlicer, a tool that allows the GUI tester to specify and generate all realistic test cases relevant to achieve a stated goal. The user first captures relevant events on the interface, then adds constraints to provide restrictions on the task. An event flow graph is extracted containing only the widgets of interest for that goal. Next all test cases are generated for edges in the graph which respect the constraints. The test cases can then be replayed using a modified version of GUITAR. A video demonstration of EventFlowSlicer can be found at https://youtu.be/hw7WYz8WYVU.
[Java, program testing, graphical user interfaces, Color, human performance regression testing, Tools, important end-user tasks, automated testing techniques, functional test case generator, Keyboards, test case semantics, goal-based testing, Software test generation, Testing, Graphical user interfaces, realistic goal-driven GUI tests]
ANDROFLEET: Testing WiFi peer-to-peer mobile apps in the large
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
WiFi P2P allows mobile apps to connect to each other via WiFi without an intermediate access point. This communication mode is widely used by mobile apps to support interactions with one or more devices simultaneously. However, testing such P2P apps remains a challenge for app developers as i) existing testing frameworks lack support for WiFi P2P, and ii) WiFi P2P testing fails to scale when considering a deployment on more than two devices. In this paper, we therefore propose an acceptance testing framework, named Androfleet, to automate testing of WiFi P2P mobile apps at scale. Beyond the capability of testing point-to-point interactions under various conditions, An-drofleet supports the deployment and the emulation of a fleet of mobile devices as part of an alpha testing phase in order to assess the robustness of a WiFi P2P app once deployed in the field. To validate Androfleet, we demonstrate the detection of failing black-box acceptance tests for WiFi P2P apps and we capture the conditions under which such a mobile app can correctly work in the field. The demo video of Androfleet is made available from https://youtu.be/gJ5_Ed7XL04.
[peer-to-peer computing, program testing, acceptance testing framework, alpha testing phase, Humanoid robots, black-box acceptance tests, Mobile communication, Mobile handsets, An-drofleet, P2P testing, mobile computing, WiFi peer-to-peer mobile application testing, point-to-point interaction testing, mobile devices, Androfleet, Peer-to-peer computing, Androids, wireless LAN, Wireless fidelity, Testing]
FEMIR: A tool for recommending framework extension examples
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Software frameworks enable developers to reuse existing well tested functionalities instead of taking the burden of implementing everything from scratch. However, to meet application specific requirements, the frameworks need to be customized via extension points. This is often done by passing a framework related object as an argument to an API call. To enable such customizations, the object can be created by extending a framework class, implementing an interface, or changing the properties of the object via API calls. However, it is both a common and non-trivial task to find all the details related to the customizations. In this paper, we present a tool, called FEMIR, that utilizes partial program analysis and graph mining technique to detect, group, and rank framework extension examples. The tool extends existing code completion infrastructure to inform developers about customization choices, enabling them to browse through extension points of a framework, and frequent usages of each point in terms of code examples. A video demo is made available at https://asaduzzamanparvez.wordpress.com/femir.
[tested functionalities, software frameworks, reuse, extension, application program interfaces, graph theory, data mining, rank framework extension examples, partial program analysis, Proposals, application specific requirements, code examples, framework class, framework related object, graph mining technique, Tree data structures, Java, program diagnostics, API call, Receivers, Tools, Indexes, FEMIR, graph mining, framework, extension point, API, customization choices]
TiQi: A natural language interface for querying software project data
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Software projects produce large quantities of data such as feature requests, requirements, design artifacts, source code, tests, safety cases, release plans, and bug reports. If leveraged effectively, this data can be used to provide project intelligence that supports diverse software engineering activities such as release planning, impact analysis, and software analytics. However, project stakeholders often lack skills to formulate complex queries needed to retrieve, manipulate, and display the data in meaningful ways. To address these challenges we introduce TiQi, a natural language interface, which allows users to express software-related queries verbally or written in natural language. TiQi is a web-based tool. It visualizes available project data as a prompt to the user, accepts Natural Language (NL) queries, transforms those queries into SQL, and then executes the queries against a centralized or distributed database. Raw data is stored either directly in the database or retrieved dynamically at runtime from case tools and repositories such as Github and Jira. The transformed query is visualized back to the user as SQL and augmented UML, and raw data results are returned. Our tool demo can be found on YouTube at the following link:http://tinyurl.com/TIQIDemo.
[Structured Query Language, program debugging, public domain software, Query, Unified modeling language, software project data querying, Natural Language Interface, query processing, natural language interface, Distributed databases, distributed databases, release plans, software engineering, software projects, bug reports, release planning, software analytics, Unified Modeling Language, Natural languages, natural language interfaces, project stakeholders, source code, Tools, design artifacts, Hazards, TiQi, SQL, Project Data, complex queries, Software, safety cases, project intelligence]
Opiner: An opinion search and summarization engine for APIs
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Opinions are key determinants to many of the activities related to software development. The perceptions of developers about an API, and the choices they make about whether and how they should use it, may, to a considerable degree, be conditioned upon how other developers see and evaluate the API. Given the plethora of APIs available for a given development task and the advent of developer forums as the media to share opinions about those APIs, it can be challenging for a developer to make informed decisions about an API to support the task. We introduce Opiner, our opinion search and summarization engine for API reviews. The server side of Opiner collects and summarizes opinions about APIs by crawling online developer forums and by associating the opinions found in the forum posts to the APIs discussed in the posts. The client side of Opiner is a Website that presents different summarized viewpoints of the opinions about the APIs in an online search engine. We evaluated Opiner by asking Industrial developers to select APIs for two development tasks. We found that developers were interested to use our proposed summaries of API reviews and that while combined with Stack Overflow, Opiner helped developers to make the right decision with more accuracy and confidence. The Opiner online search engine is available at: http://opiner.polymtl.ca. A video demo is available at: https://youtu.be/XAXpfmg5Lqs.
[summary quality, search engines, development tasks, study, application program interfaces, software development, API informal documentation, API reviews, Engines, opinion summaries, Databases, summarization engine, Detectors, online developer forums, Search engines, opiner online search engine, Software, Internet, Opinion mining, Portals, industrial developers]
Defaultification refactoring: A tool for automatically converting Java methods to default
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Enabling interfaces to declare (instance) method implementations, Java 8 default methods can be used as a substitute for the ubiquitous skeletal implementation software design pattern. Performing this transformation on legacy software manually, though, may be non-trivial. The refactoring requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods. All of this is necessary to preserve type-correctness and confirm semantics preservation. We demonstrate an automated refactoring tool called MIGRATE Skeletal Implementation to Interface for transforming legacy Java code to use the new default construct. The tool, implemented as an Eclipse plug-in, is driven by an efficient, fully-automated, type constraint-based refactoring approach. It features an extensive rule set covering various corner-cases where default methods cannot be used. The resulting code is semantically equivalent to the original, more succinct, easier to comprehend, less complex, and exhibits increased modularity. A demonstration can be found at http://youtu.be/YZHIy0yePh8.
[ubiquitous skeletal implementation software design pattern, default methods, automated refactoring tool, method implementations, MIGRATE Skeletal Implementation, legacy Java code, java, refactoring, Semantics, Computer architecture, complex type hierarchies, Java, object-oriented programming, multiple implementation inheritance issues, interfaces, type constraint, defaultification refactoring, semantics preservation, type constraints, Tools, refactoring approach, dispatch precedence, software maintenance, legacy software, eclipse, Java 8 default methods, Java methods, Software, Software engineering]
Kobold: Web usability as a service
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
While Web applications have become pervasive in today's business, social interaction and information exchange, their usability is often deficient, even being a key factor for a website success. Usability problems repeat across websites, and many of them have been catalogued, but usability evaluation and repair still remains expensive. There are efforts from both the academy and industry to automate usability testing or to provide automatic statistics, but they rarely offer concrete solutions. These solutions appear as guidelines or patterns that developers can follow manually. This paper presents Kobold, a tool that detects usability problems from real user interaction (UI) events and repairs them automatically when possible, at least suggesting concrete solutions. By using the refactoring technique and its associated concept of bad smell, Kobold mines UI events to detect usability smells and applies usability refactorings on the client to correct them. The purpose of Kobold is to deliver usability advice and solutions as a service (SaaS) for developers, allowing them to respond to feedback of the real use of their applications and improve usability incrementally, even when there are no usability experts on the team. Kobold is available at: http://autorefactoring.lifia.info.unlp.edu.ar. A screencast is available at https://youtu.be/c-myYPMUh0Q.
[usability refactorings, Web applications, software quality, Servers, Kobold mines, usability evaluation, Usability Refactoring, user interaction events, Web usability, social interaction, Business, Automation, Software as a service, Tools, Web Usability, software maintenance, automatic statistics, information exchange, Concrete, Internet, Web sites, Web site, Software as a Service, Usability, business data processing]
IntPTI: Automatic integer error repair with proper-type inference
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Integer errors in C/C++ are caused by arithmetic operations yielding results which are unrepresentable in certain type. They can lead to serious safety and security issues. Due to the complicated semantics of C/C++ integers, integer errors are widely harbored in real-world programs and it is error-prone to repair them even for experts. An automatic tool is desired to 1) automatically generate fixes which assist developers to correct the buggy code, and 2) provide sufficient hints to help developers review the generated fixes and better understand integer types in C/C++. In this paper, we present a tool IntPTI that implements the desired functionalities for C programs. IntPTI infers appropriate types for variables and expressions to eliminate representation issues, and then utilizes the derived types with fix patterns codified from the successful human-written patches. IntPTI provides a user-friendly web interface which allows users to review and manage the fixes. We evaluate IntPTI on 7 real-world projects and the results show its competitive repair accuracy and its scalability on large code bases. The demo video for IntPTI is available at: https://youtu.be/9Tgd4A_FgZM.
[program debugging, Scalability, complicated semantics, security issues, user interfaces, automatic tool, Security, Runtime, competitive repair accuracy, fix pattern, Semantics, automatic integer error repair, arithmetic operations, integer error, desired functionalities, fix patterns, integer types, tool IntPTI, type inference, integer errors, program diagnostics, probability, Maintenance engineering, Tools, inference mechanisms, security of data, appropriate types, Computer bugs, derived types, proper-type inference, human computer interaction, generated fixes, Internet, C/C++ integers]
Learning effective changes for software projects
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The primary motivation of much of software analytics is decision making. How to make these decisions? Should one make decisions based on lessons that arise from within a particular project? Or should one generate these decisions from across multiple projects? This work is an attempt to answer these questions. Our work was motivated by a realization that much of the current generation software analytics tools focus primarily on prediction. Indeed prediction is a useful task, but it is usually followed by "planning" about what actions need to be taken. This research seeks to address the planning task by seeking methods that support actionable analytics by offering clear guidance on what to do. Specifically, we propose XTREE and BELLTREE algorithms for generating a set of actionable plans within and across projects. Each of these plans, if followed will improve the quality of the software project.
[actionable plans, project management, Software algorithms, Tools, software project, primary motivation, generation software analytics tools focus, planning task, actionable analytics, BELLTREE algorithms, XTREE algorithms, defect prediction, decision making, bellwethers, Software, software engineering, Planning, Decision trees, Software engineering]
Characterizing and taming non-deterministic bugs in Javascript applications
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
JavaScript has become one of the most popular programming languages for both client-side and server-side applications. In JavaScript applications, events may be generated, triggered and consumed non-deterministically. Thus, JavaScript applications may suffer from non-deterministic bugs, when events are triggered and consumed in an unexpected order. In this proposal, we aim to characterize and combat non-deterministic bugs in JavaScript applications. Specifically, we first perform a comprehensive study about real-world non-deterministic bugs in server-side JavaScript applications. In order to facilitate bug diagnosis, we further propose approaches to isolate the necessary events that are responsible for the occurrence of a failure. We also plan to design new techniques in detecting non-deterministic bugs in JavaScript applications.
[Java, program debugging, non-deterministic bug, program diagnostics, server-side applications, Debugging, JavaScript applications, Tools, empirical study, Node.js, bug detection, Proposals, system recovery, Open source software, Computer languages, record and replay, nondeterministic bugs, Computer bugs, Computer architecture, JavaScript]
Towards API-specific automatic program repair
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The domain of Automatic Program Repair (APR) had many research contributions in recent years. So far, most approaches target fixing generic bugs in programs (e.g., off-by-one errors). Nevertheless, recent studies reveal that about 50% of real bugs require API-specific fixes (e.g., adding missing API method calls or correcting method ordering), for which existing APR approaches are not designed. In this paper, we address this problem and introduce the notion of an API-specific program repair mechanism. This mechanism detects erroneous code in a similar way to existing APR approaches. However, to fix such bugs, it uses API-specific information from the erroneous code to search for API usage patterns in other software, with which we could fix the bug. We provide first insights on the applicability of this mechanism and discuss upcoming research challenges.
[Java, program debugging, Automation, Automatic Program Repair, application program interfaces, program testing, Specification Mining, API-specific information, Maintenance engineering, Fasteners, Data mining, software maintenance, API-specific automatic program repair, Computer bugs, API usage patterns, erroneous code, Benchmark testing, Software, API-specific Bugs, API-specific fixes, generic bugs]
Managing software evolution through semantic history slicing
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Software change histories are results of incremental updates made by developers. As a side-effect of the software development process, version history is a surprisingly useful source of information for understanding, maintaining and reusing software. However, traditional commit-based sequential organization of version histories lacks semantic structure and thus are insufficient for many development tasks that require high-level, semantic understanding of program functionality, such as locating feature implementations and porting hot fixes. In this work, we propose to use well-organized unit tests as identifiers for corresponding software functionalities. We then present a family of automated techniques which analyze the semantics of historical changes and assist developers in many everyday practical settings. For validation, we evaluate our approaches on a benchmark of developer-annotated version history instances obtained from real-world open source software projects on GitHub.
[Algorithm design and analysis, software development process, program testing, semantic history slicing, Heuristic algorithms, public domain software, GitHub, semantic structure, History, software changes, real-world open source software projects, Semantics, software understanding, version histories, program analysis, program slicing, sequential organization, software reuse, software maintenance, program functionality, Computer bugs, software reusability, incremental updates, software functionalities, Software, software evolution management]
Towards the automatic classification of traceability links
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
A wide range of text-based artifacts contribute to software projects (e.g., source code, test cases, use cases, project requirements, interaction diagrams, etc.). Traceability Link Recovery (TLR) is the software task in which relevant documents in these various sets are linked to one another, uncovering information about the project that is not available when considering only the documents themselves. This information is helpful for enabling other tasks such as improving test coverage, impact analysis, and ensuring that system or regulatory requirements are met. However, while traceability links are useful, performing TLR manually is time consuming and fraught with error. Previous work has applied Information Retrieval (IR) and other techniques to reduce the human effort involved; however, that effort remains significant. In this research we seek to take the next step in reducing it by using machine learning (ML) classification models to predict whether a candidate link is valid or invalid without human oversight. Preliminary results show that this approach has promise for accurately recommending valid links; however, there are several challenges that still must be addressed in order to achieve a technique with high enough performance to consider it a viable, completely automated solution.
[Measurement, text analysis, software traceability, machine learning classification models, Predictive models, project requirements, Classification algorithms, interaction diagrams, Semantics, software projects, TLR, learning (artificial intelligence), software task, relevant documents, information retrieval, traceability links, source code, software maintenance, human effort, machine learning, Tuning, automatic classification, traceability link recovery, test coverage, Software]
Towards a software vulnerability prediction model using traceable code patterns and software metrics
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Software security is an important aspect of ensuring software quality. The goal of this study is to help developers evaluate software security using traceable patterns and software metrics during development. The concept of traceable patterns is similar to design patterns but they can be automatically recognized and extracted from source code. If these patterns can better predict vulnerable code compared to traditional software metrics, they can be used in developing a vulnerability prediction model to classify code as vulnerable or not. By analyzing and comparing the performance of traceable patterns with metrics, we propose a vulnerability prediction model. This study explores the performance of some code patterns in vulnerability prediction and compares them with traditional software metrics. We use the findings to build an effective vulnerability prediction model. We evaluate security vulnerabilities reported for Apache Tomcat, Apache CXF and three stand-alone Java web applications. We use machine learning and statistical techniques for predicting vulnerabilities using traceable patterns and metrics as features. We found that patterns have a lower false negative rate and higher recall in detecting vulnerable code than the traditional software metrics.
[Java, security vulnerabilities, software vulnerability prediction model, traditional software metrics, source code, Predictive models, Tools, traceable patterns, traceable code patterns, software quality, Security, Software metrics, security of data, Software, Internet, software security, learning (artificial intelligence), vulnerable code, effective vulnerability prediction model, Testing, software metrics]
Towards search-based modelling and analysis of requirements and architecture decisions
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
Many requirements engineering and software architecture decisions are complicated by uncertainty and multiple conflicting stakeholders objectives. Using quantitative decision models helps clarify these decisions and allows the use of multi-objective simulation optimisation techniques in analysing the impact of decisions on objectives. Existing requirements and architecture decision support methods that use quantitative decision models are limited by the difficulty in elaborating problem-specific decision models and/or lack integrated tool support for automated decision analysis under uncertainty. To address these problems and facilitate requirements and architecture decision analysis, this research proposes a novel modelling language and automated decision analysis technique, implemented in a tool called RADAR. The modelling language is a simplified version of quantitative AND/OR goal models used in requirements engineering and similar to feature models used in software product lines. This research involves developing the RADAR tool and evaluating the tool's applicability, usefulness and scalability on a set of real-world examples.
[feature models, RADAR tool, software product lines, Uncertainty, architecture decision support methods, production engineering computing, Tools, multiple conflicting stakeholders objectives, Decision analysis, quantitative goal models, decision support systems, multiobjective simulation optimisation techniques, Analytical models, software architecture, quantitative decision models, requirements engineering, architecture decision analysis, software architecture decisions, Radar, decision making, problem-specific decision models, automated decision analysis technique, Stakeholders, search-based modelling a]
Privacy-aware data-intensive applications
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
None
2017
The rise of Big Data is leading to an increasing demand for data-intensive applications (DIAs), which, in many cases, are expected to process massive amounts of sensitive data. In this context, ensuring data privacy becomes paramount. While the way we design and develop DIAs has radically changed over the last few years in order to deal with Big Data, there has been relatively little effort to make such design privacy-aware. As a result, enforcing privacy policies in large-scale data processing is currently an open research problem. This thesis proposal makes one step towards this investigation: after identifying the dataflow model as the reference computational model for large-scale DIAs, (1) we propose a novel language for specifying privacy policies on dataflow applications along with (2) a dataflow rewriting mechanism to enforce such policies during DIA execution. Although a systematic evaluation still needs to be carried out, preliminary results are promising. We plan to implement our approach within a model-driven solution to ultimately simplify the design and development of privacy-aware DIAs, i.e. DIAs that ensure privacy policies at runtime.
[Access control, Data privacy, Privacy, Data-Intensive Applications, dataflow applications, dataflow rewriting mechanism, open research problem, rewriting systems, Computational modeling, privacy-aware data-intensive applications, Dataflow computing, dataflow model, data flow analysis, Big Data, large-scale DIAs, privacy policies, design privacy, reference computational model, large-scale data processing, privacy-aware DIAs, sensitive data, Data models, data privacy, DIA execution, Data Privacy, Context modeling]
